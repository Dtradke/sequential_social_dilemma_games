 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-18 09:40:05,942	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 49.98 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-18 09:40:06,236	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21422481408 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-18 12:49:51,852	WARNING util.py:137 -- The `experiment_checkpoint` operation took 26.618911504745483 seconds to complete, which may be a performance bottleneck.
slurmstepd: error: *** JOB 5013155 ON gpu003 CANCELLED AT 2021-11-18T13:41:01 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-18 13:44:08,953	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 48.83 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-18 13:44:09,255	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 19587829760 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-18 13:44:09,994	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-18 13:44:10,172	INFO trial_runner.py:169 -- Resuming trial.
2021-11-18 13:44:10,173	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-18 13:44:10,326	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
slurmstepd: error: *** JOB 5013155 ON gpu021 CANCELLED AT 2021-11-18T16:46:01 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-18 16:49:07,003	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 52.77 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-18 16:49:07,275	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21451456512 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-18 16:49:08,136	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-18 16:49:08,470	INFO trial_runner.py:169 -- Resuming trial.
2021-11-18 16:49:08,470	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-18 16:49:08,804	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
slurmstepd: error: *** JOB 5013155 ON gpu011 CANCELLED AT 2021-11-18T17:49:02 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-18 17:52:09,018	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 52.71 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-18 17:52:09,305	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21379497984 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-18 17:52:10,205	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-18 17:52:10,608	INFO trial_runner.py:169 -- Resuming trial.
2021-11-18 17:52:10,608	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-18 17:52:11,017	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
slurmstepd: error: *** JOB 5013155 ON gpu032 CANCELLED AT 2021-11-18T18:53:02 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-18 18:57:23,881	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 53.01 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-18 18:57:24,377	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21364477952 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-18 18:57:24,801	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-18 18:57:25,389	INFO trial_runner.py:169 -- Resuming trial.
2021-11-18 18:57:25,389	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-18 18:57:25,694	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-18 18:57:34,510	WARNING worker.py:1090 -- The actor or task with ID ffffffffffffffff45b95b1c0100 is pending and cannot currently be scheduled. It requires {GPU: 1.000000} for execution and {GPU: 1.000000} for placement, but this node only has remaining {node:172.17.8.5: 1.000000}, {CPU: 32.000000}, {memory: 148.974609 GiB}, {GPU: 1.000000}, {object_store_memory: 36.572266 GiB}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.
slurmstepd: error: *** JOB 5013155 ON gpu005 CANCELLED AT 2021-11-18T19:56:02 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-18 19:59:07,071	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 52.53 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-18 19:59:07,342	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21161107456 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-18 19:59:08,648	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-18 19:59:09,291	INFO trial_runner.py:169 -- Resuming trial.
2021-11-18 19:59:09,291	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-18 19:59:09,615	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-18 20:39:10,607	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5605182647705078 seconds to complete, which may be a performance bottleneck.
slurmstepd: error: *** JOB 5013155 ON gpu004 CANCELLED AT 2021-11-18T20:59:02 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-18 21:02:09,615	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 53.45 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-18 21:02:09,906	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21167382528 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-18 21:02:11,241	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-18 21:02:11,886	INFO trial_runner.py:169 -- Resuming trial.
2021-11-18 21:02:11,887	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-18 21:02:12,241	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
slurmstepd: error: *** JOB 5013155 ON gpu006 CANCELLED AT 2021-11-18T22:02:02 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-18 22:05:06,866	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 52.87 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-18 22:05:07,134	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21167276032 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-18 22:05:08,368	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-18 22:05:09,069	INFO trial_runner.py:169 -- Resuming trial.
2021-11-18 22:05:09,070	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-18 22:05:09,460	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
slurmstepd: error: *** JOB 5013155 ON gpu006 CANCELLED AT 2021-11-18T23:05:02 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-18 23:08:09,747	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 53.91 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-18 23:08:10,037	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21470466048 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-18 23:08:11,166	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-18 23:08:11,855	INFO trial_runner.py:169 -- Resuming trial.
2021-11-18 23:08:11,856	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-18 23:08:12,465	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-18 23:27:15,729	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5163624286651611 seconds to complete, which may be a performance bottleneck.
slurmstepd: error: *** JOB 5013155 ON gpu009 CANCELLED AT 2021-11-19T00:08:02 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-19 00:11:07,733	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 52.92 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-19 00:11:08,002	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21367676928 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-19 00:11:09,377	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-19 00:11:10,080	INFO trial_runner.py:169 -- Resuming trial.
2021-11-19 00:11:10,080	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-19 00:11:10,531	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
slurmstepd: error: *** JOB 5013155 ON gpu028 CANCELLED AT 2021-11-19T01:11:02 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-19 01:14:09,058	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 52.47 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-19 01:14:09,345	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21385084928 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-19 01:14:10,553	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-19 01:14:11,286	INFO trial_runner.py:169 -- Resuming trial.
2021-11-19 01:14:11,286	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-19 01:14:11,958	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-19 01:25:03,431	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5528774261474609 seconds to complete, which may be a performance bottleneck.
2021-11-19 02:04:25,261	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5975897312164307 seconds to complete, which may be a performance bottleneck.
slurmstepd: error: *** JOB 5013155 ON gpu035 CANCELLED AT 2021-11-19T02:14:02 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-19 02:18:09,284	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 52.12 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-19 02:18:09,570	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21368242176 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-19 02:18:10,823	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-19 02:18:11,685	INFO trial_runner.py:169 -- Resuming trial.
2021-11-19 02:18:11,686	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-19 02:18:12,261	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-19 02:27:12,291	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.520329475402832 seconds to complete, which may be a performance bottleneck.
2021-11-19 03:04:13,524	WARNING util.py:137 -- The `process_trial_save` operation took 0.704509973526001 seconds to complete, which may be a performance bottleneck.
2021-11-19 03:04:13,524	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
slurmstepd: error: *** JOB 5013155 ON gpu055 CANCELLED AT 2021-11-19T03:18:02 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-19 03:21:07,513	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 52.95 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-19 03:21:07,749	WARNING services.py:928 -- Redis failed to start, retrying now.
2021-11-19 03:21:07,895	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21470425088 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-19 03:21:09,140	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-19 03:21:09,943	INFO trial_runner.py:169 -- Resuming trial.
2021-11-19 03:21:09,944	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-19 03:21:10,601	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-19 03:28:05,332	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5567309856414795 seconds to complete, which may be a performance bottleneck.
2021-11-19 04:07:34,175	WARNING util.py:137 -- The `process_trial_save` operation took 0.7093708515167236 seconds to complete, which may be a performance bottleneck.
2021-11-19 04:07:34,175	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
slurmstepd: error: *** JOB 5013155 ON gpu009 CANCELLED AT 2021-11-19T04:21:02 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-19 04:24:07,881	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 52.99 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-19 04:24:08,147	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21085724672 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-19 04:24:09,475	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-19 04:24:10,431	INFO trial_runner.py:169 -- Resuming trial.
2021-11-19 04:24:10,431	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-19 04:24:10,995	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-19 04:28:49,173	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6094515323638916 seconds to complete, which may be a performance bottleneck.
2021-11-19 04:48:22,733	WARNING util.py:137 -- The `process_trial_save` operation took 0.5213263034820557 seconds to complete, which may be a performance bottleneck.
2021-11-19 04:48:22,734	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-19 05:07:49,221	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5764827728271484 seconds to complete, which may be a performance bottleneck.
2021-11-19 05:09:58,875	WARNING util.py:137 -- The `process_trial_save` operation took 0.5199923515319824 seconds to complete, which may be a performance bottleneck.
2021-11-19 05:09:58,875	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
slurmstepd: error: *** JOB 5013155 ON gpu014 CANCELLED AT 2021-11-19T05:24:02 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-19 05:27:13,016	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 52.95 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-19 05:27:13,331	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21449441280 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-19 05:27:15,031	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-19 05:27:15,769	INFO trial_runner.py:169 -- Resuming trial.
2021-11-19 05:27:15,770	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-19 05:27:16,437	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-19 05:32:24,991	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0595941543579102 seconds to complete, which may be a performance bottleneck.
2021-11-19 05:54:09,940	WARNING util.py:137 -- The `process_trial_save` operation took 0.5960040092468262 seconds to complete, which may be a performance bottleneck.
2021-11-19 05:54:09,940	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-19 06:06:12,916	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6054487228393555 seconds to complete, which may be a performance bottleneck.
2021-11-19 06:11:09,156	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6300363540649414 seconds to complete, which may be a performance bottleneck.
2021-11-19 06:18:33,454	WARNING util.py:137 -- The `process_trial_save` operation took 0.6315128803253174 seconds to complete, which may be a performance bottleneck.
2021-11-19 06:18:33,454	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-19 06:28:26,723	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5752818584442139 seconds to complete, which may be a performance bottleneck.
2021-11-19 06:40:40,690	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5344996452331543 seconds to complete, which may be a performance bottleneck.
2021-11-19 06:43:04,890	WARNING util.py:137 -- The `process_trial_save` operation took 0.8501870632171631 seconds to complete, which may be a performance bottleneck.
2021-11-19 06:43:04,890	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-19 06:47:51,582	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5831761360168457 seconds to complete, which may be a performance bottleneck.
2021-11-19 06:59:54,065	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5527584552764893 seconds to complete, which may be a performance bottleneck.
2021-11-19 07:04:40,807	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5425100326538086 seconds to complete, which may be a performance bottleneck.
2021-11-19 07:07:05,309	WARNING util.py:137 -- The `process_trial_save` operation took 0.6767382621765137 seconds to complete, which may be a performance bottleneck.
2021-11-19 07:07:05,310	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-19 07:14:22,038	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.518099308013916 seconds to complete, which may be a performance bottleneck.
2021-11-19 07:19:09,369	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5524256229400635 seconds to complete, which may be a performance bottleneck.
2021-11-19 07:21:35,043	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8251533508300781 seconds to complete, which may be a performance bottleneck.
2021-11-19 07:26:36,548	WARNING util.py:137 -- The `experiment_checkpoint` operation took 12.429300546646118 seconds to complete, which may be a performance bottleneck.
2021-11-19 07:31:18,007	WARNING util.py:137 -- The `process_trial_save` operation took 0.7371826171875 seconds to complete, which may be a performance bottleneck.
2021-11-19 07:31:18,007	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-19 07:38:29,173	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5080127716064453 seconds to complete, which may be a performance bottleneck.
2021-11-19 07:48:05,365	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5037059783935547 seconds to complete, which may be a performance bottleneck.
2021-11-19 07:50:27,815	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5913527011871338 seconds to complete, which may be a performance bottleneck.
2021-11-19 07:52:50,738	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5565264225006104 seconds to complete, which may be a performance bottleneck.
2021-11-19 07:55:14,632	WARNING util.py:137 -- The `process_trial_save` operation took 0.9807806015014648 seconds to complete, which may be a performance bottleneck.
2021-11-19 07:55:14,633	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-19 08:00:00,568	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5151524543762207 seconds to complete, which may be a performance bottleneck.
2021-11-19 08:09:40,694	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5770730972290039 seconds to complete, which may be a performance bottleneck.
slurmstepd: error: *** JOB 5013155 ON gpu083 CANCELLED AT 2021-11-19T08:12:02 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-19 08:15:08,767	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 52.77 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-19 08:15:09,038	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21099819008 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-19 08:15:10,538	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-19 08:15:11,507	INFO trial_runner.py:169 -- Resuming trial.
2021-11-19 08:15:11,507	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-19 08:15:12,183	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-19 08:25:48,303	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5152003765106201 seconds to complete, which may be a performance bottleneck.
2021-11-19 08:33:56,320	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5909857749938965 seconds to complete, which may be a performance bottleneck.
2021-11-19 08:37:59,381	WARNING util.py:137 -- The `process_trial_save` operation took 0.6109564304351807 seconds to complete, which may be a performance bottleneck.
2021-11-19 08:37:59,381	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-19 08:44:06,920	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6805815696716309 seconds to complete, which may be a performance bottleneck.
2021-11-19 08:48:10,785	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5024292469024658 seconds to complete, which may be a performance bottleneck.
2021-11-19 08:56:17,413	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5345520973205566 seconds to complete, which may be a performance bottleneck.
2021-11-19 08:58:20,402	WARNING util.py:137 -- The `process_trial_save` operation took 0.6312720775604248 seconds to complete, which may be a performance bottleneck.
2021-11-19 08:58:20,403	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-19 09:12:32,646	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6263864040374756 seconds to complete, which may be a performance bottleneck.
slurmstepd: error: *** JOB 5013155 ON gpu046 CANCELLED AT 2021-11-19T09:15:02 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-19 09:18:07,365	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 53.83 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-19 09:18:07,637	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21460697088 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-19 09:18:09,191	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-19 09:18:10,223	INFO trial_runner.py:169 -- Resuming trial.
2021-11-19 09:18:10,223	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-19 09:18:10,941	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-19 09:36:22,113	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5064029693603516 seconds to complete, which may be a performance bottleneck.
2021-11-19 09:38:35,206	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.604665994644165 seconds to complete, which may be a performance bottleneck.
2021-11-19 09:40:49,347	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5622420310974121 seconds to complete, which may be a performance bottleneck.
2021-11-19 09:43:01,528	WARNING util.py:137 -- The `process_trial_save` operation took 0.6453053951263428 seconds to complete, which may be a performance bottleneck.
2021-11-19 09:43:01,528	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-19 09:47:28,312	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5503537654876709 seconds to complete, which may be a performance bottleneck.
2021-11-19 09:49:40,930	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8602297306060791 seconds to complete, which may be a performance bottleneck.
2021-11-19 09:54:07,561	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5496494770050049 seconds to complete, which may be a performance bottleneck.
2021-11-19 09:56:19,995	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6238644123077393 seconds to complete, which may be a performance bottleneck.
2021-11-19 09:58:32,407	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5298693180084229 seconds to complete, which may be a performance bottleneck.
2021-11-19 10:00:46,512	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5798487663269043 seconds to complete, which may be a performance bottleneck.
2021-11-19 10:03:01,299	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.550398588180542 seconds to complete, which may be a performance bottleneck.
2021-11-19 10:05:13,803	WARNING util.py:137 -- The `process_trial_save` operation took 0.6671662330627441 seconds to complete, which may be a performance bottleneck.
2021-11-19 10:05:13,804	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-19 10:07:26,661	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5139892101287842 seconds to complete, which may be a performance bottleneck.
2021-11-19 10:11:50,623	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6222527027130127 seconds to complete, which may be a performance bottleneck.
2021-11-19 10:14:04,902	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5128438472747803 seconds to complete, which may be a performance bottleneck.
2021-11-19 10:16:18,234	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5609903335571289 seconds to complete, which may be a performance bottleneck.
slurmstepd: error: *** JOB 5013155 ON gpu005 CANCELLED AT 2021-11-19T10:18:02 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-19 10:21:08,126	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 55.38 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-19 10:21:08,440	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21429551104 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-19 10:21:10,304	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-19 10:21:11,134	INFO trial_runner.py:169 -- Resuming trial.
2021-11-19 10:21:11,135	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-19 10:21:12,127	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-19 10:26:01,676	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5177102088928223 seconds to complete, which may be a performance bottleneck.
2021-11-19 10:30:44,323	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5787045955657959 seconds to complete, which may be a performance bottleneck.
2021-11-19 10:33:05,545	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.536095142364502 seconds to complete, which may be a performance bottleneck.
2021-11-19 10:37:48,062	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5144400596618652 seconds to complete, which may be a performance bottleneck.
2021-11-19 10:40:08,386	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5385735034942627 seconds to complete, which may be a performance bottleneck.
2021-11-19 10:44:50,587	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5169413089752197 seconds to complete, which may be a performance bottleneck.
2021-11-19 10:47:11,029	WARNING util.py:137 -- The `process_trial_save` operation took 0.7646129131317139 seconds to complete, which may be a performance bottleneck.
2021-11-19 10:47:11,030	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-19 10:49:31,894	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5080752372741699 seconds to complete, which may be a performance bottleneck.
2021-11-19 10:51:51,986	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6536798477172852 seconds to complete, which may be a performance bottleneck.
2021-11-19 10:58:57,780	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5274879932403564 seconds to complete, which may be a performance bottleneck.
2021-11-19 11:03:39,172	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5407421588897705 seconds to complete, which may be a performance bottleneck.
2021-11-19 11:05:57,396	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5075883865356445 seconds to complete, which may be a performance bottleneck.
2021-11-19 11:08:17,005	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5416185855865479 seconds to complete, which may be a performance bottleneck.
2021-11-19 11:10:38,162	WARNING util.py:137 -- The `process_trial_save` operation took 0.745877742767334 seconds to complete, which may be a performance bottleneck.
2021-11-19 11:10:38,163	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-19 11:12:57,073	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5253324508666992 seconds to complete, which may be a performance bottleneck.
2021-11-19 11:15:15,781	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5363762378692627 seconds to complete, which may be a performance bottleneck.
2021-11-19 11:19:56,755	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5512592792510986 seconds to complete, which may be a performance bottleneck.
2021-11-19 11:22:16,166	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6807656288146973 seconds to complete, which may be a performance bottleneck.
2021-11-19 11:24:33,846	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5736722946166992 seconds to complete, which may be a performance bottleneck.
2021-11-19 11:26:52,988	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5478136539459229 seconds to complete, which may be a performance bottleneck.
2021-11-19 11:29:12,220	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6015899181365967 seconds to complete, which may be a performance bottleneck.
2021-11-19 11:31:32,285	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5656161308288574 seconds to complete, which may be a performance bottleneck.
2021-11-19 11:33:52,573	WARNING util.py:137 -- The `process_trial_save` operation took 0.7701549530029297 seconds to complete, which may be a performance bottleneck.
2021-11-19 11:33:52,573	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-19 11:36:12,124	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5917179584503174 seconds to complete, which may be a performance bottleneck.
2021-11-19 11:40:52,054	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6390054225921631 seconds to complete, which may be a performance bottleneck.
2021-11-19 11:43:11,997	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5809454917907715 seconds to complete, which may be a performance bottleneck.
2021-11-19 11:45:30,518	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.645576000213623 seconds to complete, which may be a performance bottleneck.
2021-11-19 11:47:50,225	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5039520263671875 seconds to complete, which may be a performance bottleneck.
2021-11-19 11:50:09,945	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7293787002563477 seconds to complete, which may be a performance bottleneck.
2021-11-19 11:52:28,631	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5845704078674316 seconds to complete, which may be a performance bottleneck.
2021-11-19 11:57:06,186	WARNING util.py:137 -- The `process_trial_save` operation took 0.8202493190765381 seconds to complete, which may be a performance bottleneck.
2021-11-19 11:57:06,186	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-19 11:59:25,887	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5973610877990723 seconds to complete, which may be a performance bottleneck.
2021-11-19 12:01:44,984	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5627775192260742 seconds to complete, which may be a performance bottleneck.
2021-11-19 12:06:22,426	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9640679359436035 seconds to complete, which may be a performance bottleneck.
2021-11-19 12:08:39,391	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5346002578735352 seconds to complete, which may be a performance bottleneck.
2021-11-19 12:13:17,257	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5092360973358154 seconds to complete, which may be a performance bottleneck.
2021-11-19 12:15:35,357	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5811195373535156 seconds to complete, which may be a performance bottleneck.
2021-11-19 12:17:53,611	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5131118297576904 seconds to complete, which may be a performance bottleneck.
2021-11-19 12:20:12,212	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6065573692321777 seconds to complete, which may be a performance bottleneck.
2021-11-19 12:20:13,093	WARNING util.py:137 -- The `process_trial_save` operation took 0.8418295383453369 seconds to complete, which may be a performance bottleneck.
2021-11-19 12:20:13,093	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-19 12:22:32,251	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5900688171386719 seconds to complete, which may be a performance bottleneck.
2021-11-19 12:24:50,503	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5922627449035645 seconds to complete, which may be a performance bottleneck.
2021-11-19 12:27:08,836	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.528536319732666 seconds to complete, which may be a performance bottleneck.
2021-11-19 12:29:27,781	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5575714111328125 seconds to complete, which may be a performance bottleneck.
2021-11-19 12:31:47,914	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5293424129486084 seconds to complete, which may be a performance bottleneck.
2021-11-19 12:34:06,585	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5468840599060059 seconds to complete, which may be a performance bottleneck.
slurmstepd: error: *** JOB 5013155 ON gpu088 CANCELLED AT 2021-11-19T12:36:03 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-19 12:39:07,709	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 53.83 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-19 12:39:07,998	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21461102592 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-19 12:39:10,073	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-19 12:39:10,864	INFO trial_runner.py:169 -- Resuming trial.
2021-11-19 12:39:10,864	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-19 12:39:11,815	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-19 12:41:20,802	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5242583751678467 seconds to complete, which may be a performance bottleneck.
2021-11-19 12:44:28,890	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.517958402633667 seconds to complete, which may be a performance bottleneck.
2021-11-19 12:46:49,944	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5527749061584473 seconds to complete, which may be a performance bottleneck.
2021-11-19 12:56:06,372	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5096001625061035 seconds to complete, which may be a performance bottleneck.
2021-11-19 12:58:25,846	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6162946224212646 seconds to complete, which may be a performance bottleneck.
2021-11-19 13:00:40,547	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5535032749176025 seconds to complete, which may be a performance bottleneck.
2021-11-19 13:02:56,967	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5521612167358398 seconds to complete, which may be a performance bottleneck.
2021-11-19 13:05:10,431	WARNING util.py:137 -- The `process_trial_save` operation took 0.7591650485992432 seconds to complete, which may be a performance bottleneck.
2021-11-19 13:05:10,431	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-19 13:07:29,616	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9289307594299316 seconds to complete, which may be a performance bottleneck.
2021-11-19 13:12:09,481	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5893075466156006 seconds to complete, which may be a performance bottleneck.
2021-11-19 13:14:26,572	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5375223159790039 seconds to complete, which may be a performance bottleneck.
2021-11-19 13:16:50,056	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6032454967498779 seconds to complete, which may be a performance bottleneck.
2021-11-19 13:19:09,986	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6816408634185791 seconds to complete, which may be a performance bottleneck.
2021-11-19 13:21:28,419	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.563575267791748 seconds to complete, which may be a performance bottleneck.
2021-11-19 13:23:45,246	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6724293231964111 seconds to complete, which may be a performance bottleneck.
2021-11-19 13:26:01,196	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5405070781707764 seconds to complete, which may be a performance bottleneck.
2021-11-19 13:28:17,127	WARNING util.py:137 -- The `process_trial_save` operation took 0.7752571105957031 seconds to complete, which may be a performance bottleneck.
2021-11-19 13:28:17,127	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-19 13:30:32,287	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.634244441986084 seconds to complete, which may be a performance bottleneck.
2021-11-19 13:32:46,629	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7682003974914551 seconds to complete, which may be a performance bottleneck.
2021-11-19 13:34:58,676	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5959372520446777 seconds to complete, which may be a performance bottleneck.
2021-11-19 13:37:15,245	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5921251773834229 seconds to complete, which may be a performance bottleneck.
slurmstepd: error: *** JOB 5013155 ON gpu003 CANCELLED AT 2021-11-19T13:39:02 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-19 13:42:12,106	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 55.13 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-19 13:42:12,408	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 20777398272 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-19 13:42:14,436	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-19 13:42:15,250	INFO trial_runner.py:169 -- Resuming trial.
2021-11-19 13:42:15,250	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-19 13:42:16,221	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-19 13:53:55,822	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5078847408294678 seconds to complete, which may be a performance bottleneck.
2021-11-19 13:57:59,160	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5228316783905029 seconds to complete, which may be a performance bottleneck.
2021-11-19 14:00:00,660	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6092829704284668 seconds to complete, which may be a performance bottleneck.
2021-11-19 14:04:02,163	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5323553085327148 seconds to complete, which may be a performance bottleneck.
2021-11-19 14:06:04,632	WARNING util.py:137 -- The `process_trial_save` operation took 0.8737215995788574 seconds to complete, which may be a performance bottleneck.
2021-11-19 14:06:04,633	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-19 14:08:05,514	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5003077983856201 seconds to complete, which may be a performance bottleneck.
2021-11-19 14:12:06,358	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5293900966644287 seconds to complete, which may be a performance bottleneck.
2021-11-19 14:18:19,122	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5394425392150879 seconds to complete, which may be a performance bottleneck.
2021-11-19 14:24:20,274	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.505925178527832 seconds to complete, which may be a performance bottleneck.
2021-11-19 14:26:22,087	WARNING util.py:137 -- The `process_trial_save` operation took 0.730454683303833 seconds to complete, which may be a performance bottleneck.
2021-11-19 14:26:22,087	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-19 14:28:23,223	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.510728120803833 seconds to complete, which may be a performance bottleneck.
slurmstepd: error: *** JOB 5013155 ON gpu127 CANCELLED AT 2021-11-19T14:42:02 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-19 14:45:11,510	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 49.38 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-19 14:45:11,837	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 13547282432 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-19 14:45:14,108	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-19 14:45:15,161	INFO trial_runner.py:169 -- Resuming trial.
2021-11-19 14:45:15,161	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-19 14:45:16,380	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-19 14:47:45,228	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7030019760131836 seconds to complete, which may be a performance bottleneck.
2021-11-19 14:51:35,036	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6493551731109619 seconds to complete, which may be a performance bottleneck.
2021-11-19 14:54:30,148	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5632703304290771 seconds to complete, which may be a performance bottleneck.
2021-11-19 14:57:03,939	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5215492248535156 seconds to complete, which may be a performance bottleneck.
2021-11-19 14:59:32,497	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6410927772521973 seconds to complete, which may be a performance bottleneck.
2021-11-19 15:01:59,819	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5968658924102783 seconds to complete, which may be a performance bottleneck.
2021-11-19 15:04:27,559	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5950381755828857 seconds to complete, which may be a performance bottleneck.
2021-11-19 15:06:57,945	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5909113883972168 seconds to complete, which may be a performance bottleneck.
2021-11-19 15:09:24,912	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6030707359313965 seconds to complete, which may be a performance bottleneck.
2021-11-19 15:11:50,226	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5408802032470703 seconds to complete, which may be a performance bottleneck.
2021-11-19 15:14:17,535	WARNING util.py:137 -- The `process_trial_save` operation took 1.150885820388794 seconds to complete, which may be a performance bottleneck.
2021-11-19 15:14:17,536	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-19 15:16:43,813	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.537285566329956 seconds to complete, which may be a performance bottleneck.
2021-11-19 15:19:09,294	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5530641078948975 seconds to complete, which may be a performance bottleneck.
2021-11-19 15:21:34,522	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.58540940284729 seconds to complete, which may be a performance bottleneck.
2021-11-19 15:23:59,059	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6918129920959473 seconds to complete, which may be a performance bottleneck.
2021-11-19 15:26:23,124	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6588542461395264 seconds to complete, which may be a performance bottleneck.
2021-11-19 15:28:47,250	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6813099384307861 seconds to complete, which may be a performance bottleneck.
2021-11-19 15:31:12,557	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5525462627410889 seconds to complete, which may be a performance bottleneck.
2021-11-19 15:33:37,754	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5958163738250732 seconds to complete, which may be a performance bottleneck.
2021-11-19 15:36:02,046	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.655393123626709 seconds to complete, which may be a performance bottleneck.
2021-11-19 15:38:24,725	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.50313401222229 seconds to complete, which may be a performance bottleneck.
2021-11-19 15:38:25,876	WARNING util.py:137 -- The `process_trial_save` operation took 1.107773780822754 seconds to complete, which may be a performance bottleneck.
2021-11-19 15:38:25,876	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-19 15:40:50,947	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5779073238372803 seconds to complete, which may be a performance bottleneck.
2021-11-19 15:43:14,311	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6780118942260742 seconds to complete, which may be a performance bottleneck.
2021-11-19 15:45:37,549	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6292438507080078 seconds to complete, which may be a performance bottleneck.
2021-11-19 15:48:01,985	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6303582191467285 seconds to complete, which may be a performance bottleneck.
2021-11-19 15:50:26,726	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.557121753692627 seconds to complete, which may be a performance bottleneck.
slurmstepd: error: *** JOB 5013155 ON gpu100 CANCELLED AT 2021-11-19T15:51:02 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-19 15:54:06,871	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 53.9 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-19 15:54:07,143	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21460692992 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-19 15:54:08,948	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-19 15:54:10,088	INFO trial_runner.py:169 -- Resuming trial.
2021-11-19 15:54:10,089	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-19 15:54:11,301	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-19 15:56:12,836	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5229010581970215 seconds to complete, which may be a performance bottleneck.
2021-11-19 15:58:54,115	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.517186164855957 seconds to complete, which may be a performance bottleneck.
2021-11-19 16:01:07,675	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5121753215789795 seconds to complete, which may be a performance bottleneck.
2021-11-19 16:03:19,792	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5066037178039551 seconds to complete, which may be a performance bottleneck.
2021-11-19 16:05:32,866	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5332722663879395 seconds to complete, which may be a performance bottleneck.
2021-11-19 16:07:43,240	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5547466278076172 seconds to complete, which may be a performance bottleneck.
2021-11-19 16:09:55,844	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5639190673828125 seconds to complete, which may be a performance bottleneck.
2021-11-19 16:12:07,163	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5118775367736816 seconds to complete, which may be a performance bottleneck.
2021-11-19 16:14:18,838	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5069739818572998 seconds to complete, which may be a performance bottleneck.
2021-11-19 16:18:41,889	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6833512783050537 seconds to complete, which may be a performance bottleneck.
2021-11-19 16:18:42,762	WARNING util.py:137 -- The `process_trial_save` operation took 0.8417558670043945 seconds to complete, which may be a performance bottleneck.
2021-11-19 16:18:42,763	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-19 16:20:53,577	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5847105979919434 seconds to complete, which may be a performance bottleneck.
2021-11-19 16:23:02,543	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5452404022216797 seconds to complete, which may be a performance bottleneck.
2021-11-19 16:25:11,611	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5151729583740234 seconds to complete, which may be a performance bottleneck.
2021-11-19 16:27:21,137	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5732955932617188 seconds to complete, which may be a performance bottleneck.
2021-11-19 16:29:30,773	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5282373428344727 seconds to complete, which may be a performance bottleneck.
2021-11-19 16:31:39,994	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5697627067565918 seconds to complete, which may be a performance bottleneck.
2021-11-19 16:33:48,382	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5204277038574219 seconds to complete, which may be a performance bottleneck.
2021-11-19 16:35:57,760	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5308647155761719 seconds to complete, which may be a performance bottleneck.
2021-11-19 16:38:07,461	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.552361011505127 seconds to complete, which may be a performance bottleneck.
2021-11-19 16:40:16,497	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7022566795349121 seconds to complete, which may be a performance bottleneck.
2021-11-19 16:40:17,411	WARNING util.py:137 -- The `process_trial_save` operation took 0.8824942111968994 seconds to complete, which may be a performance bottleneck.
2021-11-19 16:40:17,411	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-19 16:42:28,092	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6640679836273193 seconds to complete, which may be a performance bottleneck.
2021-11-19 16:44:36,673	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.578697681427002 seconds to complete, which may be a performance bottleneck.
2021-11-19 16:46:46,426	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5159423351287842 seconds to complete, which may be a performance bottleneck.
2021-11-19 16:48:55,000	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5637369155883789 seconds to complete, which may be a performance bottleneck.
2021-11-19 16:51:03,114	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5409765243530273 seconds to complete, which may be a performance bottleneck.
slurmstepd: error: *** JOB 5013155 ON gpu005 CANCELLED AT 2021-11-19T16:54:02 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-19 16:57:08,066	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 53.85 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-19 16:57:08,382	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21422034944 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-19 16:57:10,471	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-19 16:57:11,488	INFO trial_runner.py:169 -- Resuming trial.
2021-11-19 16:57:11,489	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-19 16:57:12,654	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-19 16:59:19,510	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7072649002075195 seconds to complete, which may be a performance bottleneck.
2021-11-19 17:02:17,395	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7845537662506104 seconds to complete, which may be a performance bottleneck.
2021-11-19 17:04:38,810	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6680774688720703 seconds to complete, which may be a performance bottleneck.
2021-11-19 17:06:59,088	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7629184722900391 seconds to complete, which may be a performance bottleneck.
2021-11-19 17:09:19,329	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.785977840423584 seconds to complete, which may be a performance bottleneck.
2021-11-19 17:11:40,423	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6541166305541992 seconds to complete, which may be a performance bottleneck.
2021-11-19 17:13:59,956	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.567133903503418 seconds to complete, which may be a performance bottleneck.
2021-11-19 17:16:17,798	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5751149654388428 seconds to complete, which may be a performance bottleneck.
2021-11-19 17:18:37,248	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6696574687957764 seconds to complete, which may be a performance bottleneck.
2021-11-19 17:20:56,229	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.015596866607666 seconds to complete, which may be a performance bottleneck.
2021-11-19 17:23:15,700	WARNING util.py:137 -- The `process_trial_save` operation took 0.9069838523864746 seconds to complete, which may be a performance bottleneck.
2021-11-19 17:23:15,701	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-19 17:25:35,533	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.71079421043396 seconds to complete, which may be a performance bottleneck.
2021-11-19 17:27:54,722	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6269345283508301 seconds to complete, which may be a performance bottleneck.
2021-11-19 17:30:16,487	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8083302974700928 seconds to complete, which may be a performance bottleneck.
2021-11-19 17:32:36,764	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6440484523773193 seconds to complete, which may be a performance bottleneck.
2021-11-19 17:34:55,890	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6582801342010498 seconds to complete, which may be a performance bottleneck.
2021-11-19 17:37:15,809	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6069166660308838 seconds to complete, which may be a performance bottleneck.
2021-11-19 17:39:38,835	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6833822727203369 seconds to complete, which may be a performance bottleneck.
2021-11-19 17:41:58,142	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7200219631195068 seconds to complete, which may be a performance bottleneck.
2021-11-19 17:44:20,914	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8652527332305908 seconds to complete, which may be a performance bottleneck.
2021-11-19 17:46:42,845	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5441596508026123 seconds to complete, which may be a performance bottleneck.
2021-11-19 17:46:43,886	WARNING util.py:137 -- The `process_trial_save` operation took 1.0029523372650146 seconds to complete, which may be a performance bottleneck.
2021-11-19 17:46:43,886	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-19 17:49:08,286	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6248342990875244 seconds to complete, which may be a performance bottleneck.
2021-11-19 17:51:32,790	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7149615287780762 seconds to complete, which may be a performance bottleneck.
2021-11-19 17:53:56,394	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6972517967224121 seconds to complete, which may be a performance bottleneck.
2021-11-19 17:56:23,623	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5972936153411865 seconds to complete, which may be a performance bottleneck.
slurmstepd: error: *** JOB 5013155 ON gpu083 CANCELLED AT 2021-11-19T17:57:02 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-19 18:00:10,125	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 53.17 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-19 18:00:10,389	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 19686354944 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-19 18:00:12,575	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-19 18:00:13,550	INFO trial_runner.py:169 -- Resuming trial.
2021-11-19 18:00:13,551	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-19 18:00:14,761	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-19 18:02:23,341	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6312916278839111 seconds to complete, which may be a performance bottleneck.
2021-11-19 18:05:19,209	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7025289535522461 seconds to complete, which may be a performance bottleneck.
2021-11-19 18:07:45,728	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7917978763580322 seconds to complete, which may be a performance bottleneck.
2021-11-19 18:10:13,034	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6374855041503906 seconds to complete, which may be a performance bottleneck.
2021-11-19 18:12:40,273	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7482006549835205 seconds to complete, which may be a performance bottleneck.
2021-11-19 18:15:06,658	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6968867778778076 seconds to complete, which may be a performance bottleneck.
2021-11-19 18:17:32,924	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7024219036102295 seconds to complete, which may be a performance bottleneck.
2021-11-19 18:19:59,753	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8942031860351562 seconds to complete, which may be a performance bottleneck.
2021-11-19 18:22:26,333	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9739010334014893 seconds to complete, which may be a performance bottleneck.
2021-11-19 18:24:51,982	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6503956317901611 seconds to complete, which may be a performance bottleneck.
2021-11-19 18:27:18,685	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5214979648590088 seconds to complete, which may be a performance bottleneck.
2021-11-19 18:27:19,651	WARNING util.py:137 -- The `process_trial_save` operation took 0.9337027072906494 seconds to complete, which may be a performance bottleneck.
2021-11-19 18:27:19,651	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-19 18:29:46,834	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7270603179931641 seconds to complete, which may be a performance bottleneck.
2021-11-19 18:32:12,946	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7316880226135254 seconds to complete, which may be a performance bottleneck.
2021-11-19 18:34:39,508	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8356809616088867 seconds to complete, which may be a performance bottleneck.
2021-11-19 18:37:05,102	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.709275484085083 seconds to complete, which may be a performance bottleneck.
2021-11-19 18:39:31,114	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7414970397949219 seconds to complete, which may be a performance bottleneck.
2021-11-19 18:41:58,324	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7967250347137451 seconds to complete, which may be a performance bottleneck.
2021-11-19 18:44:23,773	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6481258869171143 seconds to complete, which may be a performance bottleneck.
2021-11-19 18:46:50,565	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9407129287719727 seconds to complete, which may be a performance bottleneck.
2021-11-19 18:49:16,591	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7910256385803223 seconds to complete, which may be a performance bottleneck.
2021-11-19 18:51:43,085	WARNING util.py:137 -- The `process_trial_save` operation took 0.9193065166473389 seconds to complete, which may be a performance bottleneck.
2021-11-19 18:51:43,086	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-19 18:54:10,368	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8051590919494629 seconds to complete, which may be a performance bottleneck.
2021-11-19 18:56:35,828	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7276527881622314 seconds to complete, which may be a performance bottleneck.
2021-11-19 18:59:01,879	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7435097694396973 seconds to complete, which may be a performance bottleneck.
2021-11-19 19:01:28,281	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7860941886901855 seconds to complete, which may be a performance bottleneck.
2021-11-19 19:03:54,354	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.822486162185669 seconds to complete, which may be a performance bottleneck.
2021-11-19 19:06:20,037	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7553527355194092 seconds to complete, which may be a performance bottleneck.
2021-11-19 19:08:46,581	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0806999206542969 seconds to complete, which may be a performance bottleneck.
2021-11-19 19:11:11,978	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7875118255615234 seconds to complete, which may be a performance bottleneck.
2021-11-19 19:13:37,118	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6627898216247559 seconds to complete, which may be a performance bottleneck.
2021-11-19 19:16:02,965	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5370297431945801 seconds to complete, which may be a performance bottleneck.
2021-11-19 19:16:03,930	WARNING util.py:137 -- The `process_trial_save` operation took 0.933100700378418 seconds to complete, which may be a performance bottleneck.
2021-11-19 19:16:03,930	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-19 19:18:30,898	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7438738346099854 seconds to complete, which may be a performance bottleneck.
2021-11-19 19:20:56,546	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7834804058074951 seconds to complete, which may be a performance bottleneck.
2021-11-19 19:23:21,285	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8178215026855469 seconds to complete, which may be a performance bottleneck.
2021-11-19 19:25:47,288	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.778144359588623 seconds to complete, which may be a performance bottleneck.
2021-11-19 19:28:12,785	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8149163722991943 seconds to complete, which may be a performance bottleneck.
2021-11-19 19:30:38,778	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0675549507141113 seconds to complete, which may be a performance bottleneck.
2021-11-19 19:33:04,093	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6935150623321533 seconds to complete, which may be a performance bottleneck.
2021-11-19 19:35:29,390	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8131375312805176 seconds to complete, which may be a performance bottleneck.
2021-11-19 19:37:55,656	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7508118152618408 seconds to complete, which may be a performance bottleneck.
2021-11-19 19:40:20,910	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5493471622467041 seconds to complete, which may be a performance bottleneck.
2021-11-19 19:40:21,891	WARNING util.py:137 -- The `process_trial_save` operation took 0.9490256309509277 seconds to complete, which may be a performance bottleneck.
2021-11-19 19:40:21,891	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-19 19:42:48,702	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8669226169586182 seconds to complete, which may be a performance bottleneck.
2021-11-19 19:45:13,789	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7093229293823242 seconds to complete, which may be a performance bottleneck.
2021-11-19 19:47:39,499	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8322079181671143 seconds to complete, which may be a performance bottleneck.
2021-11-19 19:50:04,722	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7025256156921387 seconds to complete, which may be a performance bottleneck.
2021-11-19 19:52:30,386	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7474617958068848 seconds to complete, which may be a performance bottleneck.
2021-11-19 19:54:56,346	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9622914791107178 seconds to complete, which may be a performance bottleneck.
2021-11-19 19:57:22,653	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7675397396087646 seconds to complete, which may be a performance bottleneck.
2021-11-19 19:59:48,027	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8378279209136963 seconds to complete, which may be a performance bottleneck.
2021-11-19 20:02:13,726	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7237534523010254 seconds to complete, which may be a performance bottleneck.
2021-11-19 20:04:38,814	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.559251070022583 seconds to complete, which may be a performance bottleneck.
2021-11-19 20:04:39,807	WARNING util.py:137 -- The `process_trial_save` operation took 0.9607343673706055 seconds to complete, which may be a performance bottleneck.
2021-11-19 20:04:39,807	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-19 20:07:07,106	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8637471199035645 seconds to complete, which may be a performance bottleneck.
2021-11-19 20:09:32,748	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8606007099151611 seconds to complete, which may be a performance bottleneck.
2021-11-19 20:11:58,092	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7774295806884766 seconds to complete, which may be a performance bottleneck.
2021-11-19 20:14:24,080	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8103053569793701 seconds to complete, which may be a performance bottleneck.
2021-11-19 20:16:50,661	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0267937183380127 seconds to complete, which may be a performance bottleneck.
2021-11-19 20:19:15,707	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8089184761047363 seconds to complete, which may be a performance bottleneck.
2021-11-19 20:21:42,005	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8909244537353516 seconds to complete, which may be a performance bottleneck.
2021-11-19 20:24:08,163	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7112305164337158 seconds to complete, which may be a performance bottleneck.
2021-11-19 20:26:34,068	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7595045566558838 seconds to complete, which may be a performance bottleneck.
2021-11-19 20:28:59,674	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5610358715057373 seconds to complete, which may be a performance bottleneck.
2021-11-19 20:29:00,685	WARNING util.py:137 -- The `process_trial_save` operation took 0.9787840843200684 seconds to complete, which may be a performance bottleneck.
2021-11-19 20:29:00,685	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-19 20:31:26,599	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8979322910308838 seconds to complete, which may be a performance bottleneck.
2021-11-19 20:33:51,816	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7841751575469971 seconds to complete, which may be a performance bottleneck.
2021-11-19 20:36:18,864	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.1124470233917236 seconds to complete, which may be a performance bottleneck.
2021-11-19 20:38:44,773	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7760858535766602 seconds to complete, which may be a performance bottleneck.
2021-11-19 20:41:10,732	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8800406455993652 seconds to complete, which may be a performance bottleneck.
2021-11-19 20:43:37,515	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8448293209075928 seconds to complete, which may be a performance bottleneck.
2021-11-19 20:46:03,189	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8714666366577148 seconds to complete, which may be a performance bottleneck.
2021-11-19 20:48:30,811	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7138526439666748 seconds to complete, which may be a performance bottleneck.
2021-11-19 20:50:57,201	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7287390232086182 seconds to complete, which may be a performance bottleneck.
2021-11-19 20:53:24,012	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5629184246063232 seconds to complete, which may be a performance bottleneck.
2021-11-19 20:53:25,042	WARNING util.py:137 -- The `process_trial_save` operation took 0.9981107711791992 seconds to complete, which may be a performance bottleneck.
2021-11-19 20:53:25,042	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-19 20:55:51,161	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8827784061431885 seconds to complete, which may be a performance bottleneck.
2021-11-19 20:58:15,872	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0200471878051758 seconds to complete, which may be a performance bottleneck.
2021-11-19 21:00:40,875	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8822102546691895 seconds to complete, which may be a performance bottleneck.
2021-11-19 21:03:05,085	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7842621803283691 seconds to complete, which may be a performance bottleneck.
2021-11-19 21:05:30,846	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9066228866577148 seconds to complete, which may be a performance bottleneck.
2021-11-19 21:07:55,924	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7340443134307861 seconds to complete, which may be a performance bottleneck.
2021-11-19 21:10:21,121	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.755789041519165 seconds to complete, which may be a performance bottleneck.
2021-11-19 21:12:47,264	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.829653263092041 seconds to complete, which may be a performance bottleneck.
2021-11-19 21:15:13,324	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8883328437805176 seconds to complete, which may be a performance bottleneck.
2021-11-19 21:17:38,055	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5613584518432617 seconds to complete, which may be a performance bottleneck.
2021-11-19 21:17:39,086	WARNING util.py:137 -- The `process_trial_save` operation took 0.9993798732757568 seconds to complete, which may be a performance bottleneck.
2021-11-19 21:17:39,086	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-19 21:20:06,180	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9825561046600342 seconds to complete, which may be a performance bottleneck.
2021-11-19 21:22:30,936	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8905391693115234 seconds to complete, which may be a performance bottleneck.
2021-11-19 21:24:56,033	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8297975063323975 seconds to complete, which may be a performance bottleneck.
2021-11-19 21:27:20,572	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7561149597167969 seconds to complete, which may be a performance bottleneck.
2021-11-19 21:29:45,271	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8075308799743652 seconds to complete, which may be a performance bottleneck.
2021-11-19 21:32:08,736	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8041455745697021 seconds to complete, which may be a performance bottleneck.
2021-11-19 21:34:33,567	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7758374214172363 seconds to complete, which may be a performance bottleneck.
2021-11-19 21:36:58,949	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8672873973846436 seconds to complete, which may be a performance bottleneck.
2021-11-19 21:39:24,620	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8324434757232666 seconds to complete, which may be a performance bottleneck.
2021-11-19 21:41:49,496	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5613601207733154 seconds to complete, which may be a performance bottleneck.
2021-11-19 21:41:50,774	WARNING util.py:137 -- The `process_trial_save` operation took 1.2458760738372803 seconds to complete, which may be a performance bottleneck.
2021-11-19 21:41:50,774	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-19 21:44:17,583	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8320250511169434 seconds to complete, which may be a performance bottleneck.
2021-11-19 21:46:41,667	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8174233436584473 seconds to complete, which may be a performance bottleneck.
2021-11-19 21:49:06,243	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9829216003417969 seconds to complete, which may be a performance bottleneck.
2021-11-19 21:51:31,552	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7894635200500488 seconds to complete, which may be a performance bottleneck.
2021-11-19 21:53:55,869	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8079566955566406 seconds to complete, which may be a performance bottleneck.
2021-11-19 21:56:20,427	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7805347442626953 seconds to complete, which may be a performance bottleneck.
2021-11-19 21:58:45,809	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.843489408493042 seconds to complete, which may be a performance bottleneck.
2021-11-19 22:01:10,465	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8173849582672119 seconds to complete, which may be a performance bottleneck.
2021-11-19 22:03:35,509	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8945162296295166 seconds to complete, which may be a performance bottleneck.
2021-11-19 22:05:59,806	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5627028942108154 seconds to complete, which may be a performance bottleneck.
2021-11-19 22:06:01,058	WARNING util.py:137 -- The `process_trial_save` operation took 1.2212543487548828 seconds to complete, which may be a performance bottleneck.
2021-11-19 22:06:01,058	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-19 22:08:27,358	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9738352298736572 seconds to complete, which may be a performance bottleneck.
2021-11-19 22:10:51,998	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7907824516296387 seconds to complete, which may be a performance bottleneck.
2021-11-19 22:13:17,633	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6998777389526367 seconds to complete, which may be a performance bottleneck.
2021-11-19 22:15:43,334	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8804419040679932 seconds to complete, which may be a performance bottleneck.
2021-11-19 22:18:08,145	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6976988315582275 seconds to complete, which may be a performance bottleneck.
2021-11-19 22:20:33,033	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.789952278137207 seconds to complete, which may be a performance bottleneck.
2021-11-19 22:22:58,475	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8598654270172119 seconds to complete, which may be a performance bottleneck.
2021-11-19 22:25:23,325	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9051311016082764 seconds to complete, which may be a performance bottleneck.
2021-11-19 22:27:48,371	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8719985485076904 seconds to complete, which may be a performance bottleneck.
2021-11-19 22:30:12,510	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5863528251647949 seconds to complete, which may be a performance bottleneck.
2021-11-19 22:30:13,770	WARNING util.py:137 -- The `process_trial_save` operation took 1.2279963493347168 seconds to complete, which may be a performance bottleneck.
2021-11-19 22:30:13,771	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-19 22:32:39,847	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8981032371520996 seconds to complete, which may be a performance bottleneck.
2021-11-19 22:35:03,755	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8922059535980225 seconds to complete, which may be a performance bottleneck.
2021-11-19 22:37:29,077	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.910876989364624 seconds to complete, which may be a performance bottleneck.
2021-11-19 22:39:53,577	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8492913246154785 seconds to complete, which may be a performance bottleneck.
2021-11-19 22:42:18,233	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8304505348205566 seconds to complete, which may be a performance bottleneck.
2021-11-19 22:44:43,766	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8820695877075195 seconds to complete, which may be a performance bottleneck.
2021-11-19 22:47:08,983	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9073154926300049 seconds to complete, which may be a performance bottleneck.
2021-11-19 22:49:34,693	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.88934326171875 seconds to complete, which may be a performance bottleneck.
2021-11-19 22:52:00,607	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9332377910614014 seconds to complete, which may be a performance bottleneck.
2021-11-19 22:54:25,232	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7425384521484375 seconds to complete, which may be a performance bottleneck.
2021-11-19 22:54:26,332	WARNING util.py:137 -- The `process_trial_save` operation took 1.0686674118041992 seconds to complete, which may be a performance bottleneck.
2021-11-19 22:54:26,332	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-19 22:56:52,153	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9227619171142578 seconds to complete, which may be a performance bottleneck.
2021-11-19 22:59:17,184	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7623708248138428 seconds to complete, which may be a performance bottleneck.
2021-11-19 23:01:42,122	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9225552082061768 seconds to complete, which may be a performance bottleneck.
2021-11-19 23:04:06,724	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9518206119537354 seconds to complete, which may be a performance bottleneck.
2021-11-19 23:06:31,957	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9164958000183105 seconds to complete, which may be a performance bottleneck.
2021-11-19 23:08:57,164	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8790285587310791 seconds to complete, which may be a performance bottleneck.
2021-11-19 23:11:22,707	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.88034987449646 seconds to complete, which may be a performance bottleneck.
2021-11-19 23:13:47,418	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.1285967826843262 seconds to complete, which may be a performance bottleneck.
2021-11-19 23:16:12,554	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8878984451293945 seconds to complete, which may be a performance bottleneck.
2021-11-19 23:18:36,293	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.56654953956604 seconds to complete, which may be a performance bottleneck.
2021-11-19 23:18:37,402	WARNING util.py:137 -- The `process_trial_save` operation took 1.0784008502960205 seconds to complete, which may be a performance bottleneck.
2021-11-19 23:18:37,402	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-19 23:21:03,616	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0339593887329102 seconds to complete, which may be a performance bottleneck.
2021-11-19 23:23:29,520	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8640358448028564 seconds to complete, which may be a performance bottleneck.
2021-11-19 23:25:54,284	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9021332263946533 seconds to complete, which may be a performance bottleneck.
2021-11-19 23:28:19,127	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8428745269775391 seconds to complete, which may be a performance bottleneck.
2021-11-19 23:30:44,447	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9450619220733643 seconds to complete, which may be a performance bottleneck.
2021-11-19 23:33:10,832	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.1582622528076172 seconds to complete, which may be a performance bottleneck.
2021-11-19 23:35:37,151	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9140825271606445 seconds to complete, which may be a performance bottleneck.
2021-11-19 23:38:03,036	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9560739994049072 seconds to complete, which may be a performance bottleneck.
2021-11-19 23:40:28,044	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7300078868865967 seconds to complete, which may be a performance bottleneck.
2021-11-19 23:42:53,856	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.600196123123169 seconds to complete, which may be a performance bottleneck.
2021-11-19 23:42:55,043	WARNING util.py:137 -- The `process_trial_save` operation took 1.153059720993042 seconds to complete, which may be a performance bottleneck.
2021-11-19 23:42:55,044	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-19 23:45:21,212	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0419697761535645 seconds to complete, which may be a performance bottleneck.
2021-11-19 23:47:46,916	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8801674842834473 seconds to complete, which may be a performance bottleneck.
2021-11-19 23:50:11,787	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9309983253479004 seconds to complete, which may be a performance bottleneck.
2021-11-19 23:52:36,489	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0261073112487793 seconds to complete, which may be a performance bottleneck.
2021-11-19 23:55:01,208	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9170389175415039 seconds to complete, which may be a performance bottleneck.
2021-11-19 23:57:26,370	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9501018524169922 seconds to complete, which may be a performance bottleneck.
2021-11-19 23:59:51,511	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8714306354522705 seconds to complete, which may be a performance bottleneck.
2021-11-20 00:02:15,715	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9371135234832764 seconds to complete, which may be a performance bottleneck.
2021-11-20 00:04:40,973	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.920844554901123 seconds to complete, which may be a performance bottleneck.
2021-11-20 00:07:05,073	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5911364555358887 seconds to complete, which may be a performance bottleneck.
2021-11-20 00:07:06,210	WARNING util.py:137 -- The `process_trial_save` operation took 1.1065592765808105 seconds to complete, which may be a performance bottleneck.
2021-11-20 00:07:06,210	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-20 00:09:30,458	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8868355751037598 seconds to complete, which may be a performance bottleneck.
2021-11-20 00:11:56,022	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.1653614044189453 seconds to complete, which may be a performance bottleneck.
2021-11-20 00:14:20,507	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8659734725952148 seconds to complete, which may be a performance bottleneck.
2021-11-20 00:16:45,413	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.906104564666748 seconds to complete, which may be a performance bottleneck.
2021-11-20 00:19:10,595	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8060274124145508 seconds to complete, which may be a performance bottleneck.
2021-11-20 00:21:35,385	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.905259370803833 seconds to complete, which may be a performance bottleneck.
2021-11-20 00:24:00,058	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8426992893218994 seconds to complete, which may be a performance bottleneck.
2021-11-20 00:26:25,049	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8910064697265625 seconds to complete, which may be a performance bottleneck.
2021-11-20 00:28:50,055	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9076073169708252 seconds to complete, which may be a performance bottleneck.
2021-11-20 00:31:14,672	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6097133159637451 seconds to complete, which may be a performance bottleneck.
2021-11-20 00:31:16,008	WARNING util.py:137 -- The `process_trial_save` operation took 1.3046576976776123 seconds to complete, which may be a performance bottleneck.
2021-11-20 00:31:16,008	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-20 00:33:41,914	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8550751209259033 seconds to complete, which may be a performance bottleneck.
2021-11-20 00:36:06,458	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9808270931243896 seconds to complete, which may be a performance bottleneck.
2021-11-20 00:38:32,312	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8792152404785156 seconds to complete, which may be a performance bottleneck.
2021-11-20 00:40:56,047	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8394207954406738 seconds to complete, which may be a performance bottleneck.
2021-11-20 00:43:21,443	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8883240222930908 seconds to complete, which may be a performance bottleneck.
2021-11-20 00:45:46,433	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9385128021240234 seconds to complete, which may be a performance bottleneck.
2021-11-20 00:48:10,650	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8938605785369873 seconds to complete, which may be a performance bottleneck.
2021-11-20 00:50:35,013	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8210623264312744 seconds to complete, which may be a performance bottleneck.
2021-11-20 00:53:00,805	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0241518020629883 seconds to complete, which may be a performance bottleneck.
2021-11-20 00:55:26,461	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6090137958526611 seconds to complete, which may be a performance bottleneck.
2021-11-20 00:55:27,841	WARNING util.py:137 -- The `process_trial_save` operation took 1.3482282161712646 seconds to complete, which may be a performance bottleneck.
2021-11-20 00:55:27,842	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-20 00:57:54,907	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0016202926635742 seconds to complete, which may be a performance bottleneck.
2021-11-20 01:00:20,437	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8362991809844971 seconds to complete, which may be a performance bottleneck.
2021-11-20 01:02:43,009	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8952534198760986 seconds to complete, which may be a performance bottleneck.
2021-11-20 01:05:07,656	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0160589218139648 seconds to complete, which may be a performance bottleneck.
2021-11-20 01:07:33,104	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9119460582733154 seconds to complete, which may be a performance bottleneck.
2021-11-20 01:09:58,249	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9558572769165039 seconds to complete, which may be a performance bottleneck.
2021-11-20 01:12:24,822	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0158295631408691 seconds to complete, which may be a performance bottleneck.
2021-11-20 01:14:49,891	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0258674621582031 seconds to complete, which may be a performance bottleneck.
2021-11-20 01:17:15,286	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9596562385559082 seconds to complete, which may be a performance bottleneck.
2021-11-20 01:19:39,577	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6350393295288086 seconds to complete, which may be a performance bottleneck.
2021-11-20 01:19:40,980	WARNING util.py:137 -- The `process_trial_save` operation took 1.3716721534729004 seconds to complete, which may be a performance bottleneck.
2021-11-20 01:19:40,981	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-20 01:22:07,474	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0447802543640137 seconds to complete, which may be a performance bottleneck.
2021-11-20 01:24:32,358	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9292631149291992 seconds to complete, which may be a performance bottleneck.
2021-11-20 01:26:57,044	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0282347202301025 seconds to complete, which may be a performance bottleneck.
2021-11-20 01:29:21,742	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9882605075836182 seconds to complete, which may be a performance bottleneck.
2021-11-20 01:31:43,504	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8414125442504883 seconds to complete, which may be a performance bottleneck.
2021-11-20 01:34:08,658	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7323427200317383 seconds to complete, which may be a performance bottleneck.
2021-11-20 01:36:33,729	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6743643283843994 seconds to complete, which may be a performance bottleneck.
2021-11-20 01:38:59,305	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9597909450531006 seconds to complete, which may be a performance bottleneck.
2021-11-20 01:41:24,184	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.1606073379516602 seconds to complete, which may be a performance bottleneck.
2021-11-20 01:43:47,998	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6139872074127197 seconds to complete, which may be a performance bottleneck.
2021-11-20 01:43:49,199	WARNING util.py:137 -- The `process_trial_save` operation took 1.169477939605713 seconds to complete, which may be a performance bottleneck.
2021-11-20 01:43:49,199	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-20 01:46:15,177	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9855027198791504 seconds to complete, which may be a performance bottleneck.
2021-11-20 01:48:40,212	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9955711364746094 seconds to complete, which may be a performance bottleneck.
2021-11-20 01:51:05,572	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8963339328765869 seconds to complete, which may be a performance bottleneck.
2021-11-20 01:53:31,269	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0347437858581543 seconds to complete, which may be a performance bottleneck.
2021-11-20 01:55:56,227	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7899811267852783 seconds to complete, which may be a performance bottleneck.
2021-11-20 01:58:21,786	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9940814971923828 seconds to complete, which may be a performance bottleneck.
2021-11-20 02:00:47,848	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.1417629718780518 seconds to complete, which may be a performance bottleneck.
2021-11-20 02:03:12,654	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9978287220001221 seconds to complete, which may be a performance bottleneck.
2021-11-20 02:05:38,078	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0051867961883545 seconds to complete, which may be a performance bottleneck.
2021-11-20 02:08:01,498	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6157476902008057 seconds to complete, which may be a performance bottleneck.
2021-11-20 02:08:02,718	WARNING util.py:137 -- The `process_trial_save` operation took 1.1895651817321777 seconds to complete, which may be a performance bottleneck.
2021-11-20 02:08:02,719	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-20 02:10:27,584	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0757215023040771 seconds to complete, which may be a performance bottleneck.
2021-11-20 02:12:52,292	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9288489818572998 seconds to complete, which may be a performance bottleneck.
2021-11-20 02:15:17,267	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9354994297027588 seconds to complete, which may be a performance bottleneck.
2021-11-20 02:17:42,845	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9613780975341797 seconds to complete, which may be a performance bottleneck.
2021-11-20 02:20:08,732	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.873999834060669 seconds to complete, which may be a performance bottleneck.
2021-11-20 02:22:34,181	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9427452087402344 seconds to complete, which may be a performance bottleneck.
2021-11-20 02:24:57,902	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9151978492736816 seconds to complete, which may be a performance bottleneck.
2021-11-20 02:27:22,275	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.908592939376831 seconds to complete, which may be a performance bottleneck.
2021-11-20 02:29:47,622	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9924085140228271 seconds to complete, which may be a performance bottleneck.
2021-11-20 02:32:11,923	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6645162105560303 seconds to complete, which may be a performance bottleneck.
2021-11-20 02:32:13,134	WARNING util.py:137 -- The `process_trial_save` operation took 1.1799745559692383 seconds to complete, which may be a performance bottleneck.
2021-11-20 02:32:13,135	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-20 02:34:39,809	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9874296188354492 seconds to complete, which may be a performance bottleneck.
2021-11-20 02:37:06,032	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.3161499500274658 seconds to complete, which may be a performance bottleneck.
2021-11-20 02:39:31,180	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0442783832550049 seconds to complete, which may be a performance bottleneck.
2021-11-20 02:41:56,273	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.996819019317627 seconds to complete, which may be a performance bottleneck.
2021-11-20 02:44:21,254	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.992912769317627 seconds to complete, which may be a performance bottleneck.
2021-11-20 02:46:46,591	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.908341646194458 seconds to complete, which may be a performance bottleneck.
2021-11-20 02:49:11,983	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8527634143829346 seconds to complete, which may be a performance bottleneck.
2021-11-20 02:51:36,377	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.003239393234253 seconds to complete, which may be a performance bottleneck.
2021-11-20 02:54:02,418	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.017902135848999 seconds to complete, which may be a performance bottleneck.
2021-11-20 02:56:26,945	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.636998176574707 seconds to complete, which may be a performance bottleneck.
2021-11-20 02:56:28,402	WARNING util.py:137 -- The `process_trial_save` operation took 1.4256482124328613 seconds to complete, which may be a performance bottleneck.
2021-11-20 02:56:28,403	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-20 02:58:53,406	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0644018650054932 seconds to complete, which may be a performance bottleneck.
2021-11-20 03:01:18,150	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9225015640258789 seconds to complete, which may be a performance bottleneck.
2021-11-20 03:03:42,886	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9353337287902832 seconds to complete, which may be a performance bottleneck.
2021-11-20 03:06:07,999	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0420475006103516 seconds to complete, which may be a performance bottleneck.
2021-11-20 03:08:33,796	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.996039867401123 seconds to complete, which may be a performance bottleneck.
2021-11-20 03:10:58,544	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0156240463256836 seconds to complete, which may be a performance bottleneck.
2021-11-20 03:13:24,538	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9809513092041016 seconds to complete, which may be a performance bottleneck.
2021-11-20 03:15:49,009	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.048124074935913 seconds to complete, which may be a performance bottleneck.
2021-11-20 03:18:16,075	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0897090435028076 seconds to complete, which may be a performance bottleneck.
2021-11-20 03:20:40,817	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6638545989990234 seconds to complete, which may be a performance bottleneck.
2021-11-20 03:20:42,313	WARNING util.py:137 -- The `process_trial_save` operation took 1.4638965129852295 seconds to complete, which may be a performance bottleneck.
2021-11-20 03:20:42,313	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-20 03:23:08,357	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.1559441089630127 seconds to complete, which may be a performance bottleneck.
2021-11-20 03:25:31,320	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9279172420501709 seconds to complete, which may be a performance bottleneck.
2021-11-20 03:27:56,783	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0088005065917969 seconds to complete, which may be a performance bottleneck.
2021-11-20 03:30:22,756	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0588619709014893 seconds to complete, which may be a performance bottleneck.
2021-11-20 03:32:48,835	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8917531967163086 seconds to complete, which may be a performance bottleneck.
2021-11-20 03:35:14,025	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0774226188659668 seconds to complete, which may be a performance bottleneck.
2021-11-20 03:37:39,438	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0620837211608887 seconds to complete, which may be a performance bottleneck.
2021-11-20 03:40:05,487	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9994242191314697 seconds to complete, which may be a performance bottleneck.
2021-11-20 03:42:30,962	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0292432308197021 seconds to complete, which may be a performance bottleneck.
2021-11-20 03:44:56,185	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8393447399139404 seconds to complete, which may be a performance bottleneck.
2021-11-20 03:44:57,456	WARNING util.py:137 -- The `process_trial_save` operation took 1.2404298782348633 seconds to complete, which may be a performance bottleneck.
2021-11-20 03:44:57,457	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-20 03:47:23,975	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0643911361694336 seconds to complete, which may be a performance bottleneck.
2021-11-20 03:49:48,784	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9616391658782959 seconds to complete, which may be a performance bottleneck.
2021-11-20 03:52:14,097	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9810376167297363 seconds to complete, which may be a performance bottleneck.
2021-11-20 03:54:39,317	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0538921356201172 seconds to complete, which may be a performance bottleneck.
2021-11-20 03:57:03,883	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9083476066589355 seconds to complete, which may be a performance bottleneck.
2021-11-20 03:59:29,525	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0794479846954346 seconds to complete, which may be a performance bottleneck.
2021-11-20 04:01:53,704	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.2375938892364502 seconds to complete, which may be a performance bottleneck.
2021-11-20 04:04:19,254	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0337834358215332 seconds to complete, which may be a performance bottleneck.
2021-11-20 04:06:43,656	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0045573711395264 seconds to complete, which may be a performance bottleneck.
2021-11-20 04:09:07,638	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6789975166320801 seconds to complete, which may be a performance bottleneck.
2021-11-20 04:09:08,942	WARNING util.py:137 -- The `process_trial_save` operation took 1.2736878395080566 seconds to complete, which may be a performance bottleneck.
2021-11-20 04:09:08,943	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-20 04:11:35,363	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0111339092254639 seconds to complete, which may be a performance bottleneck.
2021-11-20 04:14:00,190	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9801852703094482 seconds to complete, which may be a performance bottleneck.
2021-11-20 04:16:25,016	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.1218299865722656 seconds to complete, which may be a performance bottleneck.
2021-11-20 04:18:51,159	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.1657395362854004 seconds to complete, which may be a performance bottleneck.
2021-11-20 04:21:15,568	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.070207118988037 seconds to complete, which may be a performance bottleneck.
2021-11-20 04:23:41,832	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0344667434692383 seconds to complete, which may be a performance bottleneck.
2021-11-20 04:26:06,898	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9307119846343994 seconds to complete, which may be a performance bottleneck.
2021-11-20 04:28:32,539	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0051782131195068 seconds to complete, which may be a performance bottleneck.
2021-11-20 04:30:59,005	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.1443321704864502 seconds to complete, which may be a performance bottleneck.
2021-11-20 04:33:22,954	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6799840927124023 seconds to complete, which may be a performance bottleneck.
2021-11-20 04:33:24,265	WARNING util.py:137 -- The `process_trial_save` operation took 1.2804739475250244 seconds to complete, which may be a performance bottleneck.
2021-11-20 04:33:24,265	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-20 04:35:49,694	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.124237060546875 seconds to complete, which may be a performance bottleneck.
2021-11-20 04:38:15,474	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.1809911727905273 seconds to complete, which may be a performance bottleneck.
2021-11-20 04:40:39,600	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.1379246711730957 seconds to complete, which may be a performance bottleneck.
2021-11-20 04:43:05,584	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0136373043060303 seconds to complete, which may be a performance bottleneck.
2021-11-20 04:45:30,323	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9881656169891357 seconds to complete, which may be a performance bottleneck.
2021-11-20 04:47:55,266	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9611868858337402 seconds to complete, which may be a performance bottleneck.
2021-11-20 04:50:20,819	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0988490581512451 seconds to complete, which may be a performance bottleneck.
2021-11-20 04:52:46,225	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7594997882843018 seconds to complete, which may be a performance bottleneck.
2021-11-20 04:55:11,815	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0958435535430908 seconds to complete, which may be a performance bottleneck.
2021-11-20 04:57:35,786	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.665623664855957 seconds to complete, which may be a performance bottleneck.
2021-11-20 04:57:37,309	WARNING util.py:137 -- The `process_trial_save` operation took 1.4923100471496582 seconds to complete, which may be a performance bottleneck.
2021-11-20 04:57:37,309	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-20 05:00:04,259	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.2401809692382812 seconds to complete, which may be a performance bottleneck.
2021-11-20 05:02:27,628	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0564842224121094 seconds to complete, which may be a performance bottleneck.
2021-11-20 05:04:52,468	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9488716125488281 seconds to complete, which may be a performance bottleneck.
2021-11-20 05:07:18,996	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0589475631713867 seconds to complete, which may be a performance bottleneck.
2021-11-20 05:09:44,319	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.09885835647583 seconds to complete, which may be a performance bottleneck.
2021-11-20 05:12:09,730	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0668349266052246 seconds to complete, which may be a performance bottleneck.
2021-11-20 05:14:34,869	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.001084804534912 seconds to complete, which may be a performance bottleneck.
2021-11-20 05:16:59,986	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9592182636260986 seconds to complete, which may be a performance bottleneck.
2021-11-20 05:19:25,272	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0373291969299316 seconds to complete, which may be a performance bottleneck.
2021-11-20 05:21:49,964	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8707828521728516 seconds to complete, which may be a performance bottleneck.
2021-11-20 05:21:51,314	WARNING util.py:137 -- The `process_trial_save` operation took 1.318859577178955 seconds to complete, which may be a performance bottleneck.
2021-11-20 05:21:51,315	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-20 05:24:17,990	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.3200609683990479 seconds to complete, which may be a performance bottleneck.
2021-11-20 05:26:42,854	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9678654670715332 seconds to complete, which may be a performance bottleneck.
2021-11-20 05:29:07,150	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9994111061096191 seconds to complete, which may be a performance bottleneck.
2021-11-20 05:31:32,910	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0730516910552979 seconds to complete, which may be a performance bottleneck.
2021-11-20 05:33:59,179	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9748125076293945 seconds to complete, which may be a performance bottleneck.
2021-11-20 05:36:24,637	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0938544273376465 seconds to complete, which may be a performance bottleneck.
2021-11-20 05:38:50,647	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9949483871459961 seconds to complete, which may be a performance bottleneck.
2021-11-20 05:41:16,669	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.3322792053222656 seconds to complete, which may be a performance bottleneck.
2021-11-20 05:43:41,579	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0311782360076904 seconds to complete, which may be a performance bottleneck.
2021-11-20 05:46:06,154	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6767463684082031 seconds to complete, which may be a performance bottleneck.
2021-11-20 05:46:07,512	WARNING util.py:137 -- The `process_trial_save` operation took 1.327606439590454 seconds to complete, which may be a performance bottleneck.
2021-11-20 05:46:07,513	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-20 05:48:33,672	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.000631332397461 seconds to complete, which may be a performance bottleneck.
2021-11-20 05:50:59,211	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9875397682189941 seconds to complete, which may be a performance bottleneck.
2021-11-20 05:53:24,543	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.063002347946167 seconds to complete, which may be a performance bottleneck.
2021-11-20 05:55:51,575	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.2499818801879883 seconds to complete, which may be a performance bottleneck.
2021-11-20 05:58:16,045	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.1298818588256836 seconds to complete, which may be a performance bottleneck.
2021-11-20 06:00:42,462	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0526838302612305 seconds to complete, which may be a performance bottleneck.
2021-11-20 06:03:07,914	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0299205780029297 seconds to complete, which may be a performance bottleneck.
2021-11-20 06:05:33,532	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.2211575508117676 seconds to complete, which may be a performance bottleneck.
2021-11-20 06:07:57,774	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.132598638534546 seconds to complete, which may be a performance bottleneck.
2021-11-20 06:10:21,736	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7177698612213135 seconds to complete, which may be a performance bottleneck.
2021-11-20 06:10:23,101	WARNING util.py:137 -- The `process_trial_save` operation took 1.3338236808776855 seconds to complete, which may be a performance bottleneck.
2021-11-20 06:10:23,101	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-20 06:12:49,237	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.3395848274230957 seconds to complete, which may be a performance bottleneck.
2021-11-20 06:15:13,547	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0087969303131104 seconds to complete, which may be a performance bottleneck.
2021-11-20 06:17:39,089	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0270230770111084 seconds to complete, which may be a performance bottleneck.
2021-11-20 06:20:03,862	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0916085243225098 seconds to complete, which may be a performance bottleneck.
2021-11-20 06:22:29,953	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0669782161712646 seconds to complete, which may be a performance bottleneck.
2021-11-20 06:24:55,833	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.150672435760498 seconds to complete, which may be a performance bottleneck.
2021-11-20 06:27:20,256	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9739019870758057 seconds to complete, which may be a performance bottleneck.
2021-11-20 06:29:45,930	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0149166584014893 seconds to complete, which may be a performance bottleneck.
2021-11-20 06:32:11,104	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0715875625610352 seconds to complete, which may be a performance bottleneck.
2021-11-20 06:34:35,846	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7108700275421143 seconds to complete, which may be a performance bottleneck.
2021-11-20 06:34:37,471	WARNING util.py:137 -- The `process_trial_save` operation took 1.5929455757141113 seconds to complete, which may be a performance bottleneck.
2021-11-20 06:34:37,471	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-20 06:37:03,333	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.100400447845459 seconds to complete, which may be a performance bottleneck.
2021-11-20 06:39:28,698	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9371178150177002 seconds to complete, which may be a performance bottleneck.
2021-11-20 06:41:53,454	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.1249923706054688 seconds to complete, which may be a performance bottleneck.
2021-11-20 06:44:18,620	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.1099143028259277 seconds to complete, which may be a performance bottleneck.
2021-11-20 06:46:43,963	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9882924556732178 seconds to complete, which may be a performance bottleneck.
2021-11-20 06:49:09,571	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.1181480884552002 seconds to complete, which may be a performance bottleneck.
2021-11-20 06:51:34,716	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0383028984069824 seconds to complete, which may be a performance bottleneck.
2021-11-20 06:54:01,232	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.057999849319458 seconds to complete, which may be a performance bottleneck.
2021-11-20 06:56:27,164	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.3815407752990723 seconds to complete, which may be a performance bottleneck.
2021-11-20 06:58:51,781	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7041435241699219 seconds to complete, which may be a performance bottleneck.
2021-11-20 06:58:53,183	WARNING util.py:137 -- The `process_trial_save` operation took 1.371891736984253 seconds to complete, which may be a performance bottleneck.
2021-11-20 06:58:53,184	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-20 07:01:18,553	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.1790685653686523 seconds to complete, which may be a performance bottleneck.
2021-11-20 07:03:43,401	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.1332967281341553 seconds to complete, which may be a performance bottleneck.
2021-11-20 07:06:08,779	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.069319486618042 seconds to complete, which may be a performance bottleneck.
2021-11-20 07:08:34,962	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9989113807678223 seconds to complete, which may be a performance bottleneck.
2021-11-20 07:11:00,225	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0287203788757324 seconds to complete, which may be a performance bottleneck.
2021-11-20 07:13:25,179	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.2920262813568115 seconds to complete, which may be a performance bottleneck.
2021-11-20 07:15:49,753	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0036451816558838 seconds to complete, which may be a performance bottleneck.
2021-11-20 07:18:16,897	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0791621208190918 seconds to complete, which may be a performance bottleneck.
2021-11-20 07:20:41,565	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0491163730621338 seconds to complete, which may be a performance bottleneck.
2021-11-20 07:23:06,671	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7405831813812256 seconds to complete, which may be a performance bottleneck.
2021-11-20 07:23:08,074	WARNING util.py:137 -- The `process_trial_save` operation took 1.3714070320129395 seconds to complete, which may be a performance bottleneck.
2021-11-20 07:23:08,074	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-20 07:25:34,310	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0903713703155518 seconds to complete, which may be a performance bottleneck.
2021-11-20 07:27:59,123	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0024614334106445 seconds to complete, which may be a performance bottleneck.
2021-11-20 07:30:24,472	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.404024600982666 seconds to complete, which may be a performance bottleneck.
2021-11-20 07:32:50,052	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0121266841888428 seconds to complete, which may be a performance bottleneck.
2021-11-20 07:35:15,590	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.063086748123169 seconds to complete, which may be a performance bottleneck.
2021-11-20 07:37:39,454	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.1429996490478516 seconds to complete, which may be a performance bottleneck.
2021-11-20 07:40:05,416	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0024917125701904 seconds to complete, which may be a performance bottleneck.
2021-11-20 07:42:30,637	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0726184844970703 seconds to complete, which may be a performance bottleneck.
2021-11-20 07:44:56,529	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0882883071899414 seconds to complete, which may be a performance bottleneck.
2021-11-20 07:47:21,857	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7355916500091553 seconds to complete, which may be a performance bottleneck.
2021-11-20 07:47:23,502	WARNING util.py:137 -- The `process_trial_save` operation took 1.614027976989746 seconds to complete, which may be a performance bottleneck.
2021-11-20 07:47:23,503	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-20 07:49:49,589	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.2052593231201172 seconds to complete, which may be a performance bottleneck.
2021-11-20 07:52:15,375	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.059995174407959 seconds to complete, which may be a performance bottleneck.
2021-11-20 07:54:41,524	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.1822261810302734 seconds to complete, which may be a performance bottleneck.
2021-11-20 07:57:06,069	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0232136249542236 seconds to complete, which may be a performance bottleneck.
2021-11-20 07:59:31,538	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0441410541534424 seconds to complete, which may be a performance bottleneck.
2021-11-20 08:01:56,010	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.1808662414550781 seconds to complete, which may be a performance bottleneck.
2021-11-20 08:04:21,164	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0801243782043457 seconds to complete, which may be a performance bottleneck.
2021-11-20 08:06:46,439	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0531470775604248 seconds to complete, which may be a performance bottleneck.
2021-11-20 08:09:11,900	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.1664965152740479 seconds to complete, which may be a performance bottleneck.
2021-11-20 08:11:35,960	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7378954887390137 seconds to complete, which may be a performance bottleneck.
2021-11-20 08:11:37,596	WARNING util.py:137 -- The `process_trial_save` operation took 1.6056537628173828 seconds to complete, which may be a performance bottleneck.
2021-11-20 08:11:37,597	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-20 08:14:03,566	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.279991626739502 seconds to complete, which may be a performance bottleneck.
2021-11-20 08:16:29,250	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.076136589050293 seconds to complete, which may be a performance bottleneck.
2021-11-20 08:18:54,045	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.030303716659546 seconds to complete, which may be a performance bottleneck.
2021-11-20 08:21:19,016	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0334272384643555 seconds to complete, which may be a performance bottleneck.
2021-11-20 08:23:44,160	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.1235194206237793 seconds to complete, which may be a performance bottleneck.
2021-11-20 08:26:09,735	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.2953236103057861 seconds to complete, which may be a performance bottleneck.
2021-11-20 08:28:34,105	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.145453691482544 seconds to complete, which may be a performance bottleneck.
2021-11-20 08:31:00,227	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.28035569190979 seconds to complete, which may be a performance bottleneck.
2021-11-20 08:33:26,221	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.2252006530761719 seconds to complete, which may be a performance bottleneck.
2021-11-20 08:35:50,867	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7283434867858887 seconds to complete, which may be a performance bottleneck.
2021-11-20 08:35:52,350	WARNING util.py:137 -- The `process_trial_save` operation took 1.4519855976104736 seconds to complete, which may be a performance bottleneck.
2021-11-20 08:35:52,351	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-20 08:38:18,510	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.2265541553497314 seconds to complete, which may be a performance bottleneck.
2021-11-20 08:40:43,498	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.1568021774291992 seconds to complete, which may be a performance bottleneck.
2021-11-20 08:43:09,194	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.2487456798553467 seconds to complete, which may be a performance bottleneck.
2021-11-20 08:45:35,610	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.4154067039489746 seconds to complete, which may be a performance bottleneck.
2021-11-20 08:48:01,151	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9846444129943848 seconds to complete, which may be a performance bottleneck.
2021-11-20 08:50:26,488	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.1226258277893066 seconds to complete, which may be a performance bottleneck.
2021-11-20 08:52:51,932	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.1087672710418701 seconds to complete, which may be a performance bottleneck.
2021-11-20 08:55:16,834	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.1245808601379395 seconds to complete, which may be a performance bottleneck.
2021-11-20 08:57:42,637	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0902268886566162 seconds to complete, which may be a performance bottleneck.
2021-11-20 09:00:07,106	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7592556476593018 seconds to complete, which may be a performance bottleneck.
2021-11-20 09:00:08,602	WARNING util.py:137 -- The `process_trial_save` operation took 1.4642603397369385 seconds to complete, which may be a performance bottleneck.
2021-11-20 09:00:08,603	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-20 09:02:35,175	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.4516007900238037 seconds to complete, which may be a performance bottleneck.
2021-11-20 09:04:59,445	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.059215784072876 seconds to complete, which may be a performance bottleneck.
2021-11-20 09:07:25,314	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.165302038192749 seconds to complete, which may be a performance bottleneck.
2021-11-20 09:09:51,225	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.1247365474700928 seconds to complete, which may be a performance bottleneck.
2021-11-20 09:12:16,412	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.1809799671173096 seconds to complete, which may be a performance bottleneck.
2021-11-20 09:14:42,049	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.214850664138794 seconds to complete, which may be a performance bottleneck.
2021-11-20 09:17:07,852	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.1386771202087402 seconds to complete, which may be a performance bottleneck.
2021-11-20 09:19:33,453	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.1687400341033936 seconds to complete, which may be a performance bottleneck.
2021-11-20 09:21:59,635	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.1689178943634033 seconds to complete, which may be a performance bottleneck.
2021-11-20 09:24:25,142	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7582061290740967 seconds to complete, which may be a performance bottleneck.
2021-11-20 09:24:26,870	WARNING util.py:137 -- The `process_trial_save` operation took 1.6958703994750977 seconds to complete, which may be a performance bottleneck.
2021-11-20 09:24:26,870	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-20 09:26:53,000	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.1308097839355469 seconds to complete, which may be a performance bottleneck.
2021-11-20 09:29:18,642	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0016627311706543 seconds to complete, which may be a performance bottleneck.
2021-11-20 09:31:43,773	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.170565128326416 seconds to complete, which may be a performance bottleneck.
2021-11-20 09:34:08,760	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0296485424041748 seconds to complete, which may be a performance bottleneck.
2021-11-20 09:36:34,034	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.1184966564178467 seconds to complete, which may be a performance bottleneck.
2021-11-20 09:38:58,404	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.1464226245880127 seconds to complete, which may be a performance bottleneck.
2021-11-20 09:41:23,695	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.2267093658447266 seconds to complete, which may be a performance bottleneck.
2021-11-20 09:43:48,422	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.5510330200195312 seconds to complete, which may be a performance bottleneck.
2021-11-20 09:46:14,238	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.1870715618133545 seconds to complete, which may be a performance bottleneck.
2021-11-20 09:48:39,402	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7551558017730713 seconds to complete, which may be a performance bottleneck.
2021-11-20 09:48:40,934	WARNING util.py:137 -- The `process_trial_save` operation took 1.5006341934204102 seconds to complete, which may be a performance bottleneck.
2021-11-20 09:48:40,934	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-20 09:51:07,103	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.203446388244629 seconds to complete, which may be a performance bottleneck.
2021-11-20 09:53:32,236	WARNING util.py:137 -- The `process_trial` operation took 1.7904434204101562 seconds to complete, which may be a performance bottleneck.
2021-11-20 09:53:33,110	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8739454746246338 seconds to complete, which may be a performance bottleneck.
