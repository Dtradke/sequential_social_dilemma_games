>>> RESTORING FROM:  /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics/BaselinePPOTrainer_cleanup_env_0_2021-11-17_17-38-56m07ou1p0/checkpoint_20
== Status ==
Memory usage on this node: 10.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------+--------+------------------+----------+----------+
| Trial name                           | status   | loc   |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  |       |    160 |          22017.4 | 15360000 |    18.37 |
+--------------------------------------+----------+-------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m 2021-11-18 02:19:11,586	INFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution
[2m[36m(pid=27405)[0m 2021-11-18 02:19:11,602	INFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=27405)[0m 2021-11-18 02:21:01,419	INFO trainable.py:180 -- _setup took 109.832 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(pid=27405)[0m 2021-11-18 02:21:01,419	INFO trainable.py:217 -- Getting current IP.
[2m[36m(pid=27405)[0m 2021-11-18 02:21:01,419	WARNING util.py:37 -- Install gputil for GPU system monitoring.
[2m[36m(pid=27405)[0m 2021-11-18 02:21:04,911	INFO trainable.py:217 -- Getting current IP.
[2m[36m(pid=27405)[0m 2021-11-18 02:21:04,911	INFO trainable.py:423 -- Restored on 172.17.8.11 from checkpoint: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics/BaselinePPOTrainer_cleanup_env_0_2021-11-17_17-39-29lhwwiwf8/tmp1mopclx1restore_from_object/checkpoint-160
[2m[36m(pid=27405)[0m 2021-11-18 02:21:04,911	INFO trainable.py:430 -- Current state after restoring: {'_iteration': 160, '_timesteps_total': 15360000, '_time_total': 22510.283242464066, '_episodes_total': 15360}
== Status ==
Memory usage on this node: 24.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------+--------+------------------+----------+----------+
| Trial name                           | status   | loc   |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  |       |    160 |          22017.4 | 15360000 |    18.37 |
+--------------------------------------+----------+-------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 1.7604166666666667
    apples_agent-0_min: 0
    apples_agent-1_max: 26
    apples_agent-1_mean: 1.1041666666666667
    apples_agent-1_min: 0
    apples_agent-2_max: 14
    apples_agent-2_mean: 1.875
    apples_agent-2_min: 0
    apples_agent-3_max: 25
    apples_agent-3_mean: 2.46875
    apples_agent-3_min: 0
    apples_agent-4_max: 10
    apples_agent-4_mean: 1.1458333333333333
    apples_agent-4_min: 0
    apples_agent-5_max: 4
    apples_agent-5_mean: 0.78125
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 279
    cleaning_beam_agent-0_mean: 97.97916666666667
    cleaning_beam_agent-0_min: 36
    cleaning_beam_agent-1_max: 534
    cleaning_beam_agent-1_mean: 275.4791666666667
    cleaning_beam_agent-1_min: 156
    cleaning_beam_agent-2_max: 105
    cleaning_beam_agent-2_mean: 23.697916666666668
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 210
    cleaning_beam_agent-3_mean: 99.0
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 239
    cleaning_beam_agent-4_mean: 105.21875
    cleaning_beam_agent-4_min: 15
    cleaning_beam_agent-5_max: 96
    cleaning_beam_agent-5_mean: 19.59375
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.010416666666666666
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.010416666666666666
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.010416666666666666
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.010416666666666666
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-23-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 52.0
  episode_reward_mean: 17.125
  episode_reward_min: -47.0
  episodes_this_iter: 96
  episodes_total: 15456
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 24958.19
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00030153599800541997
        entropy: 0.6388559341430664
        entropy_coeff: 0.0017600000137463212
        kl: 0.008685597218573093
        model: {}
        policy_loss: -0.021583015099167824
        total_loss: -0.020945463329553604
        vf_explained_var: 0.01723150908946991
        vf_loss: 0.24815022945404053
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00030153599800541997
        entropy: 0.5596258044242859
        entropy_coeff: 0.0017600000137463212
        kl: 0.006418864708393812
        model: {}
        policy_loss: -0.01497211679816246
        total_loss: -0.014657428488135338
        vf_explained_var: 0.04435959458351135
        vf_loss: 0.1585620939731598
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00030153599800541997
        entropy: 0.5625697374343872
        entropy_coeff: 0.0017600000137463212
        kl: 0.0072499969974160194
        model: {}
        policy_loss: -0.019459029659628868
        total_loss: -0.01895562931895256
        vf_explained_var: 0.016470447182655334
        vf_loss: 0.4352063238620758
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00030153599800541997
        entropy: 0.7013091444969177
        entropy_coeff: 0.0017600000137463212
        kl: 0.007207605522125959
        model: {}
        policy_loss: -0.017176557332277298
        total_loss: -0.016935886815190315
        vf_explained_var: 0.01151244342327118
        vf_loss: 0.3345271944999695
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00030153599800541997
        entropy: 0.7396146059036255
        entropy_coeff: 0.0017600000137463212
        kl: 0.007418882101774216
        model: {}
        policy_loss: -0.014734376221895218
        total_loss: -0.0143823754042387
        vf_explained_var: 0.01992630958557129
        vf_loss: 1.699416995048523
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00030153599800541997
        entropy: 0.8002849221229553
        entropy_coeff: 0.0017600000137463212
        kl: 0.006925270427018404
        model: {}
        policy_loss: -0.015441782772541046
        total_loss: -0.015455365180969238
        vf_explained_var: 0.022493615746498108
        vf_loss: 0.09869015216827393
    load_time_ms: 24357.626
    num_steps_sampled: 15456000
    num_steps_trained: 15456000
    sample_time_ms: 103830.048
    update_time_ms: 3456.351
  iterations_since_restore: 1
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 44.54834710743802
    ram_util_percent: 17.56694214876033
  pid: 27405
  policy_reward_max:
    agent-0: 17.0
    agent-1: 13.0
    agent-2: 22.0
    agent-3: 14.0
    agent-4: 20.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 2.625
    agent-1: 1.6666666666666667
    agent-2: 4.041666666666667
    agent-3: 3.6458333333333335
    agent-4: 3.4895833333333335
    agent-5: 1.65625
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -50.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.84946042999918
    mean_inference_ms: 14.758054272476691
    mean_processing_ms: 59.91387724519132
  time_since_restore: 160.18714141845703
  time_this_iter_s: 160.18714141845703
  time_total_s: 22670.470383882523
  timestamp: 1637220231
  timesteps_since_restore: 96000
  timesteps_this_iter: 96000
  timesteps_total: 15456000
  training_iteration: 161
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    161 |          22670.5 | 15456000 |   17.125 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 1.72
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.71
    apples_agent-1_min: 0
    apples_agent-2_max: 14
    apples_agent-2_mean: 1.78
    apples_agent-2_min: 0
    apples_agent-3_max: 14
    apples_agent-3_mean: 2.26
    apples_agent-3_min: 0
    apples_agent-4_max: 7
    apples_agent-4_mean: 0.71
    apples_agent-4_min: 0
    apples_agent-5_max: 73
    apples_agent-5_mean: 1.5
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 189
    cleaning_beam_agent-0_mean: 97.16
    cleaning_beam_agent-0_min: 18
    cleaning_beam_agent-1_max: 673
    cleaning_beam_agent-1_mean: 286.7
    cleaning_beam_agent-1_min: 139
    cleaning_beam_agent-2_max: 121
    cleaning_beam_agent-2_mean: 25.78
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 222
    cleaning_beam_agent-3_mean: 105.29
    cleaning_beam_agent-3_min: 24
    cleaning_beam_agent-4_max: 222
    cleaning_beam_agent-4_mean: 100.02
    cleaning_beam_agent-4_min: 14
    cleaning_beam_agent-5_max: 52
    cleaning_beam_agent-5_mean: 16.69
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-26-01
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 45.0
  episode_reward_mean: 15.32
  episode_reward_min: -73.0
  episodes_this_iter: 96
  episodes_total: 15552
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 18619.374
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00029554558568634093
        entropy: 0.628345787525177
        entropy_coeff: 0.0017600000137463212
        kl: 0.00445186160504818
        model: {}
        policy_loss: -0.008513499982655048
        total_loss: -0.008197570219635963
        vf_explained_var: 0.013499155640602112
        vf_loss: 5.314462184906006
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00029554558568634093
        entropy: 0.5764930248260498
        entropy_coeff: 0.0017600000137463212
        kl: 0.0064172931015491486
        model: {}
        policy_loss: -0.015923649072647095
        total_loss: -0.01564468815922737
        vf_explained_var: 0.03494803607463837
        vf_loss: 0.1013093888759613
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00029554558568634093
        entropy: 0.5675519704818726
        entropy_coeff: 0.0017600000137463212
        kl: 0.007504204288125038
        model: {}
        policy_loss: -0.019577817991375923
        total_loss: -0.019037380814552307
        vf_explained_var: 0.0294201523065567
        vf_loss: 0.3848758935928345
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00029554558568634093
        entropy: 0.7049458026885986
        entropy_coeff: 0.0017600000137463212
        kl: 0.006201361306011677
        model: {}
        policy_loss: -0.015131495893001556
        total_loss: -0.015105422586202621
        vf_explained_var: 0.005318358540534973
        vf_loss: 0.26506680250167847
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00029554558568634093
        entropy: 0.7368237376213074
        entropy_coeff: 0.0017600000137463212
        kl: 0.00960317812860012
        model: {}
        policy_loss: -0.02425394207239151
        total_loss: -0.023597395047545433
        vf_explained_var: 0.015850484371185303
        vf_loss: 0.32722368836402893
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00029554558568634093
        entropy: 0.8021649718284607
        entropy_coeff: 0.0017600000137463212
        kl: 0.006466630846261978
        model: {}
        policy_loss: -0.01382188219577074
        total_loss: -0.013930102810263634
        vf_explained_var: 0.013952270150184631
        vf_loss: 0.10263097286224365
    load_time_ms: 19387.707
    num_steps_sampled: 15552000
    num_steps_trained: 15552000
    sample_time_ms: 103213.19
    update_time_ms: 1738.487
  iterations_since_restore: 2
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.53135135135135
    ram_util_percent: 19.5627027027027
  pid: 27405
  policy_reward_max:
    agent-0: 9.0
    agent-1: 6.0
    agent-2: 22.0
    agent-3: 19.0
    agent-4: 12.0
    agent-5: 6.0
  policy_reward_mean:
    agent-0: 1.68
    agent-1: 1.49
    agent-2: 3.89
    agent-3: 3.31
    agent-4: 3.62
    agent-5: 1.33
  policy_reward_min:
    agent-0: -96.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 23.868218241285078
    mean_inference_ms: 14.147798975973338
    mean_processing_ms: 59.89314155565243
  time_since_restore: 289.61395144462585
  time_this_iter_s: 129.42681002616882
  time_total_s: 22799.89719390869
  timestamp: 1637220361
  timesteps_since_restore: 192000
  timesteps_this_iter: 96000
  timesteps_total: 15552000
  training_iteration: 162
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 36.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    162 |          22799.9 | 15552000 |    15.32 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 18
    apples_agent-0_mean: 2.11
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 0.86
    apples_agent-1_min: 0
    apples_agent-2_max: 12
    apples_agent-2_mean: 1.91
    apples_agent-2_min: 0
    apples_agent-3_max: 31
    apples_agent-3_mean: 2.56
    apples_agent-3_min: 0
    apples_agent-4_max: 8
    apples_agent-4_mean: 0.86
    apples_agent-4_min: 0
    apples_agent-5_max: 6
    apples_agent-5_mean: 0.73
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 191
    cleaning_beam_agent-0_mean: 90.43
    cleaning_beam_agent-0_min: 39
    cleaning_beam_agent-1_max: 527
    cleaning_beam_agent-1_mean: 284.3
    cleaning_beam_agent-1_min: 160
    cleaning_beam_agent-2_max: 87
    cleaning_beam_agent-2_mean: 21.97
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 205
    cleaning_beam_agent-3_mean: 90.8
    cleaning_beam_agent-3_min: 24
    cleaning_beam_agent-4_max: 371
    cleaning_beam_agent-4_mean: 94.06
    cleaning_beam_agent-4_min: 11
    cleaning_beam_agent-5_max: 42
    cleaning_beam_agent-5_mean: 17.2
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-28-10
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 60.0
  episode_reward_mean: 17.6
  episode_reward_min: 1.0
  episodes_this_iter: 96
  episodes_total: 15648
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 16511.706
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00028955520247109234
        entropy: 0.6295496225357056
        entropy_coeff: 0.0017600000137463212
        kl: 0.010703517124056816
        model: {}
        policy_loss: -0.022198285907506943
        total_loss: -0.02221301756799221
        vf_explained_var: -0.03389200568199158
        vf_loss: 0.22920745611190796
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00028955520247109234
        entropy: 0.5740020275115967
        entropy_coeff: 0.0017600000137463212
        kl: 0.006488379556685686
        model: {}
        policy_loss: -0.015172983519732952
        total_loss: -0.014868900179862976
        vf_explained_var: 0.02809363603591919
        vf_loss: 0.16651147603988647
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00028955520247109234
        entropy: 0.5617673397064209
        entropy_coeff: 0.0017600000137463212
        kl: 0.007237197831273079
        model: {}
        policy_loss: -0.019369103014469147
        total_loss: -0.018866516649723053
        vf_explained_var: 0.019057095050811768
        vf_loss: 0.4385627508163452
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00028955520247109234
        entropy: 0.6913563013076782
        entropy_coeff: 0.0017600000137463212
        kl: 0.006570172496140003
        model: {}
        policy_loss: -0.015553057193756104
        total_loss: -0.015422987751662731
        vf_explained_var: 0.010023027658462524
        vf_loss: 0.32824134826660156
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00028955520247109234
        entropy: 0.7246348261833191
        entropy_coeff: 0.0017600000137463212
        kl: 0.010000880807638168
        model: {}
        policy_loss: -0.02241337299346924
        total_loss: -0.021655511111021042
        vf_explained_var: 0.02471362054347992
        vf_loss: 0.33041131496429443
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00028955520247109234
        entropy: 0.7917004227638245
        entropy_coeff: 0.0017600000137463212
        kl: 0.006347189657390118
        model: {}
        policy_loss: -0.012464545667171478
        total_loss: -0.012571990489959717
        vf_explained_var: 0.029057756066322327
        vf_loss: 0.16510498523712158
    load_time_ms: 17710.659
    num_steps_sampled: 15648000
    num_steps_trained: 15648000
    sample_time_ms: 102900.107
    update_time_ms: 1165.525
  iterations_since_restore: 3
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 48.83804347826087
    ram_util_percent: 20.055978260869566
  pid: 27405
  policy_reward_max:
    agent-0: 10.0
    agent-1: 8.0
    agent-2: 20.0
    agent-3: 14.0
    agent-4: 13.0
    agent-5: 14.0
  policy_reward_mean:
    agent-0: 2.92
    agent-1: 1.97
    agent-2: 4.13
    agent-3: 3.32
    agent-4: 3.62
    agent-5: 1.64
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.850809023309058
    mean_inference_ms: 13.90524551026224
    mean_processing_ms: 60.00938210308577
  time_since_restore: 418.667845249176
  time_this_iter_s: 129.05389380455017
  time_total_s: 22928.95108771324
  timestamp: 1637220490
  timesteps_since_restore: 288000
  timesteps_this_iter: 96000
  timesteps_total: 15648000
  training_iteration: 163
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 37.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    163 |            22929 | 15648000 |     17.6 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 25
    apples_agent-0_mean: 2.42
    apples_agent-0_min: 0
    apples_agent-1_max: 27
    apples_agent-1_mean: 1.16
    apples_agent-1_min: 0
    apples_agent-2_max: 93
    apples_agent-2_mean: 3.4
    apples_agent-2_min: 0
    apples_agent-3_max: 16
    apples_agent-3_mean: 2.2
    apples_agent-3_min: 0
    apples_agent-4_max: 62
    apples_agent-4_mean: 1.57
    apples_agent-4_min: 0
    apples_agent-5_max: 5
    apples_agent-5_mean: 0.69
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 182
    cleaning_beam_agent-0_mean: 91.12
    cleaning_beam_agent-0_min: 26
    cleaning_beam_agent-1_max: 482
    cleaning_beam_agent-1_mean: 280.05
    cleaning_beam_agent-1_min: 172
    cleaning_beam_agent-2_max: 345
    cleaning_beam_agent-2_mean: 26.42
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 217
    cleaning_beam_agent-3_mean: 87.73
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 371
    cleaning_beam_agent-4_mean: 94.49
    cleaning_beam_agent-4_min: 22
    cleaning_beam_agent-5_max: 41
    cleaning_beam_agent-5_mean: 16.94
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-30-18
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 60.0
  episode_reward_mean: 15.73
  episode_reward_min: -35.0
  episodes_this_iter: 96
  episodes_total: 15744
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 15400.827
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002835647901520133
        entropy: 0.6190086603164673
        entropy_coeff: 0.0017600000137463212
        kl: 0.008510165847837925
        model: {}
        policy_loss: -0.010531304404139519
        total_loss: -0.010615933686494827
        vf_explained_var: 0.0005055069923400879
        vf_loss: 1.5381135940551758
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002835647901520133
        entropy: 0.5535507202148438
        entropy_coeff: 0.0017600000137463212
        kl: 0.005884254351258278
        model: {}
        policy_loss: -0.014104512520134449
        total_loss: -0.013886643573641777
        vf_explained_var: 0.030615389347076416
        vf_loss: 0.15269358456134796
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002835647901520133
        entropy: 0.5519638061523438
        entropy_coeff: 0.0017600000137463212
        kl: 0.0076446132734417915
        model: {}
        policy_loss: -0.018401142209768295
        total_loss: -0.01780175045132637
        vf_explained_var: 0.011735036969184875
        vf_loss: 0.4192585051059723
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002835647901520133
        entropy: 0.6748644113540649
        entropy_coeff: 0.0017600000137463212
        kl: 0.006924571469426155
        model: {}
        policy_loss: -0.013721003197133541
        total_loss: -0.013487637042999268
        vf_explained_var: 0.016047552227973938
        vf_loss: 0.36213505268096924
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002835647901520133
        entropy: 0.730159342288971
        entropy_coeff: 0.0017600000137463212
        kl: 0.008540819399058819
        model: {}
        policy_loss: -0.02232011780142784
        total_loss: -0.02186810038983822
        vf_explained_var: 0.013391196727752686
        vf_loss: 0.2893221080303192
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002835647901520133
        entropy: 0.7793145179748535
        entropy_coeff: 0.0017600000137463212
        kl: 0.004099154844880104
        model: {}
        policy_loss: -0.00711691752076149
        total_loss: -0.007619795389473438
        vf_explained_var: 0.012778744101524353
        vf_loss: 0.4888702630996704
    load_time_ms: 16787.931
    num_steps_sampled: 15744000
    num_steps_trained: 15744000
    sample_time_ms: 102623.303
    update_time_ms: 878.181
  iterations_since_restore: 4
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.18241758241758
    ram_util_percent: 20.25824175824176
  pid: 27405
  policy_reward_max:
    agent-0: 10.0
    agent-1: 11.0
    agent-2: 17.0
    agent-3: 15.0
    agent-4: 11.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 2.24
    agent-1: 1.89
    agent-2: 3.81
    agent-3: 3.49
    agent-4: 3.37
    agent-5: 0.93
  policy_reward_min:
    agent-0: -44.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -1.0
    agent-5: -49.0
  sampler_perf:
    mean_env_wait_ms: 23.81842750907836
    mean_inference_ms: 13.776616566758856
    mean_processing_ms: 59.94801508564276
  time_since_restore: 546.678733587265
  time_this_iter_s: 128.010888338089
  time_total_s: 23056.96197605133
  timestamp: 1637220618
  timesteps_since_restore: 384000
  timesteps_this_iter: 96000
  timesteps_total: 15744000
  training_iteration: 164
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 37.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    164 |            23057 | 15744000 |    15.73 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 25
    apples_agent-0_mean: 1.88
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 1.03
    apples_agent-1_min: 0
    apples_agent-2_max: 15
    apples_agent-2_mean: 1.64
    apples_agent-2_min: 0
    apples_agent-3_max: 10
    apples_agent-3_mean: 1.96
    apples_agent-3_min: 0
    apples_agent-4_max: 9
    apples_agent-4_mean: 0.96
    apples_agent-4_min: 0
    apples_agent-5_max: 3
    apples_agent-5_mean: 0.71
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 190
    cleaning_beam_agent-0_mean: 91.83
    cleaning_beam_agent-0_min: 26
    cleaning_beam_agent-1_max: 526
    cleaning_beam_agent-1_mean: 267.48
    cleaning_beam_agent-1_min: 142
    cleaning_beam_agent-2_max: 83
    cleaning_beam_agent-2_mean: 19.04
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 206
    cleaning_beam_agent-3_mean: 92.08
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 223
    cleaning_beam_agent-4_mean: 94.88
    cleaning_beam_agent-4_min: 26
    cleaning_beam_agent-5_max: 49
    cleaning_beam_agent-5_mean: 14.67
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-32-25
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 48.0
  episode_reward_mean: 15.43
  episode_reward_min: -44.0
  episodes_this_iter: 96
  episodes_total: 15840
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 14760.915
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002775744069367647
        entropy: 0.6186933517456055
        entropy_coeff: 0.0017600000137463212
        kl: 0.010143952444195747
        model: {}
        policy_loss: -0.019333137199282646
        total_loss: -0.019381366670131683
        vf_explained_var: -0.0045317113399505615
        vf_loss: 0.26278507709503174
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002775744069367647
        entropy: 0.5567506551742554
        entropy_coeff: 0.0017600000137463212
        kl: 0.006045427173376083
        model: {}
        policy_loss: -0.014895869418978691
        total_loss: -0.0146533427760005
        vf_explained_var: 0.032213106751441956
        vf_loss: 0.1332550197839737
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002775744069367647
        entropy: 0.533298134803772
        entropy_coeff: 0.0017600000137463212
        kl: 0.007275618612766266
        model: {}
        policy_loss: -0.018162328749895096
        total_loss: -0.017602944746613503
        vf_explained_var: 0.010329410433769226
        vf_loss: 0.4286680221557617
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002775744069367647
        entropy: 0.6718389987945557
        entropy_coeff: 0.0017600000137463212
        kl: 0.006784105207771063
        model: {}
        policy_loss: -0.016128413379192352
        total_loss: -0.015927931293845177
        vf_explained_var: 0.016634702682495117
        vf_loss: 0.2610003352165222
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002775744069367647
        entropy: 0.7361654043197632
        entropy_coeff: 0.0017600000137463212
        kl: 0.007936375215649605
        model: {}
        policy_loss: -0.02102074772119522
        total_loss: -0.02068956196308136
        vf_explained_var: 0.032774075865745544
        vf_loss: 0.3956182301044464
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002775744069367647
        entropy: 0.7519690990447998
        entropy_coeff: 0.0017600000137463212
        kl: 0.007111207116395235
        model: {}
        policy_loss: -0.01216716319322586
        total_loss: -0.012768803164362907
        vf_explained_var: 0.02360488474369049
        vf_loss: 0.10704346001148224
    load_time_ms: 16196.832
    num_steps_sampled: 15840000
    num_steps_trained: 15840000
    sample_time_ms: 102286.563
    update_time_ms: 705.611
  iterations_since_restore: 5
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.476243093922655
    ram_util_percent: 20.498342541436465
  pid: 27405
  policy_reward_max:
    agent-0: 14.0
    agent-1: 8.0
    agent-2: 16.0
    agent-3: 12.0
    agent-4: 16.0
    agent-5: 5.0
  policy_reward_mean:
    agent-0: 2.61
    agent-1: 1.82
    agent-2: 4.04
    agent-3: 3.08
    agent-4: 3.11
    agent-5: 0.77
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -2.0
    agent-4: -50.0
    agent-5: -49.0
  sampler_perf:
    mean_env_wait_ms: 23.76840262597828
    mean_inference_ms: 13.683456249196842
    mean_processing_ms: 59.86161666524306
  time_since_restore: 673.7803137302399
  time_this_iter_s: 127.10158014297485
  time_total_s: 23184.063556194305
  timestamp: 1637220745
  timesteps_since_restore: 480000
  timesteps_this_iter: 96000
  timesteps_total: 15840000
  training_iteration: 165
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 38.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    165 |          23184.1 | 15840000 |    15.43 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 2.52
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.76
    apples_agent-1_min: 0
    apples_agent-2_max: 6
    apples_agent-2_mean: 1.57
    apples_agent-2_min: 0
    apples_agent-3_max: 15
    apples_agent-3_mean: 2.15
    apples_agent-3_min: 0
    apples_agent-4_max: 8
    apples_agent-4_mean: 0.8
    apples_agent-4_min: 0
    apples_agent-5_max: 5
    apples_agent-5_mean: 0.75
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 283
    cleaning_beam_agent-0_mean: 97.62
    cleaning_beam_agent-0_min: 33
    cleaning_beam_agent-1_max: 426
    cleaning_beam_agent-1_mean: 263.72
    cleaning_beam_agent-1_min: 140
    cleaning_beam_agent-2_max: 66
    cleaning_beam_agent-2_mean: 21.4
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 232
    cleaning_beam_agent-3_mean: 94.01
    cleaning_beam_agent-3_min: 24
    cleaning_beam_agent-4_max: 217
    cleaning_beam_agent-4_mean: 94.22
    cleaning_beam_agent-4_min: 10
    cleaning_beam_agent-5_max: 57
    cleaning_beam_agent-5_mean: 12.7
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-34-33
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 44.0
  episode_reward_mean: 16.32
  episode_reward_min: -34.0
  episodes_this_iter: 96
  episodes_total: 15936
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 14325.306
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002715839946176857
        entropy: 0.6098909378051758
        entropy_coeff: 0.0017600000137463212
        kl: 0.009358364157378674
        model: {}
        policy_loss: -0.01992877386510372
        total_loss: -0.020030010491609573
        vf_explained_var: 0.0062249451875686646
        vf_loss: 0.3633517920970917
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002715839946176857
        entropy: 0.542942464351654
        entropy_coeff: 0.0017600000137463212
        kl: 0.006217285059392452
        model: {}
        policy_loss: -0.01502350252121687
        total_loss: -0.014723112806677818
        vf_explained_var: 0.030510127544403076
        vf_loss: 0.12507116794586182
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002715839946176857
        entropy: 0.5555975437164307
        entropy_coeff: 0.0017600000137463212
        kl: 0.005098670721054077
        model: {}
        policy_loss: -0.010029993951320648
        total_loss: -0.009818398393690586
        vf_explained_var: 0.016246363520622253
        vf_loss: 1.6971046924591064
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002715839946176857
        entropy: 0.6662752628326416
        entropy_coeff: 0.0017600000137463212
        kl: 0.006349913775920868
        model: {}
        policy_loss: -0.015440257266163826
        total_loss: -0.015314588323235512
        vf_explained_var: 0.011484041810035706
        vf_loss: 0.283329576253891
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002715839946176857
        entropy: 0.7314128279685974
        entropy_coeff: 0.0017600000137463212
        kl: 0.00906430371105671
        model: {}
        policy_loss: -0.022305119782686234
        total_loss: -0.021744627505540848
        vf_explained_var: 0.027728736400604248
        vf_loss: 0.34918075799942017
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002715839946176857
        entropy: 0.725517988204956
        entropy_coeff: 0.0017600000137463212
        kl: 0.007147232070565224
        model: {}
        policy_loss: -0.012009984813630581
        total_loss: -0.012558553367853165
        vf_explained_var: 0.00750499963760376
        vf_loss: 0.13618022203445435
    load_time_ms: 15834.894
    num_steps_sampled: 15936000
    num_steps_trained: 15936000
    sample_time_ms: 102146.158
    update_time_ms: 590.47
  iterations_since_restore: 6
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.43626373626375
    ram_util_percent: 20.668131868131866
  pid: 27405
  policy_reward_max:
    agent-0: 15.0
    agent-1: 7.0
    agent-2: 16.0
    agent-3: 13.0
    agent-4: 12.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 2.97
    agent-1: 1.62
    agent-2: 3.76
    agent-3: 3.23
    agent-4: 3.7
    agent-5: 1.04
  policy_reward_min:
    agent-0: -47.0
    agent-1: -1.0
    agent-2: -45.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -49.0
  sampler_perf:
    mean_env_wait_ms: 23.73850083641701
    mean_inference_ms: 13.623277796600679
    mean_processing_ms: 59.819476856400705
  time_since_restore: 801.5132791996002
  time_this_iter_s: 127.73296546936035
  time_total_s: 23311.796521663666
  timestamp: 1637220873
  timesteps_since_restore: 576000
  timesteps_this_iter: 96000
  timesteps_total: 15936000
  training_iteration: 166
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 38.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    166 |          23311.8 | 15936000 |    16.32 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 1.64
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.76
    apples_agent-1_min: 0
    apples_agent-2_max: 26
    apples_agent-2_mean: 1.8
    apples_agent-2_min: 0
    apples_agent-3_max: 21
    apples_agent-3_mean: 2.0
    apples_agent-3_min: 0
    apples_agent-4_max: 12
    apples_agent-4_mean: 1.21
    apples_agent-4_min: 0
    apples_agent-5_max: 8
    apples_agent-5_mean: 0.89
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 251
    cleaning_beam_agent-0_mean: 89.11
    cleaning_beam_agent-0_min: 20
    cleaning_beam_agent-1_max: 489
    cleaning_beam_agent-1_mean: 255.46
    cleaning_beam_agent-1_min: 129
    cleaning_beam_agent-2_max: 88
    cleaning_beam_agent-2_mean: 22.07
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 198
    cleaning_beam_agent-3_mean: 108.54
    cleaning_beam_agent-3_min: 26
    cleaning_beam_agent-4_max: 210
    cleaning_beam_agent-4_mean: 89.78
    cleaning_beam_agent-4_min: 21
    cleaning_beam_agent-5_max: 36
    cleaning_beam_agent-5_mean: 10.47
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-36-41
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 50.0
  episode_reward_mean: 16.02
  episode_reward_min: 0.0
  episodes_this_iter: 96
  episodes_total: 16032
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 14024.961
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002655936114024371
        entropy: 0.6070511341094971
        entropy_coeff: 0.0017600000137463212
        kl: 0.009632994420826435
        model: {}
        policy_loss: -0.019847873598337173
        total_loss: -0.01993263140320778
        vf_explained_var: 0.005725398659706116
        vf_loss: 0.20355050265789032
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002655936114024371
        entropy: 0.5242685079574585
        entropy_coeff: 0.0017600000137463212
        kl: 0.005480686668306589
        model: {}
        policy_loss: -0.013726647011935711
        total_loss: -0.013541007414460182
        vf_explained_var: 0.03263479471206665
        vf_loss: 0.12212370336055756
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002655936114024371
        entropy: 0.5482789874076843
        entropy_coeff: 0.0017600000137463212
        kl: 0.0067858509719371796
        model: {}
        policy_loss: -0.01757153309881687
        total_loss: -0.01713361032307148
        vf_explained_var: 0.009040847420692444
        vf_loss: 0.4572625756263733
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002655936114024371
        entropy: 0.672680139541626
        entropy_coeff: 0.0017600000137463212
        kl: 0.006507829762995243
        model: {}
        policy_loss: -0.015762388706207275
        total_loss: -0.015620921738445759
        vf_explained_var: 0.0015950649976730347
        vf_loss: 0.2381705641746521
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002655936114024371
        entropy: 0.7287593483924866
        entropy_coeff: 0.0017600000137463212
        kl: 0.008256105706095695
        model: {}
        policy_loss: -0.022961901500821114
        total_loss: -0.022561484947800636
        vf_explained_var: 0.0225544273853302
        vf_loss: 0.318089097738266
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002655936114024371
        entropy: 0.7183547616004944
        entropy_coeff: 0.0017600000137463212
        kl: 0.007163988426327705
        model: {}
        policy_loss: -0.013262325897812843
        total_loss: -0.013800752349197865
        vf_explained_var: 0.019350722432136536
        vf_loss: 0.09479887038469315
    load_time_ms: 15569.759
    num_steps_sampled: 16032000
    num_steps_trained: 16032000
    sample_time_ms: 101987.385
    update_time_ms: 508.377
  iterations_since_restore: 7
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 48.857458563535914
    ram_util_percent: 20.923756906077347
  pid: 27405
  policy_reward_max:
    agent-0: 12.0
    agent-1: 10.0
    agent-2: 20.0
    agent-3: 11.0
    agent-4: 10.0
    agent-5: 5.0
  policy_reward_mean:
    agent-0: 2.33
    agent-1: 1.48
    agent-2: 3.92
    agent-3: 2.95
    agent-4: 3.95
    agent-5: 1.39
  policy_reward_min:
    agent-0: 0.0
    agent-1: -1.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.715697392535905
    mean_inference_ms: 13.583068145220304
    mean_processing_ms: 59.770634550892844
  time_since_restore: 928.8674409389496
  time_this_iter_s: 127.35416173934937
  time_total_s: 23439.150683403015
  timestamp: 1637221001
  timesteps_since_restore: 672000
  timesteps_this_iter: 96000
  timesteps_total: 16032000
  training_iteration: 167
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 38.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    167 |          23439.2 | 16032000 |    16.02 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 1.97
    apples_agent-0_min: 0
    apples_agent-1_max: 11
    apples_agent-1_mean: 1.1
    apples_agent-1_min: 0
    apples_agent-2_max: 12
    apples_agent-2_mean: 1.67
    apples_agent-2_min: 0
    apples_agent-3_max: 12
    apples_agent-3_mean: 2.52
    apples_agent-3_min: 0
    apples_agent-4_max: 51
    apples_agent-4_mean: 1.47
    apples_agent-4_min: 0
    apples_agent-5_max: 44
    apples_agent-5_mean: 1.45
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 219
    cleaning_beam_agent-0_mean: 97.06
    cleaning_beam_agent-0_min: 29
    cleaning_beam_agent-1_max: 436
    cleaning_beam_agent-1_mean: 247.79
    cleaning_beam_agent-1_min: 135
    cleaning_beam_agent-2_max: 89
    cleaning_beam_agent-2_mean: 20.3
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 221
    cleaning_beam_agent-3_mean: 108.85
    cleaning_beam_agent-3_min: 29
    cleaning_beam_agent-4_max: 183
    cleaning_beam_agent-4_mean: 88.03
    cleaning_beam_agent-4_min: 10
    cleaning_beam_agent-5_max: 41
    cleaning_beam_agent-5_mean: 12.08
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-38-48
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 68.0
  episode_reward_mean: 16.44
  episode_reward_min: -34.0
  episodes_this_iter: 96
  episodes_total: 16128
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 13783.846
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00025960319908335805
        entropy: 0.6175695061683655
        entropy_coeff: 0.0017600000137463212
        kl: 0.009018484503030777
        model: {}
        policy_loss: -0.02011365257203579
        total_loss: -0.02027970179915428
        vf_explained_var: 0.006516158580780029
        vf_loss: 0.19027599692344666
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00025960319908335805
        entropy: 0.5384570360183716
        entropy_coeff: 0.0017600000137463212
        kl: 0.00572436535730958
        model: {}
        policy_loss: -0.012685052119195461
        total_loss: -0.012462489306926727
        vf_explained_var: 0.02705627679824829
        vf_loss: 0.25374433398246765
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00025960319908335805
        entropy: 0.5343287587165833
        entropy_coeff: 0.0017600000137463212
        kl: 0.006497945636510849
        model: {}
        policy_loss: -0.016551923006772995
        total_loss: -0.016135159879922867
        vf_explained_var: 0.014347583055496216
        vf_loss: 0.5759215354919434
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00025960319908335805
        entropy: 0.6773294806480408
        entropy_coeff: 0.0017600000137463212
        kl: 0.006044237874448299
        model: {}
        policy_loss: -0.014568192884325981
        total_loss: -0.01451200619339943
        vf_explained_var: 0.00838671624660492
        vf_loss: 0.3943946957588196
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00025960319908335805
        entropy: 0.716072678565979
        entropy_coeff: 0.0017600000137463212
        kl: 0.007747235242277384
        model: {}
        policy_loss: -0.020177919417619705
        total_loss: -0.019856387749314308
        vf_explained_var: 0.027211621403694153
        vf_loss: 0.323728084564209
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00025960319908335805
        entropy: 0.7409173250198364
        entropy_coeff: 0.0017600000137463212
        kl: 0.008652644231915474
        model: {}
        policy_loss: -0.006136763840913773
        total_loss: -0.006430437322705984
        vf_explained_var: 0.012655481696128845
        vf_loss: 1.4507527351379395
    load_time_ms: 15394.945
    num_steps_sampled: 16128000
    num_steps_trained: 16128000
    sample_time_ms: 101870.801
    update_time_ms: 446.777
  iterations_since_restore: 8
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.206043956043956
    ram_util_percent: 21.134065934065934
  pid: 27405
  policy_reward_max:
    agent-0: 8.0
    agent-1: 14.0
    agent-2: 21.0
    agent-3: 12.0
    agent-4: 10.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 2.55
    agent-1: 1.89
    agent-2: 4.12
    agent-3: 3.67
    agent-4: 3.17
    agent-5: 1.04
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -48.0
  sampler_perf:
    mean_env_wait_ms: 23.695991667977463
    mean_inference_ms: 13.545111266750723
    mean_processing_ms: 59.72907709588257
  time_since_restore: 1056.3011989593506
  time_this_iter_s: 127.433758020401
  time_total_s: 23566.584441423416
  timestamp: 1637221128
  timesteps_since_restore: 768000
  timesteps_this_iter: 96000
  timesteps_total: 16128000
  training_iteration: 168
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 39.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    168 |          23566.6 | 16128000 |    16.44 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 2.0
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 0.95
    apples_agent-1_min: 0
    apples_agent-2_max: 14
    apples_agent-2_mean: 1.96
    apples_agent-2_min: 0
    apples_agent-3_max: 13
    apples_agent-3_mean: 2.16
    apples_agent-3_min: 0
    apples_agent-4_max: 96
    apples_agent-4_mean: 1.94
    apples_agent-4_min: 0
    apples_agent-5_max: 11
    apples_agent-5_mean: 0.98
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 170
    cleaning_beam_agent-0_mean: 83.01
    cleaning_beam_agent-0_min: 23
    cleaning_beam_agent-1_max: 469
    cleaning_beam_agent-1_mean: 258.15
    cleaning_beam_agent-1_min: 124
    cleaning_beam_agent-2_max: 64
    cleaning_beam_agent-2_mean: 22.64
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 257
    cleaning_beam_agent-3_mean: 112.37
    cleaning_beam_agent-3_min: 23
    cleaning_beam_agent-4_max: 320
    cleaning_beam_agent-4_mean: 90.17
    cleaning_beam_agent-4_min: 17
    cleaning_beam_agent-5_max: 60
    cleaning_beam_agent-5_mean: 15.39
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-40-56
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 52.0
  episode_reward_mean: 16.16
  episode_reward_min: -84.0
  episodes_this_iter: 96
  episodes_total: 16224
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 13596.793
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000253612786764279
        entropy: 0.6125360131263733
        entropy_coeff: 0.0017600000137463212
        kl: 0.00937090627849102
        model: {}
        policy_loss: -0.019117597490549088
        total_loss: -0.01923183538019657
        vf_explained_var: 0.0003898739814758301
        vf_loss: 0.2673308253288269
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000253612786764279
        entropy: 0.532659649848938
        entropy_coeff: 0.0017600000137463212
        kl: 0.004238517489284277
        model: {}
        policy_loss: -0.006397325545549393
        total_loss: -0.006355776451528072
        vf_explained_var: 0.010661974549293518
        vf_loss: 1.3132481575012207
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000253612786764279
        entropy: 0.5473517179489136
        entropy_coeff: 0.0017600000137463212
        kl: 0.006748051382601261
        model: {}
        policy_loss: -0.01765369065105915
        total_loss: -0.01721080392599106
        vf_explained_var: 0.022839665412902832
        vf_loss: 0.566169023513794
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000253612786764279
        entropy: 0.6703004837036133
        entropy_coeff: 0.0017600000137463212
        kl: 0.006012183614075184
        model: {}
        policy_loss: -0.01507675088942051
        total_loss: -0.015024329535663128
        vf_explained_var: 0.004034146666526794
        vf_loss: 0.2970864772796631
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000253612786764279
        entropy: 0.7093740701675415
        entropy_coeff: 0.0017600000137463212
        kl: 0.005337000824511051
        model: {}
        policy_loss: -0.01199420914053917
        total_loss: -0.012029815465211868
        vf_explained_var: 0.0145464688539505
        vf_loss: 1.4549052715301514
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000253612786764279
        entropy: 0.7696210145950317
        entropy_coeff: 0.0017600000137463212
        kl: 0.0058125522918999195
        model: {}
        policy_loss: -0.0074045960791409016
        total_loss: -0.008050881326198578
        vf_explained_var: 0.002416551113128662
        vf_loss: 1.2699216604232788
    load_time_ms: 15202.281
    num_steps_sampled: 16224000
    num_steps_trained: 16224000
    sample_time_ms: 101899.935
    update_time_ms: 398.886
  iterations_since_restore: 9
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.36043956043956
    ram_util_percent: 21.313186813186814
  pid: 27405
  policy_reward_max:
    agent-0: 10.0
    agent-1: 8.0
    agent-2: 18.0
    agent-3: 12.0
    agent-4: 13.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 3.0
    agent-1: 1.29
    agent-2: 4.48
    agent-3: 3.24
    agent-4: 3.55
    agent-5: 0.6
  policy_reward_min:
    agent-0: 0.0
    agent-1: -49.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -45.0
    agent-5: -50.0
  sampler_perf:
    mean_env_wait_ms: 23.690910428792044
    mean_inference_ms: 13.520728414245024
    mean_processing_ms: 59.7264264087846
  time_since_restore: 1184.271020412445
  time_this_iter_s: 127.96982145309448
  time_total_s: 23694.55426287651
  timestamp: 1637221256
  timesteps_since_restore: 864000
  timesteps_this_iter: 96000
  timesteps_total: 16224000
  training_iteration: 169
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 39.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    169 |          23694.6 | 16224000 |    16.16 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 1.84
    apples_agent-0_min: 0
    apples_agent-1_max: 37
    apples_agent-1_mean: 1.21
    apples_agent-1_min: 0
    apples_agent-2_max: 35
    apples_agent-2_mean: 2.29
    apples_agent-2_min: 0
    apples_agent-3_max: 10
    apples_agent-3_mean: 2.36
    apples_agent-3_min: 0
    apples_agent-4_max: 18
    apples_agent-4_mean: 1.02
    apples_agent-4_min: 0
    apples_agent-5_max: 5
    apples_agent-5_mean: 0.71
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 197
    cleaning_beam_agent-0_mean: 89.87
    cleaning_beam_agent-0_min: 33
    cleaning_beam_agent-1_max: 502
    cleaning_beam_agent-1_mean: 257.57
    cleaning_beam_agent-1_min: 121
    cleaning_beam_agent-2_max: 67
    cleaning_beam_agent-2_mean: 20.95
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 230
    cleaning_beam_agent-3_mean: 115.2
    cleaning_beam_agent-3_min: 27
    cleaning_beam_agent-4_max: 255
    cleaning_beam_agent-4_mean: 93.77
    cleaning_beam_agent-4_min: 18
    cleaning_beam_agent-5_max: 41
    cleaning_beam_agent-5_mean: 14.13
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 5
    fire_beam_agent-5_mean: 0.07
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-43-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 52.0
  episode_reward_mean: 14.55
  episode_reward_min: -138.0
  episodes_this_iter: 96
  episodes_total: 16320
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 13469.979
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002476224035490304
        entropy: 0.6187851428985596
        entropy_coeff: 0.0017600000137463212
        kl: 0.00798092596232891
        model: {}
        policy_loss: -0.011053038761019707
        total_loss: -0.011191284283995628
        vf_explained_var: 0.018875688314437866
        vf_loss: 1.5272291898727417
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002476224035490304
        entropy: 0.5278141498565674
        entropy_coeff: 0.0017600000137463212
        kl: 0.004905960522592068
        model: {}
        policy_loss: -0.006304034497588873
        total_loss: -0.006603577174246311
        vf_explained_var: 0.010549277067184448
        vf_loss: 1.388134241104126
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002476224035490304
        entropy: 0.532036304473877
        entropy_coeff: 0.0017600000137463212
        kl: 0.006247850134968758
        model: {}
        policy_loss: -0.011604513972997665
        total_loss: -0.010844350792467594
        vf_explained_var: 0.011046528816223145
        vf_loss: 4.469768047332764
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002476224035490304
        entropy: 0.6658197641372681
        entropy_coeff: 0.0017600000137463212
        kl: 0.006425381172448397
        model: {}
        policy_loss: -0.015945017337799072
        total_loss: -0.015803514048457146
        vf_explained_var: 0.009398221969604492
        vf_loss: 0.2826994061470032
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002476224035490304
        entropy: 0.7219464182853699
        entropy_coeff: 0.0017600000137463212
        kl: 0.005780201870948076
        model: {}
        policy_loss: -0.01188772451132536
        total_loss: -0.011705254204571247
        vf_explained_var: 0.01070934534072876
        vf_loss: 2.970552682876587
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002476224035490304
        entropy: 0.7492939233779907
        entropy_coeff: 0.0017600000137463212
        kl: 0.006210164166986942
        model: {}
        policy_loss: -0.008266754448413849
        total_loss: -0.008816884830594063
        vf_explained_var: 0.00047531723976135254
        vf_loss: 1.4761056900024414
    load_time_ms: 15044.355
    num_steps_sampled: 16320000
    num_steps_trained: 16320000
    sample_time_ms: 101823.301
    update_time_ms: 360.676
  iterations_since_restore: 10
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.35549450549451
    ram_util_percent: 21.474175824175827
  pid: 27405
  policy_reward_max:
    agent-0: 10.0
    agent-1: 7.0
    agent-2: 18.0
    agent-3: 11.0
    agent-4: 18.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 2.43
    agent-1: 1.38
    agent-2: 3.04
    agent-3: 3.66
    agent-4: 3.05
    agent-5: 0.99
  policy_reward_min:
    agent-0: -47.0
    agent-1: -47.0
    agent-2: -50.0
    agent-3: 0.0
    agent-4: -45.0
    agent-5: -50.0
  sampler_perf:
    mean_env_wait_ms: 23.696847452197158
    mean_inference_ms: 13.500470628756846
    mean_processing_ms: 59.7200786579327
  time_since_restore: 1311.4442670345306
  time_this_iter_s: 127.17324662208557
  time_total_s: 23821.727509498596
  timestamp: 1637221384
  timesteps_since_restore: 960000
  timesteps_this_iter: 96000
  timesteps_total: 16320000
  training_iteration: 170
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 39.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    170 |          23821.7 | 16320000 |    14.55 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 2.0
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 0.83
    apples_agent-1_min: 0
    apples_agent-2_max: 11
    apples_agent-2_mean: 1.69
    apples_agent-2_min: 0
    apples_agent-3_max: 14
    apples_agent-3_mean: 2.47
    apples_agent-3_min: 0
    apples_agent-4_max: 29
    apples_agent-4_mean: 1.14
    apples_agent-4_min: 0
    apples_agent-5_max: 5
    apples_agent-5_mean: 0.76
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 197
    cleaning_beam_agent-0_mean: 87.11
    cleaning_beam_agent-0_min: 35
    cleaning_beam_agent-1_max: 505
    cleaning_beam_agent-1_mean: 273.27
    cleaning_beam_agent-1_min: 127
    cleaning_beam_agent-2_max: 143
    cleaning_beam_agent-2_mean: 26.53
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 230
    cleaning_beam_agent-3_mean: 106.9
    cleaning_beam_agent-3_min: 31
    cleaning_beam_agent-4_max: 197
    cleaning_beam_agent-4_mean: 87.83
    cleaning_beam_agent-4_min: 20
    cleaning_beam_agent-5_max: 48
    cleaning_beam_agent-5_mean: 15.69
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.05
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-45-18
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 59.0
  episode_reward_mean: 17.34
  episode_reward_min: -89.0
  episodes_this_iter: 96
  episodes_total: 16416
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12194.21
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002416320057818666
        entropy: 0.6160738468170166
        entropy_coeff: 0.0017600000137463212
        kl: 0.007493424694985151
        model: {}
        policy_loss: -0.013772409409284592
        total_loss: -0.014053583145141602
        vf_explained_var: 0.0021953582763671875
        vf_loss: 0.5377206802368164
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0002416320057818666
        entropy: 0.540879487991333
        entropy_coeff: 0.0017600000137463212
        kl: 0.007655399385839701
        model: {}
        policy_loss: -0.014431542716920376
        total_loss: -0.014986740425229073
        vf_explained_var: 0.03984050452709198
        vf_loss: 0.1398189812898636
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002416320057818666
        entropy: 0.527342677116394
        entropy_coeff: 0.0017600000137463212
        kl: 0.006672301329672337
        model: {}
        policy_loss: -0.016551360487937927
        total_loss: -0.016092544421553612
        vf_explained_var: -0.0017536133527755737
        vf_loss: 0.52479487657547
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002416320057818666
        entropy: 0.6434291005134583
        entropy_coeff: 0.0017600000137463212
        kl: 0.006252247374504805
        model: {}
        policy_loss: -0.01391844917088747
        total_loss: -0.013763361610472202
        vf_explained_var: 0.0073146820068359375
        vf_loss: 0.37070393562316895
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002416320057818666
        entropy: 0.7186484932899475
        entropy_coeff: 0.0017600000137463212
        kl: 0.008265756070613861
        model: {}
        policy_loss: -0.01945672556757927
        total_loss: -0.019028987735509872
        vf_explained_var: 0.020847603678703308
        vf_loss: 0.39407750964164734
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002416320057818666
        entropy: 0.7700433135032654
        entropy_coeff: 0.0017600000137463212
        kl: 0.00813315063714981
        model: {}
        policy_loss: -0.012722701765596867
        total_loss: -0.01325099915266037
        vf_explained_var: -0.006054803729057312
        vf_loss: 0.13663600385189056
    load_time_ms: 14618.245
    num_steps_sampled: 16416000
    num_steps_trained: 16416000
    sample_time_ms: 101573.45
    update_time_ms: 16.7
  iterations_since_restore: 11
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 48.33821989528795
    ram_util_percent: 21.78586387434555
  pid: 27405
  policy_reward_max:
    agent-0: 14.0
    agent-1: 13.0
    agent-2: 24.0
    agent-3: 16.0
    agent-4: 15.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 2.14
    agent-1: 1.85
    agent-2: 4.17
    agent-3: 3.85
    agent-4: 3.76
    agent-5: 1.57
  policy_reward_min:
    agent-0: -97.0
    agent-1: 0.0
    agent-2: -1.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.688613096210428
    mean_inference_ms: 13.486590953558002
    mean_processing_ms: 59.713740131816884
  time_since_restore: 1445.190910100937
  time_this_iter_s: 133.74664306640625
  time_total_s: 23955.474152565002
  timestamp: 1637221518
  timesteps_since_restore: 1056000
  timesteps_this_iter: 96000
  timesteps_total: 16416000
  training_iteration: 171
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 40.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    171 |          23955.5 | 16416000 |    17.34 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 2.45
    apples_agent-0_min: 0
    apples_agent-1_max: 41
    apples_agent-1_mean: 1.34
    apples_agent-1_min: 0
    apples_agent-2_max: 20
    apples_agent-2_mean: 1.74
    apples_agent-2_min: 0
    apples_agent-3_max: 17
    apples_agent-3_mean: 2.33
    apples_agent-3_min: 0
    apples_agent-4_max: 5
    apples_agent-4_mean: 0.88
    apples_agent-4_min: 0
    apples_agent-5_max: 5
    apples_agent-5_mean: 0.75
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 153
    cleaning_beam_agent-0_mean: 87.34
    cleaning_beam_agent-0_min: 36
    cleaning_beam_agent-1_max: 465
    cleaning_beam_agent-1_mean: 270.88
    cleaning_beam_agent-1_min: 140
    cleaning_beam_agent-2_max: 116
    cleaning_beam_agent-2_mean: 23.41
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 247
    cleaning_beam_agent-3_mean: 116.63
    cleaning_beam_agent-3_min: 34
    cleaning_beam_agent-4_max: 258
    cleaning_beam_agent-4_mean: 90.0
    cleaning_beam_agent-4_min: 17
    cleaning_beam_agent-5_max: 52
    cleaning_beam_agent-5_mean: 16.68
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-47-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 71.0
  episode_reward_mean: 13.73
  episode_reward_min: -92.0
  episodes_this_iter: 96
  episodes_total: 16512
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12162.157
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00023564159346278757
        entropy: 0.6167424917221069
        entropy_coeff: 0.0017600000137463212
        kl: 0.008004724979400635
        model: {}
        policy_loss: -0.010114236734807491
        total_loss: -0.010110163129866123
        vf_explained_var: 0.002704247832298279
        vf_loss: 2.8906915187835693
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00023564159346278757
        entropy: 0.5487804412841797
        entropy_coeff: 0.0017600000137463212
        kl: 0.007423568516969681
        model: {}
        policy_loss: -0.006731683854013681
        total_loss: -0.007178376894444227
        vf_explained_var: 0.012406781315803528
        vf_loss: 1.4798331260681152
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00023564159346278757
        entropy: 0.5413455963134766
        entropy_coeff: 0.0017600000137463212
        kl: 0.004685952328145504
        model: {}
        policy_loss: -0.009707779623568058
        total_loss: -0.009585820138454437
        vf_explained_var: 0.006398648023605347
        vf_loss: 1.375391960144043
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00023564159346278757
        entropy: 0.6660961508750916
        entropy_coeff: 0.0017600000137463212
        kl: 0.004027176182717085
        model: {}
        policy_loss: -0.005813437979668379
        total_loss: -0.0057653095573186874
        vf_explained_var: 0.003766685724258423
        vf_loss: 4.150241851806641
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00023564159346278757
        entropy: 0.727981448173523
        entropy_coeff: 0.0017600000137463212
        kl: 0.005525191314518452
        model: {}
        policy_loss: -0.012342329137027264
        total_loss: -0.012387393973767757
        vf_explained_var: 0.019176438450813293
        vf_loss: 1.3114509582519531
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00023564159346278757
        entropy: 0.7750528454780579
        entropy_coeff: 0.0017600000137463212
        kl: 0.008273781277239323
        model: {}
        policy_loss: -0.01408523041754961
        total_loss: -0.014613781124353409
        vf_explained_var: 0.012413561344146729
        vf_loss: 0.0816355049610138
    load_time_ms: 14588.421
    num_steps_sampled: 16512000
    num_steps_trained: 16512000
    sample_time_ms: 101505.298
    update_time_ms: 16.219
  iterations_since_restore: 12
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.5412087912088
    ram_util_percent: 21.7489010989011
  pid: 27405
  policy_reward_max:
    agent-0: 10.0
    agent-1: 16.0
    agent-2: 17.0
    agent-3: 21.0
    agent-4: 13.0
    agent-5: 6.0
  policy_reward_mean:
    agent-0: 1.72
    agent-1: 1.21
    agent-2: 3.58
    agent-3: 2.47
    agent-4: 3.59
    agent-5: 1.16
  policy_reward_min:
    agent-0: -50.0
    agent-1: -50.0
    agent-2: -49.0
    agent-3: -97.0
    agent-4: -43.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.682411143546645
    mean_inference_ms: 13.473777826055192
    mean_processing_ms: 59.72519669805909
  time_since_restore: 1573.3664200305939
  time_this_iter_s: 128.17550992965698
  time_total_s: 24083.64966249466
  timestamp: 1637221646
  timesteps_since_restore: 1152000
  timesteps_this_iter: 96000
  timesteps_total: 16512000
  training_iteration: 172
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 40.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    172 |          24083.6 | 16512000 |    13.73 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 35
    apples_agent-0_mean: 2.52
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.89
    apples_agent-1_min: 0
    apples_agent-2_max: 13
    apples_agent-2_mean: 1.89
    apples_agent-2_min: 0
    apples_agent-3_max: 20
    apples_agent-3_mean: 2.47
    apples_agent-3_min: 0
    apples_agent-4_max: 24
    apples_agent-4_mean: 1.16
    apples_agent-4_min: 0
    apples_agent-5_max: 45
    apples_agent-5_mean: 1.13
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 218
    cleaning_beam_agent-0_mean: 97.44
    cleaning_beam_agent-0_min: 30
    cleaning_beam_agent-1_max: 407
    cleaning_beam_agent-1_mean: 270.36
    cleaning_beam_agent-1_min: 134
    cleaning_beam_agent-2_max: 147
    cleaning_beam_agent-2_mean: 23.82
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 260
    cleaning_beam_agent-3_mean: 109.97
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 214
    cleaning_beam_agent-4_mean: 92.29
    cleaning_beam_agent-4_min: 20
    cleaning_beam_agent-5_max: 45
    cleaning_beam_agent-5_mean: 15.17
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-49-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 48.0
  episode_reward_mean: 17.84
  episode_reward_min: -90.0
  episodes_this_iter: 96
  episodes_total: 16608
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12151.448
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00022965119569562376
        entropy: 0.6051714420318604
        entropy_coeff: 0.0017600000137463212
        kl: 0.009105534292757511
        model: {}
        policy_loss: -0.019478723406791687
        total_loss: -0.01960269920527935
        vf_explained_var: -0.011751919984817505
        vf_loss: 0.3057830035686493
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00022965119569562376
        entropy: 0.5380336046218872
        entropy_coeff: 0.0017600000137463212
        kl: 0.0076128095388412476
        model: {}
        policy_loss: -0.013937685638666153
        total_loss: -0.014491086825728416
        vf_explained_var: 0.023684337735176086
        vf_loss: 0.1290002167224884
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00022965119569562376
        entropy: 0.5306396484375
        entropy_coeff: 0.0017600000137463212
        kl: 0.008008478209376335
        model: {}
        policy_loss: -0.01750846579670906
        total_loss: -0.017596879974007607
        vf_explained_var: 0.005227029323577881
        vf_loss: 0.44664257764816284
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00022965119569562376
        entropy: 0.6563288569450378
        entropy_coeff: 0.0017600000137463212
        kl: 0.006816244684159756
        model: {}
        policy_loss: -0.014551158994436264
        total_loss: -0.014986307360231876
        vf_explained_var: -0.0053747594356536865
        vf_loss: 0.38366585969924927
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00022965119569562376
        entropy: 0.7356681227684021
        entropy_coeff: 0.0017600000137463212
        kl: 0.008000481873750687
        model: {}
        policy_loss: -0.02087307535111904
        total_loss: -0.02053428441286087
        vf_explained_var: 0.01609751582145691
        vf_loss: 0.33473795652389526
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00022965119569562376
        entropy: 0.763832151889801
        entropy_coeff: 0.0017600000137463212
        kl: 0.007787404581904411
        model: {}
        policy_loss: -0.013912543654441833
        total_loss: -0.014467325061559677
        vf_explained_var: 0.016336306929588318
        vf_loss: 0.10821215808391571
    load_time_ms: 14546.364
    num_steps_sampled: 16608000
    num_steps_trained: 16608000
    sample_time_ms: 101397.495
    update_time_ms: 15.907
  iterations_since_restore: 13
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.08131868131868
    ram_util_percent: 22.003846153846148
  pid: 27405
  policy_reward_max:
    agent-0: 15.0
    agent-1: 7.0
    agent-2: 14.0
    agent-3: 11.0
    agent-4: 12.0
    agent-5: 6.0
  policy_reward_mean:
    agent-0: 3.33
    agent-1: 1.72
    agent-2: 3.82
    agent-3: 3.27
    agent-4: 4.13
    agent-5: 1.57
  policy_reward_min:
    agent-0: -1.0
    agent-1: 0.0
    agent-2: -49.0
    agent-3: -44.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.67853470099463
    mean_inference_ms: 13.46073802790583
    mean_processing_ms: 59.705645601932254
  time_since_restore: 1700.8098075389862
  time_this_iter_s: 127.44338750839233
  time_total_s: 24211.09305000305
  timestamp: 1637221774
  timesteps_since_restore: 1248000
  timesteps_this_iter: 96000
  timesteps_total: 16608000
  training_iteration: 173
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 40.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    173 |          24211.1 | 16608000 |    17.84 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 2.11
    apples_agent-0_min: 0
    apples_agent-1_max: 3
    apples_agent-1_mean: 0.77
    apples_agent-1_min: 0
    apples_agent-2_max: 116
    apples_agent-2_mean: 3.18
    apples_agent-2_min: 0
    apples_agent-3_max: 16
    apples_agent-3_mean: 2.34
    apples_agent-3_min: 0
    apples_agent-4_max: 5
    apples_agent-4_mean: 1.2
    apples_agent-4_min: 0
    apples_agent-5_max: 13
    apples_agent-5_mean: 0.91
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 235
    cleaning_beam_agent-0_mean: 94.33
    cleaning_beam_agent-0_min: 25
    cleaning_beam_agent-1_max: 420
    cleaning_beam_agent-1_mean: 266.58
    cleaning_beam_agent-1_min: 132
    cleaning_beam_agent-2_max: 162
    cleaning_beam_agent-2_mean: 26.04
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 247
    cleaning_beam_agent-3_mean: 110.14
    cleaning_beam_agent-3_min: 27
    cleaning_beam_agent-4_max: 222
    cleaning_beam_agent-4_mean: 90.72
    cleaning_beam_agent-4_min: 21
    cleaning_beam_agent-5_max: 48
    cleaning_beam_agent-5_mean: 12.86
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 5
    fire_beam_agent-0_mean: 0.06
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-51-41
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 58.0
  episode_reward_mean: 17.8
  episode_reward_min: -87.0
  episodes_this_iter: 96
  episodes_total: 16704
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12152.573
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00022366079792845994
        entropy: 0.5921741724014282
        entropy_coeff: 0.0017600000137463212
        kl: 0.00888875499367714
        model: {}
        policy_loss: -0.0190180242061615
        total_loss: -0.019144272431731224
        vf_explained_var: 0.006886795163154602
        vf_loss: 0.27100870013237
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00022366079792845994
        entropy: 0.53193199634552
        entropy_coeff: 0.0017600000137463212
        kl: 0.008401632308959961
        model: {}
        policy_loss: -0.014712821692228317
        total_loss: -0.015215994790196419
        vf_explained_var: 0.03841383755207062
        vf_loss: 0.1294485330581665
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00022366079792845994
        entropy: 0.5346247553825378
        entropy_coeff: 0.0017600000137463212
        kl: 0.008510058745741844
        model: {}
        policy_loss: -0.019114531576633453
        total_loss: -0.01916401833295822
        vf_explained_var: 0.002817302942276001
        vf_loss: 0.4044671952724457
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00022366079792845994
        entropy: 0.6487681865692139
        entropy_coeff: 0.0017600000137463212
        kl: 0.005915113724768162
        model: {}
        policy_loss: -0.008938542567193508
        total_loss: -0.00932591874152422
        vf_explained_var: 0.01470676064491272
        vf_loss: 1.6294164657592773
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00022366079792845994
        entropy: 0.7368289828300476
        entropy_coeff: 0.0017600000137463212
        kl: 0.00796268880367279
        model: {}
        policy_loss: -0.020062286406755447
        total_loss: -0.019722839817404747
        vf_explained_var: 0.025972098112106323
        vf_loss: 0.43729740381240845
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00022366079792845994
        entropy: 0.7733561992645264
        entropy_coeff: 0.0017600000137463212
        kl: 0.006623489316552877
        model: {}
        policy_loss: -0.00976075604557991
        total_loss: -0.01032009907066822
        vf_explained_var: 0.023978978395462036
        vf_loss: 1.3941736221313477
    load_time_ms: 14535.03
    num_steps_sampled: 16704000
    num_steps_trained: 16704000
    sample_time_ms: 101313.092
    update_time_ms: 15.787
  iterations_since_restore: 14
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 48.97790055248619
    ram_util_percent: 22.09447513812155
  pid: 27405
  policy_reward_max:
    agent-0: 15.0
    agent-1: 7.0
    agent-2: 18.0
    agent-3: 18.0
    agent-4: 16.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 3.12
    agent-1: 1.76
    agent-2: 4.39
    agent-3: 3.32
    agent-4: 4.26
    agent-5: 0.95
  policy_reward_min:
    agent-0: -3.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -46.0
    agent-4: 0.0
    agent-5: -50.0
  sampler_perf:
    mean_env_wait_ms: 23.675147445751126
    mean_inference_ms: 13.447495769569734
    mean_processing_ms: 59.685291446524424
  time_since_restore: 1827.827083349228
  time_this_iter_s: 127.0172758102417
  time_total_s: 24338.110325813293
  timestamp: 1637221901
  timesteps_since_restore: 1344000
  timesteps_this_iter: 96000
  timesteps_total: 16704000
  training_iteration: 174
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 41.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    174 |          24338.1 | 16704000 |     17.8 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 36
    apples_agent-0_mean: 2.02
    apples_agent-0_min: 0
    apples_agent-1_max: 3
    apples_agent-1_mean: 0.66
    apples_agent-1_min: 0
    apples_agent-2_max: 16
    apples_agent-2_mean: 1.95
    apples_agent-2_min: 0
    apples_agent-3_max: 11
    apples_agent-3_mean: 2.04
    apples_agent-3_min: 0
    apples_agent-4_max: 13
    apples_agent-4_mean: 0.88
    apples_agent-4_min: 0
    apples_agent-5_max: 9
    apples_agent-5_mean: 0.95
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 198
    cleaning_beam_agent-0_mean: 97.44
    cleaning_beam_agent-0_min: 28
    cleaning_beam_agent-1_max: 439
    cleaning_beam_agent-1_mean: 277.74
    cleaning_beam_agent-1_min: 153
    cleaning_beam_agent-2_max: 104
    cleaning_beam_agent-2_mean: 21.41
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 193
    cleaning_beam_agent-3_mean: 105.31
    cleaning_beam_agent-3_min: 40
    cleaning_beam_agent-4_max: 566
    cleaning_beam_agent-4_mean: 97.91
    cleaning_beam_agent-4_min: 8
    cleaning_beam_agent-5_max: 40
    cleaning_beam_agent-5_mean: 13.58
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-53-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 53.0
  episode_reward_mean: 15.63
  episode_reward_min: -38.0
  episodes_this_iter: 96
  episodes_total: 16800
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12138.83
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021767040016129613
        entropy: 0.6036436557769775
        entropy_coeff: 0.0017600000137463212
        kl: 0.006904407870024443
        model: {}
        policy_loss: -0.009975721128284931
        total_loss: -0.01007352489978075
        vf_explained_var: 0.0062912702560424805
        vf_loss: 2.741687774658203
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00021767040016129613
        entropy: 0.5212348699569702
        entropy_coeff: 0.0017600000137463212
        kl: 0.007175616454333067
        model: {}
        policy_loss: -0.013276638463139534
        total_loss: -0.013824805617332458
        vf_explained_var: 0.03127136826515198
        vf_loss: 0.10428161919116974
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021767040016129613
        entropy: 0.5310230255126953
        entropy_coeff: 0.0017600000137463212
        kl: 0.008026616647839546
        model: {}
        policy_loss: -0.017400292679667473
        total_loss: -0.017494529485702515
        vf_explained_var: -0.00014676153659820557
        vf_loss: 0.3770126402378082
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021767040016129613
        entropy: 0.6414087414741516
        entropy_coeff: 0.0017600000137463212
        kl: 0.007218936458230019
        model: {}
        policy_loss: -0.014739771373569965
        total_loss: -0.015115952119231224
        vf_explained_var: -0.0027419477701187134
        vf_loss: 0.3080170154571533
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00021767040016129613
        entropy: 0.7227249145507812
        entropy_coeff: 0.0017600000137463212
        kl: 0.0077593205496668816
        model: {}
        policy_loss: -0.019687321037054062
        total_loss: -0.01937505044043064
        vf_explained_var: 0.03222724795341492
        vf_loss: 0.32402345538139343
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021767040016129613
        entropy: 0.7905128598213196
        entropy_coeff: 0.0017600000137463212
        kl: 0.00762732420116663
        model: {}
        policy_loss: -0.013395819813013077
        total_loss: -0.014013794250786304
        vf_explained_var: -0.010086536407470703
        vf_loss: 0.1059262603521347
    load_time_ms: 14541.167
    num_steps_sampled: 16800000
    num_steps_trained: 16800000
    sample_time_ms: 101225.085
    update_time_ms: 16.385
  iterations_since_restore: 15
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.89333333333333
    ram_util_percent: 22.235
  pid: 27405
  policy_reward_max:
    agent-0: 9.0
    agent-1: 5.0
    agent-2: 19.0
    agent-3: 13.0
    agent-4: 11.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 1.4
    agent-1: 1.52
    agent-2: 4.18
    agent-3: 3.38
    agent-4: 3.69
    agent-5: 1.46
  policy_reward_min:
    agent-0: -49.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.67666236848304
    mean_inference_ms: 13.440399673617241
    mean_processing_ms: 59.66019096659884
  time_since_restore: 1953.9565153121948
  time_this_iter_s: 126.12943196296692
  time_total_s: 24464.23975777626
  timestamp: 1637222027
  timesteps_since_restore: 1440000
  timesteps_this_iter: 96000
  timesteps_total: 16800000
  training_iteration: 175
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 41.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    175 |          24464.2 | 16800000 |    15.63 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 2.23
    apples_agent-0_min: 0
    apples_agent-1_max: 3
    apples_agent-1_mean: 0.59
    apples_agent-1_min: 0
    apples_agent-2_max: 16
    apples_agent-2_mean: 1.99
    apples_agent-2_min: 0
    apples_agent-3_max: 27
    apples_agent-3_mean: 2.89
    apples_agent-3_min: 0
    apples_agent-4_max: 52
    apples_agent-4_mean: 1.67
    apples_agent-4_min: 0
    apples_agent-5_max: 15
    apples_agent-5_mean: 1.02
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 190
    cleaning_beam_agent-0_mean: 93.43
    cleaning_beam_agent-0_min: 36
    cleaning_beam_agent-1_max: 409
    cleaning_beam_agent-1_mean: 276.18
    cleaning_beam_agent-1_min: 161
    cleaning_beam_agent-2_max: 95
    cleaning_beam_agent-2_mean: 20.76
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 242
    cleaning_beam_agent-3_mean: 98.06
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 264
    cleaning_beam_agent-4_mean: 106.07
    cleaning_beam_agent-4_min: 28
    cleaning_beam_agent-5_max: 33
    cleaning_beam_agent-5_mean: 12.31
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-55-56
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 47.0
  episode_reward_mean: 18.07
  episode_reward_min: -82.0
  episodes_this_iter: 96
  episodes_total: 16896
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12136.584
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021168000239413232
        entropy: 0.5953077077865601
        entropy_coeff: 0.0017600000137463212
        kl: 0.007978485897183418
        model: {}
        policy_loss: -0.015705380588769913
        total_loss: -0.01591075398027897
        vf_explained_var: -0.0008421242237091064
        vf_loss: 0.44519859552383423
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00021168000239413232
        entropy: 0.519766092300415
        entropy_coeff: 0.0017600000137463212
        kl: 0.006972865667194128
        model: {}
        policy_loss: -0.013205213472247124
        total_loss: -0.013762172311544418
        vf_explained_var: 0.040822163224220276
        vf_loss: 0.09190813452005386
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021168000239413232
        entropy: 0.5195049047470093
        entropy_coeff: 0.0017600000137463212
        kl: 0.007423418574035168
        model: {}
        policy_loss: -0.017023582011461258
        total_loss: -0.0171438567340374
        vf_explained_var: 0.004543006420135498
        vf_loss: 0.5171169638633728
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021168000239413232
        entropy: 0.625278651714325
        entropy_coeff: 0.0017600000137463212
        kl: 0.006726534105837345
        model: {}
        policy_loss: -0.014448528178036213
        total_loss: -0.014839855954051018
        vf_explained_var: 0.000813901424407959
        vf_loss: 0.3650897145271301
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00021168000239413232
        entropy: 0.7193547487258911
        entropy_coeff: 0.0017600000137463212
        kl: 0.007814045995473862
        model: {}
        policy_loss: -0.01949453353881836
        total_loss: -0.019153941422700882
        vf_explained_var: 0.02888236939907074
        vf_loss: 0.4385073184967041
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021168000239413232
        entropy: 0.7807776927947998
        entropy_coeff: 0.0017600000137463212
        kl: 0.00562832597643137
        model: {}
        policy_loss: -0.00914972648024559
        total_loss: -0.009940382093191147
        vf_explained_var: -0.0042737871408462524
        vf_loss: 0.20685088634490967
    load_time_ms: 14537.059
    num_steps_sampled: 16896000
    num_steps_trained: 16896000
    sample_time_ms: 101316.303
    update_time_ms: 16.585
  iterations_since_restore: 16
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.228415300546445
    ram_util_percent: 22.428415300546447
  pid: 27405
  policy_reward_max:
    agent-0: 14.0
    agent-1: 8.0
    agent-2: 17.0
    agent-3: 20.0
    agent-4: 15.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 2.59
    agent-1: 1.45
    agent-2: 4.74
    agent-3: 3.96
    agent-4: 4.2
    agent-5: 1.13
  policy_reward_min:
    agent-0: -49.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -49.0
  sampler_perf:
    mean_env_wait_ms: 23.68276350024446
    mean_inference_ms: 13.433001993867022
    mean_processing_ms: 59.66274004403645
  time_since_restore: 2082.551011323929
  time_this_iter_s: 128.594496011734
  time_total_s: 24592.834253787994
  timestamp: 1637222156
  timesteps_since_restore: 1536000
  timesteps_this_iter: 96000
  timesteps_total: 16896000
  training_iteration: 176
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 41.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    176 |          24592.8 | 16896000 |    18.07 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 22
    apples_agent-0_mean: 2.19
    apples_agent-0_min: 0
    apples_agent-1_max: 11
    apples_agent-1_mean: 0.75
    apples_agent-1_min: 0
    apples_agent-2_max: 56
    apples_agent-2_mean: 2.63
    apples_agent-2_min: 0
    apples_agent-3_max: 11
    apples_agent-3_mean: 2.38
    apples_agent-3_min: 0
    apples_agent-4_max: 13
    apples_agent-4_mean: 1.21
    apples_agent-4_min: 0
    apples_agent-5_max: 9
    apples_agent-5_mean: 0.88
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 181
    cleaning_beam_agent-0_mean: 90.9
    cleaning_beam_agent-0_min: 30
    cleaning_beam_agent-1_max: 453
    cleaning_beam_agent-1_mean: 280.24
    cleaning_beam_agent-1_min: 133
    cleaning_beam_agent-2_max: 87
    cleaning_beam_agent-2_mean: 22.78
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 183
    cleaning_beam_agent-3_mean: 97.18
    cleaning_beam_agent-3_min: 20
    cleaning_beam_agent-4_max: 261
    cleaning_beam_agent-4_mean: 87.72
    cleaning_beam_agent-4_min: 13
    cleaning_beam_agent-5_max: 44
    cleaning_beam_agent-5_mean: 11.47
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-58-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 46.0
  episode_reward_mean: 18.61
  episode_reward_min: 1.0
  episodes_this_iter: 96
  episodes_total: 16992
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12122.501
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002056896046269685
        entropy: 0.5905700325965881
        entropy_coeff: 0.0017600000137463212
        kl: 0.008471360430121422
        model: {}
        policy_loss: -0.017188187688589096
        total_loss: -0.017352521419525146
        vf_explained_var: -0.00178469717502594
        vf_loss: 0.27932804822921753
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0002056896046269685
        entropy: 0.510958194732666
        entropy_coeff: 0.0017600000137463212
        kl: 0.0072441608645021915
        model: {}
        policy_loss: -0.013281213119626045
        total_loss: -0.013805394992232323
        vf_explained_var: 0.02512270212173462
        vf_loss: 0.12897682189941406
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002056896046269685
        entropy: 0.5278083086013794
        entropy_coeff: 0.0017600000137463212
        kl: 0.008139189332723618
        model: {}
        policy_loss: -0.01769958809018135
        total_loss: -0.01776444911956787
        vf_explained_var: 0.0010147243738174438
        vf_loss: 0.5016347765922546
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002056896046269685
        entropy: 0.6233539581298828
        entropy_coeff: 0.0017600000137463212
        kl: 0.006765963044017553
        model: {}
        policy_loss: -0.014388070441782475
        total_loss: -0.014779937453567982
        vf_explained_var: -0.0007058531045913696
        vf_loss: 0.28639885783195496
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002056896046269685
        entropy: 0.7224640846252441
        entropy_coeff: 0.0017600000137463212
        kl: 0.007518666796386242
        model: {}
        policy_loss: -0.019553521648049355
        total_loss: -0.019278334453701973
        vf_explained_var: 0.022110462188720703
        vf_loss: 0.42986607551574707
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002056896046269685
        entropy: 0.7531419992446899
        entropy_coeff: 0.0017600000137463212
        kl: 0.007176281418651342
        model: {}
        policy_loss: -0.011639442294836044
        total_loss: -0.01223631389439106
        vf_explained_var: 0.010190322995185852
        vf_loss: 0.1103135347366333
    load_time_ms: 14518.366
    num_steps_sampled: 16992000
    num_steps_trained: 16992000
    sample_time_ms: 101276.321
    update_time_ms: 17.172
  iterations_since_restore: 17
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.47944444444444
    ram_util_percent: 22.592777777777776
  pid: 27405
  policy_reward_max:
    agent-0: 11.0
    agent-1: 8.0
    agent-2: 20.0
    agent-3: 9.0
    agent-4: 12.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 3.04
    agent-1: 1.69
    agent-2: 4.51
    agent-3: 3.51
    agent-4: 4.48
    agent-5: 1.38
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 23.66547379246325
    mean_inference_ms: 13.423464336060706
    mean_processing_ms: 59.633858387109
  time_since_restore: 2209.183809518814
  time_this_iter_s: 126.63279819488525
  time_total_s: 24719.46705198288
  timestamp: 1637222283
  timesteps_since_restore: 1632000
  timesteps_this_iter: 96000
  timesteps_total: 16992000
  training_iteration: 177
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 41.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    177 |          24719.5 | 16992000 |    18.61 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 14
    apples_agent-0_mean: 2.53
    apples_agent-0_min: 0
    apples_agent-1_max: 13
    apples_agent-1_mean: 0.97
    apples_agent-1_min: 0
    apples_agent-2_max: 26
    apples_agent-2_mean: 2.51
    apples_agent-2_min: 0
    apples_agent-3_max: 14
    apples_agent-3_mean: 3.0
    apples_agent-3_min: 0
    apples_agent-4_max: 16
    apples_agent-4_mean: 1.08
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 0.93
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 213
    cleaning_beam_agent-0_mean: 88.68
    cleaning_beam_agent-0_min: 29
    cleaning_beam_agent-1_max: 559
    cleaning_beam_agent-1_mean: 272.89
    cleaning_beam_agent-1_min: 162
    cleaning_beam_agent-2_max: 74
    cleaning_beam_agent-2_mean: 24.39
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 219
    cleaning_beam_agent-3_mean: 89.93
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 221
    cleaning_beam_agent-4_mean: 106.65
    cleaning_beam_agent-4_min: 27
    cleaning_beam_agent-5_max: 53
    cleaning_beam_agent-5_mean: 11.46
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 25
    fire_beam_agent-5_mean: 0.25
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-00-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 51.0
  episode_reward_mean: -6.47
  episode_reward_min: -2516.0
  episodes_this_iter: 96
  episodes_total: 17088
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12124.366
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001996992068598047
        entropy: 0.6008388996124268
        entropy_coeff: 0.0017600000137463212
        kl: 0.0054442691616714
        model: {}
        policy_loss: -0.007788562215864658
        total_loss: 0.062377121299505234
        vf_explained_var: 0.002903565764427185
        vf_loss: 706.787353515625
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0001996992068598047
        entropy: 0.4864880442619324
        entropy_coeff: 0.0017600000137463212
        kl: 0.006910833530128002
        model: {}
        policy_loss: -0.012157879769802094
        total_loss: -0.01264885812997818
        vf_explained_var: 0.028869539499282837
        vf_loss: 0.19702966511249542
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001996992068598047
        entropy: 0.5177083015441895
        entropy_coeff: 0.0017600000137463212
        kl: 0.007924024015665054
        model: {}
        policy_loss: -0.018080534413456917
        total_loss: -0.018145577982068062
        vf_explained_var: 0.008218720555305481
        vf_loss: 0.5372189879417419
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001996992068598047
        entropy: 0.629413366317749
        entropy_coeff: 0.0017600000137463212
        kl: 0.006431560963392258
        model: {}
        policy_loss: -0.013527515344321728
        total_loss: -0.013952266424894333
        vf_explained_var: 0.0069783031940460205
        vf_loss: 0.39858460426330566
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001996992068598047
        entropy: 0.7309237718582153
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007903621881268919
        model: {}
        policy_loss: -0.0008555928943678737
        total_loss: 0.06855950504541397
        vf_explained_var: 0.005857989192008972
        vf_loss: 705.4344482421875
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001996992068598047
        entropy: 0.7839950919151306
        entropy_coeff: 0.0017600000137463212
        kl: 0.0060805706307291985
        model: {}
        policy_loss: -0.011826513335108757
        total_loss: -0.012555437162518501
        vf_explained_var: 0.03198450803756714
        vf_loss: 0.42851847410202026
    load_time_ms: 14496.39
    num_steps_sampled: 17088000
    num_steps_trained: 17088000
    sample_time_ms: 101212.756
    update_time_ms: 17.636
  iterations_since_restore: 18
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.238674033149174
    ram_util_percent: 22.718784530386742
  pid: 27405
  policy_reward_max:
    agent-0: 11.0
    agent-1: 8.0
    agent-2: 19.0
    agent-3: 13.0
    agent-4: 15.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: -9.58
    agent-1: 2.07
    agent-2: 5.16
    agent-3: 3.99
    agent-4: -9.02
    agent-5: 0.91
  policy_reward_min:
    agent-0: -1249.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -1248.0
    agent-5: -47.0
  sampler_perf:
    mean_env_wait_ms: 23.655771487016278
    mean_inference_ms: 13.415526963285192
    mean_processing_ms: 59.60133815763869
  time_since_restore: 2335.7557475566864
  time_this_iter_s: 126.57193803787231
  time_total_s: 24846.038990020752
  timestamp: 1637222409
  timesteps_since_restore: 1728000
  timesteps_this_iter: 96000
  timesteps_total: 17088000
  training_iteration: 178
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 42.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    178 |            24846 | 17088000 |    -6.47 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 2.07
    apples_agent-0_min: 0
    apples_agent-1_max: 9
    apples_agent-1_mean: 1.02
    apples_agent-1_min: 0
    apples_agent-2_max: 33
    apples_agent-2_mean: 2.48
    apples_agent-2_min: 0
    apples_agent-3_max: 12
    apples_agent-3_mean: 2.55
    apples_agent-3_min: 0
    apples_agent-4_max: 25
    apples_agent-4_mean: 1.23
    apples_agent-4_min: 0
    apples_agent-5_max: 6
    apples_agent-5_mean: 0.81
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 238
    cleaning_beam_agent-0_mean: 90.36
    cleaning_beam_agent-0_min: 19
    cleaning_beam_agent-1_max: 444
    cleaning_beam_agent-1_mean: 283.23
    cleaning_beam_agent-1_min: 127
    cleaning_beam_agent-2_max: 88
    cleaning_beam_agent-2_mean: 21.65
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 218
    cleaning_beam_agent-3_mean: 96.57
    cleaning_beam_agent-3_min: 24
    cleaning_beam_agent-4_max: 208
    cleaning_beam_agent-4_mean: 97.49
    cleaning_beam_agent-4_min: 25
    cleaning_beam_agent-5_max: 34
    cleaning_beam_agent-5_mean: 9.74
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-02-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 54.0
  episode_reward_mean: 20.5
  episode_reward_min: 2.0
  episodes_this_iter: 96
  episodes_total: 17184
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12124.012
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00019370879454072565
        entropy: 0.5848509669303894
        entropy_coeff: 0.0017600000137463212
        kl: 0.009639257565140724
        model: {}
        policy_loss: -0.01916978880763054
        total_loss: -0.019188638776540756
        vf_explained_var: -0.30453330278396606
        vf_loss: 0.46562278270721436
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00019370879454072565
        entropy: 0.4838390648365021
        entropy_coeff: 0.0017600000137463212
        kl: 0.006467113271355629
        model: {}
        policy_loss: -0.01305065955966711
        total_loss: -0.01356700249016285
        vf_explained_var: 0.030990198254585266
        vf_loss: 0.11855422705411911
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00019370879454072565
        entropy: 0.5071219205856323
        entropy_coeff: 0.0017600000137463212
        kl: 0.007099681533873081
        model: {}
        policy_loss: -0.016695918515324593
        total_loss: -0.016830269247293472
        vf_explained_var: 0.00042569637298583984
        vf_loss: 0.4822176694869995
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00019370879454072565
        entropy: 0.6283626556396484
        entropy_coeff: 0.0017600000137463212
        kl: 0.006464304402470589
        model: {}
        policy_loss: -0.01391830574721098
        total_loss: -0.014346212148666382
        vf_explained_var: 0.005081504583358765
        vf_loss: 0.3158135414123535
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00019370879454072565
        entropy: 0.7153753042221069
        entropy_coeff: 0.0017600000137463212
        kl: 0.009892367757856846
        model: {}
        policy_loss: -0.02072416990995407
        total_loss: -0.020918583497405052
        vf_explained_var: -0.24306213855743408
        vf_loss: 0.754125714302063
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00019370879454072565
        entropy: 0.7661043405532837
        entropy_coeff: 0.0017600000137463212
        kl: 0.006799191702157259
        model: {}
        policy_loss: -0.012763089500367641
        total_loss: -0.013420727103948593
        vf_explained_var: -0.008197546005249023
        vf_loss: 0.10788808763027191
    load_time_ms: 14512.041
    num_steps_sampled: 17184000
    num_steps_trained: 17184000
    sample_time_ms: 101054.306
    update_time_ms: 18.364
  iterations_since_restore: 19
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.44888888888889
    ram_util_percent: 22.952777777777772
  pid: 27405
  policy_reward_max:
    agent-0: 10.0
    agent-1: 8.0
    agent-2: 24.0
    agent-3: 11.0
    agent-4: 14.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 3.45
    agent-1: 1.89
    agent-2: 5.0
    agent-3: 3.86
    agent-4: 4.67
    agent-5: 1.63
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.655251146782383
    mean_inference_ms: 13.407140566700377
    mean_processing_ms: 59.59193780201599
  time_since_restore: 2462.3320779800415
  time_this_iter_s: 126.5763304233551
  time_total_s: 24972.615320444107
  timestamp: 1637222536
  timesteps_since_restore: 1824000
  timesteps_this_iter: 96000
  timesteps_total: 17184000
  training_iteration: 179
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 42.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    179 |          24972.6 | 17184000 |     20.5 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 1.89
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.81
    apples_agent-1_min: 0
    apples_agent-2_max: 17
    apples_agent-2_mean: 2.42
    apples_agent-2_min: 0
    apples_agent-3_max: 12
    apples_agent-3_mean: 2.39
    apples_agent-3_min: 0
    apples_agent-4_max: 7
    apples_agent-4_mean: 1.15
    apples_agent-4_min: 0
    apples_agent-5_max: 9
    apples_agent-5_mean: 0.95
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 208
    cleaning_beam_agent-0_mean: 86.28
    cleaning_beam_agent-0_min: 23
    cleaning_beam_agent-1_max: 434
    cleaning_beam_agent-1_mean: 277.72
    cleaning_beam_agent-1_min: 137
    cleaning_beam_agent-2_max: 71
    cleaning_beam_agent-2_mean: 20.72
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 182
    cleaning_beam_agent-3_mean: 84.89
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 223
    cleaning_beam_agent-4_mean: 91.15
    cleaning_beam_agent-4_min: 25
    cleaning_beam_agent-5_max: 31
    cleaning_beam_agent-5_mean: 10.46
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-04-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 44.0
  episode_reward_mean: 18.41
  episode_reward_min: -134.0
  episodes_this_iter: 96
  episodes_total: 17280
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12098.904
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00018771839677356184
        entropy: 0.589843213558197
        entropy_coeff: 0.0017600000137463212
        kl: 0.008958781138062477
        model: {}
        policy_loss: -0.019268913194537163
        total_loss: -0.019376497715711594
        vf_explained_var: -0.19865387678146362
        vf_loss: 0.3466268479824066
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00018771839677356184
        entropy: 0.4902207553386688
        entropy_coeff: 0.0017600000137463212
        kl: 0.005745187867432833
        model: {}
        policy_loss: -0.006452261470258236
        total_loss: -0.006883748807013035
        vf_explained_var: 0.00946149230003357
        vf_loss: 1.4403977394104004
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00018771839677356184
        entropy: 0.5053941011428833
        entropy_coeff: 0.0017600000137463212
        kl: 0.005966148804873228
        model: {}
        policy_loss: -0.011313272640109062
        total_loss: -0.011426246725022793
        vf_explained_var: 0.010309189558029175
        vf_loss: 1.7990081310272217
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00018771839677356184
        entropy: 0.6175910234451294
        entropy_coeff: 0.0017600000137463212
        kl: 0.0062772794626653194
        model: {}
        policy_loss: -0.012510961852967739
        total_loss: -0.012927783653140068
        vf_explained_var: 0.005697488784790039
        vf_loss: 0.4240867495536804
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00018771839677356184
        entropy: 0.7259622812271118
        entropy_coeff: 0.0017600000137463212
        kl: 0.009536086581647396
        model: {}
        policy_loss: -0.02104947343468666
        total_loss: -0.021327920258045197
        vf_explained_var: -0.17046964168548584
        vf_loss: 0.4563761055469513
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00018771839677356184
        entropy: 0.7949789762496948
        entropy_coeff: 0.0017600000137463212
        kl: 0.009494567289948463
        model: {}
        policy_loss: -0.008448923006653786
        total_loss: -0.008758166804909706
        vf_explained_var: 0.007245838642120361
        vf_loss: 1.4046298265457153
    load_time_ms: 14524.482
    num_steps_sampled: 17280000
    num_steps_trained: 17280000
    sample_time_ms: 101001.731
    update_time_ms: 18.314
  iterations_since_restore: 20
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.94055555555555
    ram_util_percent: 23.082777777777775
  pid: 27405
  policy_reward_max:
    agent-0: 10.0
    agent-1: 8.0
    agent-2: 14.0
    agent-3: 17.0
    agent-4: 12.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 3.5
    agent-1: 1.33
    agent-2: 4.2
    agent-3: 3.83
    agent-4: 4.5
    agent-5: 1.05
  policy_reward_min:
    agent-0: 0.0
    agent-1: -50.0
    agent-2: -43.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -48.0
  sampler_perf:
    mean_env_wait_ms: 23.64301749984861
    mean_inference_ms: 13.401585703423468
    mean_processing_ms: 59.57474345692652
  time_since_restore: 2588.8817088603973
  time_this_iter_s: 126.54963088035583
  time_total_s: 25099.164951324463
  timestamp: 1637222663
  timesteps_since_restore: 1920000
  timesteps_this_iter: 96000
  timesteps_total: 17280000
  training_iteration: 180
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 42.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    180 |          25099.2 | 17280000 |    18.41 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 8
    apples_agent-0_mean: 2.46
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 1.08
    apples_agent-1_min: 0
    apples_agent-2_max: 53
    apples_agent-2_mean: 4.09
    apples_agent-2_min: 0
    apples_agent-3_max: 19
    apples_agent-3_mean: 2.77
    apples_agent-3_min: 0
    apples_agent-4_max: 63
    apples_agent-4_mean: 1.78
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 1.11
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 189
    cleaning_beam_agent-0_mean: 80.82
    cleaning_beam_agent-0_min: 23
    cleaning_beam_agent-1_max: 417
    cleaning_beam_agent-1_mean: 276.41
    cleaning_beam_agent-1_min: 189
    cleaning_beam_agent-2_max: 96
    cleaning_beam_agent-2_mean: 21.82
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 177
    cleaning_beam_agent-3_mean: 95.07
    cleaning_beam_agent-3_min: 23
    cleaning_beam_agent-4_max: 258
    cleaning_beam_agent-4_mean: 100.62
    cleaning_beam_agent-4_min: 27
    cleaning_beam_agent-5_max: 43
    cleaning_beam_agent-5_mean: 10.03
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-06-30
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 86.0
  episode_reward_mean: 22.04
  episode_reward_min: -34.0
  episodes_this_iter: 96
  episodes_total: 17376
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12091.516
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00018172799900639802
        entropy: 0.5869596004486084
        entropy_coeff: 0.0017600000137463212
        kl: 0.00937553122639656
        model: {}
        policy_loss: -0.018669161945581436
        total_loss: -0.018721751868724823
        vf_explained_var: -0.18952345848083496
        vf_loss: 0.4290681481361389
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00018172799900639802
        entropy: 0.47956085205078125
        entropy_coeff: 0.0017600000137463212
        kl: 0.006766056641936302
        model: {}
        policy_loss: -0.011666637845337391
        total_loss: -0.012146545574069023
        vf_explained_var: 0.01793275773525238
        vf_loss: 0.25812938809394836
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00018172799900639802
        entropy: 0.5039715766906738
        entropy_coeff: 0.0017600000137463212
        kl: 0.007051269523799419
        model: {}
        policy_loss: -0.015634912997484207
        total_loss: -0.015739524737000465
        vf_explained_var: -0.002955108880996704
        vf_loss: 0.7725224494934082
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00018172799900639802
        entropy: 0.6317197680473328
        entropy_coeff: 0.0017600000137463212
        kl: 0.005413612350821495
        model: {}
        policy_loss: -0.008436741307377815
        total_loss: -0.008817438036203384
        vf_explained_var: 0.004959329962730408
        vf_loss: 1.8976938724517822
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00018172799900639802
        entropy: 0.7114567756652832
        entropy_coeff: 0.0017600000137463212
        kl: 0.009133334271609783
        model: {}
        policy_loss: -0.019079145044088364
        total_loss: -0.019355040043592453
        vf_explained_var: -0.14842379093170166
        vf_loss: 0.6293373107910156
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00018172799900639802
        entropy: 0.7968586683273315
        entropy_coeff: 0.0017600000137463212
        kl: 0.006572066806256771
        model: {}
        policy_loss: -0.011959698051214218
        total_loss: -0.012681898660957813
        vf_explained_var: 0.0020812004804611206
        vf_loss: 0.23065464198589325
    load_time_ms: 13882.107
    num_steps_sampled: 17376000
    num_steps_trained: 17376000
    sample_time_ms: 100957.427
    update_time_ms: 18.668
  iterations_since_restore: 21
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.352197802197814
    ram_util_percent: 23.164835164835164
  pid: 27405
  policy_reward_max:
    agent-0: 14.0
    agent-1: 13.0
    agent-2: 23.0
    agent-3: 31.0
    agent-4: 15.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 3.68
    agent-1: 2.48
    agent-2: 5.37
    agent-3: 3.67
    agent-4: 4.72
    agent-5: 2.12
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -47.0
    agent-4: 0.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 23.63583232936282
    mean_inference_ms: 13.395669567695492
    mean_processing_ms: 59.55330620923227
  time_since_restore: 2715.7095386981964
  time_this_iter_s: 126.82782983779907
  time_total_s: 25225.992781162262
  timestamp: 1637222790
  timesteps_since_restore: 2016000
  timesteps_this_iter: 96000
  timesteps_total: 17376000
  training_iteration: 181
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 43.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    181 |            25226 | 17376000 |    22.04 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 2.02
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 1.08
    apples_agent-1_min: 0
    apples_agent-2_max: 201
    apples_agent-2_mean: 4.1
    apples_agent-2_min: 0
    apples_agent-3_max: 17
    apples_agent-3_mean: 2.79
    apples_agent-3_min: 0
    apples_agent-4_max: 38
    apples_agent-4_mean: 1.19
    apples_agent-4_min: 0
    apples_agent-5_max: 27
    apples_agent-5_mean: 1.09
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 172
    cleaning_beam_agent-0_mean: 80.14
    cleaning_beam_agent-0_min: 31
    cleaning_beam_agent-1_max: 396
    cleaning_beam_agent-1_mean: 275.35
    cleaning_beam_agent-1_min: 180
    cleaning_beam_agent-2_max: 89
    cleaning_beam_agent-2_mean: 22.61
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 233
    cleaning_beam_agent-3_mean: 91.64
    cleaning_beam_agent-3_min: 20
    cleaning_beam_agent-4_max: 186
    cleaning_beam_agent-4_mean: 93.91
    cleaning_beam_agent-4_min: 26
    cleaning_beam_agent-5_max: 36
    cleaning_beam_agent-5_mean: 11.43
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-08-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 71.0
  episode_reward_mean: 20.14
  episode_reward_min: 0.0
  episodes_this_iter: 96
  episodes_total: 17472
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12105.824
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001757376012392342
        entropy: 0.5873880386352539
        entropy_coeff: 0.0017600000137463212
        kl: 0.008199994452297688
        model: {}
        policy_loss: -0.01828785426914692
        total_loss: -0.018470827490091324
        vf_explained_var: -0.21181342005729675
        vf_loss: 0.30828821659088135
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0001757376012392342
        entropy: 0.4738709330558777
        entropy_coeff: 0.0017600000137463212
        kl: 0.005897967144846916
        model: {}
        policy_loss: -0.010996207594871521
        total_loss: -0.011519267223775387
        vf_explained_var: 0.02135111391544342
        vf_loss: 0.16056805849075317
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001757376012392342
        entropy: 0.5065867304801941
        entropy_coeff: 0.0017600000137463212
        kl: 0.006542581133544445
        model: {}
        policy_loss: -0.015887530520558357
        total_loss: -0.016062259674072266
        vf_explained_var: 0.006662100553512573
        vf_loss: 0.6260524988174438
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001757376012392342
        entropy: 0.6205553412437439
        entropy_coeff: 0.0017600000137463212
        kl: 0.006660802755504847
        model: {}
        policy_loss: -0.013677522540092468
        total_loss: -0.014064298942685127
        vf_explained_var: -0.0038962960243225098
        vf_loss: 0.3932337462902069
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001757376012392342
        entropy: 0.737693190574646
        entropy_coeff: 0.0017600000137463212
        kl: 0.009242753498256207
        model: {}
        policy_loss: -0.01947135105729103
        total_loss: -0.019798433408141136
        vf_explained_var: -0.12458807229995728
        vf_loss: 0.4698065519332886
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001757376012392342
        entropy: 0.8044102191925049
        entropy_coeff: 0.0017600000137463212
        kl: 0.006907296366989613
        model: {}
        policy_loss: -0.012091580778360367
        total_loss: -0.012805088423192501
        vf_explained_var: 0.004341110587120056
        vf_loss: 0.11525554209947586
    load_time_ms: 13844.135
    num_steps_sampled: 17472000
    num_steps_trained: 17472000
    sample_time_ms: 100738.205
    update_time_ms: 18.597
  iterations_since_restore: 22
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.78938547486033
    ram_util_percent: 23.360893854748607
  pid: 27405
  policy_reward_max:
    agent-0: 13.0
    agent-1: 13.0
    agent-2: 22.0
    agent-3: 22.0
    agent-4: 18.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 3.16
    agent-1: 2.06
    agent-2: 5.21
    agent-3: 3.95
    agent-4: 4.16
    agent-5: 1.6
  policy_reward_min:
    agent-0: 0.0
    agent-1: -1.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.62812577584954
    mean_inference_ms: 13.390354486508834
    mean_processing_ms: 59.53047374123127
  time_since_restore: 2841.3545196056366
  time_this_iter_s: 125.64498090744019
  time_total_s: 25351.637762069702
  timestamp: 1637222916
  timesteps_since_restore: 2112000
  timesteps_this_iter: 96000
  timesteps_total: 17472000
  training_iteration: 182
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 43.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    182 |          25351.6 | 17472000 |    20.14 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 2.54
    apples_agent-0_min: 0
    apples_agent-1_max: 22
    apples_agent-1_mean: 1.32
    apples_agent-1_min: 0
    apples_agent-2_max: 14
    apples_agent-2_mean: 1.77
    apples_agent-2_min: 0
    apples_agent-3_max: 48
    apples_agent-3_mean: 3.37
    apples_agent-3_min: 0
    apples_agent-4_max: 38
    apples_agent-4_mean: 1.06
    apples_agent-4_min: 0
    apples_agent-5_max: 20
    apples_agent-5_mean: 1.22
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 223
    cleaning_beam_agent-0_mean: 83.21
    cleaning_beam_agent-0_min: 27
    cleaning_beam_agent-1_max: 404
    cleaning_beam_agent-1_mean: 275.24
    cleaning_beam_agent-1_min: 163
    cleaning_beam_agent-2_max: 100
    cleaning_beam_agent-2_mean: 20.1
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 166
    cleaning_beam_agent-3_mean: 86.9
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 282
    cleaning_beam_agent-4_mean: 92.6
    cleaning_beam_agent-4_min: 22
    cleaning_beam_agent-5_max: 31
    cleaning_beam_agent-5_mean: 9.88
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-10-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 52.0
  episode_reward_mean: 19.33
  episode_reward_min: 2.0
  episodes_this_iter: 96
  episodes_total: 17568
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12121.581
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001697472034720704
        entropy: 0.5869153141975403
        entropy_coeff: 0.0017600000137463212
        kl: 0.007805131375789642
        model: {}
        policy_loss: -0.017117377370595932
        total_loss: -0.017332246527075768
        vf_explained_var: -0.18868446350097656
        vf_loss: 0.3758673667907715
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0001697472034720704
        entropy: 0.47614824771881104
        entropy_coeff: 0.0017600000137463212
        kl: 0.006405860185623169
        model: {}
        policy_loss: -0.012334946542978287
        total_loss: -0.012836601585149765
        vf_explained_var: 0.035502731800079346
        vf_loss: 0.16072626411914825
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001697472034720704
        entropy: 0.5057586431503296
        entropy_coeff: 0.0017600000137463212
        kl: 0.00671273423358798
        model: {}
        policy_loss: -0.015717102214694023
        total_loss: -0.015892615541815758
        vf_explained_var: 0.01784685254096985
        vf_loss: 0.433463454246521
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001697472034720704
        entropy: 0.6156353950500488
        entropy_coeff: 0.0017600000137463212
        kl: 0.0063340370543301105
        model: {}
        policy_loss: -0.014109890908002853
        total_loss: -0.014528104104101658
        vf_explained_var: 0.006811901926994324
        vf_loss: 0.31903955340385437
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001697472034720704
        entropy: 0.733536422252655
        entropy_coeff: 0.0017600000137463212
        kl: 0.00896561797708273
        model: {}
        policy_loss: -0.020281996577978134
        total_loss: -0.020634694024920464
        vf_explained_var: -0.16145029664039612
        vf_loss: 0.41766464710235596
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001697472034720704
        entropy: 0.7929255962371826
        entropy_coeff: 0.0017600000137463212
        kl: 0.0064620072953403
        model: {}
        policy_loss: -0.011419039219617844
        total_loss: -0.012157614342868328
        vf_explained_var: 0.0012997239828109741
        vf_loss: 0.10774615406990051
    load_time_ms: 13833.15
    num_steps_sampled: 17568000
    num_steps_trained: 17568000
    sample_time_ms: 100693.85
    update_time_ms: 18.587
  iterations_since_restore: 23
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.214364640883986
    ram_util_percent: 23.356906077348068
  pid: 27405
  policy_reward_max:
    agent-0: 15.0
    agent-1: 9.0
    agent-2: 16.0
    agent-3: 15.0
    agent-4: 15.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 3.34
    agent-1: 2.17
    agent-2: 4.35
    agent-3: 3.98
    agent-4: 3.91
    agent-5: 1.58
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.619423326937426
    mean_inference_ms: 13.38628671189594
    mean_processing_ms: 59.51880471512326
  time_since_restore: 2968.3963935375214
  time_this_iter_s: 127.04187393188477
  time_total_s: 25478.679636001587
  timestamp: 1637223043
  timesteps_since_restore: 2208000
  timesteps_this_iter: 96000
  timesteps_total: 17568000
  training_iteration: 183
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 43.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    183 |          25478.7 | 17568000 |    19.33 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 1.85
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 0.86
    apples_agent-1_min: 0
    apples_agent-2_max: 27
    apples_agent-2_mean: 1.75
    apples_agent-2_min: 0
    apples_agent-3_max: 48
    apples_agent-3_mean: 3.26
    apples_agent-3_min: 0
    apples_agent-4_max: 34
    apples_agent-4_mean: 1.26
    apples_agent-4_min: 0
    apples_agent-5_max: 5
    apples_agent-5_mean: 0.8
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 183
    cleaning_beam_agent-0_mean: 78.05
    cleaning_beam_agent-0_min: 2
    cleaning_beam_agent-1_max: 416
    cleaning_beam_agent-1_mean: 286.44
    cleaning_beam_agent-1_min: 162
    cleaning_beam_agent-2_max: 100
    cleaning_beam_agent-2_mean: 22.56
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 194
    cleaning_beam_agent-3_mean: 94.01
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 216
    cleaning_beam_agent-4_mean: 80.86
    cleaning_beam_agent-4_min: 10
    cleaning_beam_agent-5_max: 28
    cleaning_beam_agent-5_mean: 10.17
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-12-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 48.0
  episode_reward_mean: 18.01
  episode_reward_min: -23.0
  episodes_this_iter: 96
  episodes_total: 17664
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12116.695
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00016375680570490658
        entropy: 0.557730495929718
        entropy_coeff: 0.0017600000137463212
        kl: 0.00756310299038887
        model: {}
        policy_loss: -0.017063556239008904
        total_loss: -0.01725693605840206
        vf_explained_var: -0.14292356371879578
        vf_loss: 0.31914031505584717
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00016375680570490658
        entropy: 0.4691096544265747
        entropy_coeff: 0.0017600000137463212
        kl: 0.006068982649594545
        model: {}
        policy_loss: -0.011870250105857849
        total_loss: -0.012379485182464123
        vf_explained_var: 0.010666906833648682
        vf_loss: 0.12950101494789124
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00016375680570490658
        entropy: 0.5100928544998169
        entropy_coeff: 0.0017600000137463212
        kl: 0.006096610799431801
        model: {}
        policy_loss: -0.013594523072242737
        total_loss: -0.013833778910338879
        vf_explained_var: 0.02151714265346527
        vf_loss: 0.48843494057655334
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00016375680570490658
        entropy: 0.6217769980430603
        entropy_coeff: 0.0017600000137463212
        kl: 0.006031829863786697
        model: {}
        policy_loss: -0.013961909338831902
        total_loss: -0.014422805048525333
        vf_explained_var: 0.004631027579307556
        vf_loss: 0.3024793267250061
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00016375680570490658
        entropy: 0.7240933179855347
        entropy_coeff: 0.0017600000137463212
        kl: 0.007992768660187721
        model: {}
        policy_loss: -0.018123865127563477
        total_loss: -0.01855454221367836
        vf_explained_var: -0.08498063683509827
        vf_loss: 0.44453269243240356
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00016375680570490658
        entropy: 0.7772647738456726
        entropy_coeff: 0.0017600000137463212
        kl: 0.006393010262399912
        model: {}
        policy_loss: -0.011686512269079685
        total_loss: -0.012399460189044476
        vf_explained_var: 0.025557130575180054
        vf_loss: 0.15737666189670563
    load_time_ms: 13829.341
    num_steps_sampled: 17664000
    num_steps_trained: 17664000
    sample_time_ms: 100740.411
    update_time_ms: 18.873
  iterations_since_restore: 24
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 46.74585635359116
    ram_util_percent: 21.12099447513812
  pid: 27405
  policy_reward_max:
    agent-0: 11.0
    agent-1: 10.0
    agent-2: 22.0
    agent-3: 10.0
    agent-4: 13.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 2.99
    agent-1: 1.74
    agent-2: 3.65
    agent-3: 3.59
    agent-4: 4.36
    agent-5: 1.68
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -36.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.615772326521896
    mean_inference_ms: 13.386830349699714
    mean_processing_ms: 59.51790787332479
  time_since_restore: 3095.794451713562
  time_this_iter_s: 127.39805817604065
  time_total_s: 25606.077694177628
  timestamp: 1637223171
  timesteps_since_restore: 2304000
  timesteps_this_iter: 96000
  timesteps_total: 17664000
  training_iteration: 184
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 36.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    184 |          25606.1 | 17664000 |    18.01 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 14
    apples_agent-0_mean: 2.03
    apples_agent-0_min: 0
    apples_agent-1_max: 22
    apples_agent-1_mean: 1.09
    apples_agent-1_min: 0
    apples_agent-2_max: 32
    apples_agent-2_mean: 2.47
    apples_agent-2_min: 0
    apples_agent-3_max: 11
    apples_agent-3_mean: 2.61
    apples_agent-3_min: 0
    apples_agent-4_max: 14
    apples_agent-4_mean: 1.3
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 0.81
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 173
    cleaning_beam_agent-0_mean: 76.58
    cleaning_beam_agent-0_min: 21
    cleaning_beam_agent-1_max: 382
    cleaning_beam_agent-1_mean: 279.0
    cleaning_beam_agent-1_min: 183
    cleaning_beam_agent-2_max: 97
    cleaning_beam_agent-2_mean: 19.68
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 169
    cleaning_beam_agent-3_mean: 87.99
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 177
    cleaning_beam_agent-4_mean: 79.45
    cleaning_beam_agent-4_min: 21
    cleaning_beam_agent-5_max: 47
    cleaning_beam_agent-5_mean: 11.42
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-14-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 59.0
  episode_reward_mean: 18.29
  episode_reward_min: -45.0
  episodes_this_iter: 96
  episodes_total: 17760
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12112.739
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00015776639338582754
        entropy: 0.5767718553543091
        entropy_coeff: 0.0017600000137463212
        kl: 0.008187036961317062
        model: {}
        policy_loss: -0.017085406929254532
        total_loss: -0.017246274277567863
        vf_explained_var: -0.1490953266620636
        vf_loss: 0.3554764986038208
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00015776639338582754
        entropy: 0.47058579325675964
        entropy_coeff: 0.0017600000137463212
        kl: 0.005446550436317921
        model: {}
        policy_loss: -0.01070965826511383
        total_loss: -0.011251688003540039
        vf_explained_var: 0.03994934260845184
        vf_loss: 0.1387585997581482
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00015776639338582754
        entropy: 0.49805963039398193
        entropy_coeff: 0.0017600000137463212
        kl: 0.006566869560629129
        model: {}
        policy_loss: -0.015155734494328499
        total_loss: -0.015331652946770191
        vf_explained_var: 0.012330368161201477
        vf_loss: 0.4398157000541687
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00015776639338582754
        entropy: 0.6252242922782898
        entropy_coeff: 0.0017600000137463212
        kl: 0.006299105938524008
        model: {}
        policy_loss: -0.013386241160333157
        total_loss: -0.01382511854171753
        vf_explained_var: 0.00108279287815094
        vf_loss: 0.3160594701766968
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00015776639338582754
        entropy: 0.7097166776657104
        entropy_coeff: 0.0017600000137463212
        kl: 0.007962474599480629
        model: {}
        policy_loss: -0.018627870827913284
        total_loss: -0.019032178446650505
        vf_explained_var: -0.08875834941864014
        vf_loss: 0.4854696989059448
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00015776639338582754
        entropy: 0.7736441493034363
        entropy_coeff: 0.0017600000137463212
        kl: 0.004261965863406658
        model: {}
        policy_loss: -0.00448248814791441
        total_loss: -0.005277687683701515
        vf_explained_var: 0.00993049144744873
        vf_loss: 1.402169108390808
    load_time_ms: 13836.139
    num_steps_sampled: 17760000
    num_steps_trained: 17760000
    sample_time_ms: 100725.77
    update_time_ms: 18.238
  iterations_since_restore: 25
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.73833333333333
    ram_util_percent: 19.41111111111111
  pid: 27405
  policy_reward_max:
    agent-0: 15.0
    agent-1: 7.0
    agent-2: 18.0
    agent-3: 18.0
    agent-4: 15.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 3.21
    agent-1: 1.89
    agent-2: 3.85
    agent-3: 3.88
    agent-4: 4.47
    agent-5: 0.99
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -45.0
    agent-3: -1.0
    agent-4: 0.0
    agent-5: -50.0
  sampler_perf:
    mean_env_wait_ms: 23.610982804641104
    mean_inference_ms: 13.381966460268991
    mean_processing_ms: 59.52030803365782
  time_since_restore: 3221.7814123630524
  time_this_iter_s: 125.98696064949036
  time_total_s: 25732.064654827118
  timestamp: 1637223297
  timesteps_since_restore: 2400000
  timesteps_this_iter: 96000
  timesteps_total: 17760000
  training_iteration: 185
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 36.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    185 |          25732.1 | 17760000 |    18.29 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 1.87
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 1.02
    apples_agent-1_min: 0
    apples_agent-2_max: 11
    apples_agent-2_mean: 1.67
    apples_agent-2_min: 0
    apples_agent-3_max: 13
    apples_agent-3_mean: 2.61
    apples_agent-3_min: 0
    apples_agent-4_max: 5
    apples_agent-4_mean: 0.98
    apples_agent-4_min: 0
    apples_agent-5_max: 23
    apples_agent-5_mean: 1.1
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 224
    cleaning_beam_agent-0_mean: 75.98
    cleaning_beam_agent-0_min: 25
    cleaning_beam_agent-1_max: 421
    cleaning_beam_agent-1_mean: 278.63
    cleaning_beam_agent-1_min: 187
    cleaning_beam_agent-2_max: 61
    cleaning_beam_agent-2_mean: 20.23
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 202
    cleaning_beam_agent-3_mean: 92.57
    cleaning_beam_agent-3_min: 35
    cleaning_beam_agent-4_max: 145
    cleaning_beam_agent-4_mean: 77.92
    cleaning_beam_agent-4_min: 20
    cleaning_beam_agent-5_max: 47
    cleaning_beam_agent-5_mean: 14.44
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-17-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 46.0
  episode_reward_mean: 16.85
  episode_reward_min: -59.0
  episodes_this_iter: 96
  episodes_total: 17856
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12110.128
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00015177599561866373
        entropy: 0.5735508799552917
        entropy_coeff: 0.0017600000137463212
        kl: 0.004953059367835522
        model: {}
        policy_loss: -0.007952991873025894
        total_loss: -0.008177408948540688
        vf_explained_var: -0.012153387069702148
        vf_loss: 2.89725399017334
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00015177599561866373
        entropy: 0.4585157036781311
        entropy_coeff: 0.0017600000137463212
        kl: 0.005431083030998707
        model: {}
        policy_loss: -0.006990447640419006
        total_loss: -0.007390269078314304
        vf_explained_var: 0.01744435727596283
        vf_loss: 1.3561303615570068
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00015177599561866373
        entropy: 0.5093331933021545
        entropy_coeff: 0.0017600000137463212
        kl: 0.006934348959475756
        model: {}
        policy_loss: -0.016232900321483612
        total_loss: -0.016406722366809845
        vf_explained_var: 0.0035436004400253296
        vf_loss: 0.2917432487010956
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00015177599561866373
        entropy: 0.6407150030136108
        entropy_coeff: 0.0017600000137463212
        kl: 0.006983686704188585
        model: {}
        policy_loss: -0.00869426317512989
        total_loss: -0.008955847471952438
        vf_explained_var: 0.007398813962936401
        vf_loss: 1.6770401000976562
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00015177599561866373
        entropy: 0.7020108699798584
        entropy_coeff: 0.0017600000137463212
        kl: 0.007530015427619219
        model: {}
        policy_loss: -0.0183138158172369
        total_loss: -0.01875934936106205
        vf_explained_var: -0.08493161201477051
        vf_loss: 0.3700507879257202
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00015177599561866373
        entropy: 0.8056095242500305
        entropy_coeff: 0.0017600000137463212
        kl: 0.008181935176253319
        model: {}
        policy_loss: -0.012276394292712212
        total_loss: -0.013270444236695766
        vf_explained_var: 0.027106642723083496
        vf_loss: 0.147300586104393
    load_time_ms: 13823.643
    num_steps_sampled: 17856000
    num_steps_trained: 17856000
    sample_time_ms: 100551.84
    update_time_ms: 18.276
  iterations_since_restore: 26
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.63833333333332
    ram_util_percent: 19.69
  pid: 27405
  policy_reward_max:
    agent-0: 9.0
    agent-1: 10.0
    agent-2: 13.0
    agent-3: 14.0
    agent-4: 12.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 2.16
    agent-1: 1.59
    agent-2: 3.76
    agent-3: 3.64
    agent-4: 3.88
    agent-5: 1.82
  policy_reward_min:
    agent-0: -49.0
    agent-1: -49.0
    agent-2: -1.0
    agent-3: -46.0
    agent-4: 0.0
    agent-5: -2.0
  sampler_perf:
    mean_env_wait_ms: 23.604436776763052
    mean_inference_ms: 13.377841607282852
    mean_processing_ms: 59.52239775672823
  time_since_restore: 3348.494436264038
  time_this_iter_s: 126.71302390098572
  time_total_s: 25858.777678728104
  timestamp: 1637223424
  timesteps_since_restore: 2496000
  timesteps_this_iter: 96000
  timesteps_total: 17856000
  training_iteration: 186
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 36.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    186 |          25858.8 | 17856000 |    16.85 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 2.38
    apples_agent-0_min: 0
    apples_agent-1_max: 20
    apples_agent-1_mean: 0.98
    apples_agent-1_min: 0
    apples_agent-2_max: 21
    apples_agent-2_mean: 2.14
    apples_agent-2_min: 0
    apples_agent-3_max: 12
    apples_agent-3_mean: 2.84
    apples_agent-3_min: 0
    apples_agent-4_max: 21
    apples_agent-4_mean: 1.44
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 1.16
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 184
    cleaning_beam_agent-0_mean: 73.99
    cleaning_beam_agent-0_min: 15
    cleaning_beam_agent-1_max: 398
    cleaning_beam_agent-1_mean: 263.32
    cleaning_beam_agent-1_min: 179
    cleaning_beam_agent-2_max: 108
    cleaning_beam_agent-2_mean: 22.07
    cleaning_beam_agent-2_min: 6
    cleaning_beam_agent-3_max: 168
    cleaning_beam_agent-3_mean: 92.55
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 160
    cleaning_beam_agent-4_mean: 76.29
    cleaning_beam_agent-4_min: 11
    cleaning_beam_agent-5_max: 52
    cleaning_beam_agent-5_mean: 14.69
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-19-11
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 60.0
  episode_reward_mean: 19.26
  episode_reward_min: -41.0
  episodes_this_iter: 96
  episodes_total: 17952
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12121.445
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00014578559785149992
        entropy: 0.5707733631134033
        entropy_coeff: 0.0017600000137463212
        kl: 0.008194043301045895
        model: {}
        policy_loss: -0.015308224596083164
        total_loss: -0.015854649245738983
        vf_explained_var: -0.08253568410873413
        vf_loss: 0.48438525199890137
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00014578559785149992
        entropy: 0.4684422016143799
        entropy_coeff: 0.0017600000137463212
        kl: 0.0056061712093651295
        model: {}
        policy_loss: -0.01108299195766449
        total_loss: -0.01160875428467989
        vf_explained_var: 0.01949305832386017
        vf_loss: 0.1838923692703247
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00014578559785149992
        entropy: 0.5055385231971741
        entropy_coeff: 0.0017600000137463212
        kl: 0.004962297156453133
        model: {}
        policy_loss: -0.008274953812360764
        total_loss: -0.008485567755997181
        vf_explained_var: 0.0039503127336502075
        vf_loss: 1.8290507793426514
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00014578559785149992
        entropy: 0.6147798299789429
        entropy_coeff: 0.0017600000137463212
        kl: 0.006221684627234936
        model: {}
        policy_loss: -0.012518263421952724
        total_loss: -0.012935657054185867
        vf_explained_var: 0.004271984100341797
        vf_loss: 0.424495667219162
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00014578559785149992
        entropy: 0.6963501572608948
        entropy_coeff: 0.0017600000137463212
        kl: 0.006703374907374382
        model: {}
        policy_loss: -0.016215287148952484
        total_loss: -0.016721341758966446
        vf_explained_var: -0.05581209063529968
        vf_loss: 0.4918208122253418
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00014578559785149992
        entropy: 0.8125005960464478
        entropy_coeff: 0.0017600000137463212
        kl: 0.007731940597295761
        model: {}
        policy_loss: -0.01195306982845068
        total_loss: -0.012980220839381218
        vf_explained_var: 0.014646202325820923
        vf_loss: 0.16252118349075317
    load_time_ms: 13836.014
    num_steps_sampled: 17952000
    num_steps_trained: 17952000
    sample_time_ms: 100619.651
    update_time_ms: 17.638
  iterations_since_restore: 27
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.503296703296705
    ram_util_percent: 19.92967032967033
  pid: 27405
  policy_reward_max:
    agent-0: 17.0
    agent-1: 11.0
    agent-2: 22.0
    agent-3: 16.0
    agent-4: 18.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 3.13
    agent-1: 2.03
    agent-2: 3.89
    agent-3: 4.08
    agent-4: 4.18
    agent-5: 1.95
  policy_reward_min:
    agent-0: -47.0
    agent-1: 0.0
    agent-2: -48.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 23.599954408406173
    mean_inference_ms: 13.37579180407413
    mean_processing_ms: 59.52546088458971
  time_since_restore: 3476.0370733737946
  time_this_iter_s: 127.54263710975647
  time_total_s: 25986.32031583786
  timestamp: 1637223551
  timesteps_since_restore: 2592000
  timesteps_this_iter: 96000
  timesteps_total: 17952000
  training_iteration: 187
  trial_id: '00000'
  
[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
== Status ==
Memory usage on this node: 37.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    187 |          25986.3 | 17952000 |    19.26 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 85
    apples_agent-0_mean: 3.12
    apples_agent-0_min: 0
    apples_agent-1_max: 20
    apples_agent-1_mean: 1.3
    apples_agent-1_min: 0
    apples_agent-2_max: 37
    apples_agent-2_mean: 2.06
    apples_agent-2_min: 0
    apples_agent-3_max: 61
    apples_agent-3_mean: 3.57
    apples_agent-3_min: 0
    apples_agent-4_max: 18
    apples_agent-4_mean: 1.44
    apples_agent-4_min: 0
    apples_agent-5_max: 11
    apples_agent-5_mean: 1.27
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 231
    cleaning_beam_agent-0_mean: 76.49
    cleaning_beam_agent-0_min: 18
    cleaning_beam_agent-1_max: 392
    cleaning_beam_agent-1_mean: 269.29
    cleaning_beam_agent-1_min: 189
    cleaning_beam_agent-2_max: 62
    cleaning_beam_agent-2_mean: 20.98
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 164
    cleaning_beam_agent-3_mean: 88.25
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 330
    cleaning_beam_agent-4_mean: 91.73
    cleaning_beam_agent-4_min: 11
    cleaning_beam_agent-5_max: 35
    cleaning_beam_agent-5_mean: 13.12
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-21-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 76.0
  episode_reward_mean: 20.0
  episode_reward_min: -83.0
  episodes_this_iter: 96
  episodes_total: 18048
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12133.205
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0001397952000843361
        entropy: 0.5646940469741821
        entropy_coeff: 0.0017600000137463212
        kl: 0.007803926710039377
        model: {}
        policy_loss: -0.01639242097735405
        total_loss: -0.016969449818134308
        vf_explained_var: -0.12265914678573608
        vf_loss: 0.26639387011528015
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0001397952000843361
        entropy: 0.4557706117630005
        entropy_coeff: 0.0017600000137463212
        kl: 0.0037660817615687847
        model: {}
        policy_loss: -0.007092459127306938
        total_loss: -0.007659049239009619
        vf_explained_var: 0.014456987380981445
        vf_loss: 0.47263622283935547
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0001397952000843361
        entropy: 0.5187579393386841
        entropy_coeff: 0.0017600000137463212
        kl: 0.006918719969689846
        model: {}
        policy_loss: -0.014277061447501183
        total_loss: -0.01479709055274725
        vf_explained_var: 0.02739495038986206
        vf_loss: 0.47047311067581177
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001397952000843361
        entropy: 0.6121328473091125
        entropy_coeff: 0.0017600000137463212
        kl: 0.005743388086557388
        model: {}
        policy_loss: -0.01237855851650238
        total_loss: -0.012835920788347721
        vf_explained_var: 0.0032707154750823975
        vf_loss: 0.45651084184646606
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001397952000843361
        entropy: 0.6896286010742188
        entropy_coeff: 0.0017600000137463212
        kl: 0.005635186098515987
        model: {}
        policy_loss: -0.01243911124765873
        total_loss: -0.013017430901527405
        vf_explained_var: -0.06204712390899658
        vf_loss: 0.7190766930580139
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0001397952000843361
        entropy: 0.8034214377403259
        entropy_coeff: 0.0017600000137463212
        kl: 0.006394883617758751
        model: {}
        policy_loss: -0.010383302345871925
        total_loss: -0.011460662819445133
        vf_explained_var: 0.008308663964271545
        vf_loss: 0.1691499650478363
    load_time_ms: 13819.62
    num_steps_sampled: 18048000
    num_steps_trained: 18048000
    sample_time_ms: 100728.653
    update_time_ms: 17.101
  iterations_since_restore: 28
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.589010989010994
    ram_util_percent: 20.164835164835164
  pid: 27405
  policy_reward_max:
    agent-0: 10.0
    agent-1: 11.0
    agent-2: 23.0
    agent-3: 20.0
    agent-4: 17.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 3.47
    agent-1: 1.73
    agent-2: 4.67
    agent-3: 4.35
    agent-4: 3.9
    agent-5: 1.88
  policy_reward_min:
    agent-0: -1.0
    agent-1: -50.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -47.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.60358089390791
    mean_inference_ms: 13.37801449482317
    mean_processing_ms: 59.54560496055252
  time_since_restore: 3603.6481404304504
  time_this_iter_s: 127.61106705665588
  time_total_s: 26113.931382894516
  timestamp: 1637223679
  timesteps_since_restore: 2688000
  timesteps_this_iter: 96000
  timesteps_total: 18048000
  training_iteration: 188
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 37.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    188 |          26113.9 | 18048000 |       20 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 32
    apples_agent-0_mean: 2.75
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.93
    apples_agent-1_min: 0
    apples_agent-2_max: 16
    apples_agent-2_mean: 1.97
    apples_agent-2_min: 0
    apples_agent-3_max: 23
    apples_agent-3_mean: 3.68
    apples_agent-3_min: 0
    apples_agent-4_max: 6
    apples_agent-4_mean: 0.9
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 0.95
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 154
    cleaning_beam_agent-0_mean: 74.67
    cleaning_beam_agent-0_min: 26
    cleaning_beam_agent-1_max: 406
    cleaning_beam_agent-1_mean: 271.63
    cleaning_beam_agent-1_min: 154
    cleaning_beam_agent-2_max: 52
    cleaning_beam_agent-2_mean: 20.0
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 170
    cleaning_beam_agent-3_mean: 86.25
    cleaning_beam_agent-3_min: 27
    cleaning_beam_agent-4_max: 176
    cleaning_beam_agent-4_mean: 82.6
    cleaning_beam_agent-4_min: 23
    cleaning_beam_agent-5_max: 45
    cleaning_beam_agent-5_mean: 14.46
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-23-25
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 61.0
  episode_reward_mean: 19.24
  episode_reward_min: -92.0
  episodes_this_iter: 96
  episodes_total: 18144
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12149.268
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0001338048023171723
        entropy: 0.5790407061576843
        entropy_coeff: 0.0017600000137463212
        kl: 0.006063636392354965
        model: {}
        policy_loss: -0.008259125053882599
        total_loss: -0.008805729448795319
        vf_explained_var: -0.009444952011108398
        vf_loss: 1.6932355165481567
      agent-1:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0001338048023171723
        entropy: 0.4525263011455536
        entropy_coeff: 0.0017600000137463212
        kl: 0.003890505526214838
        model: {}
        policy_loss: -0.004471160471439362
        total_loss: -0.00502501055598259
        vf_explained_var: 0.002901330590248108
        vf_loss: 1.4533679485321045
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0001338048023171723
        entropy: 0.5136725306510925
        entropy_coeff: 0.0017600000137463212
        kl: 0.006754576228559017
        model: {}
        policy_loss: -0.01476522721350193
        total_loss: -0.015279821120202541
        vf_explained_var: 0.013351008296012878
        vf_loss: 0.5174424648284912
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001338048023171723
        entropy: 0.6059483289718628
        entropy_coeff: 0.0017600000137463212
        kl: 0.005619218572974205
        model: {}
        policy_loss: -0.01190736424177885
        total_loss: -0.012373990379273891
        vf_explained_var: 0.004853293299674988
        vf_loss: 0.3792237937450409
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001338048023171723
        entropy: 0.6768858432769775
        entropy_coeff: 0.0017600000137463212
        kl: 0.007068156730383635
        model: {}
        policy_loss: -0.01631198637187481
        total_loss: -0.016747716814279556
        vf_explained_var: -0.05232512950897217
        vf_loss: 0.4877193570137024
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0001338048023171723
        entropy: 0.823447048664093
        entropy_coeff: 0.0017600000137463212
        kl: 0.009986985474824905
        model: {}
        policy_loss: -0.008527616038918495
        total_loss: -0.009207440540194511
        vf_explained_var: 0.006170555949211121
        vf_loss: 2.7009482383728027
    load_time_ms: 13812.163
    num_steps_sampled: 18144000
    num_steps_trained: 18144000
    sample_time_ms: 100663.831
    update_time_ms: 16.559
  iterations_since_restore: 29
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.705027932960895
    ram_util_percent: 20.335754189944137
  pid: 27405
  policy_reward_max:
    agent-0: 16.0
    agent-1: 9.0
    agent-2: 17.0
    agent-3: 14.0
    agent-4: 13.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 3.2
    agent-1: 1.68
    agent-2: 4.79
    agent-3: 4.47
    agent-4: 4.47
    agent-5: 0.63
  policy_reward_min:
    agent-0: -47.0
    agent-1: -49.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -50.0
  sampler_perf:
    mean_env_wait_ms: 23.596780926313368
    mean_inference_ms: 13.375344886215364
    mean_processing_ms: 59.53555956391879
  time_since_restore: 3729.669802427292
  time_this_iter_s: 126.02166199684143
  time_total_s: 26239.953044891357
  timestamp: 1637223805
  timesteps_since_restore: 2784000
  timesteps_this_iter: 96000
  timesteps_total: 18144000
  training_iteration: 189
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 37.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    189 |            26240 | 18144000 |    19.24 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 14
    apples_agent-0_mean: 1.95
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 1.01
    apples_agent-1_min: 0
    apples_agent-2_max: 82
    apples_agent-2_mean: 2.92
    apples_agent-2_min: 0
    apples_agent-3_max: 17
    apples_agent-3_mean: 2.66
    apples_agent-3_min: 0
    apples_agent-4_max: 5
    apples_agent-4_mean: 0.76
    apples_agent-4_min: 0
    apples_agent-5_max: 5
    apples_agent-5_mean: 0.75
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 210
    cleaning_beam_agent-0_mean: 76.7
    cleaning_beam_agent-0_min: 6
    cleaning_beam_agent-1_max: 422
    cleaning_beam_agent-1_mean: 281.12
    cleaning_beam_agent-1_min: 196
    cleaning_beam_agent-2_max: 53
    cleaning_beam_agent-2_mean: 20.04
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 183
    cleaning_beam_agent-3_mean: 84.28
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 343
    cleaning_beam_agent-4_mean: 83.4
    cleaning_beam_agent-4_min: 19
    cleaning_beam_agent-5_max: 43
    cleaning_beam_agent-5_mean: 16.95
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-25-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 75.0
  episode_reward_mean: 17.48
  episode_reward_min: -85.0
  episodes_this_iter: 96
  episodes_total: 18240
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12169.034
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00012781440455000848
        entropy: 0.5738199353218079
        entropy_coeff: 0.0017600000137463212
        kl: 0.006039462983608246
        model: {}
        policy_loss: -0.007042635232210159
        total_loss: -0.007595554925501347
        vf_explained_var: -0.014671355485916138
        vf_loss: 1.5502851009368896
      agent-1:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 0.00012781440455000848
        entropy: 0.46908244490623474
        entropy_coeff: 0.0017600000137463212
        kl: 0.0037629883736371994
        model: {}
        policy_loss: -0.0038701000157743692
        total_loss: -0.004497971385717392
        vf_explained_var: 0.008850932121276855
        vf_loss: 1.5067672729492188
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00012781440455000848
        entropy: 0.5192470550537109
        entropy_coeff: 0.0017600000137463212
        kl: 0.006882650312036276
        model: {}
        policy_loss: -0.01427013985812664
        total_loss: -0.014790014363825321
        vf_explained_var: 0.017821744084358215
        vf_loss: 0.49868810176849365
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00012781440455000848
        entropy: 0.6006501913070679
        entropy_coeff: 0.0017600000137463212
        kl: 0.004906885325908661
        model: {}
        policy_loss: -0.01004344318062067
        total_loss: -0.010565012693405151
        vf_explained_var: 0.0017538070678710938
        vf_loss: 0.44885480403900146
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00012781440455000848
        entropy: 0.6921741962432861
        entropy_coeff: 0.0017600000137463212
        kl: 0.006708355620503426
        model: {}
        policy_loss: -0.015592253766953945
        total_loss: -0.016092555597424507
        vf_explained_var: -0.04928481578826904
        vf_loss: 0.4709165692329407
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00012781440455000848
        entropy: 0.8536096811294556
        entropy_coeff: 0.0017600000137463212
        kl: 0.00729017099365592
        model: {}
        policy_loss: -0.006188581697642803
        total_loss: -0.007184429094195366
        vf_explained_var: 0.003411814570426941
        vf_loss: 1.4199624061584473
    load_time_ms: 13835.875
    num_steps_sampled: 18240000
    num_steps_trained: 18240000
    sample_time_ms: 100660.168
    update_time_ms: 16.473
  iterations_since_restore: 30
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.96408839779006
    ram_util_percent: 20.5707182320442
  pid: 27405
  policy_reward_max:
    agent-0: 15.0
    agent-1: 12.0
    agent-2: 24.0
    agent-3: 24.0
    agent-4: 16.0
    agent-5: 6.0
  policy_reward_mean:
    agent-0: 2.64
    agent-1: 1.78
    agent-2: 4.32
    agent-3: 4.06
    agent-4: 3.66
    agent-5: 1.02
  policy_reward_min:
    agent-0: -45.0
    agent-1: -50.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -46.0
    agent-5: -47.0
  sampler_perf:
    mean_env_wait_ms: 23.588873608526907
    mean_inference_ms: 13.373007754001282
    mean_processing_ms: 59.53387292309458
  time_since_restore: 3856.6147701740265
  time_this_iter_s: 126.94496774673462
  time_total_s: 26366.898012638092
  timestamp: 1637223932
  timesteps_since_restore: 2880000
  timesteps_this_iter: 96000
  timesteps_total: 18240000
  training_iteration: 190
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 38.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    190 |          26366.9 | 18240000 |    17.48 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 8
    apples_agent-0_mean: 2.1
    apples_agent-0_min: 0
    apples_agent-1_max: 14
    apples_agent-1_mean: 1.31
    apples_agent-1_min: 0
    apples_agent-2_max: 24
    apples_agent-2_mean: 2.23
    apples_agent-2_min: 0
    apples_agent-3_max: 16
    apples_agent-3_mean: 3.11
    apples_agent-3_min: 0
    apples_agent-4_max: 50
    apples_agent-4_mean: 1.19
    apples_agent-4_min: 0
    apples_agent-5_max: 11
    apples_agent-5_mean: 0.96
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 190
    cleaning_beam_agent-0_mean: 80.15
    cleaning_beam_agent-0_min: 21
    cleaning_beam_agent-1_max: 500
    cleaning_beam_agent-1_mean: 275.04
    cleaning_beam_agent-1_min: 186
    cleaning_beam_agent-2_max: 50
    cleaning_beam_agent-2_mean: 18.26
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 156
    cleaning_beam_agent-3_mean: 83.04
    cleaning_beam_agent-3_min: 34
    cleaning_beam_agent-4_max: 172
    cleaning_beam_agent-4_mean: 79.46
    cleaning_beam_agent-4_min: 26
    cleaning_beam_agent-5_max: 70
    cleaning_beam_agent-5_mean: 17.94
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-27-39
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 63.0
  episode_reward_mean: 20.87
  episode_reward_min: -32.0
  episodes_this_iter: 96
  episodes_total: 18336
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12162.211
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00012182399950688705
        entropy: 0.5786209106445312
        entropy_coeff: 0.0017600000137463212
        kl: 0.007458062376827002
        model: {}
        policy_loss: -0.015194782987236977
        total_loss: -0.015804586932063103
        vf_explained_var: -0.0989823043346405
        vf_loss: 0.3566755950450897
      agent-1:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 0.00012182399950688705
        entropy: 0.45939499139785767
        entropy_coeff: 0.0017600000137463212
        kl: 0.005406621843576431
        model: {}
        policy_loss: -0.010638006031513214
        total_loss: -0.011394437402486801
        vf_explained_var: 0.025282010436058044
        vf_loss: 0.1831454485654831
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00012182399950688705
        entropy: 0.5026913285255432
        entropy_coeff: 0.0017600000137463212
        kl: 0.006039291154593229
        model: {}
        policy_loss: -0.01283024437725544
        total_loss: -0.013355078175663948
        vf_explained_var: 0.0027934759855270386
        vf_loss: 0.5793735980987549
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00012182399950688705
        entropy: 0.6045413613319397
        entropy_coeff: 0.0017600000137463212
        kl: 0.005930311046540737
        model: {}
        policy_loss: -0.01201433315873146
        total_loss: -0.012740503996610641
        vf_explained_var: -0.0002915412187576294
        vf_loss: 0.4130632281303406
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00012182399950688705
        entropy: 0.6835378408432007
        entropy_coeff: 0.0017600000137463212
        kl: 0.006262096110731363
        model: {}
        policy_loss: -0.01539226621389389
        total_loss: -0.015923868864774704
        vf_explained_var: -0.03430658578872681
        vf_loss: 0.45214998722076416
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00012182399950688705
        entropy: 0.8694864511489868
        entropy_coeff: 0.0017600000137463212
        kl: 0.006799762137234211
        model: {}
        policy_loss: -0.01121429167687893
        total_loss: -0.012391295284032822
        vf_explained_var: -0.017695575952529907
        vf_loss: 0.13304045796394348
    load_time_ms: 13844.991
    num_steps_sampled: 18336000
    num_steps_trained: 18336000
    sample_time_ms: 100613.125
    update_time_ms: 15.889
  iterations_since_restore: 31
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.882872928176795
    ram_util_percent: 20.740331491712706
  pid: 27405
  policy_reward_max:
    agent-0: 13.0
    agent-1: 8.0
    agent-2: 19.0
    agent-3: 24.0
    agent-4: 18.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 3.6
    agent-1: 2.16
    agent-2: 4.41
    agent-3: 4.46
    agent-4: 4.51
    agent-5: 1.73
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -48.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.58130585623093
    mean_inference_ms: 13.371445384319868
    mean_processing_ms: 59.530793522016566
  time_since_restore: 3982.9350407123566
  time_this_iter_s: 126.32027053833008
  time_total_s: 26493.218283176422
  timestamp: 1637224059
  timesteps_since_restore: 2976000
  timesteps_this_iter: 96000
  timesteps_total: 18336000
  training_iteration: 191
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 38.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    191 |          26493.2 | 18336000 |    20.87 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 2.25
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.99
    apples_agent-1_min: 0
    apples_agent-2_max: 22
    apples_agent-2_mean: 2.61
    apples_agent-2_min: 0
    apples_agent-3_max: 24
    apples_agent-3_mean: 2.88
    apples_agent-3_min: 0
    apples_agent-4_max: 6
    apples_agent-4_mean: 1.01
    apples_agent-4_min: 0
    apples_agent-5_max: 9
    apples_agent-5_mean: 1.12
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 212
    cleaning_beam_agent-0_mean: 79.02
    cleaning_beam_agent-0_min: 24
    cleaning_beam_agent-1_max: 409
    cleaning_beam_agent-1_mean: 273.22
    cleaning_beam_agent-1_min: 177
    cleaning_beam_agent-2_max: 135
    cleaning_beam_agent-2_mean: 20.43
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 165
    cleaning_beam_agent-3_mean: 85.51
    cleaning_beam_agent-3_min: 35
    cleaning_beam_agent-4_max: 177
    cleaning_beam_agent-4_mean: 76.13
    cleaning_beam_agent-4_min: 14
    cleaning_beam_agent-5_max: 58
    cleaning_beam_agent-5_mean: 17.45
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-29-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 66.0
  episode_reward_mean: 20.68
  episode_reward_min: 3.0
  episodes_this_iter: 96
  episodes_total: 18432
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12166.234
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00011583360173972324
        entropy: 0.5678027272224426
        entropy_coeff: 0.0017600000137463212
        kl: 0.006785040721297264
        model: {}
        policy_loss: -0.013928741216659546
        total_loss: -0.014550929889082909
        vf_explained_var: -0.049735426902770996
        vf_loss: 0.37890738248825073
      agent-1:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 0.00011583360173972324
        entropy: 0.4511878490447998
        entropy_coeff: 0.0017600000137463212
        kl: 0.005589877720922232
        model: {}
        policy_loss: -0.009718128480017185
        total_loss: -0.010458185337483883
        vf_explained_var: 0.031640663743019104
        vf_loss: 0.19096331298351288
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00011583360173972324
        entropy: 0.5041398406028748
        entropy_coeff: 0.0017600000137463212
        kl: 0.006064429879188538
        model: {}
        policy_loss: -0.013833782635629177
        total_loss: -0.014365525916218758
        vf_explained_var: 0.003989726305007935
        vf_loss: 0.5232119560241699
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00011583360173972324
        entropy: 0.5951647162437439
        entropy_coeff: 0.0017600000137463212
        kl: 0.005491145420819521
        model: {}
        policy_loss: -0.011414267122745514
        total_loss: -0.012153358198702335
        vf_explained_var: 0.0004937201738357544
        vf_loss: 0.33838793635368347
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00011583360173972324
        entropy: 0.6774206161499023
        entropy_coeff: 0.0017600000137463212
        kl: 0.006600943394005299
        model: {}
        policy_loss: -0.01628798618912697
        total_loss: -0.016781676560640335
        vf_explained_var: -0.05966460704803467
        vf_loss: 0.3847402334213257
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00011583360173972324
        entropy: 0.8453892469406128
        entropy_coeff: 0.0017600000137463212
        kl: 0.006468743085861206
        model: {}
        policy_loss: -0.010496756061911583
        total_loss: -0.011647354811429977
        vf_explained_var: 0.0014338642358779907
        vf_loss: 0.13846729695796967
    load_time_ms: 13847.932
    num_steps_sampled: 18432000
    num_steps_trained: 18432000
    sample_time_ms: 100680.668
    update_time_ms: 15.751
  iterations_since_restore: 32
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.53
    ram_util_percent: 20.973888888888887
  pid: 27405
  policy_reward_max:
    agent-0: 18.0
    agent-1: 12.0
    agent-2: 21.0
    agent-3: 17.0
    agent-4: 12.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 3.71
    agent-1: 2.21
    agent-2: 4.74
    agent-3: 4.06
    agent-4: 4.16
    agent-5: 1.8
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.575498073576977
    mean_inference_ms: 13.370470852606536
    mean_processing_ms: 59.52978312009734
  time_since_restore: 4109.365332841873
  time_this_iter_s: 126.4302921295166
  time_total_s: 26619.64857530594
  timestamp: 1637224186
  timesteps_since_restore: 3072000
  timesteps_this_iter: 96000
  timesteps_total: 18432000
  training_iteration: 192
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 39.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    192 |          26619.6 | 18432000 |    20.68 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 2.28
    apples_agent-0_min: 0
    apples_agent-1_max: 8
    apples_agent-1_mean: 0.98
    apples_agent-1_min: 0
    apples_agent-2_max: 10
    apples_agent-2_mean: 2.0
    apples_agent-2_min: 0
    apples_agent-3_max: 14
    apples_agent-3_mean: 2.84
    apples_agent-3_min: 0
    apples_agent-4_max: 28
    apples_agent-4_mean: 1.4
    apples_agent-4_min: 0
    apples_agent-5_max: 13
    apples_agent-5_mean: 1.14
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 289
    cleaning_beam_agent-0_mean: 84.75
    cleaning_beam_agent-0_min: 29
    cleaning_beam_agent-1_max: 497
    cleaning_beam_agent-1_mean: 258.09
    cleaning_beam_agent-1_min: 130
    cleaning_beam_agent-2_max: 58
    cleaning_beam_agent-2_mean: 17.22
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 150
    cleaning_beam_agent-3_mean: 79.54
    cleaning_beam_agent-3_min: 24
    cleaning_beam_agent-4_max: 181
    cleaning_beam_agent-4_mean: 78.1
    cleaning_beam_agent-4_min: 14
    cleaning_beam_agent-5_max: 61
    cleaning_beam_agent-5_mean: 21.21
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-31-54
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 70.0
  episode_reward_mean: 19.44
  episode_reward_min: -127.0
  episodes_this_iter: 96
  episodes_total: 18528
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12152.753
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00010984319669660181
        entropy: 0.5824507474899292
        entropy_coeff: 0.0017600000137463212
        kl: 0.005983557552099228
        model: {}
        policy_loss: -0.008553103543817997
        total_loss: -0.009115617722272873
        vf_explained_var: -0.009018093347549438
        vf_loss: 1.6342089176177979
      agent-1:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 0.00010984319669660181
        entropy: 0.4676722288131714
        entropy_coeff: 0.0017600000137463212
        kl: 0.005127093754708767
        model: {}
        policy_loss: -0.006590949837118387
        total_loss: -0.007235679775476456
        vf_explained_var: 0.004186198115348816
        vf_loss: 1.4633065462112427
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00010984319669660181
        entropy: 0.49577951431274414
        entropy_coeff: 0.0017600000137463212
        kl: 0.006045554298907518
        model: {}
        policy_loss: -0.009253494441509247
        total_loss: -0.009637273848056793
        vf_explained_var: 0.012541785836219788
        vf_loss: 1.8651444911956787
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00010984319669660181
        entropy: 0.5854219198226929
        entropy_coeff: 0.0017600000137463212
        kl: 0.005703441333025694
        model: {}
        policy_loss: -0.010994473472237587
        total_loss: -0.011699574999511242
        vf_explained_var: 0.0034565329551696777
        vf_loss: 0.4006883502006531
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00010984319669660181
        entropy: 0.6702695488929749
        entropy_coeff: 0.0017600000137463212
        kl: 0.0056267050094902515
        model: {}
        policy_loss: -0.014584925957024097
        total_loss: -0.015157666057348251
        vf_explained_var: -0.054519861936569214
        vf_loss: 0.4426918029785156
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00010984319669660181
        entropy: 0.8539739847183228
        entropy_coeff: 0.0017600000137463212
        kl: 0.006389899179339409
        model: {}
        policy_loss: -0.010510086081922054
        total_loss: -0.011677468195557594
        vf_explained_var: 0.0009663552045822144
        vf_loss: 0.1611865758895874
    load_time_ms: 13853.23
    num_steps_sampled: 18528000
    num_steps_trained: 18528000
    sample_time_ms: 100748.652
    update_time_ms: 15.677
  iterations_since_restore: 33
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.328571428571436
    ram_util_percent: 21.15604395604396
  pid: 27405
  policy_reward_max:
    agent-0: 15.0
    agent-1: 14.0
    agent-2: 20.0
    agent-3: 13.0
    agent-4: 14.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 3.07
    agent-1: 1.69
    agent-2: 4.4
    agent-3: 4.15
    agent-4: 4.2
    agent-5: 1.93
  policy_reward_min:
    agent-0: -43.0
    agent-1: -48.0
    agent-2: -46.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.571670949286567
    mean_inference_ms: 13.370255072693242
    mean_processing_ms: 59.535390921551524
  time_since_restore: 4237.017943143845
  time_this_iter_s: 127.65261030197144
  time_total_s: 26747.30118560791
  timestamp: 1637224314
  timesteps_since_restore: 3168000
  timesteps_this_iter: 96000
  timesteps_total: 18528000
  training_iteration: 193
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 39.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    193 |          26747.3 | 18528000 |    19.44 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 21
    apples_agent-0_mean: 3.04
    apples_agent-0_min: 0
    apples_agent-1_max: 16
    apples_agent-1_mean: 1.34
    apples_agent-1_min: 0
    apples_agent-2_max: 17
    apples_agent-2_mean: 2.51
    apples_agent-2_min: 0
    apples_agent-3_max: 18
    apples_agent-3_mean: 3.73
    apples_agent-3_min: 0
    apples_agent-4_max: 10
    apples_agent-4_mean: 0.96
    apples_agent-4_min: 0
    apples_agent-5_max: 16
    apples_agent-5_mean: 1.09
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 243
    cleaning_beam_agent-0_mean: 83.0
    cleaning_beam_agent-0_min: 23
    cleaning_beam_agent-1_max: 410
    cleaning_beam_agent-1_mean: 243.95
    cleaning_beam_agent-1_min: 131
    cleaning_beam_agent-2_max: 58
    cleaning_beam_agent-2_mean: 16.96
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 180
    cleaning_beam_agent-3_mean: 70.79
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 260
    cleaning_beam_agent-4_mean: 80.78
    cleaning_beam_agent-4_min: 22
    cleaning_beam_agent-5_max: 52
    cleaning_beam_agent-5_mean: 16.8
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-34-00
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 54.0
  episode_reward_mean: 21.98
  episode_reward_min: 3.0
  episodes_this_iter: 96
  episodes_total: 18624
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12163.406
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.000103852798929438
        entropy: 0.5639714598655701
        entropy_coeff: 0.0017600000137463212
        kl: 0.0063820527866482735
        model: {}
        policy_loss: -0.014188610017299652
        total_loss: -0.014830213971436024
        vf_explained_var: -0.04950782656669617
        vf_loss: 0.3188171684741974
      agent-1:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 0.000103852798929438
        entropy: 0.4587446451187134
        entropy_coeff: 0.0017600000137463212
        kl: 0.005542340688407421
        model: {}
        policy_loss: -0.009826206602156162
        total_loss: -0.010578693822026253
        vf_explained_var: 0.012508422136306763
        vf_loss: 0.20264483988285065
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.000103852798929438
        entropy: 0.502691388130188
        entropy_coeff: 0.0017600000137463212
        kl: 0.005677605047821999
        model: {}
        policy_loss: -0.012750226072967052
        total_loss: -0.01329825073480606
        vf_explained_var: 0.005296647548675537
        vf_loss: 0.5283093452453613
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.000103852798929438
        entropy: 0.5713210701942444
        entropy_coeff: 0.0017600000137463212
        kl: 0.005399354733526707
        model: {}
        policy_loss: -0.010296005755662918
        total_loss: -0.010985223576426506
        vf_explained_var: 0.0005469322204589844
        vf_loss: 0.463381290435791
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000103852798929438
        entropy: 0.6659139394760132
        entropy_coeff: 0.0017600000137463212
        kl: 0.005498811136931181
        model: {}
        policy_loss: -0.014726456254720688
        total_loss: -0.01531368587166071
        vf_explained_var: -0.040968090295791626
        vf_loss: 0.34897613525390625
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.000103852798929438
        entropy: 0.8437491655349731
        entropy_coeff: 0.0017600000137463212
        kl: 0.006343214772641659
        model: {}
        policy_loss: -0.010454030707478523
        total_loss: -0.011607231572270393
        vf_explained_var: 0.005725234746932983
        vf_loss: 0.14637091755867004
    load_time_ms: 13831.773
    num_steps_sampled: 18624000
    num_steps_trained: 18624000
    sample_time_ms: 100629.568
    update_time_ms: 15.777
  iterations_since_restore: 34
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.64333333333334
    ram_util_percent: 21.283333333333335
  pid: 27405
  policy_reward_max:
    agent-0: 13.0
    agent-1: 9.0
    agent-2: 20.0
    agent-3: 14.0
    agent-4: 14.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 3.67
    agent-1: 2.4
    agent-2: 5.17
    agent-3: 4.75
    agent-4: 4.1
    agent-5: 1.89
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 23.56131052024659
    mean_inference_ms: 13.368924094628833
    mean_processing_ms: 59.52786821070497
  time_since_restore: 4363.1175944805145
  time_this_iter_s: 126.09965133666992
  time_total_s: 26873.40083694458
  timestamp: 1637224440
  timesteps_since_restore: 3264000
  timesteps_this_iter: 96000
  timesteps_total: 18624000
  training_iteration: 194
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 39.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    194 |          26873.4 | 18624000 |    21.98 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 2.3
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 1.18
    apples_agent-1_min: 0
    apples_agent-2_max: 23
    apples_agent-2_mean: 2.01
    apples_agent-2_min: 0
    apples_agent-3_max: 28
    apples_agent-3_mean: 2.96
    apples_agent-3_min: 0
    apples_agent-4_max: 21
    apples_agent-4_mean: 1.29
    apples_agent-4_min: 0
    apples_agent-5_max: 11
    apples_agent-5_mean: 1.39
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 328
    cleaning_beam_agent-0_mean: 82.41
    cleaning_beam_agent-0_min: 34
    cleaning_beam_agent-1_max: 402
    cleaning_beam_agent-1_mean: 242.61
    cleaning_beam_agent-1_min: 146
    cleaning_beam_agent-2_max: 79
    cleaning_beam_agent-2_mean: 17.49
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 160
    cleaning_beam_agent-3_mean: 70.34
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 214
    cleaning_beam_agent-4_mean: 77.68
    cleaning_beam_agent-4_min: 18
    cleaning_beam_agent-5_max: 38
    cleaning_beam_agent-5_mean: 14.32
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-36-07
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 63.0
  episode_reward_mean: 19.54
  episode_reward_min: -31.0
  episodes_this_iter: 96
  episodes_total: 18720
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12181.295
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 9.786240116227418e-05
        entropy: 0.5623564124107361
        entropy_coeff: 0.0017600000137463212
        kl: 0.005966342054307461
        model: {}
        policy_loss: -0.006721259094774723
        total_loss: -0.007257565855979919
        vf_explained_var: -0.022059142589569092
        vf_loss: 1.551276445388794
      agent-1:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 9.786240116227418e-05
        entropy: 0.45858386158943176
        entropy_coeff: 0.0017600000137463212
        kl: 0.00500129908323288
        model: {}
        policy_loss: -0.009781453758478165
        total_loss: -0.010542015545070171
        vf_explained_var: 0.0365428626537323
        vf_loss: 0.15289732813835144
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 9.786240116227418e-05
        entropy: 0.49864697456359863
        entropy_coeff: 0.0017600000137463212
        kl: 0.005448861047625542
        model: {}
        policy_loss: -0.011983687058091164
        total_loss: -0.012544608674943447
        vf_explained_var: 0.011124610900878906
        vf_loss: 0.4425353407859802
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 9.786240116227418e-05
        entropy: 0.5796919465065002
        entropy_coeff: 0.0017600000137463212
        kl: 0.004900417756289244
        model: {}
        policy_loss: -0.009511743672192097
        total_loss: -0.010248150676488876
        vf_explained_var: 0.007339656352996826
        vf_loss: 0.38830795884132385
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 9.786240116227418e-05
        entropy: 0.6618461608886719
        entropy_coeff: 0.0017600000137463212
        kl: 0.005304392427206039
        model: {}
        policy_loss: -0.014215930365025997
        total_loss: -0.014804044738411903
        vf_explained_var: -0.03083828091621399
        vf_loss: 0.4629591703414917
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 9.786240116227418e-05
        entropy: 0.8347718715667725
        entropy_coeff: 0.0017600000137463212
        kl: 0.005358124617487192
        model: {}
        policy_loss: -0.008457249030470848
        total_loss: -0.009643873199820518
        vf_explained_var: 0.011579394340515137
        vf_loss: 0.1466677337884903
    load_time_ms: 13806.271
    num_steps_sampled: 18720000
    num_steps_trained: 18720000
    sample_time_ms: 100764.868
    update_time_ms: 15.691
  iterations_since_restore: 35
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.387845303867394
    ram_util_percent: 21.449723756906074
  pid: 27405
  policy_reward_max:
    agent-0: 11.0
    agent-1: 8.0
    agent-2: 21.0
    agent-3: 13.0
    agent-4: 18.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 2.85
    agent-1: 2.49
    agent-2: 4.39
    agent-3: 3.81
    agent-4: 4.23
    agent-5: 1.77
  policy_reward_min:
    agent-0: -44.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 23.553822991328243
    mean_inference_ms: 13.368079205443005
    mean_processing_ms: 59.54316582958745
  time_since_restore: 4490.404812574387
  time_this_iter_s: 127.28721809387207
  time_total_s: 27000.688055038452
  timestamp: 1637224567
  timesteps_since_restore: 3360000
  timesteps_this_iter: 96000
  timesteps_total: 18720000
  training_iteration: 195
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 39.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    195 |          27000.7 | 18720000 |    19.54 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 1.89
    apples_agent-0_min: 0
    apples_agent-1_max: 73
    apples_agent-1_mean: 1.9
    apples_agent-1_min: 0
    apples_agent-2_max: 24
    apples_agent-2_mean: 1.61
    apples_agent-2_min: 0
    apples_agent-3_max: 17
    apples_agent-3_mean: 2.61
    apples_agent-3_min: 0
    apples_agent-4_max: 82
    apples_agent-4_mean: 1.59
    apples_agent-4_min: 0
    apples_agent-5_max: 15
    apples_agent-5_mean: 1.33
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 200
    cleaning_beam_agent-0_mean: 84.29
    cleaning_beam_agent-0_min: 32
    cleaning_beam_agent-1_max: 366
    cleaning_beam_agent-1_mean: 241.02
    cleaning_beam_agent-1_min: 159
    cleaning_beam_agent-2_max: 66
    cleaning_beam_agent-2_mean: 17.28
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 150
    cleaning_beam_agent-3_mean: 74.57
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 156
    cleaning_beam_agent-4_mean: 68.51
    cleaning_beam_agent-4_min: 20
    cleaning_beam_agent-5_max: 34
    cleaning_beam_agent-5_mean: 12.34
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-38-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 43.0
  episode_reward_mean: 17.88
  episode_reward_min: 4.0
  episodes_this_iter: 96
  episodes_total: 18816
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12179.246
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 9.187200339511037e-05
        entropy: 0.5616984367370605
        entropy_coeff: 0.0017600000137463212
        kl: 0.006122071761637926
        model: {}
        policy_loss: -0.013559816405177116
        total_loss: -0.014216514304280281
        vf_explained_var: -0.04211428761482239
        vf_loss: 0.2578495740890503
      agent-1:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 9.187200339511037e-05
        entropy: 0.4507826864719391
        entropy_coeff: 0.0017600000137463212
        kl: 0.004746998194605112
        model: {}
        policy_loss: -0.008838932029902935
        total_loss: -0.009590121917426586
        vf_explained_var: 0.02735811471939087
        vf_loss: 0.1252032071352005
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 9.187200339511037e-05
        entropy: 0.4970012307167053
        entropy_coeff: 0.0017600000137463212
        kl: 0.005425842013210058
        model: {}
        policy_loss: -0.012371836230158806
        total_loss: -0.01293586939573288
        vf_explained_var: 0.008046060800552368
        vf_loss: 0.3939487636089325
      agent-3:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 9.187200339511037e-05
        entropy: 0.5910433530807495
        entropy_coeff: 0.0017600000137463212
        kl: 0.005087995436042547
        model: {}
        policy_loss: -0.009640182368457317
        total_loss: -0.010518803261220455
        vf_explained_var: 0.011424928903579712
        vf_loss: 0.3441508412361145
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 9.187200339511037e-05
        entropy: 0.6676663160324097
        entropy_coeff: 0.0017600000137463212
        kl: 0.005080375820398331
        model: {}
        policy_loss: -0.012818989343941212
        total_loss: -0.013450907543301582
        vf_explained_var: -0.038459569215774536
        vf_loss: 0.3513812720775604
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 9.187200339511037e-05
        entropy: 0.8381091356277466
        entropy_coeff: 0.0017600000137463212
        kl: 0.005731671117246151
        model: {}
        policy_loss: -0.00955111999064684
        total_loss: -0.010726116597652435
        vf_explained_var: 0.012816444039344788
        vf_loss: 0.1349281519651413
    load_time_ms: 13793.338
    num_steps_sampled: 18816000
    num_steps_trained: 18816000
    sample_time_ms: 100715.933
    update_time_ms: 15.714
  iterations_since_restore: 36
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.59333333333333
    ram_util_percent: 21.622222222222224
  pid: 27405
  policy_reward_max:
    agent-0: 10.0
    agent-1: 7.0
    agent-2: 13.0
    agent-3: 12.0
    agent-4: 13.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 2.94
    agent-1: 1.81
    agent-2: 4.05
    agent-3: 3.7
    agent-4: 3.57
    agent-5: 1.81
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.54325778859998
    mean_inference_ms: 13.366181948746325
    mean_processing_ms: 59.53517506459182
  time_since_restore: 4616.472292661667
  time_this_iter_s: 126.06748008728027
  time_total_s: 27126.755535125732
  timestamp: 1637224694
  timesteps_since_restore: 3456000
  timesteps_this_iter: 96000
  timesteps_total: 18816000
  training_iteration: 196
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 40.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    196 |          27126.8 | 18816000 |    17.88 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 1.96
    apples_agent-0_min: 0
    apples_agent-1_max: 21
    apples_agent-1_mean: 1.25
    apples_agent-1_min: 0
    apples_agent-2_max: 19
    apples_agent-2_mean: 1.95
    apples_agent-2_min: 0
    apples_agent-3_max: 30
    apples_agent-3_mean: 2.91
    apples_agent-3_min: 0
    apples_agent-4_max: 19
    apples_agent-4_mean: 1.15
    apples_agent-4_min: 0
    apples_agent-5_max: 19
    apples_agent-5_mean: 1.29
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 211
    cleaning_beam_agent-0_mean: 85.16
    cleaning_beam_agent-0_min: 35
    cleaning_beam_agent-1_max: 402
    cleaning_beam_agent-1_mean: 250.37
    cleaning_beam_agent-1_min: 165
    cleaning_beam_agent-2_max: 56
    cleaning_beam_agent-2_mean: 15.89
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 154
    cleaning_beam_agent-3_mean: 71.36
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 154
    cleaning_beam_agent-4_mean: 66.87
    cleaning_beam_agent-4_min: 6
    cleaning_beam_agent-5_max: 36
    cleaning_beam_agent-5_mean: 14.07
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-40-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 49.0
  episode_reward_mean: 21.08
  episode_reward_min: -33.0
  episodes_this_iter: 96
  episodes_total: 18912
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12174.819
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 8.588159835198894e-05
        entropy: 0.5608106851577759
        entropy_coeff: 0.0017600000137463212
        kl: 0.006190852262079716
        model: {}
        policy_loss: -0.012779384851455688
        total_loss: -0.013426389545202255
        vf_explained_var: -0.063006192445755
        vf_loss: 0.30481722950935364
      agent-1:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 8.588159835198894e-05
        entropy: 0.45364439487457275
        entropy_coeff: 0.0017600000137463212
        kl: 0.004266681149601936
        model: {}
        policy_loss: -0.008156483061611652
        total_loss: -0.008921781554818153
        vf_explained_var: 0.03571705520153046
        vf_loss: 0.19783727824687958
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 8.588159835198894e-05
        entropy: 0.48547810316085815
        entropy_coeff: 0.0017600000137463212
        kl: 0.004870171658694744
        model: {}
        policy_loss: -0.01181037351489067
        total_loss: -0.012366771697998047
        vf_explained_var: 0.0005139261484146118
        vf_loss: 0.5453526377677917
      agent-3:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 8.588159835198894e-05
        entropy: 0.57686448097229
        entropy_coeff: 0.0017600000137463212
        kl: 0.005071192979812622
        model: {}
        policy_loss: -0.009302567690610886
        total_loss: -0.010149361565709114
        vf_explained_var: 0.004788562655448914
        vf_loss: 0.4170520305633545
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 8.588159835198894e-05
        entropy: 0.6712332367897034
        entropy_coeff: 0.0017600000137463212
        kl: 0.005297038704156876
        model: {}
        policy_loss: -0.013302017003297806
        total_loss: -0.013907087035477161
        vf_explained_var: -0.02895459532737732
        vf_loss: 0.4659769833087921
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 8.588159835198894e-05
        entropy: 0.8247047066688538
        entropy_coeff: 0.0017600000137463212
        kl: 0.0054249526001513
        model: {}
        policy_loss: -0.008807919919490814
        total_loss: -0.009973326697945595
        vf_explained_var: 0.007124736905097961
        vf_loss: 0.14824238419532776
    load_time_ms: 13780.416
    num_steps_sampled: 18912000
    num_steps_trained: 18912000
    sample_time_ms: 100593.335
    update_time_ms: 15.895
  iterations_since_restore: 37
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.563687150837985
    ram_util_percent: 21.737430167597765
  pid: 27405
  policy_reward_max:
    agent-0: 13.0
    agent-1: 11.0
    agent-2: 18.0
    agent-3: 13.0
    agent-4: 15.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 3.52
    agent-1: 2.22
    agent-2: 5.1
    agent-3: 3.75
    agent-4: 4.47
    agent-5: 2.02
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -45.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.530866088024233
    mean_inference_ms: 13.363753674569345
    mean_processing_ms: 59.52629709683223
  time_since_restore: 4742.5803565979
  time_this_iter_s: 126.10806393623352
  time_total_s: 27252.863599061966
  timestamp: 1637224820
  timesteps_since_restore: 3552000
  timesteps_this_iter: 96000
  timesteps_total: 18912000
  training_iteration: 197
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 40.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    197 |          27252.9 | 18912000 |    21.08 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 42
    apples_agent-0_mean: 2.89
    apples_agent-0_min: 0
    apples_agent-1_max: 14
    apples_agent-1_mean: 1.21
    apples_agent-1_min: 0
    apples_agent-2_max: 14
    apples_agent-2_mean: 2.11
    apples_agent-2_min: 0
    apples_agent-3_max: 14
    apples_agent-3_mean: 3.03
    apples_agent-3_min: 0
    apples_agent-4_max: 5
    apples_agent-4_mean: 0.89
    apples_agent-4_min: 0
    apples_agent-5_max: 32
    apples_agent-5_mean: 1.92
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 154
    cleaning_beam_agent-0_mean: 71.66
    cleaning_beam_agent-0_min: 19
    cleaning_beam_agent-1_max: 402
    cleaning_beam_agent-1_mean: 258.36
    cleaning_beam_agent-1_min: 176
    cleaning_beam_agent-2_max: 54
    cleaning_beam_agent-2_mean: 18.05
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 216
    cleaning_beam_agent-3_mean: 78.5
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 180
    cleaning_beam_agent-4_mean: 72.43
    cleaning_beam_agent-4_min: 6
    cleaning_beam_agent-5_max: 41
    cleaning_beam_agent-5_mean: 14.82
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-42-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 41.0
  episode_reward_mean: 20.15
  episode_reward_min: 3.0
  episodes_this_iter: 96
  episodes_total: 19008
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12161.697
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 7.989120058482513e-05
        entropy: 0.5579937100410461
        entropy_coeff: 0.0017600000137463212
        kl: 0.006146800238639116
        model: {}
        policy_loss: -0.013239159248769283
        total_loss: -0.013880390673875809
        vf_explained_var: -0.04486161470413208
        vf_loss: 0.33497318625450134
      agent-1:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 7.989120058482513e-05
        entropy: 0.45187264680862427
        entropy_coeff: 0.0017600000137463212
        kl: 0.004028843715786934
        model: {}
        policy_loss: -0.008435465395450592
        total_loss: -0.009210426360368729
        vf_explained_var: 0.015927940607070923
        vf_loss: 0.14040547609329224
      agent-2:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 7.989120058482513e-05
        entropy: 0.49976325035095215
        entropy_coeff: 0.0017600000137463212
        kl: 0.005580206401646137
        model: {}
        policy_loss: -0.011936526745557785
        total_loss: -0.01263420656323433
        vf_explained_var: 0.009346023201942444
        vf_loss: 0.42398688197135925
      agent-3:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 7.989120058482513e-05
        entropy: 0.5920894145965576
        entropy_coeff: 0.0017600000137463212
        kl: 0.00508513767272234
        model: {}
        policy_loss: -0.009948411025106907
        total_loss: -0.01082959957420826
        vf_explained_var: 0.0024315565824508667
        vf_loss: 0.33757245540618896
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 7.989120058482513e-05
        entropy: 0.6786048412322998
        entropy_coeff: 0.0017600000137463212
        kl: 0.005284765735268593
        model: {}
        policy_loss: -0.013129782862961292
        total_loss: -0.013758578337728977
        vf_explained_var: -0.04394647479057312
        vf_loss: 0.3707258105278015
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 7.989120058482513e-05
        entropy: 0.8025000095367432
        entropy_coeff: 0.0017600000137463212
        kl: 0.005275673232972622
        model: {}
        policy_loss: -0.008554399013519287
        total_loss: -0.00969055574387312
        vf_explained_var: -0.005100280046463013
        vf_loss: 0.12461642920970917
    load_time_ms: 13785.002
    num_steps_sampled: 19008000
    num_steps_trained: 19008000
    sample_time_ms: 100413.177
    update_time_ms: 15.928
  iterations_since_restore: 38
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.78379888268156
    ram_util_percent: 21.877094972067038
  pid: 27405
  policy_reward_max:
    agent-0: 10.0
    agent-1: 8.0
    agent-2: 18.0
    agent-3: 12.0
    agent-4: 15.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 3.87
    agent-1: 1.85
    agent-2: 4.53
    agent-3: 4.06
    agent-4: 3.98
    agent-5: 1.86
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.527644738212206
    mean_inference_ms: 13.362323797093197
    mean_processing_ms: 59.52164831228408
  time_since_restore: 4868.353854894638
  time_this_iter_s: 125.77349829673767
  time_total_s: 27378.637097358704
  timestamp: 1637224946
  timesteps_since_restore: 3648000
  timesteps_this_iter: 96000
  timesteps_total: 19008000
  training_iteration: 198
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 40.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    198 |          27378.6 | 19008000 |    20.15 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 2.43
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.89
    apples_agent-1_min: 0
    apples_agent-2_max: 13
    apples_agent-2_mean: 1.63
    apples_agent-2_min: 0
    apples_agent-3_max: 15
    apples_agent-3_mean: 2.83
    apples_agent-3_min: 0
    apples_agent-4_max: 41
    apples_agent-4_mean: 1.33
    apples_agent-4_min: 0
    apples_agent-5_max: 8
    apples_agent-5_mean: 0.98
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 148
    cleaning_beam_agent-0_mean: 69.27
    cleaning_beam_agent-0_min: 22
    cleaning_beam_agent-1_max: 358
    cleaning_beam_agent-1_mean: 248.59
    cleaning_beam_agent-1_min: 157
    cleaning_beam_agent-2_max: 62
    cleaning_beam_agent-2_mean: 18.77
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 162
    cleaning_beam_agent-3_mean: 78.96
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 181
    cleaning_beam_agent-4_mean: 69.65
    cleaning_beam_agent-4_min: 16
    cleaning_beam_agent-5_max: 47
    cleaning_beam_agent-5_mean: 15.34
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-44-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 70.0
  episode_reward_mean: 19.9
  episode_reward_min: -44.0
  episodes_this_iter: 96
  episodes_total: 19104
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12166.68
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 7.390080281766132e-05
        entropy: 0.5544536113739014
        entropy_coeff: 0.0017600000137463212
        kl: 0.00573620293289423
        model: {}
        policy_loss: -0.011540188454091549
        total_loss: -0.012186424806714058
        vf_explained_var: -0.03267988562583923
        vf_loss: 0.42795005440711975
      agent-1:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 7.390080281766132e-05
        entropy: 0.44398194551467896
        entropy_coeff: 0.0017600000137463212
        kl: 0.004138491116464138
        model: {}
        policy_loss: -0.008508110418915749
        total_loss: -0.009273496456444263
        vf_explained_var: 0.01607714593410492
        vf_loss: 0.12787942588329315
      agent-2:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 7.390080281766132e-05
        entropy: 0.5028201937675476
        entropy_coeff: 0.0017600000137463212
        kl: 0.005231412127614021
        model: {}
        policy_loss: -0.010643426328897476
        total_loss: -0.011352097615599632
        vf_explained_var: 0.0077936649322509766
        vf_loss: 0.4550432562828064
      agent-3:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 7.390080281766132e-05
        entropy: 0.5737128257751465
        entropy_coeff: 0.0017600000137463212
        kl: 0.004717825911939144
        model: {}
        policy_loss: -0.008870930410921574
        total_loss: -0.009715965017676353
        vf_explained_var: 0.0086611807346344
        vf_loss: 0.46753913164138794
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 7.390080281766132e-05
        entropy: 0.6699994802474976
        entropy_coeff: 0.0017600000137463212
        kl: 0.004326033405959606
        model: {}
        policy_loss: -0.0071596200577914715
        total_loss: -0.007739274296909571
        vf_explained_var: -0.0017542839050292969
        vf_loss: 1.66943359375
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 7.390080281766132e-05
        entropy: 0.8205269575119019
        entropy_coeff: 0.0017600000137463212
        kl: 0.005529525224119425
        model: {}
        policy_loss: -0.008171976543962955
        total_loss: -0.009329434484243393
        vf_explained_var: 0.004560843110084534
        vf_loss: 0.1019345074892044
    load_time_ms: 13814.518
    num_steps_sampled: 19104000
    num_steps_trained: 19104000
    sample_time_ms: 100419.534
    update_time_ms: 15.849
  iterations_since_restore: 39
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.771111111111104
    ram_util_percent: 22.052222222222227
  pid: 27405
  policy_reward_max:
    agent-0: 18.0
    agent-1: 6.0
    agent-2: 22.0
    agent-3: 26.0
    agent-4: 12.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 3.98
    agent-1: 2.05
    agent-2: 4.53
    agent-3: 4.12
    agent-4: 3.6
    agent-5: 1.62
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -1.0
    agent-3: 0.0
    agent-4: -48.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.518105035871567
    mean_inference_ms: 13.360802026521055
    mean_processing_ms: 59.519404486488945
  time_since_restore: 4994.782868146896
  time_this_iter_s: 126.4290132522583
  time_total_s: 27505.066110610962
  timestamp: 1637225072
  timesteps_since_restore: 3744000
  timesteps_this_iter: 96000
  timesteps_total: 19104000
  training_iteration: 199
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 41.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    199 |          27505.1 | 19104000 |     19.9 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 14
    apples_agent-0_mean: 1.99
    apples_agent-0_min: 0
    apples_agent-1_max: 20
    apples_agent-1_mean: 1.38
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 2.81
    apples_agent-2_min: 0
    apples_agent-3_max: 16
    apples_agent-3_mean: 2.95
    apples_agent-3_min: 0
    apples_agent-4_max: 10
    apples_agent-4_mean: 1.34
    apples_agent-4_min: 0
    apples_agent-5_max: 13
    apples_agent-5_mean: 1.21
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 168
    cleaning_beam_agent-0_mean: 68.82
    cleaning_beam_agent-0_min: 15
    cleaning_beam_agent-1_max: 352
    cleaning_beam_agent-1_mean: 254.39
    cleaning_beam_agent-1_min: 131
    cleaning_beam_agent-2_max: 65
    cleaning_beam_agent-2_mean: 18.39
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 162
    cleaning_beam_agent-3_mean: 77.41
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 256
    cleaning_beam_agent-4_mean: 77.67
    cleaning_beam_agent-4_min: 18
    cleaning_beam_agent-5_max: 61
    cleaning_beam_agent-5_mean: 17.71
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-46-39
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 61.0
  episode_reward_mean: 21.3
  episode_reward_min: 2.0
  episodes_this_iter: 96
  episodes_total: 19200
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12170.624
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 6.791039777453989e-05
        entropy: 0.5516108870506287
        entropy_coeff: 0.0017600000137463212
        kl: 0.004979138728231192
        model: {}
        policy_loss: -0.011153570376336575
        total_loss: -0.011844011023640633
        vf_explained_var: -0.0400736927986145
        vf_loss: 0.31437742710113525
      agent-1:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 6.791039777453989e-05
        entropy: 0.4420088529586792
        entropy_coeff: 0.0017600000137463212
        kl: 0.0034176416229456663
        model: {}
        policy_loss: -0.007646692916750908
        total_loss: -0.00840663630515337
        vf_explained_var: 0.02884174883365631
        vf_loss: 0.16655749082565308
      agent-2:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 6.791039777453989e-05
        entropy: 0.49304866790771484
        entropy_coeff: 0.0017600000137463212
        kl: 0.0048142364248633385
        model: {}
        policy_loss: -0.011130938306450844
        total_loss: -0.011831626296043396
        vf_explained_var: 0.007340773940086365
        vf_loss: 0.4672014117240906
      agent-3:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 6.791039777453989e-05
        entropy: 0.5878586769104004
        entropy_coeff: 0.0017600000137463212
        kl: 0.004600870423018932
        model: {}
        policy_loss: -0.008594843558967113
        total_loss: -0.009523393586277962
        vf_explained_var: 0.004544943571090698
        vf_loss: 0.485748291015625
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 6.791039777453989e-05
        entropy: 0.6798117756843567
        entropy_coeff: 0.0017600000137463212
        kl: 0.005307873245328665
        model: {}
        policy_loss: -0.012122238986194134
        total_loss: -0.013006490655243397
        vf_explained_var: -0.01973898708820343
        vf_loss: 0.46825525164604187
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 6.791039777453989e-05
        entropy: 0.840925931930542
        entropy_coeff: 0.0017600000137463212
        kl: 0.004992721602320671
        model: {}
        policy_loss: -0.007844529114663601
        total_loss: -0.009061012417078018
        vf_explained_var: 0.008260756731033325
        vf_loss: 0.13909362256526947
    load_time_ms: 13796.458
    num_steps_sampled: 19200000
    num_steps_trained: 19200000
    sample_time_ms: 100431.652
    update_time_ms: 16.036
  iterations_since_restore: 40
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.64088397790055
    ram_util_percent: 22.25911602209945
  pid: 27405
  policy_reward_max:
    agent-0: 10.0
    agent-1: 8.0
    agent-2: 21.0
    agent-3: 15.0
    agent-4: 22.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 3.79
    agent-1: 2.17
    agent-2: 4.69
    agent-3: 4.41
    agent-4: 4.45
    agent-5: 1.79
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.511580734457876
    mean_inference_ms: 13.360053943176686
    mean_processing_ms: 59.51407076994039
  time_since_restore: 5121.718118429184
  time_this_iter_s: 126.9352502822876
  time_total_s: 27632.00136089325
  timestamp: 1637225199
  timesteps_since_restore: 3840000
  timesteps_this_iter: 96000
  timesteps_total: 19200000
  training_iteration: 200
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 41.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    200 |            27632 | 19200000 |     21.3 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 14
    apples_agent-0_mean: 2.47
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.9
    apples_agent-1_min: 0
    apples_agent-2_max: 25
    apples_agent-2_mean: 2.16
    apples_agent-2_min: 0
    apples_agent-3_max: 11
    apples_agent-3_mean: 2.84
    apples_agent-3_min: 0
    apples_agent-4_max: 9
    apples_agent-4_mean: 1.22
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 1.19
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 122
    cleaning_beam_agent-0_mean: 69.44
    cleaning_beam_agent-0_min: 20
    cleaning_beam_agent-1_max: 361
    cleaning_beam_agent-1_mean: 263.02
    cleaning_beam_agent-1_min: 163
    cleaning_beam_agent-2_max: 94
    cleaning_beam_agent-2_mean: 18.74
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 122
    cleaning_beam_agent-3_mean: 61.73
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 182
    cleaning_beam_agent-4_mean: 75.61
    cleaning_beam_agent-4_min: 18
    cleaning_beam_agent-5_max: 61
    cleaning_beam_agent-5_mean: 16.31
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-48-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 60.0
  episode_reward_mean: 21.98
  episode_reward_min: -39.0
  episodes_this_iter: 96
  episodes_total: 19296
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12182.983
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 6.192000000737607e-05
        entropy: 0.5556560754776001
        entropy_coeff: 0.0017600000137463212
        kl: 0.0049872626550495625
        model: {}
        policy_loss: -0.010325733572244644
        total_loss: -0.01114878710359335
        vf_explained_var: -0.04685705900192261
        vf_loss: 0.30221229791641235
      agent-1:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 6.192000000737607e-05
        entropy: 0.44218844175338745
        entropy_coeff: 0.0017600000137463212
        kl: 0.0031449971720576286
        model: {}
        policy_loss: -0.0027724409010261297
        total_loss: -0.0034096790477633476
        vf_explained_var: 0.0017686635255813599
        vf_loss: 1.4040031433105469
      agent-2:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 6.192000000737607e-05
        entropy: 0.49253666400909424
        entropy_coeff: 0.0017600000137463212
        kl: 0.004734296351671219
        model: {}
        policy_loss: -0.010315528139472008
        total_loss: -0.01107026543468237
        vf_explained_var: 0.0022761374711990356
        vf_loss: 0.5294917821884155
      agent-3:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 6.192000000737607e-05
        entropy: 0.5687177181243896
        entropy_coeff: 0.0017600000137463212
        kl: 0.004092238377779722
        model: {}
        policy_loss: -0.008281337097287178
        total_loss: -0.009216765873134136
        vf_explained_var: 0.0028310418128967285
        vf_loss: 0.39938053488731384
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 6.192000000737607e-05
        entropy: 0.6906012892723083
        entropy_coeff: 0.0017600000137463212
        kl: 0.004803189542144537
        model: {}
        policy_loss: -0.011254962533712387
        total_loss: -0.012176429852843285
        vf_explained_var: 0.010672345757484436
        vf_loss: 0.5383055806159973
      agent-5:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 6.192000000737607e-05
        entropy: 0.8361744284629822
        entropy_coeff: 0.0017600000137463212
        kl: 0.004773126915097237
        model: {}
        policy_loss: -0.007842752151191235
        total_loss: -0.009174197912216187
        vf_explained_var: 0.009492143988609314
        vf_loss: 0.2089238464832306
    load_time_ms: 13794.999
    num_steps_sampled: 19296000
    num_steps_trained: 19296000
    sample_time_ms: 100301.464
    update_time_ms: 15.995
  iterations_since_restore: 41
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.82905027932961
    ram_util_percent: 22.324022346368714
  pid: 27405
  policy_reward_max:
    agent-0: 16.0
    agent-1: 8.0
    agent-2: 19.0
    agent-3: 15.0
    agent-4: 16.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 3.66
    agent-1: 1.74
    agent-2: 5.18
    agent-3: 4.31
    agent-4: 4.88
    agent-5: 2.21
  policy_reward_min:
    agent-0: 0.0
    agent-1: -46.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.503093038498363
    mean_inference_ms: 13.358366920243524
    mean_processing_ms: 59.504664576211006
  time_since_restore: 5246.842334985733
  time_this_iter_s: 125.12421655654907
  time_total_s: 27757.1255774498
  timestamp: 1637225325
  timesteps_since_restore: 3936000
  timesteps_this_iter: 96000
  timesteps_total: 19296000
  training_iteration: 201
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 41.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    201 |          27757.1 | 19296000 |    21.98 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 2.67
    apples_agent-0_min: 0
    apples_agent-1_max: 22
    apples_agent-1_mean: 1.44
    apples_agent-1_min: 0
    apples_agent-2_max: 20
    apples_agent-2_mean: 2.17
    apples_agent-2_min: 0
    apples_agent-3_max: 29
    apples_agent-3_mean: 3.08
    apples_agent-3_min: 0
    apples_agent-4_max: 11
    apples_agent-4_mean: 1.13
    apples_agent-4_min: 0
    apples_agent-5_max: 14
    apples_agent-5_mean: 1.51
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 152
    cleaning_beam_agent-0_mean: 66.91
    cleaning_beam_agent-0_min: 15
    cleaning_beam_agent-1_max: 376
    cleaning_beam_agent-1_mean: 263.42
    cleaning_beam_agent-1_min: 72
    cleaning_beam_agent-2_max: 48
    cleaning_beam_agent-2_mean: 16.59
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 158
    cleaning_beam_agent-3_mean: 61.3
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 196
    cleaning_beam_agent-4_mean: 75.39
    cleaning_beam_agent-4_min: 21
    cleaning_beam_agent-5_max: 50
    cleaning_beam_agent-5_mean: 16.83
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-50-52
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 75.0
  episode_reward_mean: 21.94
  episode_reward_min: 5.0
  episodes_this_iter: 96
  episodes_total: 19392
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12194.423
    learner:
      agent-0:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 5.5929598602233455e-05
        entropy: 0.5486962795257568
        entropy_coeff: 0.0017600000137463212
        kl: 0.004998584743589163
        model: {}
        policy_loss: -0.010384684428572655
        total_loss: -0.011250780895352364
        vf_explained_var: -0.0274927020072937
        vf_loss: 0.37130433320999146
      agent-1:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 5.5929598602233455e-05
        entropy: 0.4291505217552185
        entropy_coeff: 0.0017600000137463212
        kl: 0.003914583474397659
        model: {}
        policy_loss: -0.005865003913640976
        total_loss: -0.0066016921773552895
        vf_explained_var: 0.021197408437728882
        vf_loss: 0.1823524683713913
      agent-2:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 5.5929598602233455e-05
        entropy: 0.48527032136917114
        entropy_coeff: 0.0017600000137463212
        kl: 0.004626433365046978
        model: {}
        policy_loss: -0.009805569425225258
        total_loss: -0.010576289147138596
        vf_explained_var: 0.0015296787023544312
        vf_loss: 0.544406533241272
      agent-3:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 5.5929598602233455e-05
        entropy: 0.5771147608757019
        entropy_coeff: 0.0017600000137463212
        kl: 0.003831605426967144
        model: {}
        policy_loss: -0.007013201713562012
        total_loss: -0.00797739066183567
        vf_explained_var: 0.005973681807518005
        vf_loss: 0.3955739140510559
      agent-4:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 5.5929598602233455e-05
        entropy: 0.68340665102005
        entropy_coeff: 0.0017600000137463212
        kl: 0.0042962306179106236
        model: {}
        policy_loss: -0.010332123376429081
        total_loss: -0.011380613781511784
        vf_explained_var: -0.025432318449020386
        vf_loss: 0.4689808785915375
      agent-5:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 5.5929598602233455e-05
        entropy: 0.8441770076751709
        entropy_coeff: 0.0017600000137463212
        kl: 0.004633293021470308
        model: {}
        policy_loss: -0.00740513950586319
        total_loss: -0.008813275024294853
        vf_explained_var: 0.004568204283714294
        vf_loss: 0.1970190405845642
    load_time_ms: 13794.659
    num_steps_sampled: 19392000
    num_steps_trained: 19392000
    sample_time_ms: 100277.101
    update_time_ms: 16.221
  iterations_since_restore: 42
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 48.11611111111111
    ram_util_percent: 22.473333333333336
  pid: 27405
  policy_reward_max:
    agent-0: 19.0
    agent-1: 17.0
    agent-2: 15.0
    agent-3: 20.0
    agent-4: 15.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 4.07
    agent-1: 1.85
    agent-2: 5.18
    agent-3: 4.07
    agent-4: 4.57
    agent-5: 2.2
  policy_reward_min:
    agent-0: 0.0
    agent-1: -1.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.49781646466589
    mean_inference_ms: 13.356290810114333
    mean_processing_ms: 59.4996715737283
  time_since_restore: 5373.102662086487
  time_this_iter_s: 126.26032710075378
  time_total_s: 27883.385904550552
  timestamp: 1637225452
  timesteps_since_restore: 4032000
  timesteps_this_iter: 96000
  timesteps_total: 19392000
  training_iteration: 202
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 41.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    202 |          27883.4 | 19392000 |    21.94 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 2.25
    apples_agent-0_min: 0
    apples_agent-1_max: 17
    apples_agent-1_mean: 1.43
    apples_agent-1_min: 0
    apples_agent-2_max: 16
    apples_agent-2_mean: 1.75
    apples_agent-2_min: 0
    apples_agent-3_max: 27
    apples_agent-3_mean: 3.17
    apples_agent-3_min: 0
    apples_agent-4_max: 72
    apples_agent-4_mean: 2.06
    apples_agent-4_min: 0
    apples_agent-5_max: 59
    apples_agent-5_mean: 1.53
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 214
    cleaning_beam_agent-0_mean: 68.0
    cleaning_beam_agent-0_min: 22
    cleaning_beam_agent-1_max: 391
    cleaning_beam_agent-1_mean: 249.69
    cleaning_beam_agent-1_min: 172
    cleaning_beam_agent-2_max: 65
    cleaning_beam_agent-2_mean: 17.73
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 131
    cleaning_beam_agent-3_mean: 63.32
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 199
    cleaning_beam_agent-4_mean: 80.24
    cleaning_beam_agent-4_min: 19
    cleaning_beam_agent-5_max: 48
    cleaning_beam_agent-5_mean: 16.88
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-52-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 61.0
  episode_reward_mean: 21.08
  episode_reward_min: 3.0
  episodes_this_iter: 96
  episodes_total: 19488
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12198.825
    learner:
      agent-0:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 4.993920083506964e-05
        entropy: 0.5425153970718384
        entropy_coeff: 0.0017600000137463212
        kl: 0.004550856538116932
        model: {}
        policy_loss: -0.009332473389804363
        total_loss: -0.01022671815007925
        vf_explained_var: -0.04231235384941101
        vf_loss: 0.32136762142181396
      agent-1:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 4.993920083506964e-05
        entropy: 0.42357486486434937
        entropy_coeff: 0.0017600000137463212
        kl: 0.003202146152034402
        model: {}
        policy_loss: -0.0067566209472715855
        total_loss: -0.0074854749254882336
        vf_explained_var: 0.028188124299049377
        vf_loss: 0.16481620073318481
      agent-2:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 4.993920083506964e-05
        entropy: 0.49450233578681946
        entropy_coeff: 0.0017600000137463212
        kl: 0.003977132495492697
        model: {}
        policy_loss: -0.008801615796983242
        total_loss: -0.009612073190510273
        vf_explained_var: 0.013227656483650208
        vf_loss: 0.47441214323043823
      agent-3:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 4.993920083506964e-05
        entropy: 0.5857902765274048
        entropy_coeff: 0.0017600000137463212
        kl: 0.0033508928027004004
        model: {}
        policy_loss: -0.006822795607149601
        total_loss: -0.007811693008989096
        vf_explained_var: 0.005577579140663147
        vf_loss: 0.3685813546180725
      agent-4:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 4.993920083506964e-05
        entropy: 0.6837254762649536
        entropy_coeff: 0.0017600000137463212
        kl: 0.004588393960148096
        model: {}
        policy_loss: -0.010339953005313873
        total_loss: -0.011442427523434162
        vf_explained_var: -0.004779800772666931
        vf_loss: 0.4352949857711792
      agent-5:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 4.993920083506964e-05
        entropy: 0.8614514470100403
        entropy_coeff: 0.0017600000137463212
        kl: 0.004633259028196335
        model: {}
        policy_loss: -0.0070946551859378815
        total_loss: -0.0085724126547575
        vf_explained_var: 0.0002399832010269165
        vf_loss: 0.094403475522995
    load_time_ms: 13793.345
    num_steps_sampled: 19488000
    num_steps_trained: 19488000
    sample_time_ms: 100192.419
    update_time_ms: 16.648
  iterations_since_restore: 43
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.69613259668508
    ram_util_percent: 22.655248618784533
  pid: 27405
  policy_reward_max:
    agent-0: 13.0
    agent-1: 7.0
    agent-2: 20.0
    agent-3: 14.0
    agent-4: 14.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 3.65
    agent-1: 2.39
    agent-2: 4.66
    agent-3: 4.06
    agent-4: 4.71
    agent-5: 1.61
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.49164208004239
    mean_inference_ms: 13.35424974877205
    mean_processing_ms: 59.502278032108315
  time_since_restore: 5499.9384570121765
  time_this_iter_s: 126.8357949256897
  time_total_s: 28010.221699476242
  timestamp: 1637225579
  timesteps_since_restore: 4128000
  timesteps_this_iter: 96000
  timesteps_total: 19488000
  training_iteration: 203
  trial_id: '00000'
  
[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
== Status ==
Memory usage on this node: 42.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    203 |          28010.2 | 19488000 |    21.08 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 18
    apples_agent-0_mean: 2.51
    apples_agent-0_min: 0
    apples_agent-1_max: 8
    apples_agent-1_mean: 1.13
    apples_agent-1_min: 0
    apples_agent-2_max: 20
    apples_agent-2_mean: 1.9
    apples_agent-2_min: 0
    apples_agent-3_max: 34
    apples_agent-3_mean: 2.99
    apples_agent-3_min: 0
    apples_agent-4_max: 21
    apples_agent-4_mean: 1.76
    apples_agent-4_min: 0
    apples_agent-5_max: 8
    apples_agent-5_mean: 1.06
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 150
    cleaning_beam_agent-0_mean: 60.21
    cleaning_beam_agent-0_min: 21
    cleaning_beam_agent-1_max: 341
    cleaning_beam_agent-1_mean: 252.12
    cleaning_beam_agent-1_min: 186
    cleaning_beam_agent-2_max: 64
    cleaning_beam_agent-2_mean: 19.46
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 148
    cleaning_beam_agent-3_mean: 60.87
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 240
    cleaning_beam_agent-4_mean: 74.09
    cleaning_beam_agent-4_min: 12
    cleaning_beam_agent-5_max: 44
    cleaning_beam_agent-5_mean: 16.05
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-55-05
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 56.0
  episode_reward_mean: 20.7
  episode_reward_min: -35.0
  episodes_this_iter: 96
  episodes_total: 19584
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12213.559
    learner:
      agent-0:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 4.394879942992702e-05
        entropy: 0.5305634140968323
        entropy_coeff: 0.0017600000137463212
        kl: 0.004341906867921352
        model: {}
        policy_loss: -0.008836647495627403
        total_loss: -0.009725406765937805
        vf_explained_var: -0.04311639070510864
        vf_loss: 0.3146311044692993
      agent-1:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 4.394879942992702e-05
        entropy: 0.4191724359989166
        entropy_coeff: 0.0017600000137463212
        kl: 0.0028389657381922007
        model: {}
        policy_loss: -0.005840870551764965
        total_loss: -0.00656495476141572
        vf_explained_var: 0.028835713863372803
        vf_loss: 0.13590218126773834
      agent-2:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 4.394879942992702e-05
        entropy: 0.5083144903182983
        entropy_coeff: 0.0017600000137463212
        kl: 0.003844825318083167
        model: {}
        policy_loss: -0.008914655074477196
        total_loss: -0.009758830070495605
        vf_explained_var: 0.005281805992126465
        vf_loss: 0.4444889724254608
      agent-3:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 4.394879942992702e-05
        entropy: 0.5965482592582703
        entropy_coeff: 0.0017600000137463212
        kl: 0.003400888992473483
        model: {}
        policy_loss: -0.0063543119467794895
        total_loss: -0.0073576127178967
        vf_explained_var: 0.002572089433670044
        vf_loss: 0.43964719772338867
      agent-4:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 4.394879942992702e-05
        entropy: 0.6739437580108643
        entropy_coeff: 0.0017600000137463212
        kl: 0.004308108240365982
        model: {}
        policy_loss: -0.009551143273711205
        total_loss: -0.01066567562520504
        vf_explained_var: -0.0073176175355911255
        vf_loss: 0.4468308091163635
      agent-5:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 4.394879942992702e-05
        entropy: 0.8646699786186218
        entropy_coeff: 0.0017600000137463212
        kl: 0.004068452399224043
        model: {}
        policy_loss: -0.006432871334254742
        total_loss: -0.007928215898573399
        vf_explained_var: 0.009791448712348938
        vf_loss: 0.13763542473316193
    load_time_ms: 13805.073
    num_steps_sampled: 19584000
    num_steps_trained: 19584000
    sample_time_ms: 100179.346
    update_time_ms: 16.582
  iterations_since_restore: 44
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.84166666666667
    ram_util_percent: 22.817222222222224
  pid: 27405
  policy_reward_max:
    agent-0: 12.0
    agent-1: 9.0
    agent-2: 14.0
    agent-3: 15.0
    agent-4: 18.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 3.74
    agent-1: 2.08
    agent-2: 4.87
    agent-3: 3.79
    agent-4: 4.39
    agent-5: 1.83
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -44.0
    agent-4: -48.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.48088707249056
    mean_inference_ms: 13.351657543043036
    mean_processing_ms: 59.4947740391853
  time_since_restore: 5626.219066619873
  time_this_iter_s: 126.28060960769653
  time_total_s: 28136.50230908394
  timestamp: 1637225705
  timesteps_since_restore: 4224000
  timesteps_this_iter: 96000
  timesteps_total: 19584000
  training_iteration: 204
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 42.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    204 |          28136.5 | 19584000 |     20.7 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 82
    apples_agent-0_mean: 3.18
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 1.19
    apples_agent-1_min: 0
    apples_agent-2_max: 12
    apples_agent-2_mean: 2.11
    apples_agent-2_min: 0
    apples_agent-3_max: 16
    apples_agent-3_mean: 3.11
    apples_agent-3_min: 0
    apples_agent-4_max: 24
    apples_agent-4_mean: 1.42
    apples_agent-4_min: 0
    apples_agent-5_max: 9
    apples_agent-5_mean: 1.21
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 148
    cleaning_beam_agent-0_mean: 62.82
    cleaning_beam_agent-0_min: 17
    cleaning_beam_agent-1_max: 471
    cleaning_beam_agent-1_mean: 249.53
    cleaning_beam_agent-1_min: 126
    cleaning_beam_agent-2_max: 67
    cleaning_beam_agent-2_mean: 22.72
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 138
    cleaning_beam_agent-3_mean: 58.07
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 235
    cleaning_beam_agent-4_mean: 80.66
    cleaning_beam_agent-4_min: 18
    cleaning_beam_agent-5_max: 47
    cleaning_beam_agent-5_mean: 14.04
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-57-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 82.0
  episode_reward_mean: 23.3
  episode_reward_min: 0.0
  episodes_this_iter: 96
  episodes_total: 19680
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12208.294
    learner:
      agent-0:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 3.795840166276321e-05
        entropy: 0.5427613258361816
        entropy_coeff: 0.0017600000137463212
        kl: 0.00358681776560843
        model: {}
        policy_loss: -0.008263222873210907
        total_loss: -0.0091776791960001
        vf_explained_var: -0.021686404943466187
        vf_loss: 0.3519923686981201
      agent-1:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 3.795840166276321e-05
        entropy: 0.4164232611656189
        entropy_coeff: 0.0017600000137463212
        kl: 0.0026865312829613686
        model: {}
        policy_loss: -0.005412278231233358
        total_loss: -0.006126630585640669
        vf_explained_var: 0.027606964111328125
        vf_loss: 0.18523025512695312
      agent-2:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 3.795840166276321e-05
        entropy: 0.49699637293815613
        entropy_coeff: 0.0017600000137463212
        kl: 0.0033805961720645428
        model: {}
        policy_loss: -0.007534782402217388
        total_loss: -0.008342893794178963
        vf_explained_var: 0.009485051035881042
        vf_loss: 0.6396202445030212
      agent-3:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 3.795840166276321e-05
        entropy: 0.5932321548461914
        entropy_coeff: 0.0017600000137463212
        kl: 0.0029195400420576334
        model: {}
        policy_loss: -0.006336310412734747
        total_loss: -0.007336035370826721
        vf_explained_var: -1.895427703857422e-05
        vf_loss: 0.4322437345981598
      agent-4:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 3.795840166276321e-05
        entropy: 0.6826469898223877
        entropy_coeff: 0.0017600000137463212
        kl: 0.00368166109547019
        model: {}
        policy_loss: -0.008869352750480175
        total_loss: -0.010009882040321827
        vf_explained_var: 0.009894207119941711
        vf_loss: 0.49426543712615967
      agent-5:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 3.795840166276321e-05
        entropy: 0.8470549583435059
        entropy_coeff: 0.0017600000137463212
        kl: 0.004060813691467047
        model: {}
        policy_loss: -0.006534984335303307
        total_loss: -0.008003978058695793
        vf_explained_var: 0.012761667370796204
        vf_loss: 0.1548178493976593
    load_time_ms: 13800.975
    num_steps_sampled: 19680000
    num_steps_trained: 19680000
    sample_time_ms: 100173.863
    update_time_ms: 16.812
  iterations_since_restore: 45
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.904419889502755
    ram_util_percent: 22.921546961325962
  pid: 27405
  policy_reward_max:
    agent-0: 18.0
    agent-1: 10.0
    agent-2: 23.0
    agent-3: 17.0
    agent-4: 19.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 3.94
    agent-1: 2.51
    agent-2: 5.5
    agent-3: 4.37
    agent-4: 5.02
    agent-5: 1.96
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.478553097634695
    mean_inference_ms: 13.349859788050539
    mean_processing_ms: 59.50170252672931
  time_since_restore: 5753.330366849899
  time_this_iter_s: 127.11130023002625
  time_total_s: 28263.613609313965
  timestamp: 1637225832
  timesteps_since_restore: 4320000
  timesteps_this_iter: 96000
  timesteps_total: 19680000
  training_iteration: 205
  trial_id: '00000'
  
[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
== Status ==
Memory usage on this node: 42.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    205 |          28263.6 | 19680000 |     23.3 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 2.79
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 1.22
    apples_agent-1_min: 0
    apples_agent-2_max: 15
    apples_agent-2_mean: 2.21
    apples_agent-2_min: 0
    apples_agent-3_max: 27
    apples_agent-3_mean: 2.74
    apples_agent-3_min: 0
    apples_agent-4_max: 13
    apples_agent-4_mean: 1.14
    apples_agent-4_min: 0
    apples_agent-5_max: 73
    apples_agent-5_mean: 1.72
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 165
    cleaning_beam_agent-0_mean: 63.98
    cleaning_beam_agent-0_min: 16
    cleaning_beam_agent-1_max: 329
    cleaning_beam_agent-1_mean: 243.57
    cleaning_beam_agent-1_min: 169
    cleaning_beam_agent-2_max: 61
    cleaning_beam_agent-2_mean: 21.0
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 149
    cleaning_beam_agent-3_mean: 64.35
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 207
    cleaning_beam_agent-4_mean: 83.75
    cleaning_beam_agent-4_min: 28
    cleaning_beam_agent-5_max: 37
    cleaning_beam_agent-5_mean: 12.24
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-59-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 62.0
  episode_reward_mean: 22.09
  episode_reward_min: -37.0
  episodes_this_iter: 96
  episodes_total: 19776
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12211.195
    learner:
      agent-0:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 3.196800025762059e-05
        entropy: 0.5385051369667053
        entropy_coeff: 0.0017600000137463212
        kl: 0.0031355356331914663
        model: {}
        policy_loss: -0.006989855319261551
        total_loss: -0.007886532694101334
        vf_explained_var: -0.022259414196014404
        vf_loss: 0.4864204525947571
      agent-1:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 3.196800025762059e-05
        entropy: 0.41446059942245483
        entropy_coeff: 0.0017600000137463212
        kl: 0.002146243816241622
        model: {}
        policy_loss: -0.004824906587600708
        total_loss: -0.005536443088203669
        vf_explained_var: 0.027870535850524902
        vf_loss: 0.1790277510881424
      agent-2:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 3.196800025762059e-05
        entropy: 0.5067477226257324
        entropy_coeff: 0.0017600000137463212
        kl: 0.0032895184122025967
        model: {}
        policy_loss: -0.007485735230147839
        total_loss: -0.008316611871123314
        vf_explained_var: -0.0011967867612838745
        vf_loss: 0.5971271991729736
      agent-3:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 3.196800025762059e-05
        entropy: 0.5970954298973083
        entropy_coeff: 0.0017600000137463212
        kl: 0.0027935386169701815
        model: {}
        policy_loss: -0.005374818108975887
        total_loss: -0.006387536879628897
        vf_explained_var: 0.0004917383193969727
        vf_loss: 0.3762720227241516
      agent-4:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 3.196800025762059e-05
        entropy: 0.6842018961906433
        entropy_coeff: 0.0017600000137463212
        kl: 0.003590691601857543
        model: {}
        policy_loss: -0.008457515388727188
        total_loss: -0.009606766514480114
        vf_explained_var: -0.008318692445755005
        vf_loss: 0.49335265159606934
      agent-5:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 3.196800025762059e-05
        entropy: 0.8685529828071594
        entropy_coeff: 0.0017600000137463212
        kl: 0.004287706688046455
        model: {}
        policy_loss: -0.004497009329497814
        total_loss: -0.005884082056581974
        vf_explained_var: 0.005153357982635498
        vf_loss: 1.3822897672653198
    load_time_ms: 13846.398
    num_steps_sampled: 19776000
    num_steps_trained: 19776000
    sample_time_ms: 100150.365
    update_time_ms: 16.864
  iterations_since_restore: 46
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.81555555555555
    ram_util_percent: 23.094444444444445
  pid: 27405
  policy_reward_max:
    agent-0: 15.0
    agent-1: 11.0
    agent-2: 19.0
    agent-3: 13.0
    agent-4: 18.0
    agent-5: 6.0
  policy_reward_mean:
    agent-0: 4.54
    agent-1: 2.41
    agent-2: 5.22
    agent-3: 4.03
    agent-4: 5.01
    agent-5: 0.88
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -1.0
    agent-4: 0.0
    agent-5: -50.0
  sampler_perf:
    mean_env_wait_ms: 23.472469048149865
    mean_inference_ms: 13.349387415216679
    mean_processing_ms: 59.502598680246194
  time_since_restore: 5879.636579751968
  time_this_iter_s: 126.30621290206909
  time_total_s: 28389.919822216034
  timestamp: 1637225959
  timesteps_since_restore: 4416000
  timesteps_this_iter: 96000
  timesteps_total: 19776000
  training_iteration: 206
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 42.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    206 |          28389.9 | 19776000 |    22.09 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 2.29
    apples_agent-0_min: 0
    apples_agent-1_max: 18
    apples_agent-1_mean: 1.29
    apples_agent-1_min: 0
    apples_agent-2_max: 13
    apples_agent-2_mean: 2.16
    apples_agent-2_min: 0
    apples_agent-3_max: 18
    apples_agent-3_mean: 3.02
    apples_agent-3_min: 0
    apples_agent-4_max: 13
    apples_agent-4_mean: 1.52
    apples_agent-4_min: 0
    apples_agent-5_max: 26
    apples_agent-5_mean: 1.76
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 118
    cleaning_beam_agent-0_mean: 61.61
    cleaning_beam_agent-0_min: 28
    cleaning_beam_agent-1_max: 349
    cleaning_beam_agent-1_mean: 251.19
    cleaning_beam_agent-1_min: 177
    cleaning_beam_agent-2_max: 97
    cleaning_beam_agent-2_mean: 21.04
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 117
    cleaning_beam_agent-3_mean: 51.55
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 162
    cleaning_beam_agent-4_mean: 73.18
    cleaning_beam_agent-4_min: 20
    cleaning_beam_agent-5_max: 35
    cleaning_beam_agent-5_mean: 14.01
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-01-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 67.0
  episode_reward_mean: 22.4
  episode_reward_min: -37.0
  episodes_this_iter: 96
  episodes_total: 19872
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12204.114
    learner:
      agent-0:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 2.597760067146737e-05
        entropy: 0.5281818509101868
        entropy_coeff: 0.0017600000137463212
        kl: 0.0028517749160528183
        model: {}
        policy_loss: -0.005951248575001955
        total_loss: -0.0068419091403484344
        vf_explained_var: -0.032021790742874146
        vf_loss: 0.37828579545021057
      agent-1:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 2.597760067146737e-05
        entropy: 0.41840434074401855
        entropy_coeff: 0.0017600000137463212
        kl: 0.001719520310871303
        model: {}
        policy_loss: -0.0038906456902623177
        total_loss: -0.004599898587912321
        vf_explained_var: 0.02176453173160553
        vf_loss: 0.2713291347026825
      agent-2:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 2.597760067146737e-05
        entropy: 0.4816555678844452
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024267907720059156
        model: {}
        policy_loss: -0.006274037063121796
        total_loss: -0.007069502491503954
        vf_explained_var: 0.006113708019256592
        vf_loss: 0.5177413821220398
      agent-3:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 2.597760067146737e-05
        entropy: 0.6045358777046204
        entropy_coeff: 0.0017600000137463212
        kl: 0.0029399418272078037
        model: {}
        policy_loss: -0.005098953377455473
        total_loss: -0.00612239446491003
        vf_explained_var: 0.0022179782390594482
        vf_loss: 0.40255698561668396
      agent-4:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 2.597760067146737e-05
        entropy: 0.6773498058319092
        entropy_coeff: 0.0017600000137463212
        kl: 0.0029297289438545704
        model: {}
        policy_loss: -0.006820503156632185
        total_loss: -0.00796114094555378
        vf_explained_var: -0.006219938397407532
        vf_loss: 0.4920867681503296
      agent-5:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 2.597760067146737e-05
        entropy: 0.8972085118293762
        entropy_coeff: 0.0017600000137463212
        kl: 0.003111504251137376
        model: {}
        policy_loss: -0.004825836978852749
        total_loss: -0.006385348737239838
        vf_explained_var: 0.0005556493997573853
        vf_loss: 0.18361413478851318
    load_time_ms: 13865.78
    num_steps_sampled: 19872000
    num_steps_trained: 19872000
    sample_time_ms: 100246.65
    update_time_ms: 16.765
  iterations_since_restore: 47
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.803296703296695
    ram_util_percent: 23.171428571428564
  pid: 27405
  policy_reward_max:
    agent-0: 14.0
    agent-1: 16.0
    agent-2: 20.0
    agent-3: 19.0
    agent-4: 15.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 4.0
    agent-1: 2.45
    agent-2: 5.24
    agent-3: 4.18
    agent-4: 4.62
    agent-5: 1.91
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -46.0
    agent-5: -50.0
  sampler_perf:
    mean_env_wait_ms: 23.465987740868353
    mean_inference_ms: 13.35091264016024
    mean_processing_ms: 59.50569073092607
  time_since_restore: 6006.853282928467
  time_this_iter_s: 127.21670317649841
  time_total_s: 28517.136525392532
  timestamp: 1637226086
  timesteps_since_restore: 4512000
  timesteps_this_iter: 96000
  timesteps_total: 19872000
  training_iteration: 207
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 43.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    207 |          28517.1 | 19872000 |     22.4 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 2.77
    apples_agent-0_min: 0
    apples_agent-1_max: 28
    apples_agent-1_mean: 1.22
    apples_agent-1_min: 0
    apples_agent-2_max: 26
    apples_agent-2_mean: 2.19
    apples_agent-2_min: 0
    apples_agent-3_max: 24
    apples_agent-3_mean: 3.58
    apples_agent-3_min: 0
    apples_agent-4_max: 116
    apples_agent-4_mean: 2.83
    apples_agent-4_min: 0
    apples_agent-5_max: 13
    apples_agent-5_mean: 1.3
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 156
    cleaning_beam_agent-0_mean: 59.91
    cleaning_beam_agent-0_min: 16
    cleaning_beam_agent-1_max: 344
    cleaning_beam_agent-1_mean: 249.32
    cleaning_beam_agent-1_min: 167
    cleaning_beam_agent-2_max: 79
    cleaning_beam_agent-2_mean: 20.89
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 161
    cleaning_beam_agent-3_mean: 58.17
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 146
    cleaning_beam_agent-4_mean: 69.37
    cleaning_beam_agent-4_min: 24
    cleaning_beam_agent-5_max: 49
    cleaning_beam_agent-5_mean: 15.55
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-03-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 51.0
  episode_reward_mean: 21.15
  episode_reward_min: -85.0
  episodes_this_iter: 96
  episodes_total: 19968
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12204.984
    learner:
      agent-0:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.998719926632475e-05
        entropy: 0.5340867638587952
        entropy_coeff: 0.0017600000137463212
        kl: 0.0025055096484720707
        model: {}
        policy_loss: -0.005605433136224747
        total_loss: -0.006507541984319687
        vf_explained_var: -0.021955221891403198
        vf_loss: 0.37397336959838867
      agent-1:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.998719926632475e-05
        entropy: 0.41388946771621704
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015361886471509933
        model: {}
        policy_loss: -0.0036120598670095205
        total_loss: -0.004324224777519703
        vf_explained_var: 0.01588064432144165
        vf_loss: 0.1628122180700302
      agent-2:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.998719926632475e-05
        entropy: 0.4809366762638092
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020532167982310057
        model: {}
        policy_loss: -0.003459664061665535
        total_loss: -0.004129183478653431
        vf_explained_var: 0.0013923197984695435
        vf_loss: 1.76729154586792
      agent-3:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.998719926632475e-05
        entropy: 0.608691394329071
        entropy_coeff: 0.0017600000137463212
        kl: 0.002754785818979144
        model: {}
        policy_loss: -0.004369327332824469
        total_loss: -0.005397571250796318
        vf_explained_var: 0.0012623071670532227
        vf_loss: 0.4291647970676422
      agent-4:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 1.998719926632475e-05
        entropy: 0.6698612570762634
        entropy_coeff: 0.0017600000137463212
        kl: 0.002940103877335787
        model: {}
        policy_loss: -0.004538688808679581
        total_loss: -0.005578737705945969
        vf_explained_var: -0.009444981813430786
        vf_loss: 1.377587914466858
      agent-5:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.998719926632475e-05
        entropy: 0.888963520526886
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023880386725068092
        model: {}
        policy_loss: -0.0023050555028021336
        total_loss: -0.0037647616118192673
        vf_explained_var: 0.00015169382095336914
        vf_loss: 1.0440125465393066
    load_time_ms: 13859.618
    num_steps_sampled: 19968000
    num_steps_trained: 19968000
    sample_time_ms: 100279.011
    update_time_ms: 16.78
  iterations_since_restore: 48
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.87150837988827
    ram_util_percent: 23.2949720670391
  pid: 27405
  policy_reward_max:
    agent-0: 14.0
    agent-1: 8.0
    agent-2: 16.0
    agent-3: 15.0
    agent-4: 17.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 4.17
    agent-1: 2.07
    agent-2: 4.23
    agent-3: 4.57
    agent-4: 4.51
    agent-5: 1.6
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -48.0
    agent-3: 0.0
    agent-4: -45.0
    agent-5: -48.0
  sampler_perf:
    mean_env_wait_ms: 23.45617847655779
    mean_inference_ms: 13.350166049435458
    mean_processing_ms: 59.499510586404796
  time_since_restore: 6132.883354187012
  time_this_iter_s: 126.03007125854492
  time_total_s: 28643.166596651077
  timestamp: 1637226212
  timesteps_since_restore: 4608000
  timesteps_this_iter: 96000
  timesteps_total: 19968000
  training_iteration: 208
  trial_id: '00000'
  
[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
== Status ==
Memory usage on this node: 43.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    208 |          28643.2 | 19968000 |    21.15 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 38
    apples_agent-0_mean: 3.25
    apples_agent-0_min: 0
    apples_agent-1_max: 28
    apples_agent-1_mean: 1.43
    apples_agent-1_min: 0
    apples_agent-2_max: 16
    apples_agent-2_mean: 2.11
    apples_agent-2_min: 0
    apples_agent-3_max: 16
    apples_agent-3_mean: 3.6
    apples_agent-3_min: 0
    apples_agent-4_max: 20
    apples_agent-4_mean: 1.15
    apples_agent-4_min: 0
    apples_agent-5_max: 12
    apples_agent-5_mean: 1.48
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 150
    cleaning_beam_agent-0_mean: 60.25
    cleaning_beam_agent-0_min: 18
    cleaning_beam_agent-1_max: 341
    cleaning_beam_agent-1_mean: 254.06
    cleaning_beam_agent-1_min: 159
    cleaning_beam_agent-2_max: 87
    cleaning_beam_agent-2_mean: 19.08
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 129
    cleaning_beam_agent-3_mean: 54.71
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 187
    cleaning_beam_agent-4_mean: 71.0
    cleaning_beam_agent-4_min: 24
    cleaning_beam_agent-5_max: 47
    cleaning_beam_agent-5_mean: 17.05
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-05-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 61.0
  episode_reward_mean: 23.79
  episode_reward_min: 1.0
  episodes_this_iter: 96
  episodes_total: 20064
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12194.625
    learner:
      agent-0:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.3996799680171534e-05
        entropy: 0.5270545482635498
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014836182817816734
        model: {}
        policy_loss: -0.00421646423637867
        total_loss: -0.005104979500174522
        vf_explained_var: -0.024404823780059814
        vf_loss: 0.3895722031593323
      agent-1:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.3996799680171534e-05
        entropy: 0.40669113397598267
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012250954750925303
        model: {}
        policy_loss: -0.0030661518685519695
        total_loss: -0.0037662514951080084
        vf_explained_var: 0.01708616316318512
        vf_loss: 0.15679024159908295
      agent-2:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.3996799680171534e-05
        entropy: 0.4822559952735901
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019398524891585112
        model: {}
        policy_loss: -0.004371085669845343
        total_loss: -0.0051692514680325985
        vf_explained_var: 0.00974532961845398
        vf_loss: 0.505146324634552
      agent-3:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.3996799680171534e-05
        entropy: 0.6074275970458984
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015736587811261415
        model: {}
        policy_loss: -0.003251225221902132
        total_loss: -0.004273388534784317
        vf_explained_var: 0.002062752842903137
        vf_loss: 0.4687628448009491
      agent-4:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.3996799680171534e-05
        entropy: 0.6745195388793945
        entropy_coeff: 0.0017600000137463212
        kl: 0.001738755265250802
        model: {}
        policy_loss: -0.0047989096492528915
        total_loss: -0.005943451076745987
        vf_explained_var: 0.014487400650978088
        vf_loss: 0.42276686429977417
      agent-5:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.3996799680171534e-05
        entropy: 0.9132668972015381
        entropy_coeff: 0.0017600000137463212
        kl: 0.0028483730275183916
        model: {}
        policy_loss: -0.0038257199339568615
        total_loss: -0.005413138307631016
        vf_explained_var: -0.0010334104299545288
        vf_loss: 0.19656094908714294
    load_time_ms: 13831.37
    num_steps_sampled: 20064000
    num_steps_trained: 20064000
    sample_time_ms: 100185.196
    update_time_ms: 16.687
  iterations_since_restore: 49
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.844134078212285
    ram_util_percent: 23.406145251396648
  pid: 27405
  policy_reward_max:
    agent-0: 13.0
    agent-1: 8.0
    agent-2: 16.0
    agent-3: 20.0
    agent-4: 15.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 4.16
    agent-1: 2.07
    agent-2: 5.41
    agent-3: 4.68
    agent-4: 5.03
    agent-5: 2.44
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.44677129468696
    mean_inference_ms: 13.347341805729759
    mean_processing_ms: 59.48819464728665
  time_since_restore: 6257.949707746506
  time_this_iter_s: 125.06635355949402
  time_total_s: 28768.23295021057
  timestamp: 1637226337
  timesteps_since_restore: 4704000
  timesteps_this_iter: 96000
  timesteps_total: 20064000
  training_iteration: 209
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 43.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    209 |          28768.2 | 20064000 |    23.79 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 22
    apples_agent-0_mean: 2.75
    apples_agent-0_min: 0
    apples_agent-1_max: 18
    apples_agent-1_mean: 1.3
    apples_agent-1_min: 0
    apples_agent-2_max: 15
    apples_agent-2_mean: 2.22
    apples_agent-2_min: 0
    apples_agent-3_max: 38
    apples_agent-3_mean: 3.47
    apples_agent-3_min: 0
    apples_agent-4_max: 9
    apples_agent-4_mean: 1.16
    apples_agent-4_min: 0
    apples_agent-5_max: 12
    apples_agent-5_mean: 1.29
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 187
    cleaning_beam_agent-0_mean: 60.13
    cleaning_beam_agent-0_min: 15
    cleaning_beam_agent-1_max: 341
    cleaning_beam_agent-1_mean: 264.79
    cleaning_beam_agent-1_min: 175
    cleaning_beam_agent-2_max: 69
    cleaning_beam_agent-2_mean: 22.16
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 153
    cleaning_beam_agent-3_mean: 62.81
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 291
    cleaning_beam_agent-4_mean: 81.78
    cleaning_beam_agent-4_min: 25
    cleaning_beam_agent-5_max: 54
    cleaning_beam_agent-5_mean: 16.58
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-07-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 54.0
  episode_reward_mean: 21.67
  episode_reward_min: -47.0
  episodes_this_iter: 96
  episodes_total: 20160
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12179.713
    learner:
      agent-0:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5182878375053406
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012896305415779352
        model: {}
        policy_loss: -0.0035482128150761127
        total_loss: -0.004429584834724665
        vf_explained_var: -0.051576048135757446
        vf_loss: 0.30752450227737427
      agent-1:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4277673065662384
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015995303401723504
        model: {}
        policy_loss: -0.002731907879933715
        total_loss: -0.003462802618741989
        vf_explained_var: 0.010107696056365967
        vf_loss: 0.2197517305612564
      agent-2:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4898621439933777
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014481752878054976
        model: {}
        policy_loss: -0.002507380675524473
        total_loss: -0.0031882664188742638
        vf_explained_var: 0.003558233380317688
        vf_loss: 1.8123652935028076
      agent-3:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6091604232788086
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011359318159520626
        model: {}
        policy_loss: -0.0017596249235793948
        total_loss: -0.0026629730127751827
        vf_explained_var: -0.004022359848022461
        vf_loss: 1.6876248121261597
      agent-4:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6714015007019043
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017644948093220592
        model: {}
        policy_loss: -0.002853820100426674
        total_loss: -0.0038630831986665726
        vf_explained_var: 0.013675585389137268
        vf_loss: 1.7223355770111084
      agent-5:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9248237609863281
        entropy_coeff: 0.0017600000137463212
        kl: 0.002187625039368868
        model: {}
        policy_loss: -0.0032636160030961037
        total_loss: -0.0048723770305514336
        vf_explained_var: 0.008946731686592102
        vf_loss: 0.18823885917663574
    load_time_ms: 13832.665
    num_steps_sampled: 20160000
    num_steps_trained: 20160000
    sample_time_ms: 100041.621
    update_time_ms: 16.807
  iterations_since_restore: 50
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.69213483146067
    ram_util_percent: 23.528089887640448
  pid: 27405
  policy_reward_max:
    agent-0: 11.0
    agent-1: 15.0
    agent-2: 19.0
    agent-3: 16.0
    agent-4: 16.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 3.97
    agent-1: 2.31
    agent-2: 5.01
    agent-3: 3.83
    agent-4: 4.14
    agent-5: 2.41
  policy_reward_min:
    agent-0: -1.0
    agent-1: 0.0
    agent-2: -44.0
    agent-3: -48.0
    agent-4: -49.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.439041118528657
    mean_inference_ms: 13.346176200504956
    mean_processing_ms: 59.47841142131961
  time_since_restore: 6383.321053028107
  time_this_iter_s: 125.37134528160095
  time_total_s: 28893.604295492172
  timestamp: 1637226463
  timesteps_since_restore: 4800000
  timesteps_this_iter: 96000
  timesteps_total: 20160000
  training_iteration: 210
  trial_id: '00000'
  
[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
== Status ==
Memory usage on this node: 43.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    210 |          28893.6 | 20160000 |    21.67 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 40
    apples_agent-0_mean: 3.18
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 1.13
    apples_agent-1_min: 0
    apples_agent-2_max: 22
    apples_agent-2_mean: 2.08
    apples_agent-2_min: 0
    apples_agent-3_max: 12
    apples_agent-3_mean: 2.83
    apples_agent-3_min: 0
    apples_agent-4_max: 23
    apples_agent-4_mean: 1.5
    apples_agent-4_min: 0
    apples_agent-5_max: 9
    apples_agent-5_mean: 1.33
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 142
    cleaning_beam_agent-0_mean: 60.85
    cleaning_beam_agent-0_min: 16
    cleaning_beam_agent-1_max: 360
    cleaning_beam_agent-1_mean: 250.33
    cleaning_beam_agent-1_min: 172
    cleaning_beam_agent-2_max: 71
    cleaning_beam_agent-2_mean: 21.69
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 151
    cleaning_beam_agent-3_mean: 64.12
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 268
    cleaning_beam_agent-4_mean: 69.77
    cleaning_beam_agent-4_min: 15
    cleaning_beam_agent-5_max: 49
    cleaning_beam_agent-5_mean: 17.93
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-09-50
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 76.0
  episode_reward_mean: 23.13
  episode_reward_min: -34.0
  episodes_this_iter: 96
  episodes_total: 20256
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12208.581
    learner:
      agent-0:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5359694361686707
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016018543392419815
        model: {}
        policy_loss: -0.0040822504088282585
        total_loss: -0.0049797105602920055
        vf_explained_var: -0.013041466474533081
        vf_loss: 0.4580659866333008
      agent-1:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41677403450012207
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006528618978336453
        model: {}
        policy_loss: -0.001720840111374855
        total_loss: -0.0023045488633215427
        vf_explained_var: 0.0033436566591262817
        vf_loss: 1.498145341873169
      agent-2:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4790601134300232
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016151875024661422
        model: {}
        policy_loss: -0.004125204868614674
        total_loss: -0.004916980396956205
        vf_explained_var: 0.0038753747940063477
        vf_loss: 0.5135765075683594
      agent-3:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6062982678413391
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011269757524132729
        model: {}
        policy_loss: -0.0027330508455634117
        total_loss: -0.0037555699236691
        vf_explained_var: 0.0032158195972442627
        vf_loss: 0.4456034302711487
      agent-4:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6801334023475647
        entropy_coeff: 0.0017600000137463212
        kl: 0.002090062014758587
        model: {}
        policy_loss: -0.004001948516815901
        total_loss: -0.00513851223513484
        vf_explained_var: -0.003935664892196655
        vf_loss: 0.6037206649780273
      agent-5:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9349299669265747
        entropy_coeff: 0.0017600000137463212
        kl: 0.001924595795571804
        model: {}
        policy_loss: -0.003107826691120863
        total_loss: -0.004735366441309452
        vf_explained_var: 0.0031753331422805786
        vf_loss: 0.17894895374774933
    load_time_ms: 13843.841
    num_steps_sampled: 20256000
    num_steps_trained: 20256000
    sample_time_ms: 100170.359
    update_time_ms: 16.974
  iterations_since_restore: 51
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.470165745856356
    ram_util_percent: 23.6475138121547
  pid: 27405
  policy_reward_max:
    agent-0: 23.0
    agent-1: 9.0
    agent-2: 16.0
    agent-3: 22.0
    agent-4: 30.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 3.84
    agent-1: 1.89
    agent-2: 5.51
    agent-3: 4.08
    agent-4: 5.3
    agent-5: 2.51
  policy_reward_min:
    agent-0: 0.0
    agent-1: -50.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.432913537643195
    mean_inference_ms: 13.344932609339407
    mean_processing_ms: 59.47542379007362
  time_since_restore: 6510.176173686981
  time_this_iter_s: 126.85512065887451
  time_total_s: 29020.459416151047
  timestamp: 1637226590
  timesteps_since_restore: 4896000
  timesteps_this_iter: 96000
  timesteps_total: 20256000
  training_iteration: 211
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 44.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    211 |          29020.5 | 20256000 |    23.13 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 3.18
    apples_agent-0_min: 0
    apples_agent-1_max: 8
    apples_agent-1_mean: 1.09
    apples_agent-1_min: 0
    apples_agent-2_max: 8
    apples_agent-2_mean: 2.12
    apples_agent-2_min: 0
    apples_agent-3_max: 12
    apples_agent-3_mean: 2.75
    apples_agent-3_min: 0
    apples_agent-4_max: 9
    apples_agent-4_mean: 1.43
    apples_agent-4_min: 0
    apples_agent-5_max: 8
    apples_agent-5_mean: 1.4
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 150
    cleaning_beam_agent-0_mean: 60.78
    cleaning_beam_agent-0_min: 22
    cleaning_beam_agent-1_max: 349
    cleaning_beam_agent-1_mean: 261.05
    cleaning_beam_agent-1_min: 189
    cleaning_beam_agent-2_max: 56
    cleaning_beam_agent-2_mean: 21.52
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 154
    cleaning_beam_agent-3_mean: 68.92
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 268
    cleaning_beam_agent-4_mean: 70.7
    cleaning_beam_agent-4_min: 14
    cleaning_beam_agent-5_max: 59
    cleaning_beam_agent-5_mean: 19.63
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-11-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 62.0
  episode_reward_mean: 23.91
  episode_reward_min: 4.0
  episodes_this_iter: 96
  episodes_total: 20352
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12207.035
    learner:
      agent-0:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5336762070655823
        entropy_coeff: 0.0017600000137463212
        kl: 0.001548118656501174
        model: {}
        policy_loss: -0.003802771680057049
        total_loss: -0.00470106303691864
        vf_explained_var: -0.0243377685546875
        vf_loss: 0.4095975458621979
      agent-1:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4145612120628357
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009327979059889913
        model: {}
        policy_loss: -0.002690431661903858
        total_loss: -0.003402761649340391
        vf_explained_var: 0.026348888874053955
        vf_loss: 0.1729639619588852
      agent-2:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49460574984550476
        entropy_coeff: 0.0017600000137463212
        kl: 0.001644386094994843
        model: {}
        policy_loss: -0.003998172469437122
        total_loss: -0.00480924267321825
        vf_explained_var: 0.0025837570428848267
        vf_loss: 0.5942655205726624
      agent-3:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.620612621307373
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013800093438476324
        model: {}
        policy_loss: -0.003279701806604862
        total_loss: -0.004338947590440512
        vf_explained_var: 0.0028746575117111206
        vf_loss: 0.330329954624176
      agent-4:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6659465432167053
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017599756829440594
        model: {}
        policy_loss: -0.0042643616907298565
        total_loss: -0.005396412685513496
        vf_explained_var: -0.01606401801109314
        vf_loss: 0.39977896213531494
      agent-5:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9232155084609985
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017398324562236667
        model: {}
        policy_loss: -0.0027243553195148706
        total_loss: -0.004325805231928825
        vf_explained_var: 0.008972644805908203
        vf_loss: 0.23388904333114624
    load_time_ms: 13860.483
    num_steps_sampled: 20352000
    num_steps_trained: 20352000
    sample_time_ms: 100156.692
    update_time_ms: 17.012
  iterations_since_restore: 52
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.774444444444455
    ram_util_percent: 23.794444444444444
  pid: 27405
  policy_reward_max:
    agent-0: 15.0
    agent-1: 10.0
    agent-2: 18.0
    agent-3: 13.0
    agent-4: 17.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 4.57
    agent-1: 2.36
    agent-2: 5.3
    agent-3: 4.2
    agent-4: 4.77
    agent-5: 2.71
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.428621186870775
    mean_inference_ms: 13.344551759062364
    mean_processing_ms: 59.473464230711336
  time_since_restore: 6636.450533151627
  time_this_iter_s: 126.27435946464539
  time_total_s: 29146.733775615692
  timestamp: 1637226717
  timesteps_since_restore: 4992000
  timesteps_this_iter: 96000
  timesteps_total: 20352000
  training_iteration: 212
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 44.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    212 |          29146.7 | 20352000 |    23.91 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 21
    apples_agent-0_mean: 3.03
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 0.91
    apples_agent-1_min: 0
    apples_agent-2_max: 29
    apples_agent-2_mean: 2.36
    apples_agent-2_min: 0
    apples_agent-3_max: 34
    apples_agent-3_mean: 3.32
    apples_agent-3_min: 0
    apples_agent-4_max: 27
    apples_agent-4_mean: 1.49
    apples_agent-4_min: 0
    apples_agent-5_max: 23
    apples_agent-5_mean: 1.71
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 119
    cleaning_beam_agent-0_mean: 55.45
    cleaning_beam_agent-0_min: 12
    cleaning_beam_agent-1_max: 366
    cleaning_beam_agent-1_mean: 267.79
    cleaning_beam_agent-1_min: 184
    cleaning_beam_agent-2_max: 103
    cleaning_beam_agent-2_mean: 24.61
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 138
    cleaning_beam_agent-3_mean: 64.21
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 185
    cleaning_beam_agent-4_mean: 68.66
    cleaning_beam_agent-4_min: 19
    cleaning_beam_agent-5_max: 53
    cleaning_beam_agent-5_mean: 20.01
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-14-05
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 65.0
  episode_reward_mean: 23.62
  episode_reward_min: 4.0
  episodes_this_iter: 96
  episodes_total: 20448
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12199.616
    learner:
      agent-0:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.526897132396698
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012748839799314737
        model: {}
        policy_loss: -0.0035785913933068514
        total_loss: -0.004464614205062389
        vf_explained_var: -0.03097391128540039
        vf_loss: 0.4130963683128357
      agent-1:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41728901863098145
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009882950689643621
        model: {}
        policy_loss: -0.0024852193892002106
        total_loss: -0.0032028499990701675
        vf_explained_var: 0.028376296162605286
        vf_loss: 0.16798748075962067
      agent-2:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5017087459564209
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015818837564438581
        model: {}
        policy_loss: -0.003539558732882142
        total_loss: -0.004358983598649502
        vf_explained_var: 0.011950328946113586
        vf_loss: 0.6357804536819458
      agent-3:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.60942542552948
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015351766487583518
        model: {}
        policy_loss: -0.003064022632315755
        total_loss: -0.004099363926798105
        vf_explained_var: 0.007237270474433899
        vf_loss: 0.3724600374698639
      agent-4:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6691757440567017
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016145901754498482
        model: {}
        policy_loss: -0.0041233086958527565
        total_loss: -0.005254448391497135
        vf_explained_var: -0.0048937201499938965
        vf_loss: 0.4659208655357361
      agent-5:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9299749135971069
        entropy_coeff: 0.0017600000137463212
        kl: 0.001181350788101554
        model: {}
        policy_loss: -0.0030347881838679314
        total_loss: -0.004652251489460468
        vf_explained_var: 0.0009681880474090576
        vf_loss: 0.19285471737384796
    load_time_ms: 13889.327
    num_steps_sampled: 20448000
    num_steps_trained: 20448000
    sample_time_ms: 100198.211
    update_time_ms: 16.651
  iterations_since_restore: 53
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.98351648351648
    ram_util_percent: 23.854945054945052
  pid: 27405
  policy_reward_max:
    agent-0: 17.0
    agent-1: 11.0
    agent-2: 24.0
    agent-3: 15.0
    agent-4: 16.0
    agent-5: 16.0
  policy_reward_mean:
    agent-0: 4.1
    agent-1: 2.26
    agent-2: 5.63
    agent-3: 4.31
    agent-4: 4.87
    agent-5: 2.45
  policy_reward_min:
    agent-0: 0.0
    agent-1: -1.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.42895669236775
    mean_inference_ms: 13.345168794551691
    mean_processing_ms: 59.477450705471504
  time_since_restore: 6763.906629323959
  time_this_iter_s: 127.45609617233276
  time_total_s: 29274.189871788025
  timestamp: 1637226845
  timesteps_since_restore: 5088000
  timesteps_this_iter: 96000
  timesteps_total: 20448000
  training_iteration: 213
  trial_id: '00000'
  
[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
== Status ==
Memory usage on this node: 44.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    213 |          29274.2 | 20448000 |    23.62 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 2.57
    apples_agent-0_min: 0
    apples_agent-1_max: 19
    apples_agent-1_mean: 1.77
    apples_agent-1_min: 0
    apples_agent-2_max: 29
    apples_agent-2_mean: 2.35
    apples_agent-2_min: 0
    apples_agent-3_max: 34
    apples_agent-3_mean: 3.31
    apples_agent-3_min: 0
    apples_agent-4_max: 24
    apples_agent-4_mean: 1.49
    apples_agent-4_min: 0
    apples_agent-5_max: 57
    apples_agent-5_mean: 2.12
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 120
    cleaning_beam_agent-0_mean: 57.31
    cleaning_beam_agent-0_min: 14
    cleaning_beam_agent-1_max: 426
    cleaning_beam_agent-1_mean: 252.32
    cleaning_beam_agent-1_min: 164
    cleaning_beam_agent-2_max: 65
    cleaning_beam_agent-2_mean: 19.66
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 205
    cleaning_beam_agent-3_mean: 71.29
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 179
    cleaning_beam_agent-4_mean: 73.78
    cleaning_beam_agent-4_min: 19
    cleaning_beam_agent-5_max: 51
    cleaning_beam_agent-5_mean: 20.2
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-16-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 62.0
  episode_reward_mean: 23.7
  episode_reward_min: 5.0
  episodes_this_iter: 96
  episodes_total: 20544
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12203.772
    learner:
      agent-0:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5231562852859497
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013601717073470354
        model: {}
        policy_loss: -0.003641796298325062
        total_loss: -0.004525307100266218
        vf_explained_var: -0.01837119460105896
        vf_loss: 0.3723997473716736
      agent-1:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42667436599731445
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011127573670819402
        model: {}
        policy_loss: -0.0026844260282814503
        total_loss: -0.0034188106656074524
        vf_explained_var: 0.02196425199508667
        vf_loss: 0.1656399667263031
      agent-2:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4792639911174774
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012395792873576283
        model: {}
        policy_loss: -0.003750863019376993
        total_loss: -0.0045516593381762505
        vf_explained_var: 0.0018474608659744263
        vf_loss: 0.4270670413970947
      agent-3:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6083546876907349
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011582410661503673
        model: {}
        policy_loss: -0.003008550265803933
        total_loss: -0.0040444363839924335
        vf_explained_var: -0.0023592114448547363
        vf_loss: 0.34817156195640564
      agent-4:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6665576696395874
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012866650940850377
        model: {}
        policy_loss: -0.004210451152175665
        total_loss: -0.005334580782800913
        vf_explained_var: -0.006878197193145752
        vf_loss: 0.49003228545188904
      agent-5:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9322338104248047
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016078889602795243
        model: {}
        policy_loss: -0.002916942350566387
        total_loss: -0.004538317210972309
        vf_explained_var: 0.0062711238861083984
        vf_loss: 0.19354912638664246
    load_time_ms: 13902.537
    num_steps_sampled: 20544000
    num_steps_trained: 20544000
    sample_time_ms: 100309.636
    update_time_ms: 16.565
  iterations_since_restore: 54
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.82747252747252
    ram_util_percent: 23.951098901098902
  pid: 27405
  policy_reward_max:
    agent-0: 20.0
    agent-1: 9.0
    agent-2: 15.0
    agent-3: 13.0
    agent-4: 13.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 4.09
    agent-1: 2.46
    agent-2: 5.1
    agent-3: 4.32
    agent-4: 5.23
    agent-5: 2.5
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.42732857296427
    mean_inference_ms: 13.34494451735108
    mean_processing_ms: 59.48208589742835
  time_since_restore: 6891.429111480713
  time_this_iter_s: 127.52248215675354
  time_total_s: 29401.71235394478
  timestamp: 1637226972
  timesteps_since_restore: 5184000
  timesteps_this_iter: 96000
  timesteps_total: 20544000
  training_iteration: 214
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 44.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    214 |          29401.7 | 20544000 |     23.7 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 2.87
    apples_agent-0_min: 0
    apples_agent-1_max: 13
    apples_agent-1_mean: 1.33
    apples_agent-1_min: 0
    apples_agent-2_max: 21
    apples_agent-2_mean: 2.48
    apples_agent-2_min: 0
    apples_agent-3_max: 19
    apples_agent-3_mean: 3.66
    apples_agent-3_min: 0
    apples_agent-4_max: 8
    apples_agent-4_mean: 1.56
    apples_agent-4_min: 0
    apples_agent-5_max: 8
    apples_agent-5_mean: 1.25
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 121
    cleaning_beam_agent-0_mean: 59.54
    cleaning_beam_agent-0_min: 19
    cleaning_beam_agent-1_max: 323
    cleaning_beam_agent-1_mean: 248.47
    cleaning_beam_agent-1_min: 175
    cleaning_beam_agent-2_max: 64
    cleaning_beam_agent-2_mean: 19.93
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 142
    cleaning_beam_agent-3_mean: 68.21
    cleaning_beam_agent-3_min: 25
    cleaning_beam_agent-4_max: 171
    cleaning_beam_agent-4_mean: 73.09
    cleaning_beam_agent-4_min: 23
    cleaning_beam_agent-5_max: 53
    cleaning_beam_agent-5_mean: 19.02
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-18-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 53.0
  episode_reward_mean: 24.43
  episode_reward_min: 3.0
  episodes_this_iter: 96
  episodes_total: 20640
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12213.668
    learner:
      agent-0:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5230590105056763
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017949104076251388
        model: {}
        policy_loss: -0.003478582017123699
        total_loss: -0.004364311695098877
        vf_explained_var: -0.02911233901977539
        vf_loss: 0.3485649526119232
      agent-1:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41863423585891724
        entropy_coeff: 0.0017600000137463212
        kl: 0.001317754853516817
        model: {}
        policy_loss: -0.002746840938925743
        total_loss: -0.003469907445833087
        vf_explained_var: 0.022156164050102234
        vf_loss: 0.13725754618644714
      agent-2:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47485649585723877
        entropy_coeff: 0.0017600000137463212
        kl: 0.001443229615688324
        model: {}
        policy_loss: -0.003937178291380405
        total_loss: -0.004723693709820509
        vf_explained_var: 0.004768893122673035
        vf_loss: 0.49233490228652954
      agent-3:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6193633079528809
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015149358659982681
        model: {}
        policy_loss: -0.0029755085706710815
        total_loss: -0.004010683856904507
        vf_explained_var: 0.0011678189039230347
        vf_loss: 0.5490344166755676
      agent-4:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6715409755706787
        entropy_coeff: 0.0017600000137463212
        kl: 0.001883429940789938
        model: {}
        policy_loss: -0.004392789676785469
        total_loss: -0.005528572481125593
        vf_explained_var: -0.003834158182144165
        vf_loss: 0.46125736832618713
      agent-5:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9139509797096252
        entropy_coeff: 0.0017600000137463212
        kl: 0.001573691377416253
        model: {}
        policy_loss: -0.00308181787841022
        total_loss: -0.00467165932059288
        vf_explained_var: 0.010160639882087708
        vf_loss: 0.1871553212404251
    load_time_ms: 13917.761
    num_steps_sampled: 20640000
    num_steps_trained: 20640000
    sample_time_ms: 100229.338
    update_time_ms: 16.734
  iterations_since_restore: 55
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.98166666666667
    ram_util_percent: 24.101666666666667
  pid: 27405
  policy_reward_max:
    agent-0: 12.0
    agent-1: 9.0
    agent-2: 15.0
    agent-3: 17.0
    agent-4: 18.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 4.16
    agent-1: 2.22
    agent-2: 5.37
    agent-3: 5.04
    agent-4: 5.24
    agent-5: 2.4
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 1.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.424599050544362
    mean_inference_ms: 13.344532718785938
    mean_processing_ms: 59.48829348995615
  time_since_restore: 7018.041762113571
  time_this_iter_s: 126.61265063285828
  time_total_s: 29528.325004577637
  timestamp: 1637227099
  timesteps_since_restore: 5280000
  timesteps_this_iter: 96000
  timesteps_total: 20640000
  training_iteration: 215
  trial_id: '00000'
  
[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
== Status ==
Memory usage on this node: 44.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    215 |          29528.3 | 20640000 |    24.43 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 43
    apples_agent-0_mean: 3.96
    apples_agent-0_min: 0
    apples_agent-1_max: 59
    apples_agent-1_mean: 1.9
    apples_agent-1_min: 0
    apples_agent-2_max: 19
    apples_agent-2_mean: 2.45
    apples_agent-2_min: 0
    apples_agent-3_max: 20
    apples_agent-3_mean: 3.39
    apples_agent-3_min: 0
    apples_agent-4_max: 15
    apples_agent-4_mean: 1.45
    apples_agent-4_min: 0
    apples_agent-5_max: 13
    apples_agent-5_mean: 1.79
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 112
    cleaning_beam_agent-0_mean: 58.31
    cleaning_beam_agent-0_min: 15
    cleaning_beam_agent-1_max: 355
    cleaning_beam_agent-1_mean: 261.62
    cleaning_beam_agent-1_min: 175
    cleaning_beam_agent-2_max: 64
    cleaning_beam_agent-2_mean: 18.24
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 147
    cleaning_beam_agent-3_mean: 69.85
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 195
    cleaning_beam_agent-4_mean: 70.16
    cleaning_beam_agent-4_min: 16
    cleaning_beam_agent-5_max: 46
    cleaning_beam_agent-5_mean: 17.47
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-20-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 86.0
  episode_reward_mean: 24.65
  episode_reward_min: 6.0
  episodes_this_iter: 96
  episodes_total: 20736
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12218.749
    learner:
      agent-0:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5237226486206055
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015174387954175472
        model: {}
        policy_loss: -0.0036753471940755844
        total_loss: -0.0045484332367777824
        vf_explained_var: -0.006209522485733032
        vf_loss: 0.48663753271102905
      agent-1:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4297856390476227
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008626139024272561
        model: {}
        policy_loss: -0.00222765002399683
        total_loss: -0.002962191589176655
        vf_explained_var: 0.023259609937667847
        vf_loss: 0.21881204843521118
      agent-2:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4730803370475769
        entropy_coeff: 0.0017600000137463212
        kl: 0.001625414122827351
        model: {}
        policy_loss: -0.0036790110170841217
        total_loss: -0.004445693921297789
        vf_explained_var: 0.0011343657970428467
        vf_loss: 0.6594247817993164
      agent-3:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6057556867599487
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016887717647477984
        model: {}
        policy_loss: -0.0031117992475628853
        total_loss: -0.004128608386963606
        vf_explained_var: 0.001942470669746399
        vf_loss: 0.4931678771972656
      agent-4:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6617041230201721
        entropy_coeff: 0.0017600000137463212
        kl: 0.001740394625812769
        model: {}
        policy_loss: -0.004370853770524263
        total_loss: -0.005485359579324722
        vf_explained_var: -0.0059385597705841064
        vf_loss: 0.5009172558784485
      agent-5:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9182015657424927
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014199384022504091
        model: {}
        policy_loss: -0.00278403889387846
        total_loss: -0.004373215604573488
        vf_explained_var: 0.013636201620101929
        vf_loss: 0.26859527826309204
    load_time_ms: 13873.433
    num_steps_sampled: 20736000
    num_steps_trained: 20736000
    sample_time_ms: 100369.795
    update_time_ms: 16.802
  iterations_since_restore: 56
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.693406593406586
    ram_util_percent: 24.14395604395604
  pid: 27405
  policy_reward_max:
    agent-0: 19.0
    agent-1: 12.0
    agent-2: 24.0
    agent-3: 17.0
    agent-4: 18.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 4.5
    agent-1: 2.31
    agent-2: 5.41
    agent-3: 4.6
    agent-4: 5.1
    agent-5: 2.73
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.422320207486592
    mean_inference_ms: 13.342741870817145
    mean_processing_ms: 59.492399457188505
  time_since_restore: 7145.348463773727
  time_this_iter_s: 127.30670166015625
  time_total_s: 29655.631706237793
  timestamp: 1637227226
  timesteps_since_restore: 5376000
  timesteps_this_iter: 96000
  timesteps_total: 20736000
  training_iteration: 216
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 44.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    216 |          29655.6 | 20736000 |    24.65 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 2.6
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.91
    apples_agent-1_min: 0
    apples_agent-2_max: 27
    apples_agent-2_mean: 2.08
    apples_agent-2_min: 0
    apples_agent-3_max: 10
    apples_agent-3_mean: 2.6
    apples_agent-3_min: 0
    apples_agent-4_max: 50
    apples_agent-4_mean: 2.51
    apples_agent-4_min: 0
    apples_agent-5_max: 8
    apples_agent-5_mean: 1.36
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 113
    cleaning_beam_agent-0_mean: 56.08
    cleaning_beam_agent-0_min: 21
    cleaning_beam_agent-1_max: 385
    cleaning_beam_agent-1_mean: 251.79
    cleaning_beam_agent-1_min: 164
    cleaning_beam_agent-2_max: 64
    cleaning_beam_agent-2_mean: 18.7
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 134
    cleaning_beam_agent-3_mean: 69.6
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 176
    cleaning_beam_agent-4_mean: 71.8
    cleaning_beam_agent-4_min: 16
    cleaning_beam_agent-5_max: 37
    cleaning_beam_agent-5_mean: 15.5
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-22-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 86.0
  episode_reward_mean: 20.98
  episode_reward_min: -181.0
  episodes_this_iter: 96
  episodes_total: 20832
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12232.986
    learner:
      agent-0:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5266831517219543
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016134907491505146
        model: {}
        policy_loss: -0.003508299821987748
        total_loss: -0.004398660734295845
        vf_explained_var: -0.01532679796218872
        vf_loss: 0.36601197719573975
      agent-1:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4154927730560303
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008775638416409492
        model: {}
        policy_loss: -0.0025930136907845736
        total_loss: -0.0033031869679689407
        vf_explained_var: 0.025029093027114868
        vf_loss: 0.21094045042991638
      agent-2:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4737374186515808
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011238991282880306
        model: {}
        policy_loss: -0.0030583611223846674
        total_loss: -0.0034396699629724026
        vf_explained_var: 0.001641586422920227
        vf_loss: 4.524691581726074
      agent-3:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6060646772384644
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007318362477235496
        model: {}
        policy_loss: -0.0021683962550014257
        total_loss: -0.002801730763167143
        vf_explained_var: -4.4405460357666016e-05
        vf_loss: 4.3334059715271
      agent-4:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6589146852493286
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013823248445987701
        model: {}
        policy_loss: -0.004274759907275438
        total_loss: -0.0053877937607467175
        vf_explained_var: -0.013805419206619263
        vf_loss: 0.46658769249916077
      agent-5:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9373729228973389
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013979905052110553
        model: {}
        policy_loss: -0.002919880673289299
        total_loss: -0.004551632795482874
        vf_explained_var: 0.0017789006233215332
        vf_loss: 0.1802142858505249
    load_time_ms: 13862.731
    num_steps_sampled: 20832000
    num_steps_trained: 20832000
    sample_time_ms: 100358.732
    update_time_ms: 16.751
  iterations_since_restore: 57
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.34309392265194
    ram_util_percent: 24.26408839779005
  pid: 27405
  policy_reward_max:
    agent-0: 15.0
    agent-1: 12.0
    agent-2: 24.0
    agent-3: 16.0
    agent-4: 15.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 3.93
    agent-1: 2.47
    agent-2: 4.09
    agent-3: 3.07
    agent-4: 4.83
    agent-5: 2.59
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -98.0
    agent-3: -94.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.419887237208478
    mean_inference_ms: 13.341499496747906
    mean_processing_ms: 59.49516724519392
  time_since_restore: 7272.516977071762
  time_this_iter_s: 127.16851329803467
  time_total_s: 29782.800219535828
  timestamp: 1637227354
  timesteps_since_restore: 5472000
  timesteps_this_iter: 96000
  timesteps_total: 20832000
  training_iteration: 217
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 45.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    217 |          29782.8 | 20832000 |    20.98 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 43
    apples_agent-0_mean: 3.1
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 1.15
    apples_agent-1_min: 0
    apples_agent-2_max: 13
    apples_agent-2_mean: 2.27
    apples_agent-2_min: 0
    apples_agent-3_max: 22
    apples_agent-3_mean: 3.56
    apples_agent-3_min: 0
    apples_agent-4_max: 9
    apples_agent-4_mean: 1.25
    apples_agent-4_min: 0
    apples_agent-5_max: 6
    apples_agent-5_mean: 1.43
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 128
    cleaning_beam_agent-0_mean: 59.31
    cleaning_beam_agent-0_min: 18
    cleaning_beam_agent-1_max: 340
    cleaning_beam_agent-1_mean: 253.44
    cleaning_beam_agent-1_min: 174
    cleaning_beam_agent-2_max: 56
    cleaning_beam_agent-2_mean: 20.73
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 145
    cleaning_beam_agent-3_mean: 66.86
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 157
    cleaning_beam_agent-4_mean: 64.53
    cleaning_beam_agent-4_min: 17
    cleaning_beam_agent-5_max: 41
    cleaning_beam_agent-5_mean: 19.09
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-24-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 62.0
  episode_reward_mean: 22.69
  episode_reward_min: -43.0
  episodes_this_iter: 96
  episodes_total: 20928
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12238.113
    learner:
      agent-0:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5301367044448853
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016101049259305
        model: {}
        policy_loss: -0.003996142186224461
        total_loss: -0.004896421451121569
        vf_explained_var: -0.025305986404418945
        vf_loss: 0.3276010751724243
      agent-1:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4239179790019989
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013638189993798733
        model: {}
        policy_loss: -0.0025481516495347023
        total_loss: -0.0032739141024649143
        vf_explained_var: 0.02031773328781128
        vf_loss: 0.20333954691886902
      agent-2:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4822967052459717
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013817035360261798
        model: {}
        policy_loss: -0.0034725728910416365
        total_loss: -0.004135577008128166
        vf_explained_var: 0.005482092499732971
        vf_loss: 1.858362078666687
      agent-3:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6058290004730225
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016774104442447424
        model: {}
        policy_loss: -0.0021379562094807625
        total_loss: -0.0030395009089261293
        vf_explained_var: 0.00045894086360931396
        vf_loss: 1.6471896171569824
      agent-4:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.661432147026062
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018982221372425556
        model: {}
        policy_loss: -0.0041696191765367985
        total_loss: -0.005290070082992315
        vf_explained_var: -0.01093938946723938
        vf_loss: 0.4366680681705475
      agent-5:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9435452222824097
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019878072198480368
        model: {}
        policy_loss: -0.0031983437947928905
        total_loss: -0.004839045461267233
        vf_explained_var: 0.005615487694740295
        vf_loss: 0.19939306378364563
    load_time_ms: 13879.275
    num_steps_sampled: 20928000
    num_steps_trained: 20928000
    sample_time_ms: 100344.609
    update_time_ms: 16.953
  iterations_since_restore: 58
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.60888888888889
    ram_util_percent: 24.352777777777778
  pid: 27405
  policy_reward_max:
    agent-0: 12.0
    agent-1: 10.0
    agent-2: 15.0
    agent-3: 13.0
    agent-4: 16.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 4.11
    agent-1: 2.44
    agent-2: 4.95
    agent-3: 3.76
    agent-4: 4.75
    agent-5: 2.68
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -44.0
    agent-3: -49.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.41573876446752
    mean_inference_ms: 13.34218726299145
    mean_processing_ms: 59.49594054833885
  time_since_restore: 7398.591107130051
  time_this_iter_s: 126.07413005828857
  time_total_s: 29908.874349594116
  timestamp: 1637227480
  timesteps_since_restore: 5568000
  timesteps_this_iter: 96000
  timesteps_total: 20928000
  training_iteration: 218
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 45.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    218 |          29908.9 | 20928000 |    22.69 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 40
    apples_agent-0_mean: 3.11
    apples_agent-0_min: 0
    apples_agent-1_max: 94
    apples_agent-1_mean: 2.09
    apples_agent-1_min: 0
    apples_agent-2_max: 30
    apples_agent-2_mean: 2.62
    apples_agent-2_min: 0
    apples_agent-3_max: 22
    apples_agent-3_mean: 3.38
    apples_agent-3_min: 0
    apples_agent-4_max: 17
    apples_agent-4_mean: 1.27
    apples_agent-4_min: 0
    apples_agent-5_max: 6
    apples_agent-5_mean: 1.53
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 118
    cleaning_beam_agent-0_mean: 58.39
    cleaning_beam_agent-0_min: 19
    cleaning_beam_agent-1_max: 339
    cleaning_beam_agent-1_mean: 250.62
    cleaning_beam_agent-1_min: 96
    cleaning_beam_agent-2_max: 74
    cleaning_beam_agent-2_mean: 22.02
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 157
    cleaning_beam_agent-3_mean: 73.65
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 163
    cleaning_beam_agent-4_mean: 65.14
    cleaning_beam_agent-4_min: 12
    cleaning_beam_agent-5_max: 59
    cleaning_beam_agent-5_mean: 18.0
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-26-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 60.0
  episode_reward_mean: 21.22
  episode_reward_min: -85.0
  episodes_this_iter: 96
  episodes_total: 21024
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12232.443
    learner:
      agent-0:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5249130725860596
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012390000047162175
        model: {}
        policy_loss: -0.0021661538630723953
        total_loss: -0.0029807258397340775
        vf_explained_var: -0.010477453470230103
        vf_loss: 1.092724323272705
      agent-1:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4256022572517395
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010895277373492718
        model: {}
        policy_loss: -0.002199278213083744
        total_loss: -0.0029282851610332727
        vf_explained_var: 0.02136172354221344
        vf_loss: 0.20051608979701996
      agent-2:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49280938506126404
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016144225373864174
        model: {}
        policy_loss: -0.0033228397369384766
        total_loss: -0.004057370126247406
        vf_explained_var: 0.009505584836006165
        vf_loss: 1.3281749486923218
      agent-3:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.62135910987854
        entropy_coeff: 0.0017600000137463212
        kl: 0.001284918049350381
        model: {}
        policy_loss: -0.0030049625784158707
        total_loss: -0.004053131211549044
        vf_explained_var: -0.0008281916379928589
        vf_loss: 0.45425447821617126
      agent-4:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6574446558952332
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011171033838763833
        model: {}
        policy_loss: -0.001495999749749899
        total_loss: -0.002477498259395361
        vf_explained_var: 0.023366659879684448
        vf_loss: 1.7560501098632812
      agent-5:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.929147481918335
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015343690756708384
        model: {}
        policy_loss: -0.0028503676876425743
        total_loss: -0.004465301521122456
        vf_explained_var: 0.012556031346321106
        vf_loss: 0.203696146607399
    load_time_ms: 13887.893
    num_steps_sampled: 21024000
    num_steps_trained: 21024000
    sample_time_ms: 100564.401
    update_time_ms: 16.989
  iterations_since_restore: 59
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 48.26132596685083
    ram_util_percent: 24.474033149171266
  pid: 27405
  policy_reward_max:
    agent-0: 13.0
    agent-1: 11.0
    agent-2: 20.0
    agent-3: 12.0
    agent-4: 16.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 2.8
    agent-1: 2.26
    agent-2: 4.64
    agent-3: 4.52
    agent-4: 4.4
    agent-5: 2.6
  policy_reward_min:
    agent-0: -47.0
    agent-1: 0.0
    agent-2: -47.0
    agent-3: 0.0
    agent-4: -48.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.415086085224043
    mean_inference_ms: 13.342472354626556
    mean_processing_ms: 59.50336753642026
  time_since_restore: 7525.928666114807
  time_this_iter_s: 127.33755898475647
  time_total_s: 30036.211908578873
  timestamp: 1637227607
  timesteps_since_restore: 5664000
  timesteps_this_iter: 96000
  timesteps_total: 21024000
  training_iteration: 219
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 45.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    219 |          30036.2 | 21024000 |    21.22 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 7
    apples_agent-0_mean: 2.53
    apples_agent-0_min: 0
    apples_agent-1_max: 10
    apples_agent-1_mean: 1.22
    apples_agent-1_min: 0
    apples_agent-2_max: 17
    apples_agent-2_mean: 2.54
    apples_agent-2_min: 0
    apples_agent-3_max: 33
    apples_agent-3_mean: 3.3
    apples_agent-3_min: 0
    apples_agent-4_max: 32
    apples_agent-4_mean: 1.84
    apples_agent-4_min: 0
    apples_agent-5_max: 10
    apples_agent-5_mean: 1.58
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 116
    cleaning_beam_agent-0_mean: 59.71
    cleaning_beam_agent-0_min: 12
    cleaning_beam_agent-1_max: 397
    cleaning_beam_agent-1_mean: 249.36
    cleaning_beam_agent-1_min: 148
    cleaning_beam_agent-2_max: 49
    cleaning_beam_agent-2_mean: 18.59
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 160
    cleaning_beam_agent-3_mean: 79.61
    cleaning_beam_agent-3_min: 26
    cleaning_beam_agent-4_max: 219
    cleaning_beam_agent-4_mean: 70.68
    cleaning_beam_agent-4_min: 7
    cleaning_beam_agent-5_max: 53
    cleaning_beam_agent-5_mean: 19.81
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-28-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 64.0
  episode_reward_mean: 23.62
  episode_reward_min: -22.0
  episodes_this_iter: 96
  episodes_total: 21120
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12240.863
    learner:
      agent-0:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.533724308013916
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013779031578451395
        model: {}
        policy_loss: -0.003778434358537197
        total_loss: -0.004684293642640114
        vf_explained_var: -0.02651861310005188
        vf_loss: 0.3349737823009491
      agent-1:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.421887069940567
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011158130364492536
        model: {}
        policy_loss: -0.002524190116673708
        total_loss: -0.003248292487114668
        vf_explained_var: 0.018297046422958374
        vf_loss: 0.1841837614774704
      agent-2:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47642379999160767
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016447234665974975
        model: {}
        policy_loss: -0.00364837609231472
        total_loss: -0.004430295899510384
        vf_explained_var: 0.004916861653327942
        vf_loss: 0.5658394694328308
      agent-3:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6243992447853088
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014665612252429128
        model: {}
        policy_loss: -0.003053253749385476
        total_loss: -0.0041118040680885315
        vf_explained_var: -0.006569042801856995
        vf_loss: 0.4039161205291748
      agent-4:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6664917469024658
        entropy_coeff: 0.0017600000137463212
        kl: 0.001212369417771697
        model: {}
        policy_loss: -0.003937981557101011
        total_loss: -0.005067483521997929
        vf_explained_var: -0.0044801682233810425
        vf_loss: 0.435212641954422
      agent-5:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.950238823890686
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018124073976650834
        model: {}
        policy_loss: -0.001954514067620039
        total_loss: -0.0034801429137587547
        vf_explained_var: 0.004820123314857483
        vf_loss: 1.4678939580917358
    load_time_ms: 13892.79
    num_steps_sampled: 21120000
    num_steps_trained: 21120000
    sample_time_ms: 100734.861
    update_time_ms: 16.868
  iterations_since_restore: 60
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.9657458563536
    ram_util_percent: 24.497790055248622
  pid: 27405
  policy_reward_max:
    agent-0: 11.0
    agent-1: 11.0
    agent-2: 16.0
    agent-3: 21.0
    agent-4: 15.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 4.16
    agent-1: 2.2
    agent-2: 5.79
    agent-3: 4.41
    agent-4: 4.74
    agent-5: 2.32
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -47.0
  sampler_perf:
    mean_env_wait_ms: 23.41249126893275
    mean_inference_ms: 13.342331386598508
    mean_processing_ms: 59.50041667963908
  time_since_restore: 7653.142447948456
  time_this_iter_s: 127.21378183364868
  time_total_s: 30163.42569041252
  timestamp: 1637227735
  timesteps_since_restore: 5760000
  timesteps_this_iter: 96000
  timesteps_total: 21120000
  training_iteration: 220
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 45.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    220 |          30163.4 | 21120000 |    23.62 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 2.6
    apples_agent-0_min: 0
    apples_agent-1_max: 9
    apples_agent-1_mean: 1.31
    apples_agent-1_min: 0
    apples_agent-2_max: 10
    apples_agent-2_mean: 1.94
    apples_agent-2_min: 0
    apples_agent-3_max: 17
    apples_agent-3_mean: 3.15
    apples_agent-3_min: 0
    apples_agent-4_max: 6
    apples_agent-4_mean: 1.05
    apples_agent-4_min: 0
    apples_agent-5_max: 51
    apples_agent-5_mean: 2.19
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 133
    cleaning_beam_agent-0_mean: 55.37
    cleaning_beam_agent-0_min: 14
    cleaning_beam_agent-1_max: 345
    cleaning_beam_agent-1_mean: 247.96
    cleaning_beam_agent-1_min: 146
    cleaning_beam_agent-2_max: 71
    cleaning_beam_agent-2_mean: 18.79
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 173
    cleaning_beam_agent-3_mean: 71.81
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 192
    cleaning_beam_agent-4_mean: 65.04
    cleaning_beam_agent-4_min: 17
    cleaning_beam_agent-5_max: 64
    cleaning_beam_agent-5_mean: 19.12
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-31-02
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 63.0
  episode_reward_mean: 22.82
  episode_reward_min: 0.0
  episodes_this_iter: 96
  episodes_total: 21216
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12214.369
    learner:
      agent-0:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.530155599117279
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016236485680565238
        model: {}
        policy_loss: -0.00392054533585906
        total_loss: -0.004819190129637718
        vf_explained_var: -0.02899956703186035
        vf_loss: 0.3442702889442444
      agent-1:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4084846079349518
        entropy_coeff: 0.0017600000137463212
        kl: 0.001334676519036293
        model: {}
        policy_loss: -0.0025210268795490265
        total_loss: -0.0032254504039883614
        vf_explained_var: 0.025130361318588257
        vf_loss: 0.1450883001089096
      agent-2:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46865689754486084
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016815433045849204
        model: {}
        policy_loss: -0.0035120095126330853
        total_loss: -0.0042763822712004185
        vf_explained_var: 0.0006552338600158691
        vf_loss: 0.60466068983078
      agent-3:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6107103228569031
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013854368589818478
        model: {}
        policy_loss: -0.002962627448141575
        total_loss: -0.003996690269559622
        vf_explained_var: 0.0015317052602767944
        vf_loss: 0.4078529477119446
      agent-4:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6618261337280273
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018464828608557582
        model: {}
        policy_loss: -0.004231773316860199
        total_loss: -0.005352211184799671
        vf_explained_var: -0.0034577250480651855
        vf_loss: 0.4437246024608612
      agent-5:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9473761916160583
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022062610369175673
        model: {}
        policy_loss: -0.003256765427067876
        total_loss: -0.004905545152723789
        vf_explained_var: 0.001956656575202942
        vf_loss: 0.1860315054655075
    load_time_ms: 13896.009
    num_steps_sampled: 21216000
    num_steps_trained: 21216000
    sample_time_ms: 100729.556
    update_time_ms: 16.882
  iterations_since_restore: 61
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 48.034254143646415
    ram_util_percent: 24.622099447513815
  pid: 27405
  policy_reward_max:
    agent-0: 14.0
    agent-1: 8.0
    agent-2: 19.0
    agent-3: 16.0
    agent-4: 14.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 3.86
    agent-1: 2.08
    agent-2: 5.51
    agent-3: 4.01
    agent-4: 4.84
    agent-5: 2.52
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -1.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.407956360999346
    mean_inference_ms: 13.341332290329643
    mean_processing_ms: 59.49846489495194
  time_since_restore: 7779.679070234299
  time_this_iter_s: 126.5366222858429
  time_total_s: 30289.962312698364
  timestamp: 1637227862
  timesteps_since_restore: 5856000
  timesteps_this_iter: 96000
  timesteps_total: 21216000
  training_iteration: 221
  trial_id: '00000'
  
[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
== Status ==
Memory usage on this node: 45.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    221 |            30290 | 21216000 |    22.82 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 63
    apples_agent-0_mean: 3.42
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.96
    apples_agent-1_min: 0
    apples_agent-2_max: 20
    apples_agent-2_mean: 2.47
    apples_agent-2_min: 0
    apples_agent-3_max: 25
    apples_agent-3_mean: 3.19
    apples_agent-3_min: 0
    apples_agent-4_max: 7
    apples_agent-4_mean: 1.14
    apples_agent-4_min: 0
    apples_agent-5_max: 18
    apples_agent-5_mean: 1.69
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 136
    cleaning_beam_agent-0_mean: 58.43
    cleaning_beam_agent-0_min: 13
    cleaning_beam_agent-1_max: 361
    cleaning_beam_agent-1_mean: 252.01
    cleaning_beam_agent-1_min: 193
    cleaning_beam_agent-2_max: 59
    cleaning_beam_agent-2_mean: 20.29
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 147
    cleaning_beam_agent-3_mean: 66.41
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 176
    cleaning_beam_agent-4_mean: 72.96
    cleaning_beam_agent-4_min: 14
    cleaning_beam_agent-5_max: 38
    cleaning_beam_agent-5_mean: 15.19
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-33-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 40.0
  episode_reward_mean: 21.11
  episode_reward_min: -77.0
  episodes_this_iter: 96
  episodes_total: 21312
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12205.935
    learner:
      agent-0:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5223684310913086
        entropy_coeff: 0.0017600000137463212
        kl: 0.001408688840456307
        model: {}
        policy_loss: -0.003646879456937313
        total_loss: -0.004532480612397194
        vf_explained_var: -0.021820902824401855
        vf_loss: 0.33768439292907715
      agent-1:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41323739290237427
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008779857307672501
        model: {}
        policy_loss: -0.00235054362565279
        total_loss: -0.00305899977684021
        vf_explained_var: 0.02080540359020233
        vf_loss: 0.1884271204471588
      agent-2:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46411871910095215
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011193652171641588
        model: {}
        policy_loss: -0.003306344151496887
        total_loss: -0.004071834497153759
        vf_explained_var: 0.004971235990524292
        vf_loss: 0.5136013031005859
      agent-3:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6200660467147827
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012699264334514737
        model: {}
        policy_loss: -0.002292826771736145
        total_loss: -0.0032167229801416397
        vf_explained_var: 0.004249170422554016
        vf_loss: 1.6742010116577148
      agent-4:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6766380071640015
        entropy_coeff: 0.0017600000137463212
        kl: 0.001763687003403902
        model: {}
        policy_loss: -0.004227367229759693
        total_loss: -0.0053786225616931915
        vf_explained_var: -0.020048290491104126
        vf_loss: 0.3962540924549103
      agent-5:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.940910279750824
        entropy_coeff: 0.0017600000137463212
        kl: 0.002808372722938657
        model: {}
        policy_loss: -0.0022394685074687004
        total_loss: -0.003744297195225954
        vf_explained_var: 0.008021563291549683
        vf_loss: 1.511754035949707
    load_time_ms: 13886.326
    num_steps_sampled: 21312000
    num_steps_trained: 21312000
    sample_time_ms: 100689.604
    update_time_ms: 16.795
  iterations_since_restore: 62
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.77988826815642
    ram_util_percent: 24.755307262569836
  pid: 27405
  policy_reward_max:
    agent-0: 13.0
    agent-1: 9.0
    agent-2: 15.0
    agent-3: 13.0
    agent-4: 11.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 4.01
    agent-1: 2.37
    agent-2: 5.24
    agent-3: 3.01
    agent-4: 4.53
    agent-5: 1.95
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -38.0
    agent-3: -46.0
    agent-4: 0.0
    agent-5: -50.0
  sampler_perf:
    mean_env_wait_ms: 23.40408685985052
    mean_inference_ms: 13.34137388210094
    mean_processing_ms: 59.497698584402976
  time_since_restore: 7905.387085676193
  time_this_iter_s: 125.70801544189453
  time_total_s: 30415.67032814026
  timestamp: 1637227988
  timesteps_since_restore: 5952000
  timesteps_this_iter: 96000
  timesteps_total: 21312000
  training_iteration: 222
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 45.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    222 |          30415.7 | 21312000 |    21.11 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 63
    apples_agent-0_mean: 3.18
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 1.14
    apples_agent-1_min: 0
    apples_agent-2_max: 16
    apples_agent-2_mean: 2.2
    apples_agent-2_min: 0
    apples_agent-3_max: 16
    apples_agent-3_mean: 3.19
    apples_agent-3_min: 0
    apples_agent-4_max: 18
    apples_agent-4_mean: 1.15
    apples_agent-4_min: 0
    apples_agent-5_max: 26
    apples_agent-5_mean: 1.7
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 186
    cleaning_beam_agent-0_mean: 55.96
    cleaning_beam_agent-0_min: 17
    cleaning_beam_agent-1_max: 354
    cleaning_beam_agent-1_mean: 251.08
    cleaning_beam_agent-1_min: 165
    cleaning_beam_agent-2_max: 48
    cleaning_beam_agent-2_mean: 18.87
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 169
    cleaning_beam_agent-3_mean: 68.41
    cleaning_beam_agent-3_min: 23
    cleaning_beam_agent-4_max: 184
    cleaning_beam_agent-4_mean: 66.92
    cleaning_beam_agent-4_min: 12
    cleaning_beam_agent-5_max: 42
    cleaning_beam_agent-5_mean: 15.91
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-35-15
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 66.0
  episode_reward_mean: 25.26
  episode_reward_min: 7.0
  episodes_this_iter: 96
  episodes_total: 21408
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12214.466
    learner:
      agent-0:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5165179967880249
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013294325908645988
        model: {}
        policy_loss: -0.0035787788219749928
        total_loss: -0.004447960294783115
        vf_explained_var: -0.015168890357017517
        vf_loss: 0.3989035487174988
      agent-1:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40233561396598816
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009995137806981802
        model: {}
        policy_loss: -0.002545945579186082
        total_loss: -0.0032386102247983217
        vf_explained_var: 0.029167696833610535
        vf_loss: 0.1544661819934845
      agent-2:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46779635548591614
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012443593004718423
        model: {}
        policy_loss: -0.003548163454979658
        total_loss: -0.004298876039683819
        vf_explained_var: 0.001057058572769165
        vf_loss: 0.7260779738426208
      agent-3:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6195545196533203
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012403063010424376
        model: {}
        policy_loss: -0.0032831309363245964
        total_loss: -0.004335339646786451
        vf_explained_var: -0.0030211657285690308
        vf_loss: 0.3820640444755554
      agent-4:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6747997999191284
        entropy_coeff: 0.0017600000137463212
        kl: 0.001395182334817946
        model: {}
        policy_loss: -0.0038645570166409016
        total_loss: -0.005007166415452957
        vf_explained_var: -0.0007368326187133789
        vf_loss: 0.45040658116340637
      agent-5:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9362505674362183
        entropy_coeff: 0.0017600000137463212
        kl: 0.001599106821231544
        model: {}
        policy_loss: -0.0027098366990685463
        total_loss: -0.004330671392381191
        vf_explained_var: 0.007349222898483276
        vf_loss: 0.26968783140182495
    load_time_ms: 13855.27
    num_steps_sampled: 21408000
    num_steps_trained: 21408000
    sample_time_ms: 100699.783
    update_time_ms: 17.094
  iterations_since_restore: 63
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.52527472527473
    ram_util_percent: 24.78846153846154
  pid: 27405
  policy_reward_max:
    agent-0: 15.0
    agent-1: 8.0
    agent-2: 21.0
    agent-3: 16.0
    agent-4: 15.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 4.6
    agent-1: 2.23
    agent-2: 6.32
    agent-3: 4.34
    agent-4: 4.89
    agent-5: 2.88
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.403989499535765
    mean_inference_ms: 13.341789767537037
    mean_processing_ms: 59.501405639121515
  time_since_restore: 8032.711244106293
  time_this_iter_s: 127.32415843009949
  time_total_s: 30542.99448657036
  timestamp: 1637228115
  timesteps_since_restore: 6048000
  timesteps_this_iter: 96000
  timesteps_total: 21408000
  training_iteration: 223
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 46.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    223 |            30543 | 21408000 |    25.26 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 2.49
    apples_agent-0_min: 0
    apples_agent-1_max: 26
    apples_agent-1_mean: 1.41
    apples_agent-1_min: 0
    apples_agent-2_max: 10
    apples_agent-2_mean: 1.97
    apples_agent-2_min: 0
    apples_agent-3_max: 10
    apples_agent-3_mean: 2.5
    apples_agent-3_min: 0
    apples_agent-4_max: 12
    apples_agent-4_mean: 1.15
    apples_agent-4_min: 0
    apples_agent-5_max: 9
    apples_agent-5_mean: 1.6
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 108
    cleaning_beam_agent-0_mean: 54.16
    cleaning_beam_agent-0_min: 9
    cleaning_beam_agent-1_max: 387
    cleaning_beam_agent-1_mean: 265.71
    cleaning_beam_agent-1_min: 167
    cleaning_beam_agent-2_max: 87
    cleaning_beam_agent-2_mean: 18.94
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 154
    cleaning_beam_agent-3_mean: 69.96
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 192
    cleaning_beam_agent-4_mean: 61.19
    cleaning_beam_agent-4_min: 11
    cleaning_beam_agent-5_max: 52
    cleaning_beam_agent-5_mean: 19.33
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-37-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 52.0
  episode_reward_mean: 22.6
  episode_reward_min: -18.0
  episodes_this_iter: 96
  episodes_total: 21504
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12210.837
    learner:
      agent-0:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.523908793926239
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013601866085082293
        model: {}
        policy_loss: -0.0036440175026655197
        total_loss: -0.004534522071480751
        vf_explained_var: -0.017073988914489746
        vf_loss: 0.31574302911758423
      agent-1:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4228212237358093
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009866163600236177
        model: {}
        policy_loss: -0.0027918510604649782
        total_loss: -0.003521184204146266
        vf_explained_var: 0.020365670323371887
        vf_loss: 0.14831413328647614
      agent-2:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46940577030181885
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015617875615134835
        model: {}
        policy_loss: -0.0036046591121703386
        total_loss: -0.004382487386465073
        vf_explained_var: 0.005546107888221741
        vf_loss: 0.48326271772384644
      agent-3:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6168773174285889
        entropy_coeff: 0.0017600000137463212
        kl: 0.001286473823711276
        model: {}
        policy_loss: -0.0031161904335021973
        total_loss: -0.004172149114310741
        vf_explained_var: 0.003293648362159729
        vf_loss: 0.2974570691585541
      agent-4:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6708382368087769
        entropy_coeff: 0.0017600000137463212
        kl: 0.001531421672552824
        model: {}
        policy_loss: -0.0038556642830371857
        total_loss: -0.004991631489247084
        vf_explained_var: 0.006388634443283081
        vf_loss: 0.44707974791526794
      agent-5:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9400028586387634
        entropy_coeff: 0.0017600000137463212
        kl: 0.0026732785627245903
        model: {}
        policy_loss: -0.003068911377340555
        total_loss: -0.004704983904957771
        vf_explained_var: 0.0012357831001281738
        vf_loss: 0.18331097066402435
    load_time_ms: 13846.474
    num_steps_sampled: 21504000
    num_steps_trained: 21504000
    sample_time_ms: 100704.941
    update_time_ms: 16.962
  iterations_since_restore: 64
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.42651933701658
    ram_util_percent: 24.93204419889502
  pid: 27405
  policy_reward_max:
    agent-0: 14.0
    agent-1: 7.0
    agent-2: 15.0
    agent-3: 14.0
    agent-4: 16.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 3.97
    agent-1: 1.97
    agent-2: 4.98
    agent-3: 3.88
    agent-4: 5.04
    agent-5: 2.76
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -39.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.4020551281429
    mean_inference_ms: 13.341219504024952
    mean_processing_ms: 59.501293960641526
  time_since_restore: 8160.219900608063
  time_this_iter_s: 127.50865650177002
  time_total_s: 30670.50314307213
  timestamp: 1637228243
  timesteps_since_restore: 6144000
  timesteps_this_iter: 96000
  timesteps_total: 21504000
  training_iteration: 224
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 46.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    224 |          30670.5 | 21504000 |     22.6 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 2.18
    apples_agent-0_min: 0
    apples_agent-1_max: 28
    apples_agent-1_mean: 1.52
    apples_agent-1_min: 0
    apples_agent-2_max: 11
    apples_agent-2_mean: 2.1
    apples_agent-2_min: 0
    apples_agent-3_max: 14
    apples_agent-3_mean: 3.06
    apples_agent-3_min: 0
    apples_agent-4_max: 12
    apples_agent-4_mean: 1.29
    apples_agent-4_min: 0
    apples_agent-5_max: 10
    apples_agent-5_mean: 1.61
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 145
    cleaning_beam_agent-0_mean: 55.1
    cleaning_beam_agent-0_min: 16
    cleaning_beam_agent-1_max: 379
    cleaning_beam_agent-1_mean: 258.05
    cleaning_beam_agent-1_min: 162
    cleaning_beam_agent-2_max: 57
    cleaning_beam_agent-2_mean: 17.4
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 172
    cleaning_beam_agent-3_mean: 69.64
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 192
    cleaning_beam_agent-4_mean: 61.57
    cleaning_beam_agent-4_min: 17
    cleaning_beam_agent-5_max: 70
    cleaning_beam_agent-5_mean: 18.98
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-39-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 59.0
  episode_reward_mean: 22.4
  episode_reward_min: 6.0
  episodes_this_iter: 96
  episodes_total: 21600
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12207.997
    learner:
      agent-0:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5220546722412109
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014395215548574924
        model: {}
        policy_loss: -0.0036033056676387787
        total_loss: -0.004488825798034668
        vf_explained_var: -0.03304222226142883
        vf_loss: 0.33294677734375
      agent-1:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41803064942359924
        entropy_coeff: 0.0017600000137463212
        kl: 0.001130807213485241
        model: {}
        policy_loss: -0.0021652947179973125
        total_loss: -0.0028855735436081886
        vf_explained_var: 0.014976754784584045
        vf_loss: 0.15452836453914642
      agent-2:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4560474455356598
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014078504173085093
        model: {}
        policy_loss: -0.003489186055958271
        total_loss: -0.004245169460773468
        vf_explained_var: 0.0006795525550842285
        vf_loss: 0.4666064977645874
      agent-3:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6160256862640381
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012033615494146943
        model: {}
        policy_loss: -0.0029938840307295322
        total_loss: -0.0040483977645635605
        vf_explained_var: -0.0037009119987487793
        vf_loss: 0.2969173789024353
      agent-4:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6780988574028015
        entropy_coeff: 0.0017600000137463212
        kl: 0.001920672133564949
        model: {}
        policy_loss: -0.004144642967730761
        total_loss: -0.005293854977935553
        vf_explained_var: -0.02426058053970337
        vf_loss: 0.442405641078949
      agent-5:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.975832462310791
        entropy_coeff: 0.0017600000137463212
        kl: 0.002072164323180914
        model: {}
        policy_loss: -0.0029001785442233086
        total_loss: -0.0046011535450816154
        vf_explained_var: -0.004674643278121948
        vf_loss: 0.16491533815860748
    load_time_ms: 13860.379
    num_steps_sampled: 21600000
    num_steps_trained: 21600000
    sample_time_ms: 100667.559
    update_time_ms: 16.834
  iterations_since_restore: 65
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.91611111111111
    ram_util_percent: 25.03166666666667
  pid: 27405
  policy_reward_max:
    agent-0: 15.0
    agent-1: 8.0
    agent-2: 16.0
    agent-3: 11.0
    agent-4: 13.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 3.93
    agent-1: 2.18
    agent-2: 4.95
    agent-3: 4.11
    agent-4: 4.9
    agent-5: 2.33
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.400607932585107
    mean_inference_ms: 13.34067276320922
    mean_processing_ms: 59.50220200631144
  time_since_restore: 8286.520027160645
  time_this_iter_s: 126.30012655258179
  time_total_s: 30796.80326962471
  timestamp: 1637228369
  timesteps_since_restore: 6240000
  timesteps_this_iter: 96000
  timesteps_total: 21600000
  training_iteration: 225
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 46.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    225 |          30796.8 | 21600000 |     22.4 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 2.47
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.97
    apples_agent-1_min: 0
    apples_agent-2_max: 8
    apples_agent-2_mean: 1.72
    apples_agent-2_min: 0
    apples_agent-3_max: 12
    apples_agent-3_mean: 2.78
    apples_agent-3_min: 0
    apples_agent-4_max: 70
    apples_agent-4_mean: 2.41
    apples_agent-4_min: 0
    apples_agent-5_max: 23
    apples_agent-5_mean: 1.71
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 163
    cleaning_beam_agent-0_mean: 53.66
    cleaning_beam_agent-0_min: 16
    cleaning_beam_agent-1_max: 369
    cleaning_beam_agent-1_mean: 266.49
    cleaning_beam_agent-1_min: 166
    cleaning_beam_agent-2_max: 42
    cleaning_beam_agent-2_mean: 16.62
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 157
    cleaning_beam_agent-3_mean: 64.36
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 131
    cleaning_beam_agent-4_mean: 63.85
    cleaning_beam_agent-4_min: 15
    cleaning_beam_agent-5_max: 64
    cleaning_beam_agent-5_mean: 17.67
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-41-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 47.0
  episode_reward_mean: 22.09
  episode_reward_min: -36.0
  episodes_this_iter: 96
  episodes_total: 21696
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12226.775
    learner:
      agent-0:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5214501619338989
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013023389037698507
        model: {}
        policy_loss: -0.0035488330759108067
        total_loss: -0.004429060034453869
        vf_explained_var: -0.022460103034973145
        vf_loss: 0.37525931000709534
      agent-1:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4165016710758209
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008814268512651324
        model: {}
        policy_loss: -0.0019346671178936958
        total_loss: -0.0026169600896537304
        vf_explained_var: 0.022102609276771545
        vf_loss: 0.5075020790100098
      agent-2:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4485500454902649
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014837891794741154
        model: {}
        policy_loss: -0.003823570441454649
        total_loss: -0.004568647593259811
        vf_explained_var: 0.0035663843154907227
        vf_loss: 0.44370096921920776
      agent-3:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6012325286865234
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010117624187842011
        model: {}
        policy_loss: -0.0018737189238891006
        total_loss: -0.0027683409862220287
        vf_explained_var: 0.0019513070583343506
        vf_loss: 1.635456919670105
      agent-4:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.679581880569458
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016366229392588139
        model: {}
        policy_loss: -0.003932430408895016
        total_loss: -0.005082143936306238
        vf_explained_var: 0.004972860217094421
        vf_loss: 0.463512122631073
      agent-5:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9692302942276001
        entropy_coeff: 0.0017600000137463212
        kl: 0.001658463035710156
        model: {}
        policy_loss: -0.002963278442621231
        total_loss: -0.004647905007004738
        vf_explained_var: -0.004606306552886963
        vf_loss: 0.21221286058425903
    load_time_ms: 13898.243
    num_steps_sampled: 21696000
    num_steps_trained: 21696000
    sample_time_ms: 100573.248
    update_time_ms: 16.625
  iterations_since_restore: 66
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.70386740331492
    ram_util_percent: 25.072928176795585
  pid: 27405
  policy_reward_max:
    agent-0: 15.0
    agent-1: 10.0
    agent-2: 15.0
    agent-3: 15.0
    agent-4: 16.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 4.13
    agent-1: 1.85
    agent-2: 4.94
    agent-3: 3.77
    agent-4: 4.96
    agent-5: 2.44
  policy_reward_min:
    agent-0: 0.0
    agent-1: -49.0
    agent-2: 0.0
    agent-3: -45.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.39651841295495
    mean_inference_ms: 13.339029282979782
    mean_processing_ms: 59.50209810735016
  time_since_restore: 8413.458749055862
  time_this_iter_s: 126.9387218952179
  time_total_s: 30923.741991519928
  timestamp: 1637228496
  timesteps_since_restore: 6336000
  timesteps_this_iter: 96000
  timesteps_total: 21696000
  training_iteration: 226
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 46.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    226 |          30923.7 | 21696000 |    22.09 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 2.57
    apples_agent-0_min: 0
    apples_agent-1_max: 8
    apples_agent-1_mean: 1.2
    apples_agent-1_min: 0
    apples_agent-2_max: 34
    apples_agent-2_mean: 2.65
    apples_agent-2_min: 0
    apples_agent-3_max: 12
    apples_agent-3_mean: 3.01
    apples_agent-3_min: 0
    apples_agent-4_max: 6
    apples_agent-4_mean: 0.96
    apples_agent-4_min: 0
    apples_agent-5_max: 43
    apples_agent-5_mean: 1.76
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 161
    cleaning_beam_agent-0_mean: 53.23
    cleaning_beam_agent-0_min: 14
    cleaning_beam_agent-1_max: 334
    cleaning_beam_agent-1_mean: 261.76
    cleaning_beam_agent-1_min: 177
    cleaning_beam_agent-2_max: 79
    cleaning_beam_agent-2_mean: 20.31
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 156
    cleaning_beam_agent-3_mean: 69.27
    cleaning_beam_agent-3_min: 20
    cleaning_beam_agent-4_max: 117
    cleaning_beam_agent-4_mean: 54.01
    cleaning_beam_agent-4_min: 18
    cleaning_beam_agent-5_max: 49
    cleaning_beam_agent-5_mean: 15.75
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-43-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 61.0
  episode_reward_mean: 23.39
  episode_reward_min: -31.0
  episodes_this_iter: 96
  episodes_total: 21792
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12247.931
    learner:
      agent-0:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5285540223121643
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013726723846048117
        model: {}
        policy_loss: -0.0022033723071217537
        total_loss: -0.0029631322249770164
        vf_explained_var: -0.00914543867111206
        vf_loss: 1.7049386501312256
      agent-1:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4170956313610077
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010210128966718912
        model: {}
        policy_loss: -0.0023928508162498474
        total_loss: -0.0031106818933039904
        vf_explained_var: 0.022410809993743896
        vf_loss: 0.16255244612693787
      agent-2:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46074536442756653
        entropy_coeff: 0.0017600000137463212
        kl: 0.00125669885892421
        model: {}
        policy_loss: -0.0031577213667333126
        total_loss: -0.003894337685778737
        vf_explained_var: -0.00302809476852417
        vf_loss: 0.7429666519165039
      agent-3:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6183170676231384
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017368841217830777
        model: {}
        policy_loss: -0.003173739416524768
        total_loss: -0.00422825338318944
        vf_explained_var: 0.005291596055030823
        vf_loss: 0.33726921677589417
      agent-4:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6727581024169922
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017631028313189745
        model: {}
        policy_loss: -0.004097233060747385
        total_loss: -0.005231509916484356
        vf_explained_var: -0.006454288959503174
        vf_loss: 0.4977904260158539
      agent-5:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9684044122695923
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021256129257380962
        model: {}
        policy_loss: -0.002814859850332141
        total_loss: -0.004503054078668356
        vf_explained_var: 0.011113598942756653
        vf_loss: 0.16197232902050018
    load_time_ms: 13897.943
    num_steps_sampled: 21792000
    num_steps_trained: 21792000
    sample_time_ms: 100644.101
    update_time_ms: 17.042
  iterations_since_restore: 67
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.48461538461539
    ram_util_percent: 25.13901098901099
  pid: 27405
  policy_reward_max:
    agent-0: 14.0
    agent-1: 9.0
    agent-2: 20.0
    agent-3: 15.0
    agent-4: 13.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 3.92
    agent-1: 2.41
    agent-2: 4.99
    agent-3: 4.34
    agent-4: 5.36
    agent-5: 2.37
  policy_reward_min:
    agent-0: -48.0
    agent-1: 0.0
    agent-2: -46.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.396112207477813
    mean_inference_ms: 13.338652769978967
    mean_processing_ms: 59.50751217098844
  time_since_restore: 8541.498782634735
  time_this_iter_s: 128.04003357887268
  time_total_s: 31051.7820250988
  timestamp: 1637228625
  timesteps_since_restore: 6432000
  timesteps_this_iter: 96000
  timesteps_total: 21792000
  training_iteration: 227
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 46.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    227 |          31051.8 | 21792000 |    23.39 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 2.49
    apples_agent-0_min: 0
    apples_agent-1_max: 10
    apples_agent-1_mean: 1.22
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 2.66
    apples_agent-2_min: 0
    apples_agent-3_max: 14
    apples_agent-3_mean: 3.04
    apples_agent-3_min: 0
    apples_agent-4_max: 14
    apples_agent-4_mean: 1.46
    apples_agent-4_min: 0
    apples_agent-5_max: 9
    apples_agent-5_mean: 1.79
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 209
    cleaning_beam_agent-0_mean: 59.28
    cleaning_beam_agent-0_min: 15
    cleaning_beam_agent-1_max: 388
    cleaning_beam_agent-1_mean: 263.04
    cleaning_beam_agent-1_min: 198
    cleaning_beam_agent-2_max: 165
    cleaning_beam_agent-2_mean: 20.03
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 149
    cleaning_beam_agent-3_mean: 72.94
    cleaning_beam_agent-3_min: 28
    cleaning_beam_agent-4_max: 159
    cleaning_beam_agent-4_mean: 56.81
    cleaning_beam_agent-4_min: 13
    cleaning_beam_agent-5_max: 55
    cleaning_beam_agent-5_mean: 16.99
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-45-52
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 69.0
  episode_reward_mean: 23.13
  episode_reward_min: -82.0
  episodes_this_iter: 96
  episodes_total: 21888
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12252.023
    learner:
      agent-0:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5305102467536926
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019376175478100777
        model: {}
        policy_loss: -0.0038844067603349686
        total_loss: -0.004773689899593592
        vf_explained_var: -0.023925572633743286
        vf_loss: 0.44418108463287354
      agent-1:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4198318421840668
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015951910754665732
        model: {}
        policy_loss: -0.0023938403464853764
        total_loss: -0.0031142421066761017
        vf_explained_var: 0.012123346328735352
        vf_loss: 0.18504612147808075
      agent-2:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46016401052474976
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012611813144758344
        model: {}
        policy_loss: -0.0031059817411005497
        total_loss: -0.0038382546044886112
        vf_explained_var: 0.009230345487594604
        vf_loss: 0.7761595845222473
      agent-3:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6176186800003052
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013107408303767443
        model: {}
        policy_loss: -0.003058271948248148
        total_loss: -0.0041111549362540245
        vf_explained_var: 0.00029027462005615234
        vf_loss: 0.3412640690803528
      agent-4:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6719903945922852
        entropy_coeff: 0.0017600000137463212
        kl: 0.001692918362095952
        model: {}
        policy_loss: -0.004116710275411606
        total_loss: -0.005254875868558884
        vf_explained_var: -0.00029440224170684814
        vf_loss: 0.4454139471054077
      agent-5:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9634094834327698
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018129335949197412
        model: {}
        policy_loss: -0.0028518280014395714
        total_loss: -0.004524931777268648
        vf_explained_var: 0.004434406757354736
        vf_loss: 0.22494836151599884
    load_time_ms: 13902.019
    num_steps_sampled: 21888000
    num_steps_trained: 21888000
    sample_time_ms: 100786.423
    update_time_ms: 16.913
  iterations_since_restore: 68
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.32527472527473
    ram_util_percent: 25.244505494505493
  pid: 27405
  policy_reward_max:
    agent-0: 19.0
    agent-1: 13.0
    agent-2: 28.0
    agent-3: 14.0
    agent-4: 13.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 3.68
    agent-1: 2.22
    agent-2: 6.16
    agent-3: 4.21
    agent-4: 4.13
    agent-5: 2.73
  policy_reward_min:
    agent-0: -50.0
    agent-1: 0.0
    agent-2: 1.0
    agent-3: -1.0
    agent-4: -45.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.396475156644293
    mean_inference_ms: 13.338738596356702
    mean_processing_ms: 59.52113233720755
  time_since_restore: 8669.123027086258
  time_this_iter_s: 127.62424445152283
  time_total_s: 31179.406269550323
  timestamp: 1637228752
  timesteps_since_restore: 6528000
  timesteps_this_iter: 96000
  timesteps_total: 21888000
  training_iteration: 228
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 46.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    228 |          31179.4 | 21888000 |    23.13 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 2.25
    apples_agent-0_min: 0
    apples_agent-1_max: 28
    apples_agent-1_mean: 1.22
    apples_agent-1_min: 0
    apples_agent-2_max: 19
    apples_agent-2_mean: 2.31
    apples_agent-2_min: 0
    apples_agent-3_max: 10
    apples_agent-3_mean: 2.92
    apples_agent-3_min: 0
    apples_agent-4_max: 5
    apples_agent-4_mean: 1.03
    apples_agent-4_min: 0
    apples_agent-5_max: 5
    apples_agent-5_mean: 1.36
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 139
    cleaning_beam_agent-0_mean: 57.14
    cleaning_beam_agent-0_min: 15
    cleaning_beam_agent-1_max: 389
    cleaning_beam_agent-1_mean: 256.91
    cleaning_beam_agent-1_min: 200
    cleaning_beam_agent-2_max: 44
    cleaning_beam_agent-2_mean: 18.07
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 171
    cleaning_beam_agent-3_mean: 70.67
    cleaning_beam_agent-3_min: 23
    cleaning_beam_agent-4_max: 129
    cleaning_beam_agent-4_mean: 54.79
    cleaning_beam_agent-4_min: 13
    cleaning_beam_agent-5_max: 60
    cleaning_beam_agent-5_mean: 18.9
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-48-01
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 48.0
  episode_reward_mean: 21.28
  episode_reward_min: -75.0
  episodes_this_iter: 96
  episodes_total: 21984
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12282.169
    learner:
      agent-0:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5250710248947144
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018702792003750801
        model: {}
        policy_loss: -0.003463977249339223
        total_loss: -0.004356866702437401
        vf_explained_var: -0.026712536811828613
        vf_loss: 0.31234753131866455
      agent-1:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42031043767929077
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007990286103449762
        model: {}
        policy_loss: -0.0020349579863250256
        total_loss: -0.002756496425718069
        vf_explained_var: 0.012425228953361511
        vf_loss: 0.18204955756664276
      agent-2:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4501364529132843
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014554172521457076
        model: {}
        policy_loss: -0.0013265390880405903
        total_loss: -0.001532214693725109
        vf_explained_var: -0.0004363507032394409
        vf_loss: 5.865625381469727
      agent-3:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6129217147827148
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013237118255347013
        model: {}
        policy_loss: -0.00337402056902647
        total_loss: -0.004418906755745411
        vf_explained_var: -0.0004000812768936157
        vf_loss: 0.3385808765888214
      agent-4:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6771125793457031
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014061866095289588
        model: {}
        policy_loss: -0.00419028103351593
        total_loss: -0.0053438167087733746
        vf_explained_var: 0.0035071372985839844
        vf_loss: 0.38183915615081787
      agent-5:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9551070332527161
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016818511066958308
        model: {}
        policy_loss: -0.002824043855071068
        total_loss: -0.004484598524868488
        vf_explained_var: 0.007589563727378845
        vf_loss: 0.204301118850708
    load_time_ms: 13937.888
    num_steps_sampled: 21984000
    num_steps_trained: 21984000
    sample_time_ms: 100864.918
    update_time_ms: 17.102
  iterations_since_restore: 69
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 46.92841530054645
    ram_util_percent: 21.62131147540984
  pid: 27405
  policy_reward_max:
    agent-0: 12.0
    agent-1: 11.0
    agent-2: 16.0
    agent-3: 12.0
    agent-4: 15.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 3.37
    agent-1: 2.13
    agent-2: 4.62
    agent-3: 4.02
    agent-4: 4.45
    agent-5: 2.69
  policy_reward_min:
    agent-0: -47.0
    agent-1: 0.0
    agent-2: -95.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.396662693808242
    mean_inference_ms: 13.33896898435287
    mean_processing_ms: 59.53430487151313
  time_since_restore: 8797.917935848236
  time_this_iter_s: 128.79490876197815
  time_total_s: 31308.2011783123
  timestamp: 1637228881
  timesteps_since_restore: 6624000
  timesteps_this_iter: 96000
  timesteps_total: 21984000
  training_iteration: 229
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 36.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    229 |          31308.2 | 21984000 |    21.28 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 2.54
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 1.21
    apples_agent-1_min: 0
    apples_agent-2_max: 18
    apples_agent-2_mean: 2.52
    apples_agent-2_min: 0
    apples_agent-3_max: 14
    apples_agent-3_mean: 2.89
    apples_agent-3_min: 0
    apples_agent-4_max: 10
    apples_agent-4_mean: 1.23
    apples_agent-4_min: 0
    apples_agent-5_max: 32
    apples_agent-5_mean: 1.93
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 145
    cleaning_beam_agent-0_mean: 58.87
    cleaning_beam_agent-0_min: 16
    cleaning_beam_agent-1_max: 336
    cleaning_beam_agent-1_mean: 257.99
    cleaning_beam_agent-1_min: 165
    cleaning_beam_agent-2_max: 53
    cleaning_beam_agent-2_mean: 17.35
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 140
    cleaning_beam_agent-3_mean: 63.81
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 146
    cleaning_beam_agent-4_mean: 53.22
    cleaning_beam_agent-4_min: 18
    cleaning_beam_agent-5_max: 63
    cleaning_beam_agent-5_mean: 19.72
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 3
    fire_beam_agent-3_mean: 0.04
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-50-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 76.0
  episode_reward_mean: 21.6
  episode_reward_min: -139.0
  episodes_this_iter: 96
  episodes_total: 22080
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12300.088
    learner:
      agent-0:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5342143774032593
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011782437795773149
        model: {}
        policy_loss: -0.0037715360522270203
        total_loss: -0.004672369919717312
        vf_explained_var: -0.026926398277282715
        vf_loss: 0.3938234746456146
      agent-1:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4177977442741394
        entropy_coeff: 0.0017600000137463212
        kl: 0.0004123511607758701
        model: {}
        policy_loss: -0.00080704758875072
        total_loss: -0.0013908848632127047
        vf_explained_var: 0.011074528098106384
        vf_loss: 1.514861822128296
      agent-2:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44691529870033264
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017135539092123508
        model: {}
        policy_loss: -0.0028777550905942917
        total_loss: -0.003388182260096073
        vf_explained_var: 0.002044960856437683
        vf_loss: 2.761453628540039
      agent-3:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6102579236030579
        entropy_coeff: 0.0017600000137463212
        kl: 0.001134204096160829
        model: {}
        policy_loss: -0.002702513011172414
        total_loss: -0.0037379388231784105
        vf_explained_var: 0.0029828697443008423
        vf_loss: 0.38627904653549194
      agent-4:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6917716860771179
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017277089646086097
        model: {}
        policy_loss: -0.004363023675978184
        total_loss: -0.005535361357033253
        vf_explained_var: 0.003967389464378357
        vf_loss: 0.45181724429130554
      agent-5:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9485862255096436
        entropy_coeff: 0.0017600000137463212
        kl: 0.003220393555238843
        model: {}
        policy_loss: -0.0020025717094540596
        total_loss: -0.0035176253877580166
        vf_explained_var: -0.003240048885345459
        vf_loss: 1.5446035861968994
    load_time_ms: 13957.021
    num_steps_sampled: 22080000
    num_steps_trained: 22080000
    sample_time_ms: 100870.159
    update_time_ms: 16.872
  iterations_since_restore: 70
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 48.60384615384616
    ram_util_percent: 19.465384615384618
  pid: 27405
  policy_reward_max:
    agent-0: 12.0
    agent-1: 9.0
    agent-2: 17.0
    agent-3: 21.0
    agent-4: 14.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 4.3
    agent-1: 1.85
    agent-2: 4.37
    agent-3: 3.6
    agent-4: 5.28
    agent-5: 2.2
  policy_reward_min:
    agent-0: 0.0
    agent-1: -50.0
    agent-2: -48.0
    agent-3: -47.0
    agent-4: 0.0
    agent-5: -48.0
  sampler_perf:
    mean_env_wait_ms: 23.393269522774194
    mean_inference_ms: 13.33868141958592
    mean_processing_ms: 59.53572713622225
  time_since_restore: 8925.537906646729
  time_this_iter_s: 127.61997079849243
  time_total_s: 31435.821149110794
  timestamp: 1637229009
  timesteps_since_restore: 6720000
  timesteps_this_iter: 96000
  timesteps_total: 22080000
  training_iteration: 230
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 36.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    230 |          31435.8 | 22080000 |     21.6 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 111
    apples_agent-0_mean: 3.47
    apples_agent-0_min: 0
    apples_agent-1_max: 9
    apples_agent-1_mean: 1.24
    apples_agent-1_min: 0
    apples_agent-2_max: 9
    apples_agent-2_mean: 1.87
    apples_agent-2_min: 0
    apples_agent-3_max: 27
    apples_agent-3_mean: 3.01
    apples_agent-3_min: 0
    apples_agent-4_max: 69
    apples_agent-4_mean: 1.78
    apples_agent-4_min: 0
    apples_agent-5_max: 40
    apples_agent-5_mean: 2.01
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 149
    cleaning_beam_agent-0_mean: 60.99
    cleaning_beam_agent-0_min: 16
    cleaning_beam_agent-1_max: 350
    cleaning_beam_agent-1_mean: 264.01
    cleaning_beam_agent-1_min: 186
    cleaning_beam_agent-2_max: 70
    cleaning_beam_agent-2_mean: 19.66
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 147
    cleaning_beam_agent-3_mean: 61.45
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 141
    cleaning_beam_agent-4_mean: 59.02
    cleaning_beam_agent-4_min: 16
    cleaning_beam_agent-5_max: 56
    cleaning_beam_agent-5_mean: 16.91
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-52-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 53.0
  episode_reward_mean: 21.81
  episode_reward_min: -88.0
  episodes_this_iter: 96
  episodes_total: 22176
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12293.96
    learner:
      agent-0:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5311114192008972
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016281516291201115
        model: {}
        policy_loss: -0.003772507421672344
        total_loss: -0.004673551768064499
        vf_explained_var: -0.02517572045326233
        vf_loss: 0.33714932203292847
      agent-1:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.429382860660553
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019483560463413596
        model: {}
        policy_loss: -0.0028748116455972195
        total_loss: -0.003359213937073946
        vf_explained_var: 0.0002018660306930542
        vf_loss: 2.7131237983703613
      agent-2:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4527431130409241
        entropy_coeff: 0.0017600000137463212
        kl: 0.001056163921020925
        model: {}
        policy_loss: -0.0035824663937091827
        total_loss: -0.004335436038672924
        vf_explained_var: -0.0014095604419708252
        vf_loss: 0.4385744035243988
      agent-3:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6083611249923706
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012600496411323547
        model: {}
        policy_loss: -0.003256707452237606
        total_loss: -0.00429114792495966
        vf_explained_var: -0.0010828226804733276
        vf_loss: 0.36273670196533203
      agent-4:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6760692000389099
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014966020826250315
        model: {}
        policy_loss: -0.0039065214805305
        total_loss: -0.005046876147389412
        vf_explained_var: 0.0032842010259628296
        vf_loss: 0.4952855110168457
      agent-5:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9472620487213135
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016227703308686614
        model: {}
        policy_loss: -0.0022350363433361053
        total_loss: -0.0037602554075419903
        vf_explained_var: 0.0008518248796463013
        vf_loss: 1.4195986986160278
    load_time_ms: 13941.236
    num_steps_sampled: 22176000
    num_steps_trained: 22176000
    sample_time_ms: 100872.615
    update_time_ms: 17.451
  iterations_since_restore: 71
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.72044198895028
    ram_util_percent: 19.760220994475137
  pid: 27405
  policy_reward_max:
    agent-0: 12.0
    agent-1: 10.0
    agent-2: 18.0
    agent-3: 12.0
    agent-4: 13.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 3.3
    agent-1: 1.24
    agent-2: 5.25
    agent-3: 4.34
    agent-4: 5.46
    agent-5: 2.22
  policy_reward_min:
    agent-0: -47.0
    agent-1: -49.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -46.0
  sampler_perf:
    mean_env_wait_ms: 23.391438824671756
    mean_inference_ms: 13.338464168499698
    mean_processing_ms: 59.53560693268122
  time_since_restore: 9051.881128787994
  time_this_iter_s: 126.34322214126587
  time_total_s: 31562.16437125206
  timestamp: 1637229136
  timesteps_since_restore: 6816000
  timesteps_this_iter: 96000
  timesteps_total: 22176000
  training_iteration: 231
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 36.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    231 |          31562.2 | 22176000 |    21.81 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 2.73
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 1.07
    apples_agent-1_min: 0
    apples_agent-2_max: 15
    apples_agent-2_mean: 2.34
    apples_agent-2_min: 0
    apples_agent-3_max: 15
    apples_agent-3_mean: 3.26
    apples_agent-3_min: 0
    apples_agent-4_max: 7
    apples_agent-4_mean: 0.97
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 1.52
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 100
    cleaning_beam_agent-0_mean: 55.07
    cleaning_beam_agent-0_min: 14
    cleaning_beam_agent-1_max: 352
    cleaning_beam_agent-1_mean: 259.76
    cleaning_beam_agent-1_min: 179
    cleaning_beam_agent-2_max: 87
    cleaning_beam_agent-2_mean: 20.75
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 144
    cleaning_beam_agent-3_mean: 68.41
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 116
    cleaning_beam_agent-4_mean: 55.7
    cleaning_beam_agent-4_min: 13
    cleaning_beam_agent-5_max: 63
    cleaning_beam_agent-5_mean: 18.43
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-54-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 58.0
  episode_reward_mean: 25.48
  episode_reward_min: 4.0
  episodes_this_iter: 96
  episodes_total: 22272
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12288.503
    learner:
      agent-0:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5202128887176514
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018146294169127941
        model: {}
        policy_loss: -0.003436783794313669
        total_loss: -0.004311682656407356
        vf_explained_var: -0.014223426580429077
        vf_loss: 0.40680745244026184
      agent-1:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4176247715950012
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011817995691671968
        model: {}
        policy_loss: -0.0025390610098838806
        total_loss: -0.003254855051636696
        vf_explained_var: 0.024002358317375183
        vf_loss: 0.1922653317451477
      agent-2:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4510349929332733
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013591006863862276
        model: {}
        policy_loss: -0.0037993602454662323
        total_loss: -0.004530994221568108
        vf_explained_var: 0.0030388087034225464
        vf_loss: 0.6218628287315369
      agent-3:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6124656796455383
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013825638452544808
        model: {}
        policy_loss: -0.0031032210681587458
        total_loss: -0.004144232254475355
        vf_explained_var: 0.004010215401649475
        vf_loss: 0.3692835569381714
      agent-4:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6709261536598206
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020969053730368614
        model: {}
        policy_loss: -0.0040305787697434425
        total_loss: -0.005152644123882055
        vf_explained_var: -0.005995512008666992
        vf_loss: 0.5876579284667969
      agent-5:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.968008279800415
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016124993562698364
        model: {}
        policy_loss: -0.002714888658374548
        total_loss: -0.004393702372908592
        vf_explained_var: 0.000929906964302063
        vf_loss: 0.24882936477661133
    load_time_ms: 13947.168
    num_steps_sampled: 22272000
    num_steps_trained: 22272000
    sample_time_ms: 100930.535
    update_time_ms: 18.158
  iterations_since_restore: 72
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.93833333333334
    ram_util_percent: 19.985
  pid: 27405
  policy_reward_max:
    agent-0: 12.0
    agent-1: 11.0
    agent-2: 25.0
    agent-3: 15.0
    agent-4: 16.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 4.4
    agent-1: 2.28
    agent-2: 5.6
    agent-3: 4.38
    agent-4: 5.65
    agent-5: 3.17
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.389593138923264
    mean_inference_ms: 13.337931387190741
    mean_processing_ms: 59.53219685635774
  time_since_restore: 9178.21330332756
  time_this_iter_s: 126.33217453956604
  time_total_s: 31688.496545791626
  timestamp: 1637229263
  timesteps_since_restore: 6912000
  timesteps_this_iter: 96000
  timesteps_total: 22272000
  training_iteration: 232
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 37.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    232 |          31688.5 | 22272000 |    25.48 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 2.56
    apples_agent-0_min: 0
    apples_agent-1_max: 29
    apples_agent-1_mean: 1.4
    apples_agent-1_min: 0
    apples_agent-2_max: 42
    apples_agent-2_mean: 2.47
    apples_agent-2_min: 0
    apples_agent-3_max: 11
    apples_agent-3_mean: 2.73
    apples_agent-3_min: 0
    apples_agent-4_max: 17
    apples_agent-4_mean: 1.21
    apples_agent-4_min: 0
    apples_agent-5_max: 10
    apples_agent-5_mean: 1.58
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 140
    cleaning_beam_agent-0_mean: 57.98
    cleaning_beam_agent-0_min: 15
    cleaning_beam_agent-1_max: 352
    cleaning_beam_agent-1_mean: 266.61
    cleaning_beam_agent-1_min: 121
    cleaning_beam_agent-2_max: 58
    cleaning_beam_agent-2_mean: 18.32
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 128
    cleaning_beam_agent-3_mean: 68.89
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 116
    cleaning_beam_agent-4_mean: 52.31
    cleaning_beam_agent-4_min: 13
    cleaning_beam_agent-5_max: 75
    cleaning_beam_agent-5_mean: 18.3
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-56-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 67.0
  episode_reward_mean: 23.36
  episode_reward_min: 3.0
  episodes_this_iter: 96
  episodes_total: 22368
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12302.634
    learner:
      agent-0:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5218815803527832
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018134834244847298
        model: {}
        policy_loss: -0.0032718649599701166
        total_loss: -0.00415408331900835
        vf_explained_var: -0.03443557024002075
        vf_loss: 0.3629147410392761
      agent-1:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42349040508270264
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011375386966392398
        model: {}
        policy_loss: -0.0024982173927128315
        total_loss: -0.0032255679834634066
        vf_explained_var: 0.019240841269493103
        vf_loss: 0.17990773916244507
      agent-2:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4469098150730133
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017772583523765206
        model: {}
        policy_loss: -0.0037930700927972794
        total_loss: -0.004519079811871052
        vf_explained_var: 0.012168601155281067
        vf_loss: 0.6055259108543396
      agent-3:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6218461990356445
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013158542569726706
        model: {}
        policy_loss: -0.0029935382772237062
        total_loss: -0.004055933095514774
        vf_explained_var: 0.002080768346786499
        vf_loss: 0.32056114077568054
      agent-4:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6714279651641846
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012531181564554572
        model: {}
        policy_loss: -0.0036341696977615356
        total_loss: -0.004770852159708738
        vf_explained_var: -0.00244024395942688
        vf_loss: 0.45030730962753296
      agent-5:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9760380983352661
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014738249592483044
        model: {}
        policy_loss: -0.0028642606921494007
        total_loss: -0.004562202841043472
        vf_explained_var: 0.008793607354164124
        vf_loss: 0.19886377453804016
    load_time_ms: 13971.913
    num_steps_sampled: 22368000
    num_steps_trained: 22368000
    sample_time_ms: 100963.965
    update_time_ms: 17.989
  iterations_since_restore: 73
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.175824175824175
    ram_util_percent: 20.230219780219784
  pid: 27405
  policy_reward_max:
    agent-0: 14.0
    agent-1: 9.0
    agent-2: 19.0
    agent-3: 12.0
    agent-4: 18.0
    agent-5: 18.0
  policy_reward_mean:
    agent-0: 3.96
    agent-1: 2.1
    agent-2: 5.74
    agent-3: 4.05
    agent-4: 4.75
    agent-5: 2.76
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.388431755513007
    mean_inference_ms: 13.336818902620289
    mean_processing_ms: 59.53723791329896
  time_since_restore: 9306.302842140198
  time_this_iter_s: 128.08953881263733
  time_total_s: 31816.586084604263
  timestamp: 1637229391
  timesteps_since_restore: 7008000
  timesteps_this_iter: 96000
  timesteps_total: 22368000
  training_iteration: 233
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 37.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    233 |          31816.6 | 22368000 |    23.36 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 2.53
    apples_agent-0_min: 0
    apples_agent-1_max: 28
    apples_agent-1_mean: 1.91
    apples_agent-1_min: 0
    apples_agent-2_max: 40
    apples_agent-2_mean: 2.4
    apples_agent-2_min: 0
    apples_agent-3_max: 13
    apples_agent-3_mean: 3.47
    apples_agent-3_min: 0
    apples_agent-4_max: 62
    apples_agent-4_mean: 1.79
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 1.63
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 155
    cleaning_beam_agent-0_mean: 57.7
    cleaning_beam_agent-0_min: 15
    cleaning_beam_agent-1_max: 354
    cleaning_beam_agent-1_mean: 249.54
    cleaning_beam_agent-1_min: 139
    cleaning_beam_agent-2_max: 72
    cleaning_beam_agent-2_mean: 16.76
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 151
    cleaning_beam_agent-3_mean: 66.91
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 204
    cleaning_beam_agent-4_mean: 58.43
    cleaning_beam_agent-4_min: 17
    cleaning_beam_agent-5_max: 61
    cleaning_beam_agent-5_mean: 18.58
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-58-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 52.0
  episode_reward_mean: 24.63
  episode_reward_min: 9.0
  episodes_this_iter: 96
  episodes_total: 22464
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12297.573
    learner:
      agent-0:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5071532130241394
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014572263462468982
        model: {}
        policy_loss: -0.0035450716968625784
        total_loss: -0.004404215142130852
        vf_explained_var: -0.009971916675567627
        vf_loss: 0.3344539701938629
      agent-1:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41925057768821716
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013745886972174048
        model: {}
        policy_loss: -0.0025241710245609283
        total_loss: -0.0032465378753840923
        vf_explained_var: 0.019485950469970703
        vf_loss: 0.15515854954719543
      agent-2:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.436181902885437
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012204747181385756
        model: {}
        policy_loss: -0.003458199091255665
        total_loss: -0.004168834537267685
        vf_explained_var: 0.0013641268014907837
        vf_loss: 0.5704641342163086
      agent-3:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6185892820358276
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011818744242191315
        model: {}
        policy_loss: -0.0030494220554828644
        total_loss: -0.004101654514670372
        vf_explained_var: 0.004170581698417664
        vf_loss: 0.3648577928543091
      agent-4:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6784740686416626
        entropy_coeff: 0.0017600000137463212
        kl: 0.001955105923116207
        model: {}
        policy_loss: -0.004258071072399616
        total_loss: -0.005402904003858566
        vf_explained_var: -0.007635548710823059
        vf_loss: 0.492840439081192
      agent-5:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9675418138504028
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017741457559168339
        model: {}
        policy_loss: -0.0026970040053129196
        total_loss: -0.00437996257096529
        vf_explained_var: 0.0020467787981033325
        vf_loss: 0.19918164610862732
    load_time_ms: 13982.877
    num_steps_sampled: 22464000
    num_steps_trained: 22464000
    sample_time_ms: 100909.403
    update_time_ms: 18.704
  iterations_since_restore: 74
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.52209944751381
    ram_util_percent: 20.48121546961326
  pid: 27405
  policy_reward_max:
    agent-0: 12.0
    agent-1: 8.0
    agent-2: 16.0
    agent-3: 10.0
    agent-4: 14.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 4.19
    agent-1: 2.14
    agent-2: 5.5
    agent-3: 4.39
    agent-4: 5.5
    agent-5: 2.91
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 1.0
    agent-3: 0.0
    agent-4: 1.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.38819616491603
    mean_inference_ms: 13.337220134392494
    mean_processing_ms: 59.541977347460914
  time_since_restore: 9433.27093076706
  time_this_iter_s: 126.96808862686157
  time_total_s: 31943.554173231125
  timestamp: 1637229518
  timesteps_since_restore: 7104000
  timesteps_this_iter: 96000
  timesteps_total: 22464000
  training_iteration: 234
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 37.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    234 |          31943.6 | 22464000 |    24.63 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 2.29
    apples_agent-0_min: 0
    apples_agent-1_max: 10
    apples_agent-1_mean: 1.33
    apples_agent-1_min: 0
    apples_agent-2_max: 70
    apples_agent-2_mean: 2.98
    apples_agent-2_min: 0
    apples_agent-3_max: 26
    apples_agent-3_mean: 3.47
    apples_agent-3_min: 0
    apples_agent-4_max: 14
    apples_agent-4_mean: 1.29
    apples_agent-4_min: 0
    apples_agent-5_max: 15
    apples_agent-5_mean: 1.86
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 115
    cleaning_beam_agent-0_mean: 55.25
    cleaning_beam_agent-0_min: 15
    cleaning_beam_agent-1_max: 356
    cleaning_beam_agent-1_mean: 250.56
    cleaning_beam_agent-1_min: 183
    cleaning_beam_agent-2_max: 86
    cleaning_beam_agent-2_mean: 20.88
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 163
    cleaning_beam_agent-3_mean: 72.96
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 152
    cleaning_beam_agent-4_mean: 52.42
    cleaning_beam_agent-4_min: 14
    cleaning_beam_agent-5_max: 48
    cleaning_beam_agent-5_mean: 19.02
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-00-44
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 60.0
  episode_reward_mean: 23.96
  episode_reward_min: 9.0
  episodes_this_iter: 96
  episodes_total: 22560
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12272.492
    learner:
      agent-0:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.508973240852356
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014028716832399368
        model: {}
        policy_loss: -0.003635719418525696
        total_loss: -0.004501740913838148
        vf_explained_var: -0.02935922145843506
        vf_loss: 0.2977083921432495
      agent-1:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41858965158462524
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009658812778070569
        model: {}
        policy_loss: -0.0024966932833194733
        total_loss: -0.0032128808088600636
        vf_explained_var: 0.03444169461727142
        vf_loss: 0.2053365409374237
      agent-2:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4517972469329834
        entropy_coeff: 0.0017600000137463212
        kl: 0.001263447804376483
        model: {}
        policy_loss: -0.003496930468827486
        total_loss: -0.00424208864569664
        vf_explained_var: 0.0022476911544799805
        vf_loss: 0.5000636577606201
      agent-3:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6242903470993042
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010517619084566832
        model: {}
        policy_loss: -0.0027848081663250923
        total_loss: -0.0038486123085021973
        vf_explained_var: 1.9222497940063477e-05
        vf_loss: 0.3494505286216736
      agent-4:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.678362250328064
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015446121105924249
        model: {}
        policy_loss: -0.0036610173992812634
        total_loss: -0.004812877159565687
        vf_explained_var: -0.00947996973991394
        vf_loss: 0.42056983709335327
      agent-5:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9644136428833008
        entropy_coeff: 0.0017600000137463212
        kl: 0.002200720366090536
        model: {}
        policy_loss: -0.0026998338289558887
        total_loss: -0.0043749515898525715
        vf_explained_var: 0.002316385507583618
        vf_loss: 0.22249093651771545
    load_time_ms: 13965.902
    num_steps_sampled: 22560000
    num_steps_trained: 22560000
    sample_time_ms: 100937.412
    update_time_ms: 18.961
  iterations_since_restore: 75
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.68944444444445
    ram_util_percent: 20.648333333333333
  pid: 27405
  policy_reward_max:
    agent-0: 11.0
    agent-1: 11.0
    agent-2: 15.0
    agent-3: 13.0
    agent-4: 14.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 4.09
    agent-1: 2.42
    agent-2: 5.3
    agent-3: 4.28
    agent-4: 4.95
    agent-5: 2.92
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.385259480375126
    mean_inference_ms: 13.336990325130076
    mean_processing_ms: 59.54066943175921
  time_since_restore: 9559.433394432068
  time_this_iter_s: 126.16246366500854
  time_total_s: 32069.716636896133
  timestamp: 1637229644
  timesteps_since_restore: 7200000
  timesteps_this_iter: 96000
  timesteps_total: 22560000
  training_iteration: 235
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 38.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    235 |          32069.7 | 22560000 |    23.96 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 21
    apples_agent-0_mean: 3.0
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 1.21
    apples_agent-1_min: 0
    apples_agent-2_max: 15
    apples_agent-2_mean: 2.42
    apples_agent-2_min: 0
    apples_agent-3_max: 10
    apples_agent-3_mean: 2.84
    apples_agent-3_min: 0
    apples_agent-4_max: 11
    apples_agent-4_mean: 1.04
    apples_agent-4_min: 0
    apples_agent-5_max: 14
    apples_agent-5_mean: 1.58
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 229
    cleaning_beam_agent-0_mean: 59.31
    cleaning_beam_agent-0_min: 17
    cleaning_beam_agent-1_max: 417
    cleaning_beam_agent-1_mean: 251.24
    cleaning_beam_agent-1_min: 160
    cleaning_beam_agent-2_max: 47
    cleaning_beam_agent-2_mean: 17.74
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 134
    cleaning_beam_agent-3_mean: 67.02
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 115
    cleaning_beam_agent-4_mean: 49.06
    cleaning_beam_agent-4_min: 15
    cleaning_beam_agent-5_max: 58
    cleaning_beam_agent-5_mean: 19.52
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-02-52
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 50.0
  episode_reward_mean: 23.63
  episode_reward_min: 5.0
  episodes_this_iter: 96
  episodes_total: 22656
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12251.35
    learner:
      agent-0:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.508470356464386
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017274124547839165
        model: {}
        policy_loss: -0.003638448193669319
        total_loss: -0.0044981976971030235
        vf_explained_var: -0.016200780868530273
        vf_loss: 0.3515690565109253
      agent-1:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4233132600784302
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011959176044911146
        model: {}
        policy_loss: -0.0027258163318037987
        total_loss: -0.0034555336460471153
        vf_explained_var: 0.021712526679039
        vf_loss: 0.15317657589912415
      agent-2:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4530428946018219
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010110675357282162
        model: {}
        policy_loss: -0.0032643084414303303
        total_loss: -0.004012791905552149
        vf_explained_var: 0.00834868848323822
        vf_loss: 0.48874354362487793
      agent-3:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6184995174407959
        entropy_coeff: 0.0017600000137463212
        kl: 0.001616742112673819
        model: {}
        policy_loss: -0.003168393624946475
        total_loss: -0.004220731556415558
        vf_explained_var: -0.001257583498954773
        vf_loss: 0.3622130751609802
      agent-4:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.666502833366394
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014886489370837808
        model: {}
        policy_loss: -0.003605298697948456
        total_loss: -0.004734651651233435
        vf_explained_var: 0.0015375465154647827
        vf_loss: 0.43693384528160095
      agent-5:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9626089334487915
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014131385833024979
        model: {}
        policy_loss: -0.0027201916091144085
        total_loss: -0.004393697250634432
        vf_explained_var: 0.010434985160827637
        vf_loss: 0.20689471065998077
    load_time_ms: 13942.745
    num_steps_sampled: 22656000
    num_steps_trained: 22656000
    sample_time_ms: 101001.469
    update_time_ms: 18.893
  iterations_since_restore: 76
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.35027624309392
    ram_util_percent: 20.811602209944756
  pid: 27405
  policy_reward_max:
    agent-0: 12.0
    agent-1: 9.0
    agent-2: 20.0
    agent-3: 13.0
    agent-4: 15.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 4.24
    agent-1: 2.19
    agent-2: 5.37
    agent-3: 4.08
    agent-4: 4.9
    agent-5: 2.85
  policy_reward_min:
    agent-0: 0.0
    agent-1: -1.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.382403178019526
    mean_inference_ms: 13.336522248399937
    mean_processing_ms: 59.54289578468501
  time_since_restore: 9686.580065965652
  time_this_iter_s: 127.1466715335846
  time_total_s: 32196.863308429718
  timestamp: 1637229772
  timesteps_since_restore: 7296000
  timesteps_this_iter: 96000
  timesteps_total: 22656000
  training_iteration: 236
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 38.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    236 |          32196.9 | 22656000 |    23.63 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 2.74
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 1.15
    apples_agent-1_min: 0
    apples_agent-2_max: 32
    apples_agent-2_mean: 2.73
    apples_agent-2_min: 0
    apples_agent-3_max: 172
    apples_agent-3_mean: 5.16
    apples_agent-3_min: 0
    apples_agent-4_max: 10
    apples_agent-4_mean: 1.47
    apples_agent-4_min: 0
    apples_agent-5_max: 8
    apples_agent-5_mean: 1.84
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 118
    cleaning_beam_agent-0_mean: 56.43
    cleaning_beam_agent-0_min: 17
    cleaning_beam_agent-1_max: 370
    cleaning_beam_agent-1_mean: 248.34
    cleaning_beam_agent-1_min: 158
    cleaning_beam_agent-2_max: 49
    cleaning_beam_agent-2_mean: 17.97
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 158
    cleaning_beam_agent-3_mean: 61.4
    cleaning_beam_agent-3_min: 20
    cleaning_beam_agent-4_max: 132
    cleaning_beam_agent-4_mean: 49.6
    cleaning_beam_agent-4_min: 13
    cleaning_beam_agent-5_max: 56
    cleaning_beam_agent-5_mean: 20.52
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-04-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 64.0
  episode_reward_mean: 24.74
  episode_reward_min: -36.0
  episodes_this_iter: 96
  episodes_total: 22752
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12213.625
    learner:
      agent-0:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5040525197982788
        entropy_coeff: 0.0017600000137463212
        kl: 0.001616313704289496
        model: {}
        policy_loss: -0.002365310676395893
        total_loss: -0.003092661965638399
        vf_explained_var: -0.009884178638458252
        vf_loss: 1.597778081893921
      agent-1:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4263325333595276
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010643524583429098
        model: {}
        policy_loss: -0.0025808075442910194
        total_loss: -0.003318449482321739
        vf_explained_var: 0.019907444715499878
        vf_loss: 0.12702083587646484
      agent-2:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4456580877304077
        entropy_coeff: 0.0017600000137463212
        kl: 0.001146333059296012
        model: {}
        policy_loss: -0.0037138587795197964
        total_loss: -0.0044486247934401035
        vf_explained_var: 0.0009538233280181885
        vf_loss: 0.4959348440170288
      agent-3:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6133134365081787
        entropy_coeff: 0.0017600000137463212
        kl: 0.001540901605039835
        model: {}
        policy_loss: -0.003453013952821493
        total_loss: -0.004496002569794655
        vf_explained_var: 0.0021670013666152954
        vf_loss: 0.3644540309906006
      agent-4:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6779772639274597
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016646893927827477
        model: {}
        policy_loss: -0.004067686386406422
        total_loss: -0.005216876044869423
        vf_explained_var: -0.003913387656211853
        vf_loss: 0.4405258297920227
      agent-5:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9653388857841492
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019105116371065378
        model: {}
        policy_loss: -0.003098921151831746
        total_loss: -0.0047755166888237
        vf_explained_var: 0.01265755295753479
        vf_loss: 0.2240133136510849
    load_time_ms: 13929.265
    num_steps_sampled: 22752000
    num_steps_trained: 22752000
    sample_time_ms: 100895.552
    update_time_ms: 18.616
  iterations_since_restore: 77
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.736111111111114
    ram_util_percent: 21.03777777777778
  pid: 27405
  policy_reward_max:
    agent-0: 13.0
    agent-1: 6.0
    agent-2: 17.0
    agent-3: 14.0
    agent-4: 16.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 4.13
    agent-1: 2.06
    agent-2: 5.7
    agent-3: 4.46
    agent-4: 5.39
    agent-5: 3.0
  policy_reward_min:
    agent-0: -47.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.37948831963519
    mean_inference_ms: 13.335275661278793
    mean_processing_ms: 59.54279747733312
  time_since_restore: 9813.04656124115
  time_this_iter_s: 126.46649527549744
  time_total_s: 32323.329803705215
  timestamp: 1637229898
  timesteps_since_restore: 7392000
  timesteps_this_iter: 96000
  timesteps_total: 22752000
  training_iteration: 237
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 39.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    237 |          32323.3 | 22752000 |    24.74 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 2.58
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 1.23
    apples_agent-1_min: 0
    apples_agent-2_max: 18
    apples_agent-2_mean: 2.01
    apples_agent-2_min: 0
    apples_agent-3_max: 27
    apples_agent-3_mean: 3.29
    apples_agent-3_min: 0
    apples_agent-4_max: 21
    apples_agent-4_mean: 1.66
    apples_agent-4_min: 0
    apples_agent-5_max: 48
    apples_agent-5_mean: 2.1
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 120
    cleaning_beam_agent-0_mean: 52.12
    cleaning_beam_agent-0_min: 9
    cleaning_beam_agent-1_max: 351
    cleaning_beam_agent-1_mean: 242.19
    cleaning_beam_agent-1_min: 159
    cleaning_beam_agent-2_max: 48
    cleaning_beam_agent-2_mean: 20.73
    cleaning_beam_agent-2_min: 6
    cleaning_beam_agent-3_max: 147
    cleaning_beam_agent-3_mean: 59.44
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 129
    cleaning_beam_agent-4_mean: 46.06
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 71
    cleaning_beam_agent-5_mean: 20.65
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 3
    fire_beam_agent-0_mean: 0.08
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-07-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 54.0
  episode_reward_mean: 21.86
  episode_reward_min: -77.0
  episodes_this_iter: 96
  episodes_total: 22848
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12210.373
    learner:
      agent-0:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5056383609771729
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014736396260559559
        model: {}
        policy_loss: -0.003486527595669031
        total_loss: -0.004347754176706076
        vf_explained_var: -0.029700130224227905
        vf_loss: 0.2869858145713806
      agent-1:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4309374988079071
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011130734346807003
        model: {}
        policy_loss: -0.0024151159450411797
        total_loss: -0.003157694824039936
        vf_explained_var: 0.03170764446258545
        vf_loss: 0.15869687497615814
      agent-2:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46385645866394043
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013268840266391635
        model: {}
        policy_loss: -0.0022426587529480457
        total_loss: -0.002960360608994961
        vf_explained_var: 0.009859919548034668
        vf_loss: 0.9868709444999695
      agent-3:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.605358362197876
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014695129357278347
        model: {}
        policy_loss: -0.00286873709410429
        total_loss: -0.0038976925425231457
        vf_explained_var: 0.0043925940990448
        vf_loss: 0.3647427260875702
      agent-4:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6724554300308228
        entropy_coeff: 0.0017600000137463212
        kl: 0.001321552786976099
        model: {}
        policy_loss: -0.0030775240156799555
        total_loss: -0.004083937965333462
        vf_explained_var: 0.00160139799118042
        vf_loss: 1.771056890487671
      agent-5:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9614859819412231
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010250889463350177
        model: {}
        policy_loss: -0.0025084717199206352
        total_loss: -0.004177932161837816
        vf_explained_var: 0.006758600473403931
        vf_loss: 0.22753438353538513
    load_time_ms: 13914.299
    num_steps_sampled: 22848000
    num_steps_trained: 22848000
    sample_time_ms: 100703.603
    update_time_ms: 18.544
  iterations_since_restore: 78
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.889944134078206
    ram_util_percent: 21.2804469273743
  pid: 27405
  policy_reward_max:
    agent-0: 12.0
    agent-1: 7.0
    agent-2: 17.0
    agent-3: 10.0
    agent-4: 16.0
    agent-5: 14.0
  policy_reward_mean:
    agent-0: 3.81
    agent-1: 2.02
    agent-2: 4.37
    agent-3: 4.33
    agent-4: 4.59
    agent-5: 2.74
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -94.0
    agent-3: 0.0
    agent-4: -42.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.372962638201017
    mean_inference_ms: 13.33475042605173
    mean_processing_ms: 59.53947555451538
  time_since_restore: 9938.54640007019
  time_this_iter_s: 125.49983882904053
  time_total_s: 32448.829642534256
  timestamp: 1637230024
  timesteps_since_restore: 7488000
  timesteps_this_iter: 96000
  timesteps_total: 22848000
  training_iteration: 238
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 39.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    238 |          32448.8 | 22848000 |    21.86 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 2.64
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 1.22
    apples_agent-1_min: 0
    apples_agent-2_max: 46
    apples_agent-2_mean: 2.69
    apples_agent-2_min: 0
    apples_agent-3_max: 11
    apples_agent-3_mean: 3.01
    apples_agent-3_min: 0
    apples_agent-4_max: 4
    apples_agent-4_mean: 1.06
    apples_agent-4_min: 0
    apples_agent-5_max: 10
    apples_agent-5_mean: 1.66
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 95
    cleaning_beam_agent-0_mean: 51.67
    cleaning_beam_agent-0_min: 15
    cleaning_beam_agent-1_max: 397
    cleaning_beam_agent-1_mean: 249.33
    cleaning_beam_agent-1_min: 188
    cleaning_beam_agent-2_max: 65
    cleaning_beam_agent-2_mean: 21.73
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 152
    cleaning_beam_agent-3_mean: 57.13
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 167
    cleaning_beam_agent-4_mean: 49.3
    cleaning_beam_agent-4_min: 7
    cleaning_beam_agent-5_max: 56
    cleaning_beam_agent-5_mean: 21.81
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-09-10
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 56.0
  episode_reward_mean: 21.06
  episode_reward_min: -77.0
  episodes_this_iter: 96
  episodes_total: 22944
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12193.185
    learner:
      agent-0:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49358096718788147
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014512886991724372
        model: {}
        policy_loss: -0.0021523854229599237
        total_loss: -0.0029094782657921314
        vf_explained_var: -0.007878512144088745
        vf_loss: 1.1161201000213623
      agent-1:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4229976236820221
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012421516003087163
        model: {}
        policy_loss: -0.0029792720451951027
        total_loss: -0.0036069373600184917
        vf_explained_var: 0.012830451130867004
        vf_loss: 1.1681280136108398
      agent-2:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4536508619785309
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012825315352529287
        model: {}
        policy_loss: -0.001747580012306571
        total_loss: -0.002372274873778224
        vf_explained_var: 0.0017067790031433105
        vf_loss: 1.7373191118240356
      agent-3:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5995329022407532
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013682396383956075
        model: {}
        policy_loss: -0.0028963694348931313
        total_loss: -0.0039061736315488815
        vf_explained_var: 0.0031998753547668457
        vf_loss: 0.4537503123283386
      agent-4:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6821856498718262
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013073401059955359
        model: {}
        policy_loss: -0.0022448392119258642
        total_loss: -0.0033033518120646477
        vf_explained_var: -0.003130599856376648
        vf_loss: 1.4213330745697021
      agent-5:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9508298635482788
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019256377127021551
        model: {}
        policy_loss: -0.0025431467220187187
        total_loss: -0.0041946289129555225
        vf_explained_var: 0.004156753420829773
        vf_loss: 0.21980968117713928
    load_time_ms: 13856.29
    num_steps_sampled: 22944000
    num_steps_trained: 22944000
    sample_time_ms: 100560.817
    update_time_ms: 18.4
  iterations_since_restore: 79
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.89833333333334
    ram_util_percent: 21.401111111111113
  pid: 27405
  policy_reward_max:
    agent-0: 15.0
    agent-1: 9.0
    agent-2: 17.0
    agent-3: 17.0
    agent-4: 12.0
    agent-5: 14.0
  policy_reward_mean:
    agent-0: 3.52
    agent-1: 1.64
    agent-2: 4.58
    agent-3: 4.52
    agent-4: 4.18
    agent-5: 2.62
  policy_reward_min:
    agent-0: -44.0
    agent-1: -48.0
    agent-2: -44.0
    agent-3: 0.0
    agent-4: -44.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.3702812872285
    mean_inference_ms: 13.33445174377516
    mean_processing_ms: 59.538338647056236
  time_since_restore: 10065.14333319664
  time_this_iter_s: 126.59693312644958
  time_total_s: 32575.426575660706
  timestamp: 1637230150
  timesteps_since_restore: 7584000
  timesteps_this_iter: 96000
  timesteps_total: 22944000
  training_iteration: 239
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 39.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    239 |          32575.4 | 22944000 |    21.06 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 23
    apples_agent-0_mean: 2.14
    apples_agent-0_min: 0
    apples_agent-1_max: 17
    apples_agent-1_mean: 1.26
    apples_agent-1_min: 0
    apples_agent-2_max: 23
    apples_agent-2_mean: 2.48
    apples_agent-2_min: 0
    apples_agent-3_max: 42
    apples_agent-3_mean: 3.63
    apples_agent-3_min: 0
    apples_agent-4_max: 21
    apples_agent-4_mean: 1.28
    apples_agent-4_min: 0
    apples_agent-5_max: 13
    apples_agent-5_mean: 1.56
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 109
    cleaning_beam_agent-0_mean: 50.67
    cleaning_beam_agent-0_min: 18
    cleaning_beam_agent-1_max: 376
    cleaning_beam_agent-1_mean: 247.43
    cleaning_beam_agent-1_min: 124
    cleaning_beam_agent-2_max: 134
    cleaning_beam_agent-2_mean: 20.35
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 135
    cleaning_beam_agent-3_mean: 55.83
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 146
    cleaning_beam_agent-4_mean: 47.83
    cleaning_beam_agent-4_min: 14
    cleaning_beam_agent-5_max: 63
    cleaning_beam_agent-5_mean: 24.53
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-11-17
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 80.0
  episode_reward_mean: 19.46
  episode_reward_min: -86.0
  episodes_this_iter: 96
  episodes_total: 23040
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12174.064
    learner:
      agent-0:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5148783922195435
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014063098933547735
        model: {}
        policy_loss: -0.003602681215852499
        total_loss: -0.004347181413322687
        vf_explained_var: -0.0038863271474838257
        vf_loss: 1.6168633699417114
      agent-1:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4217703938484192
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009962976910173893
        model: {}
        policy_loss: -0.0019475319422781467
        total_loss: -0.002668990520760417
        vf_explained_var: 0.023985043168067932
        vf_loss: 0.20857039093971252
      agent-2:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4618203938007355
        entropy_coeff: 0.0017600000137463212
        kl: 0.002049960196018219
        model: {}
        policy_loss: -0.0025700137484818697
        total_loss: -0.0032103760167956352
        vf_explained_var: 0.0017419308423995972
        vf_loss: 1.724407434463501
      agent-3:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.607848584651947
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012258855858817697
        model: {}
        policy_loss: -0.0024705880787223577
        total_loss: -0.0034967262763530016
        vf_explained_var: 0.009410783648490906
        vf_loss: 0.43675899505615234
      agent-4:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6818809509277344
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014393194578588009
        model: {}
        policy_loss: -0.0034753973595798016
        total_loss: -0.004499412607401609
        vf_explained_var: -0.001511693000793457
        vf_loss: 1.7609515190124512
      agent-5:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9616172313690186
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015451995423063636
        model: {}
        policy_loss: -0.0027010596822947264
        total_loss: -0.004369370173662901
        vf_explained_var: 0.002524659037590027
        vf_loss: 0.24133466184139252
    load_time_ms: 13821.298
    num_steps_sampled: 23040000
    num_steps_trained: 23040000
    sample_time_ms: 100496.096
    update_time_ms: 18.351
  iterations_since_restore: 80
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.775555555555556
    ram_util_percent: 21.630555555555556
  pid: 27405
  policy_reward_max:
    agent-0: 12.0
    agent-1: 10.0
    agent-2: 16.0
    agent-3: 18.0
    agent-4: 27.0
    agent-5: 14.0
  policy_reward_mean:
    agent-0: 3.28
    agent-1: 1.17
    agent-2: 3.06
    agent-3: 4.16
    agent-4: 5.01
    agent-5: 2.78
  policy_reward_min:
    agent-0: -48.0
    agent-1: -49.0
    agent-2: -46.0
    agent-3: 0.0
    agent-4: -40.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.367359790650447
    mean_inference_ms: 13.334006670198065
    mean_processing_ms: 59.53697202170874
  time_since_restore: 10191.533401966095
  time_this_iter_s: 126.39006876945496
  time_total_s: 32701.81664443016
  timestamp: 1637230277
  timesteps_since_restore: 7680000
  timesteps_this_iter: 96000
  timesteps_total: 23040000
  training_iteration: 240
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 40.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    240 |          32701.8 | 23040000 |    19.46 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 70
    apples_agent-0_mean: 3.54
    apples_agent-0_min: 0
    apples_agent-1_max: 85
    apples_agent-1_mean: 2.47
    apples_agent-1_min: 0
    apples_agent-2_max: 7
    apples_agent-2_mean: 1.91
    apples_agent-2_min: 0
    apples_agent-3_max: 21
    apples_agent-3_mean: 3.32
    apples_agent-3_min: 0
    apples_agent-4_max: 23
    apples_agent-4_mean: 1.68
    apples_agent-4_min: 0
    apples_agent-5_max: 23
    apples_agent-5_mean: 2.06
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 112
    cleaning_beam_agent-0_mean: 46.98
    cleaning_beam_agent-0_min: 19
    cleaning_beam_agent-1_max: 369
    cleaning_beam_agent-1_mean: 237.01
    cleaning_beam_agent-1_min: 164
    cleaning_beam_agent-2_max: 52
    cleaning_beam_agent-2_mean: 19.35
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 140
    cleaning_beam_agent-3_mean: 57.6
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 158
    cleaning_beam_agent-4_mean: 50.38
    cleaning_beam_agent-4_min: 8
    cleaning_beam_agent-5_max: 75
    cleaning_beam_agent-5_mean: 21.14
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-13-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 85.0
  episode_reward_mean: 24.89
  episode_reward_min: 1.0
  episodes_this_iter: 96
  episodes_total: 23136
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12171.537
    learner:
      agent-0:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5088580846786499
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012380103580653667
        model: {}
        policy_loss: -0.0031408481299877167
        total_loss: -0.0039965687319636345
        vf_explained_var: -0.020016223192214966
        vf_loss: 0.39868906140327454
      agent-1:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4228242039680481
        entropy_coeff: 0.0017600000137463212
        kl: 0.000932794064283371
        model: {}
        policy_loss: -0.0025608460418879986
        total_loss: -0.0032864802051335573
        vf_explained_var: 0.026591330766677856
        vf_loss: 0.1853182017803192
      agent-2:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4525514543056488
        entropy_coeff: 0.0017600000137463212
        kl: 0.001274342997930944
        model: {}
        policy_loss: -0.0035758549347519875
        total_loss: -0.0043251775205135345
        vf_explained_var: 0.001184418797492981
        vf_loss: 0.4716581702232361
      agent-3:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6191523671150208
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011462786933407187
        model: {}
        policy_loss: -0.0029818054754287004
        total_loss: -0.004027652088552713
        vf_explained_var: -0.001838788390159607
        vf_loss: 0.43856918811798096
      agent-4:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6718096733093262
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015023141168057919
        model: {}
        policy_loss: -0.003780393861234188
        total_loss: -0.004911230411380529
        vf_explained_var: 0.020496606826782227
        vf_loss: 0.5154719352722168
      agent-5:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9474191069602966
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013761066365987062
        model: {}
        policy_loss: -0.0027716420590877533
        total_loss: -0.0044193146750330925
        vf_explained_var: 0.00715404748916626
        vf_loss: 0.19786515831947327
    load_time_ms: 13826.736
    num_steps_sampled: 23136000
    num_steps_trained: 23136000
    sample_time_ms: 100436.352
    update_time_ms: 17.585
  iterations_since_restore: 81
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.59723756906077
    ram_util_percent: 21.775690607734806
  pid: 27405
  policy_reward_max:
    agent-0: 17.0
    agent-1: 10.0
    agent-2: 16.0
    agent-3: 18.0
    agent-4: 21.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 4.1
    agent-1: 2.43
    agent-2: 5.15
    agent-3: 4.71
    agent-4: 5.65
    agent-5: 2.85
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.36271706638391
    mean_inference_ms: 13.33492336528277
    mean_processing_ms: 59.53736707878286
  time_since_restore: 10317.337036371231
  time_this_iter_s: 125.80363440513611
  time_total_s: 32827.6202788353
  timestamp: 1637230404
  timesteps_since_restore: 7776000
  timesteps_this_iter: 96000
  timesteps_total: 23136000
  training_iteration: 241
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 40.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    241 |          32827.6 | 23136000 |    24.89 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 8
    apples_agent-0_mean: 2.44
    apples_agent-0_min: 0
    apples_agent-1_max: 16
    apples_agent-1_mean: 1.41
    apples_agent-1_min: 0
    apples_agent-2_max: 43
    apples_agent-2_mean: 3.07
    apples_agent-2_min: 0
    apples_agent-3_max: 20
    apples_agent-3_mean: 2.77
    apples_agent-3_min: 0
    apples_agent-4_max: 12
    apples_agent-4_mean: 1.14
    apples_agent-4_min: 0
    apples_agent-5_max: 13
    apples_agent-5_mean: 2.19
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 94
    cleaning_beam_agent-0_mean: 44.76
    cleaning_beam_agent-0_min: 10
    cleaning_beam_agent-1_max: 383
    cleaning_beam_agent-1_mean: 243.16
    cleaning_beam_agent-1_min: 162
    cleaning_beam_agent-2_max: 85
    cleaning_beam_agent-2_mean: 21.22
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 144
    cleaning_beam_agent-3_mean: 63.76
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 213
    cleaning_beam_agent-4_mean: 53.48
    cleaning_beam_agent-4_min: 11
    cleaning_beam_agent-5_max: 54
    cleaning_beam_agent-5_mean: 19.5
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 3
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-15-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 60.0
  episode_reward_mean: 22.09
  episode_reward_min: -20.0
  episodes_this_iter: 96
  episodes_total: 23232
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12185.583
    learner:
      agent-0:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5028449892997742
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012709113070741296
        model: {}
        policy_loss: -0.0033942931331694126
        total_loss: -0.004250139929354191
        vf_explained_var: -0.017855793237686157
        vf_loss: 0.29158151149749756
      agent-1:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4201395511627197
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010324072791263461
        model: {}
        policy_loss: -0.0023434022441506386
        total_loss: -0.0030699819326400757
        vf_explained_var: 0.025293514132499695
        vf_loss: 0.1286398321390152
      agent-2:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4541141390800476
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014264725614339113
        model: {}
        policy_loss: -0.0037138122133910656
        total_loss: -0.004457315430045128
        vf_explained_var: -0.006106913089752197
        vf_loss: 0.5573740601539612
      agent-3:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6093780994415283
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011204160982742906
        model: {}
        policy_loss: -0.0023132613860070705
        total_loss: -0.0032209509517997503
        vf_explained_var: 0.0035194605588912964
        vf_loss: 1.6481752395629883
      agent-4:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6810520887374878
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012232696171849966
        model: {}
        policy_loss: -0.003500509774312377
        total_loss: -0.0046515315771102905
        vf_explained_var: 0.0015159696340560913
        vf_loss: 0.4763033986091614
      agent-5:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9446231722831726
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015148079255595803
        model: {}
        policy_loss: -0.0024478542618453503
        total_loss: -0.004085763357579708
        vf_explained_var: 0.0007583200931549072
        vf_loss: 0.24628838896751404
    load_time_ms: 13838.826
    num_steps_sampled: 23232000
    num_steps_trained: 23232000
    sample_time_ms: 100505.821
    update_time_ms: 16.883
  iterations_since_restore: 82
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.58895027624309
    ram_util_percent: 21.989502762430938
  pid: 27405
  policy_reward_max:
    agent-0: 14.0
    agent-1: 7.0
    agent-2: 24.0
    agent-3: 19.0
    agent-4: 13.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 3.66
    agent-1: 1.99
    agent-2: 5.2
    agent-3: 3.58
    agent-4: 4.7
    agent-5: 2.96
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -44.0
    agent-4: -35.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.35956359031729
    mean_inference_ms: 13.334634508585795
    mean_processing_ms: 59.541038908191695
  time_since_restore: 10444.590920686722
  time_this_iter_s: 127.25388431549072
  time_total_s: 32954.87416315079
  timestamp: 1637230531
  timesteps_since_restore: 7872000
  timesteps_this_iter: 96000
  timesteps_total: 23232000
  training_iteration: 242
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 40.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    242 |          32954.9 | 23232000 |    22.09 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 14
    apples_agent-0_mean: 2.56
    apples_agent-0_min: 0
    apples_agent-1_max: 201
    apples_agent-1_mean: 3.06
    apples_agent-1_min: 0
    apples_agent-2_max: 25
    apples_agent-2_mean: 2.44
    apples_agent-2_min: 0
    apples_agent-3_max: 33
    apples_agent-3_mean: 3.1
    apples_agent-3_min: 0
    apples_agent-4_max: 10
    apples_agent-4_mean: 1.47
    apples_agent-4_min: 0
    apples_agent-5_max: 8
    apples_agent-5_mean: 1.9
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 250
    cleaning_beam_agent-0_mean: 47.53
    cleaning_beam_agent-0_min: 5
    cleaning_beam_agent-1_max: 393
    cleaning_beam_agent-1_mean: 241.06
    cleaning_beam_agent-1_min: 161
    cleaning_beam_agent-2_max: 76
    cleaning_beam_agent-2_mean: 19.43
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 121
    cleaning_beam_agent-3_mean: 60.63
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 125
    cleaning_beam_agent-4_mean: 47.32
    cleaning_beam_agent-4_min: 10
    cleaning_beam_agent-5_max: 53
    cleaning_beam_agent-5_mean: 21.09
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 3
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-17-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 69.0
  episode_reward_mean: 23.05
  episode_reward_min: -20.0
  episodes_this_iter: 96
  episodes_total: 23328
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12154.873
    learner:
      agent-0:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5100871324539185
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012238118797540665
        model: {}
        policy_loss: -0.0030996371060609818
        total_loss: -0.0039583779871463776
        vf_explained_var: -0.004004180431365967
        vf_loss: 0.39010196924209595
      agent-1:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4175211191177368
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010289184283465147
        model: {}
        policy_loss: -0.002394065260887146
        total_loss: -0.0031155517790466547
        vf_explained_var: 0.02383314073085785
        vf_loss: 0.13348959386348724
      agent-2:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45429298281669617
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014165230095386505
        model: {}
        policy_loss: -0.0036223214119672775
        total_loss: -0.004359540529549122
        vf_explained_var: -0.006060540676116943
        vf_loss: 0.6233782768249512
      agent-3:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6029548645019531
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015216331230476499
        model: {}
        policy_loss: -0.0031205713748931885
        total_loss: -0.0041495622135698795
        vf_explained_var: 0.002801761031150818
        vf_loss: 0.3220835328102112
      agent-4:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6881442666053772
        entropy_coeff: 0.0017600000137463212
        kl: 0.001956585329025984
        model: {}
        policy_loss: -0.0042040301486849785
        total_loss: -0.005371781997382641
        vf_explained_var: 0.00903850793838501
        vf_loss: 0.4338478744029999
      agent-5:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9711080193519592
        entropy_coeff: 0.0017600000137463212
        kl: 0.002095340983942151
        model: {}
        policy_loss: -0.002585012000054121
        total_loss: -0.004267686977982521
        vf_explained_var: 0.006076052784919739
        vf_loss: 0.26471227407455444
    load_time_ms: 13799.58
    num_steps_sampled: 23328000
    num_steps_trained: 23328000
    sample_time_ms: 100327.95
    update_time_ms: 16.709
  iterations_since_restore: 83
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.840223463687146
    ram_util_percent: 22.086033519553073
  pid: 27405
  policy_reward_max:
    agent-0: 16.0
    agent-1: 7.0
    agent-2: 23.0
    agent-3: 11.0
    agent-4: 12.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 4.29
    agent-1: 1.96
    agent-2: 5.33
    agent-3: 3.53
    agent-4: 4.78
    agent-5: 3.16
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -44.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.356207673242594
    mean_inference_ms: 13.333761847880844
    mean_processing_ms: 59.539724247034485
  time_since_restore: 10570.132617235184
  time_this_iter_s: 125.54169654846191
  time_total_s: 33080.41585969925
  timestamp: 1637230657
  timesteps_since_restore: 7968000
  timesteps_this_iter: 96000
  timesteps_total: 23328000
  training_iteration: 243
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 41.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    243 |          33080.4 | 23328000 |    23.05 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 2.39
    apples_agent-0_min: 0
    apples_agent-1_max: 24
    apples_agent-1_mean: 1.56
    apples_agent-1_min: 0
    apples_agent-2_max: 9
    apples_agent-2_mean: 2.14
    apples_agent-2_min: 0
    apples_agent-3_max: 57
    apples_agent-3_mean: 3.4
    apples_agent-3_min: 0
    apples_agent-4_max: 16
    apples_agent-4_mean: 1.58
    apples_agent-4_min: 0
    apples_agent-5_max: 14
    apples_agent-5_mean: 1.92
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 90
    cleaning_beam_agent-0_mean: 43.99
    cleaning_beam_agent-0_min: 7
    cleaning_beam_agent-1_max: 393
    cleaning_beam_agent-1_mean: 238.91
    cleaning_beam_agent-1_min: 146
    cleaning_beam_agent-2_max: 50
    cleaning_beam_agent-2_mean: 22.03
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 160
    cleaning_beam_agent-3_mean: 65.51
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 123
    cleaning_beam_agent-4_mean: 48.59
    cleaning_beam_agent-4_min: 13
    cleaning_beam_agent-5_max: 56
    cleaning_beam_agent-5_mean: 20.28
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-19-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 63.0
  episode_reward_mean: 24.61
  episode_reward_min: -27.0
  episodes_this_iter: 96
  episodes_total: 23424
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12161.908
    learner:
      agent-0:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5084066987037659
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008944523287937045
        model: {}
        policy_loss: -0.003154648467898369
        total_loss: -0.00400935485959053
        vf_explained_var: -0.025272876024246216
        vf_loss: 0.40090295672416687
      agent-1:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4267015755176544
        entropy_coeff: 0.0017600000137463212
        kl: 0.000989978201687336
        model: {}
        policy_loss: -0.0024331894237548113
        total_loss: -0.003168803174048662
        vf_explained_var: 0.025528356432914734
        vf_loss: 0.15383008122444153
      agent-2:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4559570252895355
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013620945392176509
        model: {}
        policy_loss: -0.0035544971469789743
        total_loss: -0.004297330044209957
        vf_explained_var: 0.0018312782049179077
        vf_loss: 0.5965040326118469
      agent-3:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6141242980957031
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013468367978930473
        model: {}
        policy_loss: -0.002724899910390377
        total_loss: -0.0037662130780518055
        vf_explained_var: 0.008096292614936829
        vf_loss: 0.3954888582229614
      agent-4:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6779467463493347
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018179615726694465
        model: {}
        policy_loss: -0.003845975501462817
        total_loss: -0.004984703846275806
        vf_explained_var: -0.003049418330192566
        vf_loss: 0.5445785522460938
      agent-5:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9399764537811279
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015212533762678504
        model: {}
        policy_loss: -0.0025320504792034626
        total_loss: -0.004160119686275721
        vf_explained_var: 0.005408972501754761
        vf_loss: 0.2629072368144989
    load_time_ms: 13803.347
    num_steps_sampled: 23424000
    num_steps_trained: 23424000
    sample_time_ms: 100255.171
    update_time_ms: 16.029
  iterations_since_restore: 84
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.90277777777778
    ram_util_percent: 22.33111111111111
  pid: 27405
  policy_reward_max:
    agent-0: 14.0
    agent-1: 7.0
    agent-2: 15.0
    agent-3: 15.0
    agent-4: 17.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 4.15
    agent-1: 2.06
    agent-2: 5.59
    agent-3: 4.02
    agent-4: 5.69
    agent-5: 3.1
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -48.0
    agent-4: 0.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 23.351144973201894
    mean_inference_ms: 13.332875743164509
    mean_processing_ms: 59.53553839447134
  time_since_restore: 10696.50779390335
  time_this_iter_s: 126.37517666816711
  time_total_s: 33206.79103636742
  timestamp: 1637230783
  timesteps_since_restore: 8064000
  timesteps_this_iter: 96000
  timesteps_total: 23424000
  training_iteration: 244
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 41.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    244 |          33206.8 | 23424000 |    24.61 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 2.68
    apples_agent-0_min: 0
    apples_agent-1_max: 9
    apples_agent-1_mean: 1.23
    apples_agent-1_min: 0
    apples_agent-2_max: 28
    apples_agent-2_mean: 2.87
    apples_agent-2_min: 0
    apples_agent-3_max: 57
    apples_agent-3_mean: 3.81
    apples_agent-3_min: 0
    apples_agent-4_max: 36
    apples_agent-4_mean: 1.69
    apples_agent-4_min: 0
    apples_agent-5_max: 13
    apples_agent-5_mean: 2.12
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 98
    cleaning_beam_agent-0_mean: 42.34
    cleaning_beam_agent-0_min: 13
    cleaning_beam_agent-1_max: 295
    cleaning_beam_agent-1_mean: 236.11
    cleaning_beam_agent-1_min: 146
    cleaning_beam_agent-2_max: 52
    cleaning_beam_agent-2_mean: 19.67
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 160
    cleaning_beam_agent-3_mean: 70.45
    cleaning_beam_agent-3_min: 26
    cleaning_beam_agent-4_max: 212
    cleaning_beam_agent-4_mean: 53.12
    cleaning_beam_agent-4_min: 7
    cleaning_beam_agent-5_max: 48
    cleaning_beam_agent-5_mean: 19.46
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-21-50
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 64.0
  episode_reward_mean: 24.31
  episode_reward_min: -40.0
  episodes_this_iter: 96
  episodes_total: 23520
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12171.938
    learner:
      agent-0:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4979402422904968
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012029273202642798
        model: {}
        policy_loss: -0.003353449981659651
        total_loss: -0.004191782791167498
        vf_explained_var: -0.017847329378128052
        vf_loss: 0.38041627407073975
      agent-1:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.427693247795105
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009682223899289966
        model: {}
        policy_loss: -0.0023799173068255186
        total_loss: -0.0031158956699073315
        vf_explained_var: 0.01761731505393982
        vf_loss: 0.16764092445373535
      agent-2:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4601622223854065
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012316335923969746
        model: {}
        policy_loss: -0.003594172652810812
        total_loss: -0.004353231284767389
        vf_explained_var: -4.3511390686035156e-05
        vf_loss: 0.5082664489746094
      agent-3:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6196280717849731
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012598608154803514
        model: {}
        policy_loss: -0.002788027748465538
        total_loss: -0.0038422993384301662
        vf_explained_var: -0.0032989829778671265
        vf_loss: 0.3626844584941864
      agent-4:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6858757734298706
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019275887170806527
        model: {}
        policy_loss: -0.00425006402656436
        total_loss: -0.00540191400796175
        vf_explained_var: -0.00022277235984802246
        vf_loss: 0.5528885722160339
      agent-5:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9338808655738831
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018476755358278751
        model: {}
        policy_loss: -0.0014152952935546637
        total_loss: -0.0029058409854769707
        vf_explained_var: 0.0028803199529647827
        vf_loss: 1.5308504104614258
    load_time_ms: 13795.742
    num_steps_sampled: 23520000
    num_steps_trained: 23520000
    sample_time_ms: 100331.089
    update_time_ms: 15.551
  iterations_since_restore: 85
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.58618784530388
    ram_util_percent: 22.469613259668503
  pid: 27405
  policy_reward_max:
    agent-0: 12.0
    agent-1: 11.0
    agent-2: 16.0
    agent-3: 16.0
    agent-4: 18.0
    agent-5: 14.0
  policy_reward_mean:
    agent-0: 3.74
    agent-1: 2.2
    agent-2: 5.39
    agent-3: 4.46
    agent-4: 5.77
    agent-5: 2.75
  policy_reward_min:
    agent-0: -49.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -47.0
  sampler_perf:
    mean_env_wait_ms: 23.347761799544188
    mean_inference_ms: 13.332865978371409
    mean_processing_ms: 59.53817862796429
  time_since_restore: 10823.442903518677
  time_this_iter_s: 126.93510961532593
  time_total_s: 33333.72614598274
  timestamp: 1637230910
  timesteps_since_restore: 8160000
  timesteps_this_iter: 96000
  timesteps_total: 23520000
  training_iteration: 245
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 41.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    245 |          33333.7 | 23520000 |    24.31 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 21
    apples_agent-0_mean: 2.99
    apples_agent-0_min: 0
    apples_agent-1_max: 11
    apples_agent-1_mean: 1.09
    apples_agent-1_min: 0
    apples_agent-2_max: 24
    apples_agent-2_mean: 2.21
    apples_agent-2_min: 0
    apples_agent-3_max: 18
    apples_agent-3_mean: 3.3
    apples_agent-3_min: 0
    apples_agent-4_max: 37
    apples_agent-4_mean: 1.67
    apples_agent-4_min: 0
    apples_agent-5_max: 10
    apples_agent-5_mean: 1.52
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 108
    cleaning_beam_agent-0_mean: 48.24
    cleaning_beam_agent-0_min: 12
    cleaning_beam_agent-1_max: 318
    cleaning_beam_agent-1_mean: 245.89
    cleaning_beam_agent-1_min: 163
    cleaning_beam_agent-2_max: 52
    cleaning_beam_agent-2_mean: 18.6
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 145
    cleaning_beam_agent-3_mean: 68.51
    cleaning_beam_agent-3_min: 26
    cleaning_beam_agent-4_max: 147
    cleaning_beam_agent-4_mean: 50.87
    cleaning_beam_agent-4_min: 5
    cleaning_beam_agent-5_max: 63
    cleaning_beam_agent-5_mean: 23.75
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-23-56
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 57.0
  episode_reward_mean: 24.99
  episode_reward_min: 4.0
  episodes_this_iter: 96
  episodes_total: 23616
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12181.276
    learner:
      agent-0:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5058314800262451
        entropy_coeff: 0.0017600000137463212
        kl: 0.001759763341397047
        model: {}
        policy_loss: -0.0033569661900401115
        total_loss: -0.004203006625175476
        vf_explained_var: -0.01326662302017212
        vf_loss: 0.44223153591156006
      agent-1:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4190322458744049
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011146249016746879
        model: {}
        policy_loss: -0.0024598105810582638
        total_loss: -0.003178644459694624
        vf_explained_var: 0.028174355626106262
        vf_loss: 0.18660204112529755
      agent-2:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44609975814819336
        entropy_coeff: 0.0017600000137463212
        kl: 0.001256624236702919
        model: {}
        policy_loss: -0.0035066541749984026
        total_loss: -0.00423171604052186
        vf_explained_var: -0.003979027271270752
        vf_loss: 0.6007367372512817
      agent-3:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6190655827522278
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009053812245838344
        model: {}
        policy_loss: -0.0028557470068335533
        total_loss: -0.0039043230935931206
        vf_explained_var: 0.0028986483812332153
        vf_loss: 0.40980225801467896
      agent-4:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6899422407150269
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019982457160949707
        model: {}
        policy_loss: -0.004030782729387283
        total_loss: -0.005189995281398296
        vf_explained_var: -0.009030774235725403
        vf_loss: 0.5508495569229126
      agent-5:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.947773814201355
        entropy_coeff: 0.0017600000137463212
        kl: 0.001462227781303227
        model: {}
        policy_loss: -0.0028152940794825554
        total_loss: -0.00445576710626483
        vf_explained_var: 0.0006488710641860962
        vf_loss: 0.2760746479034424
    load_time_ms: 13781.588
    num_steps_sampled: 23616000
    num_steps_trained: 23616000
    sample_time_ms: 100218.142
    update_time_ms: 15.612
  iterations_since_restore: 86
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.832960893854754
    ram_util_percent: 22.5536312849162
  pid: 27405
  policy_reward_max:
    agent-0: 18.0
    agent-1: 11.0
    agent-2: 25.0
    agent-3: 16.0
    agent-4: 13.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 4.42
    agent-1: 2.38
    agent-2: 5.59
    agent-3: 4.26
    agent-4: 5.35
    agent-5: 2.99
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 1.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.344650070900865
    mean_inference_ms: 13.332045912124949
    mean_processing_ms: 59.538786703954266
  time_since_restore: 10949.387258291245
  time_this_iter_s: 125.94435477256775
  time_total_s: 33459.67050075531
  timestamp: 1637231036
  timesteps_since_restore: 8256000
  timesteps_this_iter: 96000
  timesteps_total: 23616000
  training_iteration: 246
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 42.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    246 |          33459.7 | 23616000 |    24.99 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 21
    apples_agent-0_mean: 2.57
    apples_agent-0_min: 0
    apples_agent-1_max: 9
    apples_agent-1_mean: 1.23
    apples_agent-1_min: 0
    apples_agent-2_max: 18
    apples_agent-2_mean: 2.27
    apples_agent-2_min: 0
    apples_agent-3_max: 16
    apples_agent-3_mean: 3.42
    apples_agent-3_min: 0
    apples_agent-4_max: 32
    apples_agent-4_mean: 1.7
    apples_agent-4_min: 0
    apples_agent-5_max: 13
    apples_agent-5_mean: 1.94
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 103
    cleaning_beam_agent-0_mean: 48.81
    cleaning_beam_agent-0_min: 11
    cleaning_beam_agent-1_max: 312
    cleaning_beam_agent-1_mean: 236.74
    cleaning_beam_agent-1_min: 161
    cleaning_beam_agent-2_max: 87
    cleaning_beam_agent-2_mean: 21.4
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 169
    cleaning_beam_agent-3_mean: 62.14
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 131
    cleaning_beam_agent-4_mean: 53.22
    cleaning_beam_agent-4_min: 11
    cleaning_beam_agent-5_max: 73
    cleaning_beam_agent-5_mean: 23.1
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-26-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 63.0
  episode_reward_mean: 24.35
  episode_reward_min: -29.0
  episodes_this_iter: 96
  episodes_total: 23712
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12194.609
    learner:
      agent-0:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5079438090324402
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010818219743669033
        model: {}
        policy_loss: -0.0034238744992762804
        total_loss: -0.004284074530005455
        vf_explained_var: -0.01844090223312378
        vf_loss: 0.33782362937927246
      agent-1:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4251033663749695
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007812305120751262
        model: {}
        policy_loss: -0.0017758123576641083
        total_loss: -0.0023840973153710365
        vf_explained_var: 0.01093660295009613
        vf_loss: 1.3989802598953247
      agent-2:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45393431186676025
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014069235185161233
        model: {}
        policy_loss: -0.0035477369092404842
        total_loss: -0.004297144245356321
        vf_explained_var: 0.003699585795402527
        vf_loss: 0.49514931440353394
      agent-3:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6081688404083252
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009509564843028784
        model: {}
        policy_loss: -0.0027622999623417854
        total_loss: -0.0037888986989855766
        vf_explained_var: -8.410215377807617e-05
        vf_loss: 0.4377542734146118
      agent-4:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6881561279296875
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013902854407206178
        model: {}
        policy_loss: -0.0038721703458577394
        total_loss: -0.0050330329686403275
        vf_explained_var: -0.0014856010675430298
        vf_loss: 0.5029428005218506
      agent-5:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9406536817550659
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013791313394904137
        model: {}
        policy_loss: -0.0024765562266111374
        total_loss: -0.004105960950255394
        vf_explained_var: 0.0016261488199234009
        vf_loss: 0.26143842935562134
    load_time_ms: 13780.537
    num_steps_sampled: 23712000
    num_steps_trained: 23712000
    sample_time_ms: 100202.314
    update_time_ms: 16.232
  iterations_since_restore: 87
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.942222222222235
    ram_util_percent: 22.771666666666665
  pid: 27405
  policy_reward_max:
    agent-0: 16.0
    agent-1: 7.0
    agent-2: 19.0
    agent-3: 16.0
    agent-4: 17.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 4.04
    agent-1: 1.58
    agent-2: 5.14
    agent-3: 4.63
    agent-4: 5.58
    agent-5: 3.38
  policy_reward_min:
    agent-0: -1.0
    agent-1: -49.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.341466045543797
    mean_inference_ms: 13.332052512706479
    mean_processing_ms: 59.537013486244184
  time_since_restore: 11075.881207942963
  time_this_iter_s: 126.49394965171814
  time_total_s: 33586.16445040703
  timestamp: 1637231163
  timesteps_since_restore: 8352000
  timesteps_this_iter: 96000
  timesteps_total: 23712000
  training_iteration: 247
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 42.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    247 |          33586.2 | 23712000 |    24.35 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 28
    apples_agent-0_mean: 2.91
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.98
    apples_agent-1_min: 0
    apples_agent-2_max: 34
    apples_agent-2_mean: 2.59
    apples_agent-2_min: 0
    apples_agent-3_max: 21
    apples_agent-3_mean: 2.95
    apples_agent-3_min: 0
    apples_agent-4_max: 8
    apples_agent-4_mean: 1.12
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 1.64
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 120
    cleaning_beam_agent-0_mean: 46.99
    cleaning_beam_agent-0_min: 11
    cleaning_beam_agent-1_max: 356
    cleaning_beam_agent-1_mean: 239.59
    cleaning_beam_agent-1_min: 180
    cleaning_beam_agent-2_max: 72
    cleaning_beam_agent-2_mean: 22.73
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 132
    cleaning_beam_agent-3_mean: 65.46
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 174
    cleaning_beam_agent-4_mean: 49.27
    cleaning_beam_agent-4_min: 13
    cleaning_beam_agent-5_max: 61
    cleaning_beam_agent-5_mean: 23.41
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-28-11
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 57.0
  episode_reward_mean: 24.13
  episode_reward_min: -15.0
  episodes_this_iter: 96
  episodes_total: 23808
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12195.835
    learner:
      agent-0:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5006103515625
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012205513194203377
        model: {}
        policy_loss: -0.003155321115627885
        total_loss: -0.004001060500741005
        vf_explained_var: -0.02504870295524597
        vf_loss: 0.35332199931144714
      agent-1:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4189806282520294
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009545892826281488
        model: {}
        policy_loss: -0.0023492658510804176
        total_loss: -0.0030669448897242546
        vf_explained_var: 0.029109671711921692
        vf_loss: 0.19726607203483582
      agent-2:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45534124970436096
        entropy_coeff: 0.0017600000137463212
        kl: 0.001091809244826436
        model: {}
        policy_loss: -0.003311328124254942
        total_loss: -0.0040565854869782925
        vf_explained_var: -0.006807476282119751
        vf_loss: 0.5614486932754517
      agent-3:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6109442114830017
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010075385216623545
        model: {}
        policy_loss: -0.002702772617340088
        total_loss: -0.0037426045164465904
        vf_explained_var: -0.0018770545721054077
        vf_loss: 0.3543083965778351
      agent-4:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6814821362495422
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016036019660532475
        model: {}
        policy_loss: -0.0037648475263267756
        total_loss: -0.004914386197924614
        vf_explained_var: 0.01559998095035553
        vf_loss: 0.49870291352272034
      agent-5:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9665777087211609
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020616038236767054
        model: {}
        policy_loss: -0.0030623883940279484
        total_loss: -0.004745584446936846
        vf_explained_var: -0.00039142370223999023
        vf_loss: 0.179814413189888
    load_time_ms: 13805.22
    num_steps_sampled: 23808000
    num_steps_trained: 23808000
    sample_time_ms: 100461.929
    update_time_ms: 16.354
  iterations_since_restore: 88
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.197267759562834
    ram_util_percent: 22.871584699453546
  pid: 27405
  policy_reward_max:
    agent-0: 14.0
    agent-1: 8.0
    agent-2: 16.0
    agent-3: 14.0
    agent-4: 15.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 3.66
    agent-1: 2.26
    agent-2: 5.64
    agent-3: 4.18
    agent-4: 5.71
    agent-5: 2.68
  policy_reward_min:
    agent-0: -43.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.341978214859218
    mean_inference_ms: 13.331170608802047
    mean_processing_ms: 59.54209693452536
  time_since_restore: 11204.21212053299
  time_this_iter_s: 128.33091259002686
  time_total_s: 33714.495362997055
  timestamp: 1637231291
  timesteps_since_restore: 8448000
  timesteps_this_iter: 96000
  timesteps_total: 23808000
  training_iteration: 248
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 42.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    248 |          33714.5 | 23808000 |    24.13 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 2.73
    apples_agent-0_min: 0
    apples_agent-1_max: 35
    apples_agent-1_mean: 1.63
    apples_agent-1_min: 0
    apples_agent-2_max: 18
    apples_agent-2_mean: 2.12
    apples_agent-2_min: 0
    apples_agent-3_max: 9
    apples_agent-3_mean: 3.13
    apples_agent-3_min: 0
    apples_agent-4_max: 15
    apples_agent-4_mean: 1.41
    apples_agent-4_min: 0
    apples_agent-5_max: 18
    apples_agent-5_mean: 1.76
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 107
    cleaning_beam_agent-0_mean: 48.13
    cleaning_beam_agent-0_min: 14
    cleaning_beam_agent-1_max: 317
    cleaning_beam_agent-1_mean: 240.5
    cleaning_beam_agent-1_min: 160
    cleaning_beam_agent-2_max: 67
    cleaning_beam_agent-2_mean: 21.15
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 156
    cleaning_beam_agent-3_mean: 65.7
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 100
    cleaning_beam_agent-4_mean: 49.79
    cleaning_beam_agent-4_min: 13
    cleaning_beam_agent-5_max: 71
    cleaning_beam_agent-5_mean: 21.72
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-30-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 52.0
  episode_reward_mean: 24.21
  episode_reward_min: -38.0
  episodes_this_iter: 96
  episodes_total: 23904
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12186.19
    learner:
      agent-0:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5151750445365906
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015886793844401836
        model: {}
        policy_loss: -0.0034747603349387646
        total_loss: -0.004341925960034132
        vf_explained_var: -0.01626792550086975
        vf_loss: 0.3954010605812073
      agent-1:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4205205738544464
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009542752522975206
        model: {}
        policy_loss: -0.002642973093315959
        total_loss: -0.0033647262025624514
        vf_explained_var: 0.0322791188955307
        vf_loss: 0.18361549079418182
      agent-2:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45063090324401855
        entropy_coeff: 0.0017600000137463212
        kl: 0.001421986031346023
        model: {}
        policy_loss: -0.0034775231033563614
        total_loss: -0.004211966414004564
        vf_explained_var: -0.0008317679166793823
        vf_loss: 0.5866585969924927
      agent-3:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6052797436714172
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012955048587173223
        model: {}
        policy_loss: -0.002855174243450165
        total_loss: -0.003886324353516102
        vf_explained_var: 0.006118416786193848
        vf_loss: 0.3414478600025177
      agent-4:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.68668532371521
        entropy_coeff: 0.0017600000137463212
        kl: 0.00203033653087914
        model: {}
        policy_loss: -0.003893248736858368
        total_loss: -0.00505572697147727
        vf_explained_var: 0.011229410767555237
        vf_loss: 0.4608743190765381
      agent-5:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9577499032020569
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015172925777733326
        model: {}
        policy_loss: -0.001974628306925297
        total_loss: -0.0035046571865677834
        vf_explained_var: 7.322430610656738e-05
        vf_loss: 1.556121826171875
    load_time_ms: 13854.659
    num_steps_sampled: 23904000
    num_steps_trained: 23904000
    sample_time_ms: 100456.523
    update_time_ms: 16.093
  iterations_since_restore: 89
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.75027624309392
    ram_util_percent: 23.039779005524867
  pid: 27405
  policy_reward_max:
    agent-0: 15.0
    agent-1: 8.0
    agent-2: 20.0
    agent-3: 12.0
    agent-4: 16.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 3.94
    agent-1: 2.16
    agent-2: 5.66
    agent-3: 4.39
    agent-4: 5.64
    agent-5: 2.42
  policy_reward_min:
    agent-0: -49.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -47.0
  sampler_perf:
    mean_env_wait_ms: 23.339627489827855
    mean_inference_ms: 13.330628647013416
    mean_processing_ms: 59.542987204773134
  time_since_restore: 11331.152164697647
  time_this_iter_s: 126.94004416465759
  time_total_s: 33841.43540716171
  timestamp: 1637231419
  timesteps_since_restore: 8544000
  timesteps_this_iter: 96000
  timesteps_total: 23904000
  training_iteration: 249
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 42.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    249 |          33841.4 | 23904000 |    24.21 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 2.96
    apples_agent-0_min: 0
    apples_agent-1_max: 15
    apples_agent-1_mean: 1.24
    apples_agent-1_min: 0
    apples_agent-2_max: 93
    apples_agent-2_mean: 3.17
    apples_agent-2_min: 0
    apples_agent-3_max: 17
    apples_agent-3_mean: 3.24
    apples_agent-3_min: 0
    apples_agent-4_max: 21
    apples_agent-4_mean: 1.38
    apples_agent-4_min: 0
    apples_agent-5_max: 35
    apples_agent-5_mean: 2.08
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 115
    cleaning_beam_agent-0_mean: 48.6
    cleaning_beam_agent-0_min: 13
    cleaning_beam_agent-1_max: 308
    cleaning_beam_agent-1_mean: 228.78
    cleaning_beam_agent-1_min: 166
    cleaning_beam_agent-2_max: 61
    cleaning_beam_agent-2_mean: 20.3
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 122
    cleaning_beam_agent-3_mean: 62.03
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 129
    cleaning_beam_agent-4_mean: 55.08
    cleaning_beam_agent-4_min: 17
    cleaning_beam_agent-5_max: 74
    cleaning_beam_agent-5_mean: 28.09
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-32-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 54.0
  episode_reward_mean: 23.0
  episode_reward_min: -82.0
  episodes_this_iter: 96
  episodes_total: 24000
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12175.578
    learner:
      agent-0:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5068833231925964
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011766941752284765
        model: {}
        policy_loss: -0.0018881876021623611
        total_loss: -0.0026326151564717293
        vf_explained_var: -0.008108243346214294
        vf_loss: 1.476864218711853
      agent-1:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4123910665512085
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013174593914300203
        model: {}
        policy_loss: -0.00245861429721117
        total_loss: -0.0031700320541858673
        vf_explained_var: 0.019329369068145752
        vf_loss: 0.14390331506729126
      agent-2:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44220608472824097
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014955704100430012
        model: {}
        policy_loss: -0.0033383327536284924
        total_loss: -0.0040656705386936665
        vf_explained_var: 0.0027397125959396362
        vf_loss: 0.5094455480575562
      agent-3:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5978108644485474
        entropy_coeff: 0.0017600000137463212
        kl: 0.001142099848948419
        model: {}
        policy_loss: -0.0029568588361144066
        total_loss: -0.003976958803832531
        vf_explained_var: -0.001050412654876709
        vf_loss: 0.3204408884048462
      agent-4:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6883447170257568
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013777934946119785
        model: {}
        policy_loss: -0.004099070560187101
        total_loss: -0.0052605257369577885
        vf_explained_var: 0.001694992184638977
        vf_loss: 0.5003076195716858
      agent-5:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9862436056137085
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011077227536588907
        model: {}
        policy_loss: -0.0015548751689493656
        total_loss: -0.003154151374474168
        vf_explained_var: 0.0047720372676849365
        vf_loss: 1.3651211261749268
    load_time_ms: 13886.624
    num_steps_sampled: 24000000
    num_steps_trained: 24000000
    sample_time_ms: 100483.029
    update_time_ms: 16.383
  iterations_since_restore: 90
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.30331491712706
    ram_util_percent: 23.141988950276243
  pid: 27405
  policy_reward_max:
    agent-0: 12.0
    agent-1: 8.0
    agent-2: 14.0
    agent-3: 11.0
    agent-4: 15.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 3.4
    agent-1: 2.08
    agent-2: 5.08
    agent-3: 4.2
    agent-4: 5.37
    agent-5: 2.87
  policy_reward_min:
    agent-0: -49.0
    agent-1: 0.0
    agent-2: -42.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -48.0
  sampler_perf:
    mean_env_wait_ms: 23.336561434156817
    mean_inference_ms: 13.329866246153012
    mean_processing_ms: 59.542170731746964
  time_since_restore: 11458.061863660812
  time_this_iter_s: 126.90969896316528
  time_total_s: 33968.34510612488
  timestamp: 1637231546
  timesteps_since_restore: 8640000
  timesteps_this_iter: 96000
  timesteps_total: 24000000
  training_iteration: 250
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 43.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    250 |          33968.3 | 24000000 |       23 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 2.64
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 1.01
    apples_agent-1_min: 0
    apples_agent-2_max: 24
    apples_agent-2_mean: 2.4
    apples_agent-2_min: 0
    apples_agent-3_max: 11
    apples_agent-3_mean: 2.47
    apples_agent-3_min: 0
    apples_agent-4_max: 10
    apples_agent-4_mean: 1.14
    apples_agent-4_min: 0
    apples_agent-5_max: 39
    apples_agent-5_mean: 2.08
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 102
    cleaning_beam_agent-0_mean: 44.47
    cleaning_beam_agent-0_min: 4
    cleaning_beam_agent-1_max: 343
    cleaning_beam_agent-1_mean: 225.94
    cleaning_beam_agent-1_min: 174
    cleaning_beam_agent-2_max: 86
    cleaning_beam_agent-2_mean: 18.16
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 198
    cleaning_beam_agent-3_mean: 64.69
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 100
    cleaning_beam_agent-4_mean: 43.14
    cleaning_beam_agent-4_min: 6
    cleaning_beam_agent-5_max: 74
    cleaning_beam_agent-5_mean: 24.02
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-34-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 65.0
  episode_reward_mean: 21.59
  episode_reward_min: -37.0
  episodes_this_iter: 96
  episodes_total: 24096
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12183.681
    learner:
      agent-0:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5125745534896851
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013603449333459139
        model: {}
        policy_loss: -0.0032111655455082655
        total_loss: -0.0040839724242687225
        vf_explained_var: -0.01737159490585327
        vf_loss: 0.2932276129722595
      agent-1:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41318613290786743
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011534156510606408
        model: {}
        policy_loss: -0.0023310547694563866
        total_loss: -0.0030442234128713608
        vf_explained_var: 0.009692370891571045
        vf_loss: 0.1403905600309372
      agent-2:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43415945768356323
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013426246587187052
        model: {}
        policy_loss: -0.0032996947411447763
        total_loss: -0.00401377072557807
        vf_explained_var: 0.0031614601612091064
        vf_loss: 0.5004652738571167
      agent-3:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5965968370437622
        entropy_coeff: 0.0017600000137463212
        kl: 0.001352979219518602
        model: {}
        policy_loss: -0.0030088871717453003
        total_loss: -0.004031853750348091
        vf_explained_var: 0.0024110227823257446
        vf_loss: 0.2704532742500305
      agent-4:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6784835457801819
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015361226396635175
        model: {}
        policy_loss: -0.0036052782088518143
        total_loss: -0.004762754775583744
        vf_explained_var: -0.01072189211845398
        vf_loss: 0.3665500581264496
      agent-5:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9519186019897461
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012886786134913564
        model: {}
        policy_loss: -0.0018029960338026285
        total_loss: -0.003325064666569233
        vf_explained_var: -0.0008524656295776367
        vf_loss: 1.533096432685852
    load_time_ms: 13879.826
    num_steps_sampled: 24096000
    num_steps_trained: 24096000
    sample_time_ms: 100620.001
    update_time_ms: 16.473
  iterations_since_restore: 91
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.8489010989011
    ram_util_percent: 23.39945054945055
  pid: 27405
  policy_reward_max:
    agent-0: 18.0
    agent-1: 9.0
    agent-2: 15.0
    agent-3: 16.0
    agent-4: 10.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 4.06
    agent-1: 1.97
    agent-2: 5.53
    agent-3: 3.6
    agent-4: 4.83
    agent-5: 1.6
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -47.0
  sampler_perf:
    mean_env_wait_ms: 23.335009893307223
    mean_inference_ms: 13.330271447177923
    mean_processing_ms: 59.54671586482387
  time_since_restore: 11585.213783979416
  time_this_iter_s: 127.15192031860352
  time_total_s: 34095.49702644348
  timestamp: 1637231674
  timesteps_since_restore: 8736000
  timesteps_this_iter: 96000
  timesteps_total: 24096000
  training_iteration: 251
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 43.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    251 |          34095.5 | 24096000 |    21.59 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 2.52
    apples_agent-0_min: 0
    apples_agent-1_max: 23
    apples_agent-1_mean: 1.56
    apples_agent-1_min: 0
    apples_agent-2_max: 43
    apples_agent-2_mean: 3.42
    apples_agent-2_min: 0
    apples_agent-3_max: 11
    apples_agent-3_mean: 3.16
    apples_agent-3_min: 0
    apples_agent-4_max: 9
    apples_agent-4_mean: 1.12
    apples_agent-4_min: 0
    apples_agent-5_max: 5
    apples_agent-5_mean: 1.62
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 101
    cleaning_beam_agent-0_mean: 43.12
    cleaning_beam_agent-0_min: 12
    cleaning_beam_agent-1_max: 359
    cleaning_beam_agent-1_mean: 238.56
    cleaning_beam_agent-1_min: 156
    cleaning_beam_agent-2_max: 66
    cleaning_beam_agent-2_mean: 20.1
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 156
    cleaning_beam_agent-3_mean: 59.18
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 93
    cleaning_beam_agent-4_mean: 49.95
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 67
    cleaning_beam_agent-5_mean: 27.09
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-36-39
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 50.0
  episode_reward_mean: 22.98
  episode_reward_min: -137.0
  episodes_this_iter: 96
  episodes_total: 24192
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12178.901
    learner:
      agent-0:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5021863579750061
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009565782966092229
        model: {}
        policy_loss: -0.0021278359927237034
        total_loss: -0.002843244932591915
        vf_explained_var: -0.003941535949707031
        vf_loss: 1.6843658685684204
      agent-1:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41029274463653564
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008515658555552363
        model: {}
        policy_loss: -0.0022272029891610146
        total_loss: -0.0029327524825930595
        vf_explained_var: 0.03002128005027771
        vf_loss: 0.16564717888832092
      agent-2:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44049137830734253
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010918363695964217
        model: {}
        policy_loss: -0.0023361602798104286
        total_loss: -0.0029342584311962128
        vf_explained_var: 0.00032076239585876465
        vf_loss: 1.7716596126556396
      agent-3:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.576124906539917
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011848128633573651
        model: {}
        policy_loss: -0.0025147772394120693
        total_loss: -0.00349768646992743
        vf_explained_var: 0.0033340007066726685
        vf_loss: 0.3107171058654785
      agent-4:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6948814392089844
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017044178675860167
        model: {}
        policy_loss: -0.0033990899100899696
        total_loss: -0.004440163262188435
        vf_explained_var: 0.003003224730491638
        vf_loss: 1.8191862106323242
      agent-5:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9423431158065796
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017296848818659782
        model: {}
        policy_loss: -0.002566082403063774
        total_loss: -0.004200872499495745
        vf_explained_var: 0.0048007965087890625
        vf_loss: 0.23735707998275757
    load_time_ms: 13856.326
    num_steps_sampled: 24192000
    num_steps_trained: 24192000
    sample_time_ms: 100472.461
    update_time_ms: 16.173
  iterations_since_restore: 92
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.91516853932584
    ram_util_percent: 23.47134831460674
  pid: 27405
  policy_reward_max:
    agent-0: 13.0
    agent-1: 8.0
    agent-2: 18.0
    agent-3: 11.0
    agent-4: 18.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 3.69
    agent-1: 2.36
    agent-2: 4.83
    agent-3: 4.24
    agent-4: 4.77
    agent-5: 3.09
  policy_reward_min:
    agent-0: -47.0
    agent-1: 0.0
    agent-2: -46.0
    agent-3: 0.0
    agent-4: -49.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.331595122417284
    mean_inference_ms: 13.33091762797827
    mean_processing_ms: 59.5435810536126
  time_since_restore: 11710.726820230484
  time_this_iter_s: 125.51303625106812
  time_total_s: 34221.01006269455
  timestamp: 1637231799
  timesteps_since_restore: 8832000
  timesteps_this_iter: 96000
  timesteps_total: 24192000
  training_iteration: 252
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 43.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    252 |            34221 | 24192000 |    22.98 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 136
    apples_agent-0_mean: 3.83
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 1.18
    apples_agent-1_min: 0
    apples_agent-2_max: 29
    apples_agent-2_mean: 2.75
    apples_agent-2_min: 0
    apples_agent-3_max: 10
    apples_agent-3_mean: 3.49
    apples_agent-3_min: 0
    apples_agent-4_max: 15
    apples_agent-4_mean: 1.43
    apples_agent-4_min: 0
    apples_agent-5_max: 46
    apples_agent-5_mean: 2.89
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 185
    cleaning_beam_agent-0_mean: 46.57
    cleaning_beam_agent-0_min: 7
    cleaning_beam_agent-1_max: 312
    cleaning_beam_agent-1_mean: 234.29
    cleaning_beam_agent-1_min: 112
    cleaning_beam_agent-2_max: 55
    cleaning_beam_agent-2_mean: 20.69
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 102
    cleaning_beam_agent-3_mean: 59.14
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 185
    cleaning_beam_agent-4_mean: 46.83
    cleaning_beam_agent-4_min: 13
    cleaning_beam_agent-5_max: 69
    cleaning_beam_agent-5_mean: 27.26
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-38-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 54.0
  episode_reward_mean: 25.78
  episode_reward_min: -9.0
  episodes_this_iter: 96
  episodes_total: 24288
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12195.179
    learner:
      agent-0:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5049973130226135
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016749735223129392
        model: {}
        policy_loss: -0.0023980317637324333
        total_loss: -0.0031224177218973637
        vf_explained_var: -0.005157411098480225
        vf_loss: 1.6440848112106323
      agent-1:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4075125455856323
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009694999316707253
        model: {}
        policy_loss: -0.0021610534749925137
        total_loss: -0.0028559775091707706
        vf_explained_var: 0.018616408109664917
        vf_loss: 0.22300006449222565
      agent-2:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4374159574508667
        entropy_coeff: 0.0017600000137463212
        kl: 0.001346902223303914
        model: {}
        policy_loss: -0.0033838795498013496
        total_loss: -0.004097600467503071
        vf_explained_var: 0.00480768084526062
        vf_loss: 0.5613168478012085
      agent-3:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5893005132675171
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009010374778881669
        model: {}
        policy_loss: -0.0026652654632925987
        total_loss: -0.0036682728677988052
        vf_explained_var: 0.0008218437433242798
        vf_loss: 0.34159958362579346
      agent-4:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6898956298828125
        entropy_coeff: 0.0017600000137463212
        kl: 0.001826118677854538
        model: {}
        policy_loss: -0.004181943833827972
        total_loss: -0.005350914783775806
        vf_explained_var: -0.015035182237625122
        vf_loss: 0.45244625210762024
      agent-5:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9683066606521606
        entropy_coeff: 0.0017600000137463212
        kl: 0.001803361694328487
        model: {}
        policy_loss: -0.002442352008074522
        total_loss: -0.004123346880078316
        vf_explained_var: 0.002734765410423279
        vf_loss: 0.23224805295467377
    load_time_ms: 13859.674
    num_steps_sampled: 24288000
    num_steps_trained: 24288000
    sample_time_ms: 100574.931
    update_time_ms: 16.268
  iterations_since_restore: 93
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.653038674033155
    ram_util_percent: 23.646961325966853
  pid: 27405
  policy_reward_max:
    agent-0: 12.0
    agent-1: 12.0
    agent-2: 18.0
    agent-3: 14.0
    agent-4: 14.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 3.91
    agent-1: 2.58
    agent-2: 5.86
    agent-3: 4.7
    agent-4: 5.21
    agent-5: 3.52
  policy_reward_min:
    agent-0: -40.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.330338156215557
    mean_inference_ms: 13.331366882543412
    mean_processing_ms: 59.545770283236386
  time_since_restore: 11837.52168393135
  time_this_iter_s: 126.7948637008667
  time_total_s: 34347.804926395416
  timestamp: 1637231926
  timesteps_since_restore: 8928000
  timesteps_this_iter: 96000
  timesteps_total: 24288000
  training_iteration: 253
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 43.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    253 |          34347.8 | 24288000 |    25.78 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 2.38
    apples_agent-0_min: 0
    apples_agent-1_max: 8
    apples_agent-1_mean: 1.08
    apples_agent-1_min: 0
    apples_agent-2_max: 13
    apples_agent-2_mean: 2.73
    apples_agent-2_min: 0
    apples_agent-3_max: 12
    apples_agent-3_mean: 3.51
    apples_agent-3_min: 0
    apples_agent-4_max: 23
    apples_agent-4_mean: 1.56
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 1.64
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 185
    cleaning_beam_agent-0_mean: 45.62
    cleaning_beam_agent-0_min: 12
    cleaning_beam_agent-1_max: 337
    cleaning_beam_agent-1_mean: 227.08
    cleaning_beam_agent-1_min: 158
    cleaning_beam_agent-2_max: 75
    cleaning_beam_agent-2_mean: 19.49
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 125
    cleaning_beam_agent-3_mean: 61.63
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 108
    cleaning_beam_agent-4_mean: 46.28
    cleaning_beam_agent-4_min: 10
    cleaning_beam_agent-5_max: 70
    cleaning_beam_agent-5_mean: 27.43
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-40-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 55.0
  episode_reward_mean: 25.46
  episode_reward_min: -75.0
  episodes_this_iter: 96
  episodes_total: 24384
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12174.035
    learner:
      agent-0:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5078044533729553
        entropy_coeff: 0.0017600000137463212
        kl: 0.001364206662401557
        model: {}
        policy_loss: -0.0037365946918725967
        total_loss: -0.004595954902470112
        vf_explained_var: -0.014423727989196777
        vf_loss: 0.34377092123031616
      agent-1:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4059358239173889
        entropy_coeff: 0.0017600000137463212
        kl: 0.00027752952883020043
        model: {}
        policy_loss: -0.0003955317661166191
        total_loss: -0.0009643286466598511
        vf_explained_var: 0.02139778435230255
        vf_loss: 1.4565236568450928
      agent-2:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.424304723739624
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006474516121670604
        model: {}
        policy_loss: -0.001132562872953713
        total_loss: -0.0016858517192304134
        vf_explained_var: 0.005977854132652283
        vf_loss: 1.9348936080932617
      agent-3:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5918827652931213
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007511327276006341
        model: {}
        policy_loss: -0.002670134184882045
        total_loss: -0.003673139028251171
        vf_explained_var: 0.00018562376499176025
        vf_loss: 0.387079656124115
      agent-4:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6909875869750977
        entropy_coeff: 0.0017600000137463212
        kl: 0.001532263238914311
        model: {}
        policy_loss: -0.0038260025903582573
        total_loss: -0.004989174660295248
        vf_explained_var: -0.003012537956237793
        vf_loss: 0.529677152633667
      agent-5:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9714869260787964
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013274243101477623
        model: {}
        policy_loss: -0.0026643401943147182
        total_loss: -0.004351923707872629
        vf_explained_var: -0.00043047964572906494
        vf_loss: 0.22233563661575317
    load_time_ms: 13865.506
    num_steps_sampled: 24384000
    num_steps_trained: 24384000
    sample_time_ms: 100641.368
    update_time_ms: 16.252
  iterations_since_restore: 94
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.313259668508294
    ram_util_percent: 23.701657458563535
  pid: 27405
  policy_reward_max:
    agent-0: 12.0
    agent-1: 8.0
    agent-2: 18.0
    agent-3: 13.0
    agent-4: 18.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 4.29
    agent-1: 1.77
    agent-2: 5.52
    agent-3: 4.87
    agent-4: 5.9
    agent-5: 3.11
  policy_reward_min:
    agent-0: 0.0
    agent-1: -47.0
    agent-2: -43.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.328720541000408
    mean_inference_ms: 13.33045027466925
    mean_processing_ms: 59.546073028801466
  time_since_restore: 11964.374985933304
  time_this_iter_s: 126.85330200195312
  time_total_s: 34474.65822839737
  timestamp: 1637232053
  timesteps_since_restore: 9024000
  timesteps_this_iter: 96000
  timesteps_total: 24384000
  training_iteration: 254
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 44.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    254 |          34474.7 | 24384000 |    25.46 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 3.08
    apples_agent-0_min: 0
    apples_agent-1_max: 23
    apples_agent-1_mean: 1.67
    apples_agent-1_min: 0
    apples_agent-2_max: 13
    apples_agent-2_mean: 2.34
    apples_agent-2_min: 0
    apples_agent-3_max: 13
    apples_agent-3_mean: 3.15
    apples_agent-3_min: 0
    apples_agent-4_max: 19
    apples_agent-4_mean: 1.29
    apples_agent-4_min: 0
    apples_agent-5_max: 8
    apples_agent-5_mean: 1.6
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 119
    cleaning_beam_agent-0_mean: 45.77
    cleaning_beam_agent-0_min: 9
    cleaning_beam_agent-1_max: 295
    cleaning_beam_agent-1_mean: 225.97
    cleaning_beam_agent-1_min: 161
    cleaning_beam_agent-2_max: 69
    cleaning_beam_agent-2_mean: 20.66
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 125
    cleaning_beam_agent-3_mean: 62.11
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 93
    cleaning_beam_agent-4_mean: 41.34
    cleaning_beam_agent-4_min: 4
    cleaning_beam_agent-5_max: 71
    cleaning_beam_agent-5_mean: 25.18
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-42-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 56.0
  episode_reward_mean: 26.83
  episode_reward_min: 4.0
  episodes_this_iter: 96
  episodes_total: 24480
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12192.836
    learner:
      agent-0:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5196106433868408
        entropy_coeff: 0.0017600000137463212
        kl: 0.001767011359333992
        model: {}
        policy_loss: -0.003959241323173046
        total_loss: -0.004833925049751997
        vf_explained_var: -0.0105399489402771
        vf_loss: 0.3983038663864136
      agent-1:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41503655910491943
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008307366515509784
        model: {}
        policy_loss: -0.002365180989727378
        total_loss: -0.0030770055018365383
        vf_explained_var: 0.02892264723777771
        vf_loss: 0.18640679121017456
      agent-2:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42388737201690674
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014513037167489529
        model: {}
        policy_loss: -0.0035433615557849407
        total_loss: -0.004242537543177605
        vf_explained_var: 0.007999077439308167
        vf_loss: 0.46865659952163696
      agent-3:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5932598114013672
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011938274838030338
        model: {}
        policy_loss: -0.0030668163672089577
        total_loss: -0.0040735043585300446
        vf_explained_var: -0.0005438625812530518
        vf_loss: 0.37449002265930176
      agent-4:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6855208873748779
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015281812520697713
        model: {}
        policy_loss: -0.003846407402306795
        total_loss: -0.005000556353479624
        vf_explained_var: 0.0008351951837539673
        vf_loss: 0.5236790776252747
      agent-5:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.955819845199585
        entropy_coeff: 0.0017600000137463212
        kl: 0.001768244430422783
        model: {}
        policy_loss: -0.002923725638538599
        total_loss: -0.0045829052105546
        vf_explained_var: 0.010049968957901001
        vf_loss: 0.2305981069803238
    load_time_ms: 13888.302
    num_steps_sampled: 24480000
    num_steps_trained: 24480000
    sample_time_ms: 100513.647
    update_time_ms: 16.417
  iterations_since_restore: 95
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.74301675977654
    ram_util_percent: 23.8927374301676
  pid: 27405
  policy_reward_max:
    agent-0: 13.0
    agent-1: 10.0
    agent-2: 17.0
    agent-3: 13.0
    agent-4: 15.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 4.9
    agent-1: 2.39
    agent-2: 5.74
    agent-3: 4.49
    agent-4: 6.1
    agent-5: 3.21
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.326636302530975
    mean_inference_ms: 13.32992680231426
    mean_processing_ms: 59.545337119032375
  time_since_restore: 12090.501963376999
  time_this_iter_s: 126.12697744369507
  time_total_s: 34600.785205841064
  timestamp: 1637232179
  timesteps_since_restore: 9120000
  timesteps_this_iter: 96000
  timesteps_total: 24480000
  training_iteration: 255
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 44.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    255 |          34600.8 | 24480000 |    26.83 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 28
    apples_agent-0_mean: 3.1
    apples_agent-0_min: 0
    apples_agent-1_max: 9
    apples_agent-1_mean: 1.39
    apples_agent-1_min: 0
    apples_agent-2_max: 21
    apples_agent-2_mean: 2.54
    apples_agent-2_min: 0
    apples_agent-3_max: 24
    apples_agent-3_mean: 4.14
    apples_agent-3_min: 0
    apples_agent-4_max: 20
    apples_agent-4_mean: 1.72
    apples_agent-4_min: 0
    apples_agent-5_max: 14
    apples_agent-5_mean: 2.15
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 101
    cleaning_beam_agent-0_mean: 45.19
    cleaning_beam_agent-0_min: 12
    cleaning_beam_agent-1_max: 275
    cleaning_beam_agent-1_mean: 223.35
    cleaning_beam_agent-1_min: 165
    cleaning_beam_agent-2_max: 69
    cleaning_beam_agent-2_mean: 20.85
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 135
    cleaning_beam_agent-3_mean: 69.12
    cleaning_beam_agent-3_min: 25
    cleaning_beam_agent-4_max: 187
    cleaning_beam_agent-4_mean: 47.19
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 77
    cleaning_beam_agent-5_mean: 27.48
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-45-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 59.0
  episode_reward_mean: 26.83
  episode_reward_min: 7.0
  episodes_this_iter: 96
  episodes_total: 24576
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12194.796
    learner:
      agent-0:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5094595551490784
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013134322362020612
        model: {}
        policy_loss: -0.003209441900253296
        total_loss: -0.004070806782692671
        vf_explained_var: -0.0205669105052948
        vf_loss: 0.352826327085495
      agent-1:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4113602638244629
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008110658964142203
        model: {}
        policy_loss: -0.0022250947076827288
        total_loss: -0.002928235335275531
        vf_explained_var: 0.023185938596725464
        vf_loss: 0.20852263271808624
      agent-2:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43166589736938477
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012872571824118495
        model: {}
        policy_loss: -0.0035842417273670435
        total_loss: -0.004290260374546051
        vf_explained_var: 0.0015240758657455444
        vf_loss: 0.5371155142784119
      agent-3:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6016254425048828
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012003424344584346
        model: {}
        policy_loss: -0.0027793983463197947
        total_loss: -0.0038032105658203363
        vf_explained_var: -0.00011432170867919922
        vf_loss: 0.35048267245292664
      agent-4:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6778539419174194
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015366338193416595
        model: {}
        policy_loss: -0.003906897734850645
        total_loss: -0.005044321529567242
        vf_explained_var: 0.009902313351631165
        vf_loss: 0.5559773445129395
      agent-5:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9492012858390808
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012019530404359102
        model: {}
        policy_loss: -0.0024868655018508434
        total_loss: -0.004127733875066042
        vf_explained_var: 0.007851749658584595
        vf_loss: 0.29724591970443726
    load_time_ms: 13908.57
    num_steps_sampled: 24576000
    num_steps_trained: 24576000
    sample_time_ms: 100497.664
    update_time_ms: 16.279
  iterations_since_restore: 96
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.545
    ram_util_percent: 23.95444444444444
  pid: 27405
  policy_reward_max:
    agent-0: 14.0
    agent-1: 11.0
    agent-2: 19.0
    agent-3: 12.0
    agent-4: 17.0
    agent-5: 17.0
  policy_reward_mean:
    agent-0: 4.35
    agent-1: 2.51
    agent-2: 5.97
    agent-3: 4.74
    agent-4: 5.85
    agent-5: 3.41
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 1.0
    agent-4: -1.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.323112870191494
    mean_inference_ms: 13.328877172723361
    mean_processing_ms: 59.54001865169126
  time_since_restore: 12216.520456075668
  time_this_iter_s: 126.01849269866943
  time_total_s: 34726.803698539734
  timestamp: 1637232306
  timesteps_since_restore: 9216000
  timesteps_this_iter: 96000
  timesteps_total: 24576000
  training_iteration: 256
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 44.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    256 |          34726.8 | 24576000 |    26.83 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 60
    apples_agent-0_mean: 3.55
    apples_agent-0_min: 0
    apples_agent-1_max: 10
    apples_agent-1_mean: 1.29
    apples_agent-1_min: 0
    apples_agent-2_max: 80
    apples_agent-2_mean: 3.09
    apples_agent-2_min: 0
    apples_agent-3_max: 40
    apples_agent-3_mean: 3.66
    apples_agent-3_min: 0
    apples_agent-4_max: 57
    apples_agent-4_mean: 1.93
    apples_agent-4_min: 0
    apples_agent-5_max: 59
    apples_agent-5_mean: 2.83
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 102
    cleaning_beam_agent-0_mean: 44.12
    cleaning_beam_agent-0_min: 12
    cleaning_beam_agent-1_max: 317
    cleaning_beam_agent-1_mean: 233.03
    cleaning_beam_agent-1_min: 171
    cleaning_beam_agent-2_max: 57
    cleaning_beam_agent-2_mean: 18.89
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 115
    cleaning_beam_agent-3_mean: 60.28
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 143
    cleaning_beam_agent-4_mean: 54.92
    cleaning_beam_agent-4_min: 7
    cleaning_beam_agent-5_max: 59
    cleaning_beam_agent-5_mean: 23.64
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-47-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 61.0
  episode_reward_mean: 26.36
  episode_reward_min: -22.0
  episodes_this_iter: 96
  episodes_total: 24672
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12195.618
    learner:
      agent-0:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49965792894363403
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016017807647585869
        model: {}
        policy_loss: -0.0034161137882620096
        total_loss: -0.004258707631379366
        vf_explained_var: -0.01589033007621765
        vf_loss: 0.367997944355011
      agent-1:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4112217128276825
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012640981003642082
        model: {}
        policy_loss: -0.0024585770443081856
        total_loss: -0.0031635493505746126
        vf_explained_var: 0.029528111219406128
        vf_loss: 0.18776558339595795
      agent-2:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42248618602752686
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012786429142579436
        model: {}
        policy_loss: -0.002358952071517706
        total_loss: -0.0029196608811616898
        vf_explained_var: 0.002156764268875122
        vf_loss: 1.8286843299865723
      agent-3:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5890994071960449
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011452581966295838
        model: {}
        policy_loss: -0.002821805188432336
        total_loss: -0.003822565544396639
        vf_explained_var: -0.0009624958038330078
        vf_loss: 0.3605375289916992
      agent-4:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6944451332092285
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020734798163175583
        model: {}
        policy_loss: -0.0039388323202729225
        total_loss: -0.005101501010358334
        vf_explained_var: 0.004238203167915344
        vf_loss: 0.5955347418785095
      agent-5:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9303293228149414
        entropy_coeff: 0.0017600000137463212
        kl: 0.001993197249248624
        model: {}
        policy_loss: -0.0028538666665554047
        total_loss: -0.004464807920157909
        vf_explained_var: 0.004003807902336121
        vf_loss: 0.26438620686531067
    load_time_ms: 13920.539
    num_steps_sampled: 24672000
    num_steps_trained: 24672000
    sample_time_ms: 100476.158
    update_time_ms: 15.753
  iterations_since_restore: 97
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.925
    ram_util_percent: 24.053888888888892
  pid: 27405
  policy_reward_max:
    agent-0: 12.0
    agent-1: 11.0
    agent-2: 19.0
    agent-3: 13.0
    agent-4: 19.0
    agent-5: 14.0
  policy_reward_mean:
    agent-0: 4.68
    agent-1: 2.66
    agent-2: 5.38
    agent-3: 4.69
    agent-4: 5.83
    agent-5: 3.12
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -40.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.32105645765971
    mean_inference_ms: 13.328175498389319
    mean_processing_ms: 59.53808397905794
  time_since_restore: 12342.862400531769
  time_this_iter_s: 126.34194445610046
  time_total_s: 34853.145642995834
  timestamp: 1637232432
  timesteps_since_restore: 9312000
  timesteps_this_iter: 96000
  timesteps_total: 24672000
  training_iteration: 257
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 44.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    257 |          34853.1 | 24672000 |    26.36 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 3.24
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 1.21
    apples_agent-1_min: 0
    apples_agent-2_max: 77
    apples_agent-2_mean: 2.77
    apples_agent-2_min: 0
    apples_agent-3_max: 27
    apples_agent-3_mean: 3.23
    apples_agent-3_min: 0
    apples_agent-4_max: 6
    apples_agent-4_mean: 1.11
    apples_agent-4_min: 0
    apples_agent-5_max: 77
    apples_agent-5_mean: 2.34
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 87
    cleaning_beam_agent-0_mean: 43.76
    cleaning_beam_agent-0_min: 10
    cleaning_beam_agent-1_max: 342
    cleaning_beam_agent-1_mean: 239.56
    cleaning_beam_agent-1_min: 161
    cleaning_beam_agent-2_max: 73
    cleaning_beam_agent-2_mean: 19.23
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 109
    cleaning_beam_agent-3_mean: 61.18
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 143
    cleaning_beam_agent-4_mean: 49.73
    cleaning_beam_agent-4_min: 10
    cleaning_beam_agent-5_max: 82
    cleaning_beam_agent-5_mean: 23.79
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-49-18
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 64.0
  episode_reward_mean: 23.92
  episode_reward_min: 4.0
  episodes_this_iter: 96
  episodes_total: 24768
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12180.454
    learner:
      agent-0:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5040389895439148
        entropy_coeff: 0.0017600000137463212
        kl: 0.000961988465860486
        model: {}
        policy_loss: -0.0030172341503202915
        total_loss: -0.003865326289087534
        vf_explained_var: -0.011155098676681519
        vf_loss: 0.3901804983615875
      agent-1:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4132450222969055
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012339814566075802
        model: {}
        policy_loss: -0.002228659577667713
        total_loss: -0.002941281534731388
        vf_explained_var: 0.04127487540245056
        vf_loss: 0.14690813422203064
      agent-2:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4183431565761566
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010930381249636412
        model: {}
        policy_loss: -0.0034629141446202993
        total_loss: -0.004154702182859182
        vf_explained_var: 0.0001807957887649536
        vf_loss: 0.44494760036468506
      agent-3:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.598797082901001
        entropy_coeff: 0.0017600000137463212
        kl: 0.001505725085735321
        model: {}
        policy_loss: -0.0029206150211393833
        total_loss: -0.003937465604394674
        vf_explained_var: 0.004399552941322327
        vf_loss: 0.3702905774116516
      agent-4:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6937779784202576
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014957789098843932
        model: {}
        policy_loss: -0.003516433760523796
        total_loss: -0.004686977714300156
        vf_explained_var: 0.01086437702178955
        vf_loss: 0.5050693154335022
      agent-5:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9562922716140747
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018570853862911463
        model: {}
        policy_loss: -0.003122930647805333
        total_loss: -0.004786565899848938
        vf_explained_var: -0.0037509500980377197
        vf_loss: 0.19439275562763214
    load_time_ms: 13901.773
    num_steps_sampled: 24768000
    num_steps_trained: 24768000
    sample_time_ms: 100213.919
    update_time_ms: 15.628
  iterations_since_restore: 98
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.69888268156425
    ram_util_percent: 24.183240223463685
  pid: 27405
  policy_reward_max:
    agent-0: 18.0
    agent-1: 8.0
    agent-2: 16.0
    agent-3: 15.0
    agent-4: 18.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 4.44
    agent-1: 2.18
    agent-2: 4.96
    agent-3: 4.29
    agent-4: 5.28
    agent-5: 2.77
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.319006478456096
    mean_inference_ms: 13.32786303120782
    mean_processing_ms: 59.53559535379334
  time_since_restore: 12468.27046585083
  time_this_iter_s: 125.40806531906128
  time_total_s: 34978.553708314896
  timestamp: 1637232558
  timesteps_since_restore: 9408000
  timesteps_this_iter: 96000
  timesteps_total: 24768000
  training_iteration: 258
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 44.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    258 |          34978.6 | 24768000 |    23.92 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 2.94
    apples_agent-0_min: 0
    apples_agent-1_max: 24
    apples_agent-1_mean: 1.35
    apples_agent-1_min: 0
    apples_agent-2_max: 10
    apples_agent-2_mean: 2.21
    apples_agent-2_min: 0
    apples_agent-3_max: 15
    apples_agent-3_mean: 3.76
    apples_agent-3_min: 0
    apples_agent-4_max: 13
    apples_agent-4_mean: 1.33
    apples_agent-4_min: 0
    apples_agent-5_max: 13
    apples_agent-5_mean: 1.82
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 99
    cleaning_beam_agent-0_mean: 42.32
    cleaning_beam_agent-0_min: 7
    cleaning_beam_agent-1_max: 324
    cleaning_beam_agent-1_mean: 245.51
    cleaning_beam_agent-1_min: 181
    cleaning_beam_agent-2_max: 73
    cleaning_beam_agent-2_mean: 18.71
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 138
    cleaning_beam_agent-3_mean: 58.45
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 125
    cleaning_beam_agent-4_mean: 53.29
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 86
    cleaning_beam_agent-5_mean: 23.84
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-51-25
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 65.0
  episode_reward_mean: 27.65
  episode_reward_min: -39.0
  episodes_this_iter: 96
  episodes_total: 24864
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12191.508
    learner:
      agent-0:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5144317150115967
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013275715755298734
        model: {}
        policy_loss: -0.0031977472826838493
        total_loss: -0.004066770896315575
        vf_explained_var: -0.018155395984649658
        vf_loss: 0.36375829577445984
      agent-1:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40680256485939026
        entropy_coeff: 0.0017600000137463212
        kl: 0.001872291904874146
        model: {}
        policy_loss: -0.0029060521628707647
        total_loss: -0.0034720117691904306
        vf_explained_var: 0.0012672841548919678
        vf_loss: 1.5001434087753296
      agent-2:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4273253083229065
        entropy_coeff: 0.0017600000137463212
        kl: 0.000905296765267849
        model: {}
        policy_loss: -0.0029739991296082735
        total_loss: -0.003651048056781292
        vf_explained_var: 0.005228310823440552
        vf_loss: 0.7504343390464783
      agent-3:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5936360359191895
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014514505164697766
        model: {}
        policy_loss: -0.002592044649645686
        total_loss: -0.0035897556226700544
        vf_explained_var: 0.0023857057094573975
        vf_loss: 0.4708951711654663
      agent-4:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6944890022277832
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015117900911718607
        model: {}
        policy_loss: -0.004035291261970997
        total_loss: -0.005202243570238352
        vf_explained_var: 0.0203915536403656
        vf_loss: 0.5534927845001221
      agent-5:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9352201223373413
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016335253603756428
        model: {}
        policy_loss: -0.002909441478550434
        total_loss: -0.004528345540165901
        vf_explained_var: 0.0075790733098983765
        vf_loss: 0.2708442509174347
    load_time_ms: 13879.082
    num_steps_sampled: 24864000
    num_steps_trained: 24864000
    sample_time_ms: 100274.574
    update_time_ms: 15.707
  iterations_since_restore: 99
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.23535911602209
    ram_util_percent: 24.286187845303868
  pid: 27405
  policy_reward_max:
    agent-0: 13.0
    agent-1: 11.0
    agent-2: 24.0
    agent-3: 15.0
    agent-4: 15.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 4.49
    agent-1: 1.81
    agent-2: 6.68
    agent-3: 5.25
    agent-4: 6.17
    agent-5: 3.25
  policy_reward_min:
    agent-0: 0.0
    agent-1: -50.0
    agent-2: 0.0
    agent-3: 1.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.31896782963155
    mean_inference_ms: 13.32749553036243
    mean_processing_ms: 59.53665911683464
  time_since_restore: 12595.71468091011
  time_this_iter_s: 127.4442150592804
  time_total_s: 35105.997923374176
  timestamp: 1637232685
  timesteps_since_restore: 9504000
  timesteps_this_iter: 96000
  timesteps_total: 24864000
  training_iteration: 259
  trial_id: '00000'
  
[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
== Status ==
Memory usage on this node: 45.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    259 |            35106 | 24864000 |    27.65 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 85
    apples_agent-0_mean: 3.72
    apples_agent-0_min: 0
    apples_agent-1_max: 17
    apples_agent-1_mean: 1.24
    apples_agent-1_min: 0
    apples_agent-2_max: 95
    apples_agent-2_mean: 4.41
    apples_agent-2_min: 0
    apples_agent-3_max: 18
    apples_agent-3_mean: 3.58
    apples_agent-3_min: 0
    apples_agent-4_max: 112
    apples_agent-4_mean: 3.15
    apples_agent-4_min: 0
    apples_agent-5_max: 10
    apples_agent-5_mean: 2.2
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 184
    cleaning_beam_agent-0_mean: 45.82
    cleaning_beam_agent-0_min: 11
    cleaning_beam_agent-1_max: 317
    cleaning_beam_agent-1_mean: 239.51
    cleaning_beam_agent-1_min: 163
    cleaning_beam_agent-2_max: 59
    cleaning_beam_agent-2_mean: 18.38
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 134
    cleaning_beam_agent-3_mean: 52.76
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 159
    cleaning_beam_agent-4_mean: 59.91
    cleaning_beam_agent-4_min: 22
    cleaning_beam_agent-5_max: 64
    cleaning_beam_agent-5_mean: 23.45
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 10
    fire_beam_agent-0_mean: 0.1
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-53-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 56.0
  episode_reward_mean: 24.07
  episode_reward_min: -462.0
  episodes_this_iter: 96
  episodes_total: 24960
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12200.746
    learner:
      agent-0:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5152527689933777
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017328023677691817
        model: {}
        policy_loss: -0.003714953549206257
        total_loss: -0.004575863014906645
        vf_explained_var: -0.014766216278076172
        vf_loss: 0.4593559503555298
      agent-1:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40630221366882324
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009869964560493827
        model: {}
        policy_loss: -0.0023371719289571047
        total_loss: -0.0030343348626047373
        vf_explained_var: 0.01643301546573639
        vf_loss: 0.17926859855651855
      agent-2:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42912277579307556
        entropy_coeff: 0.0017600000137463212
        kl: 0.0033836858347058296
        model: {}
        policy_loss: -0.0017168994527310133
        total_loss: 0.0094147315248847
        vf_explained_var: -0.00012730062007904053
        vf_loss: 118.86890411376953
      agent-3:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5867043733596802
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013801779132336378
        model: {}
        policy_loss: -0.00252486951649189
        total_loss: -0.0035186726599931717
        vf_explained_var: 0.0008007287979125977
        vf_loss: 0.3879771828651428
      agent-4:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6956691741943359
        entropy_coeff: 0.0017600000137463212
        kl: 0.002571348799392581
        model: {}
        policy_loss: -0.004470767453312874
        total_loss: -0.00564536452293396
        vf_explained_var: 0.006718695163726807
        vf_loss: 0.49784934520721436
      agent-5:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9497199058532715
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018693706952035427
        model: {}
        policy_loss: -0.0027539939619600773
        total_loss: -0.0043951235711574554
        vf_explained_var: 0.012320920825004578
        vf_loss: 0.303754985332489
    load_time_ms: 13850.337
    num_steps_sampled: 24960000
    num_steps_trained: 24960000
    sample_time_ms: 100292.407
    update_time_ms: 15.575
  iterations_since_restore: 100
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.549723756906076
    ram_util_percent: 24.36795580110497
  pid: 27405
  policy_reward_max:
    agent-0: 12.0
    agent-1: 6.0
    agent-2: 23.0
    agent-3: 12.0
    agent-4: 13.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 4.86
    agent-1: 2.45
    agent-2: 2.25
    agent-3: 4.87
    agent-4: 6.09
    agent-5: 3.55
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -484.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.316985860334427
    mean_inference_ms: 13.32690814239549
    mean_processing_ms: 59.5368193938286
  time_since_restore: 12722.570052623749
  time_this_iter_s: 126.8553717136383
  time_total_s: 35232.853295087814
  timestamp: 1637232812
  timesteps_since_restore: 9600000
  timesteps_this_iter: 96000
  timesteps_total: 24960000
  training_iteration: 260
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 45.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    260 |          35232.9 | 24960000 |    24.07 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 2.93
    apples_agent-0_min: 0
    apples_agent-1_max: 61
    apples_agent-1_mean: 2.01
    apples_agent-1_min: 0
    apples_agent-2_max: 17
    apples_agent-2_mean: 2.21
    apples_agent-2_min: 0
    apples_agent-3_max: 13
    apples_agent-3_mean: 3.5
    apples_agent-3_min: 0
    apples_agent-4_max: 32
    apples_agent-4_mean: 1.55
    apples_agent-4_min: 0
    apples_agent-5_max: 11
    apples_agent-5_mean: 2.09
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 100
    cleaning_beam_agent-0_mean: 42.87
    cleaning_beam_agent-0_min: 9
    cleaning_beam_agent-1_max: 316
    cleaning_beam_agent-1_mean: 239.45
    cleaning_beam_agent-1_min: 176
    cleaning_beam_agent-2_max: 43
    cleaning_beam_agent-2_mean: 18.58
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 119
    cleaning_beam_agent-3_mean: 56.66
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 136
    cleaning_beam_agent-4_mean: 56.82
    cleaning_beam_agent-4_min: 15
    cleaning_beam_agent-5_max: 80
    cleaning_beam_agent-5_mean: 26.84
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 4
    fire_beam_agent-0_mean: 0.05
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-55-39
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 62.0
  episode_reward_mean: 26.25
  episode_reward_min: -35.0
  episodes_this_iter: 96
  episodes_total: 25056
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12209.341
    learner:
      agent-0:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5053785443305969
        entropy_coeff: 0.0017600000137463212
        kl: 0.001422313041985035
        model: {}
        policy_loss: -0.003539389930665493
        total_loss: -0.004391183145344257
        vf_explained_var: -0.02727755904197693
        vf_loss: 0.3767286539077759
      agent-1:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.404901921749115
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019074614392593503
        model: {}
        policy_loss: -0.002911071293056011
        total_loss: -0.0036040665581822395
        vf_explained_var: 0.019435733556747437
        vf_loss: 0.19631630182266235
      agent-2:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42099058628082275
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010718709090724587
        model: {}
        policy_loss: -0.001684082904830575
        total_loss: -0.002237181644886732
        vf_explained_var: -0.0004056692123413086
        vf_loss: 1.8784630298614502
      agent-3:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5894702672958374
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015122415497899055
        model: {}
        policy_loss: -0.0028443322516977787
        total_loss: -0.0038426495157182217
        vf_explained_var: -0.0012647062540054321
        vf_loss: 0.39145779609680176
      agent-4:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6838077902793884
        entropy_coeff: 0.0017600000137463212
        kl: 0.001340780290775001
        model: {}
        policy_loss: -0.0036450717598199844
        total_loss: -0.004794605541974306
        vf_explained_var: 0.014192074537277222
        vf_loss: 0.5396774411201477
      agent-5:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9442739486694336
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015938624273985624
        model: {}
        policy_loss: -0.002497508656233549
        total_loss: -0.004001219291239977
        vf_explained_var: 0.0020872503519058228
        vf_loss: 1.5820986032485962
    load_time_ms: 13873.212
    num_steps_sampled: 25056000
    num_steps_trained: 25056000
    sample_time_ms: 100141.835
    update_time_ms: 15.403
  iterations_since_restore: 101
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.54777777777778
    ram_util_percent: 24.48166666666667
  pid: 27405
  policy_reward_max:
    agent-0: 14.0
    agent-1: 9.0
    agent-2: 17.0
    agent-3: 14.0
    agent-4: 19.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 4.31
    agent-1: 2.66
    agent-2: 5.75
    agent-3: 4.96
    agent-4: 5.72
    agent-5: 2.85
  policy_reward_min:
    agent-0: -1.0
    agent-1: 0.0
    agent-2: -47.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -46.0
  sampler_perf:
    mean_env_wait_ms: 23.314773000982335
    mean_inference_ms: 13.326556999428636
    mean_processing_ms: 59.53527464412109
  time_since_restore: 12848.585121154785
  time_this_iter_s: 126.01506853103638
  time_total_s: 35358.86836361885
  timestamp: 1637232939
  timesteps_since_restore: 9696000
  timesteps_this_iter: 96000
  timesteps_total: 25056000
  training_iteration: 261
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 45.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    261 |          35358.9 | 25056000 |    26.25 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 3.22
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 1.4
    apples_agent-1_min: 0
    apples_agent-2_max: 14
    apples_agent-2_mean: 2.54
    apples_agent-2_min: 0
    apples_agent-3_max: 13
    apples_agent-3_mean: 3.37
    apples_agent-3_min: 0
    apples_agent-4_max: 18
    apples_agent-4_mean: 1.47
    apples_agent-4_min: 0
    apples_agent-5_max: 32
    apples_agent-5_mean: 2.64
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 166
    cleaning_beam_agent-0_mean: 45.67
    cleaning_beam_agent-0_min: 9
    cleaning_beam_agent-1_max: 297
    cleaning_beam_agent-1_mean: 226.02
    cleaning_beam_agent-1_min: 155
    cleaning_beam_agent-2_max: 61
    cleaning_beam_agent-2_mean: 19.02
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 106
    cleaning_beam_agent-3_mean: 49.41
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 179
    cleaning_beam_agent-4_mean: 60.6
    cleaning_beam_agent-4_min: 17
    cleaning_beam_agent-5_max: 55
    cleaning_beam_agent-5_mean: 25.72
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-57-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 72.0
  episode_reward_mean: 29.04
  episode_reward_min: 9.0
  episodes_this_iter: 96
  episodes_total: 25152
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12209.965
    learner:
      agent-0:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5152466893196106
        entropy_coeff: 0.0017600000137463212
        kl: 0.001550640445202589
        model: {}
        policy_loss: -0.0037430720403790474
        total_loss: -0.00460998248308897
        vf_explained_var: -0.012535840272903442
        vf_loss: 0.3992396593093872
      agent-1:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4009970724582672
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010351253440603614
        model: {}
        policy_loss: -0.002236046362668276
        total_loss: -0.002921412233263254
        vf_explained_var: 0.031340956687927246
        vf_loss: 0.20388595759868622
      agent-2:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42181411385536194
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010759354336187243
        model: {}
        policy_loss: -0.0031827997881919146
        total_loss: -0.0038607148453593254
        vf_explained_var: 0.0003744661808013916
        vf_loss: 0.6447749733924866
      agent-3:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.597464919090271
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013485206291079521
        model: {}
        policy_loss: -0.002733506029471755
        total_loss: -0.0037413008976727724
        vf_explained_var: 0.0026064813137054443
        vf_loss: 0.437422513961792
      agent-4:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6910303831100464
        entropy_coeff: 0.0017600000137463212
        kl: 0.001625868957489729
        model: {}
        policy_loss: -0.003872606437653303
        total_loss: -0.005032045301049948
        vf_explained_var: 0.004984945058822632
        vf_loss: 0.5677482485771179
      agent-5:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9500797986984253
        entropy_coeff: 0.0017600000137463212
        kl: 0.001960942056030035
        model: {}
        policy_loss: -0.0030449600890278816
        total_loss: -0.004685121588408947
        vf_explained_var: 0.001832321286201477
        vf_loss: 0.3198014497756958
    load_time_ms: 13883.053
    num_steps_sampled: 25152000
    num_steps_trained: 25152000
    sample_time_ms: 100350.903
    update_time_ms: 15.754
  iterations_since_restore: 102
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.535714285714285
    ram_util_percent: 24.57857142857143
  pid: 27405
  policy_reward_max:
    agent-0: 12.0
    agent-1: 7.0
    agent-2: 19.0
    agent-3: 18.0
    agent-4: 17.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 4.81
    agent-1: 2.89
    agent-2: 6.11
    agent-3: 5.22
    agent-4: 6.28
    agent-5: 3.73
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.31405500689829
    mean_inference_ms: 13.326598272209036
    mean_processing_ms: 59.53788413920996
  time_since_restore: 12976.278080701828
  time_this_iter_s: 127.69295954704285
  time_total_s: 35486.56132316589
  timestamp: 1637233067
  timesteps_since_restore: 9792000
  timesteps_this_iter: 96000
  timesteps_total: 25152000
  training_iteration: 262
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 45.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    262 |          35486.6 | 25152000 |    29.04 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 2.75
    apples_agent-0_min: 0
    apples_agent-1_max: 49
    apples_agent-1_mean: 1.98
    apples_agent-1_min: 0
    apples_agent-2_max: 12
    apples_agent-2_mean: 2.19
    apples_agent-2_min: 0
    apples_agent-3_max: 11
    apples_agent-3_mean: 3.39
    apples_agent-3_min: 0
    apples_agent-4_max: 13
    apples_agent-4_mean: 1.3
    apples_agent-4_min: 0
    apples_agent-5_max: 28
    apples_agent-5_mean: 2.16
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 143
    cleaning_beam_agent-0_mean: 42.78
    cleaning_beam_agent-0_min: 11
    cleaning_beam_agent-1_max: 293
    cleaning_beam_agent-1_mean: 230.76
    cleaning_beam_agent-1_min: 172
    cleaning_beam_agent-2_max: 87
    cleaning_beam_agent-2_mean: 18.48
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 114
    cleaning_beam_agent-3_mean: 49.59
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 117
    cleaning_beam_agent-4_mean: 49.21
    cleaning_beam_agent-4_min: 3
    cleaning_beam_agent-5_max: 110
    cleaning_beam_agent-5_mean: 25.3
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-59-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 67.0
  episode_reward_mean: 26.53
  episode_reward_min: -6.0
  episodes_this_iter: 96
  episodes_total: 25248
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12212.342
    learner:
      agent-0:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5101463198661804
        entropy_coeff: 0.0017600000137463212
        kl: 0.001324680633842945
        model: {}
        policy_loss: -0.003472206648439169
        total_loss: -0.004335297737270594
        vf_explained_var: -0.012979060411453247
        vf_loss: 0.34767016768455505
      agent-1:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3936387300491333
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012477340642362833
        model: {}
        policy_loss: -0.0022330819629132748
        total_loss: -0.0029082964174449444
        vf_explained_var: 0.016700848937034607
        vf_loss: 0.17589807510375977
      agent-2:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42348799109458923
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011827768757939339
        model: {}
        policy_loss: -0.0031377822160720825
        total_loss: -0.0038207308389246464
        vf_explained_var: -0.0007617473602294922
        vf_loss: 0.6239027380943298
      agent-3:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.592323362827301
        entropy_coeff: 0.0017600000137463212
        kl: 0.001437681377865374
        model: {}
        policy_loss: -0.0025703306309878826
        total_loss: -0.0035689817741513252
        vf_explained_var: 0.0019590556621551514
        vf_loss: 0.43837064504623413
      agent-4:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6913204789161682
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014778152108192444
        model: {}
        policy_loss: -0.003980808891355991
        total_loss: -0.005150665063410997
        vf_explained_var: 0.0023928433656692505
        vf_loss: 0.4687020778656006
      agent-5:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9561636447906494
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019120818469673395
        model: {}
        policy_loss: -0.002933552023023367
        total_loss: -0.004592726938426495
        vf_explained_var: 0.00386732816696167
        vf_loss: 0.23677366971969604
    load_time_ms: 13873.01
    num_steps_sampled: 25248000
    num_steps_trained: 25248000
    sample_time_ms: 100323.933
    update_time_ms: 15.527
  iterations_since_restore: 103
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.59777777777778
    ram_util_percent: 24.707222222222228
  pid: 27405
  policy_reward_max:
    agent-0: 13.0
    agent-1: 7.0
    agent-2: 18.0
    agent-3: 18.0
    agent-4: 23.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 4.4
    agent-1: 1.84
    agent-2: 6.3
    agent-3: 5.11
    agent-4: 5.54
    agent-5: 3.34
  policy_reward_min:
    agent-0: 0.0
    agent-1: -47.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.31078679086889
    mean_inference_ms: 13.325867734548426
    mean_processing_ms: 59.53688085993858
  time_since_restore: 13102.725672006607
  time_this_iter_s: 126.44759130477905
  time_total_s: 35613.00891447067
  timestamp: 1637233193
  timesteps_since_restore: 9888000
  timesteps_this_iter: 96000
  timesteps_total: 25248000
  training_iteration: 263
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 45.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    263 |            35613 | 25248000 |    26.53 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 3.04
    apples_agent-0_min: 0
    apples_agent-1_max: 22
    apples_agent-1_mean: 1.24
    apples_agent-1_min: 0
    apples_agent-2_max: 30
    apples_agent-2_mean: 2.69
    apples_agent-2_min: 0
    apples_agent-3_max: 11
    apples_agent-3_mean: 2.8
    apples_agent-3_min: 0
    apples_agent-4_max: 6
    apples_agent-4_mean: 1.18
    apples_agent-4_min: 0
    apples_agent-5_max: 14
    apples_agent-5_mean: 1.74
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 93
    cleaning_beam_agent-0_mean: 45.41
    cleaning_beam_agent-0_min: 13
    cleaning_beam_agent-1_max: 302
    cleaning_beam_agent-1_mean: 240.95
    cleaning_beam_agent-1_min: 181
    cleaning_beam_agent-2_max: 50
    cleaning_beam_agent-2_mean: 16.52
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 136
    cleaning_beam_agent-3_mean: 54.85
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 122
    cleaning_beam_agent-4_mean: 48.93
    cleaning_beam_agent-4_min: 8
    cleaning_beam_agent-5_max: 64
    cleaning_beam_agent-5_mean: 27.22
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-02-00
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 59.0
  episode_reward_mean: 26.14
  episode_reward_min: 11.0
  episodes_this_iter: 96
  episodes_total: 25344
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12218.89
    learner:
      agent-0:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5239770412445068
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012882708106189966
        model: {}
        policy_loss: -0.003422265872359276
        total_loss: -0.004309776239097118
        vf_explained_var: -0.01394692063331604
        vf_loss: 0.3469221293926239
      agent-1:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4000139832496643
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009289776207879186
        model: {}
        policy_loss: -0.0022263438440859318
        total_loss: -0.0029167362954467535
        vf_explained_var: 0.02027370035648346
        vf_loss: 0.1363241970539093
      agent-2:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42226967215538025
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012811757624149323
        model: {}
        policy_loss: -0.0034781782887876034
        total_loss: -0.004162224009633064
        vf_explained_var: 0.0031589120626449585
        vf_loss: 0.5914629697799683
      agent-3:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6012597680091858
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010265684686601162
        model: {}
        policy_loss: -0.0026547920424491167
        total_loss: -0.0036720808129757643
        vf_explained_var: 0.007073521614074707
        vf_loss: 0.40927886962890625
      agent-4:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6813531517982483
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014322237111628056
        model: {}
        policy_loss: -0.003980677109211683
        total_loss: -0.00512775918468833
        vf_explained_var: 0.004526659846305847
        vf_loss: 0.520993709564209
      agent-5:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9734408855438232
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018540784949436784
        model: {}
        policy_loss: -0.002821896690875292
        total_loss: -0.004513588268309832
        vf_explained_var: -0.0003683716058731079
        vf_loss: 0.21566730737686157
    load_time_ms: 13844.731
    num_steps_sampled: 25344000
    num_steps_trained: 25344000
    sample_time_ms: 100264.824
    update_time_ms: 15.457
  iterations_since_restore: 104
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.58111111111111
    ram_util_percent: 24.767777777777773
  pid: 27405
  policy_reward_max:
    agent-0: 16.0
    agent-1: 8.0
    agent-2: 20.0
    agent-3: 18.0
    agent-4: 13.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 4.43
    agent-1: 2.16
    agent-2: 6.12
    agent-3: 4.63
    agent-4: 5.75
    agent-5: 3.05
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 1.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.30825623400259
    mean_inference_ms: 13.325163571202241
    mean_processing_ms: 59.5374540797988
  time_since_restore: 13228.765480279922
  time_this_iter_s: 126.03980827331543
  time_total_s: 35739.04872274399
  timestamp: 1637233320
  timesteps_since_restore: 9984000
  timesteps_this_iter: 96000
  timesteps_total: 25344000
  training_iteration: 264
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 46.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    264 |            35739 | 25344000 |    26.14 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 3.32
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.95
    apples_agent-1_min: 0
    apples_agent-2_max: 24
    apples_agent-2_mean: 3.01
    apples_agent-2_min: 0
    apples_agent-3_max: 26
    apples_agent-3_mean: 3.83
    apples_agent-3_min: 0
    apples_agent-4_max: 39
    apples_agent-4_mean: 1.62
    apples_agent-4_min: 0
    apples_agent-5_max: 16
    apples_agent-5_mean: 2.42
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 85
    cleaning_beam_agent-0_mean: 42.72
    cleaning_beam_agent-0_min: 8
    cleaning_beam_agent-1_max: 386
    cleaning_beam_agent-1_mean: 238.86
    cleaning_beam_agent-1_min: 161
    cleaning_beam_agent-2_max: 59
    cleaning_beam_agent-2_mean: 17.29
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 138
    cleaning_beam_agent-3_mean: 54.62
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 148
    cleaning_beam_agent-4_mean: 53.33
    cleaning_beam_agent-4_min: 14
    cleaning_beam_agent-5_max: 86
    cleaning_beam_agent-5_mean: 27.33
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-04-07
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 68.0
  episode_reward_mean: 28.45
  episode_reward_min: 5.0
  episodes_this_iter: 96
  episodes_total: 25440
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12211.96
    learner:
      agent-0:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5083558559417725
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013687259051948786
        model: {}
        policy_loss: -0.003350198734551668
        total_loss: -0.00420415960252285
        vf_explained_var: -0.01868528127670288
        vf_loss: 0.407481849193573
      agent-1:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3971998989582062
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006915371050126851
        model: {}
        policy_loss: -0.002181336283683777
        total_loss: -0.0028673645574599504
        vf_explained_var: 0.009565755724906921
        vf_loss: 0.13042497634887695
      agent-2:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4176824688911438
        entropy_coeff: 0.0017600000137463212
        kl: 0.001148400129750371
        model: {}
        policy_loss: -0.002967870095744729
        total_loss: -0.0036436421796679497
        vf_explained_var: -0.001171022653579712
        vf_loss: 0.5934996604919434
      agent-3:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5966364741325378
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010677343234419823
        model: {}
        policy_loss: -0.0025252080522477627
        total_loss: -0.0035302434116601944
        vf_explained_var: 0.004619210958480835
        vf_loss: 0.450440376996994
      agent-4:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6835850477218628
        entropy_coeff: 0.0017600000137463212
        kl: 0.002144142286852002
        model: {}
        policy_loss: -0.003997057676315308
        total_loss: -0.005144509486854076
        vf_explained_var: 0.025503188371658325
        vf_loss: 0.556576132774353
      agent-5:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.941438615322113
        entropy_coeff: 0.0017600000137463212
        kl: 0.001324522658251226
        model: {}
        policy_loss: -0.0025634290650486946
        total_loss: -0.004191884770989418
        vf_explained_var: 0.0006217658519744873
        vf_loss: 0.2847728729248047
    load_time_ms: 13844.929
    num_steps_sampled: 25440000
    num_steps_trained: 25440000
    sample_time_ms: 100398.362
    update_time_ms: 15.386
  iterations_since_restore: 105
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.31546961325966
    ram_util_percent: 24.915469613259663
  pid: 27405
  policy_reward_max:
    agent-0: 18.0
    agent-1: 7.0
    agent-2: 20.0
    agent-3: 18.0
    agent-4: 15.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 5.01
    agent-1: 2.02
    agent-2: 6.48
    agent-3: 5.17
    agent-4: 6.18
    agent-5: 3.59
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 1.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.308416507984344
    mean_inference_ms: 13.325061863853385
    mean_processing_ms: 59.54150183201497
  time_since_restore: 13356.139786243439
  time_this_iter_s: 127.37430596351624
  time_total_s: 35866.423028707504
  timestamp: 1637233447
  timesteps_since_restore: 10080000
  timesteps_this_iter: 96000
  timesteps_total: 25440000
  training_iteration: 265
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 46.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    265 |          35866.4 | 25440000 |    28.45 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 38
    apples_agent-0_mean: 3.96
    apples_agent-0_min: 0
    apples_agent-1_max: 8
    apples_agent-1_mean: 1.48
    apples_agent-1_min: 0
    apples_agent-2_max: 18
    apples_agent-2_mean: 2.94
    apples_agent-2_min: 0
    apples_agent-3_max: 14
    apples_agent-3_mean: 4.17
    apples_agent-3_min: 0
    apples_agent-4_max: 6
    apples_agent-4_mean: 1.11
    apples_agent-4_min: 0
    apples_agent-5_max: 17
    apples_agent-5_mean: 2.4
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 92
    cleaning_beam_agent-0_mean: 41.24
    cleaning_beam_agent-0_min: 11
    cleaning_beam_agent-1_max: 386
    cleaning_beam_agent-1_mean: 239.45
    cleaning_beam_agent-1_min: 166
    cleaning_beam_agent-2_max: 107
    cleaning_beam_agent-2_mean: 17.45
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 134
    cleaning_beam_agent-3_mean: 58.16
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 168
    cleaning_beam_agent-4_mean: 53.46
    cleaning_beam_agent-4_min: 3
    cleaning_beam_agent-5_max: 67
    cleaning_beam_agent-5_mean: 28.21
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-06-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 77.0
  episode_reward_mean: 31.12
  episode_reward_min: 6.0
  episodes_this_iter: 96
  episodes_total: 25536
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12200.508
    learner:
      agent-0:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5101947784423828
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012672306038439274
        model: {}
        policy_loss: -0.0034626489505171776
        total_loss: -0.004313006065785885
        vf_explained_var: -0.011063069105148315
        vf_loss: 0.47587087750434875
      agent-1:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39390668272972107
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009288361761718988
        model: {}
        policy_loss: -0.002089492743834853
        total_loss: -0.0027566824574023485
        vf_explained_var: 0.016923025250434875
        vf_loss: 0.26085588335990906
      agent-2:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4262438416481018
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007773801335133612
        model: {}
        policy_loss: -0.003053870750591159
        total_loss: -0.003717882325872779
        vf_explained_var: 0.002034813165664673
        vf_loss: 0.8617748618125916
      agent-3:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5917826890945435
        entropy_coeff: 0.0017600000137463212
        kl: 0.000997294788248837
        model: {}
        policy_loss: -0.00269653950817883
        total_loss: -0.0036902364809066057
        vf_explained_var: -0.0028712302446365356
        vf_loss: 0.4784175157546997
      agent-4:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.677286684513092
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015419943956658244
        model: {}
        policy_loss: -0.003654642030596733
        total_loss: -0.004782485775649548
        vf_explained_var: 0.012387111783027649
        vf_loss: 0.6418159008026123
      agent-5:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9600793123245239
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012782941339537501
        model: {}
        policy_loss: -0.0025224732235074043
        total_loss: -0.0041730222292244434
        vf_explained_var: 0.004745453596115112
        vf_loss: 0.391914427280426
    load_time_ms: 13839.181
    num_steps_sampled: 25536000
    num_steps_trained: 25536000
    sample_time_ms: 100487.182
    update_time_ms: 15.376
  iterations_since_restore: 106
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.06022099447514
    ram_util_percent: 25.08674033149172
  pid: 27405
  policy_reward_max:
    agent-0: 12.0
    agent-1: 13.0
    agent-2: 22.0
    agent-3: 19.0
    agent-4: 26.0
    agent-5: 17.0
  policy_reward_mean:
    agent-0: 5.18
    agent-1: 2.95
    agent-2: 7.09
    agent-3: 5.8
    agent-4: 6.21
    agent-5: 3.89
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 1.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.30845199886153
    mean_inference_ms: 13.324575550677398
    mean_processing_ms: 59.53998650721087
  time_since_restore: 13482.88950419426
  time_this_iter_s: 126.74971795082092
  time_total_s: 35993.172746658325
  timestamp: 1637233574
  timesteps_since_restore: 10176000
  timesteps_this_iter: 96000
  timesteps_total: 25536000
  training_iteration: 266
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 46.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    266 |          35993.2 | 25536000 |    31.12 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 3.36
    apples_agent-0_min: 0
    apples_agent-1_max: 8
    apples_agent-1_mean: 1.43
    apples_agent-1_min: 0
    apples_agent-2_max: 13
    apples_agent-2_mean: 2.35
    apples_agent-2_min: 0
    apples_agent-3_max: 14
    apples_agent-3_mean: 3.73
    apples_agent-3_min: 0
    apples_agent-4_max: 20
    apples_agent-4_mean: 2.15
    apples_agent-4_min: 0
    apples_agent-5_max: 14
    apples_agent-5_mean: 2.43
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 96
    cleaning_beam_agent-0_mean: 39.25
    cleaning_beam_agent-0_min: 11
    cleaning_beam_agent-1_max: 308
    cleaning_beam_agent-1_mean: 231.68
    cleaning_beam_agent-1_min: 169
    cleaning_beam_agent-2_max: 44
    cleaning_beam_agent-2_mean: 16.26
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 135
    cleaning_beam_agent-3_mean: 56.61
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 106
    cleaning_beam_agent-4_mean: 45.48
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 67
    cleaning_beam_agent-5_mean: 27.88
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-08-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 60.0
  episode_reward_mean: 30.08
  episode_reward_min: 11.0
  episodes_this_iter: 96
  episodes_total: 25632
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12202.153
    learner:
      agent-0:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4991651177406311
        entropy_coeff: 0.0017600000137463212
        kl: 0.001469117822125554
        model: {}
        policy_loss: -0.003555344184860587
        total_loss: -0.004395956639200449
        vf_explained_var: -0.008456796407699585
        vf_loss: 0.37918373942375183
      agent-1:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3919312655925751
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013778512366116047
        model: {}
        policy_loss: -0.0021798149682581425
        total_loss: -0.002845863113179803
        vf_explained_var: 0.014344021677970886
        vf_loss: 0.23754194378852844
      agent-2:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41775646805763245
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012246284168213606
        model: {}
        policy_loss: -0.0034026270732283592
        total_loss: -0.004063169006258249
        vf_explained_var: 0.0013812631368637085
        vf_loss: 0.7470939755439758
      agent-3:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5787062644958496
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009464742615818977
        model: {}
        policy_loss: -0.002595100086182356
        total_loss: -0.00356385949999094
        vf_explained_var: 0.003038957715034485
        vf_loss: 0.49763691425323486
      agent-4:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6795897483825684
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017309942049905658
        model: {}
        policy_loss: -0.0037073332350701094
        total_loss: -0.004849717952311039
        vf_explained_var: 0.011677056550979614
        vf_loss: 0.5369422435760498
      agent-5:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9353259801864624
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012890675570815802
        model: {}
        policy_loss: -0.002655358985066414
        total_loss: -0.004269207827746868
        vf_explained_var: 0.0037865042686462402
        vf_loss: 0.3232256770133972
    load_time_ms: 13839.035
    num_steps_sampled: 25632000
    num_steps_trained: 25632000
    sample_time_ms: 100462.476
    update_time_ms: 15.127
  iterations_since_restore: 107
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.399444444444434
    ram_util_percent: 25.16611111111111
  pid: 27405
  policy_reward_max:
    agent-0: 12.0
    agent-1: 10.0
    agent-2: 17.0
    agent-3: 14.0
    agent-4: 13.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 4.89
    agent-1: 2.59
    agent-2: 6.79
    agent-3: 5.73
    agent-4: 6.1
    agent-5: 3.98
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.30560451388255
    mean_inference_ms: 13.323635956540597
    mean_processing_ms: 59.537142139855284
  time_since_restore: 13608.995949029922
  time_this_iter_s: 126.10644483566284
  time_total_s: 36119.27919149399
  timestamp: 1637233700
  timesteps_since_restore: 10272000
  timesteps_this_iter: 96000
  timesteps_total: 25632000
  training_iteration: 267
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 46.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    267 |          36119.3 | 25632000 |    30.08 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 49
    apples_agent-0_mean: 3.53
    apples_agent-0_min: 0
    apples_agent-1_max: 15
    apples_agent-1_mean: 1.52
    apples_agent-1_min: 0
    apples_agent-2_max: 36
    apples_agent-2_mean: 2.38
    apples_agent-2_min: 0
    apples_agent-3_max: 24
    apples_agent-3_mean: 3.76
    apples_agent-3_min: 0
    apples_agent-4_max: 17
    apples_agent-4_mean: 1.48
    apples_agent-4_min: 0
    apples_agent-5_max: 30
    apples_agent-5_mean: 2.73
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 78
    cleaning_beam_agent-0_mean: 38.54
    cleaning_beam_agent-0_min: 5
    cleaning_beam_agent-1_max: 355
    cleaning_beam_agent-1_mean: 245.44
    cleaning_beam_agent-1_min: 145
    cleaning_beam_agent-2_max: 69
    cleaning_beam_agent-2_mean: 16.9
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 117
    cleaning_beam_agent-3_mean: 58.46
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 130
    cleaning_beam_agent-4_mean: 45.39
    cleaning_beam_agent-4_min: 8
    cleaning_beam_agent-5_max: 67
    cleaning_beam_agent-5_mean: 29.54
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-10-25
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 70.0
  episode_reward_mean: 26.91
  episode_reward_min: -25.0
  episodes_this_iter: 96
  episodes_total: 25728
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12213.293
    learner:
      agent-0:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5002889633178711
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012149946996942163
        model: {}
        policy_loss: -0.003490699455142021
        total_loss: -0.004334509838372469
        vf_explained_var: -0.008992403745651245
        vf_loss: 0.3669756352901459
      agent-1:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3983273506164551
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009815434459596872
        model: {}
        policy_loss: -0.002163057215511799
        total_loss: -0.002847755327820778
        vf_explained_var: 0.022746950387954712
        vf_loss: 0.1635618954896927
      agent-2:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40931031107902527
        entropy_coeff: 0.0017600000137463212
        kl: 0.001477329060435295
        model: {}
        policy_loss: -0.003141845343634486
        total_loss: -0.003804875072091818
        vf_explained_var: 0.0005239397287368774
        vf_loss: 0.5735727548599243
      agent-3:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5916945934295654
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014765250962227583
        model: {}
        policy_loss: -0.002924262546002865
        total_loss: -0.003913799300789833
        vf_explained_var: -0.0029925554990768433
        vf_loss: 0.5184234976768494
      agent-4:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6784282326698303
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012057590065523982
        model: {}
        policy_loss: -0.002648604102432728
        total_loss: -0.0036661888007074594
        vf_explained_var: 0.007280290126800537
        vf_loss: 1.7645113468170166
      agent-5:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.964992880821228
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008652239921502769
        model: {}
        policy_loss: -0.002499498426914215
        total_loss: -0.00417122058570385
        vf_explained_var: 0.0031012892723083496
        vf_loss: 0.2666305899620056
    load_time_ms: 13845.567
    num_steps_sampled: 25728000
    num_steps_trained: 25728000
    sample_time_ms: 100398.342
    update_time_ms: 15.197
  iterations_since_restore: 108
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.804494382022476
    ram_util_percent: 25.279213483146066
  pid: 27405
  policy_reward_max:
    agent-0: 12.0
    agent-1: 10.0
    agent-2: 19.0
    agent-3: 22.0
    agent-4: 16.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 4.9
    agent-1: 2.23
    agent-2: 6.05
    agent-3: 5.06
    agent-4: 5.24
    agent-5: 3.43
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 1.0
    agent-3: 0.0
    agent-4: -43.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.302765739396982
    mean_inference_ms: 13.322697403229864
    mean_processing_ms: 59.530402456322214
  time_since_restore: 13733.93373465538
  time_this_iter_s: 124.93778562545776
  time_total_s: 36244.216977119446
  timestamp: 1637233825
  timesteps_since_restore: 10368000
  timesteps_this_iter: 96000
  timesteps_total: 25728000
  training_iteration: 268
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 46.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    268 |          36244.2 | 25728000 |    26.91 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 3.09
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 1.29
    apples_agent-1_min: 0
    apples_agent-2_max: 22
    apples_agent-2_mean: 2.71
    apples_agent-2_min: 0
    apples_agent-3_max: 18
    apples_agent-3_mean: 4.07
    apples_agent-3_min: 0
    apples_agent-4_max: 20
    apples_agent-4_mean: 1.48
    apples_agent-4_min: 0
    apples_agent-5_max: 16
    apples_agent-5_mean: 2.43
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 96
    cleaning_beam_agent-0_mean: 36.21
    cleaning_beam_agent-0_min: 6
    cleaning_beam_agent-1_max: 355
    cleaning_beam_agent-1_mean: 241.28
    cleaning_beam_agent-1_min: 167
    cleaning_beam_agent-2_max: 65
    cleaning_beam_agent-2_mean: 18.94
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 128
    cleaning_beam_agent-3_mean: 57.46
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 123
    cleaning_beam_agent-4_mean: 43.36
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 68
    cleaning_beam_agent-5_mean: 28.29
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-12-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 62.0
  episode_reward_mean: 28.75
  episode_reward_min: -12.0
  episodes_this_iter: 96
  episodes_total: 25824
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12203.752
    learner:
      agent-0:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5047982931137085
        entropy_coeff: 0.0017600000137463212
        kl: 0.001384949078783393
        model: {}
        policy_loss: -0.0034744110889732838
        total_loss: -0.004324713256210089
        vf_explained_var: -0.010369151830673218
        vf_loss: 0.3814617693424225
      agent-1:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39460501074790955
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012797603849321604
        model: {}
        policy_loss: -0.002440323121845722
        total_loss: -0.003116878680884838
        vf_explained_var: 0.022867843508720398
        vf_loss: 0.1795012205839157
      agent-2:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41563180088996887
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015413140645250678
        model: {}
        policy_loss: -0.0036729546263813972
        total_loss: -0.004342852160334587
        vf_explained_var: 0.0006939470767974854
        vf_loss: 0.6161649823188782
      agent-3:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5788020491600037
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008074475335888565
        model: {}
        policy_loss: -0.0017557069659233093
        total_loss: -0.002592497505247593
        vf_explained_var: 0.0006227493286132812
        vf_loss: 1.8190207481384277
      agent-4:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6728453636169434
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017765184165909886
        model: {}
        policy_loss: -0.004079038742929697
        total_loss: -0.005214218515902758
        vf_explained_var: 0.006223708391189575
        vf_loss: 0.4903024435043335
      agent-5:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9476409554481506
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013890033587813377
        model: {}
        policy_loss: -0.002487483434379101
        total_loss: -0.0041248993948102
        vf_explained_var: 0.00560249388217926
        vf_loss: 0.30431118607521057
    load_time_ms: 13844.985
    num_steps_sampled: 25824000
    num_steps_trained: 25824000
    sample_time_ms: 100354.153
    update_time_ms: 15.653
  iterations_since_restore: 109
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.7950276243094
    ram_util_percent: 25.3585635359116
  pid: 27405
  policy_reward_max:
    agent-0: 14.0
    agent-1: 11.0
    agent-2: 20.0
    agent-3: 16.0
    agent-4: 17.0
    agent-5: 14.0
  policy_reward_mean:
    agent-0: 4.99
    agent-1: 2.47
    agent-2: 6.42
    agent-3: 5.05
    agent-4: 6.0
    agent-5: 3.82
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -44.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.302267243110354
    mean_inference_ms: 13.32206874069421
    mean_processing_ms: 59.52952401455992
  time_since_restore: 13860.843527793884
  time_this_iter_s: 126.90979313850403
  time_total_s: 36371.12677025795
  timestamp: 1637233952
  timesteps_since_restore: 10464000
  timesteps_this_iter: 96000
  timesteps_total: 25824000
  training_iteration: 269
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 47.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    269 |          36371.1 | 25824000 |    28.75 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 3.09
    apples_agent-0_min: 0
    apples_agent-1_max: 111
    apples_agent-1_mean: 2.66
    apples_agent-1_min: 0
    apples_agent-2_max: 29
    apples_agent-2_mean: 3.35
    apples_agent-2_min: 0
    apples_agent-3_max: 13
    apples_agent-3_mean: 3.92
    apples_agent-3_min: 0
    apples_agent-4_max: 35
    apples_agent-4_mean: 1.94
    apples_agent-4_min: 0
    apples_agent-5_max: 13
    apples_agent-5_mean: 2.24
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 73
    cleaning_beam_agent-0_mean: 31.7
    cleaning_beam_agent-0_min: 9
    cleaning_beam_agent-1_max: 309
    cleaning_beam_agent-1_mean: 241.88
    cleaning_beam_agent-1_min: 142
    cleaning_beam_agent-2_max: 55
    cleaning_beam_agent-2_mean: 16.54
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 109
    cleaning_beam_agent-3_mean: 59.25
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 139
    cleaning_beam_agent-4_mean: 45.95
    cleaning_beam_agent-4_min: 10
    cleaning_beam_agent-5_max: 84
    cleaning_beam_agent-5_mean: 28.74
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-14-39
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 64.0
  episode_reward_mean: 29.58
  episode_reward_min: 8.0
  episodes_this_iter: 96
  episodes_total: 25920
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12201.74
    learner:
      agent-0:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4895133674144745
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011901190737262368
        model: {}
        policy_loss: -0.0032611158676445484
        total_loss: -0.004079547245055437
        vf_explained_var: -0.013103693723678589
        vf_loss: 0.4311155676841736
      agent-1:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4004727602005005
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010944382520392537
        model: {}
        policy_loss: -0.002292739227414131
        total_loss: -0.00298240315169096
        vf_explained_var: 0.01813654601573944
        vf_loss: 0.15169943869113922
      agent-2:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41105249524116516
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013558841310441494
        model: {}
        policy_loss: -0.0034127729013562202
        total_loss: -0.004065028391778469
        vf_explained_var: 0.0009184926748275757
        vf_loss: 0.7119821310043335
      agent-3:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5863522887229919
        entropy_coeff: 0.0017600000137463212
        kl: 0.001380290137603879
        model: {}
        policy_loss: -0.0027736565098166466
        total_loss: -0.0037602046504616737
        vf_explained_var: -0.0019982755184173584
        vf_loss: 0.4543321132659912
      agent-4:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6799944043159485
        entropy_coeff: 0.0017600000137463212
        kl: 0.001515562180429697
        model: {}
        policy_loss: -0.0038593322969973087
        total_loss: -0.0050014229491353035
        vf_explained_var: 0.010264232754707336
        vf_loss: 0.5469964146614075
      agent-5:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9468252658843994
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015225517563521862
        model: {}
        policy_loss: -0.0024114029947668314
        total_loss: -0.004046854563057423
        vf_explained_var: 0.006898611783981323
        vf_loss: 0.30964112281799316
    load_time_ms: 13843.607
    num_steps_sampled: 25920000
    num_steps_trained: 25920000
    sample_time_ms: 100312.987
    update_time_ms: 15.656
  iterations_since_restore: 110
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.14388888888889
    ram_util_percent: 25.477777777777774
  pid: 27405
  policy_reward_max:
    agent-0: 14.0
    agent-1: 8.0
    agent-2: 24.0
    agent-3: 16.0
    agent-4: 14.0
    agent-5: 14.0
  policy_reward_mean:
    agent-0: 5.02
    agent-1: 2.24
    agent-2: 6.66
    agent-3: 5.68
    agent-4: 6.2
    agent-5: 3.78
  policy_reward_min:
    agent-0: 1.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.300107304562726
    mean_inference_ms: 13.321619701750778
    mean_processing_ms: 59.52794074625339
  time_since_restore: 13987.257295131683
  time_this_iter_s: 126.41376733779907
  time_total_s: 36497.54053759575
  timestamp: 1637234079
  timesteps_since_restore: 10560000
  timesteps_this_iter: 96000
  timesteps_total: 25920000
  training_iteration: 270
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 47.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    270 |          36497.5 | 25920000 |    29.58 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 3.28
    apples_agent-0_min: 0
    apples_agent-1_max: 14
    apples_agent-1_mean: 1.54
    apples_agent-1_min: 0
    apples_agent-2_max: 38
    apples_agent-2_mean: 2.87
    apples_agent-2_min: 0
    apples_agent-3_max: 23
    apples_agent-3_mean: 3.74
    apples_agent-3_min: 0
    apples_agent-4_max: 52
    apples_agent-4_mean: 2.05
    apples_agent-4_min: 0
    apples_agent-5_max: 5
    apples_agent-5_mean: 1.46
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 126
    cleaning_beam_agent-0_mean: 35.61
    cleaning_beam_agent-0_min: 7
    cleaning_beam_agent-1_max: 353
    cleaning_beam_agent-1_mean: 246.55
    cleaning_beam_agent-1_min: 177
    cleaning_beam_agent-2_max: 67
    cleaning_beam_agent-2_mean: 18.73
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 129
    cleaning_beam_agent-3_mean: 57.93
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 107
    cleaning_beam_agent-4_mean: 42.38
    cleaning_beam_agent-4_min: 5
    cleaning_beam_agent-5_max: 87
    cleaning_beam_agent-5_mean: 32.31
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-16-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 62.0
  episode_reward_mean: 28.07
  episode_reward_min: -22.0
  episodes_this_iter: 96
  episodes_total: 26016
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12173.965
    learner:
      agent-0:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49797794222831726
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010341159068048
        model: {}
        policy_loss: -0.003250659676268697
        total_loss: -0.004083680920302868
        vf_explained_var: -0.008171409368515015
        vf_loss: 0.43422290682792664
      agent-1:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4078112840652466
        entropy_coeff: 0.0017600000137463212
        kl: 0.001199847087264061
        model: {}
        policy_loss: -0.0024131410755217075
        total_loss: -0.003115016734227538
        vf_explained_var: 0.028363525867462158
        vf_loss: 0.1587148755788803
      agent-2:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43285515904426575
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013304336462169886
        model: {}
        policy_loss: -0.00351836159825325
        total_loss: -0.004221267532557249
        vf_explained_var: -0.006401151418685913
        vf_loss: 0.5891885757446289
      agent-3:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5970684885978699
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010702160652726889
        model: {}
        policy_loss: -0.001902356743812561
        total_loss: -0.002777247689664364
        vf_explained_var: 0.0009480118751525879
        vf_loss: 1.759494662284851
      agent-4:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6800978779792786
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017911584582179785
        model: {}
        policy_loss: -0.0038671744987368584
        total_loss: -0.00500144436955452
        vf_explained_var: 0.01996295154094696
        vf_loss: 0.6270221471786499
      agent-5:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9497345685958862
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015431152423843741
        model: {}
        policy_loss: -0.0024563653860241175
        total_loss: -0.00410446897149086
        vf_explained_var: 0.006455555558204651
        vf_loss: 0.2342659831047058
    load_time_ms: 13854.774
    num_steps_sampled: 26016000
    num_steps_trained: 26016000
    sample_time_ms: 100425.813
    update_time_ms: 15.756
  iterations_since_restore: 111
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 46.777472527472526
    ram_util_percent: 22.545604395604396
  pid: 27405
  policy_reward_max:
    agent-0: 18.0
    agent-1: 7.0
    agent-2: 23.0
    agent-3: 16.0
    agent-4: 18.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 5.26
    agent-1: 2.45
    agent-2: 6.29
    agent-3: 4.79
    agent-4: 5.95
    agent-5: 3.33
  policy_reward_min:
    agent-0: 1.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -49.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.300238962553298
    mean_inference_ms: 13.321355021857006
    mean_processing_ms: 59.531745829717266
  time_since_restore: 14114.227573633194
  time_this_iter_s: 126.97027850151062
  time_total_s: 36624.51081609726
  timestamp: 1637234207
  timesteps_since_restore: 10656000
  timesteps_this_iter: 96000
  timesteps_total: 26016000
  training_iteration: 271
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 36.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    271 |          36624.5 | 26016000 |    28.07 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 3.19
    apples_agent-0_min: 0
    apples_agent-1_max: 33
    apples_agent-1_mean: 1.62
    apples_agent-1_min: 0
    apples_agent-2_max: 12
    apples_agent-2_mean: 2.73
    apples_agent-2_min: 0
    apples_agent-3_max: 15
    apples_agent-3_mean: 3.72
    apples_agent-3_min: 0
    apples_agent-4_max: 17
    apples_agent-4_mean: 1.66
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 1.76
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 77
    cleaning_beam_agent-0_mean: 33.25
    cleaning_beam_agent-0_min: 5
    cleaning_beam_agent-1_max: 349
    cleaning_beam_agent-1_mean: 242.96
    cleaning_beam_agent-1_min: 131
    cleaning_beam_agent-2_max: 50
    cleaning_beam_agent-2_mean: 17.27
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 112
    cleaning_beam_agent-3_mean: 56.73
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 96
    cleaning_beam_agent-4_mean: 45.47
    cleaning_beam_agent-4_min: 14
    cleaning_beam_agent-5_max: 70
    cleaning_beam_agent-5_mean: 30.02
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-18-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 63.0
  episode_reward_mean: 28.28
  episode_reward_min: -15.0
  episodes_this_iter: 96
  episodes_total: 26112
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12179.997
    learner:
      agent-0:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5081189870834351
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013542212545871735
        model: {}
        policy_loss: -0.0018821879057213664
        total_loss: -0.0025912895798683167
        vf_explained_var: -0.004308164119720459
        vf_loss: 1.8518612384796143
      agent-1:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40195080637931824
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008603811147622764
        model: {}
        policy_loss: -0.0020568049512803555
        total_loss: -0.0027454709634184837
        vf_explained_var: 0.02011817693710327
        vf_loss: 0.1876748502254486
      agent-2:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42094528675079346
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012490394292399287
        model: {}
        policy_loss: -0.0033352167811244726
        total_loss: -0.004004409071058035
        vf_explained_var: -0.0032347142696380615
        vf_loss: 0.7166711688041687
      agent-3:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5814299583435059
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014610975049436092
        model: {}
        policy_loss: -0.002929126378148794
        total_loss: -0.003910547122359276
        vf_explained_var: 0.003082275390625
        vf_loss: 0.41896313428878784
      agent-4:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6773042678833008
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016732646618038416
        model: {}
        policy_loss: -0.004213942680507898
        total_loss: -0.0053465659730136395
        vf_explained_var: 0.0247204452753067
        vf_loss: 0.594314455986023
      agent-5:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9594353437423706
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013109666761010885
        model: {}
        policy_loss: -0.002712271409109235
        total_loss: -0.00437516113743186
        vf_explained_var: 0.005564287304878235
        vf_loss: 0.25716203451156616
    load_time_ms: 13859.009
    num_steps_sampled: 26112000
    num_steps_trained: 26112000
    sample_time_ms: 100297.858
    update_time_ms: 15.577
  iterations_since_restore: 112
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 48.36277777777777
    ram_util_percent: 19.602777777777778
  pid: 27405
  policy_reward_max:
    agent-0: 15.0
    agent-1: 7.0
    agent-2: 21.0
    agent-3: 15.0
    agent-4: 18.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 4.73
    agent-1: 1.76
    agent-2: 6.92
    agent-3: 4.91
    agent-4: 6.4
    agent-5: 3.56
  policy_reward_min:
    agent-0: -48.0
    agent-1: -47.0
    agent-2: 1.0
    agent-3: 0.0
    agent-4: 1.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.29809051350287
    mean_inference_ms: 13.320539156651456
    mean_processing_ms: 59.5307979381482
  time_since_restore: 14240.724325180054
  time_this_iter_s: 126.49675154685974
  time_total_s: 36751.00756764412
  timestamp: 1637234333
  timesteps_since_restore: 10752000
  timesteps_this_iter: 96000
  timesteps_total: 26112000
  training_iteration: 272
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 36.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    272 |            36751 | 26112000 |    28.28 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 22
    apples_agent-0_mean: 3.15
    apples_agent-0_min: 0
    apples_agent-1_max: 9
    apples_agent-1_mean: 1.48
    apples_agent-1_min: 0
    apples_agent-2_max: 34
    apples_agent-2_mean: 2.77
    apples_agent-2_min: 0
    apples_agent-3_max: 35
    apples_agent-3_mean: 5.17
    apples_agent-3_min: 0
    apples_agent-4_max: 16
    apples_agent-4_mean: 1.72
    apples_agent-4_min: 0
    apples_agent-5_max: 13
    apples_agent-5_mean: 2.04
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 77
    cleaning_beam_agent-0_mean: 35.25
    cleaning_beam_agent-0_min: 5
    cleaning_beam_agent-1_max: 331
    cleaning_beam_agent-1_mean: 248.08
    cleaning_beam_agent-1_min: 150
    cleaning_beam_agent-2_max: 47
    cleaning_beam_agent-2_mean: 15.97
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 100
    cleaning_beam_agent-3_mean: 52.41
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 143
    cleaning_beam_agent-4_mean: 45.14
    cleaning_beam_agent-4_min: 5
    cleaning_beam_agent-5_max: 73
    cleaning_beam_agent-5_mean: 33.72
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-20-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 67.0
  episode_reward_mean: 29.16
  episode_reward_min: -32.0
  episodes_this_iter: 96
  episodes_total: 26208
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12183.931
    learner:
      agent-0:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5021916627883911
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012838559923693538
        model: {}
        policy_loss: -0.001790294423699379
        total_loss: -0.002359674545004964
        vf_explained_var: -0.004157423973083496
        vf_loss: 3.1447935104370117
      agent-1:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4038638472557068
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012751608155667782
        model: {}
        policy_loss: -0.002313352655619383
        total_loss: -0.0030044051818549633
        vf_explained_var: 0.03195229172706604
        vf_loss: 0.19747745990753174
      agent-2:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4059305191040039
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014044424751773477
        model: {}
        policy_loss: -0.003074000356718898
        total_loss: -0.0037252851761877537
        vf_explained_var: -0.002563104033470154
        vf_loss: 0.6315157413482666
      agent-3:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.580693781375885
        entropy_coeff: 0.0017600000137463212
        kl: 0.001389635493978858
        model: {}
        policy_loss: -0.0027499778661876917
        total_loss: -0.003725932678207755
        vf_explained_var: -0.002094060182571411
        vf_loss: 0.4606771171092987
      agent-4:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6776210069656372
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015337240183725953
        model: {}
        policy_loss: -0.004104592837393284
        total_loss: -0.005237464793026447
        vf_explained_var: 0.00433659553527832
        vf_loss: 0.5973583459854126
      agent-5:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9487476944923401
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015594575088471174
        model: {}
        policy_loss: -0.002777794376015663
        total_loss: -0.004420465789735317
        vf_explained_var: -8.36104154586792e-05
        vf_loss: 0.2712646722793579
    load_time_ms: 13894.061
    num_steps_sampled: 26208000
    num_steps_trained: 26208000
    sample_time_ms: 100186.917
    update_time_ms: 15.783
  iterations_since_restore: 113
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 50.311731843575416
    ram_util_percent: 19.91396648044693
  pid: 27405
  policy_reward_max:
    agent-0: 17.0
    agent-1: 10.0
    agent-2: 16.0
    agent-3: 15.0
    agent-4: 17.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 4.45
    agent-1: 2.04
    agent-2: 6.87
    agent-3: 5.43
    agent-4: 6.5
    agent-5: 3.87
  policy_reward_min:
    agent-0: -50.0
    agent-1: -49.0
    agent-2: 1.0
    agent-3: 1.0
    agent-4: 1.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.296181599777952
    mean_inference_ms: 13.319988069239017
    mean_processing_ms: 59.52721541256989
  time_since_restore: 14366.447510242462
  time_this_iter_s: 125.72318506240845
  time_total_s: 36876.73075270653
  timestamp: 1637234459
  timesteps_since_restore: 10848000
  timesteps_this_iter: 96000
  timesteps_total: 26208000
  training_iteration: 273
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 36.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    273 |          36876.7 | 26208000 |    29.16 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 2.88
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 1.42
    apples_agent-1_min: 0
    apples_agent-2_max: 35
    apples_agent-2_mean: 2.97
    apples_agent-2_min: 0
    apples_agent-3_max: 12
    apples_agent-3_mean: 3.54
    apples_agent-3_min: 0
    apples_agent-4_max: 28
    apples_agent-4_mean: 1.6
    apples_agent-4_min: 0
    apples_agent-5_max: 12
    apples_agent-5_mean: 2.02
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 82
    cleaning_beam_agent-0_mean: 35.1
    cleaning_beam_agent-0_min: 8
    cleaning_beam_agent-1_max: 326
    cleaning_beam_agent-1_mean: 237.64
    cleaning_beam_agent-1_min: 117
    cleaning_beam_agent-2_max: 64
    cleaning_beam_agent-2_mean: 17.87
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 125
    cleaning_beam_agent-3_mean: 55.93
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 120
    cleaning_beam_agent-4_mean: 43.82
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 65
    cleaning_beam_agent-5_mean: 30.85
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-23-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 68.0
  episode_reward_mean: 27.76
  episode_reward_min: -8.0
  episodes_this_iter: 96
  episodes_total: 26304
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12183.429
    learner:
      agent-0:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4927797317504883
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012581082992255688
        model: {}
        policy_loss: -0.0035267435014247894
        total_loss: -0.004351822659373283
        vf_explained_var: -0.013749659061431885
        vf_loss: 0.4221453070640564
      agent-1:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40204888582229614
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013819427695125341
        model: {}
        policy_loss: -0.00222573010250926
        total_loss: -0.0029144510626792908
        vf_explained_var: 0.028584346175193787
        vf_loss: 0.18885856866836548
      agent-2:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4154816269874573
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011397699126973748
        model: {}
        policy_loss: -0.003466663882136345
        total_loss: -0.004135113209486008
        vf_explained_var: -0.0020782798528671265
        vf_loss: 0.6279966831207275
      agent-3:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5742108821868896
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017074684146791697
        model: {}
        policy_loss: -0.002655930817127228
        total_loss: -0.0036238646134734154
        vf_explained_var: -2.7135014533996582e-05
        vf_loss: 0.4267600178718567
      agent-4:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6834267377853394
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015726035926491022
        model: {}
        policy_loss: -0.0037612037267535925
        total_loss: -0.004909649956971407
        vf_explained_var: 0.0140455961227417
        vf_loss: 0.543854296207428
      agent-5:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9541140794754028
        entropy_coeff: 0.0017600000137463212
        kl: 0.001236573327332735
        model: {}
        policy_loss: -0.002442506607621908
        total_loss: -0.004001375753432512
        vf_explained_var: 0.0011472702026367188
        vf_loss: 1.2037063837051392
    load_time_ms: 13912.464
    num_steps_sampled: 26304000
    num_steps_trained: 26304000
    sample_time_ms: 100257.225
    update_time_ms: 15.747
  iterations_since_restore: 114
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.41436464088398
    ram_util_percent: 20.12817679558011
  pid: 27405
  policy_reward_max:
    agent-0: 16.0
    agent-1: 18.0
    agent-2: 18.0
    agent-3: 15.0
    agent-4: 18.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 4.76
    agent-1: 2.3
    agent-2: 6.63
    agent-3: 5.09
    agent-4: 6.1
    agent-5: 2.88
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -46.0
  sampler_perf:
    mean_env_wait_ms: 23.295362311486457
    mean_inference_ms: 13.319782100196614
    mean_processing_ms: 59.5281337379381
  time_since_restore: 14493.43334197998
  time_this_iter_s: 126.98583173751831
  time_total_s: 37003.716584444046
  timestamp: 1637234586
  timesteps_since_restore: 10944000
  timesteps_this_iter: 96000
  timesteps_total: 26304000
  training_iteration: 274
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 37.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    274 |          37003.7 | 26304000 |    27.76 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 2.73
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 1.3
    apples_agent-1_min: 0
    apples_agent-2_max: 15
    apples_agent-2_mean: 2.38
    apples_agent-2_min: 0
    apples_agent-3_max: 14
    apples_agent-3_mean: 3.87
    apples_agent-3_min: 0
    apples_agent-4_max: 21
    apples_agent-4_mean: 1.38
    apples_agent-4_min: 0
    apples_agent-5_max: 34
    apples_agent-5_mean: 2.72
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 82
    cleaning_beam_agent-0_mean: 34.05
    cleaning_beam_agent-0_min: 6
    cleaning_beam_agent-1_max: 398
    cleaning_beam_agent-1_mean: 237.68
    cleaning_beam_agent-1_min: 177
    cleaning_beam_agent-2_max: 167
    cleaning_beam_agent-2_mean: 18.54
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 106
    cleaning_beam_agent-3_mean: 51.13
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 132
    cleaning_beam_agent-4_mean: 44.61
    cleaning_beam_agent-4_min: 6
    cleaning_beam_agent-5_max: 108
    cleaning_beam_agent-5_mean: 29.48
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-25-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 66.0
  episode_reward_mean: 28.06
  episode_reward_min: -21.0
  episodes_this_iter: 96
  episodes_total: 26400
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12170.637
    learner:
      agent-0:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4884723126888275
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013967030681669712
        model: {}
        policy_loss: -0.0036261500790715218
        total_loss: -0.004448080435395241
        vf_explained_var: -0.010096639394760132
        vf_loss: 0.37779906392097473
      agent-1:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4084984064102173
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005807660636492074
        model: {}
        policy_loss: -0.0020685705821961164
        total_loss: -0.002768970560282469
        vf_explained_var: 0.003198757767677307
        vf_loss: 0.1856069266796112
      agent-2:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4072020649909973
        entropy_coeff: 0.0017600000137463212
        kl: 0.001294372254051268
        model: {}
        policy_loss: -0.003161119995638728
        total_loss: -0.0038121358957141638
        vf_explained_var: 0.004053935408592224
        vf_loss: 0.6565656661987305
      agent-3:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5742532014846802
        entropy_coeff: 0.0017600000137463212
        kl: 0.001178060658276081
        model: {}
        policy_loss: -0.002682008780539036
        total_loss: -0.003641212359070778
        vf_explained_var: 0.0008042603731155396
        vf_loss: 0.5148249864578247
      agent-4:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6737052798271179
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014144473243504763
        model: {}
        policy_loss: -0.004025615751743317
        total_loss: -0.005168858915567398
        vf_explained_var: 0.013918593525886536
        vf_loss: 0.4247991740703583
      agent-5:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9407184720039368
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018894914537668228
        model: {}
        policy_loss: -0.00258902320638299
        total_loss: -0.004116085357964039
        vf_explained_var: 0.0038471221923828125
        vf_loss: 1.2860413789749146
    load_time_ms: 13900.195
    num_steps_sampled: 26400000
    num_steps_trained: 26400000
    sample_time_ms: 100162.535
    update_time_ms: 15.735
  iterations_since_restore: 115
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.78833333333334
    ram_util_percent: 20.416666666666668
  pid: 27405
  policy_reward_max:
    agent-0: 14.0
    agent-1: 10.0
    agent-2: 18.0
    agent-3: 19.0
    agent-4: 14.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 4.82
    agent-1: 2.59
    agent-2: 6.9
    agent-3: 5.38
    agent-4: 5.27
    agent-5: 3.1
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 1.0
    agent-4: 0.0
    agent-5: -44.0
  sampler_perf:
    mean_env_wait_ms: 23.292562390933867
    mean_inference_ms: 13.319388591739846
    mean_processing_ms: 59.52668046816727
  time_since_restore: 14619.582698822021
  time_this_iter_s: 126.14935684204102
  time_total_s: 37129.86594128609
  timestamp: 1637234712
  timesteps_since_restore: 11040000
  timesteps_this_iter: 96000
  timesteps_total: 26400000
  training_iteration: 275
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 37.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    275 |          37129.9 | 26400000 |    28.06 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 2.72
    apples_agent-0_min: 0
    apples_agent-1_max: 12
    apples_agent-1_mean: 1.34
    apples_agent-1_min: 0
    apples_agent-2_max: 17
    apples_agent-2_mean: 2.57
    apples_agent-2_min: 0
    apples_agent-3_max: 28
    apples_agent-3_mean: 4.07
    apples_agent-3_min: 0
    apples_agent-4_max: 5
    apples_agent-4_mean: 1.14
    apples_agent-4_min: 0
    apples_agent-5_max: 14
    apples_agent-5_mean: 2.04
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 200
    cleaning_beam_agent-0_mean: 34.52
    cleaning_beam_agent-0_min: 10
    cleaning_beam_agent-1_max: 310
    cleaning_beam_agent-1_mean: 230.16
    cleaning_beam_agent-1_min: 146
    cleaning_beam_agent-2_max: 48
    cleaning_beam_agent-2_mean: 16.36
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 113
    cleaning_beam_agent-3_mean: 50.18
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 122
    cleaning_beam_agent-4_mean: 44.68
    cleaning_beam_agent-4_min: 7
    cleaning_beam_agent-5_max: 62
    cleaning_beam_agent-5_mean: 29.64
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-27-18
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 63.0
  episode_reward_mean: 29.46
  episode_reward_min: 6.0
  episodes_this_iter: 96
  episodes_total: 26496
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12173.597
    learner:
      agent-0:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5052376985549927
        entropy_coeff: 0.0017600000137463212
        kl: 0.001586858881637454
        model: {}
        policy_loss: -0.0034509908873587847
        total_loss: -0.004295257851481438
        vf_explained_var: -0.01501283049583435
        vf_loss: 0.44949814677238464
      agent-1:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40968483686447144
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010804941412061453
        model: {}
        policy_loss: -0.0024056187830865383
        total_loss: -0.0031094681471586227
        vf_explained_var: 0.02233542501926422
        vf_loss: 0.17194166779518127
      agent-2:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4021148383617401
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012649591080844402
        model: {}
        policy_loss: -0.0034160532522946596
        total_loss: -0.0040586525574326515
        vf_explained_var: -0.0033900290727615356
        vf_loss: 0.6512381434440613
      agent-3:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5896793603897095
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013717417605221272
        model: {}
        policy_loss: -0.003091588616371155
        total_loss: -0.004086491651833057
        vf_explained_var: -0.0009355098009109497
        vf_loss: 0.42931604385375977
      agent-4:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6732794642448425
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011056538205593824
        model: {}
        policy_loss: -0.00375943211838603
        total_loss: -0.004890669137239456
        vf_explained_var: 0.0104084312915802
        vf_loss: 0.5373691320419312
      agent-5:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9383870363235474
        entropy_coeff: 0.0017600000137463212
        kl: 0.0026298812590539455
        model: {}
        policy_loss: -0.002694563241675496
        total_loss: -0.004316386301070452
        vf_explained_var: 0.002324730157852173
        vf_loss: 0.2973700761795044
    load_time_ms: 13887.3
    num_steps_sampled: 26496000
    num_steps_trained: 26496000
    sample_time_ms: 100054.014
    update_time_ms: 15.662
  iterations_since_restore: 116
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.1122905027933
    ram_util_percent: 20.598882681564245
  pid: 27405
  policy_reward_max:
    agent-0: 20.0
    agent-1: 11.0
    agent-2: 17.0
    agent-3: 13.0
    agent-4: 16.0
    agent-5: 16.0
  policy_reward_mean:
    agent-0: 4.89
    agent-1: 2.35
    agent-2: 7.16
    agent-3: 5.29
    agent-4: 6.18
    agent-5: 3.59
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.290681665000992
    mean_inference_ms: 13.319196865631227
    mean_processing_ms: 59.52471557506022
  time_since_restore: 14745.145549058914
  time_this_iter_s: 125.5628502368927
  time_total_s: 37255.42879152298
  timestamp: 1637234838
  timesteps_since_restore: 11136000
  timesteps_this_iter: 96000
  timesteps_total: 26496000
  training_iteration: 276
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 38.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    276 |          37255.4 | 26496000 |    29.46 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 23
    apples_agent-0_mean: 2.87
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 1.27
    apples_agent-1_min: 0
    apples_agent-2_max: 48
    apples_agent-2_mean: 2.97
    apples_agent-2_min: 0
    apples_agent-3_max: 13
    apples_agent-3_mean: 3.64
    apples_agent-3_min: 0
    apples_agent-4_max: 32
    apples_agent-4_mean: 1.53
    apples_agent-4_min: 0
    apples_agent-5_max: 13
    apples_agent-5_mean: 2.03
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 103
    cleaning_beam_agent-0_mean: 35.3
    cleaning_beam_agent-0_min: 10
    cleaning_beam_agent-1_max: 292
    cleaning_beam_agent-1_mean: 225.44
    cleaning_beam_agent-1_min: 172
    cleaning_beam_agent-2_max: 64
    cleaning_beam_agent-2_mean: 16.69
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 104
    cleaning_beam_agent-3_mean: 53.9
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 125
    cleaning_beam_agent-4_mean: 41.5
    cleaning_beam_agent-4_min: 12
    cleaning_beam_agent-5_max: 68
    cleaning_beam_agent-5_mean: 28.01
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-29-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 68.0
  episode_reward_mean: 26.54
  episode_reward_min: -37.0
  episodes_this_iter: 96
  episodes_total: 26592
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12183.565
    learner:
      agent-0:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4945559501647949
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006678951904177666
        model: {}
        policy_loss: -0.0016835769638419151
        total_loss: -0.002264014445245266
        vf_explained_var: -0.0022855550050735474
        vf_loss: 2.8998451232910156
      agent-1:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.401573121547699
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009436543332412839
        model: {}
        policy_loss: -0.0015498162247240543
        total_loss: -0.0021108314394950867
        vf_explained_var: 0.007867783308029175
        vf_loss: 1.4575459957122803
      agent-2:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.408757746219635
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009711185120977461
        model: {}
        policy_loss: -0.0030959355644881725
        total_loss: -0.003753006923943758
        vf_explained_var: -0.00024768710136413574
        vf_loss: 0.6234214901924133
      agent-3:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5877014398574829
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011607387568801641
        model: {}
        policy_loss: -0.002555636689066887
        total_loss: -0.003547566942870617
        vf_explained_var: 0.002752542495727539
        vf_loss: 0.424238383769989
      agent-4:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6620599031448364
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016029790276661515
        model: {}
        policy_loss: -0.003838864155113697
        total_loss: -0.00495018158107996
        vf_explained_var: -0.006369978189468384
        vf_loss: 0.5390485525131226
      agent-5:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9439475536346436
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015570942778140306
        model: {}
        policy_loss: -0.002873268909752369
        total_loss: -0.004506905563175678
        vf_explained_var: 0.004632696509361267
        vf_loss: 0.27710503339767456
    load_time_ms: 13871.391
    num_steps_sampled: 26592000
    num_steps_trained: 26592000
    sample_time_ms: 100035.581
    update_time_ms: 15.749
  iterations_since_restore: 117
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.23351955307262
    ram_util_percent: 20.821787709497205
  pid: 27405
  policy_reward_max:
    agent-0: 14.0
    agent-1: 8.0
    agent-2: 21.0
    agent-3: 14.0
    agent-4: 17.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 3.44
    agent-1: 2.04
    agent-2: 6.33
    agent-3: 5.16
    agent-4: 5.96
    agent-5: 3.61
  policy_reward_min:
    agent-0: -46.0
    agent-1: -48.0
    agent-2: 1.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.287577687416253
    mean_inference_ms: 13.318410273024801
    mean_processing_ms: 59.52044437789684
  time_since_restore: 14871.014206171036
  time_this_iter_s: 125.86865711212158
  time_total_s: 37381.2974486351
  timestamp: 1637234964
  timesteps_since_restore: 11232000
  timesteps_this_iter: 96000
  timesteps_total: 26592000
  training_iteration: 277
  trial_id: '00000'
  
[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
== Status ==
Memory usage on this node: 38.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    277 |          37381.3 | 26592000 |    26.54 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 44
    apples_agent-0_mean: 3.61
    apples_agent-0_min: 0
    apples_agent-1_max: 8
    apples_agent-1_mean: 1.2
    apples_agent-1_min: 0
    apples_agent-2_max: 19
    apples_agent-2_mean: 2.25
    apples_agent-2_min: 0
    apples_agent-3_max: 15
    apples_agent-3_mean: 3.36
    apples_agent-3_min: 0
    apples_agent-4_max: 7
    apples_agent-4_mean: 1.29
    apples_agent-4_min: 0
    apples_agent-5_max: 18
    apples_agent-5_mean: 2.15
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 78
    cleaning_beam_agent-0_mean: 33.71
    cleaning_beam_agent-0_min: 8
    cleaning_beam_agent-1_max: 305
    cleaning_beam_agent-1_mean: 234.79
    cleaning_beam_agent-1_min: 134
    cleaning_beam_agent-2_max: 64
    cleaning_beam_agent-2_mean: 16.42
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 118
    cleaning_beam_agent-3_mean: 51.77
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 115
    cleaning_beam_agent-4_mean: 44.6
    cleaning_beam_agent-4_min: 5
    cleaning_beam_agent-5_max: 82
    cleaning_beam_agent-5_mean: 27.28
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-31-30
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 64.0
  episode_reward_mean: 26.9
  episode_reward_min: -28.0
  episodes_this_iter: 96
  episodes_total: 26688
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12186.497
    learner:
      agent-0:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4994766116142273
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012873300584033132
        model: {}
        policy_loss: -0.0035682267043739557
        total_loss: -0.004404498264193535
        vf_explained_var: -0.012086957693099976
        vf_loss: 0.42807650566101074
      agent-1:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4032524824142456
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012192734284326434
        model: {}
        policy_loss: -0.002501850947737694
        total_loss: -0.0031991014257073402
        vf_explained_var: 0.02007809281349182
        vf_loss: 0.12472283095121384
      agent-2:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41135695576667786
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014285389333963394
        model: {}
        policy_loss: -0.0024612448178231716
        total_loss: -0.003000287339091301
        vf_explained_var: 0.0024522095918655396
        vf_loss: 1.8494653701782227
      agent-3:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5937438011169434
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010016867890954018
        model: {}
        policy_loss: -0.002871876349672675
        total_loss: -0.0038798179011791945
        vf_explained_var: 0.0002484172582626343
        vf_loss: 0.3704826235771179
      agent-4:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6772486567497253
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016238319221884012
        model: {}
        policy_loss: -0.003922114614397287
        total_loss: -0.005060678347945213
        vf_explained_var: 0.0022592395544052124
        vf_loss: 0.5339601635932922
      agent-5:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9508334398269653
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020268855150789022
        model: {}
        policy_loss: -0.003025372512638569
        total_loss: -0.004670393653213978
        vf_explained_var: 0.00515119731426239
        vf_loss: 0.2844608426094055
    load_time_ms: 13838.912
    num_steps_sampled: 26688000
    num_steps_trained: 26688000
    sample_time_ms: 100145.776
    update_time_ms: 15.927
  iterations_since_restore: 118
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.525842696629205
    ram_util_percent: 21.05561797752809
  pid: 27405
  policy_reward_max:
    agent-0: 14.0
    agent-1: 8.0
    agent-2: 21.0
    agent-3: 16.0
    agent-4: 18.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 5.2
    agent-1: 2.06
    agent-2: 5.46
    agent-3: 4.74
    agent-4: 5.88
    agent-5: 3.56
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -44.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.285639823758217
    mean_inference_ms: 13.318532705807353
    mean_processing_ms: 59.51988495791326
  time_since_restore: 14996.754920721054
  time_this_iter_s: 125.74071455001831
  time_total_s: 37507.03816318512
  timestamp: 1637235090
  timesteps_since_restore: 11328000
  timesteps_this_iter: 96000
  timesteps_total: 26688000
  training_iteration: 278
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 38.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    278 |            37507 | 26688000 |     26.9 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 3.07
    apples_agent-0_min: 0
    apples_agent-1_max: 32
    apples_agent-1_mean: 1.9
    apples_agent-1_min: 0
    apples_agent-2_max: 12
    apples_agent-2_mean: 2.64
    apples_agent-2_min: 0
    apples_agent-3_max: 15
    apples_agent-3_mean: 4.05
    apples_agent-3_min: 0
    apples_agent-4_max: 14
    apples_agent-4_mean: 1.48
    apples_agent-4_min: 0
    apples_agent-5_max: 9
    apples_agent-5_mean: 2.13
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 137
    cleaning_beam_agent-0_mean: 31.72
    cleaning_beam_agent-0_min: 7
    cleaning_beam_agent-1_max: 336
    cleaning_beam_agent-1_mean: 233.93
    cleaning_beam_agent-1_min: 178
    cleaning_beam_agent-2_max: 72
    cleaning_beam_agent-2_mean: 17.15
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 113
    cleaning_beam_agent-3_mean: 50.22
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 112
    cleaning_beam_agent-4_mean: 45.29
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 79
    cleaning_beam_agent-5_mean: 33.42
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-33-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 64.0
  episode_reward_mean: 27.57
  episode_reward_min: -12.0
  episodes_this_iter: 96
  episodes_total: 26784
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12188.213
    learner:
      agent-0:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48868390917778015
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017342985374853015
        model: {}
        policy_loss: -0.003322392236441374
        total_loss: -0.004144582897424698
        vf_explained_var: -0.01044657826423645
        vf_loss: 0.37894338369369507
      agent-1:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4115294814109802
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012613330036401749
        model: {}
        policy_loss: -0.002463611541315913
        total_loss: -0.003176657250151038
        vf_explained_var: 0.025921553373336792
        vf_loss: 0.11250775307416916
      agent-2:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41421419382095337
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014181036967784166
        model: {}
        policy_loss: -0.003667708020657301
        total_loss: -0.004326936323195696
        vf_explained_var: 0.0010582059621810913
        vf_loss: 0.6978773474693298
      agent-3:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5876970291137695
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007824930362403393
        model: {}
        policy_loss: -0.002529607154428959
        total_loss: -0.00351318484172225
        vf_explained_var: 0.003749743103981018
        vf_loss: 0.5077252984046936
      agent-4:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6711746454238892
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012208614498376846
        model: {}
        policy_loss: -0.0035618033725768328
        total_loss: -0.004693996161222458
        vf_explained_var: 0.008509933948516846
        vf_loss: 0.4907263219356537
      agent-5:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9717232584953308
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012764406856149435
        model: {}
        policy_loss: -0.002142820507287979
        total_loss: -0.00370628759264946
        vf_explained_var: -0.0002700239419937134
        vf_loss: 1.4676567316055298
    load_time_ms: 13832.107
    num_steps_sampled: 26784000
    num_steps_trained: 26784000
    sample_time_ms: 100203.393
    update_time_ms: 15.678
  iterations_since_restore: 119
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.712637362637366
    ram_util_percent: 21.177472527472528
  pid: 27405
  policy_reward_max:
    agent-0: 14.0
    agent-1: 8.0
    agent-2: 20.0
    agent-3: 17.0
    agent-4: 14.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 4.6
    agent-1: 1.97
    agent-2: 6.94
    agent-3: 5.79
    agent-4: 5.85
    agent-5: 2.42
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 1.0
    agent-4: 0.0
    agent-5: -48.0
  sampler_perf:
    mean_env_wait_ms: 23.284209886458058
    mean_inference_ms: 13.318021961490523
    mean_processing_ms: 59.51933566425035
  time_since_restore: 15124.166230678558
  time_this_iter_s: 127.41130995750427
  time_total_s: 37634.449473142624
  timestamp: 1637235218
  timesteps_since_restore: 11424000
  timesteps_this_iter: 96000
  timesteps_total: 26784000
  training_iteration: 279
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 39.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    279 |          37634.4 | 26784000 |    27.57 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 14
    apples_agent-0_mean: 3.14
    apples_agent-0_min: 0
    apples_agent-1_max: 9
    apples_agent-1_mean: 1.47
    apples_agent-1_min: 0
    apples_agent-2_max: 123
    apples_agent-2_mean: 3.77
    apples_agent-2_min: 0
    apples_agent-3_max: 13
    apples_agent-3_mean: 3.87
    apples_agent-3_min: 0
    apples_agent-4_max: 6
    apples_agent-4_mean: 1.28
    apples_agent-4_min: 0
    apples_agent-5_max: 54
    apples_agent-5_mean: 2.91
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 105
    cleaning_beam_agent-0_mean: 31.55
    cleaning_beam_agent-0_min: 6
    cleaning_beam_agent-1_max: 317
    cleaning_beam_agent-1_mean: 233.15
    cleaning_beam_agent-1_min: 143
    cleaning_beam_agent-2_max: 108
    cleaning_beam_agent-2_mean: 20.11
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 165
    cleaning_beam_agent-3_mean: 51.57
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 102
    cleaning_beam_agent-4_mean: 47.54
    cleaning_beam_agent-4_min: 10
    cleaning_beam_agent-5_max: 79
    cleaning_beam_agent-5_mean: 28.01
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-35-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 57.0
  episode_reward_mean: 27.73
  episode_reward_min: -75.0
  episodes_this_iter: 96
  episodes_total: 26880
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12171.324
    learner:
      agent-0:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49474117159843445
        entropy_coeff: 0.0017600000137463212
        kl: 0.001544437138363719
        model: {}
        policy_loss: -0.0035650888457894325
        total_loss: -0.004398812539875507
        vf_explained_var: -0.00391755998134613
        vf_loss: 0.37025779485702515
      agent-1:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40743288397789
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009958279551938176
        model: {}
        policy_loss: -0.0024224547669291496
        total_loss: -0.0031264452263712883
        vf_explained_var: 0.02748565375804901
        vf_loss: 0.13090652227401733
      agent-2:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4105483889579773
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010607800213620067
        model: {}
        policy_loss: -0.003510209498926997
        total_loss: -0.004170921165496111
        vf_explained_var: -0.0008601993322372437
        vf_loss: 0.6185294389724731
      agent-3:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.574200451374054
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010856942972168326
        model: {}
        policy_loss: -0.0025952798314392567
        total_loss: -0.0035621458664536476
        vf_explained_var: -0.0012263357639312744
        vf_loss: 0.4372769892215729
      agent-4:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6773746013641357
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017086195293813944
        model: {}
        policy_loss: -0.004173912573605776
        total_loss: -0.005314937327057123
        vf_explained_var: -0.0002561807632446289
        vf_loss: 0.5115057229995728
      agent-5:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9562389850616455
        entropy_coeff: 0.0017600000137463212
        kl: 0.002000089269131422
        model: {}
        policy_loss: -0.002325505018234253
        total_loss: -0.00397629477083683
        vf_explained_var: 0.004621326923370361
        vf_loss: 0.32192182540893555
    load_time_ms: 13855.308
    num_steps_sampled: 26880000
    num_steps_trained: 26880000
    sample_time_ms: 100111.371
    update_time_ms: 15.734
  iterations_since_restore: 120
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.322905027932954
    ram_util_percent: 21.439106145251397
  pid: 27405
  policy_reward_max:
    agent-0: 13.0
    agent-1: 6.0
    agent-2: 21.0
    agent-3: 12.0
    agent-4: 20.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 4.82
    agent-1: 2.34
    agent-2: 5.99
    agent-3: 5.3
    agent-4: 6.02
    agent-5: 3.26
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -46.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -46.0
  sampler_perf:
    mean_env_wait_ms: 23.28121840775079
    mean_inference_ms: 13.317421629095225
    mean_processing_ms: 59.51500998045295
  time_since_restore: 15249.720302581787
  time_this_iter_s: 125.55407190322876
  time_total_s: 37760.00354504585
  timestamp: 1637235343
  timesteps_since_restore: 11520000
  timesteps_this_iter: 96000
  timesteps_total: 26880000
  training_iteration: 280
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 39.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    280 |            37760 | 26880000 |    27.73 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 25
    apples_agent-0_mean: 2.95
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 1.24
    apples_agent-1_min: 0
    apples_agent-2_max: 23
    apples_agent-2_mean: 2.23
    apples_agent-2_min: 0
    apples_agent-3_max: 20
    apples_agent-3_mean: 3.4
    apples_agent-3_min: 0
    apples_agent-4_max: 20
    apples_agent-4_mean: 1.95
    apples_agent-4_min: 0
    apples_agent-5_max: 45
    apples_agent-5_mean: 2.87
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 92
    cleaning_beam_agent-0_mean: 32.81
    cleaning_beam_agent-0_min: 10
    cleaning_beam_agent-1_max: 312
    cleaning_beam_agent-1_mean: 240.24
    cleaning_beam_agent-1_min: 161
    cleaning_beam_agent-2_max: 71
    cleaning_beam_agent-2_mean: 19.48
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 124
    cleaning_beam_agent-3_mean: 48.11
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 114
    cleaning_beam_agent-4_mean: 46.54
    cleaning_beam_agent-4_min: 10
    cleaning_beam_agent-5_max: 60
    cleaning_beam_agent-5_mean: 24.72
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-37-50
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 62.0
  episode_reward_mean: 24.41
  episode_reward_min: -92.0
  episodes_this_iter: 96
  episodes_total: 26976
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12198.074
    learner:
      agent-0:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4900188446044922
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012042650487273932
        model: {}
        policy_loss: -0.003430986078456044
        total_loss: -0.004252471495419741
        vf_explained_var: -0.007775813341140747
        vf_loss: 0.40949007868766785
      agent-1:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4059331715106964
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011408517602831125
        model: {}
        policy_loss: -0.0021580951288342476
        total_loss: -0.0028597728814929724
        vf_explained_var: 0.03288620710372925
        vf_loss: 0.12763360142707825
      agent-2:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40673086047172546
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014164452441036701
        model: {}
        policy_loss: -0.0033491812646389008
        total_loss: -0.004005500115454197
        vf_explained_var: -0.0010506361722946167
        vf_loss: 0.5952990055084229
      agent-3:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5621283054351807
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010209594620391726
        model: {}
        policy_loss: -0.002604263834655285
        total_loss: -0.0035540559329092503
        vf_explained_var: -0.0011789798736572266
        vf_loss: 0.3955191373825073
      agent-4:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6806579232215881
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018532912945374846
        model: {}
        policy_loss: -0.003834642469882965
        total_loss: -0.00498268473893404
        vf_explained_var: 0.008169770240783691
        vf_loss: 0.49914541840553284
      agent-5:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.932587742805481
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011619023280218244
        model: {}
        policy_loss: -0.002678905613720417
        total_loss: -0.00429762527346611
        vf_explained_var: 0.0035237669944763184
        vf_loss: 0.2263401597738266
    load_time_ms: 13841.584
    num_steps_sampled: 26976000
    num_steps_trained: 26976000
    sample_time_ms: 99990.372
    update_time_ms: 15.644
  iterations_since_restore: 121
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.49111111111112
    ram_util_percent: 21.616111111111113
  pid: 27405
  policy_reward_max:
    agent-0: 11.0
    agent-1: 8.0
    agent-2: 25.0
    agent-3: 13.0
    agent-4: 14.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 3.68
    agent-1: 2.05
    agent-2: 5.52
    agent-3: 3.96
    agent-4: 6.1
    agent-5: 3.1
  policy_reward_min:
    agent-0: -49.0
    agent-1: 0.0
    agent-2: -50.0
    agent-3: -47.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.279162276263488
    mean_inference_ms: 13.316800380652953
    mean_processing_ms: 59.51405755182261
  time_since_restore: 15375.613533258438
  time_this_iter_s: 125.893230676651
  time_total_s: 37885.896775722504
  timestamp: 1637235470
  timesteps_since_restore: 11616000
  timesteps_this_iter: 96000
  timesteps_total: 26976000
  training_iteration: 281
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 40.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    281 |          37885.9 | 26976000 |    24.41 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 3.2
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 1.39
    apples_agent-1_min: 0
    apples_agent-2_max: 17
    apples_agent-2_mean: 2.68
    apples_agent-2_min: 0
    apples_agent-3_max: 103
    apples_agent-3_mean: 5.19
    apples_agent-3_min: 0
    apples_agent-4_max: 75
    apples_agent-4_mean: 2.22
    apples_agent-4_min: 0
    apples_agent-5_max: 11
    apples_agent-5_mean: 2.14
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 109
    cleaning_beam_agent-0_mean: 35.01
    cleaning_beam_agent-0_min: 8
    cleaning_beam_agent-1_max: 349
    cleaning_beam_agent-1_mean: 239.8
    cleaning_beam_agent-1_min: 150
    cleaning_beam_agent-2_max: 58
    cleaning_beam_agent-2_mean: 15.65
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 98
    cleaning_beam_agent-3_mean: 57.57
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 105
    cleaning_beam_agent-4_mean: 42.57
    cleaning_beam_agent-4_min: 6
    cleaning_beam_agent-5_max: 69
    cleaning_beam_agent-5_mean: 24.89
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-39-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 63.0
  episode_reward_mean: 29.41
  episode_reward_min: 11.0
  episodes_this_iter: 96
  episodes_total: 27072
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12187.553
    learner:
      agent-0:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4943871796131134
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012323278933763504
        model: {}
        policy_loss: -0.0034126369282603264
        total_loss: -0.004245742689818144
        vf_explained_var: -0.01300676167011261
        vf_loss: 0.3701704740524292
      agent-1:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40496930480003357
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010392675176262856
        model: {}
        policy_loss: -0.0024984292685985565
        total_loss: -0.003195560071617365
        vf_explained_var: 0.028594762086868286
        vf_loss: 0.15615932643413544
      agent-2:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40467569231987
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009730178862810135
        model: {}
        policy_loss: -0.003195755183696747
        total_loss: -0.003840625286102295
        vf_explained_var: 0.0018492192029953003
        vf_loss: 0.6735644936561584
      agent-3:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5749786496162415
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013134332839399576
        model: {}
        policy_loss: -0.0026268630754202604
        total_loss: -0.0035897106863558292
        vf_explained_var: 0.004150718450546265
        vf_loss: 0.4911515414714813
      agent-4:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6791152954101562
        entropy_coeff: 0.0017600000137463212
        kl: 0.001383865950629115
        model: {}
        policy_loss: -0.0034900661557912827
        total_loss: -0.00462345639243722
        vf_explained_var: 0.013543188571929932
        vf_loss: 0.6184787154197693
      agent-5:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9555837512016296
        entropy_coeff: 0.0017600000137463212
        kl: 0.001565558835864067
        model: {}
        policy_loss: -0.0025714871007949114
        total_loss: -0.004231941886246204
        vf_explained_var: 0.0025115013122558594
        vf_loss: 0.21372240781784058
    load_time_ms: 13829.393
    num_steps_sampled: 27072000
    num_steps_trained: 27072000
    sample_time_ms: 100018.713
    update_time_ms: 16.018
  iterations_since_restore: 122
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 50.00555555555555
    ram_util_percent: 21.79388888888889
  pid: 27405
  policy_reward_max:
    agent-0: 14.0
    agent-1: 7.0
    agent-2: 21.0
    agent-3: 17.0
    agent-4: 19.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 4.85
    agent-1: 2.32
    agent-2: 7.01
    agent-3: 5.56
    agent-4: 6.49
    agent-5: 3.18
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 1.0
    agent-3: 1.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.2787777713005
    mean_inference_ms: 13.316684141816813
    mean_processing_ms: 59.51406347676356
  time_since_restore: 15502.19990992546
  time_this_iter_s: 126.5863766670227
  time_total_s: 38012.48315238953
  timestamp: 1637235597
  timesteps_since_restore: 11712000
  timesteps_this_iter: 96000
  timesteps_total: 27072000
  training_iteration: 282
  trial_id: '00000'
  
[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
== Status ==
Memory usage on this node: 40.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    282 |          38012.5 | 27072000 |    29.41 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 3.21
    apples_agent-0_min: 0
    apples_agent-1_max: 10
    apples_agent-1_mean: 1.39
    apples_agent-1_min: 0
    apples_agent-2_max: 33
    apples_agent-2_mean: 2.41
    apples_agent-2_min: 0
    apples_agent-3_max: 11
    apples_agent-3_mean: 3.79
    apples_agent-3_min: 0
    apples_agent-4_max: 7
    apples_agent-4_mean: 1.25
    apples_agent-4_min: 0
    apples_agent-5_max: 13
    apples_agent-5_mean: 2.35
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 77
    cleaning_beam_agent-0_mean: 29.83
    cleaning_beam_agent-0_min: 3
    cleaning_beam_agent-1_max: 302
    cleaning_beam_agent-1_mean: 240.61
    cleaning_beam_agent-1_min: 158
    cleaning_beam_agent-2_max: 78
    cleaning_beam_agent-2_mean: 16.68
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 102
    cleaning_beam_agent-3_mean: 50.83
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 97
    cleaning_beam_agent-4_mean: 48.4
    cleaning_beam_agent-4_min: 15
    cleaning_beam_agent-5_max: 61
    cleaning_beam_agent-5_mean: 26.24
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-42-05
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 62.0
  episode_reward_mean: 29.02
  episode_reward_min: 7.0
  episodes_this_iter: 96
  episodes_total: 27168
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12188.376
    learner:
      agent-0:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4813684821128845
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010901386849582195
        model: {}
        policy_loss: -0.0032642423175275326
        total_loss: -0.004069858230650425
        vf_explained_var: -0.005183190107345581
        vf_loss: 0.41593632102012634
      agent-1:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41324371099472046
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015481560258194804
        model: {}
        policy_loss: -0.0025815076660364866
        total_loss: -0.0032936525531113148
        vf_explained_var: 0.01267765462398529
        vf_loss: 0.15166741609573364
      agent-2:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4057348072528839
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013001438928768039
        model: {}
        policy_loss: -0.003303895704448223
        total_loss: -0.003964882809668779
        vf_explained_var: -0.001534193754196167
        vf_loss: 0.531074583530426
      agent-3:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5661804676055908
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010350726079195738
        model: {}
        policy_loss: -0.002685540122911334
        total_loss: -0.003641004441305995
        vf_explained_var: -0.00026075541973114014
        vf_loss: 0.4101105034351349
      agent-4:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6942209005355835
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017548734322190285
        model: {}
        policy_loss: -0.004007450304925442
        total_loss: -0.005179652012884617
        vf_explained_var: 0.00044080615043640137
        vf_loss: 0.4962645471096039
      agent-5:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9634225368499756
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016851222608238459
        model: {}
        policy_loss: -0.0026137293316423893
        total_loss: -0.00428179744631052
        vf_explained_var: 0.004844591021537781
        vf_loss: 0.27554675936698914
    load_time_ms: 13808.226
    num_steps_sampled: 27168000
    num_steps_trained: 27168000
    sample_time_ms: 100243.75
    update_time_ms: 15.829
  iterations_since_restore: 123
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.7207650273224
    ram_util_percent: 21.991803278688526
  pid: 27405
  policy_reward_max:
    agent-0: 11.0
    agent-1: 6.0
    agent-2: 16.0
    agent-3: 13.0
    agent-4: 17.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 5.05
    agent-1: 2.17
    agent-2: 6.42
    agent-3: 5.49
    agent-4: 6.08
    agent-5: 3.81
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.280688233786055
    mean_inference_ms: 13.31727762461302
    mean_processing_ms: 59.51943549850678
  time_since_restore: 15629.978190422058
  time_this_iter_s: 127.77828049659729
  time_total_s: 38140.261432886124
  timestamp: 1637235725
  timesteps_since_restore: 11808000
  timesteps_this_iter: 96000
  timesteps_total: 27168000
  training_iteration: 283
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 40.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    283 |          38140.3 | 27168000 |    29.02 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 3.33
    apples_agent-0_min: 0
    apples_agent-1_max: 14
    apples_agent-1_mean: 1.26
    apples_agent-1_min: 0
    apples_agent-2_max: 10
    apples_agent-2_mean: 2.49
    apples_agent-2_min: 0
    apples_agent-3_max: 18
    apples_agent-3_mean: 3.99
    apples_agent-3_min: 0
    apples_agent-4_max: 5
    apples_agent-4_mean: 1.22
    apples_agent-4_min: 0
    apples_agent-5_max: 11
    apples_agent-5_mean: 2.23
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 81
    cleaning_beam_agent-0_mean: 31.05
    cleaning_beam_agent-0_min: 6
    cleaning_beam_agent-1_max: 314
    cleaning_beam_agent-1_mean: 229.96
    cleaning_beam_agent-1_min: 151
    cleaning_beam_agent-2_max: 69
    cleaning_beam_agent-2_mean: 17.19
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 109
    cleaning_beam_agent-3_mean: 47.92
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 126
    cleaning_beam_agent-4_mean: 50.02
    cleaning_beam_agent-4_min: 16
    cleaning_beam_agent-5_max: 57
    cleaning_beam_agent-5_mean: 26.07
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-44-11
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 69.0
  episode_reward_mean: 28.56
  episode_reward_min: 6.0
  episodes_this_iter: 96
  episodes_total: 27264
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12207.844
    learner:
      agent-0:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4838734269142151
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014004823751747608
        model: {}
        policy_loss: -0.0034983817022293806
        total_loss: -0.004309393931180239
        vf_explained_var: -0.004783228039741516
        vf_loss: 0.40606099367141724
      agent-1:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4072501063346863
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010946206748485565
        model: {}
        policy_loss: -0.002405008301138878
        total_loss: -0.0031058453023433685
        vf_explained_var: 0.009678870439529419
        vf_loss: 0.15922021865844727
      agent-2:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4061870574951172
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011053477646782994
        model: {}
        policy_loss: -0.0034202085807919502
        total_loss: -0.004078146070241928
        vf_explained_var: -0.005357563495635986
        vf_loss: 0.5695277452468872
      agent-3:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.559622049331665
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017196258995682001
        model: {}
        policy_loss: -0.002727195620536804
        total_loss: -0.0036677494645118713
        vf_explained_var: 0.003788173198699951
        vf_loss: 0.4437907934188843
      agent-4:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.681373655796051
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012990555260330439
        model: {}
        policy_loss: -0.003906844183802605
        total_loss: -0.005053439177572727
        vf_explained_var: 0.00979432463645935
        vf_loss: 0.5262314081192017
      agent-5:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9468053579330444
        entropy_coeff: 0.0017600000137463212
        kl: 0.001480257953517139
        model: {}
        policy_loss: -0.002472538501024246
        total_loss: -0.004113895818591118
        vf_explained_var: 0.004278600215911865
        vf_loss: 0.25022226572036743
    load_time_ms: 13787.961
    num_steps_sampled: 27264000
    num_steps_trained: 27264000
    sample_time_ms: 100140.038
    update_time_ms: 16.482
  iterations_since_restore: 124
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.42122905027933
    ram_util_percent: 22.184916201117318
  pid: 27405
  policy_reward_max:
    agent-0: 16.0
    agent-1: 8.0
    agent-2: 16.0
    agent-3: 15.0
    agent-4: 19.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 4.81
    agent-1: 2.26
    agent-2: 6.72
    agent-3: 5.17
    agent-4: 5.97
    agent-5: 3.63
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 1.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.279591061609043
    mean_inference_ms: 13.317079764471616
    mean_processing_ms: 59.51949083011827
  time_since_restore: 15755.866516113281
  time_this_iter_s: 125.88832569122314
  time_total_s: 38266.14975857735
  timestamp: 1637235851
  timesteps_since_restore: 11904000
  timesteps_this_iter: 96000
  timesteps_total: 27264000
  training_iteration: 284
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 41.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    284 |          38266.1 | 27264000 |    28.56 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 25
    apples_agent-0_mean: 3.09
    apples_agent-0_min: 0
    apples_agent-1_max: 19
    apples_agent-1_mean: 1.51
    apples_agent-1_min: 0
    apples_agent-2_max: 11
    apples_agent-2_mean: 2.38
    apples_agent-2_min: 0
    apples_agent-3_max: 20
    apples_agent-3_mean: 3.85
    apples_agent-3_min: 0
    apples_agent-4_max: 14
    apples_agent-4_mean: 1.54
    apples_agent-4_min: 0
    apples_agent-5_max: 17
    apples_agent-5_mean: 2.07
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 87
    cleaning_beam_agent-0_mean: 31.71
    cleaning_beam_agent-0_min: 8
    cleaning_beam_agent-1_max: 339
    cleaning_beam_agent-1_mean: 227.77
    cleaning_beam_agent-1_min: 155
    cleaning_beam_agent-2_max: 83
    cleaning_beam_agent-2_mean: 16.66
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 101
    cleaning_beam_agent-3_mean: 46.42
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 145
    cleaning_beam_agent-4_mean: 49.61
    cleaning_beam_agent-4_min: 8
    cleaning_beam_agent-5_max: 70
    cleaning_beam_agent-5_mean: 25.33
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-46-17
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 73.0
  episode_reward_mean: 29.76
  episode_reward_min: 10.0
  episodes_this_iter: 96
  episodes_total: 27360
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12210.331
    learner:
      agent-0:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4800507426261902
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013668108731508255
        model: {}
        policy_loss: -0.003410492092370987
        total_loss: -0.004215184599161148
        vf_explained_var: -0.012778878211975098
        vf_loss: 0.4019179344177246
      agent-1:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39854443073272705
        entropy_coeff: 0.0017600000137463212
        kl: 0.001084622461348772
        model: {}
        policy_loss: -0.0023804570082575083
        total_loss: -0.003063895972445607
        vf_explained_var: 0.020520120859146118
        vf_loss: 0.17997747659683228
      agent-2:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40136900544166565
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013253869256004691
        model: {}
        policy_loss: -0.003101787529885769
        total_loss: -0.0037533636204898357
        vf_explained_var: 0.0005676746368408203
        vf_loss: 0.5483081340789795
      agent-3:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5628932118415833
        entropy_coeff: 0.0017600000137463212
        kl: 0.001321566989645362
        model: {}
        policy_loss: -0.002781120827421546
        total_loss: -0.0037229075096547604
        vf_explained_var: -0.00015749037265777588
        vf_loss: 0.4890705347061157
      agent-4:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6889090538024902
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013487169053405523
        model: {}
        policy_loss: -0.0039093270897865295
        total_loss: -0.005058547481894493
        vf_explained_var: 0.0015894919633865356
        vf_loss: 0.6326171159744263
      agent-5:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9394546747207642
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011537253158167005
        model: {}
        policy_loss: -0.0024966432247310877
        total_loss: -0.004118537530303001
        vf_explained_var: -0.001662224531173706
        vf_loss: 0.31544041633605957
    load_time_ms: 13805.024
    num_steps_sampled: 27360000
    num_steps_trained: 27360000
    sample_time_ms: 100137.765
    update_time_ms: 16.566
  iterations_since_restore: 125
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.33
    ram_util_percent: 22.302777777777777
  pid: 27405
  policy_reward_max:
    agent-0: 13.0
    agent-1: 8.0
    agent-2: 20.0
    agent-3: 19.0
    agent-4: 21.0
    agent-5: 14.0
  policy_reward_mean:
    agent-0: 5.08
    agent-1: 2.45
    agent-2: 6.55
    agent-3: 5.55
    agent-4: 6.31
    agent-5: 3.82
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.277586326968645
    mean_inference_ms: 13.317015922411224
    mean_processing_ms: 59.51835754352747
  time_since_restore: 15882.223824501038
  time_this_iter_s: 126.35730838775635
  time_total_s: 38392.5070669651
  timestamp: 1637235977
  timesteps_since_restore: 12000000
  timesteps_this_iter: 96000
  timesteps_total: 27360000
  training_iteration: 285
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 41.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    285 |          38392.5 | 27360000 |    29.76 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 25
    apples_agent-0_mean: 2.78
    apples_agent-0_min: 0
    apples_agent-1_max: 10
    apples_agent-1_mean: 1.28
    apples_agent-1_min: 0
    apples_agent-2_max: 9
    apples_agent-2_mean: 2.17
    apples_agent-2_min: 0
    apples_agent-3_max: 19
    apples_agent-3_mean: 4.1
    apples_agent-3_min: 0
    apples_agent-4_max: 33
    apples_agent-4_mean: 1.63
    apples_agent-4_min: 0
    apples_agent-5_max: 29
    apples_agent-5_mean: 2.05
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 66
    cleaning_beam_agent-0_mean: 29.44
    cleaning_beam_agent-0_min: 5
    cleaning_beam_agent-1_max: 290
    cleaning_beam_agent-1_mean: 224.63
    cleaning_beam_agent-1_min: 159
    cleaning_beam_agent-2_max: 45
    cleaning_beam_agent-2_mean: 15.52
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 98
    cleaning_beam_agent-3_mean: 43.25
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 123
    cleaning_beam_agent-4_mean: 45.18
    cleaning_beam_agent-4_min: 5
    cleaning_beam_agent-5_max: 52
    cleaning_beam_agent-5_mean: 24.9
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-48-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 59.0
  episode_reward_mean: 26.54
  episode_reward_min: -32.0
  episodes_this_iter: 96
  episodes_total: 27456
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12199.184
    learner:
      agent-0:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4800834655761719
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010890283156186342
        model: {}
        policy_loss: -0.00311516341753304
        total_loss: -0.0037262127734720707
        vf_explained_var: -0.002330377697944641
        vf_loss: 2.3389594554901123
      agent-1:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4022354483604431
        entropy_coeff: 0.0017600000137463212
        kl: 0.001134777208790183
        model: {}
        policy_loss: -0.0022613571491092443
        total_loss: -0.0029568634927272797
        vf_explained_var: 0.02471005916595459
        vf_loss: 0.12429537624120712
      agent-2:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40823638439178467
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012968742521479726
        model: {}
        policy_loss: -0.0034972531720995903
        total_loss: -0.004163721110671759
        vf_explained_var: 0.003367185592651367
        vf_loss: 0.5202659368515015
      agent-3:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5635431408882141
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012774630449712276
        model: {}
        policy_loss: -0.0028740791603922844
        total_loss: -0.0038236789405345917
        vf_explained_var: 0.003764927387237549
        vf_loss: 0.42236292362213135
      agent-4:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6798155903816223
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013861546758562326
        model: {}
        policy_loss: -0.004010799340903759
        total_loss: -0.005162814166396856
        vf_explained_var: 0.010538250207901001
        vf_loss: 0.4446154534816742
      agent-5:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9476358890533447
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014769085682928562
        model: {}
        policy_loss: -0.0025992821902036667
        total_loss: -0.004241849761456251
        vf_explained_var: -0.002549991011619568
        vf_loss: 0.25274425745010376
    load_time_ms: 13795.403
    num_steps_sampled: 27456000
    num_steps_trained: 27456000
    sample_time_ms: 100188.241
    update_time_ms: 16.839
  iterations_since_restore: 126
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.05083798882682
    ram_util_percent: 22.4536312849162
  pid: 27405
  policy_reward_max:
    agent-0: 14.0
    agent-1: 9.0
    agent-2: 14.0
    agent-3: 18.0
    agent-4: 15.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 3.86
    agent-1: 2.02
    agent-2: 6.15
    agent-3: 5.32
    agent-4: 5.6
    agent-5: 3.59
  policy_reward_min:
    agent-0: -47.0
    agent-1: 0.0
    agent-2: 1.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.274875992768504
    mean_inference_ms: 13.316299564868638
    mean_processing_ms: 59.516530817831274
  time_since_restore: 16008.03843331337
  time_this_iter_s: 125.81460881233215
  time_total_s: 38518.321675777435
  timestamp: 1637236103
  timesteps_since_restore: 12096000
  timesteps_this_iter: 96000
  timesteps_total: 27456000
  training_iteration: 286
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 41.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    286 |          38518.3 | 27456000 |    26.54 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 28
    apples_agent-0_mean: 3.57
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 1.23
    apples_agent-1_min: 0
    apples_agent-2_max: 76
    apples_agent-2_mean: 3.39
    apples_agent-2_min: 0
    apples_agent-3_max: 17
    apples_agent-3_mean: 4.01
    apples_agent-3_min: 0
    apples_agent-4_max: 6
    apples_agent-4_mean: 1.24
    apples_agent-4_min: 0
    apples_agent-5_max: 85
    apples_agent-5_mean: 3.49
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 115
    cleaning_beam_agent-0_mean: 33.07
    cleaning_beam_agent-0_min: 7
    cleaning_beam_agent-1_max: 307
    cleaning_beam_agent-1_mean: 226.94
    cleaning_beam_agent-1_min: 110
    cleaning_beam_agent-2_max: 39
    cleaning_beam_agent-2_mean: 14.91
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 110
    cleaning_beam_agent-3_mean: 46.41
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 101
    cleaning_beam_agent-4_mean: 51.66
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 56
    cleaning_beam_agent-5_mean: 24.71
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-50-30
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 64.0
  episode_reward_mean: 28.11
  episode_reward_min: -32.0
  episodes_this_iter: 96
  episodes_total: 27552
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12181.488
    learner:
      agent-0:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.496873140335083
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016752042574808002
        model: {}
        policy_loss: -0.00362183153629303
        total_loss: -0.004452133551239967
        vf_explained_var: -0.01292949914932251
        vf_loss: 0.4419458508491516
      agent-1:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3988024890422821
        entropy_coeff: 0.0017600000137463212
        kl: 0.002143674064427614
        model: {}
        policy_loss: -0.0029644391033798456
        total_loss: -0.003520107362419367
        vf_explained_var: 0.008074745535850525
        vf_loss: 1.4622430801391602
      agent-2:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39955973625183105
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013168849982321262
        model: {}
        policy_loss: -0.0033216793090105057
        total_loss: -0.003966745920479298
        vf_explained_var: 0.0014526993036270142
        vf_loss: 0.5815917253494263
      agent-3:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5637683868408203
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014354645973071456
        model: {}
        policy_loss: -0.002420235425233841
        total_loss: -0.003368486650288105
        vf_explained_var: 0.002396315336227417
        vf_loss: 0.4398539066314697
      agent-4:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6979275345802307
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016586660640314221
        model: {}
        policy_loss: -0.004101371858268976
        total_loss: -0.0052824318408966064
        vf_explained_var: 0.0038671791553497314
        vf_loss: 0.47291627526283264
      agent-5:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.940102219581604
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017017642967402935
        model: {}
        policy_loss: -0.0025032470002770424
        total_loss: -0.00412445142865181
        vf_explained_var: 0.006127193570137024
        vf_loss: 0.3337612450122833
    load_time_ms: 13836.744
    num_steps_sampled: 27552000
    num_steps_trained: 27552000
    sample_time_ms: 100286.472
    update_time_ms: 16.8
  iterations_since_restore: 127
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 48.885635359116016
    ram_util_percent: 22.546408839779005
  pid: 27405
  policy_reward_max:
    agent-0: 16.0
    agent-1: 8.0
    agent-2: 22.0
    agent-3: 16.0
    agent-4: 13.0
    agent-5: 14.0
  policy_reward_mean:
    agent-0: 4.73
    agent-1: 1.87
    agent-2: 6.48
    agent-3: 5.51
    agent-4: 5.59
    agent-5: 3.93
  policy_reward_min:
    agent-0: -47.0
    agent-1: -49.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.274510624988288
    mean_inference_ms: 13.316498807968985
    mean_processing_ms: 59.52102020578123
  time_since_restore: 16135.197354078293
  time_this_iter_s: 127.1589207649231
  time_total_s: 38645.48059654236
  timestamp: 1637236230
  timesteps_since_restore: 12192000
  timesteps_this_iter: 96000
  timesteps_total: 27552000
  training_iteration: 287
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 42.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    287 |          38645.5 | 27552000 |    28.11 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 18
    apples_agent-0_mean: 3.07
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 1.39
    apples_agent-1_min: 0
    apples_agent-2_max: 9
    apples_agent-2_mean: 2.45
    apples_agent-2_min: 0
    apples_agent-3_max: 23
    apples_agent-3_mean: 3.57
    apples_agent-3_min: 0
    apples_agent-4_max: 18
    apples_agent-4_mean: 1.29
    apples_agent-4_min: 0
    apples_agent-5_max: 44
    apples_agent-5_mean: 2.62
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 77
    cleaning_beam_agent-0_mean: 33.63
    cleaning_beam_agent-0_min: 7
    cleaning_beam_agent-1_max: 308
    cleaning_beam_agent-1_mean: 228.74
    cleaning_beam_agent-1_min: 158
    cleaning_beam_agent-2_max: 42
    cleaning_beam_agent-2_mean: 13.3
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 92
    cleaning_beam_agent-3_mean: 44.06
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 150
    cleaning_beam_agent-4_mean: 50.25
    cleaning_beam_agent-4_min: 16
    cleaning_beam_agent-5_max: 55
    cleaning_beam_agent-5_mean: 20.96
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-52-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 81.0
  episode_reward_mean: 28.26
  episode_reward_min: 8.0
  episodes_this_iter: 96
  episodes_total: 27648
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12178.526
    learner:
      agent-0:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4931560456752777
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017622800078243017
        model: {}
        policy_loss: -0.003713890677317977
        total_loss: -0.004541379865258932
        vf_explained_var: -0.00938504934310913
        vf_loss: 0.40463584661483765
      agent-1:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40463000535964966
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010530976578593254
        model: {}
        policy_loss: -0.002542370930314064
        total_loss: -0.0032390248961746693
        vf_explained_var: 0.014670521020889282
        vf_loss: 0.15494601428508759
      agent-2:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3929518163204193
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011315703159198165
        model: {}
        policy_loss: -0.0031915921717882156
        total_loss: -0.00382324680685997
        vf_explained_var: 0.002976924180984497
        vf_loss: 0.5993877649307251
      agent-3:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5642910599708557
        entropy_coeff: 0.0017600000137463212
        kl: 0.001351666753180325
        model: {}
        policy_loss: -0.0026916000060737133
        total_loss: -0.0036400393582880497
        vf_explained_var: -0.00013399124145507812
        vf_loss: 0.4471249580383301
      agent-4:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6991373896598816
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014988036127761006
        model: {}
        policy_loss: -0.0037989462725818157
        total_loss: -0.004977363161742687
        vf_explained_var: 0.0024937093257904053
        vf_loss: 0.5206655263900757
      agent-5:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9357259273529053
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015055052936077118
        model: {}
        policy_loss: -0.002522567752748728
        total_loss: -0.00414046598598361
        vf_explained_var: 0.0016562789678573608
        vf_loss: 0.28981083631515503
    load_time_ms: 13873.024
    num_steps_sampled: 27648000
    num_steps_trained: 27648000
    sample_time_ms: 100353.026
    update_time_ms: 16.8
  iterations_since_restore: 128
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.51381215469613
    ram_util_percent: 22.75027624309393
  pid: 27405
  policy_reward_max:
    agent-0: 15.0
    agent-1: 8.0
    agent-2: 17.0
    agent-3: 15.0
    agent-4: 26.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 4.89
    agent-1: 2.41
    agent-2: 6.44
    agent-3: 5.14
    agent-4: 5.79
    agent-5: 3.59
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.273326838731613
    mean_inference_ms: 13.316278445817174
    mean_processing_ms: 59.52182245637849
  time_since_restore: 16261.94410610199
  time_this_iter_s: 126.7467520236969
  time_total_s: 38772.227348566055
  timestamp: 1637236357
  timesteps_since_restore: 12288000
  timesteps_this_iter: 96000
  timesteps_total: 27648000
  training_iteration: 288
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 42.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    288 |          38772.2 | 27648000 |    28.26 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 3.82
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 1.43
    apples_agent-1_min: 0
    apples_agent-2_max: 30
    apples_agent-2_mean: 2.2
    apples_agent-2_min: 0
    apples_agent-3_max: 11
    apples_agent-3_mean: 3.53
    apples_agent-3_min: 0
    apples_agent-4_max: 21
    apples_agent-4_mean: 1.67
    apples_agent-4_min: 0
    apples_agent-5_max: 50
    apples_agent-5_mean: 2.78
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 82
    cleaning_beam_agent-0_mean: 29.34
    cleaning_beam_agent-0_min: 4
    cleaning_beam_agent-1_max: 299
    cleaning_beam_agent-1_mean: 223.69
    cleaning_beam_agent-1_min: 148
    cleaning_beam_agent-2_max: 45
    cleaning_beam_agent-2_mean: 15.3
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 99
    cleaning_beam_agent-3_mean: 39.7
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 122
    cleaning_beam_agent-4_mean: 47.95
    cleaning_beam_agent-4_min: 15
    cleaning_beam_agent-5_max: 58
    cleaning_beam_agent-5_mean: 23.38
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-54-44
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 59.0
  episode_reward_mean: 27.35
  episode_reward_min: -14.0
  episodes_this_iter: 96
  episodes_total: 27744
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12177.335
    learner:
      agent-0:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4848201870918274
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013479702174663544
        model: {}
        policy_loss: -0.003464135807007551
        total_loss: -0.004279278218746185
        vf_explained_var: -0.004813283681869507
        vf_loss: 0.38144612312316895
      agent-1:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3998650908470154
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009346264414489269
        model: {}
        policy_loss: -0.002189785474911332
        total_loss: -0.002877899445593357
        vf_explained_var: 0.01514078676700592
        vf_loss: 0.1565033495426178
      agent-2:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40620851516723633
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017820782959461212
        model: {}
        policy_loss: -0.0029885098338127136
        total_loss: -0.0035153860226273537
        vf_explained_var: 0.0005615949630737305
        vf_loss: 1.8805198669433594
      agent-3:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5483691096305847
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011804805835708976
        model: {}
        policy_loss: -0.002691032364964485
        total_loss: -0.0036137914285063744
        vf_explained_var: -0.0009123384952545166
        vf_loss: 0.42373839020729065
      agent-4:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.682611346244812
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017788322875276208
        model: {}
        policy_loss: -0.00410302123054862
        total_loss: -0.005248378962278366
        vf_explained_var: 0.008318006992340088
        vf_loss: 0.5603793859481812
      agent-5:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9350977540016174
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015200412599369884
        model: {}
        policy_loss: -0.002653864212334156
        total_loss: -0.004270039964467287
        vf_explained_var: -0.0013476312160491943
        vf_loss: 0.29597461223602295
    load_time_ms: 13874.752
    num_steps_sampled: 27744000
    num_steps_trained: 27744000
    sample_time_ms: 100261.163
    update_time_ms: 16.697
  iterations_since_restore: 129
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.521111111111104
    ram_util_percent: 22.853888888888882
  pid: 27405
  policy_reward_max:
    agent-0: 16.0
    agent-1: 7.0
    agent-2: 16.0
    agent-3: 13.0
    agent-4: 13.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 4.89
    agent-1: 2.15
    agent-2: 5.3
    agent-3: 5.02
    agent-4: 6.12
    agent-5: 3.87
  policy_reward_min:
    agent-0: 1.0
    agent-1: 0.0
    agent-2: -44.0
    agent-3: 0.0
    agent-4: 1.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.27131418624077
    mean_inference_ms: 13.315832424764885
    mean_processing_ms: 59.520494630967285
  time_since_restore: 16388.44145178795
  time_this_iter_s: 126.49734568595886
  time_total_s: 38898.724694252014
  timestamp: 1637236484
  timesteps_since_restore: 12384000
  timesteps_this_iter: 96000
  timesteps_total: 27744000
  training_iteration: 289
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 42.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    289 |          38898.7 | 27744000 |    27.35 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 2.75
    apples_agent-0_min: 0
    apples_agent-1_max: 18
    apples_agent-1_mean: 1.55
    apples_agent-1_min: 0
    apples_agent-2_max: 27
    apples_agent-2_mean: 2.85
    apples_agent-2_min: 0
    apples_agent-3_max: 14
    apples_agent-3_mean: 4.23
    apples_agent-3_min: 0
    apples_agent-4_max: 66
    apples_agent-4_mean: 2.27
    apples_agent-4_min: 0
    apples_agent-5_max: 14
    apples_agent-5_mean: 2.43
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 98
    cleaning_beam_agent-0_mean: 31.24
    cleaning_beam_agent-0_min: 10
    cleaning_beam_agent-1_max: 278
    cleaning_beam_agent-1_mean: 221.14
    cleaning_beam_agent-1_min: 139
    cleaning_beam_agent-2_max: 44
    cleaning_beam_agent-2_mean: 15.4
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 92
    cleaning_beam_agent-3_mean: 42.51
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 135
    cleaning_beam_agent-4_mean: 49.12
    cleaning_beam_agent-4_min: 11
    cleaning_beam_agent-5_max: 67
    cleaning_beam_agent-5_mean: 23.31
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-56-50
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 55.0
  episode_reward_mean: 27.05
  episode_reward_min: -81.0
  episodes_this_iter: 96
  episodes_total: 27840
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12179.185
    learner:
      agent-0:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4855830669403076
        entropy_coeff: 0.0017600000137463212
        kl: 0.001205208944156766
        model: {}
        policy_loss: -0.0031409901566803455
        total_loss: -0.00395541824400425
        vf_explained_var: 0.002156585454940796
        vf_loss: 0.40200212597846985
      agent-1:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40846070647239685
        entropy_coeff: 0.0017600000137463212
        kl: 0.00144408387131989
        model: {}
        policy_loss: -0.002277481835335493
        total_loss: -0.0029804622754454613
        vf_explained_var: 0.023382410407066345
        vf_loss: 0.15910208225250244
      agent-2:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4041369557380676
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013219353277236223
        model: {}
        policy_loss: -0.0032803923822939396
        total_loss: -0.003935100045055151
        vf_explained_var: 0.0006927698850631714
        vf_loss: 0.5657470226287842
      agent-3:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5643343329429626
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011648160871118307
        model: {}
        policy_loss: -0.0029757318552583456
        total_loss: -0.003930836450308561
        vf_explained_var: -0.0014059841632843018
        vf_loss: 0.381232351064682
      agent-4:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6910727024078369
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016748247435316443
        model: {}
        policy_loss: -0.004052667412906885
        total_loss: -0.005222102161496878
        vf_explained_var: 0.00793205201625824
        vf_loss: 0.46853476762771606
      agent-5:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9152635335922241
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017332397401332855
        model: {}
        policy_loss: -0.002510299440473318
        total_loss: -0.004091895185410976
        vf_explained_var: 0.005860909819602966
        vf_loss: 0.292704313993454
    load_time_ms: 13875.856
    num_steps_sampled: 27840000
    num_steps_trained: 27840000
    sample_time_ms: 100250.183
    update_time_ms: 16.527
  iterations_since_restore: 130
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.28212290502793
    ram_util_percent: 23.040782122905036
  pid: 27405
  policy_reward_max:
    agent-0: 12.0
    agent-1: 10.0
    agent-2: 17.0
    agent-3: 11.0
    agent-4: 18.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 4.73
    agent-1: 2.16
    agent-2: 6.08
    agent-3: 5.24
    agent-4: 5.75
    agent-5: 3.09
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -45.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -46.0
  sampler_perf:
    mean_env_wait_ms: 23.26843912094475
    mean_inference_ms: 13.31553590664698
    mean_processing_ms: 59.51820427308621
  time_since_restore: 16513.917434215546
  time_this_iter_s: 125.47598242759705
  time_total_s: 39024.20067667961
  timestamp: 1637236610
  timesteps_since_restore: 12480000
  timesteps_this_iter: 96000
  timesteps_total: 27840000
  training_iteration: 290
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 42.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    290 |          39024.2 | 27840000 |    27.05 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 37
    apples_agent-0_mean: 3.58
    apples_agent-0_min: 0
    apples_agent-1_max: 19
    apples_agent-1_mean: 1.57
    apples_agent-1_min: 0
    apples_agent-2_max: 24
    apples_agent-2_mean: 2.8
    apples_agent-2_min: 0
    apples_agent-3_max: 17
    apples_agent-3_mean: 4.02
    apples_agent-3_min: 0
    apples_agent-4_max: 12
    apples_agent-4_mean: 1.34
    apples_agent-4_min: 0
    apples_agent-5_max: 9
    apples_agent-5_mean: 1.75
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 113
    cleaning_beam_agent-0_mean: 29.18
    cleaning_beam_agent-0_min: 10
    cleaning_beam_agent-1_max: 283
    cleaning_beam_agent-1_mean: 216.52
    cleaning_beam_agent-1_min: 141
    cleaning_beam_agent-2_max: 155
    cleaning_beam_agent-2_mean: 16.77
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 116
    cleaning_beam_agent-3_mean: 45.14
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 110
    cleaning_beam_agent-4_mean: 48.18
    cleaning_beam_agent-4_min: 13
    cleaning_beam_agent-5_max: 59
    cleaning_beam_agent-5_mean: 22.53
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 8
    fire_beam_agent-3_mean: 0.08
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-58-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 48.0
  episode_reward_mean: 23.43
  episode_reward_min: -342.0
  episodes_this_iter: 96
  episodes_total: 27936
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12166.16
    learner:
      agent-0:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48659172654151917
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013069281121715903
        model: {}
        policy_loss: -0.0032155681401491165
        total_loss: -0.0040400270372629166
        vf_explained_var: -0.007646769285202026
        vf_loss: 0.31943750381469727
      agent-1:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4025309979915619
        entropy_coeff: 0.0017600000137463212
        kl: 0.00046125243534334004
        model: {}
        policy_loss: -0.0022466941736638546
        total_loss: -0.0017780838534235954
        vf_explained_var: 0.0032375603914260864
        vf_loss: 11.770675659179688
      agent-2:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4000920355319977
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006530813407152891
        model: {}
        policy_loss: -0.0017906776629388332
        total_loss: -0.0004362962208688259
        vf_explained_var: 0.00012350082397460938
        vf_loss: 20.58547592163086
      agent-3:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5745558738708496
        entropy_coeff: 0.0017600000137463212
        kl: 0.001288064755499363
        model: {}
        policy_loss: -0.002632305957376957
        total_loss: -0.0036035364028066397
        vf_explained_var: 0.0032470226287841797
        vf_loss: 0.39986366033554077
      agent-4:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6904883980751038
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013300009304657578
        model: {}
        policy_loss: -0.003984240349382162
        total_loss: -0.005153324455022812
        vf_explained_var: 0.0031069964170455933
        vf_loss: 0.46177706122398376
      agent-5:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9287024140357971
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019588565919548273
        model: {}
        policy_loss: -0.002903279382735491
        total_loss: -0.004511924926191568
        vf_explained_var: -0.0023011118173599243
        vf_loss: 0.2586691975593567
    load_time_ms: 13891.572
    num_steps_sampled: 27936000
    num_steps_trained: 27936000
    sample_time_ms: 100291.598
    update_time_ms: 16.8
  iterations_since_restore: 131
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 48.99613259668508
    ram_util_percent: 23.15966850828729
  pid: 27405
  policy_reward_max:
    agent-0: 11.0
    agent-1: 7.0
    agent-2: 19.0
    agent-3: 11.0
    agent-4: 18.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 4.32
    agent-1: 0.94
    agent-2: 4.12
    agent-3: 4.77
    agent-4: 5.86
    agent-5: 3.42
  policy_reward_min:
    agent-0: 0.0
    agent-1: -149.0
    agent-2: -197.0
    agent-3: -5.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.266026601085244
    mean_inference_ms: 13.315090487915459
    mean_processing_ms: 59.51516662729862
  time_since_restore: 16640.265856027603
  time_this_iter_s: 126.3484218120575
  time_total_s: 39150.54909849167
  timestamp: 1637236737
  timesteps_since_restore: 12576000
  timesteps_this_iter: 96000
  timesteps_total: 27936000
  training_iteration: 291
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 43.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    291 |          39150.5 | 27936000 |    23.43 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 33
    apples_agent-0_mean: 3.22
    apples_agent-0_min: 0
    apples_agent-1_max: 9
    apples_agent-1_mean: 1.27
    apples_agent-1_min: 0
    apples_agent-2_max: 26
    apples_agent-2_mean: 2.59
    apples_agent-2_min: 0
    apples_agent-3_max: 25
    apples_agent-3_mean: 4.02
    apples_agent-3_min: 0
    apples_agent-4_max: 21
    apples_agent-4_mean: 1.16
    apples_agent-4_min: 0
    apples_agent-5_max: 22
    apples_agent-5_mean: 2.31
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 86
    cleaning_beam_agent-0_mean: 29.14
    cleaning_beam_agent-0_min: 9
    cleaning_beam_agent-1_max: 278
    cleaning_beam_agent-1_mean: 213.3
    cleaning_beam_agent-1_min: 157
    cleaning_beam_agent-2_max: 53
    cleaning_beam_agent-2_mean: 14.38
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 116
    cleaning_beam_agent-3_mean: 46.42
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 102
    cleaning_beam_agent-4_mean: 47.06
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 59
    cleaning_beam_agent-5_mean: 22.48
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-01-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 56.0
  episode_reward_mean: 25.77
  episode_reward_min: -86.0
  episodes_this_iter: 96
  episodes_total: 28032
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12171.165
    learner:
      agent-0:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4853397607803345
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014398501953110099
        model: {}
        policy_loss: -0.003353079780936241
        total_loss: -0.004170075990259647
        vf_explained_var: -0.003449469804763794
        vf_loss: 0.3720228970050812
      agent-1:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4052758812904358
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008591354126110673
        model: {}
        policy_loss: -0.0023131552152335644
        total_loss: -0.0030108883511275053
        vf_explained_var: 0.0199277400970459
        vf_loss: 0.15553627908229828
      agent-2:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40506812930107117
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010063451481983066
        model: {}
        policy_loss: -0.003194464137777686
        total_loss: -0.0038501571398228407
        vf_explained_var: 0.0002754777669906616
        vf_loss: 0.5722776651382446
      agent-3:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5627830028533936
        entropy_coeff: 0.0017600000137463212
        kl: 0.001184314489364624
        model: {}
        policy_loss: -0.002674733754247427
        total_loss: -0.0036211356054991484
        vf_explained_var: 0.0028585344552993774
        vf_loss: 0.4409638047218323
      agent-4:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6869038939476013
        entropy_coeff: 0.0017600000137463212
        kl: 0.002018172526732087
        model: {}
        policy_loss: -0.004110338166356087
        total_loss: -0.005277286749333143
        vf_explained_var: 0.0014196783304214478
        vf_loss: 0.4200126528739929
      agent-5:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9143588542938232
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019761743023991585
        model: {}
        policy_loss: -0.002568246331065893
        total_loss: -0.004146987572312355
        vf_explained_var: 0.005525693297386169
        vf_loss: 0.3053045868873596
    load_time_ms: 13905.712
    num_steps_sampled: 28032000
    num_steps_trained: 28032000
    sample_time_ms: 100242.91
    update_time_ms: 16.448
  iterations_since_restore: 132
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 48.778333333333336
    ram_util_percent: 23.26055555555556
  pid: 27405
  policy_reward_max:
    agent-0: 16.0
    agent-1: 8.0
    agent-2: 17.0
    agent-3: 17.0
    agent-4: 14.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 4.48
    agent-1: 2.15
    agent-2: 6.13
    agent-3: 5.19
    agent-4: 4.78
    agent-5: 3.04
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -45.0
    agent-5: -50.0
  sampler_perf:
    mean_env_wait_ms: 23.2632942596434
    mean_inference_ms: 13.314766427832744
    mean_processing_ms: 59.51447961449039
  time_since_restore: 16766.527072668076
  time_this_iter_s: 126.26121664047241
  time_total_s: 39276.81031513214
  timestamp: 1637236863
  timesteps_since_restore: 12672000
  timesteps_this_iter: 96000
  timesteps_total: 28032000
  training_iteration: 292
  trial_id: '00000'
  
[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
== Status ==
Memory usage on this node: 43.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    292 |          39276.8 | 28032000 |    25.77 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 3.66
    apples_agent-0_min: 0
    apples_agent-1_max: 8
    apples_agent-1_mean: 1.44
    apples_agent-1_min: 0
    apples_agent-2_max: 11
    apples_agent-2_mean: 2.11
    apples_agent-2_min: 0
    apples_agent-3_max: 37
    apples_agent-3_mean: 3.75
    apples_agent-3_min: 0
    apples_agent-4_max: 25
    apples_agent-4_mean: 1.62
    apples_agent-4_min: 0
    apples_agent-5_max: 18
    apples_agent-5_mean: 1.82
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 104
    cleaning_beam_agent-0_mean: 29.64
    cleaning_beam_agent-0_min: 8
    cleaning_beam_agent-1_max: 277
    cleaning_beam_agent-1_mean: 213.37
    cleaning_beam_agent-1_min: 138
    cleaning_beam_agent-2_max: 53
    cleaning_beam_agent-2_mean: 13.35
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 94
    cleaning_beam_agent-3_mean: 48.1
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 154
    cleaning_beam_agent-4_mean: 50.31
    cleaning_beam_agent-4_min: 11
    cleaning_beam_agent-5_max: 52
    cleaning_beam_agent-5_mean: 21.37
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-03-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 63.0
  episode_reward_mean: 27.22
  episode_reward_min: -86.0
  episodes_this_iter: 96
  episodes_total: 28128
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12152.709
    learner:
      agent-0:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4879375696182251
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016669814940541983
        model: {}
        policy_loss: -0.0034881564788520336
        total_loss: -0.0043070497922599316
        vf_explained_var: -0.007172077894210815
        vf_loss: 0.3987759053707123
      agent-1:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4043571352958679
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013144002296030521
        model: {}
        policy_loss: -0.002122235018759966
        total_loss: -0.002817511558532715
        vf_explained_var: 0.023287057876586914
        vf_loss: 0.163916677236557
      agent-2:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3933333158493042
        entropy_coeff: 0.0017600000137463212
        kl: 0.001222594641149044
        model: {}
        policy_loss: -0.003207629080861807
        total_loss: -0.003839319571852684
        vf_explained_var: 0.003344327211380005
        vf_loss: 0.605745255947113
      agent-3:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5580448508262634
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011356637114658952
        model: {}
        policy_loss: -0.0027861741837114096
        total_loss: -0.0037286453880369663
        vf_explained_var: 0.004120483994483948
        vf_loss: 0.3968757390975952
      agent-4:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6828019618988037
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012865436729043722
        model: {}
        policy_loss: -0.004032561089843512
        total_loss: -0.005180514417588711
        vf_explained_var: 0.019781917333602905
        vf_loss: 0.5377674102783203
      agent-5:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8949560523033142
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013827660586684942
        model: {}
        policy_loss: -0.0028786766342818737
        total_loss: -0.0044283331371843815
        vf_explained_var: 0.002404049038887024
        vf_loss: 0.25462549924850464
    load_time_ms: 13915.628
    num_steps_sampled: 28128000
    num_steps_trained: 28128000
    sample_time_ms: 100067.038
    update_time_ms: 17.008
  iterations_since_restore: 133
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 48.916666666666664
    ram_util_percent: 23.45333333333333
  pid: 27405
  policy_reward_max:
    agent-0: 12.0
    agent-1: 9.0
    agent-2: 20.0
    agent-3: 18.0
    agent-4: 14.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 4.98
    agent-1: 2.43
    agent-2: 6.22
    agent-3: 5.05
    agent-4: 5.61
    agent-5: 2.93
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -45.0
    agent-5: -50.0
  sampler_perf:
    mean_env_wait_ms: 23.260913482480778
    mean_inference_ms: 13.31462234463352
    mean_processing_ms: 59.51334577226927
  time_since_restore: 16892.464591026306
  time_this_iter_s: 125.93751835823059
  time_total_s: 39402.74783349037
  timestamp: 1637236989
  timesteps_since_restore: 12768000
  timesteps_this_iter: 96000
  timesteps_total: 28128000
  training_iteration: 293
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 43.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    293 |          39402.7 | 28128000 |    27.22 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 2.75
    apples_agent-0_min: 0
    apples_agent-1_max: 67
    apples_agent-1_mean: 2.53
    apples_agent-1_min: 0
    apples_agent-2_max: 13
    apples_agent-2_mean: 2.17
    apples_agent-2_min: 0
    apples_agent-3_max: 14
    apples_agent-3_mean: 3.7
    apples_agent-3_min: 0
    apples_agent-4_max: 102
    apples_agent-4_mean: 2.46
    apples_agent-4_min: 0
    apples_agent-5_max: 15
    apples_agent-5_mean: 2.25
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 104
    cleaning_beam_agent-0_mean: 31.92
    cleaning_beam_agent-0_min: 3
    cleaning_beam_agent-1_max: 300
    cleaning_beam_agent-1_mean: 210.73
    cleaning_beam_agent-1_min: 101
    cleaning_beam_agent-2_max: 53
    cleaning_beam_agent-2_mean: 13.16
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 122
    cleaning_beam_agent-3_mean: 52.97
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 125
    cleaning_beam_agent-4_mean: 50.62
    cleaning_beam_agent-4_min: 12
    cleaning_beam_agent-5_max: 51
    cleaning_beam_agent-5_mean: 19.7
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-05-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 58.0
  episode_reward_mean: 27.51
  episode_reward_min: 9.0
  episodes_this_iter: 96
  episodes_total: 28224
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12156.239
    learner:
      agent-0:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49601876735687256
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013308902271091938
        model: {}
        policy_loss: -0.0030772117897868156
        total_loss: -0.003911690320819616
        vf_explained_var: -0.008084774017333984
        vf_loss: 0.3851411044597626
      agent-1:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4010888934135437
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008398185018450022
        model: {}
        policy_loss: -0.002123660407960415
        total_loss: -0.0028136353939771652
        vf_explained_var: 0.011905744671821594
        vf_loss: 0.15937429666519165
      agent-2:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38922199606895447
        entropy_coeff: 0.0017600000137463212
        kl: 0.001164861605502665
        model: {}
        policy_loss: -0.003126319730654359
        total_loss: -0.0037549103144556284
        vf_explained_var: 0.0010016858577728271
        vf_loss: 0.5644165277481079
      agent-3:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5606232285499573
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014179321005940437
        model: {}
        policy_loss: -0.002666353713721037
        total_loss: -0.003611625637859106
        vf_explained_var: 0.0006679892539978027
        vf_loss: 0.41423845291137695
      agent-4:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.688217043876648
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012579646427184343
        model: {}
        policy_loss: -0.003593356814235449
        total_loss: -0.004746614024043083
        vf_explained_var: 0.008255720138549805
        vf_loss: 0.5800464749336243
      agent-5:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8827170133590698
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015904013998806477
        model: {}
        policy_loss: -0.0027092313393950462
        total_loss: -0.0042314245365560055
        vf_explained_var: 0.0009742677211761475
        vf_loss: 0.313890278339386
    load_time_ms: 13949.101
    num_steps_sampled: 28224000
    num_steps_trained: 28224000
    sample_time_ms: 100126.523
    update_time_ms: 16.421
  iterations_since_restore: 134
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 48.84777777777778
    ram_util_percent: 23.55944444444445
  pid: 27405
  policy_reward_max:
    agent-0: 17.0
    agent-1: 10.0
    agent-2: 16.0
    agent-3: 12.0
    agent-4: 15.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 4.39
    agent-1: 2.18
    agent-2: 6.18
    agent-3: 4.9
    agent-4: 6.14
    agent-5: 3.72
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.259243540070106
    mean_inference_ms: 13.314308698819751
    mean_processing_ms: 59.512895182809835
  time_since_restore: 17019.357448101044
  time_this_iter_s: 126.89285707473755
  time_total_s: 39529.64069056511
  timestamp: 1637237116
  timesteps_since_restore: 12864000
  timesteps_this_iter: 96000
  timesteps_total: 28224000
  training_iteration: 294
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 43.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    294 |          39529.6 | 28224000 |    27.51 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 2.85
    apples_agent-0_min: 0
    apples_agent-1_max: 8
    apples_agent-1_mean: 1.27
    apples_agent-1_min: 0
    apples_agent-2_max: 40
    apples_agent-2_mean: 2.74
    apples_agent-2_min: 0
    apples_agent-3_max: 15
    apples_agent-3_mean: 3.71
    apples_agent-3_min: 0
    apples_agent-4_max: 7
    apples_agent-4_mean: 1.3
    apples_agent-4_min: 0
    apples_agent-5_max: 9
    apples_agent-5_mean: 1.89
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 112
    cleaning_beam_agent-0_mean: 30.59
    cleaning_beam_agent-0_min: 4
    cleaning_beam_agent-1_max: 297
    cleaning_beam_agent-1_mean: 211.63
    cleaning_beam_agent-1_min: 94
    cleaning_beam_agent-2_max: 50
    cleaning_beam_agent-2_mean: 14.0
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 91
    cleaning_beam_agent-3_mean: 44.55
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 179
    cleaning_beam_agent-4_mean: 54.36
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 56
    cleaning_beam_agent-5_mean: 22.78
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-07-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 59.0
  episode_reward_mean: 26.33
  episode_reward_min: -13.0
  episodes_this_iter: 96
  episodes_total: 28320
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12175.621
    learner:
      agent-0:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48756539821624756
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012952404795214534
        model: {}
        policy_loss: -0.0034662990365177393
        total_loss: -0.004286882933229208
        vf_explained_var: -0.010532796382904053
        vf_loss: 0.3752898573875427
      agent-1:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3988455832004547
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015800095861777663
        model: {}
        policy_loss: -0.002323474967852235
        total_loss: -0.003011459717527032
        vf_explained_var: 0.010221883654594421
        vf_loss: 0.1398421823978424
      agent-2:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40334200859069824
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014622483868151903
        model: {}
        policy_loss: -0.0033861626870930195
        total_loss: -0.004045304376631975
        vf_explained_var: -0.001746833324432373
        vf_loss: 0.507433295249939
      agent-3:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5671336054801941
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011968881590291858
        model: {}
        policy_loss: -0.002749162260442972
        total_loss: -0.003708500415086746
        vf_explained_var: 0.0009401142597198486
        vf_loss: 0.3881670832633972
      agent-4:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.691084623336792
        entropy_coeff: 0.0017600000137463212
        kl: 0.001821654150262475
        model: {}
        policy_loss: -0.003885268233716488
        total_loss: -0.005058902315795422
        vf_explained_var: 0.007047280669212341
        vf_loss: 0.4267624616622925
      agent-5:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9017825126647949
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012447794433683157
        model: {}
        policy_loss: -0.0020241080783307552
        total_loss: -0.0035095680505037308
        vf_explained_var: 0.002912968397140503
        vf_loss: 1.0167853832244873
    load_time_ms: 13940.449
    num_steps_sampled: 28320000
    num_steps_trained: 28320000
    sample_time_ms: 100208.398
    update_time_ms: 16.459
  iterations_since_restore: 135
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 48.80164835164834
    ram_util_percent: 23.686813186813186
  pid: 27405
  policy_reward_max:
    agent-0: 13.0
    agent-1: 8.0
    agent-2: 17.0
    agent-3: 12.0
    agent-4: 13.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 4.47
    agent-1: 2.11
    agent-2: 5.93
    agent-3: 5.28
    agent-4: 5.42
    agent-5: 3.12
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -43.0
  sampler_perf:
    mean_env_wait_ms: 23.257838987991086
    mean_inference_ms: 13.314116127394184
    mean_processing_ms: 59.513204528129954
  time_since_restore: 17146.608053207397
  time_this_iter_s: 127.25060510635376
  time_total_s: 39656.89129567146
  timestamp: 1637237244
  timesteps_since_restore: 12960000
  timesteps_this_iter: 96000
  timesteps_total: 28320000
  training_iteration: 295
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 44.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    295 |          39656.9 | 28320000 |    26.33 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 3.11
    apples_agent-0_min: 0
    apples_agent-1_max: 9
    apples_agent-1_mean: 1.37
    apples_agent-1_min: 0
    apples_agent-2_max: 34
    apples_agent-2_mean: 2.56
    apples_agent-2_min: 0
    apples_agent-3_max: 25
    apples_agent-3_mean: 3.52
    apples_agent-3_min: 0
    apples_agent-4_max: 7
    apples_agent-4_mean: 1.33
    apples_agent-4_min: 0
    apples_agent-5_max: 19
    apples_agent-5_mean: 2.18
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 66
    cleaning_beam_agent-0_mean: 32.89
    cleaning_beam_agent-0_min: 3
    cleaning_beam_agent-1_max: 298
    cleaning_beam_agent-1_mean: 201.36
    cleaning_beam_agent-1_min: 121
    cleaning_beam_agent-2_max: 57
    cleaning_beam_agent-2_mean: 11.4
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 97
    cleaning_beam_agent-3_mean: 46.98
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 118
    cleaning_beam_agent-4_mean: 50.04
    cleaning_beam_agent-4_min: 12
    cleaning_beam_agent-5_max: 51
    cleaning_beam_agent-5_mean: 24.73
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-09-30
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 52.0
  episode_reward_mean: 25.21
  episode_reward_min: -40.0
  episodes_this_iter: 96
  episodes_total: 28416
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12187.577
    learner:
      agent-0:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.492446631193161
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012958019506186247
        model: {}
        policy_loss: -0.0026379218325018883
        total_loss: -0.0033385585993528366
        vf_explained_var: 0.001421019434928894
        vf_loss: 1.6607024669647217
      agent-1:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39836573600769043
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013367936480790377
        model: {}
        policy_loss: -0.0015098126605153084
        total_loss: -0.002070944756269455
        vf_explained_var: 0.0061652809381484985
        vf_loss: 1.3999013900756836
      agent-2:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.390960156917572
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014449467416852713
        model: {}
        policy_loss: -0.003099914640188217
        total_loss: -0.003731937613338232
        vf_explained_var: 0.0020188838243484497
        vf_loss: 0.5607088804244995
      agent-3:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5565914511680603
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011966531164944172
        model: {}
        policy_loss: -0.002731804735958576
        total_loss: -0.0036691823042929173
        vf_explained_var: 0.008339807391166687
        vf_loss: 0.42225849628448486
      agent-4:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6844056844711304
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016519196797162294
        model: {}
        policy_loss: -0.003986728843301535
        total_loss: -0.005146511364728212
        vf_explained_var: 0.010197997093200684
        vf_loss: 0.44771862030029297
      agent-5:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9195854663848877
        entropy_coeff: 0.0017600000137463212
        kl: 0.001014756504446268
        model: {}
        policy_loss: -0.0026044435799121857
        total_loss: -0.004192897118628025
        vf_explained_var: 0.0011822134256362915
        vf_loss: 0.30018350481987
    load_time_ms: 13956.829
    num_steps_sampled: 28416000
    num_steps_trained: 28416000
    sample_time_ms: 100171.598
    update_time_ms: 16.418
  iterations_since_restore: 136
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.70055865921788
    ram_util_percent: 23.806145251396654
  pid: 27405
  policy_reward_max:
    agent-0: 15.0
    agent-1: 9.0
    agent-2: 16.0
    agent-3: 13.0
    agent-4: 14.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 4.19
    agent-1: 1.64
    agent-2: 5.18
    agent-3: 5.03
    agent-4: 5.45
    agent-5: 3.72
  policy_reward_min:
    agent-0: -46.0
    agent-1: -45.0
    agent-2: -46.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 23.255487402483322
    mean_inference_ms: 13.313817538502
    mean_processing_ms: 59.512991983362625
  time_since_restore: 17272.377009630203
  time_this_iter_s: 125.76895642280579
  time_total_s: 39782.66025209427
  timestamp: 1637237370
  timesteps_since_restore: 13056000
  timesteps_this_iter: 96000
  timesteps_total: 28416000
  training_iteration: 296
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 44.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    296 |          39782.7 | 28416000 |    25.21 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 3.23
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 1.1
    apples_agent-1_min: 0
    apples_agent-2_max: 44
    apples_agent-2_mean: 2.46
    apples_agent-2_min: 0
    apples_agent-3_max: 27
    apples_agent-3_mean: 4.09
    apples_agent-3_min: 0
    apples_agent-4_max: 12
    apples_agent-4_mean: 1.74
    apples_agent-4_min: 0
    apples_agent-5_max: 12
    apples_agent-5_mean: 1.75
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 130
    cleaning_beam_agent-0_mean: 32.15
    cleaning_beam_agent-0_min: 4
    cleaning_beam_agent-1_max: 254
    cleaning_beam_agent-1_mean: 196.37
    cleaning_beam_agent-1_min: 135
    cleaning_beam_agent-2_max: 42
    cleaning_beam_agent-2_mean: 12.35
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 95
    cleaning_beam_agent-3_mean: 49.1
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 139
    cleaning_beam_agent-4_mean: 50.29
    cleaning_beam_agent-4_min: 13
    cleaning_beam_agent-5_max: 56
    cleaning_beam_agent-5_mean: 22.48
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-11-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 69.0
  episode_reward_mean: 27.64
  episode_reward_min: -28.0
  episodes_this_iter: 96
  episodes_total: 28512
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12203.854
    learner:
      agent-0:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48992007970809937
        entropy_coeff: 0.0017600000137463212
        kl: 0.001622796175070107
        model: {}
        policy_loss: -0.003537981305271387
        total_loss: -0.004365864209830761
        vf_explained_var: -0.007913440465927124
        vf_loss: 0.34375685453414917
      agent-1:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4093068838119507
        entropy_coeff: 0.0017600000137463212
        kl: 0.001084179151803255
        model: {}
        policy_loss: -0.0018470637733116746
        total_loss: -0.0024231926072388887
        vf_explained_var: 0.000693202018737793
        vf_loss: 1.4425126314163208
      agent-2:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3995410203933716
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010238311951979995
        model: {}
        policy_loss: -0.0030443710274994373
        total_loss: -0.0036876127123832703
        vf_explained_var: 0.0032577961683273315
        vf_loss: 0.5995225310325623
      agent-3:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5634869337081909
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012542083859443665
        model: {}
        policy_loss: -0.0027099610306322575
        total_loss: -0.003658145200461149
        vf_explained_var: 0.00397162139415741
        vf_loss: 0.4355246126651764
      agent-4:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6950010061264038
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012773183407261968
        model: {}
        policy_loss: -0.003791781608015299
        total_loss: -0.004962806589901447
        vf_explained_var: 0.019736498594284058
        vf_loss: 0.5217807292938232
      agent-5:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8930891156196594
        entropy_coeff: 0.0017600000137463212
        kl: 0.002119829412549734
        model: {}
        policy_loss: -0.00271977880038321
        total_loss: -0.004262801259756088
        vf_explained_var: 0.00570264458656311
        vf_loss: 0.2881488800048828
    load_time_ms: 13932.788
    num_steps_sampled: 28512000
    num_steps_trained: 28512000
    sample_time_ms: 100122.138
    update_time_ms: 16.491
  iterations_since_restore: 137
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.513888888888886
    ram_util_percent: 23.92888888888888
  pid: 27405
  policy_reward_max:
    agent-0: 14.0
    agent-1: 6.0
    agent-2: 21.0
    agent-3: 15.0
    agent-4: 16.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 4.35
    agent-1: 1.67
    agent-2: 6.25
    agent-3: 5.29
    agent-4: 6.48
    agent-5: 3.6
  policy_reward_min:
    agent-0: 0.0
    agent-1: -49.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 1.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.25383076311788
    mean_inference_ms: 13.313692646904133
    mean_processing_ms: 59.512886606297506
  time_since_restore: 17398.912150859833
  time_this_iter_s: 126.53514122962952
  time_total_s: 39909.1953933239
  timestamp: 1637237496
  timesteps_since_restore: 13152000
  timesteps_this_iter: 96000
  timesteps_total: 28512000
  training_iteration: 297
  trial_id: '00000'
  
[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
== Status ==
Memory usage on this node: 44.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    297 |          39909.2 | 28512000 |    27.64 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 2.82
    apples_agent-0_min: 0
    apples_agent-1_max: 19
    apples_agent-1_mean: 1.81
    apples_agent-1_min: 0
    apples_agent-2_max: 20
    apples_agent-2_mean: 2.54
    apples_agent-2_min: 0
    apples_agent-3_max: 37
    apples_agent-3_mean: 4.35
    apples_agent-3_min: 0
    apples_agent-4_max: 17
    apples_agent-4_mean: 1.6
    apples_agent-4_min: 0
    apples_agent-5_max: 12
    apples_agent-5_mean: 2.05
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 93
    cleaning_beam_agent-0_mean: 29.51
    cleaning_beam_agent-0_min: 3
    cleaning_beam_agent-1_max: 264
    cleaning_beam_agent-1_mean: 199.64
    cleaning_beam_agent-1_min: 128
    cleaning_beam_agent-2_max: 34
    cleaning_beam_agent-2_mean: 12.74
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 102
    cleaning_beam_agent-3_mean: 47.78
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 140
    cleaning_beam_agent-4_mean: 47.3
    cleaning_beam_agent-4_min: 12
    cleaning_beam_agent-5_max: 50
    cleaning_beam_agent-5_mean: 21.88
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-13-44
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 58.0
  episode_reward_mean: 28.79
  episode_reward_min: 11.0
  episodes_this_iter: 96
  episodes_total: 28608
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12212.29
    learner:
      agent-0:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4900670647621155
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015125541249290109
        model: {}
        policy_loss: -0.0036254634615033865
        total_loss: -0.004447522573173046
        vf_explained_var: 0.0009725093841552734
        vf_loss: 0.4045632481575012
      agent-1:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4089834988117218
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011408691061660647
        model: {}
        policy_loss: -0.002169698243960738
        total_loss: -0.0028725306037813425
        vf_explained_var: 0.024457260966300964
        vf_loss: 0.1698039472103119
      agent-2:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39349761605262756
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009590985137037933
        model: {}
        policy_loss: -0.0031447147484868765
        total_loss: -0.0037824881728738546
        vf_explained_var: 0.00259244441986084
        vf_loss: 0.5478291511535645
      agent-3:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5686895847320557
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013724053278565407
        model: {}
        policy_loss: -0.00275707826949656
        total_loss: -0.003718698862940073
        vf_explained_var: -0.001717895269393921
        vf_loss: 0.3927539587020874
      agent-4:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6831924915313721
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015031782677397132
        model: {}
        policy_loss: -0.003974685911089182
        total_loss: -0.005132664926350117
        vf_explained_var: -0.007451444864273071
        vf_loss: 0.4444003999233246
      agent-5:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8998370170593262
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015330510213971138
        model: {}
        policy_loss: -0.0028384835459291935
        total_loss: -0.004393065348267555
        vf_explained_var: 0.0006362646818161011
        vf_loss: 0.2912946939468384
    load_time_ms: 13910.73
    num_steps_sampled: 28608000
    num_steps_trained: 28608000
    sample_time_ms: 100164.64
    update_time_ms: 16.178
  iterations_since_restore: 138
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.67237569060774
    ram_util_percent: 23.995027624309394
  pid: 27405
  policy_reward_max:
    agent-0: 17.0
    agent-1: 11.0
    agent-2: 20.0
    agent-3: 13.0
    agent-4: 14.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 5.12
    agent-1: 2.49
    agent-2: 6.16
    agent-3: 5.22
    agent-4: 5.86
    agent-5: 3.94
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 1.0
    agent-3: 1.0
    agent-4: -1.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.251890801426462
    mean_inference_ms: 13.313584762644144
    mean_processing_ms: 59.51477505988695
  time_since_restore: 17525.940158367157
  time_this_iter_s: 127.02800750732422
  time_total_s: 40036.22340083122
  timestamp: 1637237624
  timesteps_since_restore: 13248000
  timesteps_this_iter: 96000
  timesteps_total: 28608000
  training_iteration: 298
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 44.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    298 |          40036.2 | 28608000 |    28.79 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 2.81
    apples_agent-0_min: 0
    apples_agent-1_max: 11
    apples_agent-1_mean: 1.5
    apples_agent-1_min: 0
    apples_agent-2_max: 43
    apples_agent-2_mean: 2.62
    apples_agent-2_min: 0
    apples_agent-3_max: 31
    apples_agent-3_mean: 3.83
    apples_agent-3_min: 0
    apples_agent-4_max: 14
    apples_agent-4_mean: 1.28
    apples_agent-4_min: 0
    apples_agent-5_max: 9
    apples_agent-5_mean: 2.1
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 88
    cleaning_beam_agent-0_mean: 29.99
    cleaning_beam_agent-0_min: 7
    cleaning_beam_agent-1_max: 323
    cleaning_beam_agent-1_mean: 201.99
    cleaning_beam_agent-1_min: 154
    cleaning_beam_agent-2_max: 40
    cleaning_beam_agent-2_mean: 13.3
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 101
    cleaning_beam_agent-3_mean: 46.49
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 112
    cleaning_beam_agent-4_mean: 50.35
    cleaning_beam_agent-4_min: 16
    cleaning_beam_agent-5_max: 52
    cleaning_beam_agent-5_mean: 21.8
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-15-50
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 60.0
  episode_reward_mean: 26.3
  episode_reward_min: -87.0
  episodes_this_iter: 96
  episodes_total: 28704
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12205.542
    learner:
      agent-0:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48792892694473267
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012606654781848192
        model: {}
        policy_loss: -0.0034228009171783924
        total_loss: -0.0042486595921218395
        vf_explained_var: -0.010843783617019653
        vf_loss: 0.3289525806903839
      agent-1:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40906378626823425
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006144621875137091
        model: {}
        policy_loss: -0.001759067177772522
        total_loss: -0.0023288941010832787
        vf_explained_var: 0.006408810615539551
        vf_loss: 1.5012389421463013
      agent-2:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38169294595718384
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016334831016138196
        model: {}
        policy_loss: -0.0027532184030860662
        total_loss: -0.003228498389944434
        vf_explained_var: 0.0024134963750839233
        vf_loss: 1.9649925231933594
      agent-3:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5539509654045105
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014463680563494563
        model: {}
        policy_loss: -0.0027790239546447992
        total_loss: -0.0037119793705642223
        vf_explained_var: -0.002743333578109741
        vf_loss: 0.4199444353580475
      agent-4:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6944682002067566
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014837747439742088
        model: {}
        policy_loss: -0.0038710744120180607
        total_loss: -0.005036998074501753
        vf_explained_var: 0.011072009801864624
        vf_loss: 0.5634117126464844
      agent-5:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8796243667602539
        entropy_coeff: 0.0017600000137463212
        kl: 0.001226046122610569
        model: {}
        policy_loss: -0.0018854495137929916
        total_loss: -0.0032720351591706276
        vf_explained_var: 0.0028355568647384644
        vf_loss: 1.6155203580856323
    load_time_ms: 13895.653
    num_steps_sampled: 28704000
    num_steps_trained: 28704000
    sample_time_ms: 100178.298
    update_time_ms: 16.123
  iterations_since_restore: 139
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.19833333333334
    ram_util_percent: 24.164444444444445
  pid: 27405
  policy_reward_max:
    agent-0: 11.0
    agent-1: 8.0
    agent-2: 15.0
    agent-3: 16.0
    agent-4: 22.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 4.55
    agent-1: 1.33
    agent-2: 6.0
    agent-3: 5.09
    agent-4: 6.2
    agent-5: 3.13
  policy_reward_min:
    agent-0: 0.0
    agent-1: -48.0
    agent-2: -48.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -50.0
  sampler_perf:
    mean_env_wait_ms: 23.24987825461114
    mean_inference_ms: 13.31353784365753
    mean_processing_ms: 59.51508537087391
  time_since_restore: 17652.320830345154
  time_this_iter_s: 126.38067197799683
  time_total_s: 40162.60407280922
  timestamp: 1637237750
  timesteps_since_restore: 13344000
  timesteps_this_iter: 96000
  timesteps_total: 28704000
  training_iteration: 299
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 44.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    299 |          40162.6 | 28704000 |     26.3 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 2.87
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 1.35
    apples_agent-1_min: 0
    apples_agent-2_max: 37
    apples_agent-2_mean: 3.54
    apples_agent-2_min: 0
    apples_agent-3_max: 12
    apples_agent-3_mean: 4.08
    apples_agent-3_min: 0
    apples_agent-4_max: 19
    apples_agent-4_mean: 1.31
    apples_agent-4_min: 0
    apples_agent-5_max: 10
    apples_agent-5_mean: 2.18
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 82
    cleaning_beam_agent-0_mean: 29.63
    cleaning_beam_agent-0_min: 8
    cleaning_beam_agent-1_max: 284
    cleaning_beam_agent-1_mean: 208.37
    cleaning_beam_agent-1_min: 106
    cleaning_beam_agent-2_max: 58
    cleaning_beam_agent-2_mean: 11.17
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 104
    cleaning_beam_agent-3_mean: 45.96
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 153
    cleaning_beam_agent-4_mean: 49.75
    cleaning_beam_agent-4_min: 7
    cleaning_beam_agent-5_max: 47
    cleaning_beam_agent-5_mean: 20.58
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-17-56
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 74.0
  episode_reward_mean: 28.11
  episode_reward_min: -80.0
  episodes_this_iter: 96
  episodes_total: 28800
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12213.132
    learner:
      agent-0:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4872158467769623
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011428514262661338
        model: {}
        policy_loss: -0.0017898129299283028
        total_loss: -0.00247250497341156
        vf_explained_var: -0.0034940242767333984
        vf_loss: 1.7481040954589844
      agent-1:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40442854166030884
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013167292345315218
        model: {}
        policy_loss: -0.002315743826329708
        total_loss: -0.003013093024492264
        vf_explained_var: 0.01629595458507538
        vf_loss: 0.14447852969169617
      agent-2:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3804585337638855
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010287024779245257
        model: {}
        policy_loss: -0.0030778096988797188
        total_loss: -0.0036851372569799423
        vf_explained_var: 0.0008089989423751831
        vf_loss: 0.6227908134460449
      agent-3:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5546201467514038
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011499696411192417
        model: {}
        policy_loss: -0.002543119713664055
        total_loss: -0.003466895315796137
        vf_explained_var: -0.0004032999277114868
        vf_loss: 0.5235633254051208
      agent-4:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6897786855697632
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011842572130262852
        model: {}
        policy_loss: -0.0034898994490504265
        total_loss: -0.004648488946259022
        vf_explained_var: 0.0077214837074279785
        vf_loss: 0.5542019605636597
      agent-5:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8972959518432617
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012632819125428796
        model: {}
        policy_loss: -0.002410578541457653
        total_loss: -0.003960323054343462
        vf_explained_var: 0.0033178627490997314
        vf_loss: 0.29491499066352844
    load_time_ms: 13870.395
    num_steps_sampled: 28800000
    num_steps_trained: 28800000
    sample_time_ms: 100218.228
    update_time_ms: 15.919
  iterations_since_restore: 140
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.760555555555555
    ram_util_percent: 24.264444444444447
  pid: 27405
  policy_reward_max:
    agent-0: 15.0
    agent-1: 9.0
    agent-2: 22.0
    agent-3: 16.0
    agent-4: 18.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 4.01
    agent-1: 2.27
    agent-2: 6.72
    agent-3: 5.66
    agent-4: 5.37
    agent-5: 4.08
  policy_reward_min:
    agent-0: -46.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -50.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.248012651980215
    mean_inference_ms: 13.313612321983532
    mean_processing_ms: 59.515056281028336
  time_since_restore: 17778.048000574112
  time_this_iter_s: 125.72717022895813
  time_total_s: 40288.33124303818
  timestamp: 1637237876
  timesteps_since_restore: 13440000
  timesteps_this_iter: 96000
  timesteps_total: 28800000
  training_iteration: 300
  trial_id: '00000'
  
[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
== Status ==
Memory usage on this node: 45.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    300 |          40288.3 | 28800000 |    28.11 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 3.7
    apples_agent-0_min: 0
    apples_agent-1_max: 14
    apples_agent-1_mean: 1.87
    apples_agent-1_min: 0
    apples_agent-2_max: 34
    apples_agent-2_mean: 2.14
    apples_agent-2_min: 0
    apples_agent-3_max: 12
    apples_agent-3_mean: 3.85
    apples_agent-3_min: 0
    apples_agent-4_max: 15
    apples_agent-4_mean: 1.32
    apples_agent-4_min: 0
    apples_agent-5_max: 44
    apples_agent-5_mean: 3.8
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 82
    cleaning_beam_agent-0_mean: 30.14
    cleaning_beam_agent-0_min: 5
    cleaning_beam_agent-1_max: 285
    cleaning_beam_agent-1_mean: 205.34
    cleaning_beam_agent-1_min: 121
    cleaning_beam_agent-2_max: 57
    cleaning_beam_agent-2_mean: 12.19
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 98
    cleaning_beam_agent-3_mean: 41.92
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 238
    cleaning_beam_agent-4_mean: 54.98
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 49
    cleaning_beam_agent-5_mean: 22.4
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-20-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 55.0
  episode_reward_mean: 28.38
  episode_reward_min: 13.0
  episodes_this_iter: 96
  episodes_total: 28896
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12218.467
    learner:
      agent-0:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.490315318107605
        entropy_coeff: 0.0017600000137463212
        kl: 0.001331500941887498
        model: {}
        policy_loss: -0.0037627993151545525
        total_loss: -0.004585588816553354
        vf_explained_var: -0.008967041969299316
        vf_loss: 0.4016568064689636
      agent-1:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39704012870788574
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018950646044686437
        model: {}
        policy_loss: -0.0026650475338101387
        total_loss: -0.0033500136341899633
        vf_explained_var: 0.017986878752708435
        vf_loss: 0.13821253180503845
      agent-2:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3847249150276184
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015604859218001366
        model: {}
        policy_loss: -0.003811387810856104
        total_loss: -0.0044409288093447685
        vf_explained_var: -0.00458119809627533
        vf_loss: 0.47575536370277405
      agent-3:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5479903221130371
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009811761556193233
        model: {}
        policy_loss: -0.0027550864033401012
        total_loss: -0.003682399168610573
        vf_explained_var: 0.0029710233211517334
        vf_loss: 0.3714980483055115
      agent-4:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6861962080001831
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014366856776177883
        model: {}
        policy_loss: -0.004098801873624325
        total_loss: -0.005260101519525051
        vf_explained_var: 0.013456150889396667
        vf_loss: 0.4640699625015259
      agent-5:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8828346729278564
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013405419886112213
        model: {}
        policy_loss: -0.0025162948295474052
        total_loss: -0.004037586972117424
        vf_explained_var: 0.002349838614463806
        vf_loss: 0.3249437212944031
    load_time_ms: 13860.98
    num_steps_sampled: 28896000
    num_steps_trained: 28896000
    sample_time_ms: 100287.667
    update_time_ms: 15.844
  iterations_since_restore: 141
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.39175824175824
    ram_util_percent: 24.37252747252747
  pid: 27405
  policy_reward_max:
    agent-0: 12.0
    agent-1: 9.0
    agent-2: 15.0
    agent-3: 13.0
    agent-4: 17.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 5.37
    agent-1: 2.27
    agent-2: 5.94
    agent-3: 5.01
    agent-4: 5.83
    agent-5: 3.96
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 1.0
    agent-3: 1.0
    agent-4: 1.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.246736941557252
    mean_inference_ms: 13.313512217516234
    mean_processing_ms: 59.51582324795153
  time_since_restore: 17905.053322553635
  time_this_iter_s: 127.0053219795227
  time_total_s: 40415.3365650177
  timestamp: 1637238004
  timesteps_since_restore: 13536000
  timesteps_this_iter: 96000
  timesteps_total: 28896000
  training_iteration: 301
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 45.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    301 |          40415.3 | 28896000 |    28.38 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 126
    apples_agent-0_mean: 4.32
    apples_agent-0_min: 0
    apples_agent-1_max: 10
    apples_agent-1_mean: 1.52
    apples_agent-1_min: 0
    apples_agent-2_max: 15
    apples_agent-2_mean: 2.29
    apples_agent-2_min: 0
    apples_agent-3_max: 29
    apples_agent-3_mean: 4.31
    apples_agent-3_min: 0
    apples_agent-4_max: 38
    apples_agent-4_mean: 1.87
    apples_agent-4_min: 0
    apples_agent-5_max: 73
    apples_agent-5_mean: 3.13
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 121
    cleaning_beam_agent-0_mean: 29.23
    cleaning_beam_agent-0_min: 7
    cleaning_beam_agent-1_max: 276
    cleaning_beam_agent-1_mean: 218.77
    cleaning_beam_agent-1_min: 170
    cleaning_beam_agent-2_max: 39
    cleaning_beam_agent-2_mean: 11.55
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 102
    cleaning_beam_agent-3_mean: 42.1
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 123
    cleaning_beam_agent-4_mean: 52.21
    cleaning_beam_agent-4_min: 11
    cleaning_beam_agent-5_max: 54
    cleaning_beam_agent-5_mean: 23.53
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-22-10
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 76.0
  episode_reward_mean: 30.34
  episode_reward_min: -55.0
  episodes_this_iter: 96
  episodes_total: 28992
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12229.677
    learner:
      agent-0:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48503074049949646
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013165399432182312
        model: {}
        policy_loss: -0.0032623859588056803
        total_loss: -0.004072693642228842
        vf_explained_var: -0.0052392929792404175
        vf_loss: 0.43348991870880127
      agent-1:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4025340676307678
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011097497772425413
        model: {}
        policy_loss: -0.002130117965862155
        total_loss: -0.0028200214728713036
        vf_explained_var: 0.015411868691444397
        vf_loss: 0.18557792901992798
      agent-2:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3720340430736542
        entropy_coeff: 0.0017600000137463212
        kl: 0.001399037428200245
        model: {}
        policy_loss: -0.0031242147088050842
        total_loss: -0.003715312574058771
        vf_explained_var: 0.0010403990745544434
        vf_loss: 0.6368374824523926
      agent-3:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5597538948059082
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014196550473570824
        model: {}
        policy_loss: -0.0025906998198479414
        total_loss: -0.003529204288497567
        vf_explained_var: 0.001539856195449829
        vf_loss: 0.4666420519351959
      agent-4:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6828620433807373
        entropy_coeff: 0.0017600000137463212
        kl: 0.001954011619091034
        model: {}
        policy_loss: -0.0042339772917330265
        total_loss: -0.005372765939682722
        vf_explained_var: 0.011827975511550903
        vf_loss: 0.6304600834846497
      agent-5:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8936586380004883
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015609515830874443
        model: {}
        policy_loss: -0.0026695590931922197
        total_loss: -0.004215035121887922
        vf_explained_var: 0.0009289830923080444
        vf_loss: 0.27363473176956177
    load_time_ms: 13857.853
    num_steps_sampled: 28992000
    num_steps_trained: 28992000
    sample_time_ms: 100303.331
    update_time_ms: 16.16
  iterations_since_restore: 142
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.696666666666665
    ram_util_percent: 24.438333333333333
  pid: 27405
  policy_reward_max:
    agent-0: 15.0
    agent-1: 14.0
    agent-2: 21.0
    agent-3: 18.0
    agent-4: 16.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 4.77
    agent-1: 2.43
    agent-2: 6.53
    agent-3: 5.66
    agent-4: 6.93
    agent-5: 4.02
  policy_reward_min:
    agent-0: -41.0
    agent-1: 0.0
    agent-2: -37.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.245173098019563
    mean_inference_ms: 13.313158514680227
    mean_processing_ms: 59.513367817844625
  time_since_restore: 18031.548092126846
  time_this_iter_s: 126.49476957321167
  time_total_s: 40541.83133459091
  timestamp: 1637238130
  timesteps_since_restore: 13632000
  timesteps_this_iter: 96000
  timesteps_total: 28992000
  training_iteration: 302
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 45.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    302 |          40541.8 | 28992000 |    30.34 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 2.97
    apples_agent-0_min: 0
    apples_agent-1_max: 39
    apples_agent-1_mean: 2.08
    apples_agent-1_min: 0
    apples_agent-2_max: 15
    apples_agent-2_mean: 2.31
    apples_agent-2_min: 0
    apples_agent-3_max: 18
    apples_agent-3_mean: 3.93
    apples_agent-3_min: 0
    apples_agent-4_max: 35
    apples_agent-4_mean: 1.81
    apples_agent-4_min: 0
    apples_agent-5_max: 13
    apples_agent-5_mean: 2.06
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 73
    cleaning_beam_agent-0_mean: 29.02
    cleaning_beam_agent-0_min: 9
    cleaning_beam_agent-1_max: 295
    cleaning_beam_agent-1_mean: 214.01
    cleaning_beam_agent-1_min: 153
    cleaning_beam_agent-2_max: 47
    cleaning_beam_agent-2_mean: 11.12
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 92
    cleaning_beam_agent-3_mean: 40.42
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 117
    cleaning_beam_agent-4_mean: 50.25
    cleaning_beam_agent-4_min: 16
    cleaning_beam_agent-5_max: 48
    cleaning_beam_agent-5_mean: 19.08
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-24-17
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 67.0
  episode_reward_mean: 29.23
  episode_reward_min: 0.0
  episodes_this_iter: 96
  episodes_total: 29088
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12233.802
    learner:
      agent-0:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4888352155685425
        entropy_coeff: 0.0017600000137463212
        kl: 0.001610277220606804
        model: {}
        policy_loss: -0.0033958228304982185
        total_loss: -0.004214085638523102
        vf_explained_var: -0.013074636459350586
        vf_loss: 0.4208797812461853
      agent-1:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39473262429237366
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009434424573555589
        model: {}
        policy_loss: -0.0022472357377409935
        total_loss: -0.0029258141294121742
        vf_explained_var: 0.020314648747444153
        vf_loss: 0.1614750623703003
      agent-2:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37738195061683655
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009554859716445208
        model: {}
        policy_loss: -0.003059075679630041
        total_loss: -0.0036561547312885523
        vf_explained_var: 0.0040000975131988525
        vf_loss: 0.6711401343345642
      agent-3:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.544413685798645
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010294211097061634
        model: {}
        policy_loss: -0.0023417696356773376
        total_loss: -0.0032553616911172867
        vf_explained_var: 0.004947155714035034
        vf_loss: 0.44576096534729004
      agent-4:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7029340863227844
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015555357094854116
        model: {}
        policy_loss: -0.002846274059265852
        total_loss: -0.0039316932670772076
        vf_explained_var: 0.004865080118179321
        vf_loss: 1.5174418687820435
      agent-5:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8462996482849121
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016387754585593939
        model: {}
        policy_loss: -0.0030171279795467854
        total_loss: -0.004476735834032297
        vf_explained_var: 0.004317119717597961
        vf_loss: 0.29880642890930176
    load_time_ms: 13856.848
    num_steps_sampled: 29088000
    num_steps_trained: 29088000
    sample_time_ms: 100351.661
    update_time_ms: 15.493
  iterations_since_restore: 143
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 48.89777777777778
    ram_util_percent: 24.60055555555556
  pid: 27405
  policy_reward_max:
    agent-0: 16.0
    agent-1: 8.0
    agent-2: 18.0
    agent-3: 18.0
    agent-4: 16.0
    agent-5: 15.0
  policy_reward_mean:
    agent-0: 4.89
    agent-1: 2.33
    agent-2: 7.1
    agent-3: 5.29
    agent-4: 6.01
    agent-5: 3.61
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -35.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.24467026991622
    mean_inference_ms: 13.313091470004201
    mean_processing_ms: 59.514601705893035
  time_since_restore: 18157.96803879738
  time_this_iter_s: 126.41994667053223
  time_total_s: 40668.251281261444
  timestamp: 1637238257
  timesteps_since_restore: 13728000
  timesteps_this_iter: 96000
  timesteps_total: 29088000
  training_iteration: 303
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 45.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    303 |          40668.3 | 29088000 |    29.23 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 2.84
    apples_agent-0_min: 0
    apples_agent-1_max: 44
    apples_agent-1_mean: 2.0
    apples_agent-1_min: 0
    apples_agent-2_max: 8
    apples_agent-2_mean: 2.15
    apples_agent-2_min: 0
    apples_agent-3_max: 20
    apples_agent-3_mean: 4.51
    apples_agent-3_min: 0
    apples_agent-4_max: 19
    apples_agent-4_mean: 1.77
    apples_agent-4_min: 0
    apples_agent-5_max: 35
    apples_agent-5_mean: 2.8
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 56
    cleaning_beam_agent-0_mean: 25.77
    cleaning_beam_agent-0_min: 4
    cleaning_beam_agent-1_max: 262
    cleaning_beam_agent-1_mean: 207.57
    cleaning_beam_agent-1_min: 108
    cleaning_beam_agent-2_max: 81
    cleaning_beam_agent-2_mean: 11.61
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 121
    cleaning_beam_agent-3_mean: 46.93
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 98
    cleaning_beam_agent-4_mean: 48.02
    cleaning_beam_agent-4_min: 8
    cleaning_beam_agent-5_max: 51
    cleaning_beam_agent-5_mean: 20.45
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-26-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 66.0
  episode_reward_mean: 30.95
  episode_reward_min: 4.0
  episodes_this_iter: 96
  episodes_total: 29184
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12240.691
    learner:
      agent-0:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47162315249443054
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013463448267430067
        model: {}
        policy_loss: -0.003402415197342634
        total_loss: -0.004193007946014404
        vf_explained_var: -0.01481175422668457
        vf_loss: 0.39463385939598083
      agent-1:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39364129304885864
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013771456433460116
        model: {}
        policy_loss: -0.0024591051042079926
        total_loss: -0.0031379801221191883
        vf_explained_var: 0.017985939979553223
        vf_loss: 0.1393115073442459
      agent-2:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3850470781326294
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013415778521448374
        model: {}
        policy_loss: -0.003179289400577545
        total_loss: -0.0037957821041345596
        vf_explained_var: 0.0013918578624725342
        vf_loss: 0.6118683218955994
      agent-3:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5659925937652588
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015321336686611176
        model: {}
        policy_loss: -0.002881904598325491
        total_loss: -0.003831978887319565
        vf_explained_var: -0.00015249848365783691
        vf_loss: 0.4607016444206238
      agent-4:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6839638352394104
        entropy_coeff: 0.0017600000137463212
        kl: 0.001527911052107811
        model: {}
        policy_loss: -0.003733809106051922
        total_loss: -0.004886582959443331
        vf_explained_var: 0.018158435821533203
        vf_loss: 0.5100272297859192
      agent-5:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8727174401283264
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016710625495761633
        model: {}
        policy_loss: -0.0026927636936306953
        total_loss: -0.004196388181298971
        vf_explained_var: 0.0045449137687683105
        vf_loss: 0.32360225915908813
    load_time_ms: 13830.496
    num_steps_sampled: 29184000
    num_steps_trained: 29184000
    sample_time_ms: 100398.664
    update_time_ms: 16.203
  iterations_since_restore: 144
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.6414364640884
    ram_util_percent: 24.716574585635357
  pid: 27405
  policy_reward_max:
    agent-0: 12.0
    agent-1: 7.0
    agent-2: 18.0
    agent-3: 14.0
    agent-4: 19.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 5.21
    agent-1: 2.45
    agent-2: 7.31
    agent-3: 5.88
    agent-4: 5.87
    agent-5: 4.23
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.24275895247992
    mean_inference_ms: 13.313170911435238
    mean_processing_ms: 59.51556422143125
  time_since_restore: 18285.13217329979
  time_this_iter_s: 127.16413450241089
  time_total_s: 40795.415415763855
  timestamp: 1637238384
  timesteps_since_restore: 13824000
  timesteps_this_iter: 96000
  timesteps_total: 29184000
  training_iteration: 304
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 45.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    304 |          40795.4 | 29184000 |    30.95 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 49
    apples_agent-0_mean: 3.57
    apples_agent-0_min: 0
    apples_agent-1_max: 35
    apples_agent-1_mean: 1.55
    apples_agent-1_min: 0
    apples_agent-2_max: 28
    apples_agent-2_mean: 2.63
    apples_agent-2_min: 0
    apples_agent-3_max: 31
    apples_agent-3_mean: 4.34
    apples_agent-3_min: 0
    apples_agent-4_max: 14
    apples_agent-4_mean: 1.52
    apples_agent-4_min: 0
    apples_agent-5_max: 15
    apples_agent-5_mean: 2.44
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 78
    cleaning_beam_agent-0_mean: 29.05
    cleaning_beam_agent-0_min: 6
    cleaning_beam_agent-1_max: 294
    cleaning_beam_agent-1_mean: 211.64
    cleaning_beam_agent-1_min: 112
    cleaning_beam_agent-2_max: 38
    cleaning_beam_agent-2_mean: 10.92
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 98
    cleaning_beam_agent-3_mean: 41.74
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 101
    cleaning_beam_agent-4_mean: 46.05
    cleaning_beam_agent-4_min: 15
    cleaning_beam_agent-5_max: 54
    cleaning_beam_agent-5_mean: 20.05
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 3
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-28-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 59.0
  episode_reward_mean: 27.87
  episode_reward_min: -40.0
  episodes_this_iter: 96
  episodes_total: 29280
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12235.715
    learner:
      agent-0:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48846006393432617
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014859632356092334
        model: {}
        policy_loss: -0.0035199851263314486
        total_loss: -0.00434194877743721
        vf_explained_var: 0.001108318567276001
        vf_loss: 0.3772508203983307
      agent-1:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3955383598804474
        entropy_coeff: 0.0017600000137463212
        kl: 0.001077996101230383
        model: {}
        policy_loss: -0.0022049513645470142
        total_loss: -0.002885923022404313
        vf_explained_var: 0.011756077408790588
        vf_loss: 0.1517563909292221
      agent-2:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3804948925971985
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010750070214271545
        model: {}
        policy_loss: -0.002829066477715969
        total_loss: -0.003191794967278838
        vf_explained_var: 0.002981826663017273
        vf_loss: 3.069437265396118
      agent-3:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5587484836578369
        entropy_coeff: 0.0017600000137463212
        kl: 0.001057072076946497
        model: {}
        policy_loss: -0.002636270597577095
        total_loss: -0.003573599737137556
        vf_explained_var: 0.007961943745613098
        vf_loss: 0.46070581674575806
      agent-4:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6923858523368835
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018086921190842986
        model: {}
        policy_loss: -0.003918738570064306
        total_loss: -0.005082407034933567
        vf_explained_var: 0.008014321327209473
        vf_loss: 0.5492885112762451
      agent-5:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8772672414779663
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011619327124208212
        model: {}
        policy_loss: -0.0023571134079247713
        total_loss: -0.003868123283609748
        vf_explained_var: 0.0011935979127883911
        vf_loss: 0.3298260271549225
    load_time_ms: 13809.753
    num_steps_sampled: 29280000
    num_steps_trained: 29280000
    sample_time_ms: 100400.158
    update_time_ms: 16.165
  iterations_since_restore: 145
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.129834254143645
    ram_util_percent: 24.761325966850826
  pid: 27405
  policy_reward_max:
    agent-0: 12.0
    agent-1: 9.0
    agent-2: 16.0
    agent-3: 14.0
    agent-4: 18.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 4.9
    agent-1: 2.12
    agent-2: 5.64
    agent-3: 5.31
    agent-4: 6.25
    agent-5: 3.65
  policy_reward_min:
    agent-0: -2.0
    agent-1: 0.0
    agent-2: -46.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -47.0
  sampler_perf:
    mean_env_wait_ms: 23.241176786771277
    mean_inference_ms: 13.313091951836576
    mean_processing_ms: 59.51653473305318
  time_since_restore: 18412.13796854019
  time_this_iter_s: 127.00579524040222
  time_total_s: 40922.42121100426
  timestamp: 1637238511
  timesteps_since_restore: 13920000
  timesteps_this_iter: 96000
  timesteps_total: 29280000
  training_iteration: 305
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 46.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    305 |          40922.4 | 29280000 |    27.87 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 2.54
    apples_agent-0_min: 0
    apples_agent-1_max: 71
    apples_agent-1_mean: 2.48
    apples_agent-1_min: 0
    apples_agent-2_max: 8
    apples_agent-2_mean: 2.04
    apples_agent-2_min: 0
    apples_agent-3_max: 22
    apples_agent-3_mean: 4.04
    apples_agent-3_min: 0
    apples_agent-4_max: 6
    apples_agent-4_mean: 1.21
    apples_agent-4_min: 0
    apples_agent-5_max: 37
    apples_agent-5_mean: 2.98
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 72
    cleaning_beam_agent-0_mean: 23.73
    cleaning_beam_agent-0_min: 6
    cleaning_beam_agent-1_max: 269
    cleaning_beam_agent-1_mean: 198.36
    cleaning_beam_agent-1_min: 130
    cleaning_beam_agent-2_max: 29
    cleaning_beam_agent-2_mean: 10.68
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 111
    cleaning_beam_agent-3_mean: 39.57
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 108
    cleaning_beam_agent-4_mean: 44.68
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 46
    cleaning_beam_agent-5_mean: 21.26
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-30-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 57.0
  episode_reward_mean: 28.7
  episode_reward_min: -34.0
  episodes_this_iter: 96
  episodes_total: 29376
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12231.436
    learner:
      agent-0:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4627123475074768
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015533281257376075
        model: {}
        policy_loss: -0.0037061283364892006
        total_loss: -0.004484902136027813
        vf_explained_var: -0.0026850104331970215
        vf_loss: 0.35601282119750977
      agent-1:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.389512836933136
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011181219015270472
        model: {}
        policy_loss: -0.0023410047870129347
        total_loss: -0.0030087654013186693
        vf_explained_var: 0.029306963086128235
        vf_loss: 0.17783531546592712
      agent-2:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3839527666568756
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013859941391274333
        model: {}
        policy_loss: -0.003146801143884659
        total_loss: -0.0037679653614759445
        vf_explained_var: 0.0036561787128448486
        vf_loss: 0.545920729637146
      agent-3:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5576843023300171
        entropy_coeff: 0.0017600000137463212
        kl: 0.001045380486175418
        model: {}
        policy_loss: -0.0025774817913770676
        total_loss: -0.0035164859145879745
        vf_explained_var: 0.001805335283279419
        vf_loss: 0.4252288043498993
      agent-4:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6829892992973328
        entropy_coeff: 0.0017600000137463212
        kl: 0.001684493850916624
        model: {}
        policy_loss: -0.0039658378809690475
        total_loss: -0.005106539465487003
        vf_explained_var: 0.009828329086303711
        vf_loss: 0.6135801672935486
      agent-5:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8803698420524597
        entropy_coeff: 0.0017600000137463212
        kl: 0.000992129440419376
        model: {}
        policy_loss: -0.002170599065721035
        total_loss: -0.0036904863081872463
        vf_explained_var: 0.0018245875835418701
        vf_loss: 0.29565373063087463
    load_time_ms: 13816.246
    num_steps_sampled: 29376000
    num_steps_trained: 29376000
    sample_time_ms: 100433.419
    update_time_ms: 16.056
  iterations_since_restore: 146
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.43833333333334
    ram_util_percent: 24.847777777777775
  pid: 27405
  policy_reward_max:
    agent-0: 13.0
    agent-1: 8.0
    agent-2: 17.0
    agent-3: 14.0
    agent-4: 19.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 4.62
    agent-1: 2.46
    agent-2: 6.45
    agent-3: 4.47
    agent-4: 6.56
    agent-5: 4.14
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -48.0
    agent-4: 1.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.238674680266868
    mean_inference_ms: 13.312919920314517
    mean_processing_ms: 59.51579446990543
  time_since_restore: 18538.2238407135
  time_this_iter_s: 126.08587217330933
  time_total_s: 41048.50708317757
  timestamp: 1637238638
  timesteps_since_restore: 14016000
  timesteps_this_iter: 96000
  timesteps_total: 29376000
  training_iteration: 306
  trial_id: '00000'
  
[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
== Status ==
Memory usage on this node: 46.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    306 |          41048.5 | 29376000 |     28.7 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 54
    apples_agent-0_mean: 4.01
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 1.59
    apples_agent-1_min: 0
    apples_agent-2_max: 13
    apples_agent-2_mean: 2.89
    apples_agent-2_min: 0
    apples_agent-3_max: 14
    apples_agent-3_mean: 3.42
    apples_agent-3_min: 0
    apples_agent-4_max: 15
    apples_agent-4_mean: 1.35
    apples_agent-4_min: 0
    apples_agent-5_max: 26
    apples_agent-5_mean: 2.91
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 62
    cleaning_beam_agent-0_mean: 22.41
    cleaning_beam_agent-0_min: 6
    cleaning_beam_agent-1_max: 283
    cleaning_beam_agent-1_mean: 199.77
    cleaning_beam_agent-1_min: 141
    cleaning_beam_agent-2_max: 144
    cleaning_beam_agent-2_mean: 12.42
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 79
    cleaning_beam_agent-3_mean: 38.8
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 147
    cleaning_beam_agent-4_mean: 45.12
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 46
    cleaning_beam_agent-5_mean: 21.58
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-32-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 58.0
  episode_reward_mean: 30.23
  episode_reward_min: -14.0
  episodes_this_iter: 96
  episodes_total: 29472
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12210.305
    learner:
      agent-0:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47001850605010986
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008417757344432175
        model: {}
        policy_loss: -0.0018511473899707198
        total_loss: -0.002506587654352188
        vf_explained_var: -0.006096512079238892
        vf_loss: 1.7179145812988281
      agent-1:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39235952496528625
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010860268957912922
        model: {}
        policy_loss: -0.00237865187227726
        total_loss: -0.0030509941279888153
        vf_explained_var: 0.020402774214744568
        vf_loss: 0.18212512135505676
      agent-2:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3909699022769928
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013455930165946484
        model: {}
        policy_loss: -0.0031713186763226986
        total_loss: -0.003793268697336316
        vf_explained_var: 0.000756576657295227
        vf_loss: 0.6615980267524719
      agent-3:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5630756616592407
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010989276925101876
        model: {}
        policy_loss: -0.002799811540171504
        total_loss: -0.0037461204919964075
        vf_explained_var: 4.006922245025635e-05
        vf_loss: 0.44704335927963257
      agent-4:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6807929873466492
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020651884842664003
        model: {}
        policy_loss: -0.004259451292455196
        total_loss: -0.005404488183557987
        vf_explained_var: -0.0026229768991470337
        vf_loss: 0.5315899848937988
      agent-5:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8805216550827026
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015773570630699396
        model: {}
        policy_loss: -0.002574801677837968
        total_loss: -0.004084236919879913
        vf_explained_var: 0.0028247833251953125
        vf_loss: 0.40284281969070435
    load_time_ms: 13814.54
    num_steps_sampled: 29472000
    num_steps_trained: 29472000
    sample_time_ms: 100496.229
    update_time_ms: 16.822
  iterations_since_restore: 147
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.297790055248626
    ram_util_percent: 25.022099447513817
  pid: 27405
  policy_reward_max:
    agent-0: 16.0
    agent-1: 9.0
    agent-2: 20.0
    agent-3: 14.0
    agent-4: 20.0
    agent-5: 16.0
  policy_reward_mean:
    agent-0: 4.62
    agent-1: 2.57
    agent-2: 7.37
    agent-3: 5.24
    agent-4: 6.01
    agent-5: 4.42
  policy_reward_min:
    agent-0: -44.0
    agent-1: 0.0
    agent-2: 1.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.237771115508405
    mean_inference_ms: 13.312959159781323
    mean_processing_ms: 59.5166029466898
  time_since_restore: 18665.19775247574
  time_this_iter_s: 126.97391176223755
  time_total_s: 41175.480994939804
  timestamp: 1637238765
  timesteps_since_restore: 14112000
  timesteps_this_iter: 96000
  timesteps_total: 29472000
  training_iteration: 307
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 46.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    307 |          41175.5 | 29472000 |    30.23 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 3.08
    apples_agent-0_min: 0
    apples_agent-1_max: 17
    apples_agent-1_mean: 1.36
    apples_agent-1_min: 0
    apples_agent-2_max: 29
    apples_agent-2_mean: 2.57
    apples_agent-2_min: 0
    apples_agent-3_max: 33
    apples_agent-3_mean: 3.94
    apples_agent-3_min: 0
    apples_agent-4_max: 10
    apples_agent-4_mean: 1.48
    apples_agent-4_min: 0
    apples_agent-5_max: 28
    apples_agent-5_mean: 2.53
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 60
    cleaning_beam_agent-0_mean: 24.53
    cleaning_beam_agent-0_min: 8
    cleaning_beam_agent-1_max: 261
    cleaning_beam_agent-1_mean: 199.57
    cleaning_beam_agent-1_min: 142
    cleaning_beam_agent-2_max: 25
    cleaning_beam_agent-2_mean: 9.91
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 83
    cleaning_beam_agent-3_mean: 35.28
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 105
    cleaning_beam_agent-4_mean: 42.65
    cleaning_beam_agent-4_min: 11
    cleaning_beam_agent-5_max: 52
    cleaning_beam_agent-5_mean: 19.53
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-34-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 63.0
  episode_reward_mean: 29.92
  episode_reward_min: 7.0
  episodes_this_iter: 96
  episodes_total: 29568
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12200.261
    learner:
      agent-0:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4725388288497925
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015076423296704888
        model: {}
        policy_loss: -0.0035326643846929073
        total_loss: -0.004325960762798786
        vf_explained_var: -0.004795879125595093
        vf_loss: 0.3837631344795227
      agent-1:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3937554359436035
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013008968671783805
        model: {}
        policy_loss: -0.0023962007835507393
        total_loss: -0.0030764443799853325
        vf_explained_var: 0.03209081292152405
        vf_loss: 0.12766800820827484
      agent-2:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3725133538246155
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012455754913389683
        model: {}
        policy_loss: -0.003150483826175332
        total_loss: -0.0037461351603269577
        vf_explained_var: 0.0013181418180465698
        vf_loss: 0.599729597568512
      agent-3:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.546428918838501
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012713851174339652
        model: {}
        policy_loss: -0.0024597905576229095
        total_loss: -0.0033814562484622
        vf_explained_var: -0.001162499189376831
        vf_loss: 0.40048930048942566
      agent-4:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6812130212783813
        entropy_coeff: 0.0017600000137463212
        kl: 0.001614664914086461
        model: {}
        policy_loss: -0.003866530954837799
        total_loss: -0.005005374550819397
        vf_explained_var: 0.008177712559700012
        vf_loss: 0.6009238958358765
      agent-5:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8613678216934204
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018144529312849045
        model: {}
        policy_loss: -0.0029244208708405495
        total_loss: -0.004408949054777622
        vf_explained_var: 0.005304962396621704
        vf_loss: 0.3147773742675781
    load_time_ms: 13812.447
    num_steps_sampled: 29568000
    num_steps_trained: 29568000
    sample_time_ms: 100428.21
    update_time_ms: 17.083
  iterations_since_restore: 148
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.645
    ram_util_percent: 25.081666666666667
  pid: 27405
  policy_reward_max:
    agent-0: 16.0
    agent-1: 9.0
    agent-2: 16.0
    agent-3: 15.0
    agent-4: 17.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 4.88
    agent-1: 2.1
    agent-2: 6.78
    agent-3: 5.32
    agent-4: 6.48
    agent-5: 4.36
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 1.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.23582957564189
    mean_inference_ms: 13.312969038863812
    mean_processing_ms: 59.5158566163544
  time_since_restore: 18791.39339017868
  time_this_iter_s: 126.1956377029419
  time_total_s: 41301.676632642746
  timestamp: 1637238891
  timesteps_since_restore: 14208000
  timesteps_this_iter: 96000
  timesteps_total: 29568000
  training_iteration: 308
  trial_id: '00000'
  
[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
== Status ==
Memory usage on this node: 46.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    308 |          41301.7 | 29568000 |    29.92 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 14
    apples_agent-0_mean: 3.16
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 1.29
    apples_agent-1_min: 0
    apples_agent-2_max: 41
    apples_agent-2_mean: 3.24
    apples_agent-2_min: 0
    apples_agent-3_max: 13
    apples_agent-3_mean: 3.72
    apples_agent-3_min: 0
    apples_agent-4_max: 12
    apples_agent-4_mean: 1.51
    apples_agent-4_min: 0
    apples_agent-5_max: 16
    apples_agent-5_mean: 2.7
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 81
    cleaning_beam_agent-0_mean: 25.3
    cleaning_beam_agent-0_min: 5
    cleaning_beam_agent-1_max: 261
    cleaning_beam_agent-1_mean: 207.26
    cleaning_beam_agent-1_min: 142
    cleaning_beam_agent-2_max: 56
    cleaning_beam_agent-2_mean: 12.16
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 96
    cleaning_beam_agent-3_mean: 36.09
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 150
    cleaning_beam_agent-4_mean: 46.2
    cleaning_beam_agent-4_min: 10
    cleaning_beam_agent-5_max: 59
    cleaning_beam_agent-5_mean: 20.26
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-36-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 62.0
  episode_reward_mean: 30.2
  episode_reward_min: 9.0
  episodes_this_iter: 96
  episodes_total: 29664
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12196.942
    learner:
      agent-0:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4687856435775757
        entropy_coeff: 0.0017600000137463212
        kl: 0.001228263252414763
        model: {}
        policy_loss: -0.0033797500655055046
        total_loss: -0.0041587864980101585
        vf_explained_var: -0.006877526640892029
        vf_loss: 0.46025383472442627
      agent-1:
        cur_kl_coeff: 6.018531165893214e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3940748870372772
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012285758275538683
        model: {}
        policy_loss: -0.002211509272456169
        total_loss: -0.0028893710114061832
        vf_explained_var: 0.015930205583572388
        vf_loss: 0.15708580613136292
      agent-2:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3805025517940521
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013187897857278585
        model: {}
        policy_loss: -0.0031678956001996994
        total_loss: -0.003764574881643057
        vf_explained_var: 0.007314786314964294
        vf_loss: 0.730050802230835
      agent-3:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5433504581451416
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010876376181840897
        model: {}
        policy_loss: -0.0025021927431225777
        total_loss: -0.003414972685277462
        vf_explained_var: -0.00026129186153411865
        vf_loss: 0.4351847767829895
      agent-4:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6926791667938232
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013837044825777411
        model: {}
        policy_loss: -0.003739620791748166
        total_loss: -0.004913459066301584
        vf_explained_var: 0.004940718412399292
        vf_loss: 0.4527784585952759
      agent-5:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.892683207988739
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013538911007344723
        model: {}
        policy_loss: -0.0024902932345867157
        total_loss: -0.00402856944128871
        vf_explained_var: 0.0021511316299438477
        vf_loss: 0.32846009731292725
    load_time_ms: 13842.272
    num_steps_sampled: 29664000
    num_steps_trained: 29664000
    sample_time_ms: 100298.142
    update_time_ms: 17.104
  iterations_since_restore: 149
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 48.84775280898877
    ram_util_percent: 25.141573033707868
  pid: 27405
  policy_reward_max:
    agent-0: 14.0
    agent-1: 7.0
    agent-2: 25.0
    agent-3: 15.0
    agent-4: 15.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 5.36
    agent-1: 2.24
    agent-2: 6.68
    agent-3: 5.54
    agent-4: 6.01
    agent-5: 4.37
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.233457867255396
    mean_inference_ms: 13.313028406079454
    mean_processing_ms: 59.514621852230476
  time_since_restore: 18916.741508483887
  time_this_iter_s: 125.3481183052063
  time_total_s: 41427.02475094795
  timestamp: 1637239017
  timesteps_since_restore: 14304000
  timesteps_this_iter: 96000
  timesteps_total: 29664000
  training_iteration: 309
  trial_id: '00000'
  
[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
== Status ==
Memory usage on this node: 46.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    309 |            41427 | 29664000 |     30.2 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 71
    apples_agent-0_mean: 4.46
    apples_agent-0_min: 0
    apples_agent-1_max: 53
    apples_agent-1_mean: 2.35
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 3.23
    apples_agent-2_min: 0
    apples_agent-3_max: 41
    apples_agent-3_mean: 5.05
    apples_agent-3_min: 0
    apples_agent-4_max: 19
    apples_agent-4_mean: 1.83
    apples_agent-4_min: 0
    apples_agent-5_max: 25
    apples_agent-5_mean: 2.77
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 162
    cleaning_beam_agent-0_mean: 24.58
    cleaning_beam_agent-0_min: 4
    cleaning_beam_agent-1_max: 274
    cleaning_beam_agent-1_mean: 217.05
    cleaning_beam_agent-1_min: 141
    cleaning_beam_agent-2_max: 38
    cleaning_beam_agent-2_mean: 10.22
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 87
    cleaning_beam_agent-3_mean: 35.84
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 113
    cleaning_beam_agent-4_mean: 48.04
    cleaning_beam_agent-4_min: 14
    cleaning_beam_agent-5_max: 59
    cleaning_beam_agent-5_mean: 18.46
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-39-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 68.0
  episode_reward_mean: 34.69
  episode_reward_min: 11.0
  episodes_this_iter: 96
  episodes_total: 29760
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12197.682
    learner:
      agent-0:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46755117177963257
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011462041875347495
        model: {}
        policy_loss: -0.0031977586913853884
        total_loss: -0.003973925951868296
        vf_explained_var: -0.005005538463592529
        vf_loss: 0.4672532081604004
      agent-1:
        cur_kl_coeff: 3.009265582946607e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39061522483825684
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009649060666561127
        model: {}
        policy_loss: -0.002211092971265316
        total_loss: -0.0028789653442800045
        vf_explained_var: 0.0219222754240036
        vf_loss: 0.1960992068052292
      agent-2:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36971330642700195
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011209273943677545
        model: {}
        policy_loss: -0.002836703322827816
        total_loss: -0.0034234579652547836
        vf_explained_var: 0.009690985083580017
        vf_loss: 0.639435887336731
      agent-3:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5501652359962463
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010156651260331273
        model: {}
        policy_loss: -0.002666747197508812
        total_loss: -0.0035769534297287464
        vf_explained_var: 0.002992376685142517
        vf_loss: 0.5808565020561218
      agent-4:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6812951564788818
        entropy_coeff: 0.0017600000137463212
        kl: 0.001776977675035596
        model: {}
        policy_loss: -0.0041776797734200954
        total_loss: -0.00531315803527832
        vf_explained_var: 0.010585993528366089
        vf_loss: 0.6360201835632324
      agent-5:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8839619159698486
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013664978323504329
        model: {}
        policy_loss: -0.0025204613339155912
        total_loss: -0.004039013758301735
        vf_explained_var: 0.0051191747188568115
        vf_loss: 0.37220829725265503
    load_time_ms: 13862.236
    num_steps_sampled: 29760000
    num_steps_trained: 29760000
    sample_time_ms: 100320.349
    update_time_ms: 18.227
  iterations_since_restore: 150
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.228888888888896
    ram_util_percent: 25.24666666666667
  pid: 27405
  policy_reward_max:
    agent-0: 14.0
    agent-1: 9.0
    agent-2: 20.0
    agent-3: 20.0
    agent-4: 17.0
    agent-5: 16.0
  policy_reward_mean:
    agent-0: 5.87
    agent-1: 2.94
    agent-2: 7.89
    agent-3: 6.18
    agent-4: 6.92
    agent-5: 4.89
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 1.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.231221496303665
    mean_inference_ms: 13.3130475060635
    mean_processing_ms: 59.51311375157016
  time_since_restore: 19042.94112420082
  time_this_iter_s: 126.1996157169342
  time_total_s: 41553.22436666489
  timestamp: 1637239143
  timesteps_since_restore: 14400000
  timesteps_this_iter: 96000
  timesteps_total: 29760000
  training_iteration: 310
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 47.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    310 |          41553.2 | 29760000 |    34.69 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 34
    apples_agent-0_mean: 4.16
    apples_agent-0_min: 0
    apples_agent-1_max: 12
    apples_agent-1_mean: 1.6
    apples_agent-1_min: 0
    apples_agent-2_max: 13
    apples_agent-2_mean: 2.33
    apples_agent-2_min: 0
    apples_agent-3_max: 71
    apples_agent-3_mean: 4.72
    apples_agent-3_min: 0
    apples_agent-4_max: 10
    apples_agent-4_mean: 1.26
    apples_agent-4_min: 0
    apples_agent-5_max: 25
    apples_agent-5_mean: 2.91
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 105
    cleaning_beam_agent-0_mean: 25.56
    cleaning_beam_agent-0_min: 4
    cleaning_beam_agent-1_max: 284
    cleaning_beam_agent-1_mean: 208.79
    cleaning_beam_agent-1_min: 150
    cleaning_beam_agent-2_max: 51
    cleaning_beam_agent-2_mean: 11.14
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 80
    cleaning_beam_agent-3_mean: 34.79
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 153
    cleaning_beam_agent-4_mean: 44.92
    cleaning_beam_agent-4_min: 10
    cleaning_beam_agent-5_max: 53
    cleaning_beam_agent-5_mean: 21.84
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-41-11
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 65.0
  episode_reward_mean: 31.74
  episode_reward_min: -26.0
  episodes_this_iter: 96
  episodes_total: 29856
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12188.919
    learner:
      agent-0:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4718078374862671
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016727364854887128
        model: {}
        policy_loss: -0.0024153643753379583
        total_loss: -0.003061754861846566
        vf_explained_var: 0.0012488365173339844
        vf_loss: 1.8399146795272827
      agent-1:
        cur_kl_coeff: 1.5046327914733034e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3934345841407776
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009692719904705882
        model: {}
        policy_loss: -0.002210194244980812
        total_loss: -0.002883338835090399
        vf_explained_var: 0.0168902724981308
        vf_loss: 0.19301804900169373
      agent-2:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3760482668876648
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012608616380020976
        model: {}
        policy_loss: -0.003457427490502596
        total_loss: -0.004049088340252638
        vf_explained_var: -0.0036384612321853638
        vf_loss: 0.7018274068832397
      agent-3:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5323177576065063
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013744819443672895
        model: {}
        policy_loss: -0.0024455133825540543
        total_loss: -0.0033312393352389336
        vf_explained_var: 0.004610136151313782
        vf_loss: 0.5115140676498413
      agent-4:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.690220057964325
        entropy_coeff: 0.0017600000137463212
        kl: 0.002002337481826544
        model: {}
        policy_loss: -0.0038749868981540203
        total_loss: -0.00503042945638299
        vf_explained_var: 0.0019984841346740723
        vf_loss: 0.5934470891952515
      agent-5:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8994824886322021
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009830697672441602
        model: {}
        policy_loss: -0.002461116062477231
        total_loss: -0.004012269899249077
        vf_explained_var: -0.0025635063648223877
        vf_loss: 0.3193371593952179
    load_time_ms: 13834.763
    num_steps_sampled: 29856000
    num_steps_trained: 29856000
    sample_time_ms: 100387.431
    update_time_ms: 18.076
  iterations_since_restore: 151
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.22967032967033
    ram_util_percent: 25.303296703296702
  pid: 27405
  policy_reward_max:
    agent-0: 19.0
    agent-1: 9.0
    agent-2: 22.0
    agent-3: 16.0
    agent-4: 17.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 5.14
    agent-1: 2.7
    agent-2: 7.62
    agent-3: 6.03
    agent-4: 6.02
    agent-5: 4.23
  policy_reward_min:
    agent-0: -49.0
    agent-1: 0.0
    agent-2: 1.0
    agent-3: 0.0
    agent-4: -41.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.229918931035623
    mean_inference_ms: 13.3131072056929
    mean_processing_ms: 59.514511324621836
  time_since_restore: 19170.18745446205
  time_this_iter_s: 127.24633026123047
  time_total_s: 41680.47069692612
  timestamp: 1637239271
  timesteps_since_restore: 14496000
  timesteps_this_iter: 96000
  timesteps_total: 29856000
  training_iteration: 311
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 47.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    311 |          41680.5 | 29856000 |    31.74 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 27
    apples_agent-0_mean: 3.35
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 1.34
    apples_agent-1_min: 0
    apples_agent-2_max: 8
    apples_agent-2_mean: 2.12
    apples_agent-2_min: 0
    apples_agent-3_max: 23
    apples_agent-3_mean: 3.88
    apples_agent-3_min: 0
    apples_agent-4_max: 23
    apples_agent-4_mean: 1.9
    apples_agent-4_min: 0
    apples_agent-5_max: 36
    apples_agent-5_mean: 2.48
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 112
    cleaning_beam_agent-0_mean: 26.01
    cleaning_beam_agent-0_min: 3
    cleaning_beam_agent-1_max: 252
    cleaning_beam_agent-1_mean: 202.24
    cleaning_beam_agent-1_min: 151
    cleaning_beam_agent-2_max: 87
    cleaning_beam_agent-2_mean: 13.13
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 94
    cleaning_beam_agent-3_mean: 37.93
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 91
    cleaning_beam_agent-4_mean: 42.16
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 50
    cleaning_beam_agent-5_mean: 19.67
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-43-17
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 53.0
  episode_reward_mean: 29.44
  episode_reward_min: -13.0
  episodes_this_iter: 96
  episodes_total: 29952
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12179.324
    learner:
      agent-0:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.468631386756897
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012871751096099615
        model: {}
        policy_loss: -0.0034936422016471624
        total_loss: -0.0042810579761862755
        vf_explained_var: -0.005504399538040161
        vf_loss: 0.37375038862228394
      agent-1:
        cur_kl_coeff: 7.523163957366517e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39273321628570557
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009055080008693039
        model: {}
        policy_loss: -0.002062354004010558
        total_loss: -0.002733684843406081
        vf_explained_var: 0.030524998903274536
        vf_loss: 0.19880270957946777
      agent-2:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3775710165500641
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009072102839127183
        model: {}
        policy_loss: -0.00192208681255579
        total_loss: -0.002396678552031517
        vf_explained_var: 0.0009656995534896851
        vf_loss: 1.8993463516235352
      agent-3:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.534540057182312
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007546013221144676
        model: {}
        policy_loss: -0.0024452772922813892
        total_loss: -0.0033453661017119884
        vf_explained_var: -0.0019282698631286621
        vf_loss: 0.4070018231868744
      agent-4:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6897761821746826
        entropy_coeff: 0.0017600000137463212
        kl: 0.00153689319267869
        model: {}
        policy_loss: -0.0039955368265509605
        total_loss: -0.00515733752399683
        vf_explained_var: 0.008753970265388489
        vf_loss: 0.5220166444778442
      agent-5:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8941468000411987
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016575285699218512
        model: {}
        policy_loss: -0.0022518199402838945
        total_loss: -0.0037912996485829353
        vf_explained_var: 0.005589291453361511
        vf_loss: 0.3421911597251892
    load_time_ms: 13827.826
    num_steps_sampled: 29952000
    num_steps_trained: 29952000
    sample_time_ms: 100346.677
    update_time_ms: 17.866
  iterations_since_restore: 152
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.338333333333324
    ram_util_percent: 25.434999999999995
  pid: 27405
  policy_reward_max:
    agent-0: 16.0
    agent-1: 10.0
    agent-2: 21.0
    agent-3: 16.0
    agent-4: 16.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 4.92
    agent-1: 2.55
    agent-2: 6.7
    agent-3: 5.23
    agent-4: 6.51
    agent-5: 3.53
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -41.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -46.0
  sampler_perf:
    mean_env_wait_ms: 23.22774051561818
    mean_inference_ms: 13.31278955805946
    mean_processing_ms: 59.514750336845026
  time_since_restore: 19296.166808128357
  time_this_iter_s: 125.97935366630554
  time_total_s: 41806.45005059242
  timestamp: 1637239397
  timesteps_since_restore: 14592000
  timesteps_this_iter: 96000
  timesteps_total: 29952000
  training_iteration: 312
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 47.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.11:27405 |    312 |          41806.5 | 29952000 |    29.44 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=27405)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f5dfad95588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 23
    apples_agent-0_mean: 3.57
    apples_agent-0_min: 0
    apples_agent-1_max: 11
    apples_agent-1_mean: 1.67
    apples_agent-1_min: 0
    apples_agent-2_max: 9
    apples_agent-2_mean: 2.33
    apples_agent-2_min: 0
    apples_agent-3_max: 12
    apples_agent-3_mean: 3.95
    apples_agent-3_min: 0
    apples_agent-4_max: 19
    apples_agent-4_mean: 1.77
    apples_agent-4_min: 0
    apples_agent-5_max: 24
    apples_agent-5_mean: 2.49
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 131
    cleaning_beam_agent-0_mean: 26.99
    cleaning_beam_agent-0_min: 5
    cleaning_beam_agent-1_max: 283
    cleaning_beam_agent-1_mean: 205.63
    cleaning_beam_agent-1_min: 143
    cleaning_beam_agent-2_max: 50
    cleaning_beam_agent-2_mean: 10.41
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 94
    cleaning_beam_agent-3_mean: 34.5
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 151
    cleaning_beam_agent-4_mean: 45.58
    cleaning_beam_agent-4_min: 11
    cleaning_beam_agent-5_max: 52
    cleaning_beam_agent-5_mean: 22.34
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-45-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 72.0
  episode_reward_mean: 31.85
  episode_reward_min: 10.0
  episodes_this_iter: 96
  episodes_total: 30048
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu011
  info:
    grad_time_ms: 12169.345
    learner:
      agent-0:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46581801772117615
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016157370992004871
        model: {}
        policy_loss: -0.0032407587859779596
        total_loss: -0.0040176911279559135
        vf_explained_var: -0.009578347206115723
        vf_loss: 0.4290493130683899
      agent-1:
        cur_kl_coeff: 3.7615819786832586e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3913706839084625
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011961697600781918
        model: {}
        policy_loss: -0.002484912285581231
        total_loss: -0.0031554102897644043
        vf_explained_var: 0.020240113139152527
        vf_loss: 0.1831551194190979
      agent-2:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3728373646736145
        entropy_coeff: 0.0017600000137463212
        kl: 0.001468969276174903
        model: {}
        policy_loss: -0.0032014690805226564
        total_loss: -0.0037944819778203964
        vf_explained_var: -1.8984079360961914e-05
        vf_loss: 0.6317915320396423
      agent-3:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5364731550216675
        entropy_coeff: 0.0017600000137463212
        kl: 0.001248735236003995
        model: {}
        policy_loss: -0.002505873329937458
        total_loss: -0.0034077593591064215
        vf_explained_var: -0.004827454686164856
        vf_loss: 0.4230668544769287
      agent-4:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6868261098861694
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015322737162932754
        model: {}
        policy_loss: -0.004061966203153133
        total_loss: -0.005215749144554138
        vf_explained_var: 0.014019891619682312
        vf_loss: 0.5503361225128174
      agent-5:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8882103562355042
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020409678108990192
        model: {}
        policy_loss: -0.00281737232580781
        total_loss: -0.004343317821621895
        vf_explained_var: 0.0039266496896743774
        vf_loss: 0.3730490505695343
    load_time_ms: 13839.191
    num_steps_sampled: 30048000
    num_steps_trained: 30048000
    sample_time_ms: 100386.604
    update_time_ms: 18.076
  iterations_since_restore: 153
  node_ip: 172.17.8.11
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 46.286740331491714
    ram_util_percent: 24.79337016574586
  pid: 27405
  policy_reward_max:
    agent-0: 13.0
    agent-1: 9.0
    agent-2: 21.0
    agent-3: 15.0
    agent-4: 17.0
    agent-5: 14.0
  policy_reward_mean:
    agent-0: 5.47
    agent-1: 2.54
    agent-2: 7.24
    agent-3: 5.45
    agent-4: 6.59
    agent-5: 4.56
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 1.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.22512758156394
    mean_inference_ms: 13.312324194942141
    mean_processing_ms: 59.515581457288405
  time_since_restore: 19423.05864715576
  time_this_iter_s: 126.89183902740479
  time_total_s: 41933.34188961983
  timestamp: 1637239524
  timesteps_since_restore: 14688000
  timesteps_this_iter: 96000
  timesteps_total: 30048000
  training_iteration: 313
  trial_id: '00000'
  