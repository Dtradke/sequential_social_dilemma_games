>>> RESTORING FROM:  /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics/BaselinePPOTrainer_cleanup_env_0_2021-11-17_17-38-56m07ou1p0/checkpoint_20
== Status ==
Memory usage on this node: 14.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------+--------+------------------+----------+----------+
| Trial name                           | status   | loc   |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  |       |    160 |          22017.4 | 15360000 |    18.37 |
+--------------------------------------+----------+-------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m 2021-11-18 02:17:11,297	INFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution
[2m[36m(pid=13408)[0m 2021-11-18 02:17:11,314	INFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=13408)[0m 2021-11-18 02:19:01,146	INFO trainable.py:180 -- _setup took 109.848 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(pid=13408)[0m 2021-11-18 02:19:01,146	INFO trainable.py:217 -- Getting current IP.
[2m[36m(pid=13408)[0m 2021-11-18 02:19:01,147	WARNING util.py:37 -- Install gputil for GPU system monitoring.
[2m[36m(pid=13408)[0m 2021-11-18 02:19:04,222	INFO trainable.py:217 -- Getting current IP.
[2m[36m(pid=13408)[0m 2021-11-18 02:19:04,222	INFO trainable.py:423 -- Restored on 172.17.8.5 from checkpoint: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics/BaselinePPOTrainer_cleanup_env_0_2021-11-17_17-39-29lhwwiwf8/tmph2kqnw9_restore_from_object/checkpoint-160
[2m[36m(pid=13408)[0m 2021-11-18 02:19:04,222	INFO trainable.py:430 -- Current state after restoring: {'_iteration': 160, '_timesteps_total': 15360000, '_time_total': 22510.283242464066, '_episodes_total': 15360}
== Status ==
Memory usage on this node: 20.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------+--------+------------------+----------+----------+
| Trial name                           | status   | loc   |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  |       |    160 |          22017.4 | 15360000 |    18.37 |
+--------------------------------------+----------+-------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 1.8854166666666667
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.7604166666666666
    apples_agent-1_min: 0
    apples_agent-2_max: 72
    apples_agent-2_mean: 2.3541666666666665
    apples_agent-2_min: 0
    apples_agent-3_max: 10
    apples_agent-3_mean: 2.09375
    apples_agent-3_min: 0
    apples_agent-4_max: 37
    apples_agent-4_mean: 1.1770833333333333
    apples_agent-4_min: 0
    apples_agent-5_max: 25
    apples_agent-5_mean: 1.1666666666666667
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 235
    cleaning_beam_agent-0_mean: 104.20833333333333
    cleaning_beam_agent-0_min: 35
    cleaning_beam_agent-1_max: 517
    cleaning_beam_agent-1_mean: 275.3541666666667
    cleaning_beam_agent-1_min: 153
    cleaning_beam_agent-2_max: 82
    cleaning_beam_agent-2_mean: 23.927083333333332
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 221
    cleaning_beam_agent-3_mean: 86.72916666666667
    cleaning_beam_agent-3_min: 20
    cleaning_beam_agent-4_max: 342
    cleaning_beam_agent-4_mean: 101.16666666666667
    cleaning_beam_agent-4_min: 25
    cleaning_beam_agent-5_max: 64
    cleaning_beam_agent-5_mean: 18.0
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.010416666666666666
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.020833333333333332
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.010416666666666666
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-21-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 44.0
  episode_reward_mean: 15.1875
  episode_reward_min: -50.0
  episodes_this_iter: 96
  episodes_total: 15456
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 21681.134
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00030153599800541997
        entropy: 0.6481750011444092
        entropy_coeff: 0.0017600000137463212
        kl: 0.00844875629991293
        model: {}
        policy_loss: -0.021566251292824745
        total_loss: -0.020992498844861984
        vf_explained_var: 0.018734082579612732
        vf_loss: 0.24789124727249146
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00030153599800541997
        entropy: 0.5657235980033875
        entropy_coeff: 0.0017600000137463212
        kl: 0.006482735276222229
        model: {}
        policy_loss: -0.01624651998281479
        total_loss: -0.01593312993645668
        vf_explained_var: 0.0321488082408905
        vf_loss: 0.12514452636241913
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00030153599800541997
        entropy: 0.5609476566314697
        entropy_coeff: 0.0017600000137463212
        kl: 0.00619343388825655
        model: {}
        policy_loss: -0.01307175774127245
        total_loss: -0.012643006630241871
        vf_explained_var: -0.00031970441341400146
        vf_loss: 1.7733285427093506
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00030153599800541997
        entropy: 0.6928467750549316
        entropy_coeff: 0.0017600000137463212
        kl: 0.005669602192938328
        model: {}
        policy_loss: -0.01071084849536419
        total_loss: -0.010640734806656837
        vf_explained_var: 0.004483133554458618
        vf_loss: 1.5560591220855713
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00030153599800541997
        entropy: 0.7410314083099365
        entropy_coeff: 0.0017600000137463212
        kl: 0.008124304935336113
        model: {}
        policy_loss: -0.022315576672554016
        total_loss: -0.021961862221360207
        vf_explained_var: 0.020869180560112
        vf_loss: 0.33069396018981934
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00030153599800541997
        entropy: 0.8021469712257385
        entropy_coeff: 0.0017600000137463212
        kl: 0.0061763860285282135
        model: {}
        policy_loss: -0.014684293419122696
        total_loss: -0.014850407838821411
        vf_explained_var: 0.029760047793388367
        vf_loss: 0.10380341112613678
    load_time_ms: 17605.636
    num_steps_sampled: 15456000
    num_steps_trained: 15456000
    sample_time_ms: 99851.505
    update_time_ms: 3247.899
  iterations_since_restore: 1
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.348868778280544
    ram_util_percent: 12.473755656108601
  pid: 13408
  policy_reward_max:
    agent-0: 9.0
    agent-1: 7.0
    agent-2: 22.0
    agent-3: 16.0
    agent-4: 15.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 3.0729166666666665
    agent-1: 1.625
    agent-2: 3.3125
    agent-3: 2.2604166666666665
    agent-4: 3.5729166666666665
    agent-5: 1.34375
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -33.0
    agent-3: -49.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.970630612089124
    mean_inference_ms: 14.084385428236518
    mean_processing_ms: 57.403368708533996
  time_since_restore: 145.7201521396637
  time_this_iter_s: 145.7201521396637
  time_total_s: 22656.00339460373
  timestamp: 1637220096
  timesteps_since_restore: 96000
  timesteps_this_iter: 96000
  timesteps_total: 15456000
  training_iteration: 161
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    161 |            22656 | 15456000 |  15.1875 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 6
    apples_agent-0_mean: 1.56
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.69
    apples_agent-1_min: 0
    apples_agent-2_max: 9
    apples_agent-2_mean: 1.75
    apples_agent-2_min: 0
    apples_agent-3_max: 12
    apples_agent-3_mean: 2.5
    apples_agent-3_min: 0
    apples_agent-4_max: 27
    apples_agent-4_mean: 1.29
    apples_agent-4_min: 0
    apples_agent-5_max: 9
    apples_agent-5_mean: 1.16
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 214
    cleaning_beam_agent-0_mean: 104.73
    cleaning_beam_agent-0_min: 44
    cleaning_beam_agent-1_max: 516
    cleaning_beam_agent-1_mean: 279.38
    cleaning_beam_agent-1_min: 128
    cleaning_beam_agent-2_max: 69
    cleaning_beam_agent-2_mean: 24.42
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 185
    cleaning_beam_agent-3_mean: 92.29
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 230
    cleaning_beam_agent-4_mean: 105.28
    cleaning_beam_agent-4_min: 42
    cleaning_beam_agent-5_max: 64
    cleaning_beam_agent-5_mean: 17.58
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-23-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 49.0
  episode_reward_mean: 16.39
  episode_reward_min: -88.0
  episodes_this_iter: 96
  episodes_total: 15552
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 16687.019
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00029554558568634093
        entropy: 0.6436600685119629
        entropy_coeff: 0.0017600000137463212
        kl: 0.00744611443951726
        model: {}
        policy_loss: -0.01946868933737278
        total_loss: -0.019085844978690147
        vf_explained_var: 0.009884417057037354
        vf_loss: 0.26461610198020935
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00029554558568634093
        entropy: 0.574517011642456
        entropy_coeff: 0.0017600000137463212
        kl: 0.0031120518688112497
        model: {}
        policy_loss: -0.005261650308966637
        total_loss: -0.005127560812979937
        vf_explained_var: 0.017032086849212646
        vf_loss: 5.228274345397949
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00029554558568634093
        entropy: 0.5623654127120972
        entropy_coeff: 0.0017600000137463212
        kl: 0.008502732031047344
        model: {}
        policy_loss: -0.020895905792713165
        total_loss: -0.020144116133451462
        vf_explained_var: 0.00024233758449554443
        vf_loss: 0.4100883901119232
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00029554558568634093
        entropy: 0.6925795674324036
        entropy_coeff: 0.0017600000137463212
        kl: 0.0076194643042981625
        model: {}
        policy_loss: -0.01638021320104599
        total_loss: -0.016048334538936615
        vf_explained_var: -0.003796130418777466
        vf_loss: 0.2692827582359314
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00029554558568634093
        entropy: 0.7323919534683228
        entropy_coeff: 0.0017600000137463212
        kl: 0.008121946826577187
        model: {}
        policy_loss: -0.02124471589922905
        total_loss: -0.02086871862411499
        vf_explained_var: 0.034956008195877075
        vf_loss: 0.40617308020591736
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00029554558568634093
        entropy: 0.8175869584083557
        entropy_coeff: 0.0017600000137463212
        kl: 0.006395826581865549
        model: {}
        policy_loss: -0.01356060802936554
        total_loss: -0.01370164379477501
        vf_explained_var: 0.03352685272693634
        vf_loss: 0.18757040798664093
    load_time_ms: 15578.805
    num_steps_sampled: 15552000
    num_steps_trained: 15552000
    sample_time_ms: 99265.804
    update_time_ms: 1634.22
  iterations_since_restore: 2
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.481920903954798
    ram_util_percent: 13.831638418079098
  pid: 13408
  policy_reward_max:
    agent-0: 12.0
    agent-1: 9.0
    agent-2: 20.0
    agent-3: 10.0
    agent-4: 16.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 2.96
    agent-1: 0.62
    agent-2: 3.9
    agent-3: 3.07
    agent-4: 3.97
    agent-5: 1.87
  policy_reward_min:
    agent-0: 0.0
    agent-1: -100.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 23.094485876097835
    mean_inference_ms: 13.443908718330995
    mean_processing_ms: 57.491781886180924
  time_since_restore: 269.7700915336609
  time_this_iter_s: 124.04993939399719
  time_total_s: 22780.053333997726
  timestamp: 1637220220
  timesteps_since_restore: 192000
  timesteps_this_iter: 96000
  timesteps_total: 15552000
  training_iteration: 162
  trial_id: '00000'
  
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
== Status ==
Memory usage on this node: 25.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    162 |          22780.1 | 15552000 |    16.39 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 1.92
    apples_agent-0_min: 0
    apples_agent-1_max: 20
    apples_agent-1_mean: 1.01
    apples_agent-1_min: 0
    apples_agent-2_max: 24
    apples_agent-2_mean: 2.02
    apples_agent-2_min: 0
    apples_agent-3_max: 21
    apples_agent-3_mean: 2.58
    apples_agent-3_min: 0
    apples_agent-4_max: 11
    apples_agent-4_mean: 0.99
    apples_agent-4_min: 0
    apples_agent-5_max: 23
    apples_agent-5_mean: 1.38
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 277
    cleaning_beam_agent-0_mean: 107.24
    cleaning_beam_agent-0_min: 44
    cleaning_beam_agent-1_max: 449
    cleaning_beam_agent-1_mean: 278.16
    cleaning_beam_agent-1_min: 157
    cleaning_beam_agent-2_max: 104
    cleaning_beam_agent-2_mean: 24.12
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 203
    cleaning_beam_agent-3_mean: 90.83
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 295
    cleaning_beam_agent-4_mean: 111.4
    cleaning_beam_agent-4_min: 20
    cleaning_beam_agent-5_max: 62
    cleaning_beam_agent-5_mean: 14.17
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-25-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 49.0
  episode_reward_mean: 17.61
  episode_reward_min: -86.0
  episodes_this_iter: 96
  episodes_total: 15648
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 14999.832
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00028955520247109234
        entropy: 0.6447887420654297
        entropy_coeff: 0.0017600000137463212
        kl: 0.008040342479944229
        model: {}
        policy_loss: -0.01965373195707798
        total_loss: -0.019155586138367653
        vf_explained_var: 0.011683210730552673
        vf_loss: 0.2490682303905487
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00028955520247109234
        entropy: 0.5714012384414673
        entropy_coeff: 0.0017600000137463212
        kl: 0.008952812291681767
        model: {}
        policy_loss: -0.017363373190164566
        total_loss: -0.017460107803344727
        vf_explained_var: -0.03938645124435425
        vf_loss: 0.13648325204849243
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00028955520247109234
        entropy: 0.5487769246101379
        entropy_coeff: 0.0017600000137463212
        kl: 0.007680173963308334
        model: {}
        policy_loss: -0.020520461723208427
        total_loss: -0.019900325685739517
        vf_explained_var: 0.013441115617752075
        vf_loss: 0.4994921088218689
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00028955520247109234
        entropy: 0.7006030082702637
        entropy_coeff: 0.0017600000137463212
        kl: 0.0038425892125815153
        model: {}
        policy_loss: -0.0067106857895851135
        total_loss: -0.0066367583349347115
        vf_explained_var: 0.001024678349494934
        vf_loss: 5.384715557098389
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00028955520247109234
        entropy: 0.7174469232559204
        entropy_coeff: 0.0017600000137463212
        kl: 0.008581109344959259
        model: {}
        policy_loss: -0.022727111354470253
        total_loss: -0.02224181964993477
        vf_explained_var: 0.03873412311077118
        vf_loss: 0.31775587797164917
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00028955520247109234
        entropy: 0.7993197441101074
        entropy_coeff: 0.0017600000137463212
        kl: 0.007141332142055035
        model: {}
        policy_loss: -0.01309693418443203
        total_loss: -0.013061445206403732
        vf_explained_var: 0.01886861026287079
        vf_loss: 0.1402316391468048
    load_time_ms: 14888.587
    num_steps_sampled: 15648000
    num_steps_trained: 15648000
    sample_time_ms: 98766.754
    update_time_ms: 1096.787
  iterations_since_restore: 3
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.573142857142862
    ram_util_percent: 13.90171428571428
  pid: 13408
  policy_reward_max:
    agent-0: 13.0
    agent-1: 12.0
    agent-2: 15.0
    agent-3: 15.0
    agent-4: 13.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 2.9
    agent-1: 1.74
    agent-2: 4.79
    agent-3: 2.63
    agent-4: 3.98
    agent-5: 1.57
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -99.0
    agent-4: -1.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.086233199287104
    mean_inference_ms: 13.212199316044616
    mean_processing_ms: 57.46977211183015
  time_since_restore: 392.7932598590851
  time_this_iter_s: 123.0231683254242
  time_total_s: 22903.07650232315
  timestamp: 1637220343
  timesteps_since_restore: 288000
  timesteps_this_iter: 96000
  timesteps_total: 15648000
  training_iteration: 163
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    163 |          22903.1 | 15648000 |    17.61 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 2.37
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.88
    apples_agent-1_min: 0
    apples_agent-2_max: 99
    apples_agent-2_mean: 2.63
    apples_agent-2_min: 0
    apples_agent-3_max: 12
    apples_agent-3_mean: 2.67
    apples_agent-3_min: 0
    apples_agent-4_max: 11
    apples_agent-4_mean: 1.24
    apples_agent-4_min: 0
    apples_agent-5_max: 6
    apples_agent-5_mean: 0.89
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 193
    cleaning_beam_agent-0_mean: 104.14
    cleaning_beam_agent-0_min: 31
    cleaning_beam_agent-1_max: 486
    cleaning_beam_agent-1_mean: 301.12
    cleaning_beam_agent-1_min: 145
    cleaning_beam_agent-2_max: 98
    cleaning_beam_agent-2_mean: 26.57
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 182
    cleaning_beam_agent-3_mean: 95.2
    cleaning_beam_agent-3_min: 40
    cleaning_beam_agent-4_max: 204
    cleaning_beam_agent-4_mean: 96.04
    cleaning_beam_agent-4_min: 21
    cleaning_beam_agent-5_max: 60
    cleaning_beam_agent-5_mean: 15.16
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-27-48
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 58.0
  episode_reward_mean: 18.5
  episode_reward_min: 0.0
  episodes_this_iter: 96
  episodes_total: 15744
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 14177.449
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002835647901520133
        entropy: 0.6325246095657349
        entropy_coeff: 0.0017600000137463212
        kl: 0.007704901974648237
        model: {}
        policy_loss: -0.019546259194612503
        total_loss: -0.019089151173830032
        vf_explained_var: 0.003531038761138916
        vf_loss: 0.29369208216667175
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002835647901520133
        entropy: 0.5749518275260925
        entropy_coeff: 0.0017600000137463212
        kl: 0.007866381667554379
        model: {}
        policy_loss: -0.015704361721873283
        total_loss: -0.015911618247628212
        vf_explained_var: 0.03168833255767822
        vf_loss: 0.18017904460430145
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002835647901520133
        entropy: 0.557354748249054
        entropy_coeff: 0.0017600000137463212
        kl: 0.007813439704477787
        model: {}
        policy_loss: -0.019342776387929916
        total_loss: -0.018719470128417015
        vf_explained_var: 0.005969792604446411
        vf_loss: 0.4156613349914551
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002835647901520133
        entropy: 0.6997281312942505
        entropy_coeff: 0.0017600000137463212
        kl: 0.008887248113751411
        model: {}
        policy_loss: -0.017214545980095863
        total_loss: -0.01752139814198017
        vf_explained_var: -0.0042157769203186035
        vf_loss: 0.35950297117233276
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002835647901520133
        entropy: 0.7133798599243164
        entropy_coeff: 0.0017600000137463212
        kl: 0.008254043757915497
        model: {}
        policy_loss: -0.0212816521525383
        total_loss: -0.020847653970122337
        vf_explained_var: 0.014416694641113281
        vf_loss: 0.3873475193977356
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002835647901520133
        entropy: 0.7775532603263855
        entropy_coeff: 0.0017600000137463212
        kl: 0.006117041222751141
        model: {}
        policy_loss: -0.013337681069970131
        total_loss: -0.013470452278852463
        vf_explained_var: 0.026592418551445007
        vf_loss: 0.12312769889831543
    load_time_ms: 14522.277
    num_steps_sampled: 15744000
    num_steps_trained: 15744000
    sample_time_ms: 98930.824
    update_time_ms: 827.502
  iterations_since_restore: 4
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.42824858757062
    ram_util_percent: 13.882485875706214
  pid: 13408
  policy_reward_max:
    agent-0: 17.0
    agent-1: 13.0
    agent-2: 17.0
    agent-3: 16.0
    agent-4: 17.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 3.04
    agent-1: 1.9
    agent-2: 4.33
    agent-3: 3.56
    agent-4: 4.07
    agent-5: 1.6
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -1.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.137140247509873
    mean_inference_ms: 13.093508438342543
    mean_processing_ms: 57.48831792229681
  time_since_restore: 517.4683704376221
  time_this_iter_s: 124.67511057853699
  time_total_s: 23027.751612901688
  timestamp: 1637220468
  timesteps_since_restore: 384000
  timesteps_this_iter: 96000
  timesteps_total: 15744000
  training_iteration: 164
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    164 |          23027.8 | 15744000 |     18.5 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 14
    apples_agent-0_mean: 2.11
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 0.78
    apples_agent-1_min: 0
    apples_agent-2_max: 73
    apples_agent-2_mean: 2.7
    apples_agent-2_min: 0
    apples_agent-3_max: 17
    apples_agent-3_mean: 2.15
    apples_agent-3_min: 0
    apples_agent-4_max: 17
    apples_agent-4_mean: 1.25
    apples_agent-4_min: 0
    apples_agent-5_max: 95
    apples_agent-5_mean: 1.8
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 225
    cleaning_beam_agent-0_mean: 105.97
    cleaning_beam_agent-0_min: 33
    cleaning_beam_agent-1_max: 429
    cleaning_beam_agent-1_mean: 284.07
    cleaning_beam_agent-1_min: 169
    cleaning_beam_agent-2_max: 76
    cleaning_beam_agent-2_mean: 25.7
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 193
    cleaning_beam_agent-3_mean: 94.19
    cleaning_beam_agent-3_min: 27
    cleaning_beam_agent-4_max: 276
    cleaning_beam_agent-4_mean: 98.46
    cleaning_beam_agent-4_min: 14
    cleaning_beam_agent-5_max: 61
    cleaning_beam_agent-5_mean: 13.33
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-29-52
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 48.0
  episode_reward_mean: 16.48
  episode_reward_min: -91.0
  episodes_this_iter: 96
  episodes_total: 15840
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 13676.104
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002775744069367647
        entropy: 0.6363162398338318
        entropy_coeff: 0.0017600000137463212
        kl: 0.008490932174026966
        model: {}
        policy_loss: -0.021047495305538177
        total_loss: -0.020445751026272774
        vf_explained_var: 0.010130882263183594
        vf_loss: 0.23477071523666382
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002775744069367647
        entropy: 0.5726499557495117
        entropy_coeff: 0.0017600000137463212
        kl: 0.008025658316910267
        model: {}
        policy_loss: -0.01624317280948162
        total_loss: -0.016434786841273308
        vf_explained_var: 0.01825214922428131
        vf_loss: 0.136857271194458
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002775744069367647
        entropy: 0.5617305636405945
        entropy_coeff: 0.0017600000137463212
        kl: 0.007255377247929573
        model: {}
        policy_loss: -0.019844502210617065
        total_loss: -0.019340263679623604
        vf_explained_var: 0.00877012312412262
        vf_loss: 0.418073445558548
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002775744069367647
        entropy: 0.6898356080055237
        entropy_coeff: 0.0017600000137463212
        kl: 0.0058563463389873505
        model: {}
        policy_loss: -0.00802008155733347
        total_loss: -0.008407261222600937
        vf_explained_var: 0.0074732303619384766
        vf_loss: 2.412950038909912
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002775744069367647
        entropy: 0.7263143658638
        entropy_coeff: 0.0017600000137463212
        kl: 0.007509635761380196
        model: {}
        policy_loss: -0.020355280488729477
        total_loss: -0.020091358572244644
        vf_explained_var: 0.038128986954689026
        vf_loss: 0.40306127071380615
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002775744069367647
        entropy: 0.7866082787513733
        entropy_coeff: 0.0017600000137463212
        kl: 0.0050611477345228195
        model: {}
        policy_loss: -0.006523396819829941
        total_loss: -0.0067665185779333115
        vf_explained_var: 0.0026633739471435547
        vf_loss: 1.2907938957214355
    load_time_ms: 14276.857
    num_steps_sampled: 15840000
    num_steps_trained: 15840000
    sample_time_ms: 98971.6
    update_time_ms: 666.284
  iterations_since_restore: 5
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.53050847457627
    ram_util_percent: 13.822598870056497
  pid: 13408
  policy_reward_max:
    agent-0: 12.0
    agent-1: 9.0
    agent-2: 15.0
    agent-3: 9.0
    agent-4: 14.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 3.04
    agent-1: 1.72
    agent-2: 4.26
    agent-3: 2.31
    agent-4: 3.96
    agent-5: 1.19
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -49.0
    agent-4: -1.0
    agent-5: -48.0
  sampler_perf:
    mean_env_wait_ms: 23.158282716241523
    mean_inference_ms: 13.027130655607909
    mean_processing_ms: 57.59886424717245
  time_since_restore: 641.6892971992493
  time_this_iter_s: 124.2209267616272
  time_total_s: 23151.972539663315
  timestamp: 1637220592
  timesteps_since_restore: 480000
  timesteps_this_iter: 96000
  timesteps_total: 15840000
  training_iteration: 165
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    165 |            23152 | 15840000 |    16.48 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 1.81
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.78
    apples_agent-1_min: 0
    apples_agent-2_max: 22
    apples_agent-2_mean: 2.22
    apples_agent-2_min: 0
    apples_agent-3_max: 43
    apples_agent-3_mean: 2.73
    apples_agent-3_min: 0
    apples_agent-4_max: 43
    apples_agent-4_mean: 1.24
    apples_agent-4_min: 0
    apples_agent-5_max: 13
    apples_agent-5_mean: 0.75
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 184
    cleaning_beam_agent-0_mean: 94.07
    cleaning_beam_agent-0_min: 26
    cleaning_beam_agent-1_max: 407
    cleaning_beam_agent-1_mean: 270.48
    cleaning_beam_agent-1_min: 161
    cleaning_beam_agent-2_max: 124
    cleaning_beam_agent-2_mean: 24.53
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 194
    cleaning_beam_agent-3_mean: 92.27
    cleaning_beam_agent-3_min: 29
    cleaning_beam_agent-4_max: 244
    cleaning_beam_agent-4_mean: 95.92
    cleaning_beam_agent-4_min: 30
    cleaning_beam_agent-5_max: 56
    cleaning_beam_agent-5_mean: 17.12
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-31-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 42.0
  episode_reward_mean: 17.17
  episode_reward_min: 1.0
  episodes_this_iter: 96
  episodes_total: 15936
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 13328.964
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002715839946176857
        entropy: 0.61893230676651
        entropy_coeff: 0.0017600000137463212
        kl: 0.007537891622632742
        model: {}
        policy_loss: -0.020738232880830765
        total_loss: -0.020298350602388382
        vf_explained_var: 0.015291556715965271
        vf_loss: 0.21619102358818054
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002715839946176857
        entropy: 0.5626201629638672
        entropy_coeff: 0.0017600000137463212
        kl: 0.00771283870562911
        model: {}
        policy_loss: -0.015529986470937729
        total_loss: -0.015733789652585983
        vf_explained_var: 0.02512037754058838
        vf_loss: 0.15123993158340454
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002715839946176857
        entropy: 0.552619218826294
        entropy_coeff: 0.0017600000137463212
        kl: 0.00836358405649662
        model: {}
        policy_loss: -0.018585069105029106
        total_loss: -0.017841633409261703
        vf_explained_var: 0.007109403610229492
        vf_loss: 0.43329089879989624
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002715839946176857
        entropy: 0.6917426586151123
        entropy_coeff: 0.0017600000137463212
        kl: 0.008773239329457283
        model: {}
        policy_loss: -0.017213938757777214
        total_loss: -0.017529785633087158
        vf_explained_var: -0.010400176048278809
        vf_loss: 0.2429591417312622
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002715839946176857
        entropy: 0.7132716774940491
        entropy_coeff: 0.0017600000137463212
        kl: 0.007819208316504955
        model: {}
        policy_loss: -0.01957043819129467
        total_loss: -0.019219011068344116
        vf_explained_var: 0.03326931595802307
        vf_loss: 0.4294309914112091
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002715839946176857
        entropy: 0.8039750456809998
        entropy_coeff: 0.0017600000137463212
        kl: 0.0061781145632267
        model: {}
        policy_loss: -0.013542452827095985
        total_loss: -0.01371060125529766
        vf_explained_var: 0.02478744089603424
        vf_loss: 0.11229369044303894
    load_time_ms: 14145.115
    num_steps_sampled: 15936000
    num_steps_trained: 15936000
    sample_time_ms: 98719.237
    update_time_ms: 557.594
  iterations_since_restore: 6
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.24057142857142
    ram_util_percent: 13.926857142857145
  pid: 13408
  policy_reward_max:
    agent-0: 10.0
    agent-1: 8.0
    agent-2: 16.0
    agent-3: 11.0
    agent-4: 15.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 2.81
    agent-1: 1.83
    agent-2: 4.06
    agent-3: 2.96
    agent-4: 4.03
    agent-5: 1.48
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.128896132508054
    mean_inference_ms: 12.97855745508238
    mean_processing_ms: 57.56359280910367
  time_since_restore: 764.3393478393555
  time_this_iter_s: 122.6500506401062
  time_total_s: 23274.62259030342
  timestamp: 1637220715
  timesteps_since_restore: 576000
  timesteps_this_iter: 96000
  timesteps_total: 15936000
  training_iteration: 166
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    166 |          23274.6 | 15936000 |    17.17 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 1.68
    apples_agent-0_min: 0
    apples_agent-1_max: 14
    apples_agent-1_mean: 1.07
    apples_agent-1_min: 0
    apples_agent-2_max: 13
    apples_agent-2_mean: 2.03
    apples_agent-2_min: 0
    apples_agent-3_max: 17
    apples_agent-3_mean: 2.36
    apples_agent-3_min: 0
    apples_agent-4_max: 5
    apples_agent-4_mean: 0.95
    apples_agent-4_min: 0
    apples_agent-5_max: 5
    apples_agent-5_mean: 0.99
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 220
    cleaning_beam_agent-0_mean: 95.74
    cleaning_beam_agent-0_min: 28
    cleaning_beam_agent-1_max: 464
    cleaning_beam_agent-1_mean: 285.13
    cleaning_beam_agent-1_min: 132
    cleaning_beam_agent-2_max: 79
    cleaning_beam_agent-2_mean: 24.97
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 222
    cleaning_beam_agent-3_mean: 99.42
    cleaning_beam_agent-3_min: 29
    cleaning_beam_agent-4_max: 259
    cleaning_beam_agent-4_mean: 98.84
    cleaning_beam_agent-4_min: 24
    cleaning_beam_agent-5_max: 56
    cleaning_beam_agent-5_mean: 16.47
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-33-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 43.0
  episode_reward_mean: 16.41
  episode_reward_min: 1.0
  episodes_this_iter: 96
  episodes_total: 16032
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 13072.702
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002655936114024371
        entropy: 0.6268328428268433
        entropy_coeff: 0.0017600000137463212
        kl: 0.007983741350471973
        model: {}
        policy_loss: -0.02098808065056801
        total_loss: -0.020472927019000053
        vf_explained_var: 0.00817178189754486
        vf_loss: 0.2163272500038147
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002655936114024371
        entropy: 0.5727484226226807
        entropy_coeff: 0.0017600000137463212
        kl: 0.007782864384353161
        model: {}
        policy_loss: -0.014891767874360085
        total_loss: -0.015107184648513794
        vf_explained_var: 0.033872365951538086
        vf_loss: 0.14331050217151642
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002655936114024371
        entropy: 0.5549607872962952
        entropy_coeff: 0.0017600000137463212
        kl: 0.007487580180168152
        model: {}
        policy_loss: -0.0199720598757267
        total_loss: -0.01942192204296589
        vf_explained_var: 0.01130037009716034
        vf_loss: 0.2935018539428711
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002655936114024371
        entropy: 0.6748261451721191
        entropy_coeff: 0.0017600000137463212
        kl: 0.008079309947788715
        model: {}
        policy_loss: -0.015248343348503113
        total_loss: -0.015589220449328423
        vf_explained_var: 0.004796355962753296
        vf_loss: 0.3888292610645294
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002655936114024371
        entropy: 0.7117189168930054
        entropy_coeff: 0.0017600000137463212
        kl: 0.008225003257393837
        model: {}
        policy_loss: -0.021214932203292847
        total_loss: -0.020790398120880127
        vf_explained_var: 0.04006478190422058
        vf_loss: 0.3215894103050232
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002655936114024371
        entropy: 0.8112457990646362
        entropy_coeff: 0.0017600000137463212
        kl: 0.006626286543905735
        model: {}
        policy_loss: -0.013339877128601074
        total_loss: -0.013432983309030533
        vf_explained_var: 0.018695250153541565
        vf_loss: 0.09427983313798904
    load_time_ms: 14020.529
    num_steps_sampled: 16032000
    num_steps_trained: 16032000
    sample_time_ms: 98637.844
    update_time_ms: 480.92
  iterations_since_restore: 7
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.89828571428572
    ram_util_percent: 13.946285714285716
  pid: 13408
  policy_reward_max:
    agent-0: 11.0
    agent-1: 7.0
    agent-2: 17.0
    agent-3: 12.0
    agent-4: 11.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 2.76
    agent-1: 1.79
    agent-2: 3.43
    agent-3: 3.25
    agent-4: 3.8
    agent-5: 1.38
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.137060259873444
    mean_inference_ms: 12.944068190426638
    mean_processing_ms: 57.55284363685128
  time_since_restore: 887.3819844722748
  time_this_iter_s: 123.04263663291931
  time_total_s: 23397.66522693634
  timestamp: 1637220838
  timesteps_since_restore: 672000
  timesteps_this_iter: 96000
  timesteps_total: 16032000
  training_iteration: 167
  trial_id: '00000'
  
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
== Status ==
Memory usage on this node: 25.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    167 |          23397.7 | 16032000 |    16.41 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 7
    apples_agent-0_mean: 1.68
    apples_agent-0_min: 0
    apples_agent-1_max: 16
    apples_agent-1_mean: 0.91
    apples_agent-1_min: 0
    apples_agent-2_max: 12
    apples_agent-2_mean: 1.77
    apples_agent-2_min: 0
    apples_agent-3_max: 15
    apples_agent-3_mean: 2.23
    apples_agent-3_min: 0
    apples_agent-4_max: 32
    apples_agent-4_mean: 1.41
    apples_agent-4_min: 0
    apples_agent-5_max: 8
    apples_agent-5_mean: 0.92
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 225
    cleaning_beam_agent-0_mean: 93.49
    cleaning_beam_agent-0_min: 34
    cleaning_beam_agent-1_max: 480
    cleaning_beam_agent-1_mean: 270.6
    cleaning_beam_agent-1_min: 130
    cleaning_beam_agent-2_max: 68
    cleaning_beam_agent-2_mean: 23.44
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 211
    cleaning_beam_agent-3_mean: 91.47
    cleaning_beam_agent-3_min: 26
    cleaning_beam_agent-4_max: 293
    cleaning_beam_agent-4_mean: 97.97
    cleaning_beam_agent-4_min: 35
    cleaning_beam_agent-5_max: 49
    cleaning_beam_agent-5_mean: 16.21
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 8
    fire_beam_agent-1_mean: 0.09
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-36-01
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 56.0
  episode_reward_mean: 15.5
  episode_reward_min: -34.0
  episodes_this_iter: 96
  episodes_total: 16128
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 12869.987
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00025960319908335805
        entropy: 0.6214659214019775
        entropy_coeff: 0.0017600000137463212
        kl: 0.00598284462466836
        model: {}
        policy_loss: -0.011511996388435364
        total_loss: -0.01125816535204649
        vf_explained_var: 0.0006091594696044922
        vf_loss: 1.5104389190673828
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00025960319908335805
        entropy: 0.5690569877624512
        entropy_coeff: 0.0017600000137463212
        kl: 0.0077598923817276955
        model: {}
        policy_loss: -0.015054456889629364
        total_loss: -0.015260754153132439
        vf_explained_var: 0.047605931758880615
        vf_loss: 0.19254210591316223
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00025960319908335805
        entropy: 0.5389961004257202
        entropy_coeff: 0.0017600000137463212
        kl: 0.007130085490643978
        model: {}
        policy_loss: -0.018452061340212822
        total_loss: -0.017936520278453827
        vf_explained_var: 0.010692164301872253
        vf_loss: 0.38158899545669556
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00025960319908335805
        entropy: 0.677923858165741
        entropy_coeff: 0.0017600000137463212
        kl: 0.008366590365767479
        model: {}
        policy_loss: -0.016517849639058113
        total_loss: -0.016837790608406067
        vf_explained_var: 0.004179149866104126
        vf_loss: 0.3654657006263733
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00025960319908335805
        entropy: 0.7072254419326782
        entropy_coeff: 0.0017600000137463212
        kl: 0.00800981093198061
        model: {}
        policy_loss: -0.020203176885843277
        total_loss: -0.019800692796707153
        vf_explained_var: 0.03854908049106598
        vf_loss: 0.45238229632377625
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00025960319908335805
        entropy: 0.759550929069519
        entropy_coeff: 0.0017600000137463212
        kl: 0.005129888653755188
        model: {}
        policy_loss: -0.008807923644781113
        total_loss: -0.008847958408296108
        vf_explained_var: -0.0011966973543167114
        vf_loss: 2.7079784870147705
    load_time_ms: 13947.1
    num_steps_sampled: 16128000
    num_steps_trained: 16128000
    sample_time_ms: 98484.943
    update_time_ms: 422.516
  iterations_since_restore: 8
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.012068965517244
    ram_util_percent: 13.911494252873565
  pid: 13408
  policy_reward_max:
    agent-0: 10.0
    agent-1: 7.0
    agent-2: 13.0
    agent-3: 19.0
    agent-4: 19.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 2.21
    agent-1: 1.21
    agent-2: 3.96
    agent-3: 3.53
    agent-4: 4.25
    agent-5: 0.34
  policy_reward_min:
    agent-0: -49.0
    agent-1: -49.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -50.0
  sampler_perf:
    mean_env_wait_ms: 23.15650489432596
    mean_inference_ms: 12.92709595401596
    mean_processing_ms: 57.58488644157462
  time_since_restore: 1009.7641999721527
  time_this_iter_s: 122.38221549987793
  time_total_s: 23520.04744243622
  timestamp: 1637220961
  timesteps_since_restore: 768000
  timesteps_this_iter: 96000
  timesteps_total: 16128000
  training_iteration: 168
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    168 |            23520 | 16128000 |     15.5 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 1.64
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 0.95
    apples_agent-1_min: 0
    apples_agent-2_max: 14
    apples_agent-2_mean: 1.6
    apples_agent-2_min: 0
    apples_agent-3_max: 47
    apples_agent-3_mean: 2.72
    apples_agent-3_min: 0
    apples_agent-4_max: 8
    apples_agent-4_mean: 1.03
    apples_agent-4_min: 0
    apples_agent-5_max: 5
    apples_agent-5_mean: 0.64
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 225
    cleaning_beam_agent-0_mean: 94.34
    cleaning_beam_agent-0_min: 24
    cleaning_beam_agent-1_max: 481
    cleaning_beam_agent-1_mean: 265.68
    cleaning_beam_agent-1_min: 137
    cleaning_beam_agent-2_max: 145
    cleaning_beam_agent-2_mean: 27.99
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 228
    cleaning_beam_agent-3_mean: 104.06
    cleaning_beam_agent-3_min: 43
    cleaning_beam_agent-4_max: 278
    cleaning_beam_agent-4_mean: 81.26
    cleaning_beam_agent-4_min: 19
    cleaning_beam_agent-5_max: 81
    cleaning_beam_agent-5_mean: 16.38
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-38-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 44.0
  episode_reward_mean: 13.15
  episode_reward_min: -92.0
  episodes_this_iter: 96
  episodes_total: 16224
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 12715.544
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000253612786764279
        entropy: 0.6183996200561523
        entropy_coeff: 0.0017600000137463212
        kl: 0.007220057304948568
        model: {}
        policy_loss: -0.019510090351104736
        total_loss: -0.019134480506181717
        vf_explained_var: 0.009762972593307495
        vf_loss: 0.1997886300086975
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000253612786764279
        entropy: 0.5459131598472595
        entropy_coeff: 0.0017600000137463212
        kl: 0.005536547861993313
        model: {}
        policy_loss: -0.007643533870577812
        total_loss: -0.007776511833071709
        vf_explained_var: 0.009203538298606873
        vf_loss: 2.7417497634887695
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000253612786764279
        entropy: 0.5360562801361084
        entropy_coeff: 0.0017600000137463212
        kl: 0.004977892618626356
        model: {}
        policy_loss: -0.011569753289222717
        total_loss: -0.01135793887078762
        vf_explained_var: 0.009632900357246399
        vf_loss: 1.5969481468200684
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000253612786764279
        entropy: 0.6797667741775513
        entropy_coeff: 0.0017600000137463212
        kl: 0.005991499871015549
        model: {}
        policy_loss: -0.008923695422708988
        total_loss: -0.009365145117044449
        vf_explained_var: 0.003816381096839905
        vf_loss: 1.5578947067260742
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000253612786764279
        entropy: 0.6976853609085083
        entropy_coeff: 0.0017600000137463212
        kl: 0.008248981088399887
        model: {}
        policy_loss: -0.020396409556269646
        total_loss: -0.019943153485655785
        vf_explained_var: 0.029538989067077637
        vf_loss: 0.3138579726219177
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000253612786764279
        entropy: 0.8013185262680054
        entropy_coeff: 0.0017600000137463212
        kl: 0.006816626992076635
        model: {}
        policy_loss: -0.006604999303817749
        total_loss: -0.006511019542813301
        vf_explained_var: 0.0011578947305679321
        vf_loss: 1.4097635746002197
    load_time_ms: 13881.401
    num_steps_sampled: 16224000
    num_steps_trained: 16224000
    sample_time_ms: 98483.376
    update_time_ms: 378.063
  iterations_since_restore: 9
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.026136363636365
    ram_util_percent: 13.453409090909092
  pid: 13408
  policy_reward_max:
    agent-0: 13.0
    agent-1: 7.0
    agent-2: 14.0
    agent-3: 10.0
    agent-4: 11.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 2.45
    agent-1: 0.74
    agent-2: 2.86
    agent-3: 2.5
    agent-4: 3.82
    agent-5: 0.78
  policy_reward_min:
    agent-0: 0.0
    agent-1: -50.0
    agent-2: -46.0
    agent-3: -49.0
    agent-4: 0.0
    agent-5: -49.0
  sampler_perf:
    mean_env_wait_ms: 23.146867778762235
    mean_inference_ms: 12.910489654507385
    mean_processing_ms: 57.58320346412498
  time_since_restore: 1133.2168288230896
  time_this_iter_s: 123.45262885093689
  time_total_s: 23643.500071287155
  timestamp: 1637221084
  timesteps_since_restore: 864000
  timesteps_this_iter: 96000
  timesteps_total: 16224000
  training_iteration: 169
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 23.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    169 |          23643.5 | 16224000 |    13.15 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 2.35
    apples_agent-0_min: 0
    apples_agent-1_max: 10
    apples_agent-1_mean: 1.0
    apples_agent-1_min: 0
    apples_agent-2_max: 63
    apples_agent-2_mean: 2.47
    apples_agent-2_min: 0
    apples_agent-3_max: 23
    apples_agent-3_mean: 2.33
    apples_agent-3_min: 0
    apples_agent-4_max: 40
    apples_agent-4_mean: 1.64
    apples_agent-4_min: 0
    apples_agent-5_max: 10
    apples_agent-5_mean: 0.88
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 352
    cleaning_beam_agent-0_mean: 108.34
    cleaning_beam_agent-0_min: 22
    cleaning_beam_agent-1_max: 461
    cleaning_beam_agent-1_mean: 274.42
    cleaning_beam_agent-1_min: 165
    cleaning_beam_agent-2_max: 119
    cleaning_beam_agent-2_mean: 23.92
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 208
    cleaning_beam_agent-3_mean: 103.14
    cleaning_beam_agent-3_min: 43
    cleaning_beam_agent-4_max: 220
    cleaning_beam_agent-4_mean: 79.31
    cleaning_beam_agent-4_min: 29
    cleaning_beam_agent-5_max: 76
    cleaning_beam_agent-5_mean: 19.48
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-40-17
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 63.0
  episode_reward_mean: 15.85
  episode_reward_min: -48.0
  episodes_this_iter: 96
  episodes_total: 16320
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 12645.94
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002476224035490304
        entropy: 0.619295597076416
        entropy_coeff: 0.0017600000137463212
        kl: 0.0073644788935780525
        model: {}
        policy_loss: -0.01909433864057064
        total_loss: -0.01869024895131588
        vf_explained_var: -0.0006227642297744751
        vf_loss: 0.21155479550361633
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002476224035490304
        entropy: 0.5621861219406128
        entropy_coeff: 0.0017600000137463212
        kl: 0.007262424100190401
        model: {}
        policy_loss: -0.015153499320149422
        total_loss: -0.015399595722556114
        vf_explained_var: 0.02258247137069702
        vf_loss: 0.1710992008447647
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002476224035490304
        entropy: 0.5202033519744873
        entropy_coeff: 0.0017600000137463212
        kl: 0.007520326413214207
        model: {}
        policy_loss: -0.017327910289168358
        total_loss: -0.01745203323662281
        vf_explained_var: 0.018397539854049683
        vf_loss: 0.3940123915672302
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002476224035490304
        entropy: 0.6761457920074463
        entropy_coeff: 0.0017600000137463212
        kl: 0.007542944513261318
        model: {}
        policy_loss: -0.01588026061654091
        total_loss: -0.016285132616758347
        vf_explained_var: 0.006641015410423279
        vf_loss: 0.30853962898254395
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002476224035490304
        entropy: 0.7039044499397278
        entropy_coeff: 0.0017600000137463212
        kl: 0.007574439514428377
        model: {}
        policy_loss: -0.020959090441465378
        total_loss: -0.020654713734984398
        vf_explained_var: 0.019803568720817566
        vf_loss: 0.28357237577438354
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002476224035490304
        entropy: 0.8014607429504395
        entropy_coeff: 0.0017600000137463212
        kl: 0.005670864600688219
        model: {}
        policy_loss: -0.011195795610547066
        total_loss: -0.011455193161964417
        vf_explained_var: 0.012565717101097107
        vf_loss: 0.170004740357399
    load_time_ms: 14612.123
    num_steps_sampled: 16320000
    num_steps_trained: 16320000
    sample_time_ms: 98625.258
    update_time_ms: 341.865
  iterations_since_restore: 10
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 39.10894736842105
    ram_util_percent: 17.86368421052632
  pid: 13408
  policy_reward_max:
    agent-0: 12.0
    agent-1: 11.0
    agent-2: 14.0
    agent-3: 16.0
    agent-4: 16.0
    agent-5: 16.0
  policy_reward_mean:
    agent-0: 2.72
    agent-1: 1.93
    agent-2: 3.63
    agent-3: 3.16
    agent-4: 3.53
    agent-5: 0.88
  policy_reward_min:
    agent-0: 0.0
    agent-1: -1.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -50.0
  sampler_perf:
    mean_env_wait_ms: 23.161579958048897
    mean_inference_ms: 12.913311133171726
    mean_processing_ms: 57.69633819174051
  time_since_restore: 1266.4564082622528
  time_this_iter_s: 133.2395794391632
  time_total_s: 23776.73965072632
  timestamp: 1637221217
  timesteps_since_restore: 960000
  timesteps_this_iter: 96000
  timesteps_total: 16320000
  training_iteration: 170
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    170 |          23776.7 | 16320000 |    15.85 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 14
    apples_agent-0_mean: 2.12
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.76
    apples_agent-1_min: 0
    apples_agent-2_max: 12
    apples_agent-2_mean: 1.72
    apples_agent-2_min: 0
    apples_agent-3_max: 17
    apples_agent-3_mean: 2.31
    apples_agent-3_min: 0
    apples_agent-4_max: 17
    apples_agent-4_mean: 1.38
    apples_agent-4_min: 0
    apples_agent-5_max: 5
    apples_agent-5_mean: 0.73
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 271
    cleaning_beam_agent-0_mean: 100.34
    cleaning_beam_agent-0_min: 39
    cleaning_beam_agent-1_max: 547
    cleaning_beam_agent-1_mean: 273.49
    cleaning_beam_agent-1_min: 161
    cleaning_beam_agent-2_max: 119
    cleaning_beam_agent-2_mean: 25.68
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 205
    cleaning_beam_agent-3_mean: 101.36
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 381
    cleaning_beam_agent-4_mean: 95.36
    cleaning_beam_agent-4_min: 17
    cleaning_beam_agent-5_max: 85
    cleaning_beam_agent-5_mean: 17.34
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 4
    fire_beam_agent-1_mean: 0.06
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 5
    fire_beam_agent-5_mean: 0.07
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-42-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 43.0
  episode_reward_mean: 15.42
  episode_reward_min: -146.0
  episodes_this_iter: 96
  episodes_total: 16416
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11685.629
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002416320057818666
        entropy: 0.6174139380455017
        entropy_coeff: 0.0017600000137463212
        kl: 0.0076875099912285805
        model: {}
        policy_loss: -0.018864164128899574
        total_loss: -0.01838458888232708
        vf_explained_var: 0.008514061570167542
        vf_loss: 0.2872037887573242
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002416320057818666
        entropy: 0.5548462867736816
        entropy_coeff: 0.0017600000137463212
        kl: 0.0077028353698551655
        model: {}
        policy_loss: -0.016024179756641388
        total_loss: -0.016216285526752472
        vf_explained_var: 0.012261182069778442
        vf_loss: 0.1413835883140564
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002416320057818666
        entropy: 0.5267826318740845
        entropy_coeff: 0.0017600000137463212
        kl: 0.0062238480895757675
        model: {}
        policy_loss: -0.012105748057365417
        total_loss: -0.012217105366289616
        vf_explained_var: 0.007511824369430542
        vf_loss: 1.9339405298233032
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002416320057818666
        entropy: 0.6675716638565063
        entropy_coeff: 0.0017600000137463212
        kl: 0.007952055893838406
        model: {}
        policy_loss: -0.015239294618368149
        total_loss: -0.015586202032864094
        vf_explained_var: 0.008510366082191467
        vf_loss: 0.3281497359275818
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002416320057818666
        entropy: 0.6925032138824463
        entropy_coeff: 0.0017600000137463212
        kl: 0.003432821249589324
        model: {}
        policy_loss: -0.006484531797468662
        total_loss: -0.006562770344316959
        vf_explained_var: 0.005386263132095337
        vf_loss: 4.540042400360107
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002416320057818666
        entropy: 0.7910817861557007
        entropy_coeff: 0.0017600000137463212
        kl: 0.006361204199492931
        model: {}
        policy_loss: -0.013711493462324142
        total_loss: -0.013820725493133068
        vf_explained_var: 0.005102440714836121
        vf_loss: 0.10828684270381927
    load_time_ms: 14278.638
    num_steps_sampled: 16416000
    num_steps_trained: 16416000
    sample_time_ms: 98830.298
    update_time_ms: 18.529
  iterations_since_restore: 11
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.27119565217392
    ram_util_percent: 19.565760869565217
  pid: 13408
  policy_reward_max:
    agent-0: 11.0
    agent-1: 8.0
    agent-2: 12.0
    agent-3: 21.0
    agent-4: 10.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 2.93
    agent-1: 1.58
    agent-2: 3.12
    agent-3: 3.48
    agent-4: 2.86
    agent-5: 1.45
  policy_reward_min:
    agent-0: 0.0
    agent-1: -4.0
    agent-2: -48.0
    agent-3: 0.0
    agent-4: -99.0
    agent-5: -4.0
  sampler_perf:
    mean_env_wait_ms: 23.23634065929504
    mean_inference_ms: 12.948450984482115
    mean_processing_ms: 57.898768229035895
  time_since_restore: 1394.7950837612152
  time_this_iter_s: 128.3386754989624
  time_total_s: 23905.07832622528
  timestamp: 1637221346
  timesteps_since_restore: 1056000
  timesteps_this_iter: 96000
  timesteps_total: 16416000
  training_iteration: 171
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 36.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    171 |          23905.1 | 16416000 |    15.42 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 7
    apples_agent-0_mean: 1.69
    apples_agent-0_min: 0
    apples_agent-1_max: 14
    apples_agent-1_mean: 0.79
    apples_agent-1_min: 0
    apples_agent-2_max: 21
    apples_agent-2_mean: 1.58
    apples_agent-2_min: 0
    apples_agent-3_max: 14
    apples_agent-3_mean: 2.33
    apples_agent-3_min: 0
    apples_agent-4_max: 33
    apples_agent-4_mean: 1.43
    apples_agent-4_min: 0
    apples_agent-5_max: 8
    apples_agent-5_mean: 0.8
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 205
    cleaning_beam_agent-0_mean: 88.7
    cleaning_beam_agent-0_min: 28
    cleaning_beam_agent-1_max: 476
    cleaning_beam_agent-1_mean: 275.32
    cleaning_beam_agent-1_min: 133
    cleaning_beam_agent-2_max: 91
    cleaning_beam_agent-2_mean: 25.53
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 200
    cleaning_beam_agent-3_mean: 103.97
    cleaning_beam_agent-3_min: 34
    cleaning_beam_agent-4_max: 381
    cleaning_beam_agent-4_mean: 91.77
    cleaning_beam_agent-4_min: 18
    cleaning_beam_agent-5_max: 60
    cleaning_beam_agent-5_mean: 19.59
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-44-35
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 48.0
  episode_reward_mean: 14.25
  episode_reward_min: -87.0
  episodes_this_iter: 96
  episodes_total: 16512
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11715.223
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00023564159346278757
        entropy: 0.6174027323722839
        entropy_coeff: 0.0017600000137463212
        kl: 0.005430953577160835
        model: {}
        policy_loss: -0.009822560474276543
        total_loss: -0.00953822210431099
        vf_explained_var: 0.017990857362747192
        vf_loss: 2.847756862640381
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00023564159346278757
        entropy: 0.5472981333732605
        entropy_coeff: 0.0017600000137463212
        kl: 0.006576620042324066
        model: {}
        policy_loss: -0.012708879075944424
        total_loss: -0.012999383732676506
        vf_explained_var: 0.02362404763698578
        vf_loss: 0.15078210830688477
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00023564159346278757
        entropy: 0.5169944763183594
        entropy_coeff: 0.0017600000137463212
        kl: 0.007703298237174749
        model: {}
        policy_loss: -0.016830621287226677
        total_loss: -0.01693054288625717
        vf_explained_var: 0.014977872371673584
        vf_loss: 0.39663732051849365
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00023564159346278757
        entropy: 0.6617662906646729
        entropy_coeff: 0.0017600000137463212
        kl: 0.007881911471486092
        model: {}
        policy_loss: -0.01601525768637657
        total_loss: -0.016358746215701103
        vf_explained_var: 0.003484010696411133
        vf_loss: 0.33033987879753113
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00023564159346278757
        entropy: 0.6892012357711792
        entropy_coeff: 0.0017600000137463212
        kl: 0.008734837174415588
        model: {}
        policy_loss: -0.019700665026903152
        total_loss: -0.020002424716949463
        vf_explained_var: 0.018648818135261536
        vf_loss: 0.37750470638275146
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00023564159346278757
        entropy: 0.8028292059898376
        entropy_coeff: 0.0017600000137463212
        kl: 0.00471905991435051
        model: {}
        policy_loss: -0.008043666370213032
        total_loss: -0.008374616503715515
        vf_explained_var: 0.008767247200012207
        vf_loss: 1.3821719884872437
    load_time_ms: 14325.058
    num_steps_sampled: 16512000
    num_steps_trained: 16512000
    sample_time_ms: 99204.661
    update_time_ms: 17.873
  iterations_since_restore: 12
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.050819672131155
    ram_util_percent: 19.762841530054644
  pid: 13408
  policy_reward_max:
    agent-0: 13.0
    agent-1: 11.0
    agent-2: 13.0
    agent-3: 15.0
    agent-4: 14.0
    agent-5: 5.0
  policy_reward_mean:
    agent-0: 1.16
    agent-1: 1.56
    agent-2: 3.35
    agent-3: 3.55
    agent-4: 3.96
    agent-5: 0.67
  policy_reward_min:
    agent-0: -49.0
    agent-1: 0.0
    agent-2: -48.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -48.0
  sampler_perf:
    mean_env_wait_ms: 23.29559891085512
    mean_inference_ms: 12.974531656380398
    mean_processing_ms: 58.11967133275227
  time_since_restore: 1523.3488199710846
  time_this_iter_s: 128.55373620986938
  time_total_s: 24033.63206243515
  timestamp: 1637221475
  timesteps_since_restore: 1152000
  timesteps_this_iter: 96000
  timesteps_total: 16512000
  training_iteration: 172
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 36.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    172 |          24033.6 | 16512000 |    14.25 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 1.97
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.95
    apples_agent-1_min: 0
    apples_agent-2_max: 22
    apples_agent-2_mean: 1.97
    apples_agent-2_min: 0
    apples_agent-3_max: 55
    apples_agent-3_mean: 2.69
    apples_agent-3_min: 0
    apples_agent-4_max: 33
    apples_agent-4_mean: 1.16
    apples_agent-4_min: 0
    apples_agent-5_max: 8
    apples_agent-5_mean: 1.04
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 365
    cleaning_beam_agent-0_mean: 94.73
    cleaning_beam_agent-0_min: 28
    cleaning_beam_agent-1_max: 448
    cleaning_beam_agent-1_mean: 277.82
    cleaning_beam_agent-1_min: 197
    cleaning_beam_agent-2_max: 127
    cleaning_beam_agent-2_mean: 29.94
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 193
    cleaning_beam_agent-3_mean: 108.9
    cleaning_beam_agent-3_min: 47
    cleaning_beam_agent-4_max: 368
    cleaning_beam_agent-4_mean: 97.03
    cleaning_beam_agent-4_min: 18
    cleaning_beam_agent-5_max: 81
    cleaning_beam_agent-5_mean: 17.58
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 16
    fire_beam_agent-1_mean: 0.2
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-46-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 76.0
  episode_reward_mean: 17.01
  episode_reward_min: -33.0
  episodes_this_iter: 96
  episodes_total: 16608
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11755.474
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00022965119569562376
        entropy: 0.624883770942688
        entropy_coeff: 0.0017600000137463212
        kl: 0.00734317023307085
        model: {}
        policy_loss: -0.016811106353998184
        total_loss: -0.016406886279582977
        vf_explained_var: -0.008034825325012207
        vf_loss: 0.35385239124298096
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00022965119569562376
        entropy: 0.5491798520088196
        entropy_coeff: 0.0017600000137463212
        kl: 0.0068139429204165936
        model: {}
        policy_loss: -0.013405896723270416
        total_loss: -0.01366470754146576
        vf_explained_var: 0.03263510763645172
        vf_loss: 0.26351892948150635
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00022965119569562376
        entropy: 0.5297135710716248
        entropy_coeff: 0.0017600000137463212
        kl: 0.007990231737494469
        model: {}
        policy_loss: -0.018158819526433945
        total_loss: -0.018248675391077995
        vf_explained_var: 0.018077149987220764
        vf_loss: 0.4342082738876343
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00022965119569562376
        entropy: 0.6612246036529541
        entropy_coeff: 0.0017600000137463212
        kl: 0.007060662843286991
        model: {}
        policy_loss: -0.009170476347208023
        total_loss: -0.009473809972405434
        vf_explained_var: 0.009084850549697876
        vf_loss: 1.543548583984375
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00022965119569562376
        entropy: 0.6928715705871582
        entropy_coeff: 0.0017600000137463212
        kl: 0.009176240302622318
        model: {}
        policy_loss: -0.021381409838795662
        total_loss: -0.021648958325386047
        vf_explained_var: 0.03380022943019867
        vf_loss: 0.34286367893218994
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00022965119569562376
        entropy: 0.8213819265365601
        entropy_coeff: 0.0017600000137463212
        kl: 0.008216197602450848
        model: {}
        policy_loss: -0.014388296753168106
        total_loss: -0.01500085461884737
        vf_explained_var: 0.02596968412399292
        vf_loss: 0.11452975869178772
    load_time_ms: 14366.847
    num_steps_sampled: 16608000
    num_steps_trained: 16608000
    sample_time_ms: 99581.896
    update_time_ms: 17.43
  iterations_since_restore: 13
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 50.151933701657455
    ram_util_percent: 19.439779005524866
  pid: 13408
  policy_reward_max:
    agent-0: 23.0
    agent-1: 10.0
    agent-2: 16.0
    agent-3: 18.0
    agent-4: 13.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 3.15
    agent-1: 1.61
    agent-2: 4.25
    agent-3: 2.6
    agent-4: 3.85
    agent-5: 1.55
  policy_reward_min:
    agent-0: 0.0
    agent-1: -12.0
    agent-2: 0.0
    agent-3: -46.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.34022694600713
    mean_inference_ms: 12.997755207188334
    mean_processing_ms: 58.248613070247
  time_since_restore: 1650.9846606254578
  time_this_iter_s: 127.63584065437317
  time_total_s: 24161.267903089523
  timestamp: 1637221603
  timesteps_since_restore: 1248000
  timesteps_this_iter: 96000
  timesteps_total: 16608000
  training_iteration: 173
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    173 |          24161.3 | 16608000 |    17.01 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 2.36
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 0.87
    apples_agent-1_min: 0
    apples_agent-2_max: 10
    apples_agent-2_mean: 1.98
    apples_agent-2_min: 0
    apples_agent-3_max: 11
    apples_agent-3_mean: 2.73
    apples_agent-3_min: 0
    apples_agent-4_max: 21
    apples_agent-4_mean: 1.33
    apples_agent-4_min: 0
    apples_agent-5_max: 9
    apples_agent-5_mean: 0.88
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 196
    cleaning_beam_agent-0_mean: 95.31
    cleaning_beam_agent-0_min: 36
    cleaning_beam_agent-1_max: 432
    cleaning_beam_agent-1_mean: 273.83
    cleaning_beam_agent-1_min: 180
    cleaning_beam_agent-2_max: 86
    cleaning_beam_agent-2_mean: 29.26
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 201
    cleaning_beam_agent-3_mean: 102.0
    cleaning_beam_agent-3_min: 35
    cleaning_beam_agent-4_max: 243
    cleaning_beam_agent-4_mean: 90.97
    cleaning_beam_agent-4_min: 18
    cleaning_beam_agent-5_max: 72
    cleaning_beam_agent-5_mean: 16.84
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 14
    fire_beam_agent-1_mean: 0.34
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-48-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 71.0
  episode_reward_mean: 14.51
  episode_reward_min: -128.0
  episodes_this_iter: 96
  episodes_total: 16704
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11819.863
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00022366079792845994
        entropy: 0.626041054725647
        entropy_coeff: 0.0017600000137463212
        kl: 0.004068650770932436
        model: {}
        policy_loss: -0.007751655764877796
        total_loss: -0.006979938596487045
        vf_explained_var: 0.0023905038833618164
        vf_loss: 10.598182678222656
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00022366079792845994
        entropy: 0.543407678604126
        entropy_coeff: 0.0017600000137463212
        kl: 0.006763915065675974
        model: {}
        policy_loss: -0.01419057510793209
        total_loss: -0.014434854499995708
        vf_explained_var: 0.051778241991996765
        vf_loss: 0.3572520613670349
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00022366079792845994
        entropy: 0.5320046544075012
        entropy_coeff: 0.0017600000137463212
        kl: 0.007542148232460022
        model: {}
        policy_loss: -0.01779118925333023
        total_loss: -0.01792524755001068
        vf_explained_var: -0.0019266903400421143
        vf_loss: 0.48054131865501404
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00022366079792845994
        entropy: 0.6642169952392578
        entropy_coeff: 0.0017600000137463212
        kl: 0.005928520113229752
        model: {}
        policy_loss: -0.009280030615627766
        total_loss: -0.00969120766967535
        vf_explained_var: 0.007362931966781616
        vf_loss: 1.6499359607696533
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00022366079792845994
        entropy: 0.6955307722091675
        entropy_coeff: 0.0017600000137463212
        kl: 0.008619573898613453
        model: {}
        policy_loss: -0.02071458473801613
        total_loss: -0.021043619140982628
        vf_explained_var: 0.02275368571281433
        vf_loss: 0.331434041261673
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00022366079792845994
        entropy: 0.7849975824356079
        entropy_coeff: 0.0017600000137463212
        kl: 0.007533367723226547
        model: {}
        policy_loss: -0.011465819552540779
        total_loss: -0.012081491760909557
        vf_explained_var: 0.011016100645065308
        vf_loss: 0.1258707344532013
    load_time_ms: 14420.427
    num_steps_sampled: 16704000
    num_steps_trained: 16704000
    sample_time_ms: 99745.043
    update_time_ms: 16.977
  iterations_since_restore: 14
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.20879120879121
    ram_util_percent: 18.54945054945055
  pid: 13408
  policy_reward_max:
    agent-0: 18.0
    agent-1: 17.0
    agent-2: 16.0
    agent-3: 18.0
    agent-4: 15.0
    agent-5: 6.0
  policy_reward_mean:
    agent-0: 1.81
    agent-1: 1.7
    agent-2: 4.04
    agent-3: 2.21
    agent-4: 3.94
    agent-5: 0.81
  policy_reward_min:
    agent-0: -146.0
    agent-1: -11.0
    agent-2: -44.0
    agent-3: -48.0
    agent-4: 0.0
    agent-5: -50.0
  sampler_perf:
    mean_env_wait_ms: 23.361656827506913
    mean_inference_ms: 13.01687072766141
    mean_processing_ms: 58.329597928480354
  time_since_restore: 1778.4884395599365
  time_this_iter_s: 127.50377893447876
  time_total_s: 24288.771682024002
  timestamp: 1637221731
  timesteps_since_restore: 1344000
  timesteps_this_iter: 96000
  timesteps_total: 16704000
  training_iteration: 174
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    174 |          24288.8 | 16704000 |    14.51 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 8
    apples_agent-0_mean: 1.97
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.78
    apples_agent-1_min: 0
    apples_agent-2_max: 14
    apples_agent-2_mean: 1.87
    apples_agent-2_min: 0
    apples_agent-3_max: 16
    apples_agent-3_mean: 2.35
    apples_agent-3_min: 0
    apples_agent-4_max: 25
    apples_agent-4_mean: 1.21
    apples_agent-4_min: 0
    apples_agent-5_max: 10
    apples_agent-5_mean: 0.85
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 197
    cleaning_beam_agent-0_mean: 95.34
    cleaning_beam_agent-0_min: 31
    cleaning_beam_agent-1_max: 502
    cleaning_beam_agent-1_mean: 261.86
    cleaning_beam_agent-1_min: 149
    cleaning_beam_agent-2_max: 192
    cleaning_beam_agent-2_mean: 32.84
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 264
    cleaning_beam_agent-3_mean: 111.95
    cleaning_beam_agent-3_min: 39
    cleaning_beam_agent-4_max: 318
    cleaning_beam_agent-4_mean: 98.71
    cleaning_beam_agent-4_min: 27
    cleaning_beam_agent-5_max: 57
    cleaning_beam_agent-5_mean: 15.0
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-50-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 63.0
  episode_reward_mean: 17.16
  episode_reward_min: -40.0
  episodes_this_iter: 96
  episodes_total: 16800
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11852.926
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021767040016129613
        entropy: 0.622495174407959
        entropy_coeff: 0.0017600000137463212
        kl: 0.009636041708290577
        model: {}
        policy_loss: -0.010397395119071007
        total_loss: -0.01023933757096529
        vf_explained_var: -0.003330409526824951
        vf_loss: 2.9004499912261963
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021767040016129613
        entropy: 0.5441805124282837
        entropy_coeff: 0.0017600000137463212
        kl: 0.006770903244614601
        model: {}
        policy_loss: -0.01453493908047676
        total_loss: -0.014802439138293266
        vf_explained_var: 0.056261613965034485
        vf_loss: 0.13166916370391846
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021767040016129613
        entropy: 0.5313400626182556
        entropy_coeff: 0.0017600000137463212
        kl: 0.008369624614715576
        model: {}
        policy_loss: -0.01762574538588524
        total_loss: -0.01767592504620552
        vf_explained_var: 0.021547093987464905
        vf_loss: 0.48020777106285095
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021767040016129613
        entropy: 0.6642001271247864
        entropy_coeff: 0.0017600000137463212
        kl: 0.00734756700694561
        model: {}
        policy_loss: -0.013166462071239948
        total_loss: -0.013551738113164902
        vf_explained_var: 0.0003626197576522827
        vf_loss: 0.48955708742141724
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021767040016129613
        entropy: 0.689569890499115
        entropy_coeff: 0.0017600000137463212
        kl: 0.008888901211321354
        model: {}
        policy_loss: -0.019298894330859184
        total_loss: -0.019582081586122513
        vf_explained_var: 0.023250356316566467
        vf_loss: 0.41566768288612366
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021767040016129613
        entropy: 0.7712120413780212
        entropy_coeff: 0.0017600000137463212
        kl: 0.006515454966574907
        model: {}
        policy_loss: -0.01109287142753601
        total_loss: -0.011786138638854027
        vf_explained_var: 0.023959368467330933
        vf_loss: 0.12520959973335266
    load_time_ms: 14488.669
    num_steps_sampled: 16800000
    num_steps_trained: 16800000
    sample_time_ms: 99886.744
    update_time_ms: 16.685
  iterations_since_restore: 15
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 46.238674033149174
    ram_util_percent: 18.595580110497238
  pid: 13408
  policy_reward_max:
    agent-0: 12.0
    agent-1: 8.0
    agent-2: 16.0
    agent-3: 27.0
    agent-4: 12.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 2.01
    agent-1: 1.72
    agent-2: 4.48
    agent-3: 3.92
    agent-4: 4.15
    agent-5: 0.88
  policy_reward_min:
    agent-0: -47.0
    agent-1: -1.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -49.0
  sampler_perf:
    mean_env_wait_ms: 23.382525137529132
    mean_inference_ms: 13.030576074168803
    mean_processing_ms: 58.37615581498295
  time_since_restore: 1905.107271194458
  time_this_iter_s: 126.61883163452148
  time_total_s: 24415.390513658524
  timestamp: 1637221857
  timesteps_since_restore: 1440000
  timesteps_this_iter: 96000
  timesteps_total: 16800000
  training_iteration: 175
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 33.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    175 |          24415.4 | 16800000 |    17.16 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 1.92
    apples_agent-0_min: 0
    apples_agent-1_max: 23
    apples_agent-1_mean: 1.24
    apples_agent-1_min: 0
    apples_agent-2_max: 12
    apples_agent-2_mean: 1.59
    apples_agent-2_min: 0
    apples_agent-3_max: 20
    apples_agent-3_mean: 2.9
    apples_agent-3_min: 0
    apples_agent-4_max: 94
    apples_agent-4_mean: 2.2
    apples_agent-4_min: 0
    apples_agent-5_max: 5
    apples_agent-5_mean: 0.75
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 163
    cleaning_beam_agent-0_mean: 84.51
    cleaning_beam_agent-0_min: 18
    cleaning_beam_agent-1_max: 413
    cleaning_beam_agent-1_mean: 258.74
    cleaning_beam_agent-1_min: 155
    cleaning_beam_agent-2_max: 114
    cleaning_beam_agent-2_mean: 29.6
    cleaning_beam_agent-2_min: 6
    cleaning_beam_agent-3_max: 191
    cleaning_beam_agent-3_mean: 91.71
    cleaning_beam_agent-3_min: 31
    cleaning_beam_agent-4_max: 225
    cleaning_beam_agent-4_mean: 95.33
    cleaning_beam_agent-4_min: 23
    cleaning_beam_agent-5_max: 55
    cleaning_beam_agent-5_mean: 13.56
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 19
    fire_beam_agent-5_mean: 0.19
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-53-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 63.0
  episode_reward_mean: -3.65
  episode_reward_min: -2089.0
  episodes_this_iter: 96
  episodes_total: 16896
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11904.835
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021168000239413232
        entropy: 0.6058878898620605
        entropy_coeff: 0.0017600000137463212
        kl: 0.00742918998003006
        model: {}
        policy_loss: -0.007772197015583515
        total_loss: 0.030681749805808067
        vf_explained_var: 0.0023454129695892334
        vf_loss: 387.7738952636719
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021168000239413232
        entropy: 0.5311909914016724
        entropy_coeff: 0.0017600000137463212
        kl: 0.006797379814088345
        model: {}
        policy_loss: -0.013869458809494972
        total_loss: -0.01410696841776371
        vf_explained_var: 0.009724125266075134
        vf_loss: 0.17647656798362732
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021168000239413232
        entropy: 0.5328079462051392
        entropy_coeff: 0.0017600000137463212
        kl: 0.0066088964231312275
        model: {}
        policy_loss: -0.007488820236176252
        total_loss: 0.00477756280452013
        vf_explained_var: -0.0038210302591323853
        vf_loss: 125.43231964111328
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021168000239413232
        entropy: 0.6491169333457947
        entropy_coeff: 0.0017600000137463212
        kl: 0.0073705147951841354
        model: {}
        policy_loss: -0.014499661512672901
        total_loss: -0.01486873347312212
        vf_explained_var: -0.012915492057800293
        vf_loss: 0.363233745098114
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021168000239413232
        entropy: 0.6944679021835327
        entropy_coeff: 0.0017600000137463212
        kl: 0.00575614720582962
        model: {}
        policy_loss: -0.006121004931628704
        total_loss: 0.017297377809882164
        vf_explained_var: 0.004149302840232849
        vf_loss: 240.65029907226562
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021168000239413232
        entropy: 0.7693278789520264
        entropy_coeff: 0.0017600000137463212
        kl: 0.007927123457193375
        model: {}
        policy_loss: -0.012732518836855888
        total_loss: -0.01326795294880867
        vf_explained_var: 0.014281690120697021
        vf_loss: 0.25870704650878906
    load_time_ms: 14529.231
    num_steps_sampled: 16896000
    num_steps_trained: 16896000
    sample_time_ms: 100090.205
    update_time_ms: 16.65
  iterations_since_restore: 16
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.81787709497206
    ram_util_percent: 18.407262569832405
  pid: 13408
  policy_reward_max:
    agent-0: 19.0
    agent-1: 9.0
    agent-2: 17.0
    agent-3: 14.0
    agent-4: 14.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: -6.48
    agent-1: 1.91
    agent-2: -1.13
    agent-3: 3.84
    agent-4: -2.83
    agent-5: 1.04
  policy_reward_min:
    agent-0: -895.0
    agent-1: 0.0
    agent-2: -494.0
    agent-3: 0.0
    agent-4: -693.0
    agent-5: -17.0
  sampler_perf:
    mean_env_wait_ms: 23.37712904419688
    mean_inference_ms: 13.043022371960333
    mean_processing_ms: 58.414142171564755
  time_since_restore: 2030.739223241806
  time_this_iter_s: 125.63195204734802
  time_total_s: 24541.02246570587
  timestamp: 1637221983
  timesteps_since_restore: 1536000
  timesteps_this_iter: 96000
  timesteps_total: 16896000
  training_iteration: 176
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    176 |            24541 | 16896000 |    -3.65 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 2.33
    apples_agent-0_min: 0
    apples_agent-1_max: 28
    apples_agent-1_mean: 1.16
    apples_agent-1_min: 0
    apples_agent-2_max: 9
    apples_agent-2_mean: 1.47
    apples_agent-2_min: 0
    apples_agent-3_max: 18
    apples_agent-3_mean: 2.34
    apples_agent-3_min: 0
    apples_agent-4_max: 68
    apples_agent-4_mean: 1.67
    apples_agent-4_min: 0
    apples_agent-5_max: 6
    apples_agent-5_mean: 0.87
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 160
    cleaning_beam_agent-0_mean: 77.41
    cleaning_beam_agent-0_min: 14
    cleaning_beam_agent-1_max: 440
    cleaning_beam_agent-1_mean: 276.84
    cleaning_beam_agent-1_min: 171
    cleaning_beam_agent-2_max: 105
    cleaning_beam_agent-2_mean: 26.49
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 262
    cleaning_beam_agent-3_mean: 107.72
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 432
    cleaning_beam_agent-4_mean: 98.78
    cleaning_beam_agent-4_min: 31
    cleaning_beam_agent-5_max: 66
    cleaning_beam_agent-5_mean: 14.18
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-55-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 60.0
  episode_reward_mean: 17.54
  episode_reward_min: 0.0
  episodes_this_iter: 96
  episodes_total: 16992
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11954.483
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002056896046269685
        entropy: 0.6220169067382812
        entropy_coeff: 0.0017600000137463212
        kl: 0.00939619354903698
        model: {}
        policy_loss: -0.020673029124736786
        total_loss: -0.02079123631119728
        vf_explained_var: -0.11183890700340271
        vf_loss: 0.3692684769630432
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002056896046269685
        entropy: 0.5380399823188782
        entropy_coeff: 0.0017600000137463212
        kl: 0.006856513675302267
        model: {}
        policy_loss: -0.014403510838747025
        total_loss: -0.014649514108896255
        vf_explained_var: 0.016315504908561707
        vf_loss: 0.15294551849365234
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002056896046269685
        entropy: 0.5239243507385254
        entropy_coeff: 0.0017600000137463212
        kl: 0.007239673286676407
        model: {}
        policy_loss: -0.01709687151014805
        total_loss: -0.017248449847102165
        vf_explained_var: -0.0681597888469696
        vf_loss: 0.46562373638153076
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002056896046269685
        entropy: 0.6386407017707825
        entropy_coeff: 0.0017600000137463212
        kl: 0.006482264958322048
        model: {}
        policy_loss: -0.013034258037805557
        total_loss: -0.013481165282428265
        vf_explained_var: 0.007801711559295654
        vf_loss: 0.2887495756149292
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002056896046269685
        entropy: 0.6882861256599426
        entropy_coeff: 0.0017600000137463212
        kl: 0.008492697030305862
        model: {}
        policy_loss: -0.020682498812675476
        total_loss: -0.021002575755119324
        vf_explained_var: -0.15693655610084534
        vf_loss: 0.42033281922340393
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002056896046269685
        entropy: 0.7681770324707031
        entropy_coeff: 0.0017600000137463212
        kl: 0.00727152731269598
        model: {}
        policy_loss: -0.011793541721999645
        total_loss: -0.01240667887032032
        vf_explained_var: -1.932680606842041e-05
        vf_loss: 0.1170167624950409
    load_time_ms: 14602.504
    num_steps_sampled: 16992000
    num_steps_trained: 16992000
    sample_time_ms: 100276.203
    update_time_ms: 16.316
  iterations_since_restore: 17
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.05722222222223
    ram_util_percent: 18.613888888888887
  pid: 13408
  policy_reward_max:
    agent-0: 13.0
    agent-1: 9.0
    agent-2: 19.0
    agent-3: 15.0
    agent-4: 12.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 3.38
    agent-1: 1.89
    agent-2: 3.93
    agent-3: 3.07
    agent-4: 3.92
    agent-5: 1.35
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.382399106388974
    mean_inference_ms: 13.050759489389968
    mean_processing_ms: 58.433169862815646
  time_since_restore: 2156.9514923095703
  time_this_iter_s: 126.21226906776428
  time_total_s: 24667.234734773636
  timestamp: 1637222109
  timesteps_since_restore: 1632000
  timesteps_this_iter: 96000
  timesteps_total: 16992000
  training_iteration: 177
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    177 |          24667.2 | 16992000 |    17.54 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 1.91
    apples_agent-0_min: 0
    apples_agent-1_max: 11
    apples_agent-1_mean: 0.96
    apples_agent-1_min: 0
    apples_agent-2_max: 81
    apples_agent-2_mean: 3.01
    apples_agent-2_min: 0
    apples_agent-3_max: 30
    apples_agent-3_mean: 3.05
    apples_agent-3_min: 0
    apples_agent-4_max: 10
    apples_agent-4_mean: 1.17
    apples_agent-4_min: 0
    apples_agent-5_max: 10
    apples_agent-5_mean: 0.98
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 229
    cleaning_beam_agent-0_mean: 86.03
    cleaning_beam_agent-0_min: 31
    cleaning_beam_agent-1_max: 456
    cleaning_beam_agent-1_mean: 281.47
    cleaning_beam_agent-1_min: 171
    cleaning_beam_agent-2_max: 104
    cleaning_beam_agent-2_mean: 28.43
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 275
    cleaning_beam_agent-3_mean: 109.35
    cleaning_beam_agent-3_min: 39
    cleaning_beam_agent-4_max: 432
    cleaning_beam_agent-4_mean: 104.09
    cleaning_beam_agent-4_min: 17
    cleaning_beam_agent-5_max: 57
    cleaning_beam_agent-5_mean: 12.92
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-57-17
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 79.0
  episode_reward_mean: 19.49
  episode_reward_min: 0.0
  episodes_this_iter: 96
  episodes_total: 17088
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 12024.391
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001996992068598047
        entropy: 0.6301990151405334
        entropy_coeff: 0.0017600000137463212
        kl: 0.008949771523475647
        model: {}
        policy_loss: -0.017872856929898262
        total_loss: -0.018047109246253967
        vf_explained_var: -0.044726401567459106
        vf_loss: 0.39917564392089844
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001996992068598047
        entropy: 0.5342600345611572
        entropy_coeff: 0.0017600000137463212
        kl: 0.00640065735206008
        model: {}
        policy_loss: -0.012904997915029526
        total_loss: -0.013188038021326065
        vf_explained_var: 0.030708476901054382
        vf_loss: 0.1718948781490326
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001996992068598047
        entropy: 0.5233906507492065
        entropy_coeff: 0.0017600000137463212
        kl: 0.007848299108445644
        model: {}
        policy_loss: -0.01786324568092823
        total_loss: -0.017939381301403046
        vf_explained_var: -0.05801597237586975
        vf_loss: 0.6019915342330933
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001996992068598047
        entropy: 0.6301611065864563
        entropy_coeff: 0.0017600000137463212
        kl: 0.006348851602524519
        model: {}
        policy_loss: -0.011644403450191021
        total_loss: -0.012063344940543175
        vf_explained_var: -0.0009325146675109863
        vf_loss: 0.5525753498077393
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001996992068598047
        entropy: 0.6853089332580566
        entropy_coeff: 0.0017600000137463212
        kl: 0.008683224208652973
        model: {}
        policy_loss: -0.018207376822829247
        total_loss: -0.018491435796022415
        vf_explained_var: -0.06890568137168884
        vf_loss: 0.5376525521278381
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001996992068598047
        entropy: 0.7828859090805054
        entropy_coeff: 0.0017600000137463212
        kl: 0.006789778359234333
        model: {}
        policy_loss: -0.012604737654328346
        total_loss: -0.013293758034706116
        vf_explained_var: 0.018295004963874817
        vf_loss: 0.09881418198347092
    load_time_ms: 14653.713
    num_steps_sampled: 17088000
    num_steps_trained: 17088000
    sample_time_ms: 100655.452
    update_time_ms: 16.509
  iterations_since_restore: 18
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 48.13756906077348
    ram_util_percent: 18.393370165745857
  pid: 13408
  policy_reward_max:
    agent-0: 19.0
    agent-1: 15.0
    agent-2: 20.0
    agent-3: 25.0
    agent-4: 23.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 3.33
    agent-1: 1.85
    agent-2: 4.76
    agent-3: 3.92
    agent-4: 4.31
    agent-5: 1.32
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.399575636836143
    mean_inference_ms: 13.060096733777232
    mean_processing_ms: 58.48167962985673
  time_since_restore: 2284.389251232147
  time_this_iter_s: 127.4377589225769
  time_total_s: 24794.672493696213
  timestamp: 1637222237
  timesteps_since_restore: 1728000
  timesteps_this_iter: 96000
  timesteps_total: 17088000
  training_iteration: 178
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 33.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    178 |          24794.7 | 17088000 |    19.49 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 8
    apples_agent-0_mean: 1.74
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.67
    apples_agent-1_min: 0
    apples_agent-2_max: 32
    apples_agent-2_mean: 2.1
    apples_agent-2_min: 0
    apples_agent-3_max: 15
    apples_agent-3_mean: 2.73
    apples_agent-3_min: 0
    apples_agent-4_max: 15
    apples_agent-4_mean: 1.15
    apples_agent-4_min: 0
    apples_agent-5_max: 18
    apples_agent-5_mean: 1.27
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 214
    cleaning_beam_agent-0_mean: 89.48
    cleaning_beam_agent-0_min: 36
    cleaning_beam_agent-1_max: 544
    cleaning_beam_agent-1_mean: 288.45
    cleaning_beam_agent-1_min: 148
    cleaning_beam_agent-2_max: 126
    cleaning_beam_agent-2_mean: 31.24
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 198
    cleaning_beam_agent-3_mean: 108.69
    cleaning_beam_agent-3_min: 24
    cleaning_beam_agent-4_max: 244
    cleaning_beam_agent-4_mean: 99.54
    cleaning_beam_agent-4_min: 18
    cleaning_beam_agent-5_max: 69
    cleaning_beam_agent-5_mean: 11.39
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 4
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-59-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 60.0
  episode_reward_mean: 16.66
  episode_reward_min: -78.0
  episodes_this_iter: 96
  episodes_total: 17184
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 12068.893
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00019370879454072565
        entropy: 0.6215658187866211
        entropy_coeff: 0.0017600000137463212
        kl: 0.006884764414280653
        model: {}
        policy_loss: -0.010605529882013798
        total_loss: -0.010895425453782082
        vf_explained_var: -0.03750982880592346
        vf_loss: 1.1558430194854736
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00019370879454072565
        entropy: 0.5317426323890686
        entropy_coeff: 0.0017600000137463212
        kl: 0.005980798043310642
        model: {}
        policy_loss: -0.007974246516823769
        total_loss: -0.008171454071998596
        vf_explained_var: 0.007522165775299072
        vf_loss: 1.4058090448379517
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00019370879454072565
        entropy: 0.5447260141372681
        entropy_coeff: 0.0017600000137463212
        kl: 0.005945868790149689
        model: {}
        policy_loss: -0.012496680952608585
        total_loss: -0.01271513570100069
        vf_explained_var: -0.015029221773147583
        vf_loss: 1.456749677658081
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00019370879454072565
        entropy: 0.6397020220756531
        entropy_coeff: 0.0017600000137463212
        kl: 0.005528928712010384
        model: {}
        policy_loss: -0.007753162644803524
        total_loss: -0.00790113303810358
        vf_explained_var: 0.003552451729774475
        vf_loss: 4.250107288360596
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00019370879454072565
        entropy: 0.6787824630737305
        entropy_coeff: 0.0017600000137463212
        kl: 0.008979237638413906
        model: {}
        policy_loss: -0.019837485626339912
        total_loss: -0.02009277045726776
        vf_explained_var: -0.07695263624191284
        vf_loss: 0.41448548436164856
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00019370879454072565
        entropy: 0.7725462317466736
        entropy_coeff: 0.0017600000137463212
        kl: 0.00739092705771327
        model: {}
        policy_loss: -0.011845799162983894
        total_loss: -0.012451376765966415
        vf_explained_var: 0.014004409313201904
        vf_loss: 0.1501164734363556
    load_time_ms: 14707.863
    num_steps_sampled: 17184000
    num_steps_trained: 17184000
    sample_time_ms: 100838.006
    update_time_ms: 16.09
  iterations_since_restore: 19
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.31666666666667
    ram_util_percent: 18.502222222222226
  pid: 13408
  policy_reward_max:
    agent-0: 12.0
    agent-1: 14.0
    agent-2: 18.0
    agent-3: 15.0
    agent-4: 14.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 2.56
    agent-1: 1.28
    agent-2: 4.67
    agent-3: 2.36
    agent-4: 4.07
    agent-5: 1.72
  policy_reward_min:
    agent-0: -45.0
    agent-1: -46.0
    agent-2: -46.0
    agent-3: -96.0
    agent-4: 0.0
    agent-5: -2.0
  sampler_perf:
    mean_env_wait_ms: 23.40965310517985
    mean_inference_ms: 13.069705314073598
    mean_processing_ms: 58.518033716393944
  time_since_restore: 2410.59597158432
  time_this_iter_s: 126.20672035217285
  time_total_s: 24920.879214048386
  timestamp: 1637222363
  timesteps_since_restore: 1824000
  timesteps_this_iter: 96000
  timesteps_total: 17184000
  training_iteration: 179
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    179 |          24920.9 | 17184000 |    16.66 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 2.44
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 0.89
    apples_agent-1_min: 0
    apples_agent-2_max: 44
    apples_agent-2_mean: 2.29
    apples_agent-2_min: 0
    apples_agent-3_max: 21
    apples_agent-3_mean: 2.69
    apples_agent-3_min: 0
    apples_agent-4_max: 31
    apples_agent-4_mean: 1.42
    apples_agent-4_min: 0
    apples_agent-5_max: 13
    apples_agent-5_mean: 1.11
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 287
    cleaning_beam_agent-0_mean: 91.84
    cleaning_beam_agent-0_min: 32
    cleaning_beam_agent-1_max: 462
    cleaning_beam_agent-1_mean: 296.92
    cleaning_beam_agent-1_min: 58
    cleaning_beam_agent-2_max: 90
    cleaning_beam_agent-2_mean: 31.46
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 224
    cleaning_beam_agent-3_mean: 106.52
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 395
    cleaning_beam_agent-4_mean: 97.79
    cleaning_beam_agent-4_min: 24
    cleaning_beam_agent-5_max: 63
    cleaning_beam_agent-5_mean: 11.51
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-01-30
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 67.0
  episode_reward_mean: 18.72
  episode_reward_min: -74.0
  episodes_this_iter: 96
  episodes_total: 17280
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 12064.487
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00018771839677356184
        entropy: 0.6207002401351929
        entropy_coeff: 0.0017600000137463212
        kl: 0.008163497783243656
        model: {}
        policy_loss: -0.018143469467759132
        total_loss: -0.01838422380387783
        vf_explained_var: -0.04838085174560547
        vf_loss: 0.3532814681529999
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00018771839677356184
        entropy: 0.5355579853057861
        entropy_coeff: 0.0017600000137463212
        kl: 0.003908771555870771
        model: {}
        policy_loss: -0.0055543603375554085
        total_loss: -0.005938023794442415
        vf_explained_var: 0.006886601448059082
        vf_loss: 1.6804357767105103
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00018771839677356184
        entropy: 0.5199364423751831
        entropy_coeff: 0.0017600000137463212
        kl: 0.007619685027748346
        model: {}
        policy_loss: -0.016654659062623978
        total_loss: -0.01675780490040779
        vf_explained_var: -0.012881934642791748
        vf_loss: 0.49971142411231995
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00018771839677356184
        entropy: 0.6326659917831421
        entropy_coeff: 0.0017600000137463212
        kl: 0.005972209386527538
        model: {}
        policy_loss: -0.012134859338402748
        total_loss: -0.012584274634718895
        vf_explained_var: 0.0023291558027267456
        vf_loss: 0.6685556769371033
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00018771839677356184
        entropy: 0.6792380809783936
        entropy_coeff: 0.0017600000137463212
        kl: 0.008566608652472496
        model: {}
        policy_loss: -0.01966387778520584
        total_loss: -0.019959531724452972
        vf_explained_var: -0.032014399766922
        vf_loss: 0.43149620294570923
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00018771839677356184
        entropy: 0.7844506502151489
        entropy_coeff: 0.0017600000137463212
        kl: 0.006766784004867077
        model: {}
        policy_loss: -0.007200625725090504
        total_loss: -0.007767880335450172
        vf_explained_var: 0.0070212483406066895
        vf_loss: 1.366992712020874
    load_time_ms: 13973.117
    num_steps_sampled: 17280000
    num_steps_trained: 17280000
    sample_time_ms: 100942.093
    update_time_ms: 15.937
  iterations_since_restore: 20
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 48.31878453038674
    ram_util_percent: 18.7
  pid: 13408
  policy_reward_max:
    agent-0: 15.0
    agent-1: 13.0
    agent-2: 21.0
    agent-3: 32.0
    agent-4: 14.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 3.39
    agent-1: 1.62
    agent-2: 4.27
    agent-3: 3.96
    agent-4: 4.51
    agent-5: 0.97
  policy_reward_min:
    agent-0: 0.0
    agent-1: -49.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -47.0
  sampler_perf:
    mean_env_wait_ms: 23.432529990730607
    mean_inference_ms: 13.081495071152496
    mean_processing_ms: 58.564678064394464
  time_since_restore: 2537.4792098999023
  time_this_iter_s: 126.88323831558228
  time_total_s: 25047.762452363968
  timestamp: 1637222490
  timesteps_since_restore: 1920000
  timesteps_this_iter: 96000
  timesteps_total: 17280000
  training_iteration: 180
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 33.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    180 |          25047.8 | 17280000 |    18.72 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 2.3
    apples_agent-0_min: 0
    apples_agent-1_max: 9
    apples_agent-1_mean: 1.01
    apples_agent-1_min: 0
    apples_agent-2_max: 16
    apples_agent-2_mean: 2.08
    apples_agent-2_min: 0
    apples_agent-3_max: 18
    apples_agent-3_mean: 2.35
    apples_agent-3_min: 0
    apples_agent-4_max: 16
    apples_agent-4_mean: 1.06
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 1.07
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 176
    cleaning_beam_agent-0_mean: 95.92
    cleaning_beam_agent-0_min: 31
    cleaning_beam_agent-1_max: 444
    cleaning_beam_agent-1_mean: 289.21
    cleaning_beam_agent-1_min: 170
    cleaning_beam_agent-2_max: 80
    cleaning_beam_agent-2_mean: 28.4
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 205
    cleaning_beam_agent-3_mean: 116.12
    cleaning_beam_agent-3_min: 39
    cleaning_beam_agent-4_max: 278
    cleaning_beam_agent-4_mean: 98.34
    cleaning_beam_agent-4_min: 25
    cleaning_beam_agent-5_max: 43
    cleaning_beam_agent-5_mean: 8.69
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-03-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 65.0
  episode_reward_mean: 16.1
  episode_reward_min: -88.0
  episodes_this_iter: 96
  episodes_total: 17376
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 12081.521
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00018172799900639802
        entropy: 0.6282773017883301
        entropy_coeff: 0.0017600000137463212
        kl: 0.005976767744868994
        model: {}
        policy_loss: -0.007314709015190601
        total_loss: -0.007663516327738762
        vf_explained_var: -0.006867572665214539
        vf_loss: 1.5928313732147217
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00018172799900639802
        entropy: 0.5455695390701294
        entropy_coeff: 0.0017600000137463212
        kl: 0.006623942870646715
        model: {}
        policy_loss: -0.00685445312410593
        total_loss: -0.007198620587587357
        vf_explained_var: 0.0038363635540008545
        vf_loss: 2.848361015319824
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00018172799900639802
        entropy: 0.5230976939201355
        entropy_coeff: 0.0017600000137463212
        kl: 0.005166827235370874
        model: {}
        policy_loss: -0.008508422411978245
        total_loss: -0.008714092895388603
        vf_explained_var: -0.005932033061981201
        vf_loss: 1.9829434156417847
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00018172799900639802
        entropy: 0.639326810836792
        entropy_coeff: 0.0017600000137463212
        kl: 0.0065450528636574745
        model: {}
        policy_loss: -0.013273647055029869
        total_loss: -0.013710870407521725
        vf_explained_var: -0.00022482872009277344
        vf_loss: 0.33489835262298584
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00018172799900639802
        entropy: 0.6853609085083008
        entropy_coeff: 0.0017600000137463212
        kl: 0.008468879386782646
        model: {}
        policy_loss: -0.01888582482933998
        total_loss: -0.01920635625720024
        vf_explained_var: -0.02644917368888855
        vf_loss: 0.38815557956695557
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00018172799900639802
        entropy: 0.7623631358146667
        entropy_coeff: 0.0017600000137463212
        kl: 0.006752305198460817
        model: {}
        policy_loss: -0.010970658622682095
        total_loss: -0.011616354808211327
        vf_explained_var: 0.015204697847366333
        vf_loss: 0.20831041038036346
    load_time_ms: 13926.343
    num_steps_sampled: 17376000
    num_steps_trained: 17376000
    sample_time_ms: 100833.394
    update_time_ms: 15.918
  iterations_since_restore: 21
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 48.43626373626373
    ram_util_percent: 18.46648351648352
  pid: 13408
  policy_reward_max:
    agent-0: 14.0
    agent-1: 15.0
    agent-2: 21.0
    agent-3: 15.0
    agent-4: 19.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 2.64
    agent-1: 1.06
    agent-2: 3.84
    agent-3: 3.36
    agent-4: 3.95
    agent-5: 1.25
  policy_reward_min:
    agent-0: -47.0
    agent-1: -48.0
    agent-2: -50.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -46.0
  sampler_perf:
    mean_env_wait_ms: 23.444826591368965
    mean_inference_ms: 13.08729015083442
    mean_processing_ms: 58.5950257406406
  time_since_restore: 2664.465230703354
  time_this_iter_s: 126.98602080345154
  time_total_s: 25174.74847316742
  timestamp: 1637222618
  timesteps_since_restore: 2016000
  timesteps_this_iter: 96000
  timesteps_total: 17376000
  training_iteration: 181
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    181 |          25174.7 | 17376000 |     16.1 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 2.12
    apples_agent-0_min: 0
    apples_agent-1_max: 11
    apples_agent-1_mean: 0.91
    apples_agent-1_min: 0
    apples_agent-2_max: 11
    apples_agent-2_mean: 1.92
    apples_agent-2_min: 0
    apples_agent-3_max: 8
    apples_agent-3_mean: 2.19
    apples_agent-3_min: 0
    apples_agent-4_max: 9
    apples_agent-4_mean: 0.98
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 1.1
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 211
    cleaning_beam_agent-0_mean: 86.54
    cleaning_beam_agent-0_min: 29
    cleaning_beam_agent-1_max: 468
    cleaning_beam_agent-1_mean: 295.27
    cleaning_beam_agent-1_min: 189
    cleaning_beam_agent-2_max: 119
    cleaning_beam_agent-2_mean: 32.19
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 209
    cleaning_beam_agent-3_mean: 114.1
    cleaning_beam_agent-3_min: 30
    cleaning_beam_agent-4_max: 222
    cleaning_beam_agent-4_mean: 90.37
    cleaning_beam_agent-4_min: 18
    cleaning_beam_agent-5_max: 33
    cleaning_beam_agent-5_mean: 8.37
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 8
    fire_beam_agent-5_mean: 0.16
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-05-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 49.0
  episode_reward_mean: 11.59
  episode_reward_min: -329.0
  episodes_this_iter: 96
  episodes_total: 17472
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 12077.416
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001757376012392342
        entropy: 0.620830237865448
        entropy_coeff: 0.0017600000137463212
        kl: 0.007936946116387844
        model: {}
        policy_loss: -0.017318278551101685
        total_loss: -0.01758703775703907
        vf_explained_var: -0.045227378606796265
        vf_loss: 0.3020477294921875
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0001757376012392342
        entropy: 0.5461941957473755
        entropy_coeff: 0.0017600000137463212
        kl: 0.007045540027320385
        model: {}
        policy_loss: -0.013648182153701782
        total_loss: -0.0142448665574193
        vf_explained_var: 0.03283478319644928
        vf_loss: 0.12338758260011673
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001757376012392342
        entropy: 0.5364301204681396
        entropy_coeff: 0.0017600000137463212
        kl: 0.006492462940514088
        model: {}
        policy_loss: -0.01349145732820034
        total_loss: -0.013710374012589455
        vf_explained_var: -0.012362778186798096
        vf_loss: 0.7595140933990479
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001757376012392342
        entropy: 0.6480059623718262
        entropy_coeff: 0.0017600000137463212
        kl: 0.003392565995454788
        model: {}
        policy_loss: -0.003475758945569396
        total_loss: -0.002322888933122158
        vf_explained_var: -0.007721409201622009
        vf_loss: 19.541051864624023
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001757376012392342
        entropy: 0.6829488277435303
        entropy_coeff: 0.0017600000137463212
        kl: 0.002570792566984892
        model: {}
        policy_loss: -0.00361886085011065
        total_loss: -0.0031706825830042362
        vf_explained_var: -0.016285717487335205
        vf_loss: 13.930883407592773
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001757376012392342
        entropy: 0.7981363534927368
        entropy_coeff: 0.0017600000137463212
        kl: 0.006485768128186464
        model: {}
        policy_loss: -0.012781264260411263
        total_loss: -0.013523126021027565
        vf_explained_var: 0.0033056437969207764
        vf_loss: 0.14284399151802063
    load_time_ms: 13901.942
    num_steps_sampled: 17472000
    num_steps_trained: 17472000
    sample_time_ms: 100793.751
    update_time_ms: 16.169
  iterations_since_restore: 22
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.26428571428572
    ram_util_percent: 18.72087912087912
  pid: 13408
  policy_reward_max:
    agent-0: 15.0
    agent-1: 7.0
    agent-2: 16.0
    agent-3: 10.0
    agent-4: 14.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 2.45
    agent-1: 1.65
    agent-2: 4.06
    agent-3: 1.11
    agent-4: 0.92
    agent-5: 1.4
  policy_reward_min:
    agent-0: -44.0
    agent-1: 0.0
    agent-2: -46.0
    agent-3: -248.0
    agent-4: -291.0
    agent-5: -6.0
  sampler_perf:
    mean_env_wait_ms: 23.460346353407516
    mean_inference_ms: 13.094654011760401
    mean_processing_ms: 58.63095583538691
  time_since_restore: 2792.3011677265167
  time_this_iter_s: 127.83593702316284
  time_total_s: 25302.584410190582
  timestamp: 1637222746
  timesteps_since_restore: 2112000
  timesteps_this_iter: 96000
  timesteps_total: 17472000
  training_iteration: 182
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    182 |          25302.6 | 17472000 |    11.59 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 2.13
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.74
    apples_agent-1_min: 0
    apples_agent-2_max: 23
    apples_agent-2_mean: 2.33
    apples_agent-2_min: 0
    apples_agent-3_max: 13
    apples_agent-3_mean: 2.63
    apples_agent-3_min: 0
    apples_agent-4_max: 6
    apples_agent-4_mean: 1.06
    apples_agent-4_min: 0
    apples_agent-5_max: 9
    apples_agent-5_mean: 1.0
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 211
    cleaning_beam_agent-0_mean: 89.49
    cleaning_beam_agent-0_min: 36
    cleaning_beam_agent-1_max: 486
    cleaning_beam_agent-1_mean: 291.99
    cleaning_beam_agent-1_min: 164
    cleaning_beam_agent-2_max: 100
    cleaning_beam_agent-2_mean: 27.42
    cleaning_beam_agent-2_min: 6
    cleaning_beam_agent-3_max: 266
    cleaning_beam_agent-3_mean: 124.57
    cleaning_beam_agent-3_min: 50
    cleaning_beam_agent-4_max: 251
    cleaning_beam_agent-4_mean: 87.12
    cleaning_beam_agent-4_min: 22
    cleaning_beam_agent-5_max: 58
    cleaning_beam_agent-5_mean: 9.13
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-07-52
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 53.0
  episode_reward_mean: 16.34
  episode_reward_min: -80.0
  episodes_this_iter: 96
  episodes_total: 17568
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 12071.939
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001697472034720704
        entropy: 0.6236535310745239
        entropy_coeff: 0.0017600000137463212
        kl: 0.0076983291655778885
        model: {}
        policy_loss: -0.011554239317774773
        total_loss: -0.011604443192481995
        vf_explained_var: -0.0026920735836029053
        vf_loss: 2.7759499549865723
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0001697472034720704
        entropy: 0.5437647104263306
        entropy_coeff: 0.0017600000137463212
        kl: 0.007081273943185806
        model: {}
        policy_loss: -0.013041228987276554
        total_loss: -0.013633632101118565
        vf_explained_var: 0.033616334199905396
        vf_loss: 0.10553847998380661
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001697472034720704
        entropy: 0.5237247943878174
        entropy_coeff: 0.0017600000137463212
        kl: 0.0040993457660079
        model: {}
        policy_loss: -0.006267030723392963
        total_loss: -0.006595852319151163
        vf_explained_var: 0.005180627107620239
        vf_loss: 1.830028772354126
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0001697472034720704
        entropy: 0.6531237959861755
        entropy_coeff: 0.0017600000137463212
        kl: 0.007561486214399338
        model: {}
        policy_loss: -0.014366873539984226
        total_loss: -0.01510624773800373
        vf_explained_var: -0.002152279019355774
        vf_loss: 0.3204779028892517
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0001697472034720704
        entropy: 0.670403778553009
        entropy_coeff: 0.0017600000137463212
        kl: 0.008287757635116577
        model: {}
        policy_loss: -0.01750675030052662
        total_loss: -0.018228499218821526
        vf_explained_var: -0.010023489594459534
        vf_loss: 0.4377424120903015
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001697472034720704
        entropy: 0.7729240655899048
        entropy_coeff: 0.0017600000137463212
        kl: 0.008578496053814888
        model: {}
        policy_loss: -0.007570239249616861
        total_loss: -0.00793011300265789
        vf_explained_var: 0.013698622584342957
        vf_loss: 1.4262371063232422
    load_time_ms: 13898.62
    num_steps_sampled: 17568000
    num_steps_trained: 17568000
    sample_time_ms: 100664.171
    update_time_ms: 16.21
  iterations_since_restore: 23
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 46.11777777777778
    ram_util_percent: 18.49888888888889
  pid: 13408
  policy_reward_max:
    agent-0: 11.0
    agent-1: 13.0
    agent-2: 18.0
    agent-3: 14.0
    agent-4: 14.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 1.99
    agent-1: 1.44
    agent-2: 3.87
    agent-3: 3.79
    agent-4: 4.37
    agent-5: 0.88
  policy_reward_min:
    agent-0: -47.0
    agent-1: -1.0
    agent-2: -44.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -49.0
  sampler_perf:
    mean_env_wait_ms: 23.467764571686853
    mean_inference_ms: 13.098954514029781
    mean_processing_ms: 58.65255456813795
  time_since_restore: 2918.5501792430878
  time_this_iter_s: 126.24901151657104
  time_total_s: 25428.833421707153
  timestamp: 1637222872
  timesteps_since_restore: 2208000
  timesteps_this_iter: 96000
  timesteps_total: 17568000
  training_iteration: 183
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    183 |          25428.8 | 17568000 |    16.34 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 1.86
    apples_agent-0_min: 0
    apples_agent-1_max: 23
    apples_agent-1_mean: 1.04
    apples_agent-1_min: 0
    apples_agent-2_max: 14
    apples_agent-2_mean: 1.64
    apples_agent-2_min: 0
    apples_agent-3_max: 20
    apples_agent-3_mean: 2.13
    apples_agent-3_min: 0
    apples_agent-4_max: 20
    apples_agent-4_mean: 0.96
    apples_agent-4_min: 0
    apples_agent-5_max: 8
    apples_agent-5_mean: 0.9
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 286
    cleaning_beam_agent-0_mean: 90.0
    cleaning_beam_agent-0_min: 18
    cleaning_beam_agent-1_max: 496
    cleaning_beam_agent-1_mean: 285.21
    cleaning_beam_agent-1_min: 141
    cleaning_beam_agent-2_max: 113
    cleaning_beam_agent-2_mean: 26.66
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 226
    cleaning_beam_agent-3_mean: 119.48
    cleaning_beam_agent-3_min: 26
    cleaning_beam_agent-4_max: 269
    cleaning_beam_agent-4_mean: 82.4
    cleaning_beam_agent-4_min: 13
    cleaning_beam_agent-5_max: 27
    cleaning_beam_agent-5_mean: 6.46
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 4
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-09-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 50.0
  episode_reward_mean: 14.94
  episode_reward_min: -97.0
  episodes_this_iter: 96
  episodes_total: 17664
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 12044.761
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00016375680570490658
        entropy: 0.6227268576622009
        entropy_coeff: 0.0017600000137463212
        kl: 0.00729414215311408
        model: {}
        policy_loss: -0.016757508739829063
        total_loss: -0.017100224271416664
        vf_explained_var: -0.028353333473205566
        vf_loss: 0.23873215913772583
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00016375680570490658
        entropy: 0.5402331948280334
        entropy_coeff: 0.0017600000137463212
        kl: 0.006295811850577593
        model: {}
        policy_loss: -0.007467162329703569
        total_loss: -0.007942135445773602
        vf_explained_var: 0.023334980010986328
        vf_loss: 1.6104795932769775
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00016375680570490658
        entropy: 0.519870936870575
        entropy_coeff: 0.0017600000137463212
        kl: 0.00638189259916544
        model: {}
        policy_loss: -0.008798765018582344
        total_loss: -0.00922597385942936
        vf_explained_var: -0.003204062581062317
        vf_loss: 1.6867214441299438
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00016375680570490658
        entropy: 0.6557391881942749
        entropy_coeff: 0.0017600000137463212
        kl: 0.006750478409230709
        model: {}
        policy_loss: -0.008426381275057793
        total_loss: -0.009079609997570515
        vf_explained_var: 0.0005823373794555664
        vf_loss: 1.6334847211837769
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00016375680570490658
        entropy: 0.6781542301177979
        entropy_coeff: 0.0017600000137463212
        kl: 0.008190666325390339
        model: {}
        policy_loss: -0.018179092556238174
        total_loss: -0.018928514793515205
        vf_explained_var: -0.006696969270706177
        vf_loss: 0.345961332321167
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00016375680570490658
        entropy: 0.7620949745178223
        entropy_coeff: 0.0017600000137463212
        kl: 0.006337311118841171
        model: {}
        policy_loss: -0.011010484769940376
        total_loss: -0.011705216020345688
        vf_explained_var: 0.0030350536108016968
        vf_loss: 0.1282641887664795
    load_time_ms: 13893.806
    num_steps_sampled: 17664000
    num_steps_trained: 17664000
    sample_time_ms: 100629.265
    update_time_ms: 16.489
  iterations_since_restore: 24
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 48.1828729281768
    ram_util_percent: 18.591160220994475
  pid: 13408
  policy_reward_max:
    agent-0: 10.0
    agent-1: 9.0
    agent-2: 14.0
    agent-3: 13.0
    agent-4: 13.0
    agent-5: 6.0
  policy_reward_mean:
    agent-0: 2.72
    agent-1: 0.64
    agent-2: 3.43
    agent-3: 2.88
    agent-4: 3.79
    agent-5: 1.48
  policy_reward_min:
    agent-0: 0.0
    agent-1: -50.0
    agent-2: -48.0
    agent-3: -46.0
    agent-4: 0.0
    agent-5: -3.0
  sampler_perf:
    mean_env_wait_ms: 23.475025276371767
    mean_inference_ms: 13.107305790494241
    mean_processing_ms: 58.67399086540952
  time_since_restore: 3045.3449473381042
  time_this_iter_s: 126.79476809501648
  time_total_s: 25555.62818980217
  timestamp: 1637222999
  timesteps_since_restore: 2304000
  timesteps_this_iter: 96000
  timesteps_total: 17664000
  training_iteration: 184
  trial_id: '00000'
  
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
== Status ==
Memory usage on this node: 34.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    184 |          25555.6 | 17664000 |    14.94 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 2.53
    apples_agent-0_min: 0
    apples_agent-1_max: 20
    apples_agent-1_mean: 1.16
    apples_agent-1_min: 0
    apples_agent-2_max: 8
    apples_agent-2_mean: 1.87
    apples_agent-2_min: 0
    apples_agent-3_max: 9
    apples_agent-3_mean: 2.56
    apples_agent-3_min: 0
    apples_agent-4_max: 8
    apples_agent-4_mean: 1.31
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 0.8
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 176
    cleaning_beam_agent-0_mean: 84.25
    cleaning_beam_agent-0_min: 26
    cleaning_beam_agent-1_max: 481
    cleaning_beam_agent-1_mean: 288.94
    cleaning_beam_agent-1_min: 146
    cleaning_beam_agent-2_max: 96
    cleaning_beam_agent-2_mean: 24.6
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 242
    cleaning_beam_agent-3_mean: 128.62
    cleaning_beam_agent-3_min: 51
    cleaning_beam_agent-4_max: 208
    cleaning_beam_agent-4_mean: 84.9
    cleaning_beam_agent-4_min: 23
    cleaning_beam_agent-5_max: 54
    cleaning_beam_agent-5_mean: 7.46
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-12-07
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 47.0
  episode_reward_mean: 18.78
  episode_reward_min: -22.0
  episodes_this_iter: 96
  episodes_total: 17760
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 12033.96
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00015776639338582754
        entropy: 0.6208025813102722
        entropy_coeff: 0.0017600000137463212
        kl: 0.007547468412667513
        model: {}
        policy_loss: -0.017258569598197937
        total_loss: -0.017568422481417656
        vf_explained_var: -0.032446473836898804
        vf_loss: 0.2801082134246826
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00015776639338582754
        entropy: 0.5273321866989136
        entropy_coeff: 0.0017600000137463212
        kl: 0.007651872001588345
        model: {}
        policy_loss: -0.014447237364947796
        total_loss: -0.014980066567659378
        vf_explained_var: 0.009950295090675354
        vf_loss: 0.12684005498886108
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00015776639338582754
        entropy: 0.5139335989952087
        entropy_coeff: 0.0017600000137463212
        kl: 0.007967958226799965
        model: {}
        policy_loss: -0.01681116595864296
        total_loss: -0.017270483076572418
        vf_explained_var: -0.0006776005029678345
        vf_loss: 0.4680708348751068
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00015776639338582754
        entropy: 0.6485004425048828
        entropy_coeff: 0.0017600000137463212
        kl: 0.007208434399217367
        model: {}
        policy_loss: -0.013548081740736961
        total_loss: -0.01429004967212677
        vf_explained_var: -0.007263660430908203
        vf_loss: 0.38975417613983154
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00015776639338582754
        entropy: 0.6796135902404785
        entropy_coeff: 0.0017600000137463212
        kl: 0.008970675058662891
        model: {}
        policy_loss: -0.01946958899497986
        total_loss: -0.020180096849799156
        vf_explained_var: -0.0045156776905059814
        vf_loss: 0.3708127439022064
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00015776639338582754
        entropy: 0.7741703391075134
        entropy_coeff: 0.0017600000137463212
        kl: 0.006784762255847454
        model: {}
        policy_loss: -0.01104336604475975
        total_loss: -0.011716821230947971
        vf_explained_var: 0.0035170912742614746
        vf_loss: 0.10607527941465378
    load_time_ms: 13871.825
    num_steps_sampled: 17760000
    num_steps_trained: 17760000
    sample_time_ms: 100695.918
    update_time_ms: 16.236
  iterations_since_restore: 25
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.91381215469613
    ram_util_percent: 18.622651933701654
  pid: 13408
  policy_reward_max:
    agent-0: 12.0
    agent-1: 8.0
    agent-2: 12.0
    agent-3: 14.0
    agent-4: 13.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 3.25
    agent-1: 1.75
    agent-2: 4.0
    agent-3: 3.87
    agent-4: 4.47
    agent-5: 1.44
  policy_reward_min:
    agent-0: 0.0
    agent-1: -2.0
    agent-2: -42.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.48478534642833
    mean_inference_ms: 13.112866527525274
    mean_processing_ms: 58.69537666529593
  time_since_restore: 3172.3827254772186
  time_this_iter_s: 127.03777813911438
  time_total_s: 25682.665967941284
  timestamp: 1637223127
  timesteps_since_restore: 2400000
  timesteps_this_iter: 96000
  timesteps_total: 17760000
  training_iteration: 185
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 33.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    185 |          25682.7 | 17760000 |    18.78 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 1.9
    apples_agent-0_min: 0
    apples_agent-1_max: 20
    apples_agent-1_mean: 1.16
    apples_agent-1_min: 0
    apples_agent-2_max: 12
    apples_agent-2_mean: 1.93
    apples_agent-2_min: 0
    apples_agent-3_max: 15
    apples_agent-3_mean: 2.08
    apples_agent-3_min: 0
    apples_agent-4_max: 109
    apples_agent-4_mean: 2.6
    apples_agent-4_min: 0
    apples_agent-5_max: 5
    apples_agent-5_mean: 0.79
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 211
    cleaning_beam_agent-0_mean: 86.06
    cleaning_beam_agent-0_min: 20
    cleaning_beam_agent-1_max: 400
    cleaning_beam_agent-1_mean: 280.08
    cleaning_beam_agent-1_min: 171
    cleaning_beam_agent-2_max: 92
    cleaning_beam_agent-2_mean: 27.39
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 205
    cleaning_beam_agent-3_mean: 115.61
    cleaning_beam_agent-3_min: 61
    cleaning_beam_agent-4_max: 202
    cleaning_beam_agent-4_mean: 79.52
    cleaning_beam_agent-4_min: 26
    cleaning_beam_agent-5_max: 45
    cleaning_beam_agent-5_mean: 9.78
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-14-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 61.0
  episode_reward_mean: 18.45
  episode_reward_min: 2.0
  episodes_this_iter: 96
  episodes_total: 17856
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 12029.245
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00015177599561866373
        entropy: 0.6104961633682251
        entropy_coeff: 0.0017600000137463212
        kl: 0.0068906145170331
        model: {}
        policy_loss: -0.016122862696647644
        total_loss: -0.0164740439504385
        vf_explained_var: -0.008204177021980286
        vf_loss: 0.34231042861938477
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00015177599561866373
        entropy: 0.5311499238014221
        entropy_coeff: 0.0017600000137463212
        kl: 0.006019240710884333
        model: {}
        policy_loss: -0.011477897875010967
        total_loss: -0.012095225974917412
        vf_explained_var: 0.03282447159290314
        vf_loss: 0.16531850397586823
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00015177599561866373
        entropy: 0.5144914388656616
        entropy_coeff: 0.0017600000137463212
        kl: 0.007660259027034044
        model: {}
        policy_loss: -0.015603847801685333
        total_loss: -0.016079548746347427
        vf_explained_var: 0.0028287023305892944
        vf_loss: 0.4679067134857178
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00015177599561866373
        entropy: 0.6456180214881897
        entropy_coeff: 0.0017600000137463212
        kl: 0.0067117949947714806
        model: {}
        policy_loss: -0.0120705496519804
        total_loss: -0.012836042791604996
        vf_explained_var: 0.004322528839111328
        vf_loss: 0.3520452380180359
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00015177599561866373
        entropy: 0.6880976557731628
        entropy_coeff: 0.0017600000137463212
        kl: 0.0076536862179636955
        model: {}
        policy_loss: -0.01670786552131176
        total_loss: -0.017492832615971565
        vf_explained_var: -0.00411774218082428
        vf_loss: 0.43401893973350525
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00015177599561866373
        entropy: 0.799716591835022
        entropy_coeff: 0.0017600000137463212
        kl: 0.005968880373984575
        model: {}
        policy_loss: -0.011128401383757591
        total_loss: -0.011926425620913506
        vf_explained_var: 0.015012651681900024
        vf_loss: 0.12589696049690247
    load_time_ms: 13872.561
    num_steps_sampled: 17856000
    num_steps_trained: 17856000
    sample_time_ms: 100822.111
    update_time_ms: 16.824
  iterations_since_restore: 26
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.61555555555555
    ram_util_percent: 18.427777777777777
  pid: 13408
  policy_reward_max:
    agent-0: 12.0
    agent-1: 11.0
    agent-2: 17.0
    agent-3: 19.0
    agent-4: 13.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 3.2
    agent-1: 1.86
    agent-2: 4.28
    agent-3: 3.2
    agent-4: 4.39
    agent-5: 1.52
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -2.0
  sampler_perf:
    mean_env_wait_ms: 23.487084667060603
    mean_inference_ms: 13.118295107063913
    mean_processing_ms: 58.71973791867231
  time_since_restore: 3299.227757692337
  time_this_iter_s: 126.84503221511841
  time_total_s: 25809.511000156403
  timestamp: 1637223254
  timesteps_since_restore: 2496000
  timesteps_this_iter: 96000
  timesteps_total: 17856000
  training_iteration: 186
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    186 |          25809.5 | 17856000 |    18.45 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 2.09
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 0.97
    apples_agent-1_min: 0
    apples_agent-2_max: 12
    apples_agent-2_mean: 1.9
    apples_agent-2_min: 0
    apples_agent-3_max: 16
    apples_agent-3_mean: 2.3
    apples_agent-3_min: 0
    apples_agent-4_max: 7
    apples_agent-4_mean: 1.02
    apples_agent-4_min: 0
    apples_agent-5_max: 13
    apples_agent-5_mean: 1.39
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 213
    cleaning_beam_agent-0_mean: 90.88
    cleaning_beam_agent-0_min: 20
    cleaning_beam_agent-1_max: 446
    cleaning_beam_agent-1_mean: 290.27
    cleaning_beam_agent-1_min: 191
    cleaning_beam_agent-2_max: 82
    cleaning_beam_agent-2_mean: 23.07
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 214
    cleaning_beam_agent-3_mean: 117.23
    cleaning_beam_agent-3_min: 38
    cleaning_beam_agent-4_max: 288
    cleaning_beam_agent-4_mean: 82.07
    cleaning_beam_agent-4_min: 12
    cleaning_beam_agent-5_max: 25
    cleaning_beam_agent-5_mean: 7.28
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-16-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 87.0
  episode_reward_mean: 17.28
  episode_reward_min: -90.0
  episodes_this_iter: 96
  episodes_total: 17952
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 12039.982
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00014578559785149992
        entropy: 0.6163991689682007
        entropy_coeff: 0.0017600000137463212
        kl: 0.007151941768825054
        model: {}
        policy_loss: -0.015405124984681606
        total_loss: -0.015745006501674652
        vf_explained_var: -0.017902225255966187
        vf_loss: 0.2978476285934448
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00014578559785149992
        entropy: 0.5303685069084167
        entropy_coeff: 0.0017600000137463212
        kl: 0.006683641113340855
        model: {}
        policy_loss: -0.005893021821975708
        total_loss: -0.006342766340821981
        vf_explained_var: 0.013914316892623901
        vf_loss: 1.495258092880249
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00014578559785149992
        entropy: 0.5010673999786377
        entropy_coeff: 0.0017600000137463212
        kl: 0.004859952721744776
        model: {}
        policy_loss: -0.0069980137050151825
        total_loss: -0.007455221377313137
        vf_explained_var: 0.00728432834148407
        vf_loss: 1.8167320489883423
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00014578559785149992
        entropy: 0.6477451920509338
        entropy_coeff: 0.0017600000137463212
        kl: 0.005971095059067011
        model: {}
        policy_loss: -0.00733904680237174
        total_loss: -0.00789709109812975
        vf_explained_var: 0.004393860697746277
        vf_loss: 2.8342928886413574
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00014578559785149992
        entropy: 0.686773419380188
        entropy_coeff: 0.0017600000137463212
        kl: 0.0072965966537594795
        model: {}
        policy_loss: -0.014781300909817219
        total_loss: -0.015561530366539955
        vf_explained_var: 0.008429273962974548
        vf_loss: 0.6366416215896606
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00014578559785149992
        entropy: 0.8021434545516968
        entropy_coeff: 0.0017600000137463212
        kl: 0.006117618642747402
        model: {}
        policy_loss: -0.01116605568677187
        total_loss: -0.011951960623264313
        vf_explained_var: 0.01045963168144226
        vf_loss: 0.1410510390996933
    load_time_ms: 13850.103
    num_steps_sampled: 17952000
    num_steps_trained: 17952000
    sample_time_ms: 100836.558
    update_time_ms: 16.466
  iterations_since_restore: 27
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 49.42944444444444
    ram_util_percent: 18.671666666666663
  pid: 13408
  policy_reward_max:
    agent-0: 22.0
    agent-1: 11.0
    agent-2: 21.0
    agent-3: 21.0
    agent-4: 28.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 3.0
    agent-1: 1.42
    agent-2: 3.93
    agent-3: 2.58
    agent-4: 4.58
    agent-5: 1.77
  policy_reward_min:
    agent-0: 0.0
    agent-1: -50.0
    agent-2: -48.0
    agent-3: -49.0
    agent-4: 0.0
    agent-5: -2.0
  sampler_perf:
    mean_env_wait_ms: 23.490878833197822
    mean_inference_ms: 13.121426836782797
    mean_processing_ms: 58.73354056098142
  time_since_restore: 3425.3887054920197
  time_this_iter_s: 126.16094779968262
  time_total_s: 25935.671947956085
  timestamp: 1637223380
  timesteps_since_restore: 2592000
  timesteps_this_iter: 96000
  timesteps_total: 17952000
  training_iteration: 187
  trial_id: '00000'
  
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
== Status ==
Memory usage on this node: 34.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    187 |          25935.7 | 17952000 |    17.28 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 18
    apples_agent-0_mean: 2.36
    apples_agent-0_min: 0
    apples_agent-1_max: 20
    apples_agent-1_mean: 1.09
    apples_agent-1_min: 0
    apples_agent-2_max: 15
    apples_agent-2_mean: 1.75
    apples_agent-2_min: 0
    apples_agent-3_max: 16
    apples_agent-3_mean: 2.69
    apples_agent-3_min: 0
    apples_agent-4_max: 20
    apples_agent-4_mean: 1.36
    apples_agent-4_min: 0
    apples_agent-5_max: 8
    apples_agent-5_mean: 0.93
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 184
    cleaning_beam_agent-0_mean: 84.22
    cleaning_beam_agent-0_min: 35
    cleaning_beam_agent-1_max: 438
    cleaning_beam_agent-1_mean: 294.72
    cleaning_beam_agent-1_min: 153
    cleaning_beam_agent-2_max: 82
    cleaning_beam_agent-2_mean: 23.34
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 232
    cleaning_beam_agent-3_mean: 120.46
    cleaning_beam_agent-3_min: 45
    cleaning_beam_agent-4_max: 182
    cleaning_beam_agent-4_mean: 74.82
    cleaning_beam_agent-4_min: 17
    cleaning_beam_agent-5_max: 56
    cleaning_beam_agent-5_mean: 6.36
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 4
    fire_beam_agent-1_mean: 0.07
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-18-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 82.0
  episode_reward_mean: 18.98
  episode_reward_min: -42.0
  episodes_this_iter: 96
  episodes_total: 18048
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11968.758
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001397952000843361
        entropy: 0.6096928119659424
        entropy_coeff: 0.0017600000137463212
        kl: 0.006906665861606598
        model: {}
        policy_loss: -0.015676382929086685
        total_loss: -0.016027292236685753
        vf_explained_var: -0.01807728409767151
        vf_loss: 0.3148154616355896
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0001397952000843361
        entropy: 0.5315950512886047
        entropy_coeff: 0.0017600000137463212
        kl: 0.006119139958173037
        model: {}
        policy_loss: -0.012564748525619507
        total_loss: -0.01318222563713789
        vf_explained_var: 0.011379152536392212
        vf_loss: 0.1217324286699295
      agent-2:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0001397952000843361
        entropy: 0.4934861660003662
        entropy_coeff: 0.0017600000137463212
        kl: 0.006530764978379011
        model: {}
        policy_loss: -0.010215519927442074
        total_loss: -0.010742348618805408
        vf_explained_var: -0.003703981637954712
        vf_loss: 1.784336805343628
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0001397952000843361
        entropy: 0.6537894010543823
        entropy_coeff: 0.0017600000137463212
        kl: 0.006589875556528568
        model: {}
        policy_loss: -0.013519138097763062
        total_loss: -0.014310172758996487
        vf_explained_var: -0.004242897033691406
        vf_loss: 0.30141621828079224
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0001397952000843361
        entropy: 0.6936745643615723
        entropy_coeff: 0.0017600000137463212
        kl: 0.008159049786627293
        model: {}
        policy_loss: -0.017865493893623352
        total_loss: -0.018642975017428398
        vf_explained_var: 0.012645348906517029
        vf_loss: 0.3543298840522766
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001397952000843361
        entropy: 0.7448790669441223
        entropy_coeff: 0.0017600000137463212
        kl: 0.005243576131761074
        model: {}
        policy_loss: -0.010189184918999672
        total_loss: -0.010963334701955318
        vf_explained_var: 0.01444089412689209
        vf_loss: 0.12480880320072174
    load_time_ms: 13828.162
    num_steps_sampled: 18048000
    num_steps_trained: 18048000
    sample_time_ms: 100763.787
    update_time_ms: 16.616
  iterations_since_restore: 28
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 36.37765363128492
    ram_util_percent: 16.330167597765364
  pid: 13408
  policy_reward_max:
    agent-0: 14.0
    agent-1: 11.0
    agent-2: 14.0
    agent-3: 21.0
    agent-4: 28.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 3.25
    agent-1: 1.7
    agent-2: 3.88
    agent-3: 3.74
    agent-4: 4.68
    agent-5: 1.73
  policy_reward_min:
    agent-0: 0.0
    agent-1: -2.0
    agent-2: -49.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.49214529144713
    mean_inference_ms: 13.121819337430377
    mean_processing_ms: 58.740127696999295
  time_since_restore: 3551.1532142162323
  time_this_iter_s: 125.76450872421265
  time_total_s: 26061.436456680298
  timestamp: 1637223506
  timesteps_since_restore: 2688000
  timesteps_this_iter: 96000
  timesteps_total: 18048000
  training_iteration: 188
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    188 |          26061.4 | 18048000 |    18.98 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 2.06
    apples_agent-0_min: 0
    apples_agent-1_max: 13
    apples_agent-1_mean: 0.95
    apples_agent-1_min: 0
    apples_agent-2_max: 74
    apples_agent-2_mean: 2.25
    apples_agent-2_min: 0
    apples_agent-3_max: 9
    apples_agent-3_mean: 2.05
    apples_agent-3_min: 0
    apples_agent-4_max: 10
    apples_agent-4_mean: 1.04
    apples_agent-4_min: 0
    apples_agent-5_max: 10
    apples_agent-5_mean: 0.94
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 199
    cleaning_beam_agent-0_mean: 89.48
    cleaning_beam_agent-0_min: 30
    cleaning_beam_agent-1_max: 434
    cleaning_beam_agent-1_mean: 291.7
    cleaning_beam_agent-1_min: 198
    cleaning_beam_agent-2_max: 61
    cleaning_beam_agent-2_mean: 21.72
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 204
    cleaning_beam_agent-3_mean: 111.05
    cleaning_beam_agent-3_min: 43
    cleaning_beam_agent-4_max: 214
    cleaning_beam_agent-4_mean: 68.11
    cleaning_beam_agent-4_min: 21
    cleaning_beam_agent-5_max: 37
    cleaning_beam_agent-5_mean: 8.42
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-20-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 57.0
  episode_reward_mean: 16.33
  episode_reward_min: -42.0
  episodes_this_iter: 96
  episodes_total: 18144
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11937.082
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001338048023171723
        entropy: 0.6031128764152527
        entropy_coeff: 0.0017600000137463212
        kl: 0.00647851824760437
        model: {}
        policy_loss: -0.01313537172973156
        total_loss: -0.013514608144760132
        vf_explained_var: -0.017150789499282837
        vf_loss: 0.3438940942287445
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0001338048023171723
        entropy: 0.541405439376831
        entropy_coeff: 0.0017600000137463212
        kl: 0.005914363544434309
        model: {}
        policy_loss: -0.007073123008012772
        total_loss: -0.007590139284729958
        vf_explained_var: 0.0028261393308639526
        vf_loss: 1.401397466659546
      agent-2:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0001338048023171723
        entropy: 0.4907245635986328
        entropy_coeff: 0.0017600000137463212
        kl: 0.006857634522020817
        model: {}
        policy_loss: -0.014152167364954948
        total_loss: -0.014805689454078674
        vf_explained_var: 0.001814812421798706
        vf_loss: 0.38708844780921936
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0001338048023171723
        entropy: 0.6402005553245544
        entropy_coeff: 0.0017600000137463212
        kl: 0.005889824125915766
        model: {}
        policy_loss: -0.012338044121861458
        total_loss: -0.013140318915247917
        vf_explained_var: -0.001302957534790039
        vf_loss: 0.299862802028656
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0001338048023171723
        entropy: 0.6873435974121094
        entropy_coeff: 0.0017600000137463212
        kl: 0.007950009778141975
        model: {}
        policy_loss: -0.016474412754178047
        total_loss: -0.017249803990125656
        vf_explained_var: 0.0019174069166183472
        vf_loss: 0.3683212995529175
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001338048023171723
        entropy: 0.799644947052002
        entropy_coeff: 0.0017600000137463212
        kl: 0.005716734565794468
        model: {}
        policy_loss: -0.010101208463311195
        total_loss: -0.010929172858595848
        vf_explained_var: 0.019894853234291077
        vf_loss: 0.0773705244064331
    load_time_ms: 13801.005
    num_steps_sampled: 18144000
    num_steps_trained: 18144000
    sample_time_ms: 100538.919
    update_time_ms: 16.451
  iterations_since_restore: 29
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.133522727272727
    ram_util_percent: 11.444318181818181
  pid: 13408
  policy_reward_max:
    agent-0: 20.0
    agent-1: 6.0
    agent-2: 19.0
    agent-3: 12.0
    agent-4: 13.0
    agent-5: 6.0
  policy_reward_mean:
    agent-0: 3.1
    agent-1: 0.98
    agent-2: 3.31
    agent-3: 3.37
    agent-4: 4.2
    agent-5: 1.37
  policy_reward_min:
    agent-0: 0.0
    agent-1: -50.0
    agent-2: -49.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.474279857391355
    mean_inference_ms: 13.106064259086388
    mean_processing_ms: 58.69395648538266
  time_since_restore: 3674.518943309784
  time_this_iter_s: 123.36572909355164
  time_total_s: 26184.80218577385
  timestamp: 1637223629
  timesteps_since_restore: 2784000
  timesteps_this_iter: 96000
  timesteps_total: 18144000
  training_iteration: 189
  trial_id: '00000'
  
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
== Status ==
Memory usage on this node: 20.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    189 |          26184.8 | 18144000 |    16.33 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 2.16
    apples_agent-0_min: 0
    apples_agent-1_max: 34
    apples_agent-1_mean: 1.21
    apples_agent-1_min: 0
    apples_agent-2_max: 20
    apples_agent-2_mean: 1.98
    apples_agent-2_min: 0
    apples_agent-3_max: 13
    apples_agent-3_mean: 2.91
    apples_agent-3_min: 0
    apples_agent-4_max: 21
    apples_agent-4_mean: 1.31
    apples_agent-4_min: 0
    apples_agent-5_max: 16
    apples_agent-5_mean: 0.88
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 169
    cleaning_beam_agent-0_mean: 85.18
    cleaning_beam_agent-0_min: 30
    cleaning_beam_agent-1_max: 447
    cleaning_beam_agent-1_mean: 290.54
    cleaning_beam_agent-1_min: 174
    cleaning_beam_agent-2_max: 124
    cleaning_beam_agent-2_mean: 23.11
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 274
    cleaning_beam_agent-3_mean: 113.08
    cleaning_beam_agent-3_min: 30
    cleaning_beam_agent-4_max: 289
    cleaning_beam_agent-4_mean: 70.84
    cleaning_beam_agent-4_min: 6
    cleaning_beam_agent-5_max: 27
    cleaning_beam_agent-5_mean: 7.1
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-22-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 59.0
  episode_reward_mean: 18.74
  episode_reward_min: -51.0
  episodes_this_iter: 96
  episodes_total: 18240
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11878.0
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00012781440455000848
        entropy: 0.6106975078582764
        entropy_coeff: 0.0017600000137463212
        kl: 0.006609264761209488
        model: {}
        policy_loss: -0.015536300837993622
        total_loss: -0.015919897705316544
        vf_explained_var: -0.012499779462814331
        vf_loss: 0.30307286977767944
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00012781440455000848
        entropy: 0.5432525277137756
        entropy_coeff: 0.0017600000137463212
        kl: 0.004349066875874996
        model: {}
        policy_loss: -0.005339333787560463
        total_loss: -0.005932669620960951
        vf_explained_var: 0.008265793323516846
        vf_loss: 1.4533544778823853
      agent-2:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.00012781440455000848
        entropy: 0.4945562779903412
        entropy_coeff: 0.0017600000137463212
        kl: 0.006913913879543543
        model: {}
        policy_loss: -0.013706987723708153
        total_loss: -0.014346780255436897
        vf_explained_var: 6.818771362304688e-05
        vf_loss: 0.5777667760848999
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00012781440455000848
        entropy: 0.6312203407287598
        entropy_coeff: 0.0017600000137463212
        kl: 0.00590091897174716
        model: {}
        policy_loss: -0.011131412349641323
        total_loss: -0.011900030076503754
        vf_explained_var: 0.0021961182355880737
        vf_loss: 0.47285404801368713
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00012781440455000848
        entropy: 0.6825085878372192
        entropy_coeff: 0.0017600000137463212
        kl: 0.007122776471078396
        model: {}
        policy_loss: -0.015474489890038967
        total_loss: -0.016277093440294266
        vf_explained_var: 0.030760839581489563
        vf_loss: 0.4247352182865143
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00012781440455000848
        entropy: 0.7647572755813599
        entropy_coeff: 0.0017600000137463212
        kl: 0.005434722173959017
        model: {}
        policy_loss: -0.010698633268475533
        total_loss: -0.011491015553474426
        vf_explained_var: 0.029847800731658936
        vf_loss: 0.10120326280593872
    load_time_ms: 13786.409
    num_steps_sampled: 18240000
    num_steps_trained: 18240000
    sample_time_ms: 100106.078
    update_time_ms: 16.445
  iterations_since_restore: 30
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.710982658959537
    ram_util_percent: 11.491907514450867
  pid: 13408
  policy_reward_max:
    agent-0: 9.0
    agent-1: 8.0
    agent-2: 18.0
    agent-3: 18.0
    agent-4: 17.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 3.08
    agent-1: 1.12
    agent-2: 4.63
    agent-3: 4.05
    agent-4: 4.31
    agent-5: 1.55
  policy_reward_min:
    agent-0: -1.0
    agent-1: -50.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.452254565234078
    mean_inference_ms: 13.090635080899522
    mean_processing_ms: 58.64633115614063
  time_since_restore: 3796.292192220688
  time_this_iter_s: 121.77324891090393
  time_total_s: 26306.575434684753
  timestamp: 1637223751
  timesteps_since_restore: 2880000
  timesteps_this_iter: 96000
  timesteps_total: 18240000
  training_iteration: 190
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    190 |          26306.6 | 18240000 |    18.74 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 2.17
    apples_agent-0_min: 0
    apples_agent-1_max: 18
    apples_agent-1_mean: 0.87
    apples_agent-1_min: 0
    apples_agent-2_max: 40
    apples_agent-2_mean: 2.29
    apples_agent-2_min: 0
    apples_agent-3_max: 22
    apples_agent-3_mean: 2.2
    apples_agent-3_min: 0
    apples_agent-4_max: 8
    apples_agent-4_mean: 1.0
    apples_agent-4_min: 0
    apples_agent-5_max: 5
    apples_agent-5_mean: 1.01
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 147
    cleaning_beam_agent-0_mean: 79.22
    cleaning_beam_agent-0_min: 30
    cleaning_beam_agent-1_max: 418
    cleaning_beam_agent-1_mean: 293.68
    cleaning_beam_agent-1_min: 184
    cleaning_beam_agent-2_max: 107
    cleaning_beam_agent-2_mean: 25.68
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 239
    cleaning_beam_agent-3_mean: 114.3
    cleaning_beam_agent-3_min: 43
    cleaning_beam_agent-4_max: 133
    cleaning_beam_agent-4_mean: 62.84
    cleaning_beam_agent-4_min: 15
    cleaning_beam_agent-5_max: 30
    cleaning_beam_agent-5_mean: 6.05
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 5
    fire_beam_agent-1_mean: 0.09
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-24-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 43.0
  episode_reward_mean: 17.79
  episode_reward_min: 0.0
  episodes_this_iter: 96
  episodes_total: 18336
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11818.219
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00012182399950688705
        entropy: 0.6002904772758484
        entropy_coeff: 0.0017600000137463212
        kl: 0.006561704911291599
        model: {}
        policy_loss: -0.015079646371304989
        total_loss: -0.015453239902853966
        vf_explained_var: -0.00998753309249878
        vf_loss: 0.2674950659275055
      agent-1:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.00012182399950688705
        entropy: 0.5431650876998901
        entropy_coeff: 0.0017600000137463212
        kl: 0.005448708776384592
        model: {}
        policy_loss: -0.01205588597804308
        total_loss: -0.012863270938396454
        vf_explained_var: 0.04046294093132019
        vf_loss: 0.12370425462722778
      agent-2:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.00012182399950688705
        entropy: 0.4971396327018738
        entropy_coeff: 0.0017600000137463212
        kl: 0.006226545665413141
        model: {}
        policy_loss: -0.013209422118961811
        total_loss: -0.013875036500394344
        vf_explained_var: -0.003362119197845459
        vf_loss: 0.536884069442749
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00012182399950688705
        entropy: 0.6308854222297668
        entropy_coeff: 0.0017600000137463212
        kl: 0.006024614907801151
        model: {}
        policy_loss: -0.012303791008889675
        total_loss: -0.013088256120681763
        vf_explained_var: -0.0032848119735717773
        vf_loss: 0.2466343343257904
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00012182399950688705
        entropy: 0.6804361343383789
        entropy_coeff: 0.0017600000137463212
        kl: 0.0070434445515275
        model: {}
        policy_loss: -0.015961607918143272
        total_loss: -0.01676909066736698
        vf_explained_var: 0.016695380210876465
        vf_loss: 0.3791373372077942
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00012182399950688705
        entropy: 0.7471373081207275
        entropy_coeff: 0.0017600000137463212
        kl: 0.005328904837369919
        model: {}
        policy_loss: -0.010103981010615826
        total_loss: -0.010874593630433083
        vf_explained_var: 0.014334484934806824
        vf_loss: 0.11456811428070068
    load_time_ms: 13799.491
    num_steps_sampled: 18336000
    num_steps_trained: 18336000
    sample_time_ms: 99881.136
    update_time_ms: 16.305
  iterations_since_restore: 31
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.39943820224719
    ram_util_percent: 11.434831460674157
  pid: 13408
  policy_reward_max:
    agent-0: 12.0
    agent-1: 7.0
    agent-2: 14.0
    agent-3: 12.0
    agent-4: 13.0
    agent-5: 6.0
  policy_reward_mean:
    agent-0: 3.07
    agent-1: 1.41
    agent-2: 4.44
    agent-3: 3.29
    agent-4: 3.93
    agent-5: 1.65
  policy_reward_min:
    agent-0: 0.0
    agent-1: -5.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.437972430473042
    mean_inference_ms: 13.076485584462825
    mean_processing_ms: 58.61339857747829
  time_since_restore: 3920.557160139084
  time_this_iter_s: 124.264967918396
  time_total_s: 26430.84040260315
  timestamp: 1637223876
  timesteps_since_restore: 2976000
  timesteps_this_iter: 96000
  timesteps_total: 18336000
  training_iteration: 191
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    191 |          26430.8 | 18336000 |    17.79 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 1.95
    apples_agent-0_min: 0
    apples_agent-1_max: 9
    apples_agent-1_mean: 0.77
    apples_agent-1_min: 0
    apples_agent-2_max: 10
    apples_agent-2_mean: 2.16
    apples_agent-2_min: 0
    apples_agent-3_max: 17
    apples_agent-3_mean: 2.77
    apples_agent-3_min: 0
    apples_agent-4_max: 9
    apples_agent-4_mean: 1.29
    apples_agent-4_min: 0
    apples_agent-5_max: 13
    apples_agent-5_mean: 1.05
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 216
    cleaning_beam_agent-0_mean: 83.21
    cleaning_beam_agent-0_min: 18
    cleaning_beam_agent-1_max: 435
    cleaning_beam_agent-1_mean: 288.66
    cleaning_beam_agent-1_min: 190
    cleaning_beam_agent-2_max: 107
    cleaning_beam_agent-2_mean: 26.44
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 228
    cleaning_beam_agent-3_mean: 115.51
    cleaning_beam_agent-3_min: 34
    cleaning_beam_agent-4_max: 157
    cleaning_beam_agent-4_mean: 64.26
    cleaning_beam_agent-4_min: 8
    cleaning_beam_agent-5_max: 21
    cleaning_beam_agent-5_mean: 6.55
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 66
    fire_beam_agent-5_mean: 0.66
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-26-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 59.0
  episode_reward_mean: 13.67
  episode_reward_min: -498.0
  episodes_this_iter: 96
  episodes_total: 18432
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11765.159
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00011583360173972324
        entropy: 0.5970897674560547
        entropy_coeff: 0.0017600000137463212
        kl: 0.005873840302228928
        model: {}
        policy_loss: -0.014361435547471046
        total_loss: -0.014801179990172386
        vf_explained_var: -0.013763487339019775
        vf_loss: 0.23745781183242798
      agent-1:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.00011583360173972324
        entropy: 0.5362550616264343
        entropy_coeff: 0.0017600000137463212
        kl: 0.006058993749320507
        model: {}
        policy_loss: -0.01129096932709217
        total_loss: -0.01206972450017929
        vf_explained_var: 0.0007674098014831543
        vf_loss: 0.1357950121164322
      agent-2:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.00011583360173972324
        entropy: 0.5058825612068176
        entropy_coeff: 0.0017600000137463212
        kl: 0.005888715386390686
        model: {}
        policy_loss: -0.005470333155244589
        total_loss: -0.0008096545934677124
        vf_explained_var: 0.0008961707353591919
        vf_loss: 54.03816604614258
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00011583360173972324
        entropy: 0.6300486326217651
        entropy_coeff: 0.0017600000137463212
        kl: 0.006082611158490181
        model: {}
        policy_loss: -0.01197783462703228
        total_loss: -0.012736789882183075
        vf_explained_var: 0.000853806734085083
        vf_loss: 0.4579947590827942
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00011583360173972324
        entropy: 0.6835005283355713
        entropy_coeff: 0.0017600000137463212
        kl: 0.006906009279191494
        model: {}
        policy_loss: -0.016282731667160988
        total_loss: -0.017100533470511436
        vf_explained_var: 0.001416712999343872
        vf_loss: 0.3985580801963806
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00011583360173972324
        entropy: 0.7298144102096558
        entropy_coeff: 0.0017600000137463212
        kl: 0.006384309846907854
        model: {}
        policy_loss: -0.0074535030871629715
        total_loss: -0.007843765430152416
        vf_explained_var: 0.00717046856880188
        vf_loss: 2.5578150749206543
    load_time_ms: 13778.063
    num_steps_sampled: 18432000
    num_steps_trained: 18432000
    sample_time_ms: 99526.324
    update_time_ms: 16.236
  iterations_since_restore: 32
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.477272727272727
    ram_util_percent: 11.449431818181816
  pid: 13408
  policy_reward_max:
    agent-0: 11.0
    agent-1: 7.0
    agent-2: 22.0
    agent-3: 18.0
    agent-4: 14.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 3.06
    agent-1: 1.58
    agent-2: 0.11
    agent-3: 4.01
    agent-4: 4.41
    agent-5: 0.5
  policy_reward_min:
    agent-0: 0.0
    agent-1: -1.0
    agent-2: -449.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -63.0
  sampler_perf:
    mean_env_wait_ms: 23.424776081005916
    mean_inference_ms: 13.06300785039767
    mean_processing_ms: 58.58530253073504
  time_since_restore: 4044.0925028324127
  time_this_iter_s: 123.53534269332886
  time_total_s: 26554.37574529648
  timestamp: 1637224000
  timesteps_since_restore: 3072000
  timesteps_this_iter: 96000
  timesteps_total: 18432000
  training_iteration: 192
  trial_id: '00000'
  
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
== Status ==
Memory usage on this node: 20.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    192 |          26554.4 | 18432000 |    13.67 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 2.28
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.82
    apples_agent-1_min: 0
    apples_agent-2_max: 18
    apples_agent-2_mean: 1.96
    apples_agent-2_min: 0
    apples_agent-3_max: 21
    apples_agent-3_mean: 2.74
    apples_agent-3_min: 0
    apples_agent-4_max: 10
    apples_agent-4_mean: 1.22
    apples_agent-4_min: 0
    apples_agent-5_max: 9
    apples_agent-5_mean: 1.02
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 170
    cleaning_beam_agent-0_mean: 79.82
    cleaning_beam_agent-0_min: 28
    cleaning_beam_agent-1_max: 483
    cleaning_beam_agent-1_mean: 294.63
    cleaning_beam_agent-1_min: 186
    cleaning_beam_agent-2_max: 96
    cleaning_beam_agent-2_mean: 16.46
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 235
    cleaning_beam_agent-3_mean: 116.45
    cleaning_beam_agent-3_min: 28
    cleaning_beam_agent-4_max: 301
    cleaning_beam_agent-4_mean: 66.32
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 29
    cleaning_beam_agent-5_mean: 7.4
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-28-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 77.0
  episode_reward_mean: 18.83
  episode_reward_min: -24.0
  episodes_this_iter: 96
  episodes_total: 18528
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11722.942
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00010984319669660181
        entropy: 0.5973506569862366
        entropy_coeff: 0.0017600000137463212
        kl: 0.006070524919778109
        model: {}
        policy_loss: -0.012824798002839088
        total_loss: -0.013231606222689152
        vf_explained_var: -0.0013564378023147583
        vf_loss: 0.37477999925613403
      agent-1:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.00010984319669660181
        entropy: 0.5214775204658508
        entropy_coeff: 0.0017600000137463212
        kl: 0.0058349547907710075
        model: {}
        policy_loss: -0.011458870023488998
        total_loss: -0.01221567764878273
        vf_explained_var: 0.03521208465099335
        vf_loss: 0.15115168690681458
      agent-2:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.00010984319669660181
        entropy: 0.49027305841445923
        entropy_coeff: 0.0017600000137463212
        kl: 0.006138097960501909
        model: {}
        policy_loss: -0.01281841192394495
        total_loss: -0.013476571068167686
        vf_explained_var: -0.00630459189414978
        vf_loss: 0.5126659870147705
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00010984319669660181
        entropy: 0.6389453411102295
        entropy_coeff: 0.0017600000137463212
        kl: 0.005982693750411272
        model: {}
        policy_loss: -0.0116349495947361
        total_loss: -0.012426595203578472
        vf_explained_var: -0.0018265992403030396
        vf_loss: 0.3376384377479553
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00010984319669660181
        entropy: 0.6902655363082886
        entropy_coeff: 0.0017600000137463212
        kl: 0.006144102197140455
        model: {}
        policy_loss: -0.009640060365200043
        total_loss: -0.010382466949522495
        vf_explained_var: 0.000883936882019043
        vf_loss: 1.6525356769561768
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00010984319669660181
        entropy: 0.7913530468940735
        entropy_coeff: 0.0017600000137463212
        kl: 0.005377271678298712
        model: {}
        policy_loss: -0.009614677168428898
        total_loss: -0.010455723851919174
        vf_explained_var: -0.006608396768569946
        vf_loss: 0.14007781445980072
    load_time_ms: 13767.274
    num_steps_sampled: 18528000
    num_steps_trained: 18528000
    sample_time_ms: 99260.13
    update_time_ms: 15.904
  iterations_since_restore: 33
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.326857142857143
    ram_util_percent: 11.351428571428569
  pid: 13408
  policy_reward_max:
    agent-0: 20.0
    agent-1: 12.0
    agent-2: 26.0
    agent-3: 12.0
    agent-4: 13.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 3.37
    agent-1: 1.88
    agent-2: 4.44
    agent-3: 3.5
    agent-4: 3.86
    agent-5: 1.78
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -42.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.411539936968254
    mean_inference_ms: 13.052046637317021
    mean_processing_ms: 58.55391556706893
  time_since_restore: 4167.14271235466
  time_this_iter_s: 123.05020952224731
  time_total_s: 26677.425954818726
  timestamp: 1637224123
  timesteps_since_restore: 3168000
  timesteps_this_iter: 96000
  timesteps_total: 18528000
  training_iteration: 193
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    193 |          26677.4 | 18528000 |    18.83 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 31
    apples_agent-0_mean: 2.78
    apples_agent-0_min: 0
    apples_agent-1_max: 17
    apples_agent-1_mean: 0.99
    apples_agent-1_min: 0
    apples_agent-2_max: 57
    apples_agent-2_mean: 2.59
    apples_agent-2_min: 0
    apples_agent-3_max: 13
    apples_agent-3_mean: 2.52
    apples_agent-3_min: 0
    apples_agent-4_max: 47
    apples_agent-4_mean: 1.47
    apples_agent-4_min: 0
    apples_agent-5_max: 12
    apples_agent-5_mean: 1.15
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 168
    cleaning_beam_agent-0_mean: 82.22
    cleaning_beam_agent-0_min: 17
    cleaning_beam_agent-1_max: 493
    cleaning_beam_agent-1_mean: 307.47
    cleaning_beam_agent-1_min: 196
    cleaning_beam_agent-2_max: 76
    cleaning_beam_agent-2_mean: 20.2
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 202
    cleaning_beam_agent-3_mean: 110.24
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 256
    cleaning_beam_agent-4_mean: 65.91
    cleaning_beam_agent-4_min: 3
    cleaning_beam_agent-5_max: 44
    cleaning_beam_agent-5_mean: 8.11
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 3
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-30-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 77.0
  episode_reward_mean: 20.43
  episode_reward_min: 2.0
  episodes_this_iter: 96
  episodes_total: 18624
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11656.3
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000103852798929438
        entropy: 0.5824711322784424
        entropy_coeff: 0.0017600000137463212
        kl: 0.0057593765668570995
        model: {}
        policy_loss: -0.013440113514661789
        total_loss: -0.013859011232852936
        vf_explained_var: -0.01014775037765503
        vf_loss: 0.30316784977912903
      agent-1:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.000103852798929438
        entropy: 0.5126121640205383
        entropy_coeff: 0.0017600000137463212
        kl: 0.005260764621198177
        model: {}
        policy_loss: -0.010923592373728752
        total_loss: -0.011682823300361633
        vf_explained_var: 0.032298117876052856
        vf_loss: 0.11448787152767181
      agent-2:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.000103852798929438
        entropy: 0.49466124176979065
        entropy_coeff: 0.0017600000137463212
        kl: 0.0060462309047579765
        model: {}
        policy_loss: -0.012232770211994648
        total_loss: -0.012876816093921661
        vf_explained_var: -0.004996702075004578
        vf_loss: 0.7540084719657898
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.000103852798929438
        entropy: 0.6297124028205872
        entropy_coeff: 0.0017600000137463212
        kl: 0.005379577167332172
        model: {}
        policy_loss: -0.010665327310562134
        total_loss: -0.01146585401147604
        vf_explained_var: 0.00857505202293396
        vf_loss: 0.3879057466983795
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.000103852798929438
        entropy: 0.6857916116714478
        entropy_coeff: 0.0017600000137463212
        kl: 0.006884584203362465
        model: {}
        policy_loss: -0.015913214534521103
        total_loss: -0.016739314422011375
        vf_explained_var: 0.008511364459991455
        vf_loss: 0.3666178286075592
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000103852798929438
        entropy: 0.7765573859214783
        entropy_coeff: 0.0017600000137463212
        kl: 0.004457011353224516
        model: {}
        policy_loss: -0.009119290858507156
        total_loss: -0.01002972200512886
        vf_explained_var: -0.0025953054428100586
        vf_loss: 0.10613057762384415
    load_time_ms: 13750.529
    num_steps_sampled: 18624000
    num_steps_trained: 18624000
    sample_time_ms: 98971.522
    update_time_ms: 15.837
  iterations_since_restore: 34
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.785142857142855
    ram_util_percent: 11.459999999999996
  pid: 13408
  policy_reward_max:
    agent-0: 20.0
    agent-1: 10.0
    agent-2: 26.0
    agent-3: 21.0
    agent-4: 13.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 3.59
    agent-1: 1.77
    agent-2: 5.16
    agent-3: 3.99
    agent-4: 4.3
    agent-5: 1.62
  policy_reward_min:
    agent-0: 0.0
    agent-1: -3.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.400276709948624
    mean_inference_ms: 13.041177640234348
    mean_processing_ms: 58.52310371567673
  time_since_restore: 4290.20606470108
  time_this_iter_s: 123.06335234642029
  time_total_s: 26800.489307165146
  timestamp: 1637224246
  timesteps_since_restore: 3264000
  timesteps_this_iter: 96000
  timesteps_total: 18624000
  training_iteration: 194
  trial_id: '00000'
  
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
== Status ==
Memory usage on this node: 20.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    194 |          26800.5 | 18624000 |    20.43 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 2.3
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.72
    apples_agent-1_min: 0
    apples_agent-2_max: 57
    apples_agent-2_mean: 2.64
    apples_agent-2_min: 0
    apples_agent-3_max: 11
    apples_agent-3_mean: 2.52
    apples_agent-3_min: 0
    apples_agent-4_max: 12
    apples_agent-4_mean: 1.26
    apples_agent-4_min: 0
    apples_agent-5_max: 16
    apples_agent-5_mean: 1.16
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 183
    cleaning_beam_agent-0_mean: 85.37
    cleaning_beam_agent-0_min: 30
    cleaning_beam_agent-1_max: 513
    cleaning_beam_agent-1_mean: 316.58
    cleaning_beam_agent-1_min: 177
    cleaning_beam_agent-2_max: 101
    cleaning_beam_agent-2_mean: 22.01
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 221
    cleaning_beam_agent-3_mean: 112.23
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 258
    cleaning_beam_agent-4_mean: 60.22
    cleaning_beam_agent-4_min: 13
    cleaning_beam_agent-5_max: 38
    cleaning_beam_agent-5_mean: 7.77
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-32-49
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 55.0
  episode_reward_mean: 17.5
  episode_reward_min: -40.0
  episodes_this_iter: 96
  episodes_total: 18720
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11607.486
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 9.786240116227418e-05
        entropy: 0.5786008834838867
        entropy_coeff: 0.0017600000137463212
        kl: 0.005164088681340218
        model: {}
        policy_loss: -0.012277108617126942
        total_loss: -0.012742819264531136
        vf_explained_var: -0.010402083396911621
        vf_loss: 0.3621716797351837
      agent-1:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 9.786240116227418e-05
        entropy: 0.528195858001709
        entropy_coeff: 0.0017600000137463212
        kl: 0.004383566789329052
        model: {}
        policy_loss: -0.00460363645106554
        total_loss: -0.0052887024357914925
        vf_explained_var: 0.0059654563665390015
        vf_loss: 1.3496832847595215
      agent-2:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 9.786240116227418e-05
        entropy: 0.5072734951972961
        entropy_coeff: 0.0017600000137463212
        kl: 0.005746622569859028
        model: {}
        policy_loss: -0.013268428854644299
        total_loss: -0.013972640968859196
        vf_explained_var: -0.014993071556091309
        vf_loss: 0.44921189546585083
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 9.786240116227418e-05
        entropy: 0.6227786540985107
        entropy_coeff: 0.0017600000137463212
        kl: 0.004469134379178286
        model: {}
        policy_loss: -0.005673866719007492
        total_loss: -0.00625219289213419
        vf_explained_var: 0.004617869853973389
        vf_loss: 2.9430792331695557
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 9.786240116227418e-05
        entropy: 0.6779029965400696
        entropy_coeff: 0.0017600000137463212
        kl: 0.006251447834074497
        model: {}
        policy_loss: -0.015049291774630547
        total_loss: -0.01589459925889969
        vf_explained_var: 0.0044029951095581055
        vf_loss: 0.35230380296707153
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 9.786240116227418e-05
        entropy: 0.7335308790206909
        entropy_coeff: 0.0017600000137463212
        kl: 0.0061337705701589584
        model: {}
        policy_loss: -0.010079443454742432
        total_loss: -0.01104949414730072
        vf_explained_var: 0.005569100379943848
        vf_loss: 0.14273378252983093
    load_time_ms: 13743.019
    num_steps_sampled: 18720000
    num_steps_trained: 18720000
    sample_time_ms: 98596.216
    update_time_ms: 15.717
  iterations_since_restore: 35
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.381142857142855
    ram_util_percent: 11.418285714285709
  pid: 13408
  policy_reward_max:
    agent-0: 20.0
    agent-1: 6.0
    agent-2: 12.0
    agent-3: 14.0
    agent-4: 13.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 3.49
    agent-1: 0.92
    agent-2: 4.53
    agent-3: 2.58
    agent-4: 4.37
    agent-5: 1.61
  policy_reward_min:
    agent-0: 0.0
    agent-1: -44.0
    agent-2: 0.0
    agent-3: -49.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.385827354543245
    mean_inference_ms: 13.032706907577433
    mean_processing_ms: 58.494058284140635
  time_since_restore: 4412.8462562561035
  time_this_iter_s: 122.6401915550232
  time_total_s: 26923.12949872017
  timestamp: 1637224369
  timesteps_since_restore: 3360000
  timesteps_this_iter: 96000
  timesteps_total: 18720000
  training_iteration: 195
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    195 |          26923.1 | 18720000 |     17.5 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 66
    apples_agent-0_mean: 3.33
    apples_agent-0_min: 0
    apples_agent-1_max: 64
    apples_agent-1_mean: 1.56
    apples_agent-1_min: 0
    apples_agent-2_max: 13
    apples_agent-2_mean: 2.14
    apples_agent-2_min: 0
    apples_agent-3_max: 47
    apples_agent-3_mean: 3.33
    apples_agent-3_min: 0
    apples_agent-4_max: 71
    apples_agent-4_mean: 2.3
    apples_agent-4_min: 0
    apples_agent-5_max: 9
    apples_agent-5_mean: 1.19
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 192
    cleaning_beam_agent-0_mean: 81.91
    cleaning_beam_agent-0_min: 26
    cleaning_beam_agent-1_max: 424
    cleaning_beam_agent-1_mean: 299.83
    cleaning_beam_agent-1_min: 173
    cleaning_beam_agent-2_max: 72
    cleaning_beam_agent-2_mean: 21.41
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 211
    cleaning_beam_agent-3_mean: 113.81
    cleaning_beam_agent-3_min: 32
    cleaning_beam_agent-4_max: 163
    cleaning_beam_agent-4_mean: 70.01
    cleaning_beam_agent-4_min: 14
    cleaning_beam_agent-5_max: 26
    cleaning_beam_agent-5_mean: 5.85
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-34-52
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 55.0
  episode_reward_mean: 20.02
  episode_reward_min: -54.0
  episodes_this_iter: 96
  episodes_total: 18816
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11552.096
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 9.187200339511037e-05
        entropy: 0.5737806558609009
        entropy_coeff: 0.0017600000137463212
        kl: 0.005383044946938753
        model: {}
        policy_loss: -0.012004164978861809
        total_loss: -0.01244282629340887
        vf_explained_var: -0.011948496103286743
        vf_loss: 0.32891982793807983
      agent-1:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 9.187200339511037e-05
        entropy: 0.5203235745429993
        entropy_coeff: 0.0017600000137463212
        kl: 0.00406179204583168
        model: {}
        policy_loss: -0.005857957527041435
        total_loss: -0.006600722670555115
        vf_explained_var: 0.00713716447353363
        vf_loss: 1.2223408222198486
      agent-2:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 9.187200339511037e-05
        entropy: 0.5025355815887451
        entropy_coeff: 0.0017600000137463212
        kl: 0.005250345449894667
        model: {}
        policy_loss: -0.008410327136516571
        total_loss: -0.008995701558887959
        vf_explained_var: 0.00216539204120636
        vf_loss: 1.6782829761505127
      agent-3:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 9.187200339511037e-05
        entropy: 0.633798360824585
        entropy_coeff: 0.0017600000137463212
        kl: 0.005366750061511993
        model: {}
        policy_loss: -0.009942968375980854
        total_loss: -0.010882840491831303
        vf_explained_var: -0.003301382064819336
        vf_loss: 0.4144544005393982
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 9.187200339511037e-05
        entropy: 0.6699726581573486
        entropy_coeff: 0.0017600000137463212
        kl: 0.0057386187836527824
        model: {}
        policy_loss: -0.013437200337648392
        total_loss: -0.014272266998887062
        vf_explained_var: 0.016741588711738586
        vf_loss: 0.5715311169624329
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 9.187200339511037e-05
        entropy: 0.7254222631454468
        entropy_coeff: 0.0017600000137463212
        kl: 0.004872253630310297
        model: {}
        policy_loss: -0.008665156550705433
        total_loss: -0.00968275684863329
        vf_explained_var: 0.010658994317054749
        vf_loss: 0.15530553460121155
    load_time_ms: 13703.525
    num_steps_sampled: 18816000
    num_steps_trained: 18816000
    sample_time_ms: 98271.92
    update_time_ms: 15.88
  iterations_since_restore: 36
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.248571428571427
    ram_util_percent: 11.337142857142855
  pid: 13408
  policy_reward_max:
    agent-0: 12.0
    agent-1: 9.0
    agent-2: 18.0
    agent-3: 16.0
    agent-4: 22.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 3.76
    agent-1: 1.46
    agent-2: 4.22
    agent-3: 3.57
    agent-4: 5.05
    agent-5: 1.96
  policy_reward_min:
    agent-0: 0.0
    agent-1: -42.0
    agent-2: -49.0
    agent-3: -45.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.371245412378716
    mean_inference_ms: 13.021320605600486
    mean_processing_ms: 58.461098889427056
  time_since_restore: 4535.495587587357
  time_this_iter_s: 122.64933133125305
  time_total_s: 27045.778830051422
  timestamp: 1637224492
  timesteps_since_restore: 3456000
  timesteps_this_iter: 96000
  timesteps_total: 18816000
  training_iteration: 196
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    196 |          27045.8 | 18816000 |    20.02 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 90
    apples_agent-0_mean: 2.93
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.83
    apples_agent-1_min: 0
    apples_agent-2_max: 25
    apples_agent-2_mean: 2.02
    apples_agent-2_min: 0
    apples_agent-3_max: 10
    apples_agent-3_mean: 2.35
    apples_agent-3_min: 0
    apples_agent-4_max: 16
    apples_agent-4_mean: 1.35
    apples_agent-4_min: 0
    apples_agent-5_max: 16
    apples_agent-5_mean: 1.06
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 269
    cleaning_beam_agent-0_mean: 86.18
    cleaning_beam_agent-0_min: 24
    cleaning_beam_agent-1_max: 483
    cleaning_beam_agent-1_mean: 298.19
    cleaning_beam_agent-1_min: 176
    cleaning_beam_agent-2_max: 57
    cleaning_beam_agent-2_mean: 21.03
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 229
    cleaning_beam_agent-3_mean: 110.15
    cleaning_beam_agent-3_min: 31
    cleaning_beam_agent-4_max: 167
    cleaning_beam_agent-4_mean: 64.36
    cleaning_beam_agent-4_min: 16
    cleaning_beam_agent-5_max: 54
    cleaning_beam_agent-5_mean: 7.96
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-36-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 53.0
  episode_reward_mean: 18.7
  episode_reward_min: -39.0
  episodes_this_iter: 96
  episodes_total: 18912
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11492.285
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 8.588159835198894e-05
        entropy: 0.5857469439506531
        entropy_coeff: 0.0017600000137463212
        kl: 0.0050626834854483604
        model: {}
        policy_loss: -0.012228095903992653
        total_loss: -0.012719910591840744
        vf_explained_var: -0.009423226118087769
        vf_loss: 0.32830819487571716
      agent-1:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 8.588159835198894e-05
        entropy: 0.4957176744937897
        entropy_coeff: 0.0017600000137463212
        kl: 0.005067914724349976
        model: {}
        policy_loss: -0.00967281311750412
        total_loss: -0.010501354932785034
        vf_explained_var: 0.02477329969406128
        vf_loss: 0.12243463844060898
      agent-2:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 8.588159835198894e-05
        entropy: 0.5026609301567078
        entropy_coeff: 0.0017600000137463212
        kl: 0.005500119179487228
        model: {}
        policy_loss: -0.012704719789326191
        total_loss: -0.013406166806817055
        vf_explained_var: 0.0028351694345474243
        vf_loss: 0.45734208822250366
      agent-3:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 8.588159835198894e-05
        entropy: 0.6251396536827087
        entropy_coeff: 0.0017600000137463212
        kl: 0.004784770309925079
        model: {}
        policy_loss: -0.009864590130746365
        total_loss: -0.01080730464309454
        vf_explained_var: 0.0010956227779388428
        vf_loss: 0.37910690903663635
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 8.588159835198894e-05
        entropy: 0.6817938089370728
        entropy_coeff: 0.0017600000137463212
        kl: 0.0049543860368430614
        model: {}
        policy_loss: -0.008898813277482986
        total_loss: -0.009675628505647182
        vf_explained_var: 0.01062871515750885
        vf_loss: 1.7542307376861572
      agent-5:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 8.588159835198894e-05
        entropy: 0.7463490962982178
        entropy_coeff: 0.0017600000137463212
        kl: 0.00586589565500617
        model: {}
        policy_loss: -0.006615623831748962
        total_loss: -0.007636296562850475
        vf_explained_var: 0.006545901298522949
        vf_loss: 1.4625325202941895
    load_time_ms: 13679.838
    num_steps_sampled: 18912000
    num_steps_trained: 18912000
    sample_time_ms: 98105.577
    update_time_ms: 15.986
  iterations_since_restore: 37
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.108522727272728
    ram_util_percent: 11.489772727272726
  pid: 13408
  policy_reward_max:
    agent-0: 17.0
    agent-1: 8.0
    agent-2: 16.0
    agent-3: 12.0
    agent-4: 15.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 3.25
    agent-1: 1.56
    agent-2: 4.51
    agent-3: 3.82
    agent-4: 4.23
    agent-5: 1.33
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -45.0
    agent-5: -50.0
  sampler_perf:
    mean_env_wait_ms: 23.361637555388047
    mean_inference_ms: 13.01180954140467
    mean_processing_ms: 58.435485348309456
  time_since_restore: 4659.156841039658
  time_this_iter_s: 123.66125345230103
  time_total_s: 27169.440083503723
  timestamp: 1637224615
  timesteps_since_restore: 3552000
  timesteps_this_iter: 96000
  timesteps_total: 18912000
  training_iteration: 197
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    197 |          27169.4 | 18912000 |     18.7 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 2.2
    apples_agent-0_min: 0
    apples_agent-1_max: 23
    apples_agent-1_mean: 1.11
    apples_agent-1_min: 0
    apples_agent-2_max: 10
    apples_agent-2_mean: 1.72
    apples_agent-2_min: 0
    apples_agent-3_max: 12
    apples_agent-3_mean: 2.44
    apples_agent-3_min: 0
    apples_agent-4_max: 28
    apples_agent-4_mean: 1.25
    apples_agent-4_min: 0
    apples_agent-5_max: 14
    apples_agent-5_mean: 1.26
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 174
    cleaning_beam_agent-0_mean: 82.68
    cleaning_beam_agent-0_min: 27
    cleaning_beam_agent-1_max: 427
    cleaning_beam_agent-1_mean: 296.29
    cleaning_beam_agent-1_min: 185
    cleaning_beam_agent-2_max: 75
    cleaning_beam_agent-2_mean: 20.45
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 225
    cleaning_beam_agent-3_mean: 114.23
    cleaning_beam_agent-3_min: 49
    cleaning_beam_agent-4_max: 231
    cleaning_beam_agent-4_mean: 61.62
    cleaning_beam_agent-4_min: 16
    cleaning_beam_agent-5_max: 57
    cleaning_beam_agent-5_mean: 9.92
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-38-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 77.0
  episode_reward_mean: 19.59
  episode_reward_min: 4.0
  episodes_this_iter: 96
  episodes_total: 19008
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11491.732
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 7.989120058482513e-05
        entropy: 0.592555046081543
        entropy_coeff: 0.0017600000137463212
        kl: 0.004849262069910765
        model: {}
        policy_loss: -0.011326410807669163
        total_loss: -0.011849509552121162
        vf_explained_var: -0.00879448652267456
        vf_loss: 0.34874361753463745
      agent-1:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 7.989120058482513e-05
        entropy: 0.5128723382949829
        entropy_coeff: 0.0017600000137463212
        kl: 0.00466020405292511
        model: {}
        policy_loss: -0.009340304881334305
        total_loss: -0.010198161005973816
        vf_explained_var: 0.025438949465751648
        vf_loss: 0.1567554473876953
      agent-2:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 7.989120058482513e-05
        entropy: 0.48807305097579956
        entropy_coeff: 0.0017600000137463212
        kl: 0.004568910226225853
        model: {}
        policy_loss: -0.010604639537632465
        total_loss: -0.011309286579489708
        vf_explained_var: -0.00031413137912750244
        vf_loss: 0.4013819098472595
      agent-3:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 7.989120058482513e-05
        entropy: 0.6203960180282593
        entropy_coeff: 0.0017600000137463212
        kl: 0.00553310289978981
        model: {}
        policy_loss: -0.008958711288869381
        total_loss: -0.009937938302755356
        vf_explained_var: -0.002741292119026184
        vf_loss: 0.4350937008857727
      agent-4:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 7.989120058482513e-05
        entropy: 0.6700509786605835
        entropy_coeff: 0.0017600000137463212
        kl: 0.00570181617513299
        model: {}
        policy_loss: -0.011885846965014935
        total_loss: -0.012868361547589302
        vf_explained_var: 0.011665821075439453
        vf_loss: 0.5422965288162231
      agent-5:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 7.989120058482513e-05
        entropy: 0.8022435903549194
        entropy_coeff: 0.0017600000137463212
        kl: 0.005868903826922178
        model: {}
        policy_loss: -0.008748662658035755
        total_loss: -0.00999840721487999
        vf_explained_var: 0.0009391605854034424
        vf_loss: 0.15480025112628937
    load_time_ms: 13658.94
    num_steps_sampled: 19008000
    num_steps_trained: 19008000
    sample_time_ms: 97750.68
    update_time_ms: 15.6
  iterations_since_restore: 38
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.731213872832374
    ram_util_percent: 11.420231213872833
  pid: 13408
  policy_reward_max:
    agent-0: 17.0
    agent-1: 12.0
    agent-2: 19.0
    agent-3: 19.0
    agent-4: 21.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 3.19
    agent-1: 1.88
    agent-2: 4.01
    agent-3: 3.73
    agent-4: 4.81
    agent-5: 1.97
  policy_reward_min:
    agent-0: -1.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.350352007374617
    mean_inference_ms: 13.001765895666445
    mean_processing_ms: 58.40452959726512
  time_since_restore: 4781.143860340118
  time_this_iter_s: 121.98701930046082
  time_total_s: 27291.427102804184
  timestamp: 1637224738
  timesteps_since_restore: 3648000
  timesteps_this_iter: 96000
  timesteps_total: 19008000
  training_iteration: 198
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    198 |          27291.4 | 19008000 |    19.59 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 8
    apples_agent-0_mean: 2.0
    apples_agent-0_min: 0
    apples_agent-1_max: 12
    apples_agent-1_mean: 1.0
    apples_agent-1_min: 0
    apples_agent-2_max: 9
    apples_agent-2_mean: 1.72
    apples_agent-2_min: 0
    apples_agent-3_max: 12
    apples_agent-3_mean: 2.5
    apples_agent-3_min: 0
    apples_agent-4_max: 11
    apples_agent-4_mean: 1.27
    apples_agent-4_min: 0
    apples_agent-5_max: 38
    apples_agent-5_mean: 1.3
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 142
    cleaning_beam_agent-0_mean: 77.55
    cleaning_beam_agent-0_min: 26
    cleaning_beam_agent-1_max: 414
    cleaning_beam_agent-1_mean: 284.0
    cleaning_beam_agent-1_min: 186
    cleaning_beam_agent-2_max: 80
    cleaning_beam_agent-2_mean: 18.4
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 161
    cleaning_beam_agent-3_mean: 95.8
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 199
    cleaning_beam_agent-4_mean: 54.31
    cleaning_beam_agent-4_min: 10
    cleaning_beam_agent-5_max: 56
    cleaning_beam_agent-5_mean: 8.16
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-41-00
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 42.0
  episode_reward_mean: 17.47
  episode_reward_min: -49.0
  episodes_this_iter: 96
  episodes_total: 19104
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11496.595
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 7.390080281766132e-05
        entropy: 0.5820271372795105
        entropy_coeff: 0.0017600000137463212
        kl: 0.005573009140789509
        model: {}
        policy_loss: -0.006787442602217197
        total_loss: -0.00737784244120121
        vf_explained_var: -0.0006782263517379761
        vf_loss: 1.553149938583374
      agent-1:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 7.390080281766132e-05
        entropy: 0.49327921867370605
        entropy_coeff: 0.0017600000137463212
        kl: 0.004544765222817659
        model: {}
        policy_loss: -0.008607454597949982
        total_loss: -0.009451700374484062
        vf_explained_var: 0.03333912789821625
        vf_loss: 0.09721778333187103
      agent-2:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 7.390080281766132e-05
        entropy: 0.47836774587631226
        entropy_coeff: 0.0017600000137463212
        kl: 0.005590883549302816
        model: {}
        policy_loss: -0.011905860155820847
        total_loss: -0.0126432990655303
        vf_explained_var: -0.008605599403381348
        vf_loss: 0.34598344564437866
      agent-3:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 7.390080281766132e-05
        entropy: 0.5988322496414185
        entropy_coeff: 0.0017600000137463212
        kl: 0.004673694260418415
        model: {}
        policy_loss: -0.009388528764247894
        total_loss: -0.010353775694966316
        vf_explained_var: 0.004765793681144714
        vf_loss: 0.30276715755462646
      agent-4:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 7.390080281766132e-05
        entropy: 0.6753241419792175
        entropy_coeff: 0.0017600000137463212
        kl: 0.004886976908892393
        model: {}
        policy_loss: -0.008042706176638603
        total_loss: -0.008999498561024666
        vf_explained_var: 4.743039608001709e-05
        vf_loss: 1.0960415601730347
      agent-5:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 7.390080281766132e-05
        entropy: 0.7873631715774536
        entropy_coeff: 0.0017600000137463212
        kl: 0.005503783002495766
        model: {}
        policy_loss: -0.00801653042435646
        total_loss: -0.009250730276107788
        vf_explained_var: 0.013591989874839783
        vf_loss: 0.13965526223182678
    load_time_ms: 13654.242
    num_steps_sampled: 19104000
    num_steps_trained: 19104000
    sample_time_ms: 97657.795
    update_time_ms: 15.44
  iterations_since_restore: 39
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.44685714285714
    ram_util_percent: 11.506285714285713
  pid: 13408
  policy_reward_max:
    agent-0: 9.0
    agent-1: 7.0
    agent-2: 16.0
    agent-3: 12.0
    agent-4: 13.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 2.43
    agent-1: 1.63
    agent-2: 4.0
    agent-3: 3.66
    agent-4: 4.15
    agent-5: 1.6
  policy_reward_min:
    agent-0: -48.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -50.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.334737199863188
    mean_inference_ms: 12.991729276297269
    mean_processing_ms: 58.374162480209904
  time_since_restore: 4903.616363763809
  time_this_iter_s: 122.4725034236908
  time_total_s: 27413.899606227875
  timestamp: 1637224860
  timesteps_since_restore: 3744000
  timesteps_this_iter: 96000
  timesteps_total: 19104000
  training_iteration: 199
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    199 |          27413.9 | 19104000 |    17.47 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 6
    apples_agent-0_mean: 1.83
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.79
    apples_agent-1_min: 0
    apples_agent-2_max: 21
    apples_agent-2_mean: 2.2
    apples_agent-2_min: 0
    apples_agent-3_max: 25
    apples_agent-3_mean: 2.84
    apples_agent-3_min: 0
    apples_agent-4_max: 16
    apples_agent-4_mean: 1.33
    apples_agent-4_min: 0
    apples_agent-5_max: 8
    apples_agent-5_mean: 1.25
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 164
    cleaning_beam_agent-0_mean: 79.59
    cleaning_beam_agent-0_min: 25
    cleaning_beam_agent-1_max: 515
    cleaning_beam_agent-1_mean: 304.56
    cleaning_beam_agent-1_min: 198
    cleaning_beam_agent-2_max: 71
    cleaning_beam_agent-2_mean: 17.11
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 219
    cleaning_beam_agent-3_mean: 102.32
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 203
    cleaning_beam_agent-4_mean: 55.38
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 64
    cleaning_beam_agent-5_mean: 9.73
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-43-02
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 54.0
  episode_reward_mean: 18.78
  episode_reward_min: 3.0
  episodes_this_iter: 96
  episodes_total: 19200
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11504.318
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 6.791039777453989e-05
        entropy: 0.577124297618866
        entropy_coeff: 0.0017600000137463212
        kl: 0.00451279804110527
        model: {}
        policy_loss: -0.010760915465652943
        total_loss: -0.011529583483934402
        vf_explained_var: -0.017621546983718872
        vf_loss: 0.21429993212223053
      agent-1:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 6.791039777453989e-05
        entropy: 0.501427412033081
        entropy_coeff: 0.0017600000137463212
        kl: 0.004214544780552387
        model: {}
        policy_loss: -0.008019557222723961
        total_loss: -0.008884424343705177
        vf_explained_var: 0.02612312138080597
        vf_loss: 0.11062140762805939
      agent-2:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 6.791039777453989e-05
        entropy: 0.4775879979133606
        entropy_coeff: 0.0017600000137463212
        kl: 0.0047177476808428764
        model: {}
        policy_loss: -0.010656077414751053
        total_loss: -0.01139295194298029
        vf_explained_var: -0.010876655578613281
        vf_loss: 0.44707953929901123
      agent-3:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 6.791039777453989e-05
        entropy: 0.5955369472503662
        entropy_coeff: 0.0017600000137463212
        kl: 0.004550276789814234
        model: {}
        policy_loss: -0.008574341423809528
        total_loss: -0.009567559696733952
        vf_explained_var: 0.005622640252113342
        vf_loss: 0.26486819982528687
      agent-4:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 6.791039777453989e-05
        entropy: 0.6736533641815186
        entropy_coeff: 0.0017600000137463212
        kl: 0.004929169546812773
        model: {}
        policy_loss: -0.012012617662549019
        total_loss: -0.01309028547257185
        vf_explained_var: 0.013019442558288574
        vf_loss: 0.46345850825309753
      agent-5:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 6.791039777453989e-05
        entropy: 0.8022507429122925
        entropy_coeff: 0.0017600000137463212
        kl: 0.005036436952650547
        model: {}
        policy_loss: -0.007617567665874958
        total_loss: -0.008886607363820076
        vf_explained_var: 0.006869882345199585
        vf_loss: 0.17007333040237427
    load_time_ms: 13632.493
    num_steps_sampled: 19200000
    num_steps_trained: 19200000
    sample_time_ms: 97713.977
    update_time_ms: 15.483
  iterations_since_restore: 40
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.559770114942527
    ram_util_percent: 11.372413793103446
  pid: 13408
  policy_reward_max:
    agent-0: 8.0
    agent-1: 7.0
    agent-2: 15.0
    agent-3: 16.0
    agent-4: 17.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 2.86
    agent-1: 1.53
    agent-2: 4.47
    agent-3: 3.42
    agent-4: 4.47
    agent-5: 2.03
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.32153158121218
    mean_inference_ms: 12.982629370787965
    mean_processing_ms: 58.34767241291158
  time_since_restore: 5025.812219619751
  time_this_iter_s: 122.19585585594177
  time_total_s: 27536.095462083817
  timestamp: 1637224982
  timesteps_since_restore: 3840000
  timesteps_this_iter: 96000
  timesteps_total: 19200000
  training_iteration: 200
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    200 |          27536.1 | 19200000 |    18.78 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 2.54
    apples_agent-0_min: 0
    apples_agent-1_max: 24
    apples_agent-1_mean: 1.21
    apples_agent-1_min: 0
    apples_agent-2_max: 14
    apples_agent-2_mean: 1.85
    apples_agent-2_min: 0
    apples_agent-3_max: 15
    apples_agent-3_mean: 2.49
    apples_agent-3_min: 0
    apples_agent-4_max: 16
    apples_agent-4_mean: 1.21
    apples_agent-4_min: 0
    apples_agent-5_max: 8
    apples_agent-5_mean: 1.05
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 177
    cleaning_beam_agent-0_mean: 74.24
    cleaning_beam_agent-0_min: 25
    cleaning_beam_agent-1_max: 444
    cleaning_beam_agent-1_mean: 296.19
    cleaning_beam_agent-1_min: 195
    cleaning_beam_agent-2_max: 94
    cleaning_beam_agent-2_mean: 16.12
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 227
    cleaning_beam_agent-3_mean: 103.33
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 167
    cleaning_beam_agent-4_mean: 56.25
    cleaning_beam_agent-4_min: 8
    cleaning_beam_agent-5_max: 63
    cleaning_beam_agent-5_mean: 7.79
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-45-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 44.0
  episode_reward_mean: 19.5
  episode_reward_min: 3.0
  episodes_this_iter: 96
  episodes_total: 19296
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11485.442
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 6.192000000737607e-05
        entropy: 0.5762439966201782
        entropy_coeff: 0.0017600000137463212
        kl: 0.004612700082361698
        model: {}
        policy_loss: -0.01070291455835104
        total_loss: -0.011572793126106262
        vf_explained_var: -0.005508556962013245
        vf_loss: 0.2899232506752014
      agent-1:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 6.192000000737607e-05
        entropy: 0.4948391914367676
        entropy_coeff: 0.0017600000137463212
        kl: 0.003862183541059494
        model: {}
        policy_loss: -0.008049102500081062
        total_loss: -0.00890419166535139
        vf_explained_var: 0.026737138628959656
        vf_loss: 0.12813927233219147
      agent-2:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 6.192000000737607e-05
        entropy: 0.4780992865562439
        entropy_coeff: 0.0017600000137463212
        kl: 0.004437710624188185
        model: {}
        policy_loss: -0.010226307436823845
        total_loss: -0.011001751758158207
        vf_explained_var: -0.0030443966388702393
        vf_loss: 0.3827156126499176
      agent-3:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 6.192000000737607e-05
        entropy: 0.6045243740081787
        entropy_coeff: 0.0017600000137463212
        kl: 0.004142626188695431
        model: {}
        policy_loss: -0.008555150590837002
        total_loss: -0.009574166499078274
        vf_explained_var: 0.0023715347051620483
        vf_loss: 0.32004475593566895
      agent-4:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 6.192000000737607e-05
        entropy: 0.6773790121078491
        entropy_coeff: 0.0017600000137463212
        kl: 0.005268198437988758
        model: {}
        policy_loss: -0.011666868813335896
        total_loss: -0.012788362801074982
        vf_explained_var: 0.01361452043056488
        vf_loss: 0.3776869773864746
      agent-5:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 6.192000000737607e-05
        entropy: 0.791232705116272
        entropy_coeff: 0.0017600000137463212
        kl: 0.004626643378287554
        model: {}
        policy_loss: -0.00789555348455906
        total_loss: -0.009158868342638016
        vf_explained_var: 0.013679802417755127
        vf_loss: 0.1358615607023239
    load_time_ms: 13600.553
    num_steps_sampled: 19296000
    num_steps_trained: 19296000
    sample_time_ms: 97599.667
    update_time_ms: 15.539
  iterations_since_restore: 41
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.296571428571426
    ram_util_percent: 11.425142857142852
  pid: 13408
  policy_reward_max:
    agent-0: 10.0
    agent-1: 7.0
    agent-2: 20.0
    agent-3: 12.0
    agent-4: 17.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 3.5
    agent-1: 1.81
    agent-2: 4.26
    agent-3: 3.67
    agent-4: 4.41
    agent-5: 1.85
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.308920884050448
    mean_inference_ms: 12.974213798232112
    mean_processing_ms: 58.32783886240381
  time_since_restore: 5148.43893289566
  time_this_iter_s: 122.62671327590942
  time_total_s: 27658.722175359726
  timestamp: 1637225106
  timesteps_since_restore: 3936000
  timesteps_this_iter: 96000
  timesteps_total: 19296000
  training_iteration: 201
  trial_id: '00000'
  
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
== Status ==
Memory usage on this node: 20.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    201 |          27658.7 | 19296000 |     19.5 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 2.29
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.6
    apples_agent-1_min: 0
    apples_agent-2_max: 45
    apples_agent-2_mean: 2.38
    apples_agent-2_min: 0
    apples_agent-3_max: 26
    apples_agent-3_mean: 2.78
    apples_agent-3_min: 0
    apples_agent-4_max: 9
    apples_agent-4_mean: 0.81
    apples_agent-4_min: 0
    apples_agent-5_max: 28
    apples_agent-5_mean: 1.33
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 185
    cleaning_beam_agent-0_mean: 71.96
    cleaning_beam_agent-0_min: 22
    cleaning_beam_agent-1_max: 405
    cleaning_beam_agent-1_mean: 302.6
    cleaning_beam_agent-1_min: 189
    cleaning_beam_agent-2_max: 61
    cleaning_beam_agent-2_mean: 17.9
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 248
    cleaning_beam_agent-3_mean: 98.86
    cleaning_beam_agent-3_min: 24
    cleaning_beam_agent-4_max: 125
    cleaning_beam_agent-4_mean: 49.73
    cleaning_beam_agent-4_min: 3
    cleaning_beam_agent-5_max: 52
    cleaning_beam_agent-5_mean: 6.29
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-47-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 54.0
  episode_reward_mean: 17.97
  episode_reward_min: -38.0
  episodes_this_iter: 96
  episodes_total: 19392
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11487.157
    learner:
      agent-0:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 5.5929598602233455e-05
        entropy: 0.5824113488197327
        entropy_coeff: 0.0017600000137463212
        kl: 0.004613208118826151
        model: {}
        policy_loss: -0.010002571158111095
        total_loss: -0.01094568707048893
        vf_explained_var: -0.0024346262216567993
        vf_loss: 0.24263864755630493
      agent-1:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 5.5929598602233455e-05
        entropy: 0.4870033860206604
        entropy_coeff: 0.0017600000137463212
        kl: 0.003620614530518651
        model: {}
        policy_loss: -0.007153320126235485
        total_loss: -0.00800035148859024
        vf_explained_var: 0.0330742746591568
        vf_loss: 0.08683773130178452
      agent-2:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 5.5929598602233455e-05
        entropy: 0.48599866032600403
        entropy_coeff: 0.0017600000137463212
        kl: 0.004539381712675095
        model: {}
        policy_loss: -0.009652604348957539
        total_loss: -0.010444281622767448
        vf_explained_var: 0.011897936463356018
        vf_loss: 0.4949581027030945
      agent-3:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 5.5929598602233455e-05
        entropy: 0.5848347544670105
        entropy_coeff: 0.0017600000137463212
        kl: 0.003584174904972315
        model: {}
        policy_loss: -0.0064272331073880196
        total_loss: -0.007408898323774338
        vf_explained_var: 0.0009351223707199097
        vf_loss: 0.420426607131958
      agent-4:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 5.5929598602233455e-05
        entropy: 0.6670743823051453
        entropy_coeff: 0.0017600000137463212
        kl: 0.004751451779156923
        model: {}
        policy_loss: -0.010675964877009392
        total_loss: -0.011786612682044506
        vf_explained_var: 0.010167613625526428
        vf_loss: 0.3370916247367859
      agent-5:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 5.5929598602233455e-05
        entropy: 0.7907496094703674
        entropy_coeff: 0.0017600000137463212
        kl: 0.0048613110557198524
        model: {}
        policy_loss: -0.007530586794018745
        total_loss: -0.008849691599607468
        vf_explained_var: 0.01510930061340332
        vf_loss: 0.11850892752408981
    load_time_ms: 13607.892
    num_steps_sampled: 19392000
    num_steps_trained: 19392000
    sample_time_ms: 97503.473
    update_time_ms: 15.333
  iterations_since_restore: 42
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.251428571428573
    ram_util_percent: 11.431999999999997
  pid: 13408
  policy_reward_max:
    agent-0: 10.0
    agent-1: 6.0
    agent-2: 26.0
    agent-3: 13.0
    agent-4: 11.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 3.2
    agent-1: 1.3
    agent-2: 4.53
    agent-3: 3.3
    agent-4: 3.88
    agent-5: 1.76
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -49.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.298561303201794
    mean_inference_ms: 12.9664119318809
    mean_processing_ms: 58.30139249356028
  time_since_restore: 5271.1018879413605
  time_this_iter_s: 122.66295504570007
  time_total_s: 27781.385130405426
  timestamp: 1637225228
  timesteps_since_restore: 4032000
  timesteps_this_iter: 96000
  timesteps_total: 19392000
  training_iteration: 202
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    202 |          27781.4 | 19392000 |    17.97 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 2.53
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 0.94
    apples_agent-1_min: 0
    apples_agent-2_max: 37
    apples_agent-2_mean: 2.65
    apples_agent-2_min: 0
    apples_agent-3_max: 18
    apples_agent-3_mean: 2.73
    apples_agent-3_min: 0
    apples_agent-4_max: 6
    apples_agent-4_mean: 0.94
    apples_agent-4_min: 0
    apples_agent-5_max: 28
    apples_agent-5_mean: 1.61
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 165
    cleaning_beam_agent-0_mean: 76.76
    cleaning_beam_agent-0_min: 19
    cleaning_beam_agent-1_max: 499
    cleaning_beam_agent-1_mean: 304.28
    cleaning_beam_agent-1_min: 145
    cleaning_beam_agent-2_max: 69
    cleaning_beam_agent-2_mean: 15.78
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 204
    cleaning_beam_agent-3_mean: 92.14
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 351
    cleaning_beam_agent-4_mean: 63.54
    cleaning_beam_agent-4_min: 14
    cleaning_beam_agent-5_max: 35
    cleaning_beam_agent-5_mean: 7.31
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-49-11
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 79.0
  episode_reward_mean: 19.82
  episode_reward_min: -146.0
  episodes_this_iter: 96
  episodes_total: 19488
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11479.416
    learner:
      agent-0:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 4.993920083506964e-05
        entropy: 0.5902761220932007
        entropy_coeff: 0.0017600000137463212
        kl: 0.004359179642051458
        model: {}
        policy_loss: -0.009210456162691116
        total_loss: -0.01019132137298584
        vf_explained_var: -0.014675140380859375
        vf_loss: 0.30777719616889954
      agent-1:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 4.993920083506964e-05
        entropy: 0.48739707469940186
        entropy_coeff: 0.0017600000137463212
        kl: 0.0043075354769825935
        model: {}
        policy_loss: -0.005011848174035549
        total_loss: -0.005712328478693962
        vf_explained_var: 0.015258222818374634
        vf_loss: 1.564988136291504
      agent-2:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 4.993920083506964e-05
        entropy: 0.4737051725387573
        entropy_coeff: 0.0017600000137463212
        kl: 0.003428741591051221
        model: {}
        policy_loss: -0.004751334898173809
        total_loss: -0.005383227486163378
        vf_explained_var: 0.0012118369340896606
        vf_loss: 1.9647026062011719
      agent-3:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 4.993920083506964e-05
        entropy: 0.5851976275444031
        entropy_coeff: 0.0017600000137463212
        kl: 0.003567399922758341
        model: {}
        policy_loss: -0.007009527646005154
        total_loss: -0.007989361882209778
        vf_explained_var: 0.00016373395919799805
        vf_loss: 0.4732822775840759
      agent-4:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 4.993920083506964e-05
        entropy: 0.6744692921638489
        entropy_coeff: 0.0017600000137463212
        kl: 0.004409993998706341
        model: {}
        policy_loss: -0.0062102219089865685
        total_loss: -0.0072088344022631645
        vf_explained_var: 0.0040467530488967896
        vf_loss: 1.7467443943023682
      agent-5:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 4.993920083506964e-05
        entropy: 0.8106526732444763
        entropy_coeff: 0.0017600000137463212
        kl: 0.004226993769407272
        model: {}
        policy_loss: -0.007040549535304308
        total_loss: -0.00842285342514515
        vf_explained_var: 0.005830585956573486
        vf_loss: 0.18026000261306763
    load_time_ms: 13584.286
    num_steps_sampled: 19488000
    num_steps_trained: 19488000
    sample_time_ms: 97459.163
    update_time_ms: 15.327
  iterations_since_restore: 43
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.41494252873563
    ram_util_percent: 11.38390804597701
  pid: 13408
  policy_reward_max:
    agent-0: 13.0
    agent-1: 12.0
    agent-2: 27.0
    agent-3: 20.0
    agent-4: 19.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 3.43
    agent-1: 1.81
    agent-2: 4.97
    agent-3: 3.58
    agent-4: 3.85
    agent-5: 2.18
  policy_reward_min:
    agent-0: 0.0
    agent-1: -50.0
    agent-2: -48.0
    agent-3: -47.0
    agent-4: -49.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.28874934796379
    mean_inference_ms: 12.957150339685994
    mean_processing_ms: 58.28449202091737
  time_since_restore: 5393.397385597229
  time_this_iter_s: 122.29549765586853
  time_total_s: 27903.680628061295
  timestamp: 1637225351
  timesteps_since_restore: 4128000
  timesteps_this_iter: 96000
  timesteps_total: 19488000
  training_iteration: 203
  trial_id: '00000'
  
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
== Status ==
Memory usage on this node: 20.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    203 |          27903.7 | 19488000 |    19.82 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 2.42
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.81
    apples_agent-1_min: 0
    apples_agent-2_max: 15
    apples_agent-2_mean: 2.1
    apples_agent-2_min: 0
    apples_agent-3_max: 36
    apples_agent-3_mean: 2.42
    apples_agent-3_min: 0
    apples_agent-4_max: 59
    apples_agent-4_mean: 2.38
    apples_agent-4_min: 0
    apples_agent-5_max: 41
    apples_agent-5_mean: 1.85
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 172
    cleaning_beam_agent-0_mean: 73.18
    cleaning_beam_agent-0_min: 22
    cleaning_beam_agent-1_max: 443
    cleaning_beam_agent-1_mean: 305.93
    cleaning_beam_agent-1_min: 181
    cleaning_beam_agent-2_max: 55
    cleaning_beam_agent-2_mean: 16.75
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 273
    cleaning_beam_agent-3_mean: 106.64
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 130
    cleaning_beam_agent-4_mean: 58.99
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 29
    cleaning_beam_agent-5_mean: 6.11
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-51-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 55.0
  episode_reward_mean: 20.38
  episode_reward_min: 1.0
  episodes_this_iter: 96
  episodes_total: 19584
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11493.329
    learner:
      agent-0:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 4.394879942992702e-05
        entropy: 0.5691844820976257
        entropy_coeff: 0.0017600000137463212
        kl: 0.003936855122447014
        model: {}
        policy_loss: -0.008576471358537674
        total_loss: -0.009537003934383392
        vf_explained_var: -0.006078511476516724
        vf_loss: 0.2893320620059967
      agent-1:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 4.394879942992702e-05
        entropy: 0.5080722570419312
        entropy_coeff: 0.0017600000137463212
        kl: 0.003381194081157446
        model: {}
        policy_loss: -0.0069860732182860374
        total_loss: -0.007868075743317604
        vf_explained_var: 0.02908627688884735
        vf_loss: 0.11876673251390457
      agent-2:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 4.394879942992702e-05
        entropy: 0.4900932013988495
        entropy_coeff: 0.0017600000137463212
        kl: 0.0036558951251208782
        model: {}
        policy_loss: -0.008804711513221264
        total_loss: -0.009616633877158165
        vf_explained_var: -0.009663611650466919
        vf_loss: 0.4778333008289337
      agent-3:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 4.394879942992702e-05
        entropy: 0.5847519040107727
        entropy_coeff: 0.0017600000137463212
        kl: 0.0034517436288297176
        model: {}
        policy_loss: -0.006701388396322727
        total_loss: -0.0076931677758693695
        vf_explained_var: -0.0025423914194107056
        vf_loss: 0.36037907004356384
      agent-4:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 4.394879942992702e-05
        entropy: 0.6698000431060791
        entropy_coeff: 0.0017600000137463212
        kl: 0.004084916785359383
        model: {}
        policy_loss: -0.009377911686897278
        total_loss: -0.010505443438887596
        vf_explained_var: 0.01917591691017151
        vf_loss: 0.44934114813804626
      agent-5:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 4.394879942992702e-05
        entropy: 0.7944328784942627
        entropy_coeff: 0.0017600000137463212
        kl: 0.003998614381998777
        model: {}
        policy_loss: -0.0062620555981993675
        total_loss: -0.007630093488842249
        vf_explained_var: -0.0016279369592666626
        vf_loss: 0.1767176240682602
    load_time_ms: 13559.267
    num_steps_sampled: 19584000
    num_steps_trained: 19584000
    sample_time_ms: 97457.88
    update_time_ms: 15.039
  iterations_since_restore: 44
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.277142857142856
    ram_util_percent: 11.418285714285712
  pid: 13408
  policy_reward_max:
    agent-0: 12.0
    agent-1: 7.0
    agent-2: 16.0
    agent-3: 16.0
    agent-4: 14.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 3.36
    agent-1: 1.62
    agent-2: 4.51
    agent-3: 3.78
    agent-4: 4.91
    agent-5: 2.2
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.279635133262435
    mean_inference_ms: 12.950721222669694
    mean_processing_ms: 58.26415055651686
  time_since_restore: 5516.376692771912
  time_this_iter_s: 122.97930717468262
  time_total_s: 28026.659935235977
  timestamp: 1637225474
  timesteps_since_restore: 4224000
  timesteps_this_iter: 96000
  timesteps_total: 19584000
  training_iteration: 204
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    204 |          28026.7 | 19584000 |    20.38 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 2.35
    apples_agent-0_min: 0
    apples_agent-1_max: 3
    apples_agent-1_mean: 0.83
    apples_agent-1_min: 0
    apples_agent-2_max: 20
    apples_agent-2_mean: 2.4
    apples_agent-2_min: 0
    apples_agent-3_max: 27
    apples_agent-3_mean: 3.01
    apples_agent-3_min: 0
    apples_agent-4_max: 25
    apples_agent-4_mean: 1.71
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 1.04
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 153
    cleaning_beam_agent-0_mean: 76.1
    cleaning_beam_agent-0_min: 17
    cleaning_beam_agent-1_max: 416
    cleaning_beam_agent-1_mean: 299.45
    cleaning_beam_agent-1_min: 146
    cleaning_beam_agent-2_max: 67
    cleaning_beam_agent-2_mean: 18.49
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 230
    cleaning_beam_agent-3_mean: 90.21
    cleaning_beam_agent-3_min: 23
    cleaning_beam_agent-4_max: 167
    cleaning_beam_agent-4_mean: 58.06
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 47
    cleaning_beam_agent-5_mean: 6.85
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-53-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 52.0
  episode_reward_mean: 18.94
  episode_reward_min: -74.0
  episodes_this_iter: 96
  episodes_total: 19680
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11499.952
    learner:
      agent-0:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 3.795840166276321e-05
        entropy: 0.5645862817764282
        entropy_coeff: 0.0017600000137463212
        kl: 0.0030469107441604137
        model: {}
        policy_loss: -0.004242477938532829
        total_loss: -0.005054847337305546
        vf_explained_var: -0.003001093864440918
        vf_loss: 1.7654571533203125
      agent-1:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 3.795840166276321e-05
        entropy: 0.49883660674095154
        entropy_coeff: 0.0017600000137463212
        kl: 0.0030127770733088255
        model: {}
        policy_loss: -0.0054311007261276245
        total_loss: -0.0062936581671237946
        vf_explained_var: 0.015692561864852905
        vf_loss: 0.15251524746418
      agent-2:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 3.795840166276321e-05
        entropy: 0.4736114740371704
        entropy_coeff: 0.0017600000137463212
        kl: 0.0034268072340637445
        model: {}
        policy_loss: -0.007180217187851667
        total_loss: -0.007954062893986702
        vf_explained_var: 0.011946097016334534
        vf_loss: 0.5837158560752869
      agent-3:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 3.795840166276321e-05
        entropy: 0.5658398270606995
        entropy_coeff: 0.0017600000137463212
        kl: 0.003414170118048787
        model: {}
        policy_loss: -0.0046847122721374035
        total_loss: -0.005543340463191271
        vf_explained_var: -0.002247363328933716
        vf_loss: 1.3658370971679688
      agent-4:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 3.795840166276321e-05
        entropy: 0.6700057983398438
        entropy_coeff: 0.0017600000137463212
        kl: 0.0030974848195910454
        model: {}
        policy_loss: -0.004524349234998226
        total_loss: -0.005523154512047768
        vf_explained_var: 0.001771494746208191
        vf_loss: 1.7798110246658325
      agent-5:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 3.795840166276321e-05
        entropy: 0.805770993232727
        entropy_coeff: 0.0017600000137463212
        kl: 0.0038457373157143593
        model: {}
        policy_loss: -0.005894176196306944
        total_loss: -0.007287909276783466
        vf_explained_var: -0.0015470832586288452
        vf_loss: 0.18414734303951263
    load_time_ms: 13539.349
    num_steps_sampled: 19680000
    num_steps_trained: 19680000
    sample_time_ms: 97398.343
    update_time_ms: 15.071
  iterations_since_restore: 45
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.119540229885057
    ram_util_percent: 11.533908045977013
  pid: 13408
  policy_reward_max:
    agent-0: 29.0
    agent-1: 8.0
    agent-2: 23.0
    agent-3: 23.0
    agent-4: 17.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 3.09
    agent-1: 1.3
    agent-2: 4.98
    agent-3: 3.48
    agent-4: 3.98
    agent-5: 2.11
  policy_reward_min:
    agent-0: -43.0
    agent-1: -42.0
    agent-2: 0.0
    agent-3: -46.0
    agent-4: -45.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.268900470387333
    mean_inference_ms: 12.943376371656186
    mean_processing_ms: 58.24018957695361
  time_since_restore: 5638.2810299396515
  time_this_iter_s: 121.90433716773987
  time_total_s: 28148.564272403717
  timestamp: 1637225596
  timesteps_since_restore: 4320000
  timesteps_this_iter: 96000
  timesteps_total: 19680000
  training_iteration: 205
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    205 |          28148.6 | 19680000 |    18.94 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 2.51
    apples_agent-0_min: 0
    apples_agent-1_max: 10
    apples_agent-1_mean: 0.8
    apples_agent-1_min: 0
    apples_agent-2_max: 15
    apples_agent-2_mean: 2.25
    apples_agent-2_min: 0
    apples_agent-3_max: 12
    apples_agent-3_mean: 2.31
    apples_agent-3_min: 0
    apples_agent-4_max: 35
    apples_agent-4_mean: 1.77
    apples_agent-4_min: 0
    apples_agent-5_max: 11
    apples_agent-5_mean: 1.45
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 218
    cleaning_beam_agent-0_mean: 74.25
    cleaning_beam_agent-0_min: 20
    cleaning_beam_agent-1_max: 454
    cleaning_beam_agent-1_mean: 300.95
    cleaning_beam_agent-1_min: 156
    cleaning_beam_agent-2_max: 67
    cleaning_beam_agent-2_mean: 19.34
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 244
    cleaning_beam_agent-3_mean: 102.63
    cleaning_beam_agent-3_min: 20
    cleaning_beam_agent-4_max: 179
    cleaning_beam_agent-4_mean: 51.45
    cleaning_beam_agent-4_min: 6
    cleaning_beam_agent-5_max: 32
    cleaning_beam_agent-5_mean: 5.81
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-55-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 64.0
  episode_reward_mean: 21.61
  episode_reward_min: 4.0
  episodes_this_iter: 96
  episodes_total: 19776
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11503.634
    learner:
      agent-0:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 3.196800025762059e-05
        entropy: 0.5798550844192505
        entropy_coeff: 0.0017600000137463212
        kl: 0.0036182841286063194
        model: {}
        policy_loss: -0.007199970073997974
        total_loss: -0.008187235333025455
        vf_explained_var: -0.0059236884117126465
        vf_loss: 0.30454811453819275
      agent-1:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 3.196800025762059e-05
        entropy: 0.504358172416687
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024429229088127613
        model: {}
        policy_loss: -0.0055235205218195915
        total_loss: -0.006399636156857014
        vf_explained_var: 0.01112338900566101
        vf_loss: 0.11496001482009888
      agent-2:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 3.196800025762059e-05
        entropy: 0.48364192247390747
        entropy_coeff: 0.0017600000137463212
        kl: 0.002843351336196065
        model: {}
        policy_loss: -0.007591748610138893
        total_loss: -0.008398164995014668
        vf_explained_var: -0.0035622715950012207
        vf_loss: 0.44241243600845337
      agent-3:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 3.196800025762059e-05
        entropy: 0.5864290595054626
        entropy_coeff: 0.0017600000137463212
        kl: 0.0028389806393533945
        model: {}
        policy_loss: -0.005691993981599808
        total_loss: -0.006686636246740818
        vf_explained_var: 0.004460528492927551
        vf_loss: 0.37200474739074707
      agent-4:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 3.196800025762059e-05
        entropy: 0.6636959910392761
        entropy_coeff: 0.0017600000137463212
        kl: 0.00348171079531312
        model: {}
        policy_loss: -0.008558697067201138
        total_loss: -0.009686412289738655
        vf_explained_var: 0.013874739408493042
        vf_loss: 0.3902930021286011
      agent-5:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 3.196800025762059e-05
        entropy: 0.7998420000076294
        entropy_coeff: 0.0017600000137463212
        kl: 0.003662695875391364
        model: {}
        policy_loss: -0.005388255231082439
        total_loss: -0.0067759305238723755
        vf_explained_var: 0.02831822633743286
        vf_loss: 0.1718575358390808
    load_time_ms: 13558.86
    num_steps_sampled: 19776000
    num_steps_trained: 19776000
    sample_time_ms: 97507.253
    update_time_ms: 14.303
  iterations_since_restore: 46
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.5864406779661
    ram_util_percent: 11.448587570621468
  pid: 13408
  policy_reward_max:
    agent-0: 14.0
    agent-1: 8.0
    agent-2: 15.0
    agent-3: 15.0
    agent-4: 16.0
    agent-5: 14.0
  policy_reward_mean:
    agent-0: 3.55
    agent-1: 1.69
    agent-2: 5.06
    agent-3: 4.04
    agent-4: 4.91
    agent-5: 2.36
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.26422515192241
    mean_inference_ms: 12.938580668863793
    mean_processing_ms: 58.23034217570615
  time_since_restore: 5762.251671075821
  time_this_iter_s: 123.97064113616943
  time_total_s: 28272.534913539886
  timestamp: 1637225720
  timesteps_since_restore: 4416000
  timesteps_this_iter: 96000
  timesteps_total: 19776000
  training_iteration: 206
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    206 |          28272.5 | 19776000 |    21.61 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 2.68
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.81
    apples_agent-1_min: 0
    apples_agent-2_max: 18
    apples_agent-2_mean: 2.06
    apples_agent-2_min: 0
    apples_agent-3_max: 24
    apples_agent-3_mean: 2.72
    apples_agent-3_min: 0
    apples_agent-4_max: 35
    apples_agent-4_mean: 1.54
    apples_agent-4_min: 0
    apples_agent-5_max: 22
    apples_agent-5_mean: 2.02
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 218
    cleaning_beam_agent-0_mean: 73.43
    cleaning_beam_agent-0_min: 18
    cleaning_beam_agent-1_max: 440
    cleaning_beam_agent-1_mean: 299.81
    cleaning_beam_agent-1_min: 167
    cleaning_beam_agent-2_max: 75
    cleaning_beam_agent-2_mean: 18.16
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 204
    cleaning_beam_agent-3_mean: 100.33
    cleaning_beam_agent-3_min: 28
    cleaning_beam_agent-4_max: 139
    cleaning_beam_agent-4_mean: 59.83
    cleaning_beam_agent-4_min: 10
    cleaning_beam_agent-5_max: 22
    cleaning_beam_agent-5_mean: 4.85
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-57-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 62.0
  episode_reward_mean: 18.92
  episode_reward_min: -138.0
  episodes_this_iter: 96
  episodes_total: 19872
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11491.955
    learner:
      agent-0:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 2.597760067146737e-05
        entropy: 0.575798749923706
        entropy_coeff: 0.0017600000137463212
        kl: 0.0029402836225926876
        model: {}
        policy_loss: -0.006720279809087515
        total_loss: -0.007704146206378937
        vf_explained_var: -0.0024961531162261963
        vf_loss: 0.2839294373989105
      agent-1:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 2.597760067146737e-05
        entropy: 0.49775663018226624
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018296000780537724
        model: {}
        policy_loss: -0.002951483242213726
        total_loss: -0.003681759350001812
        vf_explained_var: 0.01038728654384613
        vf_loss: 1.4575318098068237
      agent-2:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 2.597760067146737e-05
        entropy: 0.4870942533016205
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022229175083339214
        model: {}
        policy_loss: -0.003068023594096303
        total_loss: -0.003611939027905464
        vf_explained_var: 0.0004549175500869751
        vf_loss: 3.1315510272979736
      agent-3:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 2.597760067146737e-05
        entropy: 0.5890756249427795
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021304734982550144
        model: {}
        policy_loss: -0.005186361260712147
        total_loss: -0.006193278357386589
        vf_explained_var: -0.0017021000385284424
        vf_loss: 0.29755014181137085
      agent-4:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 2.597760067146737e-05
        entropy: 0.6558164954185486
        entropy_coeff: 0.0017600000137463212
        kl: 0.002650406677275896
        model: {}
        policy_loss: -0.006955461110919714
        total_loss: -0.008065881207585335
        vf_explained_var: 0.013185068964958191
        vf_loss: 0.43297651410102844
      agent-5:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 2.597760067146737e-05
        entropy: 0.7814655303955078
        entropy_coeff: 0.0017600000137463212
        kl: 0.0030458976980298758
        model: {}
        policy_loss: -0.0030191673431545496
        total_loss: -0.004243745468556881
        vf_explained_var: 0.002915620803833008
        vf_loss: 1.4961190223693848
    load_time_ms: 13547.266
    num_steps_sampled: 19872000
    num_steps_trained: 19872000
    sample_time_ms: 97435.688
    update_time_ms: 14.266
  iterations_since_restore: 47
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.651724137931033
    ram_util_percent: 11.41551724137931
  pid: 13408
  policy_reward_max:
    agent-0: 11.0
    agent-1: 9.0
    agent-2: 20.0
    agent-3: 11.0
    agent-4: 21.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 3.43
    agent-1: 1.45
    agent-2: 3.68
    agent-3: 3.83
    agent-4: 4.83
    agent-5: 1.7
  policy_reward_min:
    agent-0: 0.0
    agent-1: -50.0
    agent-2: -46.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -50.0
  sampler_perf:
    mean_env_wait_ms: 23.255239868411742
    mean_inference_ms: 12.933966630981445
    mean_processing_ms: 58.217462494335216
  time_since_restore: 5884.959113121033
  time_this_iter_s: 122.70744204521179
  time_total_s: 28395.2423555851
  timestamp: 1637225843
  timesteps_since_restore: 4512000
  timesteps_this_iter: 96000
  timesteps_total: 19872000
  training_iteration: 207
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    207 |          28395.2 | 19872000 |    18.92 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 23
    apples_agent-0_mean: 3.19
    apples_agent-0_min: 0
    apples_agent-1_max: 10
    apples_agent-1_mean: 1.01
    apples_agent-1_min: 0
    apples_agent-2_max: 21
    apples_agent-2_mean: 2.34
    apples_agent-2_min: 0
    apples_agent-3_max: 38
    apples_agent-3_mean: 3.2
    apples_agent-3_min: 0
    apples_agent-4_max: 20
    apples_agent-4_mean: 1.54
    apples_agent-4_min: 0
    apples_agent-5_max: 25
    apples_agent-5_mean: 1.77
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 167
    cleaning_beam_agent-0_mean: 71.21
    cleaning_beam_agent-0_min: 24
    cleaning_beam_agent-1_max: 450
    cleaning_beam_agent-1_mean: 300.38
    cleaning_beam_agent-1_min: 181
    cleaning_beam_agent-2_max: 83
    cleaning_beam_agent-2_mean: 18.99
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 211
    cleaning_beam_agent-3_mean: 99.16
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 223
    cleaning_beam_agent-4_mean: 56.66
    cleaning_beam_agent-4_min: 7
    cleaning_beam_agent-5_max: 23
    cleaning_beam_agent-5_mean: 5.26
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-59-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 58.0
  episode_reward_mean: 21.08
  episode_reward_min: -49.0
  episodes_this_iter: 96
  episodes_total: 19968
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11499.194
    learner:
      agent-0:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.998719926632475e-05
        entropy: 0.5614075064659119
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023226754274219275
        model: {}
        policy_loss: -0.0054476503282785416
        total_loss: -0.006405780557543039
        vf_explained_var: -0.005626112222671509
        vf_loss: 0.2949935793876648
      agent-1:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.998719926632475e-05
        entropy: 0.5080428123474121
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016299344133585691
        model: {}
        policy_loss: -0.0041437773033976555
        total_loss: -0.005021370947360992
        vf_explained_var: 0.033042311668395996
        vf_loss: 0.1655513048171997
      agent-2:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.998719926632475e-05
        entropy: 0.48423463106155396
        entropy_coeff: 0.0017600000137463212
        kl: 0.001459551160223782
        model: {}
        policy_loss: -0.0028316848911345005
        total_loss: -0.003490754868835211
        vf_explained_var: 0.0002011507749557495
        vf_loss: 1.9310847520828247
      agent-3:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.998719926632475e-05
        entropy: 0.6060585975646973
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015072320820763707
        model: {}
        policy_loss: -0.004221243783831596
        total_loss: -0.005254827439785004
        vf_explained_var: 0.0011461377143859863
        vf_loss: 0.3304435610771179
      agent-4:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.998719926632475e-05
        entropy: 0.6653081178665161
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021176994778215885
        model: {}
        policy_loss: -0.005530737806111574
        total_loss: -0.006659762933850288
        vf_explained_var: 0.015128135681152344
        vf_loss: 0.41712960600852966
      agent-5:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.998719926632475e-05
        entropy: 0.8088100552558899
        entropy_coeff: 0.0017600000137463212
        kl: 0.0033114952966570854
        model: {}
        policy_loss: -0.004310237243771553
        total_loss: -0.005717623978853226
        vf_explained_var: 0.004062354564666748
        vf_loss: 0.15473948419094086
    load_time_ms: 13548.322
    num_steps_sampled: 19968000
    num_steps_trained: 19968000
    sample_time_ms: 97483.107
    update_time_ms: 14.177
  iterations_since_restore: 48
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.5
    ram_util_percent: 11.316666666666666
  pid: 13408
  policy_reward_max:
    agent-0: 12.0
    agent-1: 14.0
    agent-2: 21.0
    agent-3: 15.0
    agent-4: 16.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 3.71
    agent-1: 1.94
    agent-2: 4.59
    agent-3: 4.19
    agent-4: 4.48
    agent-5: 2.17
  policy_reward_min:
    agent-0: 0.0
    agent-1: -1.0
    agent-2: -50.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.24586681599494
    mean_inference_ms: 12.928158525018976
    mean_processing_ms: 58.19853269022947
  time_since_restore: 6007.481653928757
  time_this_iter_s: 122.522540807724
  time_total_s: 28517.764896392822
  timestamp: 1637225966
  timesteps_since_restore: 4608000
  timesteps_this_iter: 96000
  timesteps_total: 19968000
  training_iteration: 208
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    208 |          28517.8 | 19968000 |    21.08 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 2.1
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.98
    apples_agent-1_min: 0
    apples_agent-2_max: 20
    apples_agent-2_mean: 2.34
    apples_agent-2_min: 0
    apples_agent-3_max: 13
    apples_agent-3_mean: 2.68
    apples_agent-3_min: 0
    apples_agent-4_max: 8
    apples_agent-4_mean: 1.07
    apples_agent-4_min: 0
    apples_agent-5_max: 17
    apples_agent-5_mean: 1.63
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 154
    cleaning_beam_agent-0_mean: 74.14
    cleaning_beam_agent-0_min: 22
    cleaning_beam_agent-1_max: 497
    cleaning_beam_agent-1_mean: 297.98
    cleaning_beam_agent-1_min: 183
    cleaning_beam_agent-2_max: 102
    cleaning_beam_agent-2_mean: 19.37
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 207
    cleaning_beam_agent-3_mean: 96.73
    cleaning_beam_agent-3_min: 24
    cleaning_beam_agent-4_max: 209
    cleaning_beam_agent-4_mean: 54.77
    cleaning_beam_agent-4_min: 8
    cleaning_beam_agent-5_max: 29
    cleaning_beam_agent-5_mean: 4.74
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-01-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 55.0
  episode_reward_mean: 20.05
  episode_reward_min: -29.0
  episodes_this_iter: 96
  episodes_total: 20064
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11487.956
    learner:
      agent-0:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.3996799680171534e-05
        entropy: 0.5687931180000305
        entropy_coeff: 0.0017600000137463212
        kl: 0.001617856789380312
        model: {}
        policy_loss: -0.003453462151810527
        total_loss: -0.0043533314019441605
        vf_explained_var: -0.004321545362472534
        vf_loss: 1.010512113571167
      agent-1:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.3996799680171534e-05
        entropy: 0.4947800040245056
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014771355781704187
        model: {}
        policy_loss: -0.0032428817357867956
        total_loss: -0.004098730627447367
        vf_explained_var: 0.009025603532791138
        vf_loss: 0.14961938560009003
      agent-2:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.3996799680171534e-05
        entropy: 0.4837132692337036
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014958548126742244
        model: {}
        policy_loss: -0.004368392284959555
        total_loss: -0.005164528265595436
        vf_explained_var: 0.006561577320098877
        vf_loss: 0.5516881942749023
      agent-3:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.3996799680171534e-05
        entropy: 0.5997563004493713
        entropy_coeff: 0.0017600000137463212
        kl: 0.001647870521992445
        model: {}
        policy_loss: -0.003376028034836054
        total_loss: -0.004396879579871893
        vf_explained_var: -0.004799544811248779
        vf_loss: 0.34703993797302246
      agent-4:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.3996799680171534e-05
        entropy: 0.6624709367752075
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016125342808663845
        model: {}
        policy_loss: -0.0045116981491446495
        total_loss: -0.005639190785586834
        vf_explained_var: 0.009657368063926697
        vf_loss: 0.3838033676147461
      agent-5:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.3996799680171534e-05
        entropy: 0.8109016418457031
        entropy_coeff: 0.0017600000137463212
        kl: 0.002088128589093685
        model: {}
        policy_loss: -0.0033280604984611273
        total_loss: -0.004737770184874535
        vf_explained_var: 0.0022714734077453613
        vf_loss: 0.1727520227432251
    load_time_ms: 13547.493
    num_steps_sampled: 20064000
    num_steps_trained: 20064000
    sample_time_ms: 97529.615
    update_time_ms: 14.075
  iterations_since_restore: 49
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.77771428571429
    ram_util_percent: 11.477714285714285
  pid: 13408
  policy_reward_max:
    agent-0: 9.0
    agent-1: 9.0
    agent-2: 20.0
    agent-3: 13.0
    agent-4: 16.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 2.48
    agent-1: 1.87
    agent-2: 5.07
    agent-3: 3.84
    agent-4: 4.48
    agent-5: 2.31
  policy_reward_min:
    agent-0: -46.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.23791615870104
    mean_inference_ms: 12.923262463817398
    mean_processing_ms: 58.187352792710925
  time_since_restore: 6130.325145244598
  time_this_iter_s: 122.84349131584167
  time_total_s: 28640.608387708664
  timestamp: 1637226089
  timesteps_since_restore: 4704000
  timesteps_this_iter: 96000
  timesteps_total: 20064000
  training_iteration: 209
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    209 |          28640.6 | 20064000 |    20.05 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 55
    apples_agent-0_mean: 2.75
    apples_agent-0_min: 0
    apples_agent-1_max: 8
    apples_agent-1_mean: 0.89
    apples_agent-1_min: 0
    apples_agent-2_max: 23
    apples_agent-2_mean: 1.83
    apples_agent-2_min: 0
    apples_agent-3_max: 16
    apples_agent-3_mean: 2.45
    apples_agent-3_min: 0
    apples_agent-4_max: 20
    apples_agent-4_mean: 1.22
    apples_agent-4_min: 0
    apples_agent-5_max: 17
    apples_agent-5_mean: 1.29
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 137
    cleaning_beam_agent-0_mean: 72.4
    cleaning_beam_agent-0_min: 22
    cleaning_beam_agent-1_max: 497
    cleaning_beam_agent-1_mean: 292.6
    cleaning_beam_agent-1_min: 161
    cleaning_beam_agent-2_max: 122
    cleaning_beam_agent-2_mean: 20.84
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 221
    cleaning_beam_agent-3_mean: 111.47
    cleaning_beam_agent-3_min: 25
    cleaning_beam_agent-4_max: 177
    cleaning_beam_agent-4_mean: 54.4
    cleaning_beam_agent-4_min: 6
    cleaning_beam_agent-5_max: 24
    cleaning_beam_agent-5_mean: 5.99
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-03-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 52.0
  episode_reward_mean: 19.77
  episode_reward_min: 3.0
  episodes_this_iter: 96
  episodes_total: 20160
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11501.249
    learner:
      agent-0:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5539959073066711
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015881475992500782
        model: {}
        policy_loss: -0.0038483014795929193
        total_loss: -0.004800044000148773
        vf_explained_var: -0.012264162302017212
        vf_loss: 0.2321644127368927
      agent-1:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5033994913101196
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017772349528968334
        model: {}
        policy_loss: -0.003030418884009123
        total_loss: -0.0039042471908032894
        vf_explained_var: 0.029197633266448975
        vf_loss: 0.12157317996025085
      agent-2:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4792802333831787
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013537249760702252
        model: {}
        policy_loss: -0.0038049411959946156
        total_loss: -0.004610950127243996
        vf_explained_var: -0.01188439130783081
        vf_loss: 0.37510159611701965
      agent-3:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6075395345687866
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012394026853144169
        model: {}
        policy_loss: -0.003122689202427864
        total_loss: -0.004161876626312733
        vf_explained_var: 0.0013288557529449463
        vf_loss: 0.3007441759109497
      agent-4:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.657044529914856
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015828147297725081
        model: {}
        policy_loss: -0.004300137050449848
        total_loss: -0.0054191965609788895
        vf_explained_var: 0.007171541452407837
        vf_loss: 0.37300512194633484
      agent-5:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8399683237075806
        entropy_coeff: 0.0017600000137463212
        kl: 0.002776490757241845
        model: {}
        policy_loss: -0.0031165778636932373
        total_loss: -0.004577265121042728
        vf_explained_var: 0.005786404013633728
        vf_loss: 0.17522889375686646
    load_time_ms: 13559.693
    num_steps_sampled: 20160000
    num_steps_trained: 20160000
    sample_time_ms: 97612.487
    update_time_ms: 14.064
  iterations_since_restore: 50
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.358522727272728
    ram_util_percent: 11.416477272727272
  pid: 13408
  policy_reward_max:
    agent-0: 10.0
    agent-1: 8.0
    agent-2: 15.0
    agent-3: 14.0
    agent-4: 15.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 3.28
    agent-1: 1.83
    agent-2: 4.1
    agent-3: 3.75
    agent-4: 4.6
    agent-5: 2.21
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.23180115393275
    mean_inference_ms: 12.917473557861177
    mean_processing_ms: 58.17417333781309
  time_since_restore: 6253.642434120178
  time_this_iter_s: 123.31728887557983
  time_total_s: 28763.925676584244
  timestamp: 1637226212
  timesteps_since_restore: 4800000
  timesteps_this_iter: 96000
  timesteps_total: 20160000
  training_iteration: 210
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    210 |          28763.9 | 20160000 |    19.77 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 2.26
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.81
    apples_agent-1_min: 0
    apples_agent-2_max: 39
    apples_agent-2_mean: 2.71
    apples_agent-2_min: 0
    apples_agent-3_max: 20
    apples_agent-3_mean: 2.66
    apples_agent-3_min: 0
    apples_agent-4_max: 4
    apples_agent-4_mean: 0.88
    apples_agent-4_min: 0
    apples_agent-5_max: 24
    apples_agent-5_mean: 1.59
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 131
    cleaning_beam_agent-0_mean: 69.54
    cleaning_beam_agent-0_min: 26
    cleaning_beam_agent-1_max: 450
    cleaning_beam_agent-1_mean: 283.83
    cleaning_beam_agent-1_min: 176
    cleaning_beam_agent-2_max: 223
    cleaning_beam_agent-2_mean: 20.86
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 235
    cleaning_beam_agent-3_mean: 99.9
    cleaning_beam_agent-3_min: 20
    cleaning_beam_agent-4_max: 133
    cleaning_beam_agent-4_mean: 52.64
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 40
    cleaning_beam_agent-5_mean: 5.35
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-05-35
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 57.0
  episode_reward_mean: 20.0
  episode_reward_min: -85.0
  episodes_this_iter: 96
  episodes_total: 20256
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11506.938
    learner:
      agent-0:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5568313598632812
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013165976852178574
        model: {}
        policy_loss: -0.0034669917076826096
        total_loss: -0.004284192807972431
        vf_explained_var: 0.0023232251405715942
        vf_loss: 1.6278938055038452
      agent-1:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.494507759809494
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014349850825965405
        model: {}
        policy_loss: -0.002954753115773201
        total_loss: -0.0038118059746921062
        vf_explained_var: 0.034590572118759155
        vf_loss: 0.13280245661735535
      agent-2:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48434457182884216
        entropy_coeff: 0.0017600000137463212
        kl: 0.001228527631610632
        model: {}
        policy_loss: -0.002312795724719763
        total_loss: -0.0029892551247030497
        vf_explained_var: 0.005077958106994629
        vf_loss: 1.7597811222076416
      agent-3:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5952454209327698
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013084840029478073
        model: {}
        policy_loss: -0.002221762202680111
        total_loss: -0.0030947993509471416
        vf_explained_var: 0.0003628730773925781
        vf_loss: 1.745893120765686
      agent-4:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6662560701370239
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016017052112147212
        model: {}
        policy_loss: -0.003965294919908047
        total_loss: -0.005093892104923725
        vf_explained_var: 0.012366250157356262
        vf_loss: 0.4399617910385132
      agent-5:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8059334754943848
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020771673880517483
        model: {}
        policy_loss: -0.0028981268405914307
        total_loss: -0.004296477418392897
        vf_explained_var: 0.006458058953285217
        vf_loss: 0.20047226548194885
    load_time_ms: 13561.693
    num_steps_sampled: 20256000
    num_steps_trained: 20256000
    sample_time_ms: 97613.512
    update_time_ms: 14.13
  iterations_since_restore: 51
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.301714285714286
    ram_util_percent: 11.377142857142857
  pid: 13408
  policy_reward_max:
    agent-0: 13.0
    agent-1: 10.0
    agent-2: 17.0
    agent-3: 18.0
    agent-4: 15.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 2.85
    agent-1: 1.96
    agent-2: 4.12
    agent-3: 3.91
    agent-4: 4.5
    agent-5: 2.66
  policy_reward_min:
    agent-0: -49.0
    agent-1: 0.0
    agent-2: -49.0
    agent-3: -43.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.223109017158677
    mean_inference_ms: 12.911984004153918
    mean_processing_ms: 58.15979858219209
  time_since_restore: 6376.30982542038
  time_this_iter_s: 122.66739130020142
  time_total_s: 28886.593067884445
  timestamp: 1637226335
  timesteps_since_restore: 4896000
  timesteps_this_iter: 96000
  timesteps_total: 20256000
  training_iteration: 211
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    211 |          28886.6 | 20256000 |       20 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 2.25
    apples_agent-0_min: 0
    apples_agent-1_max: 12
    apples_agent-1_mean: 0.94
    apples_agent-1_min: 0
    apples_agent-2_max: 39
    apples_agent-2_mean: 2.31
    apples_agent-2_min: 0
    apples_agent-3_max: 8
    apples_agent-3_mean: 2.26
    apples_agent-3_min: 0
    apples_agent-4_max: 84
    apples_agent-4_mean: 1.89
    apples_agent-4_min: 0
    apples_agent-5_max: 8
    apples_agent-5_mean: 1.26
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 152
    cleaning_beam_agent-0_mean: 70.49
    cleaning_beam_agent-0_min: 33
    cleaning_beam_agent-1_max: 426
    cleaning_beam_agent-1_mean: 273.07
    cleaning_beam_agent-1_min: 145
    cleaning_beam_agent-2_max: 51
    cleaning_beam_agent-2_mean: 15.88
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 203
    cleaning_beam_agent-3_mean: 103.21
    cleaning_beam_agent-3_min: 29
    cleaning_beam_agent-4_max: 326
    cleaning_beam_agent-4_mean: 48.26
    cleaning_beam_agent-4_min: 7
    cleaning_beam_agent-5_max: 28
    cleaning_beam_agent-5_mean: 5.57
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 3
    fire_beam_agent-1_mean: 0.05
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 21
    fire_beam_agent-5_mean: 0.22
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-07-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 48.0
  episode_reward_mean: 17.39
  episode_reward_min: -47.0
  episodes_this_iter: 96
  episodes_total: 20352
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11516.762
    learner:
      agent-0:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5625898838043213
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017228969372808933
        model: {}
        policy_loss: -0.003928661346435547
        total_loss: -0.0048952894285321236
        vf_explained_var: -0.014515876770019531
        vf_loss: 0.2351083606481552
      agent-1:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48858392238616943
        entropy_coeff: 0.0017600000137463212
        kl: 0.001019650953821838
        model: {}
        policy_loss: -0.0029151614289730787
        total_loss: -0.0037658018991351128
        vf_explained_var: 0.0072789788246154785
        vf_loss: 0.09266722947359085
      agent-2:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48836958408355713
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015337150543928146
        model: {}
        policy_loss: -0.003639625385403633
        total_loss: -0.004459247458726168
        vf_explained_var: -0.016055315732955933
        vf_loss: 0.3990672528743744
      agent-3:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6011643409729004
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014112762873992324
        model: {}
        policy_loss: -0.002337117213755846
        total_loss: -0.0032309372909367085
        vf_explained_var: 0.0017653852701187134
        vf_loss: 1.6422549486160278
      agent-4:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6682097911834717
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019625620916485786
        model: {}
        policy_loss: -0.002638742094859481
        total_loss: -0.0036635028664022684
        vf_explained_var: -0.0004272162914276123
        vf_loss: 1.5127720832824707
      agent-5:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8083642721176147
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017313967691734433
        model: {}
        policy_loss: -0.0031037302687764168
        total_loss: -0.004493565298616886
        vf_explained_var: 0.01595105230808258
        vf_loss: 0.32868829369544983
    load_time_ms: 13550.188
    num_steps_sampled: 20352000
    num_steps_trained: 20352000
    sample_time_ms: 97569.022
    update_time_ms: 14.131
  iterations_since_restore: 52
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.43103448275862
    ram_util_percent: 11.383908045977016
  pid: 13408
  policy_reward_max:
    agent-0: 13.0
    agent-1: 6.0
    agent-2: 16.0
    agent-3: 16.0
    agent-4: 12.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 3.1
    agent-1: 1.28
    agent-2: 3.76
    agent-3: 3.28
    agent-4: 3.65
    agent-5: 2.32
  policy_reward_min:
    agent-0: 0.0
    agent-1: -3.0
    agent-2: -43.0
    agent-3: -50.0
    agent-4: -46.0
    agent-5: -20.0
  sampler_perf:
    mean_env_wait_ms: 23.211263807028
    mean_inference_ms: 12.90739022655527
    mean_processing_ms: 58.142738496697284
  time_since_restore: 6498.560475587845
  time_this_iter_s: 122.25065016746521
  time_total_s: 29008.84371805191
  timestamp: 1637226458
  timesteps_since_restore: 4992000
  timesteps_this_iter: 96000
  timesteps_total: 20352000
  training_iteration: 212
  trial_id: '00000'
  
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
== Status ==
Memory usage on this node: 21.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    212 |          29008.8 | 20352000 |    17.39 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 2.94
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.91
    apples_agent-1_min: 0
    apples_agent-2_max: 24
    apples_agent-2_mean: 2.3
    apples_agent-2_min: 0
    apples_agent-3_max: 14
    apples_agent-3_mean: 2.33
    apples_agent-3_min: 0
    apples_agent-4_max: 124
    apples_agent-4_mean: 3.0
    apples_agent-4_min: 0
    apples_agent-5_max: 6
    apples_agent-5_mean: 1.25
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 147
    cleaning_beam_agent-0_mean: 68.6
    cleaning_beam_agent-0_min: 13
    cleaning_beam_agent-1_max: 408
    cleaning_beam_agent-1_mean: 271.54
    cleaning_beam_agent-1_min: 117
    cleaning_beam_agent-2_max: 83
    cleaning_beam_agent-2_mean: 19.29
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 206
    cleaning_beam_agent-3_mean: 101.11
    cleaning_beam_agent-3_min: 32
    cleaning_beam_agent-4_max: 137
    cleaning_beam_agent-4_mean: 44.07
    cleaning_beam_agent-4_min: 4
    cleaning_beam_agent-5_max: 30
    cleaning_beam_agent-5_mean: 4.59
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-09-41
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 55.0
  episode_reward_mean: 20.45
  episode_reward_min: -43.0
  episodes_this_iter: 96
  episodes_total: 20448
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11511.749
    learner:
      agent-0:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5635671019554138
        entropy_coeff: 0.0017600000137463212
        kl: 0.002055808436125517
        model: {}
        policy_loss: -0.004001409746706486
        total_loss: -0.004964807070791721
        vf_explained_var: -0.006363958120346069
        vf_loss: 0.2847297191619873
      agent-1:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47875434160232544
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011446797288954258
        model: {}
        policy_loss: -0.0026841764338314533
        total_loss: -0.0035146232694387436
        vf_explained_var: 0.025002717971801758
        vf_loss: 0.12162107974290848
      agent-2:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49012818932533264
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017457513604313135
        model: {}
        policy_loss: -0.0037872795946896076
        total_loss: -0.0045986478216946125
        vf_explained_var: 0.0021613091230392456
        vf_loss: 0.5125771164894104
      agent-3:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6127090454101562
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014928773744031787
        model: {}
        policy_loss: -0.0032462230883538723
        total_loss: -0.004291857592761517
        vf_explained_var: 0.0007380247116088867
        vf_loss: 0.3273565471172333
      agent-4:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6671639680862427
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020142497960478067
        model: {}
        policy_loss: -0.004155638162046671
        total_loss: -0.005288212094455957
        vf_explained_var: 0.011716514825820923
        vf_loss: 0.4163161814212799
      agent-5:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7935156226158142
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022827740758657455
        model: {}
        policy_loss: -0.003265286097303033
        total_loss: -0.004642427898943424
        vf_explained_var: 0.01516173779964447
        vf_loss: 0.19437992572784424
    load_time_ms: 13549.312
    num_steps_sampled: 20448000
    num_steps_trained: 20448000
    sample_time_ms: 97628.992
    update_time_ms: 14.091
  iterations_since_restore: 53
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.96057142857143
    ram_util_percent: 11.492571428571429
  pid: 13408
  policy_reward_max:
    agent-0: 12.0
    agent-1: 8.0
    agent-2: 17.0
    agent-3: 13.0
    agent-4: 17.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 3.36
    agent-1: 1.68
    agent-2: 4.37
    agent-3: 3.94
    agent-4: 4.75
    agent-5: 2.35
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -49.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.200537694166773
    mean_inference_ms: 12.902323003763316
    mean_processing_ms: 58.127385208308915
  time_since_restore: 6621.343885183334
  time_this_iter_s: 122.7834095954895
  time_total_s: 29131.6271276474
  timestamp: 1637226581
  timesteps_since_restore: 5088000
  timesteps_this_iter: 96000
  timesteps_total: 20448000
  training_iteration: 213
  trial_id: '00000'
  
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
== Status ==
Memory usage on this node: 20.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    213 |          29131.6 | 20448000 |    20.45 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 18
    apples_agent-0_mean: 2.91
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 1.04
    apples_agent-1_min: 0
    apples_agent-2_max: 20
    apples_agent-2_mean: 2.34
    apples_agent-2_min: 0
    apples_agent-3_max: 11
    apples_agent-3_mean: 2.77
    apples_agent-3_min: 0
    apples_agent-4_max: 8
    apples_agent-4_mean: 1.17
    apples_agent-4_min: 0
    apples_agent-5_max: 24
    apples_agent-5_mean: 1.87
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 161
    cleaning_beam_agent-0_mean: 69.89
    cleaning_beam_agent-0_min: 25
    cleaning_beam_agent-1_max: 395
    cleaning_beam_agent-1_mean: 266.15
    cleaning_beam_agent-1_min: 185
    cleaning_beam_agent-2_max: 66
    cleaning_beam_agent-2_mean: 19.08
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 213
    cleaning_beam_agent-3_mean: 112.52
    cleaning_beam_agent-3_min: 39
    cleaning_beam_agent-4_max: 111
    cleaning_beam_agent-4_mean: 46.57
    cleaning_beam_agent-4_min: 6
    cleaning_beam_agent-5_max: 42
    cleaning_beam_agent-5_mean: 6.11
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-11-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 65.0
  episode_reward_mean: 21.44
  episode_reward_min: -86.0
  episodes_this_iter: 96
  episodes_total: 20544
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11498.601
    learner:
      agent-0:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5563293695449829
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014418787322938442
        model: {}
        policy_loss: -0.003453800454735756
        total_loss: -0.004396993666887283
        vf_explained_var: -0.008322924375534058
        vf_loss: 0.35942989587783813
      agent-1:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4799262285232544
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015748341102153063
        model: {}
        policy_loss: -0.0016767813358455896
        total_loss: -0.0023746402002871037
        vf_explained_var: 0.003501400351524353
        vf_loss: 1.4681410789489746
      agent-2:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49062198400497437
        entropy_coeff: 0.0017600000137463212
        kl: 0.00125756801571697
        model: {}
        policy_loss: -0.004003969021141529
        total_loss: -0.004818091168999672
        vf_explained_var: -0.006292194128036499
        vf_loss: 0.49374842643737793
      agent-3:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6015700697898865
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013341096928343177
        model: {}
        policy_loss: -0.003381951479241252
        total_loss: -0.004402455408126116
        vf_explained_var: 0.005827963352203369
        vf_loss: 0.382602721452713
      agent-4:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6552631258964539
        entropy_coeff: 0.0017600000137463212
        kl: 0.001590983010828495
        model: {}
        policy_loss: -0.0039823902770876884
        total_loss: -0.005095829721540213
        vf_explained_var: 0.01042848825454712
        vf_loss: 0.39819446206092834
      agent-5:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8233872652053833
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012842916185036302
        model: {}
        policy_loss: -0.001989926677197218
        total_loss: -0.0032827984541654587
        vf_explained_var: 0.006229043006896973
        vf_loss: 1.5628467798233032
    load_time_ms: 13548.934
    num_steps_sampled: 20544000
    num_steps_trained: 20544000
    sample_time_ms: 97501.409
    update_time_ms: 14.208
  iterations_since_restore: 54
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.69942196531792
    ram_util_percent: 11.422543352601156
  pid: 13408
  policy_reward_max:
    agent-0: 11.0
    agent-1: 7.0
    agent-2: 16.0
    agent-3: 12.0
    agent-4: 20.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 3.49
    agent-1: 1.59
    agent-2: 4.98
    agent-3: 4.3
    agent-4: 4.57
    agent-5: 2.51
  policy_reward_min:
    agent-0: -45.0
    agent-1: -49.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -46.0
  sampler_perf:
    mean_env_wait_ms: 23.190151824343484
    mean_inference_ms: 12.897176924787034
    mean_processing_ms: 58.111383236331484
  time_since_restore: 6742.9030792713165
  time_this_iter_s: 121.55919408798218
  time_total_s: 29253.186321735382
  timestamp: 1637226702
  timesteps_since_restore: 5184000
  timesteps_this_iter: 96000
  timesteps_total: 20544000
  training_iteration: 214
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    214 |          29253.2 | 20544000 |    21.44 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 2.5
    apples_agent-0_min: 0
    apples_agent-1_max: 12
    apples_agent-1_mean: 1.2
    apples_agent-1_min: 0
    apples_agent-2_max: 26
    apples_agent-2_mean: 2.75
    apples_agent-2_min: 0
    apples_agent-3_max: 16
    apples_agent-3_mean: 2.96
    apples_agent-3_min: 0
    apples_agent-4_max: 15
    apples_agent-4_mean: 1.43
    apples_agent-4_min: 0
    apples_agent-5_max: 30
    apples_agent-5_mean: 1.97
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 127
    cleaning_beam_agent-0_mean: 68.51
    cleaning_beam_agent-0_min: 26
    cleaning_beam_agent-1_max: 508
    cleaning_beam_agent-1_mean: 284.66
    cleaning_beam_agent-1_min: 191
    cleaning_beam_agent-2_max: 55
    cleaning_beam_agent-2_mean: 18.49
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 261
    cleaning_beam_agent-3_mean: 106.39
    cleaning_beam_agent-3_min: 30
    cleaning_beam_agent-4_max: 133
    cleaning_beam_agent-4_mean: 44.28
    cleaning_beam_agent-4_min: 8
    cleaning_beam_agent-5_max: 42
    cleaning_beam_agent-5_mean: 6.45
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 4
    fire_beam_agent-5_mean: 0.07
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-13-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 65.0
  episode_reward_mean: 18.67
  episode_reward_min: -186.0
  episodes_this_iter: 96
  episodes_total: 20640
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11505.005
    learner:
      agent-0:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5527611970901489
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012299810769036412
        model: {}
        policy_loss: -0.003356130328029394
        total_loss: -0.004297024570405483
        vf_explained_var: -0.00810268521308899
        vf_loss: 0.3196636140346527
      agent-1:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48832470178604126
        entropy_coeff: 0.0017600000137463212
        kl: 0.00101190444547683
        model: {}
        policy_loss: -0.002625053748488426
        total_loss: -0.0034717731177806854
        vf_explained_var: 0.031492680311203
        vf_loss: 0.12728190422058105
      agent-2:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4805307984352112
        entropy_coeff: 0.0017600000137463212
        kl: 0.001233858522027731
        model: {}
        policy_loss: -0.003970739431679249
        total_loss: -0.004773394204676151
        vf_explained_var: -0.005105555057525635
        vf_loss: 0.43080025911331177
      agent-3:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.595050036907196
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014832912711426616
        model: {}
        policy_loss: -0.003023992758244276
        total_loss: -0.004029025323688984
        vf_explained_var: -0.002035856246948242
        vf_loss: 0.4225638508796692
      agent-4:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6594261527061462
        entropy_coeff: 0.0017600000137463212
        kl: 0.003319808281958103
        model: {}
        policy_loss: -0.0026516388170421124
        total_loss: -0.002043734770268202
        vf_explained_var: 0.006672531366348267
        vf_loss: 17.68496322631836
      agent-5:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8373923301696777
        entropy_coeff: 0.0017600000137463212
        kl: 0.0025391809176653624
        model: {}
        policy_loss: -0.0031265090219676495
        total_loss: -0.004581458866596222
        vf_explained_var: 0.005138322710990906
        vf_loss: 0.1885811984539032
    load_time_ms: 13558.343
    num_steps_sampled: 20640000
    num_steps_trained: 20640000
    sample_time_ms: 97563.709
    update_time_ms: 14.264
  iterations_since_restore: 55
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.40685714285714
    ram_util_percent: 11.510285714285715
  pid: 13408
  policy_reward_max:
    agent-0: 13.0
    agent-1: 8.0
    agent-2: 16.0
    agent-3: 16.0
    agent-4: 20.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 3.13
    agent-1: 1.94
    agent-2: 4.95
    agent-3: 4.33
    agent-4: 1.89
    agent-5: 2.43
  policy_reward_min:
    agent-0: -49.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -195.0
    agent-5: -4.0
  sampler_perf:
    mean_env_wait_ms: 23.179687775651033
    mean_inference_ms: 12.89272116290218
    mean_processing_ms: 58.093669404885034
  time_since_restore: 6865.6408405303955
  time_this_iter_s: 122.73776125907898
  time_total_s: 29375.92408299446
  timestamp: 1637226825
  timesteps_since_restore: 5280000
  timesteps_this_iter: 96000
  timesteps_total: 20640000
  training_iteration: 215
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    215 |          29375.9 | 20640000 |    18.67 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 2.88
    apples_agent-0_min: 0
    apples_agent-1_max: 12
    apples_agent-1_mean: 1.01
    apples_agent-1_min: 0
    apples_agent-2_max: 19
    apples_agent-2_mean: 2.27
    apples_agent-2_min: 0
    apples_agent-3_max: 20
    apples_agent-3_mean: 3.02
    apples_agent-3_min: 0
    apples_agent-4_max: 28
    apples_agent-4_mean: 1.62
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 1.31
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 165
    cleaning_beam_agent-0_mean: 70.74
    cleaning_beam_agent-0_min: 25
    cleaning_beam_agent-1_max: 406
    cleaning_beam_agent-1_mean: 271.26
    cleaning_beam_agent-1_min: 155
    cleaning_beam_agent-2_max: 98
    cleaning_beam_agent-2_mean: 18.82
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 273
    cleaning_beam_agent-3_mean: 106.92
    cleaning_beam_agent-3_min: 24
    cleaning_beam_agent-4_max: 119
    cleaning_beam_agent-4_mean: 46.5
    cleaning_beam_agent-4_min: 6
    cleaning_beam_agent-5_max: 28
    cleaning_beam_agent-5_mean: 5.6
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-15-48
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 55.0
  episode_reward_mean: 21.51
  episode_reward_min: -39.0
  episodes_this_iter: 96
  episodes_total: 20736
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11486.785
    learner:
      agent-0:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5587735772132874
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014368854463100433
        model: {}
        policy_loss: -0.0035972418263554573
        total_loss: -0.004549030680209398
        vf_explained_var: 0.0006959140300750732
        vf_loss: 0.31652894616127014
      agent-1:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4673995077610016
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009292724425904453
        model: {}
        policy_loss: -0.0029413895681500435
        total_loss: -0.0037518281023949385
        vf_explained_var: 0.019190236926078796
        vf_loss: 0.12186827510595322
      agent-2:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49421343207359314
        entropy_coeff: 0.0017600000137463212
        kl: 0.001396464416757226
        model: {}
        policy_loss: -0.0034802593290805817
        total_loss: -0.004297004546970129
        vf_explained_var: 0.0005264580249786377
        vf_loss: 0.5306727886199951
      agent-3:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.604883074760437
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016341954469680786
        model: {}
        policy_loss: -0.003281655255705118
        total_loss: -0.004311096854507923
        vf_explained_var: 0.0004984885454177856
        vf_loss: 0.3515297770500183
      agent-4:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.651194155216217
        entropy_coeff: 0.0017600000137463212
        kl: 0.002135598100721836
        model: {}
        policy_loss: -0.003631806233897805
        total_loss: -0.00460249837487936
        vf_explained_var: 0.005497053265571594
        vf_loss: 1.754105806350708
      agent-5:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8087002038955688
        entropy_coeff: 0.0017600000137463212
        kl: 0.001894504763185978
        model: {}
        policy_loss: -0.002944433130323887
        total_loss: -0.004344739019870758
        vf_explained_var: 0.015268683433532715
        vf_loss: 0.2300504893064499
    load_time_ms: 13533.126
    num_steps_sampled: 20736000
    num_steps_trained: 20736000
    sample_time_ms: 97434.244
    update_time_ms: 14.557
  iterations_since_restore: 56
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.78275862068966
    ram_util_percent: 11.4551724137931
  pid: 13408
  policy_reward_max:
    agent-0: 16.0
    agent-1: 10.0
    agent-2: 22.0
    agent-3: 14.0
    agent-4: 17.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 3.82
    agent-1: 1.69
    agent-2: 4.67
    agent-3: 4.12
    agent-4: 4.42
    agent-5: 2.79
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -47.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.171669198124636
    mean_inference_ms: 12.889018395611547
    mean_processing_ms: 58.080655491626764
  time_since_restore: 6987.83979010582
  time_this_iter_s: 122.1989495754242
  time_total_s: 29498.123032569885
  timestamp: 1637226948
  timesteps_since_restore: 5376000
  timesteps_this_iter: 96000
  timesteps_total: 20736000
  training_iteration: 216
  trial_id: '00000'
  
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
== Status ==
Memory usage on this node: 20.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    216 |          29498.1 | 20736000 |    21.51 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 2.42
    apples_agent-0_min: 0
    apples_agent-1_max: 11
    apples_agent-1_mean: 1.18
    apples_agent-1_min: 0
    apples_agent-2_max: 15
    apples_agent-2_mean: 2.41
    apples_agent-2_min: 0
    apples_agent-3_max: 15
    apples_agent-3_mean: 2.75
    apples_agent-3_min: 0
    apples_agent-4_max: 10
    apples_agent-4_mean: 1.07
    apples_agent-4_min: 0
    apples_agent-5_max: 6
    apples_agent-5_mean: 1.25
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 158
    cleaning_beam_agent-0_mean: 68.61
    cleaning_beam_agent-0_min: 29
    cleaning_beam_agent-1_max: 465
    cleaning_beam_agent-1_mean: 263.98
    cleaning_beam_agent-1_min: 169
    cleaning_beam_agent-2_max: 61
    cleaning_beam_agent-2_mean: 18.37
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 259
    cleaning_beam_agent-3_mean: 114.31
    cleaning_beam_agent-3_min: 35
    cleaning_beam_agent-4_max: 129
    cleaning_beam_agent-4_mean: 52.55
    cleaning_beam_agent-4_min: 4
    cleaning_beam_agent-5_max: 28
    cleaning_beam_agent-5_mean: 5.22
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-17-50
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 67.0
  episode_reward_mean: 21.73
  episode_reward_min: -35.0
  episodes_this_iter: 96
  episodes_total: 20832
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11509.042
    learner:
      agent-0:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5653151273727417
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017070972826331854
        model: {}
        policy_loss: -0.003753294702619314
        total_loss: -0.0047209192998707294
        vf_explained_var: -0.01035037636756897
        vf_loss: 0.27328360080718994
      agent-1:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48035740852355957
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014006453566253185
        model: {}
        policy_loss: -0.0029230332002043724
        total_loss: -0.0037527093663811684
        vf_explained_var: 0.01520337164402008
        vf_loss: 0.15749965608119965
      agent-2:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49666252732276917
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015208121621981263
        model: {}
        policy_loss: -0.003741881810128689
        total_loss: -0.004564975388348103
        vf_explained_var: 0.0007571578025817871
        vf_loss: 0.5103238224983215
      agent-3:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6123495101928711
        entropy_coeff: 0.0017600000137463212
        kl: 0.001559401978738606
        model: {}
        policy_loss: -0.003102974034845829
        total_loss: -0.004137719981372356
        vf_explained_var: 0.0001275390386581421
        vf_loss: 0.4298962950706482
      agent-4:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6556995511054993
        entropy_coeff: 0.0017600000137463212
        kl: 0.001257017138414085
        model: {}
        policy_loss: -0.0038191063795238733
        total_loss: -0.004932748153805733
        vf_explained_var: 0.01029275357723236
        vf_loss: 0.4038872718811035
      agent-5:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8152894973754883
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014114651130512357
        model: {}
        policy_loss: -0.002870948053896427
        total_loss: -0.00428247544914484
        vf_explained_var: 0.008133932948112488
        vf_loss: 0.23382538557052612
    load_time_ms: 13554.014
    num_steps_sampled: 20832000
    num_steps_trained: 20832000
    sample_time_ms: 97398.492
    update_time_ms: 14.446
  iterations_since_restore: 57
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.322857142857142
    ram_util_percent: 11.39942857142857
  pid: 13408
  policy_reward_max:
    agent-0: 12.0
    agent-1: 10.0
    agent-2: 15.0
    agent-3: 20.0
    agent-4: 18.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 3.58
    agent-1: 1.92
    agent-2: 5.1
    agent-3: 4.41
    agent-4: 3.99
    agent-5: 2.73
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -46.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.16338006098044
    mean_inference_ms: 12.88416392472842
    mean_processing_ms: 58.068564287744465
  time_since_restore: 7110.621156692505
  time_this_iter_s: 122.78136658668518
  time_total_s: 29620.90439915657
  timestamp: 1637227070
  timesteps_since_restore: 5472000
  timesteps_this_iter: 96000
  timesteps_total: 20832000
  training_iteration: 217
  trial_id: '00000'
  
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
== Status ==
Memory usage on this node: 21.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    217 |          29620.9 | 20832000 |    21.73 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 2.39
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 0.85
    apples_agent-1_min: 0
    apples_agent-2_max: 33
    apples_agent-2_mean: 1.88
    apples_agent-2_min: 0
    apples_agent-3_max: 13
    apples_agent-3_mean: 2.5
    apples_agent-3_min: 0
    apples_agent-4_max: 18
    apples_agent-4_mean: 1.61
    apples_agent-4_min: 0
    apples_agent-5_max: 21
    apples_agent-5_mean: 1.59
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 163
    cleaning_beam_agent-0_mean: 72.98
    cleaning_beam_agent-0_min: 16
    cleaning_beam_agent-1_max: 341
    cleaning_beam_agent-1_mean: 250.46
    cleaning_beam_agent-1_min: 162
    cleaning_beam_agent-2_max: 80
    cleaning_beam_agent-2_mean: 20.61
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 277
    cleaning_beam_agent-3_mean: 121.28
    cleaning_beam_agent-3_min: 33
    cleaning_beam_agent-4_max: 172
    cleaning_beam_agent-4_mean: 58.07
    cleaning_beam_agent-4_min: 10
    cleaning_beam_agent-5_max: 34
    cleaning_beam_agent-5_mean: 5.65
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-19-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 52.0
  episode_reward_mean: 20.19
  episode_reward_min: -86.0
  episodes_this_iter: 96
  episodes_total: 20928
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11509.998
    learner:
      agent-0:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5528147220611572
        entropy_coeff: 0.0017600000137463212
        kl: 0.001015219371765852
        model: {}
        policy_loss: -0.0033508967608213425
        total_loss: -0.004297167528420687
        vf_explained_var: -0.002880096435546875
        vf_loss: 0.26683276891708374
      agent-1:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4757682681083679
        entropy_coeff: 0.0017600000137463212
        kl: 0.001036337693221867
        model: {}
        policy_loss: -0.0030342014506459236
        total_loss: -0.0038537338841706514
        vf_explained_var: 0.004522979259490967
        vf_loss: 0.17821672558784485
      agent-2:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4935530424118042
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016274439403787255
        model: {}
        policy_loss: -0.003641102695837617
        total_loss: -0.004467933904379606
        vf_explained_var: -0.0001339167356491089
        vf_loss: 0.4182365834712982
      agent-3:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6019072532653809
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014499434037134051
        model: {}
        policy_loss: -0.0031495867297053337
        total_loss: -0.004176425281912088
        vf_explained_var: -0.0037344694137573242
        vf_loss: 0.32518714666366577
      agent-4:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6617710590362549
        entropy_coeff: 0.0017600000137463212
        kl: 0.001605409779585898
        model: {}
        policy_loss: -0.0038955495692789555
        total_loss: -0.005013475194573402
        vf_explained_var: 0.01337316632270813
        vf_loss: 0.46793049573898315
      agent-5:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.839617908000946
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020183147862553596
        model: {}
        policy_loss: -0.002751714549958706
        total_loss: -0.004202548880130053
        vf_explained_var: 0.009175851941108704
        vf_loss: 0.26898491382598877
    load_time_ms: 13543.079
    num_steps_sampled: 20928000
    num_steps_trained: 20928000
    sample_time_ms: 97410.003
    update_time_ms: 14.58
  iterations_since_restore: 58
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.39540229885057
    ram_util_percent: 11.4551724137931
  pid: 13408
  policy_reward_max:
    agent-0: 10.0
    agent-1: 10.0
    agent-2: 17.0
    agent-3: 10.0
    agent-4: 13.0
    agent-5: 17.0
  policy_reward_mean:
    agent-0: 3.37
    agent-1: 1.99
    agent-2: 3.78
    agent-3: 3.95
    agent-4: 4.49
    agent-5: 2.61
  policy_reward_min:
    agent-0: 0.0
    agent-1: -1.0
    agent-2: -47.0
    agent-3: 0.0
    agent-4: -47.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.155537148367724
    mean_inference_ms: 12.881599736048544
    mean_processing_ms: 58.062246295822526
  time_since_restore: 7233.187154054642
  time_this_iter_s: 122.56599736213684
  time_total_s: 29743.470396518707
  timestamp: 1637227193
  timesteps_since_restore: 5568000
  timesteps_this_iter: 96000
  timesteps_total: 20928000
  training_iteration: 218
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    218 |          29743.5 | 20928000 |    20.19 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 2.32
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.9
    apples_agent-1_min: 0
    apples_agent-2_max: 10
    apples_agent-2_mean: 1.71
    apples_agent-2_min: 0
    apples_agent-3_max: 14
    apples_agent-3_mean: 3.02
    apples_agent-3_min: 0
    apples_agent-4_max: 13
    apples_agent-4_mean: 1.35
    apples_agent-4_min: 0
    apples_agent-5_max: 9
    apples_agent-5_mean: 1.36
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 198
    cleaning_beam_agent-0_mean: 71.56
    cleaning_beam_agent-0_min: 24
    cleaning_beam_agent-1_max: 415
    cleaning_beam_agent-1_mean: 258.97
    cleaning_beam_agent-1_min: 157
    cleaning_beam_agent-2_max: 62
    cleaning_beam_agent-2_mean: 14.88
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 258
    cleaning_beam_agent-3_mean: 126.88
    cleaning_beam_agent-3_min: 34
    cleaning_beam_agent-4_max: 183
    cleaning_beam_agent-4_mean: 46.77
    cleaning_beam_agent-4_min: 5
    cleaning_beam_agent-5_max: 33
    cleaning_beam_agent-5_mean: 5.11
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-21-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 78.0
  episode_reward_mean: 21.8
  episode_reward_min: -17.0
  episodes_this_iter: 96
  episodes_total: 21024
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11510.398
    learner:
      agent-0:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5651082992553711
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015459416899830103
        model: {}
        policy_loss: -0.003960653208196163
        total_loss: -0.0049272989854216576
        vf_explained_var: -0.0021544545888900757
        vf_loss: 0.27941277623176575
      agent-1:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47129037976264954
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010420030448585749
        model: {}
        policy_loss: -0.002634366974234581
        total_loss: -0.0034469859674572945
        vf_explained_var: 0.015650421380996704
        vf_loss: 0.16848066449165344
      agent-2:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4782894551753998
        entropy_coeff: 0.0017600000137463212
        kl: 0.001410760567523539
        model: {}
        policy_loss: -0.00390723766759038
        total_loss: -0.004690991248935461
        vf_explained_var: 0.008827656507492065
        vf_loss: 0.5803184509277344
      agent-3:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6126630902290344
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011694510467350483
        model: {}
        policy_loss: -0.0028333691880106926
        total_loss: -0.0038652075454592705
        vf_explained_var: 0.0003403574228286743
        vf_loss: 0.46448013186454773
      agent-4:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6597816944122314
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017249321099370718
        model: {}
        policy_loss: -0.004280167631804943
        total_loss: -0.005396196618676186
        vf_explained_var: 0.015098556876182556
        vf_loss: 0.451887309551239
      agent-5:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8114383816719055
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019645150750875473
        model: {}
        policy_loss: -0.002887304872274399
        total_loss: -0.004293190781027079
        vf_explained_var: 0.016751393675804138
        vf_loss: 0.22246576845645905
    load_time_ms: 13555.178
    num_steps_sampled: 21024000
    num_steps_trained: 21024000
    sample_time_ms: 97533.191
    update_time_ms: 14.629
  iterations_since_restore: 59
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.201694915254237
    ram_util_percent: 11.499435028248588
  pid: 13408
  policy_reward_max:
    agent-0: 10.0
    agent-1: 12.0
    agent-2: 22.0
    agent-3: 18.0
    agent-4: 16.0
    agent-5: 15.0
  policy_reward_mean:
    agent-0: 3.42
    agent-1: 1.87
    agent-2: 4.45
    agent-3: 4.27
    agent-4: 5.09
    agent-5: 2.7
  policy_reward_min:
    agent-0: 0.0
    agent-1: -1.0
    agent-2: -44.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.150846564313756
    mean_inference_ms: 12.878299888479358
    mean_processing_ms: 58.05613336419684
  time_since_restore: 7357.388097047806
  time_this_iter_s: 124.20094299316406
  time_total_s: 29867.67133951187
  timestamp: 1637227318
  timesteps_since_restore: 5664000
  timesteps_this_iter: 96000
  timesteps_total: 21024000
  training_iteration: 219
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    219 |          29867.7 | 21024000 |     21.8 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 2.4
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.88
    apples_agent-1_min: 0
    apples_agent-2_max: 46
    apples_agent-2_mean: 2.37
    apples_agent-2_min: 0
    apples_agent-3_max: 11
    apples_agent-3_mean: 2.46
    apples_agent-3_min: 0
    apples_agent-4_max: 11
    apples_agent-4_mean: 1.1
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 1.29
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 147
    cleaning_beam_agent-0_mean: 76.17
    cleaning_beam_agent-0_min: 26
    cleaning_beam_agent-1_max: 458
    cleaning_beam_agent-1_mean: 254.15
    cleaning_beam_agent-1_min: 141
    cleaning_beam_agent-2_max: 83
    cleaning_beam_agent-2_mean: 17.9
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 258
    cleaning_beam_agent-3_mean: 118.87
    cleaning_beam_agent-3_min: 38
    cleaning_beam_agent-4_max: 177
    cleaning_beam_agent-4_mean: 46.33
    cleaning_beam_agent-4_min: 5
    cleaning_beam_agent-5_max: 59
    cleaning_beam_agent-5_mean: 6.38
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-24-00
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 60.0
  episode_reward_mean: 20.24
  episode_reward_min: -80.0
  episodes_this_iter: 96
  episodes_total: 21120
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11497.927
    learner:
      agent-0:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5689671635627747
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013989999424666166
        model: {}
        policy_loss: -0.0035876259207725525
        total_loss: -0.004557335749268532
        vf_explained_var: -0.005286037921905518
        vf_loss: 0.3167206645011902
      agent-1:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4806719720363617
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009404121665284038
        model: {}
        policy_loss: -0.002851140219718218
        total_loss: -0.0036816801875829697
        vf_explained_var: 0.01329280436038971
        vf_loss: 0.15444394946098328
      agent-2:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4776848554611206
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011706794612109661
        model: {}
        policy_loss: -0.002642338629812002
        total_loss: -0.0033150590024888515
        vf_explained_var: 0.002359151840209961
        vf_loss: 1.6800446510314941
      agent-3:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.600858211517334
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020890426822006702
        model: {}
        policy_loss: -0.002948288107290864
        total_loss: -0.0038363789208233356
        vf_explained_var: 0.004389584064483643
        vf_loss: 1.694178819656372
      agent-4:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6574504375457764
        entropy_coeff: 0.0017600000137463212
        kl: 0.001563918311148882
        model: {}
        policy_loss: -0.003977673128247261
        total_loss: -0.0050938930362463
        vf_explained_var: 0.01746828854084015
        vf_loss: 0.40892940759658813
      agent-5:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8531091809272766
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013396027497947216
        model: {}
        policy_loss: -0.002781490795314312
        total_loss: -0.004259522072970867
        vf_explained_var: -0.002363741397857666
        vf_loss: 0.23441293835639954
    load_time_ms: 13545.85
    num_steps_sampled: 21120000
    num_steps_trained: 21120000
    sample_time_ms: 97460.896
    update_time_ms: 14.687
  iterations_since_restore: 60
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.362068965517242
    ram_util_percent: 11.397126436781608
  pid: 13408
  policy_reward_max:
    agent-0: 14.0
    agent-1: 8.0
    agent-2: 19.0
    agent-3: 13.0
    agent-4: 15.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 3.66
    agent-1: 1.83
    agent-2: 3.9
    agent-3: 3.39
    agent-4: 4.75
    agent-5: 2.71
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -45.0
    agent-3: -49.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.145079165023162
    mean_inference_ms: 12.87513877585797
    mean_processing_ms: 58.04300676784509
  time_since_restore: 7479.731489896774
  time_this_iter_s: 122.3433928489685
  time_total_s: 29990.01473236084
  timestamp: 1637227440
  timesteps_since_restore: 5760000
  timesteps_this_iter: 96000
  timesteps_total: 21120000
  training_iteration: 220
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    220 |            29990 | 21120000 |    20.24 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 2.75
    apples_agent-0_min: 0
    apples_agent-1_max: 51
    apples_agent-1_mean: 1.57
    apples_agent-1_min: 0
    apples_agent-2_max: 58
    apples_agent-2_mean: 3.97
    apples_agent-2_min: 0
    apples_agent-3_max: 18
    apples_agent-3_mean: 3.06
    apples_agent-3_min: 0
    apples_agent-4_max: 32
    apples_agent-4_mean: 1.57
    apples_agent-4_min: 0
    apples_agent-5_max: 20
    apples_agent-5_mean: 1.62
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 172
    cleaning_beam_agent-0_mean: 74.76
    cleaning_beam_agent-0_min: 19
    cleaning_beam_agent-1_max: 424
    cleaning_beam_agent-1_mean: 255.78
    cleaning_beam_agent-1_min: 173
    cleaning_beam_agent-2_max: 109
    cleaning_beam_agent-2_mean: 16.81
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 270
    cleaning_beam_agent-3_mean: 126.37
    cleaning_beam_agent-3_min: 29
    cleaning_beam_agent-4_max: 234
    cleaning_beam_agent-4_mean: 50.2
    cleaning_beam_agent-4_min: 3
    cleaning_beam_agent-5_max: 43
    cleaning_beam_agent-5_mean: 6.58
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-26-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 61.0
  episode_reward_mean: 22.55
  episode_reward_min: 2.0
  episodes_this_iter: 96
  episodes_total: 21216
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11498.44
    learner:
      agent-0:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5680748820304871
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018069753423333168
        model: {}
        policy_loss: -0.003741366323083639
        total_loss: -0.004707681480795145
        vf_explained_var: 0.0032542049884796143
        vf_loss: 0.3349300026893616
      agent-1:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47329452633857727
        entropy_coeff: 0.0017600000137463212
        kl: 0.00119440583512187
        model: {}
        policy_loss: -0.0030536213889718056
        total_loss: -0.003869582898914814
        vf_explained_var: 0.02007804811000824
        vf_loss: 0.17038825154304504
      agent-2:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4710318446159363
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012729730224236846
        model: {}
        policy_loss: -0.0038124965503811836
        total_loss: -0.00458897091448307
        vf_explained_var: -0.003248244524002075
        vf_loss: 0.5254215598106384
      agent-3:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.613034725189209
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013679803814738989
        model: {}
        policy_loss: -0.00283170398324728
        total_loss: -0.0038708606734871864
        vf_explained_var: -0.0018511712551116943
        vf_loss: 0.3978322148323059
      agent-4:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6455265283584595
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016468818066641688
        model: {}
        policy_loss: -0.004303876776248217
        total_loss: -0.005397479515522718
        vf_explained_var: 0.014377474784851074
        vf_loss: 0.42523080110549927
      agent-5:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8237069249153137
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017207566415891051
        model: {}
        policy_loss: -0.002899873536080122
        total_loss: -0.004328307695686817
        vf_explained_var: -0.00037382543087005615
        vf_loss: 0.2128828912973404
    load_time_ms: 13544.415
    num_steps_sampled: 21216000
    num_steps_trained: 21216000
    sample_time_ms: 97441.268
    update_time_ms: 14.716
  iterations_since_restore: 61
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.818749999999998
    ram_util_percent: 11.439772727272727
  pid: 13408
  policy_reward_max:
    agent-0: 11.0
    agent-1: 10.0
    agent-2: 18.0
    agent-3: 23.0
    agent-4: 17.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 3.99
    agent-1: 1.97
    agent-2: 5.31
    agent-3: 4.02
    agent-4: 4.48
    agent-5: 2.78
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.138136926842073
    mean_inference_ms: 12.870917109537084
    mean_processing_ms: 58.03002201593772
  time_since_restore: 7602.246955156326
  time_this_iter_s: 122.515465259552
  time_total_s: 30112.530197620392
  timestamp: 1637227563
  timesteps_since_restore: 5856000
  timesteps_this_iter: 96000
  timesteps_total: 21216000
  training_iteration: 221
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    221 |          30112.5 | 21216000 |    22.55 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 2.71
    apples_agent-0_min: 0
    apples_agent-1_max: 13
    apples_agent-1_mean: 1.14
    apples_agent-1_min: 0
    apples_agent-2_max: 58
    apples_agent-2_mean: 2.62
    apples_agent-2_min: 0
    apples_agent-3_max: 18
    apples_agent-3_mean: 2.92
    apples_agent-3_min: 0
    apples_agent-4_max: 60
    apples_agent-4_mean: 1.71
    apples_agent-4_min: 0
    apples_agent-5_max: 17
    apples_agent-5_mean: 1.71
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 177
    cleaning_beam_agent-0_mean: 76.3
    cleaning_beam_agent-0_min: 24
    cleaning_beam_agent-1_max: 366
    cleaning_beam_agent-1_mean: 249.93
    cleaning_beam_agent-1_min: 172
    cleaning_beam_agent-2_max: 92
    cleaning_beam_agent-2_mean: 17.15
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 197
    cleaning_beam_agent-3_mean: 113.3
    cleaning_beam_agent-3_min: 48
    cleaning_beam_agent-4_max: 192
    cleaning_beam_agent-4_mean: 48.14
    cleaning_beam_agent-4_min: 2
    cleaning_beam_agent-5_max: 57
    cleaning_beam_agent-5_mean: 6.23
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-28-07
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 70.0
  episode_reward_mean: 22.21
  episode_reward_min: -76.0
  episodes_this_iter: 96
  episodes_total: 21312
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11493.716
    learner:
      agent-0:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5762037038803101
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011154422536492348
        model: {}
        policy_loss: -0.0020513106137514114
        total_loss: -0.002916666679084301
        vf_explained_var: -0.006100893020629883
        vf_loss: 1.4876385927200317
      agent-1:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4745153784751892
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009149201214313507
        model: {}
        policy_loss: -0.0017041945829987526
        total_loss: -0.002389127155765891
        vf_explained_var: 0.005338534712791443
        vf_loss: 1.5021570920944214
      agent-2:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48630404472351074
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015122474869713187
        model: {}
        policy_loss: -0.0037721924018114805
        total_loss: -0.004579962231218815
        vf_explained_var: -0.002543047070503235
        vf_loss: 0.4812445640563965
      agent-3:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6017873287200928
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016971417935565114
        model: {}
        policy_loss: -0.003026772290468216
        total_loss: -0.004044724628329277
        vf_explained_var: 0.00010332465171813965
        vf_loss: 0.41196537017822266
      agent-4:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6464062333106995
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017024347325786948
        model: {}
        policy_loss: -0.004041362553834915
        total_loss: -0.005139460787177086
        vf_explained_var: 0.013394773006439209
        vf_loss: 0.3957742750644684
      agent-5:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.841680645942688
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015495836269110441
        model: {}
        policy_loss: -0.002839759923517704
        total_loss: -0.004288695752620697
        vf_explained_var: 0.004872545599937439
        vf_loss: 0.3242189288139343
    load_time_ms: 13553.553
    num_steps_sampled: 21312000
    num_steps_trained: 21312000
    sample_time_ms: 97556.868
    update_time_ms: 14.902
  iterations_since_restore: 62
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.650285714285715
    ram_util_percent: 11.434285714285716
  pid: 13408
  policy_reward_max:
    agent-0: 12.0
    agent-1: 22.0
    agent-2: 14.0
    agent-3: 23.0
    agent-4: 14.0
    agent-5: 15.0
  policy_reward_mean:
    agent-0: 3.2
    agent-1: 1.72
    agent-2: 5.24
    agent-3: 4.33
    agent-4: 5.01
    agent-5: 2.71
  policy_reward_min:
    agent-0: -47.0
    agent-1: -49.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.133055605182737
    mean_inference_ms: 12.867316168481368
    mean_processing_ms: 58.02446121909984
  time_since_restore: 7725.65133190155
  time_this_iter_s: 123.404376745224
  time_total_s: 30235.934574365616
  timestamp: 1637227687
  timesteps_since_restore: 5952000
  timesteps_this_iter: 96000
  timesteps_total: 21312000
  training_iteration: 222
  trial_id: '00000'
  
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
== Status ==
Memory usage on this node: 20.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    222 |          30235.9 | 21312000 |    22.21 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 14
    apples_agent-0_mean: 2.47
    apples_agent-0_min: 0
    apples_agent-1_max: 16
    apples_agent-1_mean: 1.2
    apples_agent-1_min: 0
    apples_agent-2_max: 70
    apples_agent-2_mean: 3.22
    apples_agent-2_min: 0
    apples_agent-3_max: 11
    apples_agent-3_mean: 2.78
    apples_agent-3_min: 0
    apples_agent-4_max: 24
    apples_agent-4_mean: 1.62
    apples_agent-4_min: 0
    apples_agent-5_max: 10
    apples_agent-5_mean: 1.73
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 179
    cleaning_beam_agent-0_mean: 76.08
    cleaning_beam_agent-0_min: 21
    cleaning_beam_agent-1_max: 446
    cleaning_beam_agent-1_mean: 253.37
    cleaning_beam_agent-1_min: 156
    cleaning_beam_agent-2_max: 83
    cleaning_beam_agent-2_mean: 18.11
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 262
    cleaning_beam_agent-3_mean: 117.58
    cleaning_beam_agent-3_min: 38
    cleaning_beam_agent-4_max: 135
    cleaning_beam_agent-4_mean: 46.71
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 42
    cleaning_beam_agent-5_mean: 5.86
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-30-10
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 74.0
  episode_reward_mean: 24.5
  episode_reward_min: 3.0
  episodes_this_iter: 96
  episodes_total: 21408
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11522.032
    learner:
      agent-0:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5663454532623291
        entropy_coeff: 0.0017600000137463212
        kl: 0.001263310550712049
        model: {}
        policy_loss: -0.0034806705079972744
        total_loss: -0.00444515747949481
        vf_explained_var: 0.0024556368589401245
        vf_loss: 0.32285529375076294
      agent-1:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47960615158081055
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012242778902873397
        model: {}
        policy_loss: -0.0023573804646730423
        total_loss: -0.003181581851094961
        vf_explained_var: 0.026451781392097473
        vf_loss: 0.19907350838184357
      agent-2:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49725186824798584
        entropy_coeff: 0.0017600000137463212
        kl: 0.00129358668345958
        model: {}
        policy_loss: -0.0039251577109098434
        total_loss: -0.004731392487883568
        vf_explained_var: -0.0006111413240432739
        vf_loss: 0.6892656087875366
      agent-3:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.60859215259552
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014537172392010689
        model: {}
        policy_loss: -0.0031653926707804203
        total_loss: -0.004195861052721739
        vf_explained_var: -0.002974003553390503
        vf_loss: 0.4065752625465393
      agent-4:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6467447876930237
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017583866138011217
        model: {}
        policy_loss: -0.0037369923666119576
        total_loss: -0.004823025781661272
        vf_explained_var: 0.016315490007400513
        vf_loss: 0.5223357081413269
      agent-5:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8215550184249878
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013824832858517766
        model: {}
        policy_loss: -0.002734613139182329
        total_loss: -0.004154306836426258
        vf_explained_var: 0.008912533521652222
        vf_loss: 0.2623986005783081
    load_time_ms: 13564.005
    num_steps_sampled: 21408000
    num_steps_trained: 21408000
    sample_time_ms: 97538.659
    update_time_ms: 14.905
  iterations_since_restore: 63
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.761931818181818
    ram_util_percent: 11.448863636363637
  pid: 13408
  policy_reward_max:
    agent-0: 17.0
    agent-1: 12.0
    agent-2: 18.0
    agent-3: 17.0
    agent-4: 20.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 3.63
    agent-1: 2.14
    agent-2: 5.97
    agent-3: 4.29
    agent-4: 5.42
    agent-5: 3.05
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.12535852097641
    mean_inference_ms: 12.863097511000838
    mean_processing_ms: 58.01524668945271
  time_since_restore: 7848.682362794876
  time_this_iter_s: 123.0310308933258
  time_total_s: 30358.96560525894
  timestamp: 1637227810
  timesteps_since_restore: 6048000
  timesteps_this_iter: 96000
  timesteps_total: 21408000
  training_iteration: 223
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    223 |            30359 | 21408000 |     24.5 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 2.19
    apples_agent-0_min: 0
    apples_agent-1_max: 21
    apples_agent-1_mean: 1.06
    apples_agent-1_min: 0
    apples_agent-2_max: 14
    apples_agent-2_mean: 1.79
    apples_agent-2_min: 0
    apples_agent-3_max: 10
    apples_agent-3_mean: 2.85
    apples_agent-3_min: 0
    apples_agent-4_max: 25
    apples_agent-4_mean: 1.76
    apples_agent-4_min: 0
    apples_agent-5_max: 13
    apples_agent-5_mean: 1.8
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 166
    cleaning_beam_agent-0_mean: 78.42
    cleaning_beam_agent-0_min: 24
    cleaning_beam_agent-1_max: 368
    cleaning_beam_agent-1_mean: 236.24
    cleaning_beam_agent-1_min: 134
    cleaning_beam_agent-2_max: 74
    cleaning_beam_agent-2_mean: 18.64
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 233
    cleaning_beam_agent-3_mean: 105.91
    cleaning_beam_agent-3_min: 31
    cleaning_beam_agent-4_max: 130
    cleaning_beam_agent-4_mean: 45.66
    cleaning_beam_agent-4_min: 10
    cleaning_beam_agent-5_max: 50
    cleaning_beam_agent-5_mean: 6.52
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-32-13
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 49.0
  episode_reward_mean: 20.54
  episode_reward_min: 1.0
  episodes_this_iter: 96
  episodes_total: 21504
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11533.976
    learner:
      agent-0:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5706630945205688
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015380873810499907
        model: {}
        policy_loss: -0.0033447551541030407
        total_loss: -0.004321960266679525
        vf_explained_var: -0.009705930948257446
        vf_loss: 0.27162298560142517
      agent-1:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4654850959777832
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010959331411868334
        model: {}
        policy_loss: -0.0027733216993510723
        total_loss: -0.003582519944757223
        vf_explained_var: 0.012864187359809875
        vf_loss: 0.1005324125289917
      agent-2:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47995859384536743
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011166093172505498
        model: {}
        policy_loss: -0.003692180383950472
        total_loss: -0.004498832393437624
        vf_explained_var: 0.004388868808746338
        vf_loss: 0.3807370662689209
      agent-3:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5974643230438232
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013383938930928707
        model: {}
        policy_loss: -0.0032151921186596155
        total_loss: -0.004236321430653334
        vf_explained_var: -0.004322946071624756
        vf_loss: 0.3040829300880432
      agent-4:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6505935788154602
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012052151141688228
        model: {}
        policy_loss: -0.0039640492759644985
        total_loss: -0.005069397389888763
        vf_explained_var: 0.012563660740852356
        vf_loss: 0.39696037769317627
      agent-5:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8398432731628418
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016109093558043242
        model: {}
        policy_loss: -0.0031012408435344696
        total_loss: -0.0045592631213366985
        vf_explained_var: 0.0024312585592269897
        vf_loss: 0.20103666186332703
    load_time_ms: 13592.204
    num_steps_sampled: 21504000
    num_steps_trained: 21504000
    sample_time_ms: 97634.002
    update_time_ms: 14.724
  iterations_since_restore: 64
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.861142857142855
    ram_util_percent: 11.423428571428571
  pid: 13408
  policy_reward_max:
    agent-0: 11.0
    agent-1: 7.0
    agent-2: 14.0
    agent-3: 11.0
    agent-4: 14.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 3.27
    agent-1: 1.57
    agent-2: 4.47
    agent-3: 3.83
    agent-4: 4.81
    agent-5: 2.59
  policy_reward_min:
    agent-0: -1.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.1184645805081
    mean_inference_ms: 12.85995762076181
    mean_processing_ms: 58.00733994816165
  time_since_restore: 7971.597806692123
  time_this_iter_s: 122.91544389724731
  time_total_s: 30481.88104915619
  timestamp: 1637227933
  timesteps_since_restore: 6144000
  timesteps_this_iter: 96000
  timesteps_total: 21504000
  training_iteration: 224
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    224 |          30481.9 | 21504000 |    20.54 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 2.31
    apples_agent-0_min: 0
    apples_agent-1_max: 13
    apples_agent-1_mean: 1.24
    apples_agent-1_min: 0
    apples_agent-2_max: 14
    apples_agent-2_mean: 2.06
    apples_agent-2_min: 0
    apples_agent-3_max: 15
    apples_agent-3_mean: 3.12
    apples_agent-3_min: 0
    apples_agent-4_max: 17
    apples_agent-4_mean: 1.28
    apples_agent-4_min: 0
    apples_agent-5_max: 19
    apples_agent-5_mean: 1.27
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 151
    cleaning_beam_agent-0_mean: 72.56
    cleaning_beam_agent-0_min: 31
    cleaning_beam_agent-1_max: 388
    cleaning_beam_agent-1_mean: 241.43
    cleaning_beam_agent-1_min: 150
    cleaning_beam_agent-2_max: 76
    cleaning_beam_agent-2_mean: 19.02
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 250
    cleaning_beam_agent-3_mean: 119.98
    cleaning_beam_agent-3_min: 52
    cleaning_beam_agent-4_max: 135
    cleaning_beam_agent-4_mean: 46.7
    cleaning_beam_agent-4_min: 8
    cleaning_beam_agent-5_max: 55
    cleaning_beam_agent-5_mean: 6.76
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-34-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 49.0
  episode_reward_mean: 20.86
  episode_reward_min: 3.0
  episodes_this_iter: 96
  episodes_total: 21600
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11531.194
    learner:
      agent-0:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5709390044212341
        entropy_coeff: 0.0017600000137463212
        kl: 0.001226431573741138
        model: {}
        policy_loss: -0.003644648939371109
        total_loss: -0.00462719053030014
        vf_explained_var: -0.019343584775924683
        vf_loss: 0.22314143180847168
      agent-1:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4782782196998596
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011029639281332493
        model: {}
        policy_loss: -0.002698037540540099
        total_loss: -0.003525390289723873
        vf_explained_var: 0.023971915245056152
        vf_loss: 0.1441374272108078
      agent-2:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.480400949716568
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012615704908967018
        model: {}
        policy_loss: -0.003798290155827999
        total_loss: -0.004600549582391977
        vf_explained_var: -0.015588909387588501
        vf_loss: 0.4324798882007599
      agent-3:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6097392439842224
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016873293789103627
        model: {}
        policy_loss: -0.0031519951298832893
        total_loss: -0.00419466570019722
        vf_explained_var: 0.005536407232284546
        vf_loss: 0.30469873547554016
      agent-4:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6460998058319092
        entropy_coeff: 0.0017600000137463212
        kl: 0.001831998466514051
        model: {}
        policy_loss: -0.003912134096026421
        total_loss: -0.005015009082853794
        vf_explained_var: 0.01913420855998993
        vf_loss: 0.342607319355011
      agent-5:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8026388883590698
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015456255059689283
        model: {}
        policy_loss: -0.002616042736917734
        total_loss: -0.0040123192593455315
        vf_explained_var: 0.011735126376152039
        vf_loss: 0.16365157067775726
    load_time_ms: 13579.062
    num_steps_sampled: 21600000
    num_steps_trained: 21600000
    sample_time_ms: 97714.286
    update_time_ms: 14.84
  iterations_since_restore: 65
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.27771428571429
    ram_util_percent: 11.425142857142855
  pid: 13408
  policy_reward_max:
    agent-0: 11.0
    agent-1: 7.0
    agent-2: 18.0
    agent-3: 11.0
    agent-4: 13.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 3.25
    agent-1: 1.99
    agent-2: 4.82
    agent-3: 4.3
    agent-4: 4.31
    agent-5: 2.19
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -2.0
  sampler_perf:
    mean_env_wait_ms: 23.111359185779826
    mean_inference_ms: 12.857133596264532
    mean_processing_ms: 57.99786005935735
  time_since_restore: 8094.932853221893
  time_this_iter_s: 123.3350465297699
  time_total_s: 30605.21609568596
  timestamp: 1637228056
  timesteps_since_restore: 6240000
  timesteps_this_iter: 96000
  timesteps_total: 21600000
  training_iteration: 225
  trial_id: '00000'
  
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
== Status ==
Memory usage on this node: 20.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    225 |          30605.2 | 21600000 |    20.86 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 2.3
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.91
    apples_agent-1_min: 0
    apples_agent-2_max: 19
    apples_agent-2_mean: 2.62
    apples_agent-2_min: 0
    apples_agent-3_max: 10
    apples_agent-3_mean: 2.51
    apples_agent-3_min: 0
    apples_agent-4_max: 61
    apples_agent-4_mean: 1.72
    apples_agent-4_min: 0
    apples_agent-5_max: 40
    apples_agent-5_mean: 2.1
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 145
    cleaning_beam_agent-0_mean: 78.67
    cleaning_beam_agent-0_min: 26
    cleaning_beam_agent-1_max: 444
    cleaning_beam_agent-1_mean: 247.01
    cleaning_beam_agent-1_min: 129
    cleaning_beam_agent-2_max: 58
    cleaning_beam_agent-2_mean: 19.16
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 234
    cleaning_beam_agent-3_mean: 118.91
    cleaning_beam_agent-3_min: 40
    cleaning_beam_agent-4_max: 151
    cleaning_beam_agent-4_mean: 47.51
    cleaning_beam_agent-4_min: 5
    cleaning_beam_agent-5_max: 22
    cleaning_beam_agent-5_mean: 5.94
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-36-18
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 60.0
  episode_reward_mean: 21.34
  episode_reward_min: -72.0
  episodes_this_iter: 96
  episodes_total: 21696
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11534.863
    learner:
      agent-0:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5743730664253235
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015556097496300936
        model: {}
        policy_loss: -0.0034824528265744448
        total_loss: -0.004464795812964439
        vf_explained_var: -0.012022256851196289
        vf_loss: 0.2855217456817627
      agent-1:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4758546054363251
        entropy_coeff: 0.0017600000137463212
        kl: 0.00110011943615973
        model: {}
        policy_loss: -0.002568094525486231
        total_loss: -0.0033896812237799168
        vf_explained_var: 0.019703924655914307
        vf_loss: 0.15919579565525055
      agent-2:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4808272123336792
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014350421261042356
        model: {}
        policy_loss: -0.0037051497492939234
        total_loss: -0.004490346182137728
        vf_explained_var: -0.002019166946411133
        vf_loss: 0.6106054186820984
      agent-3:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6061033010482788
        entropy_coeff: 0.0017600000137463212
        kl: 0.001355965156108141
        model: {}
        policy_loss: -0.002969399094581604
        total_loss: -0.004005590919405222
        vf_explained_var: -0.001910090446472168
        vf_loss: 0.305500328540802
      agent-4:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.65514075756073
        entropy_coeff: 0.0017600000137463212
        kl: 0.001300194999203086
        model: {}
        policy_loss: -0.003662524279206991
        total_loss: -0.0047665671445429325
        vf_explained_var: 0.011648759245872498
        vf_loss: 0.49005648493766785
      agent-5:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8122038841247559
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017620122525840998
        model: {}
        policy_loss: -0.001629683538340032
        total_loss: -0.0029164294246584177
        vf_explained_var: -0.0013063251972198486
        vf_loss: 1.427342414855957
    load_time_ms: 13590.432
    num_steps_sampled: 21696000
    num_steps_trained: 21696000
    sample_time_ms: 97580.305
    update_time_ms: 14.801
  iterations_since_restore: 66
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.423121387283242
    ram_util_percent: 11.490751445086705
  pid: 13408
  policy_reward_max:
    agent-0: 9.0
    agent-1: 7.0
    agent-2: 24.0
    agent-3: 10.0
    agent-4: 14.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 2.8
    agent-1: 2.07
    agent-2: 5.69
    agent-3: 3.86
    agent-4: 4.29
    agent-5: 2.63
  policy_reward_min:
    agent-0: -45.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -45.0
    agent-5: -45.0
  sampler_perf:
    mean_env_wait_ms: 23.102410181152237
    mean_inference_ms: 12.854420176738584
    mean_processing_ms: 57.98351683662593
  time_since_restore: 8215.98008275032
  time_this_iter_s: 121.04722952842712
  time_total_s: 30726.263325214386
  timestamp: 1637228178
  timesteps_since_restore: 6336000
  timesteps_this_iter: 96000
  timesteps_total: 21696000
  training_iteration: 226
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    226 |          30726.3 | 21696000 |    21.34 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 34
    apples_agent-0_mean: 2.83
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 0.95
    apples_agent-1_min: 0
    apples_agent-2_max: 29
    apples_agent-2_mean: 1.82
    apples_agent-2_min: 0
    apples_agent-3_max: 14
    apples_agent-3_mean: 2.67
    apples_agent-3_min: 0
    apples_agent-4_max: 58
    apples_agent-4_mean: 1.73
    apples_agent-4_min: 0
    apples_agent-5_max: 17
    apples_agent-5_mean: 1.74
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 163
    cleaning_beam_agent-0_mean: 81.49
    cleaning_beam_agent-0_min: 32
    cleaning_beam_agent-1_max: 379
    cleaning_beam_agent-1_mean: 245.77
    cleaning_beam_agent-1_min: 138
    cleaning_beam_agent-2_max: 72
    cleaning_beam_agent-2_mean: 18.33
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 212
    cleaning_beam_agent-3_mean: 120.8
    cleaning_beam_agent-3_min: 44
    cleaning_beam_agent-4_max: 155
    cleaning_beam_agent-4_mean: 43.22
    cleaning_beam_agent-4_min: 3
    cleaning_beam_agent-5_max: 38
    cleaning_beam_agent-5_mean: 5.71
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-38-21
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 64.0
  episode_reward_mean: 23.09
  episode_reward_min: 1.0
  episodes_this_iter: 96
  episodes_total: 21792
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11530.964
    learner:
      agent-0:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5765877366065979
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016172381583601236
        model: {}
        policy_loss: -0.0037540076300501823
        total_loss: -0.004738884046673775
        vf_explained_var: -0.00025995075702667236
        vf_loss: 0.29919201135635376
      agent-1:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4862236976623535
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013168243458494544
        model: {}
        policy_loss: -0.0027530239894986153
        total_loss: -0.003596581518650055
        vf_explained_var: 0.03046584129333496
        vf_loss: 0.1219346821308136
      agent-2:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48041099309921265
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012446804903447628
        model: {}
        policy_loss: -0.0036730619613081217
        total_loss: -0.004471479915082455
        vf_explained_var: 0.005678936839103699
        vf_loss: 0.4710499048233032
      agent-3:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5890334248542786
        entropy_coeff: 0.0017600000137463212
        kl: 0.001109098782762885
        model: {}
        policy_loss: -0.002649532398208976
        total_loss: -0.0036394461058080196
        vf_explained_var: -0.0015758126974105835
        vf_loss: 0.46787551045417786
      agent-4:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6446030735969543
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013962388038635254
        model: {}
        policy_loss: -0.003999780863523483
        total_loss: -0.005085582844913006
        vf_explained_var: 0.02876158058643341
        vf_loss: 0.4870032072067261
      agent-5:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8162432312965393
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012613845756277442
        model: {}
        policy_loss: -0.002772328443825245
        total_loss: -0.0041875457391142845
        vf_explained_var: 0.012406274676322937
        vf_loss: 0.21370917558670044
    load_time_ms: 13599.464
    num_steps_sampled: 21792000
    num_steps_trained: 21792000
    sample_time_ms: 97603.033
    update_time_ms: 14.943
  iterations_since_restore: 67
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.386857142857146
    ram_util_percent: 11.441142857142856
  pid: 13408
  policy_reward_max:
    agent-0: 12.0
    agent-1: 7.0
    agent-2: 18.0
    agent-3: 19.0
    agent-4: 16.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 3.79
    agent-1: 1.91
    agent-2: 4.96
    agent-3: 4.23
    agent-4: 5.27
    agent-5: 2.93
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.09670345141449
    mean_inference_ms: 12.851820312031561
    mean_processing_ms: 57.97440096537911
  time_since_restore: 8339.09559226036
  time_this_iter_s: 123.11550951004028
  time_total_s: 30849.378834724426
  timestamp: 1637228301
  timesteps_since_restore: 6432000
  timesteps_this_iter: 96000
  timesteps_total: 21792000
  training_iteration: 227
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    227 |          30849.4 | 21792000 |    23.09 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 2.68
    apples_agent-0_min: 0
    apples_agent-1_max: 8
    apples_agent-1_mean: 0.94
    apples_agent-1_min: 0
    apples_agent-2_max: 30
    apples_agent-2_mean: 2.3
    apples_agent-2_min: 0
    apples_agent-3_max: 39
    apples_agent-3_mean: 3.37
    apples_agent-3_min: 0
    apples_agent-4_max: 58
    apples_agent-4_mean: 1.97
    apples_agent-4_min: 0
    apples_agent-5_max: 11
    apples_agent-5_mean: 1.44
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 186
    cleaning_beam_agent-0_mean: 77.29
    cleaning_beam_agent-0_min: 27
    cleaning_beam_agent-1_max: 383
    cleaning_beam_agent-1_mean: 238.66
    cleaning_beam_agent-1_min: 45
    cleaning_beam_agent-2_max: 105
    cleaning_beam_agent-2_mean: 19.51
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 205
    cleaning_beam_agent-3_mean: 113.53
    cleaning_beam_agent-3_min: 29
    cleaning_beam_agent-4_max: 154
    cleaning_beam_agent-4_mean: 43.18
    cleaning_beam_agent-4_min: 11
    cleaning_beam_agent-5_max: 46
    cleaning_beam_agent-5_mean: 7.23
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-40-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 55.0
  episode_reward_mean: 20.91
  episode_reward_min: -86.0
  episodes_this_iter: 96
  episodes_total: 21888
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11525.841
    learner:
      agent-0:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5630931258201599
        entropy_coeff: 0.0017600000137463212
        kl: 0.001408982090651989
        model: {}
        policy_loss: -0.0038845520466566086
        total_loss: -0.004846364725381136
        vf_explained_var: 0.0009412914514541626
        vf_loss: 0.29231470823287964
      agent-1:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4811246693134308
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007706175092607737
        model: {}
        policy_loss: -0.002189592458307743
        total_loss: -0.003021666780114174
        vf_explained_var: 0.021808579564094543
        vf_loss: 0.14705055952072144
      agent-2:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4692313075065613
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014753852738067508
        model: {}
        policy_loss: -0.0032436633482575417
        total_loss: -0.004025059752166271
        vf_explained_var: -0.005316257476806641
        vf_loss: 0.44449684023857117
      agent-3:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5877333879470825
        entropy_coeff: 0.0017600000137463212
        kl: 0.00113545055501163
        model: {}
        policy_loss: -0.0026503168046474457
        total_loss: -0.0036418428644537926
        vf_explained_var: -0.0024505257606506348
        vf_loss: 0.42882582545280457
      agent-4:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.656258225440979
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011014870833605528
        model: {}
        policy_loss: -0.0027043556328862906
        total_loss: -0.0036844410933554173
        vf_explained_var: 0.005849272012710571
        vf_loss: 1.7493009567260742
      agent-5:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8365156650543213
        entropy_coeff: 0.0017600000137463212
        kl: 0.0027687945403158665
        model: {}
        policy_loss: -0.002932465635240078
        total_loss: -0.00425464753061533
        vf_explained_var: -0.002178683876991272
        vf_loss: 1.5008587837219238
    load_time_ms: 13596.648
    num_steps_sampled: 21888000
    num_steps_trained: 21888000
    sample_time_ms: 97514.62
    update_time_ms: 15.074
  iterations_since_restore: 68
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.490751445086705
    ram_util_percent: 11.48092485549133
  pid: 13408
  policy_reward_max:
    agent-0: 12.0
    agent-1: 9.0
    agent-2: 15.0
    agent-3: 19.0
    agent-4: 12.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 3.85
    agent-1: 1.78
    agent-2: 4.49
    agent-3: 4.3
    agent-4: 4.38
    agent-5: 2.11
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -50.0
    agent-5: -45.0
  sampler_perf:
    mean_env_wait_ms: 23.089267815663572
    mean_inference_ms: 12.848660838833384
    mean_processing_ms: 57.96709614055229
  time_since_restore: 8460.66801738739
  time_this_iter_s: 121.57242512702942
  time_total_s: 30970.951259851456
  timestamp: 1637228423
  timesteps_since_restore: 6528000
  timesteps_this_iter: 96000
  timesteps_total: 21888000
  training_iteration: 228
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    228 |            30971 | 21888000 |    20.91 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 2.86
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.76
    apples_agent-1_min: 0
    apples_agent-2_max: 45
    apples_agent-2_mean: 2.76
    apples_agent-2_min: 0
    apples_agent-3_max: 30
    apples_agent-3_mean: 2.72
    apples_agent-3_min: 0
    apples_agent-4_max: 60
    apples_agent-4_mean: 2.04
    apples_agent-4_min: 0
    apples_agent-5_max: 34
    apples_agent-5_mean: 1.96
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 150
    cleaning_beam_agent-0_mean: 78.37
    cleaning_beam_agent-0_min: 26
    cleaning_beam_agent-1_max: 333
    cleaning_beam_agent-1_mean: 235.22
    cleaning_beam_agent-1_min: 139
    cleaning_beam_agent-2_max: 97
    cleaning_beam_agent-2_mean: 18.13
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 263
    cleaning_beam_agent-3_mean: 114.73
    cleaning_beam_agent-3_min: 27
    cleaning_beam_agent-4_max: 190
    cleaning_beam_agent-4_mean: 42.22
    cleaning_beam_agent-4_min: 3
    cleaning_beam_agent-5_max: 35
    cleaning_beam_agent-5_mean: 5.7
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-42-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 47.0
  episode_reward_mean: 20.41
  episode_reward_min: -37.0
  episodes_this_iter: 96
  episodes_total: 21984
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11519.406
    learner:
      agent-0:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5824102759361267
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017435044283047318
        model: {}
        policy_loss: -0.004026585724204779
        total_loss: -0.005026449449360371
        vf_explained_var: -0.005819827318191528
        vf_loss: 0.2517816424369812
      agent-1:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47055965662002563
        entropy_coeff: 0.0017600000137463212
        kl: 0.000972424284555018
        model: {}
        policy_loss: -0.0025401534512639046
        total_loss: -0.0033549852669239044
        vf_explained_var: 0.01569792628288269
        vf_loss: 0.13351643085479736
      agent-2:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46562740206718445
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011613598326221108
        model: {}
        policy_loss: -0.0018492399249225855
        total_loss: -0.0024892520159482956
        vf_explained_var: 0.0027958154678344727
        vf_loss: 1.794898509979248
      agent-3:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5881458520889282
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014778125332668424
        model: {}
        policy_loss: -0.003155332524329424
        total_loss: -0.004158619791269302
        vf_explained_var: -0.0013011395931243896
        vf_loss: 0.31851130723953247
      agent-4:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6597216129302979
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011196426348760724
        model: {}
        policy_loss: -0.002810180187225342
        total_loss: -0.00379398325458169
        vf_explained_var: 0.008840292692184448
        vf_loss: 1.7730903625488281
      agent-5:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7967159152030945
        entropy_coeff: 0.0017600000137463212
        kl: 0.001317901536822319
        model: {}
        policy_loss: -0.0025635333731770515
        total_loss: -0.0039484743028879166
        vf_explained_var: 0.02364496886730194
        vf_loss: 0.1727861613035202
    load_time_ms: 13594.688
    num_steps_sampled: 21984000
    num_steps_trained: 21984000
    sample_time_ms: 97448.041
    update_time_ms: 15.134
  iterations_since_restore: 69
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.818749999999998
    ram_util_percent: 11.438636363636364
  pid: 13408
  policy_reward_max:
    agent-0: 12.0
    agent-1: 7.0
    agent-2: 18.0
    agent-3: 12.0
    agent-4: 15.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 3.52
    agent-1: 1.8
    agent-2: 3.85
    agent-3: 4.02
    agent-4: 4.75
    agent-5: 2.47
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -49.0
    agent-3: 0.0
    agent-4: -47.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.082558097209542
    mean_inference_ms: 12.845894369436241
    mean_processing_ms: 57.96156006799116
  time_since_restore: 8584.091348171234
  time_this_iter_s: 123.423330783844
  time_total_s: 31094.3745906353
  timestamp: 1637228546
  timesteps_since_restore: 6624000
  timesteps_this_iter: 96000
  timesteps_total: 21984000
  training_iteration: 229
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    229 |          31094.4 | 21984000 |    20.41 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 60
    apples_agent-0_mean: 3.0
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 1.16
    apples_agent-1_min: 0
    apples_agent-2_max: 18
    apples_agent-2_mean: 2.3
    apples_agent-2_min: 0
    apples_agent-3_max: 21
    apples_agent-3_mean: 2.86
    apples_agent-3_min: 0
    apples_agent-4_max: 29
    apples_agent-4_mean: 1.53
    apples_agent-4_min: 0
    apples_agent-5_max: 15
    apples_agent-5_mean: 1.58
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 168
    cleaning_beam_agent-0_mean: 81.88
    cleaning_beam_agent-0_min: 29
    cleaning_beam_agent-1_max: 412
    cleaning_beam_agent-1_mean: 240.08
    cleaning_beam_agent-1_min: 138
    cleaning_beam_agent-2_max: 110
    cleaning_beam_agent-2_mean: 19.04
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 214
    cleaning_beam_agent-3_mean: 103.92
    cleaning_beam_agent-3_min: 30
    cleaning_beam_agent-4_max: 149
    cleaning_beam_agent-4_mean: 40.26
    cleaning_beam_agent-4_min: 6
    cleaning_beam_agent-5_max: 49
    cleaning_beam_agent-5_mean: 7.95
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-44-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 53.0
  episode_reward_mean: 19.97
  episode_reward_min: -35.0
  episodes_this_iter: 96
  episodes_total: 22080
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11532.664
    learner:
      agent-0:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5793245434761047
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016502703074365854
        model: {}
        policy_loss: -0.0037115139421075583
        total_loss: -0.004705263767391443
        vf_explained_var: -0.011980205774307251
        vf_loss: 0.2586353123188019
      agent-1:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4745277464389801
        entropy_coeff: 0.0017600000137463212
        kl: 0.001158461906015873
        model: {}
        policy_loss: -0.00282691465690732
        total_loss: -0.0036476925015449524
        vf_explained_var: 0.03062520921230316
        vf_loss: 0.1438918560743332
      agent-2:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47267287969589233
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009720987873151898
        model: {}
        policy_loss: -0.003343641757965088
        total_loss: -0.004131997004151344
        vf_explained_var: 0.006051495671272278
        vf_loss: 0.43550753593444824
      agent-3:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5889354944229126
        entropy_coeff: 0.0017600000137463212
        kl: 0.001831432688049972
        model: {}
        policy_loss: -0.002970891073346138
        total_loss: -0.003972615115344524
        vf_explained_var: 0.001142933964729309
        vf_loss: 0.34802788496017456
      agent-4:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6656287312507629
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014186253538355231
        model: {}
        policy_loss: -0.004179967101663351
        total_loss: -0.005315677262842655
        vf_explained_var: 0.020052269101142883
        vf_loss: 0.3579536974430084
      agent-5:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8352534770965576
        entropy_coeff: 0.0017600000137463212
        kl: 0.001968692522495985
        model: {}
        policy_loss: -0.0030449572950601578
        total_loss: -0.004495749715715647
        vf_explained_var: 0.010860681533813477
        vf_loss: 0.19253692030906677
    load_time_ms: 13592.665
    num_steps_sampled: 22080000
    num_steps_trained: 22080000
    sample_time_ms: 97506.968
    update_time_ms: 15.085
  iterations_since_restore: 70
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.52057142857143
    ram_util_percent: 11.414285714285716
  pid: 13408
  policy_reward_max:
    agent-0: 13.0
    agent-1: 7.0
    agent-2: 16.0
    agent-3: 15.0
    agent-4: 14.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 3.07
    agent-1: 1.97
    agent-2: 3.91
    agent-3: 3.85
    agent-4: 4.56
    agent-5: 2.61
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -46.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.074534546115775
    mean_inference_ms: 12.84360370201064
    mean_processing_ms: 57.95177808280709
  time_since_restore: 8707.13439655304
  time_this_iter_s: 123.04304838180542
  time_total_s: 31217.417639017105
  timestamp: 1637228669
  timesteps_since_restore: 6720000
  timesteps_this_iter: 96000
  timesteps_total: 22080000
  training_iteration: 230
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    230 |          31217.4 | 22080000 |    19.97 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 2.58
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 1.0
    apples_agent-1_min: 0
    apples_agent-2_max: 35
    apples_agent-2_mean: 2.72
    apples_agent-2_min: 0
    apples_agent-3_max: 10
    apples_agent-3_mean: 2.44
    apples_agent-3_min: 0
    apples_agent-4_max: 20
    apples_agent-4_mean: 1.56
    apples_agent-4_min: 0
    apples_agent-5_max: 8
    apples_agent-5_mean: 1.68
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 161
    cleaning_beam_agent-0_mean: 79.35
    cleaning_beam_agent-0_min: 29
    cleaning_beam_agent-1_max: 377
    cleaning_beam_agent-1_mean: 239.42
    cleaning_beam_agent-1_min: 134
    cleaning_beam_agent-2_max: 167
    cleaning_beam_agent-2_mean: 18.23
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 229
    cleaning_beam_agent-3_mean: 102.14
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 149
    cleaning_beam_agent-4_mean: 39.54
    cleaning_beam_agent-4_min: 11
    cleaning_beam_agent-5_max: 58
    cleaning_beam_agent-5_mean: 6.8
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-46-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 55.0
  episode_reward_mean: 21.34
  episode_reward_min: -70.0
  episodes_this_iter: 96
  episodes_total: 22176
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11526.915
    learner:
      agent-0:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5825241804122925
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013124854303896427
        model: {}
        policy_loss: -0.003643294330686331
        total_loss: -0.0046393005177378654
        vf_explained_var: -0.0024353861808776855
        vf_loss: 0.29236623644828796
      agent-1:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.485994815826416
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013519852655008435
        model: {}
        policy_loss: -0.0027814474888145924
        total_loss: -0.003622868563979864
        vf_explained_var: 0.033361613750457764
        vf_loss: 0.13930554687976837
      agent-2:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4659733176231384
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013341009616851807
        model: {}
        policy_loss: -0.0037444825284183025
        total_loss: -0.004517063032835722
        vf_explained_var: 0.002558082342147827
        vf_loss: 0.4753325581550598
      agent-3:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5856096744537354
        entropy_coeff: 0.0017600000137463212
        kl: 0.001597104244865477
        model: {}
        policy_loss: -0.002278488129377365
        total_loss: -0.0031499541364610195
        vf_explained_var: 0.001989200711250305
        vf_loss: 1.5920634269714355
      agent-4:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6572760343551636
        entropy_coeff: 0.0017600000137463212
        kl: 0.001032388536259532
        model: {}
        policy_loss: -0.001370720099657774
        total_loss: -0.002347639761865139
        vf_explained_var: 0.008890345692634583
        vf_loss: 1.7988227605819702
      agent-5:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.850238025188446
        entropy_coeff: 0.0017600000137463212
        kl: 0.002176228677853942
        model: {}
        policy_loss: -0.0029881703667342663
        total_loss: -0.00446300208568573
        vf_explained_var: -0.0015149414539337158
        vf_loss: 0.2158574014902115
    load_time_ms: 13569.565
    num_steps_sampled: 22176000
    num_steps_trained: 22176000
    sample_time_ms: 97459.604
    update_time_ms: 15.057
  iterations_since_restore: 71
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.544252873563217
    ram_util_percent: 11.42873563218391
  pid: 13408
  policy_reward_max:
    agent-0: 13.0
    agent-1: 7.0
    agent-2: 16.0
    agent-3: 13.0
    agent-4: 13.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 3.51
    agent-1: 2.02
    agent-2: 4.92
    agent-3: 3.37
    agent-4: 4.58
    agent-5: 2.94
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -39.0
    agent-4: -47.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.06718823667523
    mean_inference_ms: 12.841203887248378
    mean_processing_ms: 57.94396269222522
  time_since_restore: 8828.88363146782
  time_this_iter_s: 121.74923491477966
  time_total_s: 31339.166873931885
  timestamp: 1637228792
  timesteps_since_restore: 6816000
  timesteps_this_iter: 96000
  timesteps_total: 22176000
  training_iteration: 231
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    231 |          31339.2 | 22176000 |    21.34 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 8
    apples_agent-0_mean: 2.62
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.93
    apples_agent-1_min: 0
    apples_agent-2_max: 10
    apples_agent-2_mean: 1.81
    apples_agent-2_min: 0
    apples_agent-3_max: 16
    apples_agent-3_mean: 2.6
    apples_agent-3_min: 0
    apples_agent-4_max: 10
    apples_agent-4_mean: 1.51
    apples_agent-4_min: 0
    apples_agent-5_max: 12
    apples_agent-5_mean: 1.51
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 169
    cleaning_beam_agent-0_mean: 77.86
    cleaning_beam_agent-0_min: 18
    cleaning_beam_agent-1_max: 386
    cleaning_beam_agent-1_mean: 234.62
    cleaning_beam_agent-1_min: 115
    cleaning_beam_agent-2_max: 92
    cleaning_beam_agent-2_mean: 19.22
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 207
    cleaning_beam_agent-3_mean: 89.62
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 162
    cleaning_beam_agent-4_mean: 40.95
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 43
    cleaning_beam_agent-5_mean: 6.99
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-48-35
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 78.0
  episode_reward_mean: 21.7
  episode_reward_min: 4.0
  episodes_this_iter: 96
  episodes_total: 22272
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11536.211
    learner:
      agent-0:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5774229168891907
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013165459968149662
        model: {}
        policy_loss: -0.003592472756281495
        total_loss: -0.004584084264934063
        vf_explained_var: -0.007294401526451111
        vf_loss: 0.24653074145317078
      agent-1:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48794978857040405
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017099458491429687
        model: {}
        policy_loss: -0.0029618104454129934
        total_loss: -0.003808670211583376
        vf_explained_var: 0.01937367022037506
        vf_loss: 0.11932046711444855
      agent-2:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4685181677341461
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012425279710441828
        model: {}
        policy_loss: -0.00329802930355072
        total_loss: -0.004070586990565062
        vf_explained_var: 0.013387605547904968
        vf_loss: 0.5203288793563843
      agent-3:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5704424977302551
        entropy_coeff: 0.0017600000137463212
        kl: 0.001441924017854035
        model: {}
        policy_loss: -0.0030783326365053654
        total_loss: -0.00403251638635993
        vf_explained_var: 0.0014196187257766724
        vf_loss: 0.49791157245635986
      agent-4:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6539334058761597
        entropy_coeff: 0.0017600000137463212
        kl: 0.001426944276317954
        model: {}
        policy_loss: -0.003676877124235034
        total_loss: -0.004782153759151697
        vf_explained_var: 0.02290491759777069
        vf_loss: 0.45645830035209656
      agent-5:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8482346534729004
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022452801931649446
        model: {}
        policy_loss: -0.0029305717907845974
        total_loss: -0.004400231409817934
        vf_explained_var: 0.004463493824005127
        vf_loss: 0.23232904076576233
    load_time_ms: 13573.621
    num_steps_sampled: 22272000
    num_steps_trained: 22272000
    sample_time_ms: 97426.616
    update_time_ms: 14.871
  iterations_since_restore: 72
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.725142857142856
    ram_util_percent: 11.503428571428572
  pid: 13408
  policy_reward_max:
    agent-0: 10.0
    agent-1: 8.0
    agent-2: 20.0
    agent-3: 26.0
    agent-4: 24.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 3.39
    agent-1: 1.7
    agent-2: 5.07
    agent-3: 4.02
    agent-4: 4.8
    agent-5: 2.72
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.060496415753583
    mean_inference_ms: 12.838241911048296
    mean_processing_ms: 57.935293298314484
  time_since_restore: 8952.124097824097
  time_this_iter_s: 123.24046635627747
  time_total_s: 31462.407340288162
  timestamp: 1637228915
  timesteps_since_restore: 6912000
  timesteps_this_iter: 96000
  timesteps_total: 22272000
  training_iteration: 232
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    232 |          31462.4 | 22272000 |     21.7 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 2.04
    apples_agent-0_min: 0
    apples_agent-1_max: 11
    apples_agent-1_mean: 0.92
    apples_agent-1_min: 0
    apples_agent-2_max: 9
    apples_agent-2_mean: 1.77
    apples_agent-2_min: 0
    apples_agent-3_max: 17
    apples_agent-3_mean: 2.64
    apples_agent-3_min: 0
    apples_agent-4_max: 114
    apples_agent-4_mean: 2.27
    apples_agent-4_min: 0
    apples_agent-5_max: 69
    apples_agent-5_mean: 2.03
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 189
    cleaning_beam_agent-0_mean: 86.97
    cleaning_beam_agent-0_min: 18
    cleaning_beam_agent-1_max: 353
    cleaning_beam_agent-1_mean: 226.39
    cleaning_beam_agent-1_min: 132
    cleaning_beam_agent-2_max: 104
    cleaning_beam_agent-2_mean: 16.8
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 207
    cleaning_beam_agent-3_mean: 102.91
    cleaning_beam_agent-3_min: 32
    cleaning_beam_agent-4_max: 124
    cleaning_beam_agent-4_mean: 42.54
    cleaning_beam_agent-4_min: 4
    cleaning_beam_agent-5_max: 22
    cleaning_beam_agent-5_mean: 5.58
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-50-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 87.0
  episode_reward_mean: 21.32
  episode_reward_min: 2.0
  episodes_this_iter: 96
  episodes_total: 22368
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11511.259
    learner:
      agent-0:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5894134044647217
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018210000125691295
        model: {}
        policy_loss: -0.0038546163123100996
        total_loss: -0.004865369759500027
        vf_explained_var: -0.0030222088098526
        vf_loss: 0.26613497734069824
      agent-1:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47829654812812805
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014463714323937893
        model: {}
        policy_loss: -0.002732759341597557
        total_loss: -0.0035595307126641273
        vf_explained_var: 0.02553766965866089
        vf_loss: 0.15032503008842468
      agent-2:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46821504831314087
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013491420540958643
        model: {}
        policy_loss: -0.003637409768998623
        total_loss: -0.004417387302964926
        vf_explained_var: -0.013643354177474976
        vf_loss: 0.4408096373081207
      agent-3:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5662757754325867
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016018933383747935
        model: {}
        policy_loss: -0.0030619027093052864
        total_loss: -0.004010842647403479
        vf_explained_var: 5.37186861038208e-05
        vf_loss: 0.47705137729644775
      agent-4:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6573281288146973
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016649647150188684
        model: {}
        policy_loss: -0.004161323420703411
        total_loss: -0.005276472307741642
        vf_explained_var: 0.003364384174346924
        vf_loss: 0.417488694190979
      agent-5:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8624749779701233
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014482870465144515
        model: {}
        policy_loss: -0.0026242146268486977
        total_loss: -0.004121111240237951
        vf_explained_var: -0.0012944191694259644
        vf_loss: 0.21058057248592377
    load_time_ms: 13560.338
    num_steps_sampled: 22368000
    num_steps_trained: 22368000
    sample_time_ms: 97481.37
    update_time_ms: 14.982
  iterations_since_restore: 73
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.144886363636363
    ram_util_percent: 11.391477272727274
  pid: 13408
  policy_reward_max:
    agent-0: 12.0
    agent-1: 12.0
    agent-2: 14.0
    agent-3: 30.0
    agent-4: 16.0
    agent-5: 14.0
  policy_reward_mean:
    agent-0: 2.94
    agent-1: 1.86
    agent-2: 4.85
    agent-3: 4.25
    agent-4: 4.75
    agent-5: 2.67
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.054338639801085
    mean_inference_ms: 12.835413609524444
    mean_processing_ms: 57.927657418837356
  time_since_restore: 9075.280557870865
  time_this_iter_s: 123.15646004676819
  time_total_s: 31585.56380033493
  timestamp: 1637229038
  timesteps_since_restore: 7008000
  timesteps_this_iter: 96000
  timesteps_total: 22368000
  training_iteration: 233
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    233 |          31585.6 | 22368000 |    21.32 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 35
    apples_agent-0_mean: 2.67
    apples_agent-0_min: 0
    apples_agent-1_max: 8
    apples_agent-1_mean: 1.03
    apples_agent-1_min: 0
    apples_agent-2_max: 21
    apples_agent-2_mean: 2.19
    apples_agent-2_min: 0
    apples_agent-3_max: 13
    apples_agent-3_mean: 2.56
    apples_agent-3_min: 0
    apples_agent-4_max: 24
    apples_agent-4_mean: 1.49
    apples_agent-4_min: 0
    apples_agent-5_max: 20
    apples_agent-5_mean: 1.78
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 182
    cleaning_beam_agent-0_mean: 81.76
    cleaning_beam_agent-0_min: 20
    cleaning_beam_agent-1_max: 341
    cleaning_beam_agent-1_mean: 224.39
    cleaning_beam_agent-1_min: 132
    cleaning_beam_agent-2_max: 55
    cleaning_beam_agent-2_mean: 12.49
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 233
    cleaning_beam_agent-3_mean: 103.79
    cleaning_beam_agent-3_min: 35
    cleaning_beam_agent-4_max: 112
    cleaning_beam_agent-4_mean: 37.38
    cleaning_beam_agent-4_min: 8
    cleaning_beam_agent-5_max: 17
    cleaning_beam_agent-5_mean: 5.48
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-52-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 69.0
  episode_reward_mean: 21.25
  episode_reward_min: 5.0
  episodes_this_iter: 96
  episodes_total: 22464
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11537.901
    learner:
      agent-0:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5740221738815308
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012725100386887789
        model: {}
        policy_loss: -0.0035561807453632355
        total_loss: -0.004544012248516083
        vf_explained_var: -0.01022377610206604
        vf_loss: 0.22446951270103455
      agent-1:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47860321402549744
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010786977363750339
        model: {}
        policy_loss: -0.002709062770009041
        total_loss: -0.003535727970302105
        vf_explained_var: 0.024033397436141968
        vf_loss: 0.15675979852676392
      agent-2:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44944891333580017
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015695940237492323
        model: {}
        policy_loss: -0.0034917304292321205
        total_loss: -0.004234873689711094
        vf_explained_var: -0.015091240406036377
        vf_loss: 0.47885575890541077
      agent-3:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5722226500511169
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014437282225117087
        model: {}
        policy_loss: -0.0030685020610690117
        total_loss: -0.004033890552818775
        vf_explained_var: 0.002859368920326233
        vf_loss: 0.4172508418560028
      agent-4:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6562948822975159
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012295334599912167
        model: {}
        policy_loss: -0.0036432479973882437
        total_loss: -0.004762227181345224
        vf_explained_var: 0.019737601280212402
        vf_loss: 0.361019104719162
      agent-5:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8344137668609619
        entropy_coeff: 0.0017600000137463212
        kl: 0.001153340912424028
        model: {}
        policy_loss: -0.002532157115638256
        total_loss: -0.0039736926555633545
        vf_explained_var: 0.009388849139213562
        vf_loss: 0.2703477144241333
    load_time_ms: 13544.653
    num_steps_sampled: 22464000
    num_steps_trained: 22464000
    sample_time_ms: 97484.383
    update_time_ms: 14.969
  iterations_since_restore: 74
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.288571428571434
    ram_util_percent: 11.532571428571428
  pid: 13408
  policy_reward_max:
    agent-0: 10.0
    agent-1: 8.0
    agent-2: 19.0
    agent-3: 20.0
    agent-4: 12.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 3.12
    agent-1: 2.15
    agent-2: 4.74
    agent-3: 4.08
    agent-4: 4.33
    agent-5: 2.83
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.04789903432317
    mean_inference_ms: 12.833557166494629
    mean_processing_ms: 57.924743315387474
  time_since_restore: 9198.333210229874
  time_this_iter_s: 123.05265235900879
  time_total_s: 31708.61645269394
  timestamp: 1637229162
  timesteps_since_restore: 7104000
  timesteps_this_iter: 96000
  timesteps_total: 22464000
  training_iteration: 234
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    234 |          31708.6 | 22464000 |    21.25 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 27
    apples_agent-0_mean: 2.74
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 1.05
    apples_agent-1_min: 0
    apples_agent-2_max: 36
    apples_agent-2_mean: 2.59
    apples_agent-2_min: 0
    apples_agent-3_max: 13
    apples_agent-3_mean: 3.04
    apples_agent-3_min: 0
    apples_agent-4_max: 16
    apples_agent-4_mean: 1.38
    apples_agent-4_min: 0
    apples_agent-5_max: 27
    apples_agent-5_mean: 1.79
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 182
    cleaning_beam_agent-0_mean: 77.21
    cleaning_beam_agent-0_min: 20
    cleaning_beam_agent-1_max: 348
    cleaning_beam_agent-1_mean: 228.06
    cleaning_beam_agent-1_min: 132
    cleaning_beam_agent-2_max: 141
    cleaning_beam_agent-2_mean: 15.01
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 237
    cleaning_beam_agent-3_mean: 91.93
    cleaning_beam_agent-3_min: 27
    cleaning_beam_agent-4_max: 128
    cleaning_beam_agent-4_mean: 39.88
    cleaning_beam_agent-4_min: 8
    cleaning_beam_agent-5_max: 33
    cleaning_beam_agent-5_mean: 6.5
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-54-44
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 48.0
  episode_reward_mean: 21.13
  episode_reward_min: -68.0
  episodes_this_iter: 96
  episodes_total: 22560
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11551.128
    learner:
      agent-0:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5753200054168701
        entropy_coeff: 0.0017600000137463212
        kl: 0.00112708390224725
        model: {}
        policy_loss: -0.0034888286609202623
        total_loss: -0.004471030086278915
        vf_explained_var: 0.005144819617271423
        vf_loss: 0.30360788106918335
      agent-1:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4830622673034668
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008570373756811023
        model: {}
        policy_loss: -0.0021067550405859947
        total_loss: -0.0028090302366763353
        vf_explained_var: 0.0078069716691970825
        vf_loss: 1.479122281074524
      agent-2:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4627380967140198
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016573003958910704
        model: {}
        policy_loss: -0.002503936178982258
        total_loss: -0.0031317933462560177
        vf_explained_var: -0.0022505223751068115
        vf_loss: 1.8655887842178345
      agent-3:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5709993839263916
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012499808799475431
        model: {}
        policy_loss: -0.002634887583553791
        total_loss: -0.0036063422448933125
        vf_explained_var: 0.0006408244371414185
        vf_loss: 0.3350343406200409
      agent-4:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.656714141368866
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017856257036328316
        model: {}
        policy_loss: -0.003904306795448065
        total_loss: -0.005027478560805321
        vf_explained_var: 0.014851599931716919
        vf_loss: 0.32645753026008606
      agent-5:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8651992082595825
        entropy_coeff: 0.0017600000137463212
        kl: 0.0025754740927368402
        model: {}
        policy_loss: -0.002169961342588067
        total_loss: -0.0035450179129838943
        vf_explained_var: 0.0022625327110290527
        vf_loss: 1.4769389629364014
    load_time_ms: 13555.152
    num_steps_sampled: 22560000
    num_steps_trained: 22560000
    sample_time_ms: 97391.517
    update_time_ms: 14.76
  iterations_since_restore: 75
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.341714285714286
    ram_util_percent: 11.49942857142857
  pid: 13408
  policy_reward_max:
    agent-0: 11.0
    agent-1: 12.0
    agent-2: 17.0
    agent-3: 11.0
    agent-4: 14.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 3.78
    agent-1: 1.48
    agent-2: 4.99
    agent-3: 4.13
    agent-4: 4.51
    agent-5: 2.24
  policy_reward_min:
    agent-0: 0.0
    agent-1: -48.0
    agent-2: -45.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -43.0
  sampler_perf:
    mean_env_wait_ms: 23.042594001297164
    mean_inference_ms: 12.830813267075452
    mean_processing_ms: 57.91809216373725
  time_since_restore: 9321.03024148941
  time_this_iter_s: 122.69703125953674
  time_total_s: 31831.313483953476
  timestamp: 1637229284
  timesteps_since_restore: 7200000
  timesteps_this_iter: 96000
  timesteps_total: 22560000
  training_iteration: 235
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    235 |          31831.3 | 22560000 |    21.13 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 2.55
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 1.01
    apples_agent-1_min: 0
    apples_agent-2_max: 15
    apples_agent-2_mean: 2.23
    apples_agent-2_min: 0
    apples_agent-3_max: 8
    apples_agent-3_mean: 2.76
    apples_agent-3_min: 0
    apples_agent-4_max: 11
    apples_agent-4_mean: 1.14
    apples_agent-4_min: 0
    apples_agent-5_max: 19
    apples_agent-5_mean: 1.76
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 222
    cleaning_beam_agent-0_mean: 83.95
    cleaning_beam_agent-0_min: 27
    cleaning_beam_agent-1_max: 337
    cleaning_beam_agent-1_mean: 226.47
    cleaning_beam_agent-1_min: 152
    cleaning_beam_agent-2_max: 60
    cleaning_beam_agent-2_mean: 16.37
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 256
    cleaning_beam_agent-3_mean: 95.43
    cleaning_beam_agent-3_min: 33
    cleaning_beam_agent-4_max: 168
    cleaning_beam_agent-4_mean: 39.96
    cleaning_beam_agent-4_min: 2
    cleaning_beam_agent-5_max: 29
    cleaning_beam_agent-5_mean: 5.86
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-56-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 44.0
  episode_reward_mean: 21.27
  episode_reward_min: -13.0
  episodes_this_iter: 96
  episodes_total: 22656
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11556.25
    learner:
      agent-0:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5858937501907349
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013935822062194347
        model: {}
        policy_loss: -0.0038046608678996563
        total_loss: -0.004807117395102978
        vf_explained_var: -0.010201811790466309
        vf_loss: 0.2871440649032593
      agent-1:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48153626918792725
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011689821258187294
        model: {}
        policy_loss: -0.002281609922647476
        total_loss: -0.0031139031052589417
        vf_explained_var: 0.02085891366004944
        vf_loss: 0.15211685001850128
      agent-2:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4663184881210327
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012866677716374397
        model: {}
        policy_loss: -0.0033088908530771732
        total_loss: -0.0040761735290288925
        vf_explained_var: -0.00025503337383270264
        vf_loss: 0.5343983173370361
      agent-3:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5726574659347534
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012262935051694512
        model: {}
        policy_loss: -0.0028518149629235268
        total_loss: -0.003830241970717907
        vf_explained_var: 0.001173168420791626
        vf_loss: 0.29449325799942017
      agent-4:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.663611650466919
        entropy_coeff: 0.0017600000137463212
        kl: 0.001567334751598537
        model: {}
        policy_loss: -0.0037831920199096203
        total_loss: -0.0049095130525529385
        vf_explained_var: 0.017756715416908264
        vf_loss: 0.41638457775115967
      agent-5:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8506857752799988
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021506119519472122
        model: {}
        policy_loss: -0.0032521323300898075
        total_loss: -0.0047289300709962845
        vf_explained_var: 0.008796319365501404
        vf_loss: 0.20409229397773743
    load_time_ms: 13548.053
    num_steps_sampled: 22656000
    num_steps_trained: 22656000
    sample_time_ms: 97464.66
    update_time_ms: 14.614
  iterations_since_restore: 76
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.432369942196534
    ram_util_percent: 11.484393063583813
  pid: 13408
  policy_reward_max:
    agent-0: 10.0
    agent-1: 7.0
    agent-2: 16.0
    agent-3: 11.0
    agent-4: 13.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 3.45
    agent-1: 1.38
    agent-2: 4.82
    agent-3: 4.05
    agent-4: 4.79
    agent-5: 2.78
  policy_reward_min:
    agent-0: 0.0
    agent-1: -46.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.035124768938445
    mean_inference_ms: 12.828929156327321
    mean_processing_ms: 57.90935088288523
  time_since_restore: 9442.749618768692
  time_this_iter_s: 121.71937727928162
  time_total_s: 31953.032861232758
  timestamp: 1637229406
  timesteps_since_restore: 7296000
  timesteps_this_iter: 96000
  timesteps_total: 22656000
  training_iteration: 236
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    236 |            31953 | 22656000 |    21.27 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 7
    apples_agent-0_mean: 2.15
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.98
    apples_agent-1_min: 0
    apples_agent-2_max: 46
    apples_agent-2_mean: 2.64
    apples_agent-2_min: 0
    apples_agent-3_max: 17
    apples_agent-3_mean: 3.16
    apples_agent-3_min: 0
    apples_agent-4_max: 42
    apples_agent-4_mean: 1.87
    apples_agent-4_min: 0
    apples_agent-5_max: 16
    apples_agent-5_mean: 1.5
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 155
    cleaning_beam_agent-0_mean: 78.7
    cleaning_beam_agent-0_min: 19
    cleaning_beam_agent-1_max: 351
    cleaning_beam_agent-1_mean: 231.21
    cleaning_beam_agent-1_min: 119
    cleaning_beam_agent-2_max: 64
    cleaning_beam_agent-2_mean: 15.11
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 242
    cleaning_beam_agent-3_mean: 101.68
    cleaning_beam_agent-3_min: 33
    cleaning_beam_agent-4_max: 159
    cleaning_beam_agent-4_mean: 39.22
    cleaning_beam_agent-4_min: 4
    cleaning_beam_agent-5_max: 20
    cleaning_beam_agent-5_mean: 5.85
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-58-48
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 49.0
  episode_reward_mean: 22.3
  episode_reward_min: -4.0
  episodes_this_iter: 96
  episodes_total: 22752
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11552.08
    learner:
      agent-0:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5929982662200928
        entropy_coeff: 0.0017600000137463212
        kl: 0.00121173239313066
        model: {}
        policy_loss: -0.0035423394292593002
        total_loss: -0.004560514353215694
        vf_explained_var: -0.012301355600357056
        vf_loss: 0.2550405263900757
      agent-1:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5012786984443665
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010971951996907592
        model: {}
        policy_loss: -0.0027165927458554506
        total_loss: -0.003584716934710741
        vf_explained_var: 0.016679078340530396
        vf_loss: 0.1412583291530609
      agent-2:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4669651389122009
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014763871440663934
        model: {}
        policy_loss: -0.0036765574477612972
        total_loss: -0.004450066480785608
        vf_explained_var: 0.00768183171749115
        vf_loss: 0.4835265278816223
      agent-3:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5827237367630005
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007647618185728788
        model: {}
        policy_loss: -0.0015725288540124893
        total_loss: -0.0024201329797506332
        vf_explained_var: 0.00557824969291687
        vf_loss: 1.779890775680542
      agent-4:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6623240113258362
        entropy_coeff: 0.0017600000137463212
        kl: 0.002014760160818696
        model: {}
        policy_loss: -0.003933284431695938
        total_loss: -0.0050506372936069965
        vf_explained_var: 0.011417657136917114
        vf_loss: 0.48335254192352295
      agent-5:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8180553913116455
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015447658952325583
        model: {}
        policy_loss: -0.0028055179864168167
        total_loss: -0.004224421456456184
        vf_explained_var: 0.013927251100540161
        vf_loss: 0.20872420072555542
    load_time_ms: 13525.548
    num_steps_sampled: 22752000
    num_steps_trained: 22752000
    sample_time_ms: 97373.474
    update_time_ms: 14.49
  iterations_since_restore: 77
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.79022988505747
    ram_util_percent: 11.448850574712644
  pid: 13408
  policy_reward_max:
    agent-0: 10.0
    agent-1: 9.0
    agent-2: 14.0
    agent-3: 13.0
    agent-4: 14.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 3.41
    agent-1: 1.7
    agent-2: 5.07
    agent-3: 4.0
    agent-4: 5.47
    agent-5: 2.65
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -41.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.0280241819205
    mean_inference_ms: 12.826825684303081
    mean_processing_ms: 57.89986981264994
  time_since_restore: 9564.73765039444
  time_this_iter_s: 121.98803162574768
  time_total_s: 32075.020892858505
  timestamp: 1637229528
  timesteps_since_restore: 7392000
  timesteps_this_iter: 96000
  timesteps_total: 22752000
  training_iteration: 237
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    237 |            32075 | 22752000 |     22.3 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 14
    apples_agent-0_mean: 2.78
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 1.04
    apples_agent-1_min: 0
    apples_agent-2_max: 18
    apples_agent-2_mean: 2.14
    apples_agent-2_min: 0
    apples_agent-3_max: 9
    apples_agent-3_mean: 2.82
    apples_agent-3_min: 0
    apples_agent-4_max: 7
    apples_agent-4_mean: 1.18
    apples_agent-4_min: 0
    apples_agent-5_max: 28
    apples_agent-5_mean: 1.62
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 154
    cleaning_beam_agent-0_mean: 76.2
    cleaning_beam_agent-0_min: 26
    cleaning_beam_agent-1_max: 448
    cleaning_beam_agent-1_mean: 247.77
    cleaning_beam_agent-1_min: 155
    cleaning_beam_agent-2_max: 66
    cleaning_beam_agent-2_mean: 16.66
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 201
    cleaning_beam_agent-3_mean: 101.05
    cleaning_beam_agent-3_min: 24
    cleaning_beam_agent-4_max: 143
    cleaning_beam_agent-4_mean: 38.89
    cleaning_beam_agent-4_min: 3
    cleaning_beam_agent-5_max: 22
    cleaning_beam_agent-5_mean: 5.71
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-00-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 49.0
  episode_reward_mean: 21.38
  episode_reward_min: -38.0
  episodes_this_iter: 96
  episodes_total: 22848
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11557.589
    learner:
      agent-0:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.579511284828186
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012040126603096724
        model: {}
        policy_loss: -0.003529271110892296
        total_loss: -0.004515824839472771
        vf_explained_var: -0.010325044393539429
        vf_loss: 0.3338550329208374
      agent-1:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49129679799079895
        entropy_coeff: 0.0017600000137463212
        kl: 0.001317493850365281
        model: {}
        policy_loss: -0.002818524371832609
        total_loss: -0.003669205354526639
        vf_explained_var: 0.020670011639595032
        vf_loss: 0.14001013338565826
      agent-2:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4679785370826721
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013204176211729646
        model: {}
        policy_loss: -0.003554653376340866
        total_loss: -0.004329848103225231
        vf_explained_var: -0.009544998407363892
        vf_loss: 0.4844467043876648
      agent-3:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5797277688980103
        entropy_coeff: 0.0017600000137463212
        kl: 0.001012545544654131
        model: {}
        policy_loss: -0.0026553315110504627
        total_loss: -0.0036446163430809975
        vf_explained_var: -0.006553083658218384
        vf_loss: 0.31032779812812805
      agent-4:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6645678281784058
        entropy_coeff: 0.0017600000137463212
        kl: 0.001261907396838069
        model: {}
        policy_loss: -0.004044606350362301
        total_loss: -0.005169897340238094
        vf_explained_var: 0.005238398909568787
        vf_loss: 0.4434909224510193
      agent-5:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8394808173179626
        entropy_coeff: 0.0017600000137463212
        kl: 0.000994641799479723
        model: {}
        policy_loss: -0.0013529281131923199
        total_loss: -0.002545776776969433
        vf_explained_var: 0.0015730559825897217
        vf_loss: 2.846363067626953
    load_time_ms: 13536.79
    num_steps_sampled: 22848000
    num_steps_trained: 22848000
    sample_time_ms: 97423.471
    update_time_ms: 15.178
  iterations_since_restore: 78
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.085057471264367
    ram_util_percent: 11.547126436781609
  pid: 13408
  policy_reward_max:
    agent-0: 12.0
    agent-1: 7.0
    agent-2: 17.0
    agent-3: 14.0
    agent-4: 15.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 3.9
    agent-1: 2.03
    agent-2: 5.11
    agent-3: 4.08
    agent-4: 5.32
    agent-5: 0.94
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -50.0
  sampler_perf:
    mean_env_wait_ms: 23.021744145727567
    mean_inference_ms: 12.824273345906443
    mean_processing_ms: 57.89269950769165
  time_since_restore: 9686.984649896622
  time_this_iter_s: 122.246999502182
  time_total_s: 32197.267892360687
  timestamp: 1637229651
  timesteps_since_restore: 7488000
  timesteps_this_iter: 96000
  timesteps_total: 22848000
  training_iteration: 238
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    238 |          32197.3 | 22848000 |    21.38 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 2.18
    apples_agent-0_min: 0
    apples_agent-1_max: 9
    apples_agent-1_mean: 1.06
    apples_agent-1_min: 0
    apples_agent-2_max: 8
    apples_agent-2_mean: 1.7
    apples_agent-2_min: 0
    apples_agent-3_max: 19
    apples_agent-3_mean: 2.84
    apples_agent-3_min: 0
    apples_agent-4_max: 14
    apples_agent-4_mean: 1.21
    apples_agent-4_min: 0
    apples_agent-5_max: 16
    apples_agent-5_mean: 1.84
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 200
    cleaning_beam_agent-0_mean: 75.61
    cleaning_beam_agent-0_min: 29
    cleaning_beam_agent-1_max: 344
    cleaning_beam_agent-1_mean: 232.75
    cleaning_beam_agent-1_min: 124
    cleaning_beam_agent-2_max: 100
    cleaning_beam_agent-2_mean: 18.71
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 257
    cleaning_beam_agent-3_mean: 109.75
    cleaning_beam_agent-3_min: 32
    cleaning_beam_agent-4_max: 122
    cleaning_beam_agent-4_mean: 38.19
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 19
    cleaning_beam_agent-5_mean: 5.65
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-02-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 48.0
  episode_reward_mean: 18.89
  episode_reward_min: -42.0
  episodes_this_iter: 96
  episodes_total: 22944
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11556.779
    learner:
      agent-0:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5807026624679565
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018024536548182368
        model: {}
        policy_loss: -0.003830687142908573
        total_loss: -0.004830364137887955
        vf_explained_var: -0.002536401152610779
        vf_loss: 0.2235967218875885
      agent-1:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48871660232543945
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010011198464781046
        model: {}
        policy_loss: -0.00249844160862267
        total_loss: -0.0033448264002799988
        vf_explained_var: 0.017315030097961426
        vf_loss: 0.13755613565444946
      agent-2:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46861031651496887
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011195415863767266
        model: {}
        policy_loss: -0.0034690960310399532
        total_loss: -0.0042524379678070545
        vf_explained_var: -0.00957036018371582
        vf_loss: 0.41411909461021423
      agent-3:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5936227440834045
        entropy_coeff: 0.0017600000137463212
        kl: 0.001563304103910923
        model: {}
        policy_loss: -0.0024870436172932386
        total_loss: -0.003371481318026781
        vf_explained_var: 0.00019945204257965088
        vf_loss: 1.6033793687820435
      agent-4:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6654632687568665
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012686598347499967
        model: {}
        policy_loss: -0.0035850044805556536
        total_loss: -0.004581009037792683
        vf_explained_var: 0.004642605781555176
        vf_loss: 1.7521288394927979
      agent-5:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8656492829322815
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021730675362050533
        model: {}
        policy_loss: -0.00302952341735363
        total_loss: -0.0045328992418944836
        vf_explained_var: 0.004563689231872559
        vf_loss: 0.20166058838367462
    load_time_ms: 13516.57
    num_steps_sampled: 22944000
    num_steps_trained: 22944000
    sample_time_ms: 97314.058
    update_time_ms: 15.05
  iterations_since_restore: 79
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.53390804597701
    ram_util_percent: 11.432183908045976
  pid: 13408
  policy_reward_max:
    agent-0: 11.0
    agent-1: 14.0
    agent-2: 17.0
    agent-3: 12.0
    agent-4: 13.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 3.07
    agent-1: 1.67
    agent-2: 4.45
    agent-3: 3.36
    agent-4: 3.69
    agent-5: 2.65
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -49.0
    agent-4: -49.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.015863985566998
    mean_inference_ms: 12.821518597163184
    mean_processing_ms: 57.886321711828934
  time_since_restore: 9809.117424249649
  time_this_iter_s: 122.13277435302734
  time_total_s: 32319.400666713715
  timestamp: 1637229773
  timesteps_since_restore: 7584000
  timesteps_this_iter: 96000
  timesteps_total: 22944000
  training_iteration: 239
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    239 |          32319.4 | 22944000 |    18.89 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 2.42
    apples_agent-0_min: 0
    apples_agent-1_max: 10
    apples_agent-1_mean: 1.15
    apples_agent-1_min: 0
    apples_agent-2_max: 81
    apples_agent-2_mean: 3.36
    apples_agent-2_min: 0
    apples_agent-3_max: 7
    apples_agent-3_mean: 2.43
    apples_agent-3_min: 0
    apples_agent-4_max: 29
    apples_agent-4_mean: 1.5
    apples_agent-4_min: 0
    apples_agent-5_max: 42
    apples_agent-5_mean: 1.87
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 193
    cleaning_beam_agent-0_mean: 78.61
    cleaning_beam_agent-0_min: 28
    cleaning_beam_agent-1_max: 398
    cleaning_beam_agent-1_mean: 229.77
    cleaning_beam_agent-1_min: 137
    cleaning_beam_agent-2_max: 65
    cleaning_beam_agent-2_mean: 15.77
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 255
    cleaning_beam_agent-3_mean: 115.77
    cleaning_beam_agent-3_min: 40
    cleaning_beam_agent-4_max: 106
    cleaning_beam_agent-4_mean: 39.51
    cleaning_beam_agent-4_min: 3
    cleaning_beam_agent-5_max: 29
    cleaning_beam_agent-5_mean: 6.19
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-04-56
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 51.0
  episode_reward_mean: 20.62
  episode_reward_min: 3.0
  episodes_this_iter: 96
  episodes_total: 23040
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11558.88
    learner:
      agent-0:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5853054523468018
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017181362491101027
        model: {}
        policy_loss: -0.00405769282951951
        total_loss: -0.00506238266825676
        vf_explained_var: -0.0027061104774475098
        vf_loss: 0.2544777989387512
      agent-1:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4984378516674042
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011154951062053442
        model: {}
        policy_loss: -0.0028544622473418713
        total_loss: -0.0037220031954348087
        vf_explained_var: 0.025449514389038086
        vf_loss: 0.09709624946117401
      agent-2:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45802491903305054
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009100641473196447
        model: {}
        policy_loss: -0.003350314684212208
        total_loss: -0.004116246476769447
        vf_explained_var: -0.008547484874725342
        vf_loss: 0.4018622934818268
      agent-3:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5989848375320435
        entropy_coeff: 0.0017600000137463212
        kl: 0.00173568120226264
        model: {}
        policy_loss: -0.0029185451567173004
        total_loss: -0.003941314294934273
        vf_explained_var: -0.0015413165092468262
        vf_loss: 0.31449365615844727
      agent-4:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6780999302864075
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019282808061689138
        model: {}
        policy_loss: -0.004112810827791691
        total_loss: -0.00526576628908515
        vf_explained_var: 0.019784316420555115
        vf_loss: 0.40502336621284485
      agent-5:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8498173356056213
        entropy_coeff: 0.0017600000137463212
        kl: 0.001737291575409472
        model: {}
        policy_loss: -0.003013226669281721
        total_loss: -0.0044892518781125546
        vf_explained_var: 0.016124308109283447
        vf_loss: 0.19650916755199432
    load_time_ms: 13509.536
    num_steps_sampled: 23040000
    num_steps_trained: 23040000
    sample_time_ms: 97302.47
    update_time_ms: 15.558
  iterations_since_restore: 80
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.948275862068964
    ram_util_percent: 11.411494252873563
  pid: 13408
  policy_reward_max:
    agent-0: 11.0
    agent-1: 5.0
    agent-2: 13.0
    agent-3: 12.0
    agent-4: 16.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 3.36
    agent-1: 1.6
    agent-2: 4.45
    agent-3: 3.75
    agent-4: 4.95
    agent-5: 2.51
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.011985003194415
    mean_inference_ms: 12.820128343799556
    mean_processing_ms: 57.88078643352426
  time_since_restore: 9932.04838681221
  time_this_iter_s: 122.93096256256104
  time_total_s: 32442.331629276276
  timestamp: 1637229896
  timesteps_since_restore: 7680000
  timesteps_this_iter: 96000
  timesteps_total: 23040000
  training_iteration: 240
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    240 |          32442.3 | 23040000 |    20.62 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 2.24
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 1.16
    apples_agent-1_min: 0
    apples_agent-2_max: 18
    apples_agent-2_mean: 1.95
    apples_agent-2_min: 0
    apples_agent-3_max: 15
    apples_agent-3_mean: 3.22
    apples_agent-3_min: 0
    apples_agent-4_max: 32
    apples_agent-4_mean: 1.6
    apples_agent-4_min: 0
    apples_agent-5_max: 34
    apples_agent-5_mean: 2.03
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 173
    cleaning_beam_agent-0_mean: 80.35
    cleaning_beam_agent-0_min: 19
    cleaning_beam_agent-1_max: 457
    cleaning_beam_agent-1_mean: 233.39
    cleaning_beam_agent-1_min: 119
    cleaning_beam_agent-2_max: 86
    cleaning_beam_agent-2_mean: 16.28
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 231
    cleaning_beam_agent-3_mean: 105.79
    cleaning_beam_agent-3_min: 27
    cleaning_beam_agent-4_max: 132
    cleaning_beam_agent-4_mean: 43.07
    cleaning_beam_agent-4_min: 6
    cleaning_beam_agent-5_max: 41
    cleaning_beam_agent-5_mean: 6.69
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-06-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 51.0
  episode_reward_mean: 22.99
  episode_reward_min: 4.0
  episodes_this_iter: 96
  episodes_total: 23136
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11557.409
    learner:
      agent-0:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5744693875312805
        entropy_coeff: 0.0017600000137463212
        kl: 0.001248685410246253
        model: {}
        policy_loss: -0.0035242869053035975
        total_loss: -0.004511761479079723
        vf_explained_var: -0.011130660772323608
        vf_loss: 0.23592069745063782
      agent-1:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48533737659454346
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009175543673336506
        model: {}
        policy_loss: -0.002398063661530614
        total_loss: -0.003237258642911911
        vf_explained_var: 0.039485007524490356
        vf_loss: 0.15001289546489716
      agent-2:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45758771896362305
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010273351799696684
        model: {}
        policy_loss: -0.003008454106748104
        total_loss: -0.0037691728211939335
        vf_explained_var: 0.006338045001029968
        vf_loss: 0.44635772705078125
      agent-3:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5911420583724976
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014270672108978033
        model: {}
        policy_loss: -0.002818155102431774
        total_loss: -0.0038184220902621746
        vf_explained_var: -0.0005688369274139404
        vf_loss: 0.40143054723739624
      agent-4:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6533578634262085
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015215773601084948
        model: {}
        policy_loss: -0.003813312854617834
        total_loss: -0.004922994412481785
        vf_explained_var: 0.01764354109764099
        vf_loss: 0.40228426456451416
      agent-5:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8575475811958313
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018682118970900774
        model: {}
        policy_loss: -0.003033277578651905
        total_loss: -0.004520118236541748
        vf_explained_var: 0.0005123615264892578
        vf_loss: 0.2244272232055664
    load_time_ms: 13527.766
    num_steps_sampled: 23136000
    num_steps_trained: 23136000
    sample_time_ms: 97388.144
    update_time_ms: 15.606
  iterations_since_restore: 81
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.231818181818184
    ram_util_percent: 11.490909090909092
  pid: 13408
  policy_reward_max:
    agent-0: 12.0
    agent-1: 10.0
    agent-2: 15.0
    agent-3: 13.0
    agent-4: 15.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 3.41
    agent-1: 2.02
    agent-2: 5.27
    agent-3: 4.73
    agent-4: 4.77
    agent-5: 2.79
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.006505337852044
    mean_inference_ms: 12.81823700229701
    mean_processing_ms: 57.8790964830642
  time_since_restore: 10054.770558595657
  time_this_iter_s: 122.72217178344727
  time_total_s: 32565.053801059723
  timestamp: 1637230019
  timesteps_since_restore: 7776000
  timesteps_this_iter: 96000
  timesteps_total: 23136000
  training_iteration: 241
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    241 |          32565.1 | 23136000 |    22.99 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 2.48
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.84
    apples_agent-1_min: 0
    apples_agent-2_max: 10
    apples_agent-2_mean: 2.07
    apples_agent-2_min: 0
    apples_agent-3_max: 21
    apples_agent-3_mean: 3.01
    apples_agent-3_min: 0
    apples_agent-4_max: 106
    apples_agent-4_mean: 2.34
    apples_agent-4_min: 0
    apples_agent-5_max: 9
    apples_agent-5_mean: 1.66
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 174
    cleaning_beam_agent-0_mean: 80.79
    cleaning_beam_agent-0_min: 26
    cleaning_beam_agent-1_max: 364
    cleaning_beam_agent-1_mean: 232.28
    cleaning_beam_agent-1_min: 149
    cleaning_beam_agent-2_max: 67
    cleaning_beam_agent-2_mean: 15.69
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 217
    cleaning_beam_agent-3_mean: 93.97
    cleaning_beam_agent-3_min: 24
    cleaning_beam_agent-4_max: 128
    cleaning_beam_agent-4_mean: 36.8
    cleaning_beam_agent-4_min: 5
    cleaning_beam_agent-5_max: 27
    cleaning_beam_agent-5_mean: 6.15
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-09-02
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 51.0
  episode_reward_mean: 22.38
  episode_reward_min: -9.0
  episodes_this_iter: 96
  episodes_total: 23232
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11564.786
    learner:
      agent-0:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.581386148929596
        entropy_coeff: 0.0017600000137463212
        kl: 0.001642659306526184
        model: {}
        policy_loss: -0.0035665431059896946
        total_loss: -0.004560232628136873
        vf_explained_var: -0.0007431656122207642
        vf_loss: 0.29551470279693604
      agent-1:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4839221239089966
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012712484458461404
        model: {}
        policy_loss: -0.002784743905067444
        total_loss: -0.0036270844284445047
        vf_explained_var: 0.014766737818717957
        vf_loss: 0.09364678710699081
      agent-2:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46657663583755493
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014820401556789875
        model: {}
        policy_loss: -0.003645476419478655
        total_loss: -0.004414147697389126
        vf_explained_var: -0.0042082518339157104
        vf_loss: 0.5250224471092224
      agent-3:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.582155168056488
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014594047097489238
        model: {}
        policy_loss: -0.002903288695961237
        total_loss: -0.003893177956342697
        vf_explained_var: -0.0003571361303329468
        vf_loss: 0.34705284237861633
      agent-4:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6672543287277222
        entropy_coeff: 0.0017600000137463212
        kl: 0.001793815172277391
        model: {}
        policy_loss: -0.0027670245617628098
        total_loss: -0.0037750587798655033
        vf_explained_var: 0.007943853735923767
        vf_loss: 1.6632895469665527
      agent-5:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8453984260559082
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014377302723005414
        model: {}
        policy_loss: -0.0026486176066100597
        total_loss: -0.004117217380553484
        vf_explained_var: 0.017887011170387268
        vf_loss: 0.19303962588310242
    load_time_ms: 13510.801
    num_steps_sampled: 23232000
    num_steps_trained: 23232000
    sample_time_ms: 97347.877
    update_time_ms: 15.621
  iterations_since_restore: 82
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.222857142857144
    ram_util_percent: 11.482285714285712
  pid: 13408
  policy_reward_max:
    agent-0: 13.0
    agent-1: 7.0
    agent-2: 18.0
    agent-3: 12.0
    agent-4: 15.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 3.59
    agent-1: 1.62
    agent-2: 5.15
    agent-3: 4.3
    agent-4: 5.03
    agent-5: 2.69
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -35.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.9993930834637
    mean_inference_ms: 12.816222161141477
    mean_processing_ms: 57.87366975130738
  time_since_restore: 10177.513075828552
  time_this_iter_s: 122.7425172328949
  time_total_s: 32687.796318292618
  timestamp: 1637230142
  timesteps_since_restore: 7872000
  timesteps_this_iter: 96000
  timesteps_total: 23232000
  training_iteration: 242
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    242 |          32687.8 | 23232000 |    22.38 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 2.34
    apples_agent-0_min: 0
    apples_agent-1_max: 9
    apples_agent-1_mean: 0.98
    apples_agent-1_min: 0
    apples_agent-2_max: 16
    apples_agent-2_mean: 1.78
    apples_agent-2_min: 0
    apples_agent-3_max: 28
    apples_agent-3_mean: 3.17
    apples_agent-3_min: 0
    apples_agent-4_max: 8
    apples_agent-4_mean: 1.22
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 1.51
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 164
    cleaning_beam_agent-0_mean: 80.78
    cleaning_beam_agent-0_min: 32
    cleaning_beam_agent-1_max: 409
    cleaning_beam_agent-1_mean: 227.93
    cleaning_beam_agent-1_min: 128
    cleaning_beam_agent-2_max: 48
    cleaning_beam_agent-2_mean: 12.49
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 183
    cleaning_beam_agent-3_mean: 86.27
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 140
    cleaning_beam_agent-4_mean: 38.74
    cleaning_beam_agent-4_min: 5
    cleaning_beam_agent-5_max: 26
    cleaning_beam_agent-5_mean: 5.82
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-11-05
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 58.0
  episode_reward_mean: 20.15
  episode_reward_min: -71.0
  episodes_this_iter: 96
  episodes_total: 23328
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11574.79
    learner:
      agent-0:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5662986040115356
        entropy_coeff: 0.0017600000137463212
        kl: 0.001123993657529354
        model: {}
        policy_loss: -0.0022942963987588882
        total_loss: -0.003133391262963414
        vf_explained_var: -0.001589059829711914
        vf_loss: 1.575906753540039
      agent-1:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49047935009002686
        entropy_coeff: 0.0017600000137463212
        kl: 0.001062840106897056
        model: {}
        policy_loss: -0.002677383366972208
        total_loss: -0.003530144691467285
        vf_explained_var: 0.016949087381362915
        vf_loss: 0.10481423139572144
      agent-2:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4493187367916107
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016626366414129734
        model: {}
        policy_loss: -0.0035200330894440413
        total_loss: -0.004245571792125702
        vf_explained_var: 0.0013829618692398071
        vf_loss: 0.6526129245758057
      agent-3:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5806702375411987
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010953613091260195
        model: {}
        policy_loss: -0.0021188263781368732
        total_loss: -0.002982174511998892
        vf_explained_var: 7.876753807067871e-05
        vf_loss: 1.5863142013549805
      agent-4:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6815810799598694
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018431225325912237
        model: {}
        policy_loss: -0.004074574913829565
        total_loss: -0.005229376256465912
        vf_explained_var: 0.02414192259311676
        vf_loss: 0.4478223919868469
      agent-5:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8714662194252014
        entropy_coeff: 0.0017600000137463212
        kl: 0.001467682421207428
        model: {}
        policy_loss: -0.0027379710227251053
        total_loss: -0.004252138547599316
        vf_explained_var: 0.00985044240951538
        vf_loss: 0.19615548849105835
    load_time_ms: 13495.779
    num_steps_sampled: 23328000
    num_steps_trained: 23328000
    sample_time_ms: 97318.416
    update_time_ms: 15.604
  iterations_since_restore: 83
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.439655172413794
    ram_util_percent: 11.46494252873563
  pid: 13408
  policy_reward_max:
    agent-0: 13.0
    agent-1: 6.0
    agent-2: 26.0
    agent-3: 16.0
    agent-4: 17.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 2.65
    agent-1: 1.53
    agent-2: 5.04
    agent-3: 3.39
    agent-4: 4.97
    agent-5: 2.57
  policy_reward_min:
    agent-0: -48.0
    agent-1: -1.0
    agent-2: 0.0
    agent-3: -39.0
    agent-4: 0.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 22.995056944417062
    mean_inference_ms: 12.814116238494453
    mean_processing_ms: 57.871426012211685
  time_since_restore: 10300.326974630356
  time_this_iter_s: 122.81389880180359
  time_total_s: 32810.61021709442
  timestamp: 1637230265
  timesteps_since_restore: 7968000
  timesteps_this_iter: 96000
  timesteps_total: 23328000
  training_iteration: 243
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    243 |          32810.6 | 23328000 |    20.15 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 2.36
    apples_agent-0_min: 0
    apples_agent-1_max: 18
    apples_agent-1_mean: 1.02
    apples_agent-1_min: 0
    apples_agent-2_max: 31
    apples_agent-2_mean: 2.48
    apples_agent-2_min: 0
    apples_agent-3_max: 12
    apples_agent-3_mean: 3.26
    apples_agent-3_min: 0
    apples_agent-4_max: 15
    apples_agent-4_mean: 1.58
    apples_agent-4_min: 0
    apples_agent-5_max: 30
    apples_agent-5_mean: 1.42
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 164
    cleaning_beam_agent-0_mean: 78.72
    cleaning_beam_agent-0_min: 17
    cleaning_beam_agent-1_max: 333
    cleaning_beam_agent-1_mean: 223.07
    cleaning_beam_agent-1_min: 130
    cleaning_beam_agent-2_max: 110
    cleaning_beam_agent-2_mean: 15.78
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 182
    cleaning_beam_agent-3_mean: 86.5
    cleaning_beam_agent-3_min: 20
    cleaning_beam_agent-4_max: 250
    cleaning_beam_agent-4_mean: 40.21
    cleaning_beam_agent-4_min: 7
    cleaning_beam_agent-5_max: 43
    cleaning_beam_agent-5_mean: 5.52
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-13-07
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 59.0
  episode_reward_mean: 22.2
  episode_reward_min: 3.0
  episodes_this_iter: 96
  episodes_total: 23424
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11560.073
    learner:
      agent-0:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5551177859306335
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010966530535370111
        model: {}
        policy_loss: -0.003416324034333229
        total_loss: -0.004367372952401638
        vf_explained_var: -0.005593299865722656
        vf_loss: 0.25958582758903503
      agent-1:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4864521622657776
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011004917323589325
        model: {}
        policy_loss: -0.002395613119006157
        total_loss: -0.0032390435226261616
        vf_explained_var: 0.00960126519203186
        vf_loss: 0.12725727260112762
      agent-2:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4605041742324829
        entropy_coeff: 0.0017600000137463212
        kl: 0.001388199278153479
        model: {}
        policy_loss: -0.003393131075426936
        total_loss: -0.0041549354791641235
        vf_explained_var: -0.0019679516553878784
        vf_loss: 0.4868078827857971
      agent-3:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5716041922569275
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009222362423315644
        model: {}
        policy_loss: -0.0023377612233161926
        total_loss: -0.0033012398052960634
        vf_explained_var: 0.0029546916484832764
        vf_loss: 0.4254310727119446
      agent-4:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6692527532577515
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016521223587915301
        model: {}
        policy_loss: -0.0038926892448216677
        total_loss: -0.005029956344515085
        vf_explained_var: 0.010757148265838623
        vf_loss: 0.406188040971756
      agent-5:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8585302233695984
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019543017260730267
        model: {}
        policy_loss: -0.0030563613399863243
        total_loss: -0.0045493836514651775
        vf_explained_var: 0.0058574676513671875
        vf_loss: 0.1798938512802124
    load_time_ms: 13503.782
    num_steps_sampled: 23424000
    num_steps_trained: 23424000
    sample_time_ms: 97215.587
    update_time_ms: 15.603
  iterations_since_restore: 84
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.30344827586207
    ram_util_percent: 11.512643678160916
  pid: 13408
  policy_reward_max:
    agent-0: 13.0
    agent-1: 11.0
    agent-2: 15.0
    agent-3: 14.0
    agent-4: 17.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 3.45
    agent-1: 1.6
    agent-2: 5.24
    agent-3: 4.44
    agent-4: 4.83
    agent-5: 2.64
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.988783051865948
    mean_inference_ms: 12.81254325788467
    mean_processing_ms: 57.86495500276665
  time_since_restore: 10422.303902387619
  time_this_iter_s: 121.97692775726318
  time_total_s: 32932.587144851685
  timestamp: 1637230387
  timesteps_since_restore: 8064000
  timesteps_this_iter: 96000
  timesteps_total: 23424000
  training_iteration: 244
  trial_id: '00000'
  
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
== Status ==
Memory usage on this node: 20.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    244 |          32932.6 | 23424000 |     22.2 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 2.3
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.95
    apples_agent-1_min: 0
    apples_agent-2_max: 59
    apples_agent-2_mean: 2.47
    apples_agent-2_min: 0
    apples_agent-3_max: 11
    apples_agent-3_mean: 2.41
    apples_agent-3_min: 0
    apples_agent-4_max: 6
    apples_agent-4_mean: 1.17
    apples_agent-4_min: 0
    apples_agent-5_max: 12
    apples_agent-5_mean: 1.53
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 154
    cleaning_beam_agent-0_mean: 79.54
    cleaning_beam_agent-0_min: 22
    cleaning_beam_agent-1_max: 424
    cleaning_beam_agent-1_mean: 225.29
    cleaning_beam_agent-1_min: 117
    cleaning_beam_agent-2_max: 80
    cleaning_beam_agent-2_mean: 17.12
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 244
    cleaning_beam_agent-3_mean: 97.54
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 138
    cleaning_beam_agent-4_mean: 33.18
    cleaning_beam_agent-4_min: 7
    cleaning_beam_agent-5_max: 37
    cleaning_beam_agent-5_mean: 6.25
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-15-10
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 51.0
  episode_reward_mean: 20.06
  episode_reward_min: -95.0
  episodes_this_iter: 96
  episodes_total: 23520
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11548.031
    learner:
      agent-0:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5632812976837158
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021369229070842266
        model: {}
        policy_loss: -0.0033197831362485886
        total_loss: -0.004274689592421055
        vf_explained_var: -0.01026242971420288
        vf_loss: 0.3647039532661438
      agent-1:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5018900632858276
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010579923400655389
        model: {}
        policy_loss: -0.002494903514161706
        total_loss: -0.003367567900568247
        vf_explained_var: 0.021880075335502625
        vf_loss: 0.10665064305067062
      agent-2:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46863892674446106
        entropy_coeff: 0.0017600000137463212
        kl: 0.001278602983802557
        model: {}
        policy_loss: -0.003115226747468114
        total_loss: -0.0038952045142650604
        vf_explained_var: 0.007621005177497864
        vf_loss: 0.44822949171066284
      agent-3:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.57515949010849
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013659254182130098
        model: {}
        policy_loss: -0.0029416149482131004
        total_loss: -0.003925497643649578
        vf_explained_var: 0.0011908113956451416
        vf_loss: 0.2839924693107605
      agent-4:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6724446415901184
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020886538550257683
        model: {}
        policy_loss: -0.0028838315047323704
        total_loss: -0.003885839134454727
        vf_explained_var: 0.006182163953781128
        vf_loss: 1.8149596452713013
      agent-5:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8639581799507141
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016528279520571232
        model: {}
        policy_loss: -0.0020935162901878357
        total_loss: -0.003461870364844799
        vf_explained_var: 0.002244889736175537
        vf_loss: 1.522114872932434
    load_time_ms: 13509.388
    num_steps_sampled: 23520000
    num_steps_trained: 23520000
    sample_time_ms: 97243.056
    update_time_ms: 15.627
  iterations_since_restore: 85
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.734285714285715
    ram_util_percent: 11.437714285714287
  pid: 13408
  policy_reward_max:
    agent-0: 20.0
    agent-1: 7.0
    agent-2: 17.0
    agent-3: 13.0
    agent-4: 12.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 3.7
    agent-1: 1.61
    agent-2: 4.81
    agent-3: 3.46
    agent-4: 4.42
    agent-5: 2.06
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -1.0
    agent-3: 0.0
    agent-4: -49.0
    agent-5: -50.0
  sampler_perf:
    mean_env_wait_ms: 22.983649659302188
    mean_inference_ms: 12.810669602549545
    mean_processing_ms: 57.86197526345848
  time_since_restore: 10545.153605222702
  time_this_iter_s: 122.84970283508301
  time_total_s: 33055.43684768677
  timestamp: 1637230510
  timesteps_since_restore: 8160000
  timesteps_this_iter: 96000
  timesteps_total: 23520000
  training_iteration: 245
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    245 |          33055.4 | 23520000 |    20.06 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 2.65
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.68
    apples_agent-1_min: 0
    apples_agent-2_max: 16
    apples_agent-2_mean: 2.05
    apples_agent-2_min: 0
    apples_agent-3_max: 13
    apples_agent-3_mean: 2.53
    apples_agent-3_min: 0
    apples_agent-4_max: 23
    apples_agent-4_mean: 2.13
    apples_agent-4_min: 0
    apples_agent-5_max: 23
    apples_agent-5_mean: 1.67
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 159
    cleaning_beam_agent-0_mean: 80.01
    cleaning_beam_agent-0_min: 20
    cleaning_beam_agent-1_max: 427
    cleaning_beam_agent-1_mean: 231.89
    cleaning_beam_agent-1_min: 117
    cleaning_beam_agent-2_max: 58
    cleaning_beam_agent-2_mean: 11.95
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 265
    cleaning_beam_agent-3_mean: 93.45
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 143
    cleaning_beam_agent-4_mean: 38.88
    cleaning_beam_agent-4_min: 5
    cleaning_beam_agent-5_max: 44
    cleaning_beam_agent-5_mean: 6.62
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-17-13
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 47.0
  episode_reward_mean: 20.97
  episode_reward_min: -32.0
  episodes_this_iter: 96
  episodes_total: 23616
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11551.836
    learner:
      agent-0:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5616035461425781
        entropy_coeff: 0.0017600000137463212
        kl: 0.001344673102721572
        model: {}
        policy_loss: -0.0033484259620308876
        total_loss: -0.004309001844376326
        vf_explained_var: -0.008073478937149048
        vf_loss: 0.27848541736602783
      agent-1:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48647695779800415
        entropy_coeff: 0.0017600000137463212
        kl: 0.000967607309576124
        model: {}
        policy_loss: -0.0025718684773892164
        total_loss: -0.003421078436076641
        vf_explained_var: 0.014674380421638489
        vf_loss: 0.06989455968141556
      agent-2:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4524065852165222
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013670568587258458
        model: {}
        policy_loss: -0.0034639444202184677
        total_loss: -0.004201794043183327
        vf_explained_var: -0.0022779107093811035
        vf_loss: 0.5838515758514404
      agent-3:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5894553065299988
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010110902367159724
        model: {}
        policy_loss: -0.002881621941924095
        total_loss: -0.00388403981924057
        vf_explained_var: 0.004509925842285156
        vf_loss: 0.3501947224140167
      agent-4:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6681268811225891
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018230682471767068
        model: {}
        policy_loss: -0.004242424853146076
        total_loss: -0.0053816125728189945
        vf_explained_var: 0.00936707854270935
        vf_loss: 0.36715269088745117
      agent-5:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8336877822875977
        entropy_coeff: 0.0017600000137463212
        kl: 0.00161209877114743
        model: {}
        policy_loss: -0.0029191612266004086
        total_loss: -0.004368207883089781
        vf_explained_var: 0.020589783787727356
        vf_loss: 0.18240287899971008
    load_time_ms: 13515.722
    num_steps_sampled: 23616000
    num_steps_trained: 23616000
    sample_time_ms: 97312.294
    update_time_ms: 15.944
  iterations_since_restore: 86
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.60862068965517
    ram_util_percent: 11.54425287356322
  pid: 13408
  policy_reward_max:
    agent-0: 9.0
    agent-1: 8.0
    agent-2: 17.0
    agent-3: 12.0
    agent-4: 11.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 3.64
    agent-1: 1.22
    agent-2: 4.69
    agent-3: 4.14
    agent-4: 4.88
    agent-5: 2.4
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -49.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 22.977044537650418
    mean_inference_ms: 12.808493672440216
    mean_processing_ms: 57.85695604948942
  time_since_restore: 10667.697947978973
  time_this_iter_s: 122.54434275627136
  time_total_s: 33177.98119044304
  timestamp: 1637230633
  timesteps_since_restore: 8256000
  timesteps_this_iter: 96000
  timesteps_total: 23616000
  training_iteration: 246
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    246 |            33178 | 23616000 |    20.97 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 2.68
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.72
    apples_agent-1_min: 0
    apples_agent-2_max: 9
    apples_agent-2_mean: 1.69
    apples_agent-2_min: 0
    apples_agent-3_max: 32
    apples_agent-3_mean: 3.07
    apples_agent-3_min: 0
    apples_agent-4_max: 44
    apples_agent-4_mean: 1.84
    apples_agent-4_min: 0
    apples_agent-5_max: 27
    apples_agent-5_mean: 1.67
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 182
    cleaning_beam_agent-0_mean: 77.57
    cleaning_beam_agent-0_min: 25
    cleaning_beam_agent-1_max: 472
    cleaning_beam_agent-1_mean: 224.47
    cleaning_beam_agent-1_min: 95
    cleaning_beam_agent-2_max: 99
    cleaning_beam_agent-2_mean: 15.95
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 177
    cleaning_beam_agent-3_mean: 92.29
    cleaning_beam_agent-3_min: 23
    cleaning_beam_agent-4_max: 159
    cleaning_beam_agent-4_mean: 40.35
    cleaning_beam_agent-4_min: 3
    cleaning_beam_agent-5_max: 41
    cleaning_beam_agent-5_mean: 6.49
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-19-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 62.0
  episode_reward_mean: 22.89
  episode_reward_min: 1.0
  episodes_this_iter: 96
  episodes_total: 23712
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11550.772
    learner:
      agent-0:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5642953515052795
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014311386039480567
        model: {}
        policy_loss: -0.0034577548503875732
        total_loss: -0.004418986849486828
        vf_explained_var: 0.0042712390422821045
        vf_loss: 0.3192809224128723
      agent-1:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.502142071723938
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009559574536979198
        model: {}
        policy_loss: -0.0026088058948516846
        total_loss: -0.003480421844869852
        vf_explained_var: 0.028138339519500732
        vf_loss: 0.12151508778333664
      agent-2:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4577670693397522
        entropy_coeff: 0.0017600000137463212
        kl: 0.00163441919721663
        model: {}
        policy_loss: -0.0037318102549761534
        total_loss: -0.0044853417202830315
        vf_explained_var: -0.003249838948249817
        vf_loss: 0.5213825702667236
      agent-3:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5863279700279236
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015688195126131177
        model: {}
        policy_loss: -0.0028578778728842735
        total_loss: -0.0038531427271664143
        vf_explained_var: -0.0010288506746292114
        vf_loss: 0.3667173683643341
      agent-4:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.65544593334198
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014607314951717854
        model: {}
        policy_loss: -0.003914286848157644
        total_loss: -0.005014020949602127
        vf_explained_var: 0.003338530659675598
        vf_loss: 0.5385285019874573
      agent-5:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8539360761642456
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014843957033008337
        model: {}
        policy_loss: -0.002665235660970211
        total_loss: -0.004143221769481897
        vf_explained_var: 0.007971271872520447
        vf_loss: 0.2494216114282608
    load_time_ms: 13518.098
    num_steps_sampled: 23712000
    num_steps_trained: 23712000
    sample_time_ms: 97405.323
    update_time_ms: 15.971
  iterations_since_restore: 87
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.529714285714284
    ram_util_percent: 11.64685714285714
  pid: 13408
  policy_reward_max:
    agent-0: 16.0
    agent-1: 9.0
    agent-2: 19.0
    agent-3: 14.0
    agent-4: 16.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 3.72
    agent-1: 1.62
    agent-2: 4.84
    agent-3: 4.16
    agent-4: 5.65
    agent-5: 2.9
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.972131897306006
    mean_inference_ms: 12.806142652331046
    mean_processing_ms: 57.85274924214884
  time_since_restore: 10790.557957172394
  time_this_iter_s: 122.86000919342041
  time_total_s: 33300.84119963646
  timestamp: 1637230756
  timesteps_since_restore: 8352000
  timesteps_this_iter: 96000
  timesteps_total: 23712000
  training_iteration: 247
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    247 |          33300.8 | 23712000 |    22.89 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 7
    apples_agent-0_mean: 2.62
    apples_agent-0_min: 0
    apples_agent-1_max: 11
    apples_agent-1_mean: 1.15
    apples_agent-1_min: 0
    apples_agent-2_max: 13
    apples_agent-2_mean: 2.11
    apples_agent-2_min: 0
    apples_agent-3_max: 17
    apples_agent-3_mean: 3.36
    apples_agent-3_min: 0
    apples_agent-4_max: 39
    apples_agent-4_mean: 1.26
    apples_agent-4_min: 0
    apples_agent-5_max: 9
    apples_agent-5_mean: 1.47
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 145
    cleaning_beam_agent-0_mean: 82.31
    cleaning_beam_agent-0_min: 34
    cleaning_beam_agent-1_max: 343
    cleaning_beam_agent-1_mean: 232.69
    cleaning_beam_agent-1_min: 90
    cleaning_beam_agent-2_max: 83
    cleaning_beam_agent-2_mean: 14.74
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 225
    cleaning_beam_agent-3_mean: 96.45
    cleaning_beam_agent-3_min: 34
    cleaning_beam_agent-4_max: 149
    cleaning_beam_agent-4_mean: 40.33
    cleaning_beam_agent-4_min: 6
    cleaning_beam_agent-5_max: 28
    cleaning_beam_agent-5_mean: 6.95
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-21-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 52.0
  episode_reward_mean: 20.64
  episode_reward_min: -78.0
  episodes_this_iter: 96
  episodes_total: 23808
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11543.168
    learner:
      agent-0:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.561813235282898
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014138113474473357
        model: {}
        policy_loss: -0.0034784283488988876
        total_loss: -0.004435357172042131
        vf_explained_var: -0.000497773289680481
        vf_loss: 0.3186241090297699
      agent-1:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.485146164894104
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010698054684326053
        model: {}
        policy_loss: -0.001460137777030468
        total_loss: -0.0022018104791641235
        vf_explained_var: 0.010984137654304504
        vf_loss: 1.1218619346618652
      agent-2:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4482152462005615
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015716630732640624
        model: {}
        policy_loss: -0.003454554360359907
        total_loss: -0.004198351874947548
        vf_explained_var: -0.0029195845127105713
        vf_loss: 0.4505898058414459
      agent-3:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6016879677772522
        entropy_coeff: 0.0017600000137463212
        kl: 0.001089826924726367
        model: {}
        policy_loss: -0.0018756706267595291
        total_loss: -0.0027631614357233047
        vf_explained_var: -0.00038939714431762695
        vf_loss: 1.714789867401123
      agent-4:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6692297458648682
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016687553143128753
        model: {}
        policy_loss: -0.003767275484278798
        total_loss: -0.004899688996374607
        vf_explained_var: 0.02221199870109558
        vf_loss: 0.454314261674881
      agent-5:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8556785583496094
        entropy_coeff: 0.0017600000137463212
        kl: 0.0027416907250881195
        model: {}
        policy_loss: -0.002824104856699705
        total_loss: -0.0041793170385062695
        vf_explained_var: 0.0029275119304656982
        vf_loss: 1.5077897310256958
    load_time_ms: 13508.306
    num_steps_sampled: 23808000
    num_steps_trained: 23808000
    sample_time_ms: 97460.491
    update_time_ms: 15.844
  iterations_since_restore: 88
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.026857142857146
    ram_util_percent: 11.481142857142856
  pid: 13408
  policy_reward_max:
    agent-0: 12.0
    agent-1: 7.0
    agent-2: 12.0
    agent-3: 12.0
    agent-4: 16.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 3.81
    agent-1: 1.39
    agent-2: 4.45
    agent-3: 3.31
    agent-4: 5.49
    agent-5: 2.19
  policy_reward_min:
    agent-0: 0.0
    agent-1: -46.0
    agent-2: -45.0
    agent-3: -45.0
    agent-4: 0.0
    agent-5: -46.0
  sampler_perf:
    mean_env_wait_ms: 22.968132516149726
    mean_inference_ms: 12.803978417163203
    mean_processing_ms: 57.84673109783544
  time_since_restore: 10913.178359985352
  time_this_iter_s: 122.62040281295776
  time_total_s: 33423.46160244942
  timestamp: 1637230879
  timesteps_since_restore: 8448000
  timesteps_this_iter: 96000
  timesteps_total: 23808000
  training_iteration: 248
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    248 |          33423.5 | 23808000 |    20.64 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 2.51
    apples_agent-0_min: 0
    apples_agent-1_max: 35
    apples_agent-1_mean: 1.31
    apples_agent-1_min: 0
    apples_agent-2_max: 7
    apples_agent-2_mean: 1.96
    apples_agent-2_min: 0
    apples_agent-3_max: 10
    apples_agent-3_mean: 2.96
    apples_agent-3_min: 0
    apples_agent-4_max: 12
    apples_agent-4_mean: 1.15
    apples_agent-4_min: 0
    apples_agent-5_max: 16
    apples_agent-5_mean: 2.08
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 181
    cleaning_beam_agent-0_mean: 85.01
    cleaning_beam_agent-0_min: 32
    cleaning_beam_agent-1_max: 371
    cleaning_beam_agent-1_mean: 229.26
    cleaning_beam_agent-1_min: 128
    cleaning_beam_agent-2_max: 91
    cleaning_beam_agent-2_mean: 16.04
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 215
    cleaning_beam_agent-3_mean: 93.03
    cleaning_beam_agent-3_min: 28
    cleaning_beam_agent-4_max: 77
    cleaning_beam_agent-4_mean: 35.1
    cleaning_beam_agent-4_min: 5
    cleaning_beam_agent-5_max: 53
    cleaning_beam_agent-5_mean: 6.93
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-23-21
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 57.0
  episode_reward_mean: 23.39
  episode_reward_min: 3.0
  episodes_this_iter: 96
  episodes_total: 23904
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11535.889
    learner:
      agent-0:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5598999261856079
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014156486140564084
        model: {}
        policy_loss: -0.003307912964373827
        total_loss: -0.004264614079147577
        vf_explained_var: -0.004507318139076233
        vf_loss: 0.2872445583343506
      agent-1:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4976256489753723
        entropy_coeff: 0.0017600000137463212
        kl: 0.001377635169774294
        model: {}
        policy_loss: -0.0028344825841486454
        total_loss: -0.003697753883898258
        vf_explained_var: 0.027209654450416565
        vf_loss: 0.12546150386333466
      agent-2:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4672977328300476
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015459066489711404
        model: {}
        policy_loss: -0.0035220449790358543
        total_loss: -0.004296610131859779
        vf_explained_var: 0.0011147111654281616
        vf_loss: 0.47876474261283875
      agent-3:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6065625548362732
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014641426969319582
        model: {}
        policy_loss: -0.003012394532561302
        total_loss: -0.004043372813612223
        vf_explained_var: -0.0041522979736328125
        vf_loss: 0.36575859785079956
      agent-4:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6633402109146118
        entropy_coeff: 0.0017600000137463212
        kl: 0.001676021027378738
        model: {}
        policy_loss: -0.003918193280696869
        total_loss: -0.005039621144533157
        vf_explained_var: 0.00908246636390686
        vf_loss: 0.4605124592781067
      agent-5:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8726866841316223
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018530713859945536
        model: {}
        policy_loss: -0.0031979824416339397
        total_loss: -0.004707802552729845
        vf_explained_var: 0.009193062782287598
        vf_loss: 0.26109930872917175
    load_time_ms: 13513.91
    num_steps_sampled: 23904000
    num_steps_trained: 23904000
    sample_time_ms: 97423.294
    update_time_ms: 15.996
  iterations_since_restore: 89
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.41907514450867
    ram_util_percent: 11.503468208092483
  pid: 13408
  policy_reward_max:
    agent-0: 10.0
    agent-1: 9.0
    agent-2: 15.0
    agent-3: 13.0
    agent-4: 16.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 3.69
    agent-1: 1.89
    agent-2: 5.13
    agent-3: 4.43
    agent-4: 5.23
    agent-5: 3.02
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -3.0
  sampler_perf:
    mean_env_wait_ms: 22.962939300458746
    mean_inference_ms: 12.802277142937166
    mean_processing_ms: 57.84200919445884
  time_since_restore: 11034.907050132751
  time_this_iter_s: 121.7286901473999
  time_total_s: 33545.19029259682
  timestamp: 1637231001
  timesteps_since_restore: 8544000
  timesteps_this_iter: 96000
  timesteps_total: 23904000
  training_iteration: 249
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    249 |          33545.2 | 23904000 |    23.39 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 8
    apples_agent-0_mean: 2.21
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.83
    apples_agent-1_min: 0
    apples_agent-2_max: 22
    apples_agent-2_mean: 2.29
    apples_agent-2_min: 0
    apples_agent-3_max: 15
    apples_agent-3_mean: 3.14
    apples_agent-3_min: 0
    apples_agent-4_max: 14
    apples_agent-4_mean: 1.63
    apples_agent-4_min: 0
    apples_agent-5_max: 6
    apples_agent-5_mean: 1.54
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 173
    cleaning_beam_agent-0_mean: 85.95
    cleaning_beam_agent-0_min: 26
    cleaning_beam_agent-1_max: 362
    cleaning_beam_agent-1_mean: 233.86
    cleaning_beam_agent-1_min: 143
    cleaning_beam_agent-2_max: 80
    cleaning_beam_agent-2_mean: 15.12
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 238
    cleaning_beam_agent-3_mean: 106.54
    cleaning_beam_agent-3_min: 34
    cleaning_beam_agent-4_max: 137
    cleaning_beam_agent-4_mean: 35.74
    cleaning_beam_agent-4_min: 6
    cleaning_beam_agent-5_max: 24
    cleaning_beam_agent-5_mean: 6.58
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-25-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 55.0
  episode_reward_mean: 22.66
  episode_reward_min: 0.0
  episodes_this_iter: 96
  episodes_total: 24000
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11530.532
    learner:
      agent-0:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5594301819801331
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016217413358390331
        model: {}
        policy_loss: -0.0036488259211182594
        total_loss: -0.004609981086105108
        vf_explained_var: 0.0014220327138900757
        vf_loss: 0.23441050946712494
      agent-1:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5067704319953918
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007623652927577496
        model: {}
        policy_loss: -0.0023690899834036827
        total_loss: -0.0032475152984261513
        vf_explained_var: 0.025821179151535034
        vf_loss: 0.13490556180477142
      agent-2:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47776395082473755
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011541680432856083
        model: {}
        policy_loss: -0.0034347802866250277
        total_loss: -0.004235522355884314
        vf_explained_var: 0.005340367555618286
        vf_loss: 0.40118899941444397
      agent-3:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6049896478652954
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010395257268100977
        model: {}
        policy_loss: -0.0030999674927443266
        total_loss: -0.004125467035919428
        vf_explained_var: -0.001302868127822876
        vf_loss: 0.39282312989234924
      agent-4:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6621367931365967
        entropy_coeff: 0.0017600000137463212
        kl: 0.001997681800276041
        model: {}
        policy_loss: -0.0044135721400380135
        total_loss: -0.005538043566048145
        vf_explained_var: 0.00862114131450653
        vf_loss: 0.40886780619621277
      agent-5:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8779124021530151
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015146698569878936
        model: {}
        policy_loss: -0.0027509676292538643
        total_loss: -0.00427306117489934
        vf_explained_var: 0.014124885201454163
        vf_loss: 0.23033744096755981
    load_time_ms: 13527.872
    num_steps_sampled: 24000000
    num_steps_trained: 24000000
    sample_time_ms: 97444.34
    update_time_ms: 16.077
  iterations_since_restore: 90
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.207386363636363
    ram_util_percent: 11.488636363636363
  pid: 13408
  policy_reward_max:
    agent-0: 10.0
    agent-1: 7.0
    agent-2: 15.0
    agent-3: 14.0
    agent-4: 17.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 3.39
    agent-1: 1.64
    agent-2: 4.69
    agent-3: 4.68
    agent-4: 5.24
    agent-5: 3.02
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.959634587814268
    mean_inference_ms: 12.800611350151403
    mean_processing_ms: 57.83927406562766
  time_since_restore: 11158.15236210823
  time_this_iter_s: 123.24531197547913
  time_total_s: 33668.435604572296
  timestamp: 1637231124
  timesteps_since_restore: 8640000
  timesteps_this_iter: 96000
  timesteps_total: 24000000
  training_iteration: 250
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    250 |          33668.4 | 24000000 |    22.66 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 2.31
    apples_agent-0_min: 0
    apples_agent-1_max: 21
    apples_agent-1_mean: 1.4
    apples_agent-1_min: 0
    apples_agent-2_max: 10
    apples_agent-2_mean: 1.86
    apples_agent-2_min: 0
    apples_agent-3_max: 14
    apples_agent-3_mean: 3.1
    apples_agent-3_min: 0
    apples_agent-4_max: 21
    apples_agent-4_mean: 1.85
    apples_agent-4_min: 0
    apples_agent-5_max: 11
    apples_agent-5_mean: 1.63
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 211
    cleaning_beam_agent-0_mean: 83.86
    cleaning_beam_agent-0_min: 34
    cleaning_beam_agent-1_max: 325
    cleaning_beam_agent-1_mean: 229.61
    cleaning_beam_agent-1_min: 136
    cleaning_beam_agent-2_max: 48
    cleaning_beam_agent-2_mean: 13.34
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 234
    cleaning_beam_agent-3_mean: 106.18
    cleaning_beam_agent-3_min: 43
    cleaning_beam_agent-4_max: 139
    cleaning_beam_agent-4_mean: 38.62
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 27
    cleaning_beam_agent-5_mean: 6.12
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-27-27
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 53.0
  episode_reward_mean: 23.3
  episode_reward_min: 5.0
  episodes_this_iter: 96
  episodes_total: 24096
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11532.86
    learner:
      agent-0:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5498819351196289
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017249011434614658
        model: {}
        policy_loss: -0.0033461940474808216
        total_loss: -0.004286711569875479
        vf_explained_var: -0.004473894834518433
        vf_loss: 0.2727312743663788
      agent-1:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48346251249313354
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014646737836301327
        model: {}
        policy_loss: -0.002689348068088293
        total_loss: -0.003526482731103897
        vf_explained_var: 0.030041664838790894
        vf_loss: 0.1375797688961029
      agent-2:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4610842168331146
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012744518462568521
        model: {}
        policy_loss: -0.003546237014234066
        total_loss: -0.004314315505325794
        vf_explained_var: 0.0032070279121398926
        vf_loss: 0.43430987000465393
      agent-3:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5775095224380493
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014437055215239525
        model: {}
        policy_loss: -0.0030861215200275183
        total_loss: -0.004061645828187466
        vf_explained_var: -0.0019363462924957275
        vf_loss: 0.40893125534057617
      agent-4:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6532038450241089
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016139961080625653
        model: {}
        policy_loss: -0.003817805554717779
        total_loss: -0.00492533715441823
        vf_explained_var: 0.013425126671791077
        vf_loss: 0.4210738241672516
      agent-5:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.873243510723114
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021943601313978434
        model: {}
        policy_loss: -0.002883331850171089
        total_loss: -0.004395285155624151
        vf_explained_var: 0.015764489769935608
        vf_loss: 0.24951335787773132
    load_time_ms: 13513.243
    num_steps_sampled: 24096000
    num_steps_trained: 24096000
    sample_time_ms: 97361.07
    update_time_ms: 15.936
  iterations_since_restore: 91
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.39080459770115
    ram_util_percent: 11.507471264367814
  pid: 13408
  policy_reward_max:
    agent-0: 12.0
    agent-1: 8.0
    agent-2: 15.0
    agent-3: 13.0
    agent-4: 15.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 3.73
    agent-1: 1.92
    agent-2: 5.26
    agent-3: 4.54
    agent-4: 4.87
    agent-5: 2.98
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 1.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.9557512747711
    mean_inference_ms: 12.799217599468816
    mean_processing_ms: 57.83395412576816
  time_since_restore: 11279.91796875
  time_this_iter_s: 121.76560664176941
  time_total_s: 33790.201211214066
  timestamp: 1637231247
  timesteps_since_restore: 8736000
  timesteps_this_iter: 96000
  timesteps_total: 24096000
  training_iteration: 251
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    251 |          33790.2 | 24096000 |     23.3 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 2.86
    apples_agent-0_min: 0
    apples_agent-1_max: 13
    apples_agent-1_mean: 1.52
    apples_agent-1_min: 0
    apples_agent-2_max: 21
    apples_agent-2_mean: 2.45
    apples_agent-2_min: 0
    apples_agent-3_max: 26
    apples_agent-3_mean: 3.25
    apples_agent-3_min: 0
    apples_agent-4_max: 20
    apples_agent-4_mean: 1.61
    apples_agent-4_min: 0
    apples_agent-5_max: 9
    apples_agent-5_mean: 1.53
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 190
    cleaning_beam_agent-0_mean: 78.69
    cleaning_beam_agent-0_min: 22
    cleaning_beam_agent-1_max: 304
    cleaning_beam_agent-1_mean: 231.6
    cleaning_beam_agent-1_min: 115
    cleaning_beam_agent-2_max: 51
    cleaning_beam_agent-2_mean: 13.22
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 237
    cleaning_beam_agent-3_mean: 98.76
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 124
    cleaning_beam_agent-4_mean: 38.16
    cleaning_beam_agent-4_min: 8
    cleaning_beam_agent-5_max: 40
    cleaning_beam_agent-5_mean: 6.72
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-29-30
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 55.0
  episode_reward_mean: 22.77
  episode_reward_min: -43.0
  episodes_this_iter: 96
  episodes_total: 24192
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11521.107
    learner:
      agent-0:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5552388429641724
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012155574513599277
        model: {}
        policy_loss: -0.0034972880966961384
        total_loss: -0.004437962546944618
        vf_explained_var: -0.010255575180053711
        vf_loss: 0.36545416712760925
      agent-1:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4835249185562134
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010033868020400405
        model: {}
        policy_loss: -0.0024033943191170692
        total_loss: -0.003242447040975094
        vf_explained_var: 0.021688133478164673
        vf_loss: 0.11948550492525101
      agent-2:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4624016284942627
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015944935148581862
        model: {}
        policy_loss: -0.0037912935949862003
        total_loss: -0.004553294740617275
        vf_explained_var: -0.011066943407058716
        vf_loss: 0.5182360410690308
      agent-3:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5877573490142822
        entropy_coeff: 0.0017600000137463212
        kl: 0.001155685749836266
        model: {}
        policy_loss: -0.0028118514455854893
        total_loss: -0.0038137035444378853
        vf_explained_var: -0.009236752986907959
        vf_loss: 0.3260306715965271
      agent-4:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6599558591842651
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014856712659820914
        model: {}
        policy_loss: -0.004106783773750067
        total_loss: -0.005225947592407465
        vf_explained_var: 0.023479312658309937
        vf_loss: 0.42355990409851074
      agent-5:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8681631684303284
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018157877493649721
        model: {}
        policy_loss: -0.002933505456894636
        total_loss: -0.004438864067196846
        vf_explained_var: 0.008829370141029358
        vf_loss: 0.2260793000459671
    load_time_ms: 13533.627
    num_steps_sampled: 24192000
    num_steps_trained: 24192000
    sample_time_ms: 97411.376
    update_time_ms: 15.991
  iterations_since_restore: 92
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.25
    ram_util_percent: 11.436363636363636
  pid: 13408
  policy_reward_max:
    agent-0: 15.0
    agent-1: 9.0
    agent-2: 20.0
    agent-3: 11.0
    agent-4: 17.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 3.41
    agent-1: 1.9
    agent-2: 5.01
    agent-3: 4.1
    agent-4: 5.21
    agent-5: 3.14
  policy_reward_min:
    agent-0: -48.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.952412889062142
    mean_inference_ms: 12.797552765178565
    mean_processing_ms: 57.83320255487675
  time_since_restore: 11403.264477491379
  time_this_iter_s: 123.34650874137878
  time_total_s: 33913.547719955444
  timestamp: 1637231370
  timesteps_since_restore: 8832000
  timesteps_this_iter: 96000
  timesteps_total: 24192000
  training_iteration: 252
  trial_id: '00000'
  
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
== Status ==
Memory usage on this node: 21.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    252 |          33913.5 | 24192000 |    22.77 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 2.62
    apples_agent-0_min: 0
    apples_agent-1_max: 8
    apples_agent-1_mean: 1.05
    apples_agent-1_min: 0
    apples_agent-2_max: 29
    apples_agent-2_mean: 2.37
    apples_agent-2_min: 0
    apples_agent-3_max: 11
    apples_agent-3_mean: 2.99
    apples_agent-3_min: 0
    apples_agent-4_max: 18
    apples_agent-4_mean: 1.08
    apples_agent-4_min: 0
    apples_agent-5_max: 8
    apples_agent-5_mean: 1.33
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 158
    cleaning_beam_agent-0_mean: 80.57
    cleaning_beam_agent-0_min: 33
    cleaning_beam_agent-1_max: 384
    cleaning_beam_agent-1_mean: 231.65
    cleaning_beam_agent-1_min: 90
    cleaning_beam_agent-2_max: 65
    cleaning_beam_agent-2_mean: 11.66
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 204
    cleaning_beam_agent-3_mean: 94.72
    cleaning_beam_agent-3_min: 32
    cleaning_beam_agent-4_max: 138
    cleaning_beam_agent-4_mean: 35.87
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 28
    cleaning_beam_agent-5_mean: 6.09
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-31-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 56.0
  episode_reward_mean: 22.14
  episode_reward_min: 4.0
  episodes_this_iter: 96
  episodes_total: 24288
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11509.679
    learner:
      agent-0:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5472789406776428
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014014665503054857
        model: {}
        policy_loss: -0.0034015504643321037
        total_loss: -0.00433003343641758
        vf_explained_var: -0.0008065253496170044
        vf_loss: 0.34724658727645874
      agent-1:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47532159090042114
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008194833062589169
        model: {}
        policy_loss: -0.002852918580174446
        total_loss: -0.003677656874060631
        vf_explained_var: 0.026259779930114746
        vf_loss: 0.1182708814740181
      agent-2:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45242297649383545
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011610087240114808
        model: {}
        policy_loss: -0.003512646071612835
        total_loss: -0.004258815199136734
        vf_explained_var: -0.005683496594429016
        vf_loss: 0.5009703636169434
      agent-3:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5866825580596924
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015466682380065322
        model: {}
        policy_loss: -0.002803188981488347
        total_loss: -0.003802178194746375
        vf_explained_var: 0.001498594880104065
        vf_loss: 0.33571916818618774
      agent-4:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6715521812438965
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014843030367046595
        model: {}
        policy_loss: -0.003743473207578063
        total_loss: -0.004885077476501465
        vf_explained_var: 0.015127480030059814
        vf_loss: 0.40326398611068726
      agent-5:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8885292410850525
        entropy_coeff: 0.0017600000137463212
        kl: 0.002448135055601597
        model: {}
        policy_loss: -0.0029499174561351538
        total_loss: -0.004487755708396435
        vf_explained_var: 0.0006141215562820435
        vf_loss: 0.2597302496433258
    load_time_ms: 13539.516
    num_steps_sampled: 24288000
    num_steps_trained: 24288000
    sample_time_ms: 97488.34
    update_time_ms: 16.185
  iterations_since_restore: 93
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.089204545454546
    ram_util_percent: 11.622159090909092
  pid: 13408
  policy_reward_max:
    agent-0: 14.0
    agent-1: 7.0
    agent-2: 15.0
    agent-3: 14.0
    agent-4: 12.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 3.74
    agent-1: 1.8
    agent-2: 5.14
    agent-3: 4.15
    agent-4: 4.46
    agent-5: 2.85
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.94906892873084
    mean_inference_ms: 12.796773764960196
    mean_processing_ms: 57.83232436944871
  time_since_restore: 11526.792826890945
  time_this_iter_s: 123.52834939956665
  time_total_s: 34037.07606935501
  timestamp: 1637231494
  timesteps_since_restore: 8928000
  timesteps_this_iter: 96000
  timesteps_total: 24288000
  training_iteration: 253
  trial_id: '00000'
  
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
== Status ==
Memory usage on this node: 21.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    253 |          34037.1 | 24288000 |    22.14 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 62
    apples_agent-0_mean: 2.64
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.85
    apples_agent-1_min: 0
    apples_agent-2_max: 16
    apples_agent-2_mean: 1.99
    apples_agent-2_min: 0
    apples_agent-3_max: 14
    apples_agent-3_mean: 2.84
    apples_agent-3_min: 0
    apples_agent-4_max: 58
    apples_agent-4_mean: 1.83
    apples_agent-4_min: 0
    apples_agent-5_max: 26
    apples_agent-5_mean: 1.62
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 150
    cleaning_beam_agent-0_mean: 78.8
    cleaning_beam_agent-0_min: 27
    cleaning_beam_agent-1_max: 417
    cleaning_beam_agent-1_mean: 245.87
    cleaning_beam_agent-1_min: 146
    cleaning_beam_agent-2_max: 78
    cleaning_beam_agent-2_mean: 16.16
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 196
    cleaning_beam_agent-3_mean: 93.28
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 105
    cleaning_beam_agent-4_mean: 33.85
    cleaning_beam_agent-4_min: 6
    cleaning_beam_agent-5_max: 39
    cleaning_beam_agent-5_mean: 6.66
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 5
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-33-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 49.0
  episode_reward_mean: 20.18
  episode_reward_min: -35.0
  episodes_this_iter: 96
  episodes_total: 24384
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11486.677
    learner:
      agent-0:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5520635843276978
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008855151827447116
        model: {}
        policy_loss: -0.0022325406316667795
        total_loss: -0.003083169925957918
        vf_explained_var: -0.0007614046335220337
        vf_loss: 1.2100006341934204
      agent-1:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48598796129226685
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012441935250535607
        model: {}
        policy_loss: -0.0027156705036759377
        total_loss: -0.0035598278045654297
        vf_explained_var: 0.017528191208839417
        vf_loss: 0.11180318892002106
      agent-2:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4706152379512787
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011730538681149483
        model: {}
        policy_loss: -0.0032123371493071318
        total_loss: -0.0040069264359772205
        vf_explained_var: -0.005001425743103027
        vf_loss: 0.33692479133605957
      agent-3:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5824663639068604
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014819731004536152
        model: {}
        policy_loss: -0.0027622911147773266
        total_loss: -0.0037531619891524315
        vf_explained_var: 0.0025861263275146484
        vf_loss: 0.3426716923713684
      agent-4:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6700108051300049
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014671434182673693
        model: {}
        policy_loss: -0.003859112039208412
        total_loss: -0.00500011770054698
        vf_explained_var: 0.01608961820602417
        vf_loss: 0.3821282982826233
      agent-5:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8768986463546753
        entropy_coeff: 0.0017600000137463212
        kl: 0.001824694685637951
        model: {}
        policy_loss: -0.002629933413118124
        total_loss: -0.0041504246182739735
        vf_explained_var: 0.010335162281990051
        vf_loss: 0.22849301993846893
    load_time_ms: 13520.405
    num_steps_sampled: 24384000
    num_steps_trained: 24384000
    sample_time_ms: 97535.271
    update_time_ms: 16.176
  iterations_since_restore: 94
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.655747126436783
    ram_util_percent: 11.535632183908046
  pid: 13408
  policy_reward_max:
    agent-0: 9.0
    agent-1: 6.0
    agent-2: 13.0
    agent-3: 12.0
    agent-4: 16.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 2.78
    agent-1: 1.64
    agent-2: 4.42
    agent-3: 4.04
    agent-4: 4.62
    agent-5: 2.68
  policy_reward_min:
    agent-0: -45.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -4.0
  sampler_perf:
    mean_env_wait_ms: 22.945375127303013
    mean_inference_ms: 12.79503942237909
    mean_processing_ms: 57.82982074653989
  time_since_restore: 11648.81126499176
  time_this_iter_s: 122.01843810081482
  time_total_s: 34159.094507455826
  timestamp: 1637231616
  timesteps_since_restore: 9024000
  timesteps_this_iter: 96000
  timesteps_total: 24384000
  training_iteration: 254
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    254 |          34159.1 | 24384000 |    20.18 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 57
    apples_agent-0_mean: 3.4
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 1.05
    apples_agent-1_min: 0
    apples_agent-2_max: 27
    apples_agent-2_mean: 2.07
    apples_agent-2_min: 0
    apples_agent-3_max: 24
    apples_agent-3_mean: 3.27
    apples_agent-3_min: 0
    apples_agent-4_max: 17
    apples_agent-4_mean: 1.57
    apples_agent-4_min: 0
    apples_agent-5_max: 55
    apples_agent-5_mean: 2.28
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 147
    cleaning_beam_agent-0_mean: 79.94
    cleaning_beam_agent-0_min: 13
    cleaning_beam_agent-1_max: 351
    cleaning_beam_agent-1_mean: 241.59
    cleaning_beam_agent-1_min: 142
    cleaning_beam_agent-2_max: 66
    cleaning_beam_agent-2_mean: 17.03
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 170
    cleaning_beam_agent-3_mean: 89.0
    cleaning_beam_agent-3_min: 31
    cleaning_beam_agent-4_max: 150
    cleaning_beam_agent-4_mean: 39.02
    cleaning_beam_agent-4_min: 6
    cleaning_beam_agent-5_max: 40
    cleaning_beam_agent-5_mean: 5.72
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-35-39
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 96.0
  episode_reward_mean: 23.18
  episode_reward_min: -91.0
  episodes_this_iter: 96
  episodes_total: 24480
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11488.432
    learner:
      agent-0:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5392794013023376
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014564234297722578
        model: {}
        policy_loss: -0.0033516017720103264
        total_loss: -0.004254637751728296
        vf_explained_var: 0.0028478503227233887
        vf_loss: 0.46095719933509827
      agent-1:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4879310727119446
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010075222235172987
        model: {}
        policy_loss: -0.0025131525471806526
        total_loss: -0.0033548222854733467
        vf_explained_var: 0.030337229371070862
        vf_loss: 0.17092058062553406
      agent-2:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45925766229629517
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010027536191046238
        model: {}
        policy_loss: -0.003176330588757992
        total_loss: -0.0038986396975815296
        vf_explained_var: 0.0023245811462402344
        vf_loss: 0.8598625063896179
      agent-3:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5917316675186157
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007117598433978856
        model: {}
        policy_loss: -0.0013959421776235104
        total_loss: -0.0022427267394959927
        vf_explained_var: 0.00034336745738983154
        vf_loss: 1.9466471672058105
      agent-4:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6618284583091736
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016679251566529274
        model: {}
        policy_loss: -0.0040107592940330505
        total_loss: -0.00512550538405776
        vf_explained_var: 0.030498623847961426
        vf_loss: 0.5006989240646362
      agent-5:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8630293607711792
        entropy_coeff: 0.0017600000137463212
        kl: 0.00184273486956954
        model: {}
        policy_loss: -0.0028405352495610714
        total_loss: -0.004327600821852684
        vf_explained_var: 0.01557980477809906
        vf_loss: 0.31865209341049194
    load_time_ms: 13513.279
    num_steps_sampled: 24480000
    num_steps_trained: 24480000
    sample_time_ms: 97547.316
    update_time_ms: 16.6
  iterations_since_restore: 95
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.545714285714286
    ram_util_percent: 11.658285714285714
  pid: 13408
  policy_reward_max:
    agent-0: 19.0
    agent-1: 13.0
    agent-2: 31.0
    agent-3: 22.0
    agent-4: 17.0
    agent-5: 16.0
  policy_reward_mean:
    agent-0: 3.75
    agent-1: 2.17
    agent-2: 5.12
    agent-3: 3.44
    agent-4: 5.41
    agent-5: 3.29
  policy_reward_min:
    agent-0: 0.0
    agent-1: -1.0
    agent-2: -47.0
    agent-3: -49.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.941923481582524
    mean_inference_ms: 12.793326183203929
    mean_processing_ms: 57.826429647113635
  time_since_restore: 11771.783054828644
  time_this_iter_s: 122.97178983688354
  time_total_s: 34282.06629729271
  timestamp: 1637231739
  timesteps_since_restore: 9120000
  timesteps_this_iter: 96000
  timesteps_total: 24480000
  training_iteration: 255
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    255 |          34282.1 | 24480000 |    23.18 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 2.54
    apples_agent-0_min: 0
    apples_agent-1_max: 28
    apples_agent-1_mean: 1.29
    apples_agent-1_min: 0
    apples_agent-2_max: 26
    apples_agent-2_mean: 2.16
    apples_agent-2_min: 0
    apples_agent-3_max: 13
    apples_agent-3_mean: 2.9
    apples_agent-3_min: 0
    apples_agent-4_max: 35
    apples_agent-4_mean: 1.56
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 1.47
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 186
    cleaning_beam_agent-0_mean: 75.9
    cleaning_beam_agent-0_min: 18
    cleaning_beam_agent-1_max: 343
    cleaning_beam_agent-1_mean: 240.14
    cleaning_beam_agent-1_min: 132
    cleaning_beam_agent-2_max: 58
    cleaning_beam_agent-2_mean: 16.07
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 168
    cleaning_beam_agent-3_mean: 87.09
    cleaning_beam_agent-3_min: 35
    cleaning_beam_agent-4_max: 217
    cleaning_beam_agent-4_mean: 40.21
    cleaning_beam_agent-4_min: 5
    cleaning_beam_agent-5_max: 23
    cleaning_beam_agent-5_mean: 6.47
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-37-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 47.0
  episode_reward_mean: 20.86
  episode_reward_min: -25.0
  episodes_this_iter: 96
  episodes_total: 24576
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11484.777
    learner:
      agent-0:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5494564175605774
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013649982865899801
        model: {}
        policy_loss: -0.003268950153142214
        total_loss: -0.00420050835236907
        vf_explained_var: -0.0034324973821640015
        vf_loss: 0.35485726594924927
      agent-1:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.481888085603714
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011431860039010644
        model: {}
        policy_loss: -0.0026182872243225574
        total_loss: -0.0034534616861492395
        vf_explained_var: 0.023018747568130493
        vf_loss: 0.12946344912052155
      agent-2:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45519310235977173
        entropy_coeff: 0.0017600000137463212
        kl: 0.001738624181598425
        model: {}
        policy_loss: -0.0037396240513771772
        total_loss: -0.004498686641454697
        vf_explained_var: -0.007455721497535706
        vf_loss: 0.4207342267036438
      agent-3:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.586989164352417
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010480432538315654
        model: {}
        policy_loss: -0.002633335767313838
        total_loss: -0.0036324781831353903
        vf_explained_var: -0.0057318806648254395
        vf_loss: 0.3396092355251312
      agent-4:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6575612425804138
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014685881324112415
        model: {}
        policy_loss: -0.004064434207975864
        total_loss: -0.005178737919777632
        vf_explained_var: 0.019712746143341064
        vf_loss: 0.43004870414733887
      agent-5:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8725703358650208
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012708287686109543
        model: {}
        policy_loss: -0.0018903829623013735
        total_loss: -0.0032733087427914143
        vf_explained_var: 0.004102766513824463
        vf_loss: 1.5279914140701294
    load_time_ms: 13510.104
    num_steps_sampled: 24576000
    num_steps_trained: 24576000
    sample_time_ms: 97688.743
    update_time_ms: 16.325
  iterations_since_restore: 96
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.93068181818182
    ram_util_percent: 11.475000000000001
  pid: 13408
  policy_reward_max:
    agent-0: 12.0
    agent-1: 8.0
    agent-2: 13.0
    agent-3: 15.0
    agent-4: 16.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 3.09
    agent-1: 1.75
    agent-2: 4.88
    agent-3: 3.94
    agent-4: 4.81
    agent-5: 2.39
  policy_reward_min:
    agent-0: -46.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -48.0
  sampler_perf:
    mean_env_wait_ms: 22.938555479494333
    mean_inference_ms: 12.791775946614104
    mean_processing_ms: 57.82655864707018
  time_since_restore: 11895.647554397583
  time_this_iter_s: 123.86449956893921
  time_total_s: 34405.93079686165
  timestamp: 1637231863
  timesteps_since_restore: 9216000
  timesteps_this_iter: 96000
  timesteps_total: 24576000
  training_iteration: 256
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    256 |          34405.9 | 24576000 |    20.86 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 3.04
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 1.24
    apples_agent-1_min: 0
    apples_agent-2_max: 8
    apples_agent-2_mean: 2.09
    apples_agent-2_min: 0
    apples_agent-3_max: 18
    apples_agent-3_mean: 3.5
    apples_agent-3_min: 0
    apples_agent-4_max: 32
    apples_agent-4_mean: 2.14
    apples_agent-4_min: 0
    apples_agent-5_max: 8
    apples_agent-5_mean: 1.34
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 176
    cleaning_beam_agent-0_mean: 81.54
    cleaning_beam_agent-0_min: 24
    cleaning_beam_agent-1_max: 391
    cleaning_beam_agent-1_mean: 262.12
    cleaning_beam_agent-1_min: 158
    cleaning_beam_agent-2_max: 69
    cleaning_beam_agent-2_mean: 15.84
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 216
    cleaning_beam_agent-3_mean: 88.03
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 159
    cleaning_beam_agent-4_mean: 39.19
    cleaning_beam_agent-4_min: 5
    cleaning_beam_agent-5_max: 24
    cleaning_beam_agent-5_mean: 6.18
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-39-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 56.0
  episode_reward_mean: 22.55
  episode_reward_min: -130.0
  episodes_this_iter: 96
  episodes_total: 24672
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11480.49
    learner:
      agent-0:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5501057505607605
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014131697826087475
        model: {}
        policy_loss: -0.003330824663862586
        total_loss: -0.004263720940798521
        vf_explained_var: -0.0030815601348876953
        vf_loss: 0.3529013395309448
      agent-1:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4829750061035156
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008336069295182824
        model: {}
        policy_loss: -0.0019030598923563957
        total_loss: -0.0026256698183715343
        vf_explained_var: 0.009353399276733398
        vf_loss: 1.2742700576782227
      agent-2:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4640507102012634
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012253082823008299
        model: {}
        policy_loss: -0.002278513740748167
        total_loss: -0.002943941857665777
        vf_explained_var: 0.0014010965824127197
        vf_loss: 1.5130293369293213
      agent-3:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5738017559051514
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012674513272941113
        model: {}
        policy_loss: -0.0027485182508826256
        total_loss: -0.003716768231242895
        vf_explained_var: 0.002793550491333008
        vf_loss: 0.41644781827926636
      agent-4:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6653435826301575
        entropy_coeff: 0.0017600000137463212
        kl: 0.00143832853063941
        model: {}
        policy_loss: -0.0037686219438910484
        total_loss: -0.004888859577476978
        vf_explained_var: 0.007009610533714294
        vf_loss: 0.5076825618743896
      agent-5:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8705752491950989
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021306260023266077
        model: {}
        policy_loss: -0.0029341834597289562
        total_loss: -0.004330382216721773
        vf_explained_var: 0.0004618316888809204
        vf_loss: 1.3601677417755127
    load_time_ms: 13512.141
    num_steps_sampled: 24672000
    num_steps_trained: 24672000
    sample_time_ms: 97624.852
    update_time_ms: 16.826
  iterations_since_restore: 97
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.404022988505744
    ram_util_percent: 11.51781609195402
  pid: 13408
  policy_reward_max:
    agent-0: 12.0
    agent-1: 8.0
    agent-2: 14.0
    agent-3: 22.0
    agent-4: 14.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 4.23
    agent-1: 1.4
    agent-2: 4.44
    agent-3: 4.38
    agent-4: 5.79
    agent-5: 2.31
  policy_reward_min:
    agent-0: 0.0
    agent-1: -50.0
    agent-2: -40.0
    agent-3: 0.0
    agent-4: 1.0
    agent-5: -49.0
  sampler_perf:
    mean_env_wait_ms: 22.936482370105786
    mean_inference_ms: 12.789470833596845
    mean_processing_ms: 57.82331769542602
  time_since_restore: 12017.85044169426
  time_this_iter_s: 122.20288729667664
  time_total_s: 34528.133684158325
  timestamp: 1637231985
  timesteps_since_restore: 9312000
  timesteps_this_iter: 96000
  timesteps_total: 24672000
  training_iteration: 257
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    257 |          34528.1 | 24672000 |    22.55 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 2.95
    apples_agent-0_min: 0
    apples_agent-1_max: 46
    apples_agent-1_mean: 1.53
    apples_agent-1_min: 0
    apples_agent-2_max: 12
    apples_agent-2_mean: 2.22
    apples_agent-2_min: 0
    apples_agent-3_max: 19
    apples_agent-3_mean: 3.39
    apples_agent-3_min: 0
    apples_agent-4_max: 10
    apples_agent-4_mean: 1.49
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 1.72
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 164
    cleaning_beam_agent-0_mean: 72.45
    cleaning_beam_agent-0_min: 17
    cleaning_beam_agent-1_max: 398
    cleaning_beam_agent-1_mean: 255.3
    cleaning_beam_agent-1_min: 151
    cleaning_beam_agent-2_max: 74
    cleaning_beam_agent-2_mean: 16.26
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 237
    cleaning_beam_agent-3_mean: 83.27
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 138
    cleaning_beam_agent-4_mean: 45.43
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 32
    cleaning_beam_agent-5_mean: 6.74
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-41-49
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 52.0
  episode_reward_mean: 23.92
  episode_reward_min: 7.0
  episodes_this_iter: 96
  episodes_total: 24768
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11496.991
    learner:
      agent-0:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5557345747947693
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012825867161154747
        model: {}
        policy_loss: -0.0033308351412415504
        total_loss: -0.004275849089026451
        vf_explained_var: 0.00025697052478790283
        vf_loss: 0.3307688236236572
      agent-1:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47686779499053955
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011346832616254687
        model: {}
        policy_loss: -0.002524422015994787
        total_loss: -0.003348272293806076
        vf_explained_var: 0.01811806857585907
        vf_loss: 0.15435409545898438
      agent-2:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46302467584609985
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013005624059587717
        model: {}
        policy_loss: -0.0036136535927653313
        total_loss: -0.004379591904580593
        vf_explained_var: 0.00941498577594757
        vf_loss: 0.4898303747177124
      agent-3:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5738003849983215
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013267728500068188
        model: {}
        policy_loss: -0.002875437494367361
        total_loss: -0.0038489382714033127
        vf_explained_var: -0.00022542476654052734
        vf_loss: 0.3638852834701538
      agent-4:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6489355564117432
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013202485861256719
        model: {}
        policy_loss: -0.003751829732209444
        total_loss: -0.004852808080613613
        vf_explained_var: 0.01512540876865387
        vf_loss: 0.41147860884666443
      agent-5:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8812909126281738
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017381890211254358
        model: {}
        policy_loss: -0.0027759410440921783
        total_loss: -0.0043023391626775265
        vf_explained_var: 0.0022244006395339966
        vf_loss: 0.2467326819896698
    load_time_ms: 13530.809
    num_steps_sampled: 24768000
    num_steps_trained: 24768000
    sample_time_ms: 97658.284
    update_time_ms: 16.157
  iterations_since_restore: 98
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.278977272727275
    ram_util_percent: 11.517613636363636
  pid: 13408
  policy_reward_max:
    agent-0: 11.0
    agent-1: 6.0
    agent-2: 16.0
    agent-3: 20.0
    agent-4: 14.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 4.15
    agent-1: 2.14
    agent-2: 5.09
    agent-3: 4.48
    agent-4: 4.87
    agent-5: 3.19
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.93337515554294
    mean_inference_ms: 12.78813616234254
    mean_processing_ms: 57.82063301032327
  time_since_restore: 12141.189836978912
  time_this_iter_s: 123.33939528465271
  time_total_s: 34651.47307944298
  timestamp: 1637232109
  timesteps_since_restore: 9408000
  timesteps_this_iter: 96000
  timesteps_total: 24768000
  training_iteration: 258
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    258 |          34651.5 | 24768000 |    23.92 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 8
    apples_agent-0_mean: 2.25
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.91
    apples_agent-1_min: 0
    apples_agent-2_max: 9
    apples_agent-2_mean: 1.92
    apples_agent-2_min: 0
    apples_agent-3_max: 10
    apples_agent-3_mean: 2.64
    apples_agent-3_min: 0
    apples_agent-4_max: 164
    apples_agent-4_mean: 2.75
    apples_agent-4_min: 0
    apples_agent-5_max: 11
    apples_agent-5_mean: 1.48
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 139
    cleaning_beam_agent-0_mean: 71.17
    cleaning_beam_agent-0_min: 16
    cleaning_beam_agent-1_max: 384
    cleaning_beam_agent-1_mean: 259.03
    cleaning_beam_agent-1_min: 128
    cleaning_beam_agent-2_max: 54
    cleaning_beam_agent-2_mean: 14.28
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 209
    cleaning_beam_agent-3_mean: 77.51
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 255
    cleaning_beam_agent-4_mean: 37.7
    cleaning_beam_agent-4_min: 8
    cleaning_beam_agent-5_max: 23
    cleaning_beam_agent-5_mean: 5.35
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-43-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 49.0
  episode_reward_mean: 20.19
  episode_reward_min: -91.0
  episodes_this_iter: 96
  episodes_total: 24864
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11512.893
    learner:
      agent-0:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5644968152046204
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010310804937034845
        model: {}
        policy_loss: -0.0021408838219940662
        total_loss: -0.0029758336022496223
        vf_explained_var: -0.00350227952003479
        vf_loss: 1.585647463798523
      agent-1:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.488098680973053
        entropy_coeff: 0.0017600000137463212
        kl: 0.00107090943492949
        model: {}
        policy_loss: -0.0027816607616841793
        total_loss: -0.0036301580257713795
        vf_explained_var: 0.03117769956588745
        vf_loss: 0.10556621104478836
      agent-2:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4537027180194855
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013930025743320584
        model: {}
        policy_loss: -0.003435158170759678
        total_loss: -0.004194593522697687
        vf_explained_var: -0.0019088387489318848
        vf_loss: 0.3907943665981293
      agent-3:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5771969556808472
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010909963166341186
        model: {}
        policy_loss: -0.00273791141808033
        total_loss: -0.0035975989885628223
        vf_explained_var: -0.0015518367290496826
        vf_loss: 1.5618047714233398
      agent-4:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6559988856315613
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013090737629681826
        model: {}
        policy_loss: -0.0036677070893347263
        total_loss: -0.004779731389135122
        vf_explained_var: 0.007833659648895264
        vf_loss: 0.42534947395324707
      agent-5:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8104655742645264
        entropy_coeff: 0.0017600000137463212
        kl: 0.001415117410942912
        model: {}
        policy_loss: -0.0024298708885908127
        total_loss: -0.003697984851896763
        vf_explained_var: 0.007481321692466736
        vf_loss: 1.5830433368682861
    load_time_ms: 13530.639
    num_steps_sampled: 24864000
    num_steps_trained: 24864000
    sample_time_ms: 97638.705
    update_time_ms: 16.28
  iterations_since_restore: 99
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.429479768786127
    ram_util_percent: 11.452023121387283
  pid: 13408
  policy_reward_max:
    agent-0: 10.0
    agent-1: 7.0
    agent-2: 13.0
    agent-3: 15.0
    agent-4: 14.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 3.02
    agent-1: 1.71
    agent-2: 4.81
    agent-3: 3.45
    agent-4: 4.81
    agent-5: 2.39
  policy_reward_min:
    agent-0: -50.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -45.0
    agent-4: 0.0
    agent-5: -47.0
  sampler_perf:
    mean_env_wait_ms: 22.928163779525754
    mean_inference_ms: 12.786630448902851
    mean_processing_ms: 57.81612901338324
  time_since_restore: 12262.846066474915
  time_this_iter_s: 121.6562294960022
  time_total_s: 34773.12930893898
  timestamp: 1637232231
  timesteps_since_restore: 9504000
  timesteps_this_iter: 96000
  timesteps_total: 24864000
  training_iteration: 259
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    259 |          34773.1 | 24864000 |    20.19 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 2.74
    apples_agent-0_min: 0
    apples_agent-1_max: 9
    apples_agent-1_mean: 1.06
    apples_agent-1_min: 0
    apples_agent-2_max: 57
    apples_agent-2_mean: 2.26
    apples_agent-2_min: 0
    apples_agent-3_max: 31
    apples_agent-3_mean: 3.55
    apples_agent-3_min: 0
    apples_agent-4_max: 11
    apples_agent-4_mean: 1.43
    apples_agent-4_min: 0
    apples_agent-5_max: 10
    apples_agent-5_mean: 1.9
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 149
    cleaning_beam_agent-0_mean: 73.87
    cleaning_beam_agent-0_min: 22
    cleaning_beam_agent-1_max: 504
    cleaning_beam_agent-1_mean: 263.61
    cleaning_beam_agent-1_min: 135
    cleaning_beam_agent-2_max: 50
    cleaning_beam_agent-2_mean: 13.55
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 242
    cleaning_beam_agent-3_mean: 74.87
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 111
    cleaning_beam_agent-4_mean: 42.06
    cleaning_beam_agent-4_min: 6
    cleaning_beam_agent-5_max: 33
    cleaning_beam_agent-5_mean: 5.89
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-45-54
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 59.0
  episode_reward_mean: 23.66
  episode_reward_min: -31.0
  episodes_this_iter: 96
  episodes_total: 24960
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11512.096
    learner:
      agent-0:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5536600947380066
        entropy_coeff: 0.0017600000137463212
        kl: 0.001322103664278984
        model: {}
        policy_loss: -0.0032536713406443596
        total_loss: -0.00418630288913846
        vf_explained_var: 0.003292381763458252
        vf_loss: 0.4180956184864044
      agent-1:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47395503520965576
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012048378121107817
        model: {}
        policy_loss: -0.0025467241648584604
        total_loss: -0.0033627108205109835
        vf_explained_var: 0.023695170879364014
        vf_loss: 0.18177266418933868
      agent-2:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4568507969379425
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013172563631087542
        model: {}
        policy_loss: -0.0038274498656392097
        total_loss: -0.004579199943691492
        vf_explained_var: 0.009410873055458069
        vf_loss: 0.5230521559715271
      agent-3:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.566519021987915
        entropy_coeff: 0.0017600000137463212
        kl: 0.001113424776121974
        model: {}
        policy_loss: -0.0023903409019112587
        total_loss: -0.0033418366219848394
        vf_explained_var: 0.0014388561248779297
        vf_loss: 0.4557633697986603
      agent-4:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6624561548233032
        entropy_coeff: 0.0017600000137463212
        kl: 0.001357225002720952
        model: {}
        policy_loss: -0.003985768184065819
        total_loss: -0.0051053068600595
        vf_explained_var: 0.0182674378156662
        vf_loss: 0.46387726068496704
      agent-5:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8457223176956177
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011130418861284852
        model: {}
        policy_loss: -0.002536516636610031
        total_loss: -0.003996902145445347
        vf_explained_var: 0.012012124061584473
        vf_loss: 0.28085818886756897
    load_time_ms: 13537.921
    num_steps_sampled: 24960000
    num_steps_trained: 24960000
    sample_time_ms: 97658.378
    update_time_ms: 15.654
  iterations_since_restore: 100
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.155113636363634
    ram_util_percent: 11.51931818181818
  pid: 13408
  policy_reward_max:
    agent-0: 13.0
    agent-1: 9.0
    agent-2: 17.0
    agent-3: 20.0
    agent-4: 15.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 3.78
    agent-1: 2.11
    agent-2: 4.69
    agent-3: 4.39
    agent-4: 5.33
    agent-5: 3.36
  policy_reward_min:
    agent-0: -49.0
    agent-1: 0.0
    agent-2: -40.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.925354851953024
    mean_inference_ms: 12.785680831625097
    mean_processing_ms: 57.8135008604279
  time_since_restore: 12386.316921949387
  time_this_iter_s: 123.47085547447205
  time_total_s: 34896.60016441345
  timestamp: 1637232354
  timesteps_since_restore: 9600000
  timesteps_this_iter: 96000
  timesteps_total: 24960000
  training_iteration: 260
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    260 |          34896.6 | 24960000 |    23.66 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 8
    apples_agent-0_mean: 2.31
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.99
    apples_agent-1_min: 0
    apples_agent-2_max: 27
    apples_agent-2_mean: 2.58
    apples_agent-2_min: 0
    apples_agent-3_max: 34
    apples_agent-3_mean: 3.23
    apples_agent-3_min: 0
    apples_agent-4_max: 14
    apples_agent-4_mean: 1.11
    apples_agent-4_min: 0
    apples_agent-5_max: 18
    apples_agent-5_mean: 2.16
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 156
    cleaning_beam_agent-0_mean: 75.41
    cleaning_beam_agent-0_min: 24
    cleaning_beam_agent-1_max: 504
    cleaning_beam_agent-1_mean: 266.46
    cleaning_beam_agent-1_min: 163
    cleaning_beam_agent-2_max: 62
    cleaning_beam_agent-2_mean: 12.87
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 216
    cleaning_beam_agent-3_mean: 78.96
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 114
    cleaning_beam_agent-4_mean: 35.19
    cleaning_beam_agent-4_min: 10
    cleaning_beam_agent-5_max: 23
    cleaning_beam_agent-5_mean: 6.0
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-47-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 66.0
  episode_reward_mean: 24.53
  episode_reward_min: 6.0
  episodes_this_iter: 96
  episodes_total: 25056
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11519.502
    learner:
      agent-0:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5571505427360535
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013294434174895287
        model: {}
        policy_loss: -0.003203495405614376
        total_loss: -0.004157925955951214
        vf_explained_var: 0.0018161684274673462
        vf_loss: 0.26154178380966187
      agent-1:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47742900252342224
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008967827307060361
        model: {}
        policy_loss: -0.002486248966306448
        total_loss: -0.0033106375485658646
        vf_explained_var: 0.018715962767601013
        vf_loss: 0.15885452926158905
      agent-2:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45430660247802734
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013289442285895348
        model: {}
        policy_loss: -0.003140117973089218
        total_loss: -0.0038723156321793795
        vf_explained_var: 0.006032198667526245
        vf_loss: 0.6738026142120361
      agent-3:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5659974813461304
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013744629686698318
        model: {}
        policy_loss: -0.003173520090058446
        total_loss: -0.004134883638471365
        vf_explained_var: 0.0027150213718414307
        vf_loss: 0.34791669249534607
      agent-4:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6459261178970337
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017258536536246538
        model: {}
        policy_loss: -0.003912562970072031
        total_loss: -0.005007939413189888
        vf_explained_var: 0.024781838059425354
        vf_loss: 0.4145231246948242
      agent-5:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8445611000061035
        entropy_coeff: 0.0017600000137463212
        kl: 0.00204899231903255
        model: {}
        policy_loss: -0.0030787670984864235
        total_loss: -0.004541712813079357
        vf_explained_var: 0.006130903959274292
        vf_loss: 0.23484615981578827
    load_time_ms: 13540.456
    num_steps_sampled: 25056000
    num_steps_trained: 25056000
    sample_time_ms: 97725.643
    update_time_ms: 15.68
  iterations_since_restore: 101
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.49942857142857
    ram_util_percent: 11.696571428571428
  pid: 13408
  policy_reward_max:
    agent-0: 10.0
    agent-1: 11.0
    agent-2: 25.0
    agent-3: 16.0
    agent-4: 14.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 3.65
    agent-1: 1.92
    agent-2: 6.12
    agent-3: 4.41
    agent-4: 5.28
    agent-5: 3.15
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.92128415970162
    mean_inference_ms: 12.78424546661503
    mean_processing_ms: 57.806498453422954
  time_since_restore: 12508.853018522263
  time_this_iter_s: 122.53609657287598
  time_total_s: 35019.13626098633
  timestamp: 1637232477
  timesteps_since_restore: 9696000
  timesteps_this_iter: 96000
  timesteps_total: 25056000
  training_iteration: 261
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    261 |          35019.1 | 25056000 |    24.53 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 2.43
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 1.11
    apples_agent-1_min: 0
    apples_agent-2_max: 29
    apples_agent-2_mean: 2.2
    apples_agent-2_min: 0
    apples_agent-3_max: 11
    apples_agent-3_mean: 2.77
    apples_agent-3_min: 0
    apples_agent-4_max: 28
    apples_agent-4_mean: 1.61
    apples_agent-4_min: 0
    apples_agent-5_max: 27
    apples_agent-5_mean: 1.77
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 161
    cleaning_beam_agent-0_mean: 75.92
    cleaning_beam_agent-0_min: 23
    cleaning_beam_agent-1_max: 477
    cleaning_beam_agent-1_mean: 258.16
    cleaning_beam_agent-1_min: 153
    cleaning_beam_agent-2_max: 80
    cleaning_beam_agent-2_mean: 14.35
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 237
    cleaning_beam_agent-3_mean: 76.9
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 123
    cleaning_beam_agent-4_mean: 39.57
    cleaning_beam_agent-4_min: 3
    cleaning_beam_agent-5_max: 40
    cleaning_beam_agent-5_mean: 6.94
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-50-00
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 61.0
  episode_reward_mean: 23.14
  episode_reward_min: 4.0
  episodes_this_iter: 96
  episodes_total: 25152
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11523.719
    learner:
      agent-0:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5586525797843933
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014156526885926723
        model: {}
        policy_loss: -0.003428294789046049
        total_loss: -0.0043862867169082165
        vf_explained_var: -0.009405821561813354
        vf_loss: 0.2523839473724365
      agent-1:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4806661307811737
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011345872189849615
        model: {}
        policy_loss: -0.002522677183151245
        total_loss: -0.00335580762475729
        vf_explained_var: 0.020263105630874634
        vf_loss: 0.12840765714645386
      agent-2:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.463736891746521
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015345821157097816
        model: {}
        policy_loss: -0.0038374934811145067
        total_loss: -0.004601048305630684
        vf_explained_var: -0.003149479627609253
        vf_loss: 0.526211678981781
      agent-3:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5735012888908386
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011902619153261185
        model: {}
        policy_loss: -0.0029298956505954266
        total_loss: -0.003902715165168047
        vf_explained_var: 0.004984349012374878
        vf_loss: 0.36541879177093506
      agent-4:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.655457615852356
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015961616300046444
        model: {}
        policy_loss: -0.003801316488534212
        total_loss: -0.004909731447696686
        vf_explained_var: 0.020375147461891174
        vf_loss: 0.4519087076187134
      agent-5:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8571358919143677
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017544410657137632
        model: {}
        policy_loss: -0.0029134759679436684
        total_loss: -0.004399283789098263
        vf_explained_var: 0.009108498692512512
        vf_loss: 0.22749650478363037
    load_time_ms: 13542.636
    num_steps_sampled: 25152000
    num_steps_trained: 25152000
    sample_time_ms: 97631.656
    update_time_ms: 15.453
  iterations_since_restore: 102
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.768965517241377
    ram_util_percent: 11.772413793103452
  pid: 13408
  policy_reward_max:
    agent-0: 13.0
    agent-1: 11.0
    agent-2: 15.0
    agent-3: 13.0
    agent-4: 14.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 3.6
    agent-1: 1.98
    agent-2: 5.21
    agent-3: 4.09
    agent-4: 5.25
    agent-5: 3.01
  policy_reward_min:
    agent-0: -1.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.917238559614617
    mean_inference_ms: 12.7832174590889
    mean_processing_ms: 57.804828376182805
  time_since_restore: 12631.307346820831
  time_this_iter_s: 122.45432829856873
  time_total_s: 35141.5905892849
  timestamp: 1637232600
  timesteps_since_restore: 9792000
  timesteps_this_iter: 96000
  timesteps_total: 25152000
  training_iteration: 262
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    262 |          35141.6 | 25152000 |    23.14 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 2.43
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 1.19
    apples_agent-1_min: 0
    apples_agent-2_max: 15
    apples_agent-2_mean: 2.06
    apples_agent-2_min: 0
    apples_agent-3_max: 13
    apples_agent-3_mean: 3.06
    apples_agent-3_min: 0
    apples_agent-4_max: 7
    apples_agent-4_mean: 1.2
    apples_agent-4_min: 0
    apples_agent-5_max: 15
    apples_agent-5_mean: 2.01
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 121
    cleaning_beam_agent-0_mean: 69.87
    cleaning_beam_agent-0_min: 22
    cleaning_beam_agent-1_max: 424
    cleaning_beam_agent-1_mean: 252.62
    cleaning_beam_agent-1_min: 130
    cleaning_beam_agent-2_max: 54
    cleaning_beam_agent-2_mean: 13.09
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 246
    cleaning_beam_agent-3_mean: 83.57
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 110
    cleaning_beam_agent-4_mean: 36.99
    cleaning_beam_agent-4_min: 5
    cleaning_beam_agent-5_max: 24
    cleaning_beam_agent-5_mean: 5.49
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-52-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 54.0
  episode_reward_mean: 23.56
  episode_reward_min: -12.0
  episodes_this_iter: 96
  episodes_total: 25248
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11540.864
    learner:
      agent-0:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5468384623527527
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008589338976889849
        model: {}
        policy_loss: -0.0021778810769319534
        total_loss: -0.002977894153445959
        vf_explained_var: -0.005473405122756958
        vf_loss: 1.624255657196045
      agent-1:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48220598697662354
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006447770283557475
        model: {}
        policy_loss: -0.002114343224093318
        total_loss: -0.0029510166496038437
        vf_explained_var: 0.032229021191596985
        vf_loss: 0.12006582319736481
      agent-2:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45667046308517456
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014143785228952765
        model: {}
        policy_loss: -0.0034947898238897324
        total_loss: -0.004247693344950676
        vf_explained_var: -0.004797786474227905
        vf_loss: 0.5083476305007935
      agent-3:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5827009081840515
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017618434503674507
        model: {}
        policy_loss: -0.002633533673360944
        total_loss: -0.0036113581154495478
        vf_explained_var: 0.0022225379943847656
        vf_loss: 0.477301687002182
      agent-4:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6551303863525391
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015076231211423874
        model: {}
        policy_loss: -0.003919166978448629
        total_loss: -0.0050275735557079315
        vf_explained_var: 0.019953474402427673
        vf_loss: 0.4462379813194275
      agent-5:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.838624894618988
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017229593358933926
        model: {}
        policy_loss: -0.002556257415562868
        total_loss: -0.003996848128736019
        vf_explained_var: 0.013558894395828247
        vf_loss: 0.3539011776447296
    load_time_ms: 13551.8
    num_steps_sampled: 25248000
    num_steps_trained: 25248000
    sample_time_ms: 97505.609
    update_time_ms: 15.362
  iterations_since_restore: 103
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.361142857142855
    ram_util_percent: 11.653142857142859
  pid: 13408
  policy_reward_max:
    agent-0: 12.0
    agent-1: 9.0
    agent-2: 13.0
    agent-3: 17.0
    agent-4: 15.0
    agent-5: 17.0
  policy_reward_mean:
    agent-0: 3.13
    agent-1: 1.73
    agent-2: 5.36
    agent-3: 4.51
    agent-4: 5.34
    agent-5: 3.49
  policy_reward_min:
    agent-0: -48.0
    agent-1: 0.0
    agent-2: 1.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.913533936853252
    mean_inference_ms: 12.781735879107009
    mean_processing_ms: 57.80127322997234
  time_since_restore: 12753.83649969101
  time_this_iter_s: 122.52915287017822
  time_total_s: 35264.119742155075
  timestamp: 1637232723
  timesteps_since_restore: 9888000
  timesteps_this_iter: 96000
  timesteps_total: 25248000
  training_iteration: 263
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    263 |          35264.1 | 25248000 |    23.56 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 2.4
    apples_agent-0_min: 0
    apples_agent-1_max: 20
    apples_agent-1_mean: 1.41
    apples_agent-1_min: 0
    apples_agent-2_max: 21
    apples_agent-2_mean: 2.05
    apples_agent-2_min: 0
    apples_agent-3_max: 13
    apples_agent-3_mean: 3.13
    apples_agent-3_min: 0
    apples_agent-4_max: 9
    apples_agent-4_mean: 1.18
    apples_agent-4_min: 0
    apples_agent-5_max: 16
    apples_agent-5_mean: 1.81
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 226
    cleaning_beam_agent-0_mean: 77.28
    cleaning_beam_agent-0_min: 15
    cleaning_beam_agent-1_max: 377
    cleaning_beam_agent-1_mean: 245.68
    cleaning_beam_agent-1_min: 131
    cleaning_beam_agent-2_max: 87
    cleaning_beam_agent-2_mean: 14.97
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 186
    cleaning_beam_agent-3_mean: 80.03
    cleaning_beam_agent-3_min: 26
    cleaning_beam_agent-4_max: 136
    cleaning_beam_agent-4_mean: 36.63
    cleaning_beam_agent-4_min: 3
    cleaning_beam_agent-5_max: 34
    cleaning_beam_agent-5_mean: 6.63
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-54-05
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 53.0
  episode_reward_mean: 23.08
  episode_reward_min: -12.0
  episodes_this_iter: 96
  episodes_total: 25344
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11528.904
    learner:
      agent-0:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5515773892402649
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011710359249264002
        model: {}
        policy_loss: -0.0036290246061980724
        total_loss: -0.004575127735733986
        vf_explained_var: -0.003361120820045471
        vf_loss: 0.24673591554164886
      agent-1:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47482672333717346
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014141264837235212
        model: {}
        policy_loss: -0.0027650478295981884
        total_loss: -0.003585810074582696
        vf_explained_var: 0.026952877640724182
        vf_loss: 0.1493370234966278
      agent-2:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4521283507347107
        entropy_coeff: 0.0017600000137463212
        kl: 0.00132569030392915
        model: {}
        policy_loss: -0.003646058728918433
        total_loss: -0.0043855588883161545
        vf_explained_var: 0.0005508512258529663
        vf_loss: 0.5624510645866394
      agent-3:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.592274010181427
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015284582041203976
        model: {}
        policy_loss: -0.0029292425606399775
        total_loss: -0.003938023000955582
        vf_explained_var: -0.00291597843170166
        vf_loss: 0.3362053632736206
      agent-4:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6497612595558167
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013619983801618218
        model: {}
        policy_loss: -0.003985661081969738
        total_loss: -0.005086335353553295
        vf_explained_var: 0.014447525143623352
        vf_loss: 0.4290744662284851
      agent-5:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8500928282737732
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016236407682299614
        model: {}
        policy_loss: -0.0028820009902119637
        total_loss: -0.004351448267698288
        vf_explained_var: 0.008320912718772888
        vf_loss: 0.26718080043792725
    load_time_ms: 13557.189
    num_steps_sampled: 25344000
    num_steps_trained: 25344000
    sample_time_ms: 97540.291
    update_time_ms: 15.555
  iterations_since_restore: 104
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.364367816091956
    ram_util_percent: 11.607471264367815
  pid: 13408
  policy_reward_max:
    agent-0: 12.0
    agent-1: 8.0
    agent-2: 16.0
    agent-3: 17.0
    agent-4: 15.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 2.89
    agent-1: 2.07
    agent-2: 5.43
    agent-3: 4.36
    agent-4: 5.27
    agent-5: 3.06
  policy_reward_min:
    agent-0: -48.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.910526213867506
    mean_inference_ms: 12.780622598409307
    mean_processing_ms: 57.802292524881175
  time_since_restore: 12876.138401508331
  time_this_iter_s: 122.30190181732178
  time_total_s: 35386.4216439724
  timestamp: 1637232845
  timesteps_since_restore: 9984000
  timesteps_this_iter: 96000
  timesteps_total: 25344000
  training_iteration: 264
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    264 |          35386.4 | 25344000 |    23.08 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 2.48
    apples_agent-0_min: 0
    apples_agent-1_max: 53
    apples_agent-1_mean: 1.53
    apples_agent-1_min: 0
    apples_agent-2_max: 22
    apples_agent-2_mean: 2.57
    apples_agent-2_min: 0
    apples_agent-3_max: 13
    apples_agent-3_mean: 2.47
    apples_agent-3_min: 0
    apples_agent-4_max: 11
    apples_agent-4_mean: 1.52
    apples_agent-4_min: 0
    apples_agent-5_max: 16
    apples_agent-5_mean: 1.81
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 249
    cleaning_beam_agent-0_mean: 73.72
    cleaning_beam_agent-0_min: 14
    cleaning_beam_agent-1_max: 377
    cleaning_beam_agent-1_mean: 249.25
    cleaning_beam_agent-1_min: 141
    cleaning_beam_agent-2_max: 64
    cleaning_beam_agent-2_mean: 15.06
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 181
    cleaning_beam_agent-3_mean: 90.82
    cleaning_beam_agent-3_min: 25
    cleaning_beam_agent-4_max: 106
    cleaning_beam_agent-4_mean: 38.56
    cleaning_beam_agent-4_min: 7
    cleaning_beam_agent-5_max: 32
    cleaning_beam_agent-5_mean: 6.06
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-56-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 51.0
  episode_reward_mean: 21.98
  episode_reward_min: -43.0
  episodes_this_iter: 96
  episodes_total: 25440
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11529.23
    learner:
      agent-0:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.548066258430481
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017224495531991124
        model: {}
        policy_loss: -0.0035803625360131264
        total_loss: -0.0045144278556108475
        vf_explained_var: -0.001324281096458435
        vf_loss: 0.3053377866744995
      agent-1:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46659964323043823
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006041489541530609
        model: {}
        policy_loss: -0.0011988300830125809
        total_loss: -0.001879578921943903
        vf_explained_var: -0.00033789873123168945
        vf_loss: 1.404668927192688
      agent-2:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46175044775009155
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011487217852845788
        model: {}
        policy_loss: -0.002099110744893551
        total_loss: -0.002738796640187502
        vf_explained_var: 0.007241189479827881
        vf_loss: 1.7299597263336182
      agent-3:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5882788896560669
        entropy_coeff: 0.0017600000137463212
        kl: 0.001056993962265551
        model: {}
        policy_loss: -0.0024231416173279285
        total_loss: -0.003430914133787155
        vf_explained_var: 0.0011811256408691406
        vf_loss: 0.2759641408920288
      agent-4:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6560541391372681
        entropy_coeff: 0.0017600000137463212
        kl: 0.001509775873273611
        model: {}
        policy_loss: -0.003842058824375272
        total_loss: -0.004941869992762804
        vf_explained_var: 0.006676763296127319
        vf_loss: 0.5484241247177124
      agent-5:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8527072668075562
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023646207991987467
        model: {}
        policy_loss: -0.00315783079713583
        total_loss: -0.004635505378246307
        vf_explained_var: -0.0034758448600769043
        vf_loss: 0.23090723156929016
    load_time_ms: 13548.432
    num_steps_sampled: 25440000
    num_steps_trained: 25440000
    sample_time_ms: 97542.372
    update_time_ms: 15.3
  iterations_since_restore: 105
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.31714285714286
    ram_util_percent: 11.51485714285714
  pid: 13408
  policy_reward_max:
    agent-0: 13.0
    agent-1: 6.0
    agent-2: 12.0
    agent-3: 11.0
    agent-4: 22.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 3.83
    agent-1: 1.28
    agent-2: 4.46
    agent-3: 3.67
    agent-4: 5.7
    agent-5: 3.04
  policy_reward_min:
    agent-0: 0.0
    agent-1: -49.0
    agent-2: -48.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.908708389687405
    mean_inference_ms: 12.779721081733108
    mean_processing_ms: 57.80178744661362
  time_since_restore: 12998.996258497238
  time_this_iter_s: 122.85785698890686
  time_total_s: 35509.279500961304
  timestamp: 1637232968
  timesteps_since_restore: 10080000
  timesteps_this_iter: 96000
  timesteps_total: 25440000
  training_iteration: 265
  trial_id: '00000'
  
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
== Status ==
Memory usage on this node: 21.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    265 |          35509.3 | 25440000 |    21.98 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 2.39
    apples_agent-0_min: 0
    apples_agent-1_max: 14
    apples_agent-1_mean: 1.19
    apples_agent-1_min: 0
    apples_agent-2_max: 16
    apples_agent-2_mean: 1.97
    apples_agent-2_min: 0
    apples_agent-3_max: 25
    apples_agent-3_mean: 2.58
    apples_agent-3_min: 0
    apples_agent-4_max: 20
    apples_agent-4_mean: 1.34
    apples_agent-4_min: 0
    apples_agent-5_max: 11
    apples_agent-5_mean: 1.89
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 129
    cleaning_beam_agent-0_mean: 66.41
    cleaning_beam_agent-0_min: 29
    cleaning_beam_agent-1_max: 387
    cleaning_beam_agent-1_mean: 242.58
    cleaning_beam_agent-1_min: 128
    cleaning_beam_agent-2_max: 46
    cleaning_beam_agent-2_mean: 13.42
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 205
    cleaning_beam_agent-3_mean: 84.15
    cleaning_beam_agent-3_min: 25
    cleaning_beam_agent-4_max: 115
    cleaning_beam_agent-4_mean: 36.09
    cleaning_beam_agent-4_min: 2
    cleaning_beam_agent-5_max: 23
    cleaning_beam_agent-5_mean: 6.28
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-58-10
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 72.0
  episode_reward_mean: 20.91
  episode_reward_min: -86.0
  episodes_this_iter: 96
  episodes_total: 25536
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11531.375
    learner:
      agent-0:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5405746698379517
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010875960579141974
        model: {}
        policy_loss: -0.0032705003395676613
        total_loss: -0.004193266853690147
        vf_explained_var: -0.0024843811988830566
        vf_loss: 0.28646254539489746
      agent-1:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4818361699581146
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013958720955997705
        model: {}
        policy_loss: -0.002018177416175604
        total_loss: -0.002741643227636814
        vf_explained_var: 0.019765138626098633
        vf_loss: 1.2456543445587158
      agent-2:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46055930852890015
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015881361905485392
        model: {}
        policy_loss: -0.0034353667870163918
        total_loss: -0.004197186790406704
        vf_explained_var: 0.013504013419151306
        vf_loss: 0.4876621961593628
      agent-3:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5786826610565186
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011908693704754114
        model: {}
        policy_loss: -0.0028176107443869114
        total_loss: -0.0038040573708713055
        vf_explained_var: 0.0028406381607055664
        vf_loss: 0.32034897804260254
      agent-4:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6548176407814026
        entropy_coeff: 0.0017600000137463212
        kl: 0.001299816882237792
        model: {}
        policy_loss: -0.0029765900690108538
        total_loss: -0.003971251659095287
        vf_explained_var: 0.0057385265827178955
        vf_loss: 1.5781617164611816
      agent-5:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8675338625907898
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016145927365869284
        model: {}
        policy_loss: -0.0032405718229711056
        total_loss: -0.00474582239985466
        vf_explained_var: 0.0085364431142807
        vf_loss: 0.2161208838224411
    load_time_ms: 13542.017
    num_steps_sampled: 25536000
    num_steps_trained: 25536000
    sample_time_ms: 97336.139
    update_time_ms: 15.142
  iterations_since_restore: 106
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.29310344827586
    ram_util_percent: 11.513793103448274
  pid: 13408
  policy_reward_max:
    agent-0: 13.0
    agent-1: 7.0
    agent-2: 18.0
    agent-3: 15.0
    agent-4: 14.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 3.71
    agent-1: 1.35
    agent-2: 4.57
    agent-3: 3.85
    agent-4: 4.46
    agent-5: 2.97
  policy_reward_min:
    agent-0: -1.0
    agent-1: -48.0
    agent-2: -44.0
    agent-3: 0.0
    agent-4: -47.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.904609755964508
    mean_inference_ms: 12.778420554159789
    mean_processing_ms: 57.798969109454376
  time_since_restore: 13120.789980173111
  time_this_iter_s: 121.7937216758728
  time_total_s: 35631.07322263718
  timestamp: 1637233090
  timesteps_since_restore: 10176000
  timesteps_this_iter: 96000
  timesteps_total: 25536000
  training_iteration: 266
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    266 |          35631.1 | 25536000 |    20.91 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 3.08
    apples_agent-0_min: 0
    apples_agent-1_max: 14
    apples_agent-1_mean: 1.05
    apples_agent-1_min: 0
    apples_agent-2_max: 43
    apples_agent-2_mean: 3.07
    apples_agent-2_min: 0
    apples_agent-3_max: 25
    apples_agent-3_mean: 3.66
    apples_agent-3_min: 0
    apples_agent-4_max: 7
    apples_agent-4_mean: 1.22
    apples_agent-4_min: 0
    apples_agent-5_max: 9
    apples_agent-5_mean: 1.97
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 177
    cleaning_beam_agent-0_mean: 74.23
    cleaning_beam_agent-0_min: 27
    cleaning_beam_agent-1_max: 402
    cleaning_beam_agent-1_mean: 234.12
    cleaning_beam_agent-1_min: 123
    cleaning_beam_agent-2_max: 63
    cleaning_beam_agent-2_mean: 15.83
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 177
    cleaning_beam_agent-3_mean: 78.08
    cleaning_beam_agent-3_min: 23
    cleaning_beam_agent-4_max: 190
    cleaning_beam_agent-4_mean: 39.45
    cleaning_beam_agent-4_min: 5
    cleaning_beam_agent-5_max: 36
    cleaning_beam_agent-5_mean: 6.49
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-00-13
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 73.0
  episode_reward_mean: 26.15
  episode_reward_min: 1.0
  episodes_this_iter: 96
  episodes_total: 25632
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11547.998
    learner:
      agent-0:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5412997007369995
        entropy_coeff: 0.0017600000137463212
        kl: 0.001355650369077921
        model: {}
        policy_loss: -0.0033566029742360115
        total_loss: -0.004271089099347591
        vf_explained_var: 0.0016267448663711548
        vf_loss: 0.38201412558555603
      agent-1:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4771588444709778
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009589113760739565
        model: {}
        policy_loss: -0.0025601759552955627
        total_loss: -0.003386055352166295
        vf_explained_var: 0.02634260058403015
        vf_loss: 0.13917407393455505
      agent-2:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4690108597278595
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010414615971967578
        model: {}
        policy_loss: -0.0033514867536723614
        total_loss: -0.004104925785213709
        vf_explained_var: -0.004707023501396179
        vf_loss: 0.7202082872390747
      agent-3:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5679365396499634
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009566054213792086
        model: {}
        policy_loss: -0.0028565372340381145
        total_loss: -0.0037991225253790617
        vf_explained_var: -0.0010804235935211182
        vf_loss: 0.5698451399803162
      agent-4:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.663095235824585
        entropy_coeff: 0.0017600000137463212
        kl: 0.001505882479250431
        model: {}
        policy_loss: -0.003929009661078453
        total_loss: -0.005051850341260433
        vf_explained_var: 0.013061285018920898
        vf_loss: 0.4420430064201355
      agent-5:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8540663719177246
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020440842490643263
        model: {}
        policy_loss: -0.0029531954787671566
        total_loss: -0.0044279079884290695
        vf_explained_var: 0.0028143078088760376
        vf_loss: 0.2844401001930237
    load_time_ms: 13538.291
    num_steps_sampled: 25632000
    num_steps_trained: 25632000
    sample_time_ms: 97400.237
    update_time_ms: 14.804
  iterations_since_restore: 107
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.777142857142856
    ram_util_percent: 11.488000000000001
  pid: 13408
  policy_reward_max:
    agent-0: 14.0
    agent-1: 10.0
    agent-2: 29.0
    agent-3: 19.0
    agent-4: 14.0
    agent-5: 14.0
  policy_reward_mean:
    agent-0: 4.44
    agent-1: 1.93
    agent-2: 5.82
    agent-3: 5.4
    agent-4: 5.1
    agent-5: 3.46
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.901638374941626
    mean_inference_ms: 12.777308839957957
    mean_processing_ms: 57.796677266446224
  time_since_restore: 13243.779234170914
  time_this_iter_s: 122.98925399780273
  time_total_s: 35754.06247663498
  timestamp: 1637233213
  timesteps_since_restore: 10272000
  timesteps_this_iter: 96000
  timesteps_total: 25632000
  training_iteration: 267
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    267 |          35754.1 | 25632000 |    26.15 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 2.63
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 1.07
    apples_agent-1_min: 0
    apples_agent-2_max: 14
    apples_agent-2_mean: 2.05
    apples_agent-2_min: 0
    apples_agent-3_max: 11
    apples_agent-3_mean: 3.02
    apples_agent-3_min: 0
    apples_agent-4_max: 9
    apples_agent-4_mean: 1.15
    apples_agent-4_min: 0
    apples_agent-5_max: 8
    apples_agent-5_mean: 1.63
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 146
    cleaning_beam_agent-0_mean: 69.32
    cleaning_beam_agent-0_min: 23
    cleaning_beam_agent-1_max: 381
    cleaning_beam_agent-1_mean: 233.39
    cleaning_beam_agent-1_min: 133
    cleaning_beam_agent-2_max: 53
    cleaning_beam_agent-2_mean: 13.32
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 171
    cleaning_beam_agent-3_mean: 74.74
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 129
    cleaning_beam_agent-4_mean: 39.73
    cleaning_beam_agent-4_min: 4
    cleaning_beam_agent-5_max: 27
    cleaning_beam_agent-5_mean: 5.51
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-02-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 51.0
  episode_reward_mean: 22.92
  episode_reward_min: -39.0
  episodes_this_iter: 96
  episodes_total: 25728
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11545.085
    learner:
      agent-0:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5368385314941406
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012189787812530994
        model: {}
        policy_loss: -0.003636789508163929
        total_loss: -0.004553757607936859
        vf_explained_var: -0.006123155355453491
        vf_loss: 0.27868467569351196
      agent-1:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4811098873615265
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009574794676154852
        model: {}
        policy_loss: -0.002791179809719324
        total_loss: -0.0036248485557734966
        vf_explained_var: 0.021682441234588623
        vf_loss: 0.1308465301990509
      agent-2:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45502692461013794
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011419872753322124
        model: {}
        policy_loss: -0.003389413468539715
        total_loss: -0.004146404564380646
        vf_explained_var: -0.0046980977058410645
        vf_loss: 0.4385409951210022
      agent-3:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5722220540046692
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008475123322568834
        model: {}
        policy_loss: -0.0018290886655449867
        total_loss: -0.002664407715201378
        vf_explained_var: 0.00020287930965423584
        vf_loss: 1.7178784608840942
      agent-4:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6570629477500916
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017258585430681705
        model: {}
        policy_loss: -0.0026253703981637955
        total_loss: -0.003601553849875927
        vf_explained_var: 0.01134498417377472
        vf_loss: 1.8024852275848389
      agent-5:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8623103499412537
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024944888427853584
        model: {}
        policy_loss: -0.0027664005756378174
        total_loss: -0.004261413589119911
        vf_explained_var: 0.003306657075881958
        vf_loss: 0.22652652859687805
    load_time_ms: 13515.13
    num_steps_sampled: 25728000
    num_steps_trained: 25728000
    sample_time_ms: 97377.865
    update_time_ms: 14.875
  iterations_since_restore: 108
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.32528735632184
    ram_util_percent: 11.514367816091953
  pid: 13408
  policy_reward_max:
    agent-0: 12.0
    agent-1: 9.0
    agent-2: 14.0
    agent-3: 12.0
    agent-4: 14.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 3.7
    agent-1: 2.02
    agent-2: 5.27
    agent-3: 4.01
    agent-4: 4.72
    agent-5: 3.2
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -48.0
    agent-4: -49.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.897540035511646
    mean_inference_ms: 12.776094119824581
    mean_processing_ms: 57.79441107525917
  time_since_restore: 13366.595476388931
  time_this_iter_s: 122.81624221801758
  time_total_s: 35876.878718853
  timestamp: 1637233336
  timesteps_since_restore: 10368000
  timesteps_this_iter: 96000
  timesteps_total: 25728000
  training_iteration: 268
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    268 |          35876.9 | 25728000 |    22.92 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 2.61
    apples_agent-0_min: 0
    apples_agent-1_max: 34
    apples_agent-1_mean: 1.46
    apples_agent-1_min: 0
    apples_agent-2_max: 21
    apples_agent-2_mean: 2.23
    apples_agent-2_min: 0
    apples_agent-3_max: 18
    apples_agent-3_mean: 3.01
    apples_agent-3_min: 0
    apples_agent-4_max: 87
    apples_agent-4_mean: 2.24
    apples_agent-4_min: 0
    apples_agent-5_max: 18
    apples_agent-5_mean: 1.83
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 164
    cleaning_beam_agent-0_mean: 64.88
    cleaning_beam_agent-0_min: 25
    cleaning_beam_agent-1_max: 347
    cleaning_beam_agent-1_mean: 235.09
    cleaning_beam_agent-1_min: 145
    cleaning_beam_agent-2_max: 79
    cleaning_beam_agent-2_mean: 14.82
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 183
    cleaning_beam_agent-3_mean: 72.05
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 114
    cleaning_beam_agent-4_mean: 41.94
    cleaning_beam_agent-4_min: 6
    cleaning_beam_agent-5_max: 21
    cleaning_beam_agent-5_mean: 5.33
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-04-18
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 53.0
  episode_reward_mean: 24.01
  episode_reward_min: 10.0
  episodes_this_iter: 96
  episodes_total: 25824
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11551.441
    learner:
      agent-0:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5349744558334351
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016594218323007226
        model: {}
        policy_loss: -0.0034903690684586763
        total_loss: -0.004399381577968597
        vf_explained_var: -0.001364096999168396
        vf_loss: 0.3254612386226654
      agent-1:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48285773396492004
        entropy_coeff: 0.0017600000137463212
        kl: 0.001331074396148324
        model: {}
        policy_loss: -0.0025675022043287754
        total_loss: -0.0034019353333860636
        vf_explained_var: 0.03277143836021423
        vf_loss: 0.1539469063282013
      agent-2:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46216732263565063
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012294033076614141
        model: {}
        policy_loss: -0.0034929458051919937
        total_loss: -0.0042597041465342045
        vf_explained_var: -0.000553816556930542
        vf_loss: 0.4665520191192627
      agent-3:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5659175515174866
        entropy_coeff: 0.0017600000137463212
        kl: 0.001169462688267231
        model: {}
        policy_loss: -0.0027060408610850573
        total_loss: -0.0036700209602713585
        vf_explained_var: 0.002336159348487854
        vf_loss: 0.320376455783844
      agent-4:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.654353678226471
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011985113378614187
        model: {}
        policy_loss: -0.003826003521680832
        total_loss: -0.00493175582960248
        vf_explained_var: 0.026200369000434875
        vf_loss: 0.4591151475906372
      agent-5:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8443982005119324
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012192801805213094
        model: {}
        policy_loss: -0.0026217391714453697
        total_loss: -0.0040842508897185326
        vf_explained_var: 0.007558733224868774
        vf_loss: 0.2362549602985382
    load_time_ms: 13527.152
    num_steps_sampled: 25824000
    num_steps_trained: 25824000
    sample_time_ms: 97384.247
    update_time_ms: 14.674
  iterations_since_restore: 109
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.493678160919536
    ram_util_percent: 11.65
  pid: 13408
  policy_reward_max:
    agent-0: 11.0
    agent-1: 9.0
    agent-2: 16.0
    agent-3: 11.0
    agent-4: 14.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 4.25
    agent-1: 2.18
    agent-2: 5.17
    agent-3: 4.02
    agent-4: 5.4
    agent-5: 2.99
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 1.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.893089985902687
    mean_inference_ms: 12.77519578665731
    mean_processing_ms: 57.79126219590648
  time_since_restore: 13488.54862332344
  time_this_iter_s: 121.95314693450928
  time_total_s: 35998.831865787506
  timestamp: 1637233458
  timesteps_since_restore: 10464000
  timesteps_this_iter: 96000
  timesteps_total: 25824000
  training_iteration: 269
  trial_id: '00000'
  
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
== Status ==
Memory usage on this node: 21.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    269 |          35998.8 | 25824000 |    24.01 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 2.27
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.88
    apples_agent-1_min: 0
    apples_agent-2_max: 13
    apples_agent-2_mean: 1.96
    apples_agent-2_min: 0
    apples_agent-3_max: 13
    apples_agent-3_mean: 2.94
    apples_agent-3_min: 0
    apples_agent-4_max: 15
    apples_agent-4_mean: 1.48
    apples_agent-4_min: 0
    apples_agent-5_max: 25
    apples_agent-5_mean: 2.41
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 121
    cleaning_beam_agent-0_mean: 63.21
    cleaning_beam_agent-0_min: 16
    cleaning_beam_agent-1_max: 337
    cleaning_beam_agent-1_mean: 226.31
    cleaning_beam_agent-1_min: 134
    cleaning_beam_agent-2_max: 81
    cleaning_beam_agent-2_mean: 13.83
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 209
    cleaning_beam_agent-3_mean: 73.51
    cleaning_beam_agent-3_min: 27
    cleaning_beam_agent-4_max: 138
    cleaning_beam_agent-4_mean: 38.99
    cleaning_beam_agent-4_min: 8
    cleaning_beam_agent-5_max: 23
    cleaning_beam_agent-5_mean: 6.14
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-06-21
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 84.0
  episode_reward_mean: 23.39
  episode_reward_min: 6.0
  episodes_this_iter: 96
  episodes_total: 25920
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11542.15
    learner:
      agent-0:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5423604249954224
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014790988061577082
        model: {}
        policy_loss: -0.0035685154143720865
        total_loss: -0.0044928486458957195
        vf_explained_var: 0.008853420615196228
        vf_loss: 0.3022538125514984
      agent-1:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4674697518348694
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009222971275448799
        model: {}
        policy_loss: -0.002548639662563801
        total_loss: -0.0033600679598748684
        vf_explained_var: 0.029920637607574463
        vf_loss: 0.11317180097103119
      agent-2:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45103931427001953
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012688137358054519
        model: {}
        policy_loss: -0.003569214139133692
        total_loss: -0.004308214411139488
        vf_explained_var: -0.006968587636947632
        vf_loss: 0.5482712984085083
      agent-3:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5718944072723389
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012125726789236069
        model: {}
        policy_loss: -0.0022594877518713474
        total_loss: -0.0032312129624187946
        vf_explained_var: 0.001103326678276062
        vf_loss: 0.3481026887893677
      agent-4:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6726722717285156
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014606210170313716
        model: {}
        policy_loss: -0.0038098369259387255
        total_loss: -0.004946441389620304
        vf_explained_var: 0.01714855432510376
        vf_loss: 0.47297152876853943
      agent-5:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8586170673370361
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016235524090006948
        model: {}
        policy_loss: -0.002368127927184105
        total_loss: -0.003841064404696226
        vf_explained_var: 0.003496304154396057
        vf_loss: 0.3823101222515106
    load_time_ms: 13513.168
    num_steps_sampled: 25920000
    num_steps_trained: 25920000
    sample_time_ms: 97369.473
    update_time_ms: 14.655
  iterations_since_restore: 110
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.581142857142858
    ram_util_percent: 11.737142857142857
  pid: 13408
  policy_reward_max:
    agent-0: 16.0
    agent-1: 6.0
    agent-2: 21.0
    agent-3: 13.0
    agent-4: 15.0
    agent-5: 21.0
  policy_reward_mean:
    agent-0: 3.69
    agent-1: 1.81
    agent-2: 5.15
    agent-3: 4.22
    agent-4: 5.3
    agent-5: 3.22
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.889756414104017
    mean_inference_ms: 12.77369089888372
    mean_processing_ms: 57.78880083788662
  time_since_restore: 13611.601118803024
  time_this_iter_s: 123.05249547958374
  time_total_s: 36121.88436126709
  timestamp: 1637233581
  timesteps_since_restore: 10560000
  timesteps_this_iter: 96000
  timesteps_total: 25920000
  training_iteration: 270
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    270 |          36121.9 | 25920000 |    23.39 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 2.52
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.99
    apples_agent-1_min: 0
    apples_agent-2_max: 20
    apples_agent-2_mean: 2.5
    apples_agent-2_min: 0
    apples_agent-3_max: 15
    apples_agent-3_mean: 3.1
    apples_agent-3_min: 0
    apples_agent-4_max: 9
    apples_agent-4_mean: 1.5
    apples_agent-4_min: 0
    apples_agent-5_max: 11
    apples_agent-5_mean: 1.71
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 158
    cleaning_beam_agent-0_mean: 64.33
    cleaning_beam_agent-0_min: 18
    cleaning_beam_agent-1_max: 364
    cleaning_beam_agent-1_mean: 216.81
    cleaning_beam_agent-1_min: 132
    cleaning_beam_agent-2_max: 78
    cleaning_beam_agent-2_mean: 13.99
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 209
    cleaning_beam_agent-3_mean: 79.15
    cleaning_beam_agent-3_min: 32
    cleaning_beam_agent-4_max: 108
    cleaning_beam_agent-4_mean: 37.47
    cleaning_beam_agent-4_min: 6
    cleaning_beam_agent-5_max: 27
    cleaning_beam_agent-5_mean: 7.2
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-08-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 57.0
  episode_reward_mean: 22.22
  episode_reward_min: -70.0
  episodes_this_iter: 96
  episodes_total: 26016
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11544.806
    learner:
      agent-0:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5357261300086975
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015131575055420399
        model: {}
        policy_loss: -0.0035085855051875114
        total_loss: -0.004410902503877878
        vf_explained_var: 0.0066811442375183105
        vf_loss: 0.4056161344051361
      agent-1:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46808919310569763
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008414683979935944
        model: {}
        policy_loss: -0.002617890015244484
        total_loss: -0.0034286596346646547
        vf_explained_var: 0.026172474026679993
        vf_loss: 0.1306581348180771
      agent-2:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4506411850452423
        entropy_coeff: 0.0017600000137463212
        kl: 0.001274513895623386
        model: {}
        policy_loss: -0.003469204530119896
        total_loss: -0.0042131198570132256
        vf_explained_var: -0.003706797957420349
        vf_loss: 0.49208492040634155
      agent-3:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5764644145965576
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011692417319864035
        model: {}
        policy_loss: -0.0023379381746053696
        total_loss: -0.003308354876935482
        vf_explained_var: 0.0030489861965179443
        vf_loss: 0.44161394238471985
      agent-4:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6669838428497314
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016657105879858136
        model: {}
        policy_loss: -0.0038158390671014786
        total_loss: -0.0049438923597335815
        vf_explained_var: 0.01384669542312622
        vf_loss: 0.45835864543914795
      agent-5:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8659084439277649
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019501030910760164
        model: {}
        policy_loss: -0.002443872392177582
        total_loss: -0.003944889176636934
        vf_explained_var: 0.007314413785934448
        vf_loss: 0.22981955111026764
    load_time_ms: 13512.442
    num_steps_sampled: 26016000
    num_steps_trained: 26016000
    sample_time_ms: 97298.75
    update_time_ms: 14.617
  iterations_since_restore: 111
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.621264367816092
    ram_util_percent: 11.797701149425288
  pid: 13408
  policy_reward_max:
    agent-0: 13.0
    agent-1: 7.0
    agent-2: 18.0
    agent-3: 22.0
    agent-4: 13.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 3.63
    agent-1: 1.89
    agent-2: 4.96
    agent-3: 3.88
    agent-4: 5.42
    agent-5: 2.44
  policy_reward_min:
    agent-0: -47.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -43.0
    agent-4: 1.0
    agent-5: -49.0
  sampler_perf:
    mean_env_wait_ms: 22.886086732228065
    mean_inference_ms: 12.772575266004624
    mean_processing_ms: 57.78670055289781
  time_since_restore: 13733.486298322678
  time_this_iter_s: 121.88517951965332
  time_total_s: 36243.76954078674
  timestamp: 1637233704
  timesteps_since_restore: 10656000
  timesteps_this_iter: 96000
  timesteps_total: 26016000
  training_iteration: 271
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    271 |          36243.8 | 26016000 |    22.22 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 8
    apples_agent-0_mean: 2.32
    apples_agent-0_min: 0
    apples_agent-1_max: 8
    apples_agent-1_mean: 1.16
    apples_agent-1_min: 0
    apples_agent-2_max: 24
    apples_agent-2_mean: 1.96
    apples_agent-2_min: 0
    apples_agent-3_max: 11
    apples_agent-3_mean: 3.01
    apples_agent-3_min: 0
    apples_agent-4_max: 22
    apples_agent-4_mean: 1.67
    apples_agent-4_min: 0
    apples_agent-5_max: 12
    apples_agent-5_mean: 1.91
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 115
    cleaning_beam_agent-0_mean: 62.29
    cleaning_beam_agent-0_min: 19
    cleaning_beam_agent-1_max: 346
    cleaning_beam_agent-1_mean: 228.04
    cleaning_beam_agent-1_min: 128
    cleaning_beam_agent-2_max: 43
    cleaning_beam_agent-2_mean: 14.13
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 213
    cleaning_beam_agent-3_mean: 73.86
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 154
    cleaning_beam_agent-4_mean: 38.66
    cleaning_beam_agent-4_min: 8
    cleaning_beam_agent-5_max: 27
    cleaning_beam_agent-5_mean: 6.33
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-10-27
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 47.0
  episode_reward_mean: 23.7
  episode_reward_min: 8.0
  episodes_this_iter: 96
  episodes_total: 26112
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11534.061
    learner:
      agent-0:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.537918746471405
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015529320808127522
        model: {}
        policy_loss: -0.0036312625743448734
        total_loss: -0.004549816716462374
        vf_explained_var: -0.0037868916988372803
        vf_loss: 0.2818244695663452
      agent-1:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4710160493850708
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013520848006010056
        model: {}
        policy_loss: -0.002459141192957759
        total_loss: -0.0032759979367256165
        vf_explained_var: 0.013768479228019714
        vf_loss: 0.12134066969156265
      agent-2:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46374666690826416
        entropy_coeff: 0.0017600000137463212
        kl: 0.001161108841188252
        model: {}
        policy_loss: -0.0032139604445546865
        total_loss: -0.003978390712291002
        vf_explained_var: -0.0005806684494018555
        vf_loss: 0.517659604549408
      agent-3:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.566380500793457
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015057160053402185
        model: {}
        policy_loss: -0.00284612737596035
        total_loss: -0.003809258807450533
        vf_explained_var: -0.0006408244371414185
        vf_loss: 0.33698970079421997
      agent-4:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6568902730941772
        entropy_coeff: 0.0017600000137463212
        kl: 0.001700585475191474
        model: {}
        policy_loss: -0.0038402481004595757
        total_loss: -0.004946878645569086
        vf_explained_var: 0.019234061241149902
        vf_loss: 0.49498438835144043
      agent-5:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.840878427028656
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018566426588222384
        model: {}
        policy_loss: -0.002904587658122182
        total_loss: -0.004357568453997374
        vf_explained_var: 0.0027644187211990356
        vf_loss: 0.2696212828159332
    load_time_ms: 13486.546
    num_steps_sampled: 26112000
    num_steps_trained: 26112000
    sample_time_ms: 97391.053
    update_time_ms: 14.928
  iterations_since_restore: 112
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.131818181818186
    ram_util_percent: 11.361363636363638
  pid: 13408
  policy_reward_max:
    agent-0: 12.0
    agent-1: 7.0
    agent-2: 14.0
    agent-3: 11.0
    agent-4: 15.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 3.72
    agent-1: 1.8
    agent-2: 5.01
    agent-3: 4.42
    agent-4: 5.53
    agent-5: 3.22
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 1.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.881550944024198
    mean_inference_ms: 12.771285199205735
    mean_processing_ms: 57.78618633213422
  time_since_restore: 13856.465169668198
  time_this_iter_s: 122.97887134552002
  time_total_s: 36366.74841213226
  timestamp: 1637233827
  timesteps_since_restore: 10752000
  timesteps_this_iter: 96000
  timesteps_total: 26112000
  training_iteration: 272
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    272 |          36366.7 | 26112000 |     23.7 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 2.75
    apples_agent-0_min: 0
    apples_agent-1_max: 14
    apples_agent-1_mean: 1.38
    apples_agent-1_min: 0
    apples_agent-2_max: 22
    apples_agent-2_mean: 2.48
    apples_agent-2_min: 0
    apples_agent-3_max: 22
    apples_agent-3_mean: 2.94
    apples_agent-3_min: 0
    apples_agent-4_max: 11
    apples_agent-4_mean: 1.46
    apples_agent-4_min: 0
    apples_agent-5_max: 9
    apples_agent-5_mean: 1.67
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 135
    cleaning_beam_agent-0_mean: 60.7
    cleaning_beam_agent-0_min: 23
    cleaning_beam_agent-1_max: 438
    cleaning_beam_agent-1_mean: 229.78
    cleaning_beam_agent-1_min: 125
    cleaning_beam_agent-2_max: 63
    cleaning_beam_agent-2_mean: 14.87
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 174
    cleaning_beam_agent-3_mean: 80.06
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 154
    cleaning_beam_agent-4_mean: 40.38
    cleaning_beam_agent-4_min: 8
    cleaning_beam_agent-5_max: 37
    cleaning_beam_agent-5_mean: 7.0
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-12-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 58.0
  episode_reward_mean: 21.75
  episode_reward_min: -92.0
  episodes_this_iter: 96
  episodes_total: 26208
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11515.276
    learner:
      agent-0:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5209387540817261
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010459101758897305
        model: {}
        policy_loss: -0.0024848312605172396
        total_loss: -0.0032410500571131706
        vf_explained_var: -0.001658603549003601
        vf_loss: 1.6063127517700195
      agent-1:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4694777727127075
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008140149293467402
        model: {}
        policy_loss: -0.0018684653332456946
        total_loss: -0.0025481258053332567
        vf_explained_var: 0.0072363317012786865
        vf_loss: 1.4662020206451416
      agent-2:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45051077008247375
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012153778225183487
        model: {}
        policy_loss: -0.003469069954007864
        total_loss: -0.004220737610012293
        vf_explained_var: 0.004671469330787659
        vf_loss: 0.4123159646987915
      agent-3:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5810208320617676
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009089367231354117
        model: {}
        policy_loss: -0.0025720896665006876
        total_loss: -0.00356306042522192
        vf_explained_var: -0.0006863772869110107
        vf_loss: 0.3162289261817932
      agent-4:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6604764461517334
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015490257646888494
        model: {}
        policy_loss: -0.00386759452521801
        total_loss: -0.004990735091269016
        vf_explained_var: 0.003303810954093933
        vf_loss: 0.3929907977581024
      agent-5:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8551254272460938
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018997567240148783
        model: {}
        policy_loss: -0.0020476635545492172
        total_loss: -0.003395052393898368
        vf_explained_var: 0.002409607172012329
        vf_loss: 1.5763156414031982
    load_time_ms: 13485.8
    num_steps_sampled: 26208000
    num_steps_trained: 26208000
    sample_time_ms: 97309.007
    update_time_ms: 14.969
  iterations_since_restore: 113
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.428901734104052
    ram_util_percent: 11.437572254335262
  pid: 13408
  policy_reward_max:
    agent-0: 11.0
    agent-1: 9.0
    agent-2: 16.0
    agent-3: 18.0
    agent-4: 13.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 3.54
    agent-1: 1.49
    agent-2: 4.94
    agent-3: 4.23
    agent-4: 5.05
    agent-5: 2.5
  policy_reward_min:
    agent-0: -49.0
    agent-1: -48.0
    agent-2: -1.0
    agent-3: 0.0
    agent-4: 1.0
    agent-5: -49.0
  sampler_perf:
    mean_env_wait_ms: 22.877823056026156
    mean_inference_ms: 12.770005337415327
    mean_processing_ms: 57.78232451873553
  time_since_restore: 13978.015213489532
  time_this_iter_s: 121.55004382133484
  time_total_s: 36488.2984559536
  timestamp: 1637233949
  timesteps_since_restore: 10848000
  timesteps_this_iter: 96000
  timesteps_total: 26208000
  training_iteration: 273
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    273 |          36488.3 | 26208000 |    21.75 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 25
    apples_agent-0_mean: 2.85
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.89
    apples_agent-1_min: 0
    apples_agent-2_max: 43
    apples_agent-2_mean: 3.14
    apples_agent-2_min: 0
    apples_agent-3_max: 15
    apples_agent-3_mean: 2.96
    apples_agent-3_min: 0
    apples_agent-4_max: 11
    apples_agent-4_mean: 1.49
    apples_agent-4_min: 0
    apples_agent-5_max: 35
    apples_agent-5_mean: 2.28
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 131
    cleaning_beam_agent-0_mean: 60.48
    cleaning_beam_agent-0_min: 26
    cleaning_beam_agent-1_max: 369
    cleaning_beam_agent-1_mean: 220.11
    cleaning_beam_agent-1_min: 91
    cleaning_beam_agent-2_max: 64
    cleaning_beam_agent-2_mean: 14.61
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 209
    cleaning_beam_agent-3_mean: 68.16
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 97
    cleaning_beam_agent-4_mean: 36.63
    cleaning_beam_agent-4_min: 6
    cleaning_beam_agent-5_max: 34
    cleaning_beam_agent-5_mean: 6.08
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-14-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 47.0
  episode_reward_mean: 20.85
  episode_reward_min: -35.0
  episodes_this_iter: 96
  episodes_total: 26304
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11529.807
    learner:
      agent-0:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5321948528289795
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011945778969675303
        model: {}
        policy_loss: -0.003094637766480446
        total_loss: -0.004000079818069935
        vf_explained_var: 0.0030834078788757324
        vf_loss: 0.31219056248664856
      agent-1:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4633827805519104
        entropy_coeff: 0.0017600000137463212
        kl: 0.000935267424210906
        model: {}
        policy_loss: -0.0024433457292616367
        total_loss: -0.003246130421757698
        vf_explained_var: 0.027137726545333862
        vf_loss: 0.12767618894577026
      agent-2:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4523283839225769
        entropy_coeff: 0.0017600000137463212
        kl: 0.001251657959073782
        model: {}
        policy_loss: -0.0036506843753159046
        total_loss: -0.004411731846630573
        vf_explained_var: 0.006745293736457825
        vf_loss: 0.35047224164009094
      agent-3:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5700885057449341
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011212823446840048
        model: {}
        policy_loss: -0.002423542086035013
        total_loss: -0.0033941585570573807
        vf_explained_var: 0.0025059282779693604
        vf_loss: 0.32742851972579956
      agent-4:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6596320271492004
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015324968844652176
        model: {}
        policy_loss: -0.0038532009348273277
        total_loss: -0.004966265056282282
        vf_explained_var: 0.011913076043128967
        vf_loss: 0.47888895869255066
      agent-5:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.86241215467453
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014825686812400818
        model: {}
        policy_loss: -0.0021501812152564526
        total_loss: -0.003515997901558876
        vf_explained_var: 0.0012525618076324463
        vf_loss: 1.520304799079895
    load_time_ms: 13477.073
    num_steps_sampled: 26304000
    num_steps_trained: 26304000
    sample_time_ms: 97316.167
    update_time_ms: 15.419
  iterations_since_restore: 114
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.296551724137935
    ram_util_percent: 11.345977011494254
  pid: 13408
  policy_reward_max:
    agent-0: 10.0
    agent-1: 9.0
    agent-2: 16.0
    agent-3: 11.0
    agent-4: 17.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 2.97
    agent-1: 1.85
    agent-2: 4.91
    agent-3: 3.44
    agent-4: 4.88
    agent-5: 2.8
  policy_reward_min:
    agent-0: -46.0
    agent-1: 0.0
    agent-2: 1.0
    agent-3: -41.0
    agent-4: -46.0
    agent-5: -48.0
  sampler_perf:
    mean_env_wait_ms: 22.873567677190067
    mean_inference_ms: 12.768532949662909
    mean_processing_ms: 57.780251954466166
  time_since_restore: 14100.408982992172
  time_this_iter_s: 122.39376950263977
  time_total_s: 36610.69222545624
  timestamp: 1637234071
  timesteps_since_restore: 10944000
  timesteps_this_iter: 96000
  timesteps_total: 26304000
  training_iteration: 274
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    274 |          36610.7 | 26304000 |    20.85 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 2.58
    apples_agent-0_min: 0
    apples_agent-1_max: 60
    apples_agent-1_mean: 1.81
    apples_agent-1_min: 0
    apples_agent-2_max: 10
    apples_agent-2_mean: 1.9
    apples_agent-2_min: 0
    apples_agent-3_max: 19
    apples_agent-3_mean: 3.38
    apples_agent-3_min: 0
    apples_agent-4_max: 21
    apples_agent-4_mean: 1.58
    apples_agent-4_min: 0
    apples_agent-5_max: 43
    apples_agent-5_mean: 2.29
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 131
    cleaning_beam_agent-0_mean: 64.97
    cleaning_beam_agent-0_min: 19
    cleaning_beam_agent-1_max: 341
    cleaning_beam_agent-1_mean: 234.79
    cleaning_beam_agent-1_min: 145
    cleaning_beam_agent-2_max: 45
    cleaning_beam_agent-2_mean: 13.06
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 177
    cleaning_beam_agent-3_mean: 66.22
    cleaning_beam_agent-3_min: 24
    cleaning_beam_agent-4_max: 136
    cleaning_beam_agent-4_mean: 39.26
    cleaning_beam_agent-4_min: 6
    cleaning_beam_agent-5_max: 27
    cleaning_beam_agent-5_mean: 6.26
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-16-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 58.0
  episode_reward_mean: 23.46
  episode_reward_min: -83.0
  episodes_this_iter: 96
  episodes_total: 26400
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11525.927
    learner:
      agent-0:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5289973020553589
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015577562153339386
        model: {}
        policy_loss: -0.00332577433437109
        total_loss: -0.00421914691105485
        vf_explained_var: 3.510713577270508e-05
        vf_loss: 0.37664055824279785
      agent-1:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4647057056427002
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014424840919673443
        model: {}
        policy_loss: -0.002825578674674034
        total_loss: -0.0036318486090749502
        vf_explained_var: 0.012151777744293213
        vf_loss: 0.11610834300518036
      agent-2:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45384013652801514
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016856536967679858
        model: {}
        policy_loss: -0.004215386230498552
        total_loss: -0.004970094189047813
        vf_explained_var: -0.011483997106552124
        vf_loss: 0.44052931666374207
      agent-3:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5842477083206177
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011270811082795262
        model: {}
        policy_loss: -0.0025646062567830086
        total_loss: -0.0035536233335733414
        vf_explained_var: 0.0026827752590179443
        vf_loss: 0.392625629901886
      agent-4:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6594293713569641
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012778703821823
        model: {}
        policy_loss: -0.003746259957551956
        total_loss: -0.004854495637118816
        vf_explained_var: 0.019858628511428833
        vf_loss: 0.5235834717750549
      agent-5:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8451485633850098
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017315322766080499
        model: {}
        policy_loss: -0.002659580670297146
        total_loss: -0.004117428325116634
        vf_explained_var: 0.008652791380882263
        vf_loss: 0.296115517616272
    load_time_ms: 13498.235
    num_steps_sampled: 26400000
    num_steps_trained: 26400000
    sample_time_ms: 97275.1
    update_time_ms: 15.205
  iterations_since_restore: 115
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.40114285714286
    ram_util_percent: 11.498285714285714
  pid: 13408
  policy_reward_max:
    agent-0: 11.0
    agent-1: 6.0
    agent-2: 13.0
    agent-3: 11.0
    agent-4: 18.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 3.61
    agent-1: 1.73
    agent-2: 5.14
    agent-3: 3.83
    agent-4: 6.02
    agent-5: 3.13
  policy_reward_min:
    agent-0: -46.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -47.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.86978033929561
    mean_inference_ms: 12.7688032713284
    mean_processing_ms: 57.78049830542946
  time_since_restore: 14223.057971000671
  time_this_iter_s: 122.64898800849915
  time_total_s: 36733.34121346474
  timestamp: 1637234194
  timesteps_since_restore: 11040000
  timesteps_this_iter: 96000
  timesteps_total: 26400000
  training_iteration: 275
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    275 |          36733.3 | 26400000 |    23.46 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 25
    apples_agent-0_mean: 2.87
    apples_agent-0_min: 0
    apples_agent-1_max: 11
    apples_agent-1_mean: 1.26
    apples_agent-1_min: 0
    apples_agent-2_max: 26
    apples_agent-2_mean: 2.02
    apples_agent-2_min: 0
    apples_agent-3_max: 9
    apples_agent-3_mean: 2.6
    apples_agent-3_min: 0
    apples_agent-4_max: 43
    apples_agent-4_mean: 1.88
    apples_agent-4_min: 0
    apples_agent-5_max: 9
    apples_agent-5_mean: 1.8
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 131
    cleaning_beam_agent-0_mean: 67.53
    cleaning_beam_agent-0_min: 26
    cleaning_beam_agent-1_max: 486
    cleaning_beam_agent-1_mean: 238.81
    cleaning_beam_agent-1_min: 142
    cleaning_beam_agent-2_max: 69
    cleaning_beam_agent-2_mean: 15.18
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 185
    cleaning_beam_agent-3_mean: 69.81
    cleaning_beam_agent-3_min: 24
    cleaning_beam_agent-4_max: 120
    cleaning_beam_agent-4_mean: 40.22
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 29
    cleaning_beam_agent-5_mean: 6.2
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-18-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 53.0
  episode_reward_mean: 22.33
  episode_reward_min: -37.0
  episodes_this_iter: 96
  episodes_total: 26496
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11532.458
    learner:
      agent-0:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5380456447601318
        entropy_coeff: 0.0017600000137463212
        kl: 0.001207454246468842
        model: {}
        policy_loss: -0.003357641864567995
        total_loss: -0.004275220911949873
        vf_explained_var: 0.005983114242553711
        vf_loss: 0.29382893443107605
      agent-1:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46892449259757996
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010160125093534589
        model: {}
        policy_loss: -0.002390110632404685
        total_loss: -0.003202032530680299
        vf_explained_var: 0.01442447304725647
        vf_loss: 0.13385926187038422
      agent-2:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4513018727302551
        entropy_coeff: 0.0017600000137463212
        kl: 0.001494270982220769
        model: {}
        policy_loss: -0.0036857053637504578
        total_loss: -0.004436841234564781
        vf_explained_var: 0.004050403833389282
        vf_loss: 0.43156030774116516
      agent-3:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5870884656906128
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012176244053989649
        model: {}
        policy_loss: -0.0024954439140856266
        total_loss: -0.0035001137293875217
        vf_explained_var: -0.0024518072605133057
        vf_loss: 0.2860843539237976
      agent-4:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6638771295547485
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015239166095852852
        model: {}
        policy_loss: -0.003825319465249777
        total_loss: -0.004950718022882938
        vf_explained_var: 0.018764689564704895
        vf_loss: 0.43025678396224976
      agent-5:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8402982354164124
        entropy_coeff: 0.0017600000137463212
        kl: 0.002572577679529786
        model: {}
        policy_loss: -0.0020090192556381226
        total_loss: -0.003328505903482437
        vf_explained_var: -0.00011800229549407959
        vf_loss: 1.5943715572357178
    load_time_ms: 13501.878
    num_steps_sampled: 26496000
    num_steps_trained: 26496000
    sample_time_ms: 97366.743
    update_time_ms: 15.746
  iterations_since_restore: 116
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.22
    ram_util_percent: 11.431999999999999
  pid: 13408
  policy_reward_max:
    agent-0: 11.0
    agent-1: 10.0
    agent-2: 15.0
    agent-3: 15.0
    agent-4: 13.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 3.74
    agent-1: 1.73
    agent-2: 4.95
    agent-3: 3.92
    agent-4: 5.1
    agent-5: 2.89
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -49.0
  sampler_perf:
    mean_env_wait_ms: 22.86625435302866
    mean_inference_ms: 12.767829698351372
    mean_processing_ms: 57.77926569424238
  time_since_restore: 14345.886754512787
  time_this_iter_s: 122.82878351211548
  time_total_s: 36856.16999697685
  timestamp: 1637234317
  timesteps_since_restore: 11136000
  timesteps_this_iter: 96000
  timesteps_total: 26496000
  training_iteration: 276
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    276 |          36856.2 | 26496000 |    22.33 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 2.76
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 0.98
    apples_agent-1_min: 0
    apples_agent-2_max: 24
    apples_agent-2_mean: 2.32
    apples_agent-2_min: 0
    apples_agent-3_max: 10
    apples_agent-3_mean: 2.96
    apples_agent-3_min: 0
    apples_agent-4_max: 68
    apples_agent-4_mean: 2.13
    apples_agent-4_min: 0
    apples_agent-5_max: 28
    apples_agent-5_mean: 2.04
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 185
    cleaning_beam_agent-0_mean: 68.42
    cleaning_beam_agent-0_min: 12
    cleaning_beam_agent-1_max: 358
    cleaning_beam_agent-1_mean: 224.68
    cleaning_beam_agent-1_min: 121
    cleaning_beam_agent-2_max: 90
    cleaning_beam_agent-2_mean: 15.43
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 156
    cleaning_beam_agent-3_mean: 67.08
    cleaning_beam_agent-3_min: 25
    cleaning_beam_agent-4_max: 110
    cleaning_beam_agent-4_mean: 41.67
    cleaning_beam_agent-4_min: 7
    cleaning_beam_agent-5_max: 25
    cleaning_beam_agent-5_mean: 6.38
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-20-39
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 55.0
  episode_reward_mean: 21.54
  episode_reward_min: -79.0
  episodes_this_iter: 96
  episodes_total: 26592
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11510.579
    learner:
      agent-0:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5390747785568237
        entropy_coeff: 0.0017600000137463212
        kl: 0.001165245659649372
        model: {}
        policy_loss: -0.0031688520684838295
        total_loss: -0.004080030135810375
        vf_explained_var: -0.0013299137353897095
        vf_loss: 0.3759395182132721
      agent-1:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4630126357078552
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010174004128202796
        model: {}
        policy_loss: -0.0028113387525081635
        total_loss: -0.003615342080593109
        vf_explained_var: 0.006961405277252197
        vf_loss: 0.10897514969110489
      agent-2:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44664502143859863
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012168311513960361
        model: {}
        policy_loss: -0.001560758799314499
        total_loss: -0.002173270098865032
        vf_explained_var: 0.002688884735107422
        vf_loss: 1.7358434200286865
      agent-3:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5749945640563965
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015327725559473038
        model: {}
        policy_loss: -0.002624617423862219
        total_loss: -0.003601798787713051
        vf_explained_var: 0.005065247416496277
        vf_loss: 0.348091721534729
      agent-4:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6538141369819641
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016160937957465649
        model: {}
        policy_loss: -0.004016010090708733
        total_loss: -0.004983283579349518
        vf_explained_var: 0.002688363194465637
        vf_loss: 1.8344004154205322
      agent-5:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.847726047039032
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015037487028166652
        model: {}
        policy_loss: -0.0013880208134651184
        total_loss: -0.002719903364777565
        vf_explained_var: 0.000865638256072998
        vf_loss: 1.6011282205581665
    load_time_ms: 13510.791
    num_steps_sampled: 26592000
    num_steps_trained: 26592000
    sample_time_ms: 97242.575
    update_time_ms: 15.737
  iterations_since_restore: 117
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.667630057803468
    ram_util_percent: 11.50693641618497
  pid: 13408
  policy_reward_max:
    agent-0: 16.0
    agent-1: 7.0
    agent-2: 17.0
    agent-3: 11.0
    agent-4: 15.0
    agent-5: 14.0
  policy_reward_mean:
    agent-0: 3.99
    agent-1: 1.74
    agent-2: 4.21
    agent-3: 3.81
    agent-4: 4.9
    agent-5: 2.89
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -48.0
    agent-3: 0.0
    agent-4: -47.0
    agent-5: -48.0
  sampler_perf:
    mean_env_wait_ms: 22.862410765798028
    mean_inference_ms: 12.766696508491727
    mean_processing_ms: 57.77607028704078
  time_since_restore: 14467.457269191742
  time_this_iter_s: 121.57051467895508
  time_total_s: 36977.74051165581
  timestamp: 1637234439
  timesteps_since_restore: 11232000
  timesteps_this_iter: 96000
  timesteps_total: 26592000
  training_iteration: 277
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    277 |          36977.7 | 26592000 |    21.54 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 2.81
    apples_agent-0_min: 0
    apples_agent-1_max: 8
    apples_agent-1_mean: 1.26
    apples_agent-1_min: 0
    apples_agent-2_max: 40
    apples_agent-2_mean: 2.98
    apples_agent-2_min: 0
    apples_agent-3_max: 13
    apples_agent-3_mean: 3.22
    apples_agent-3_min: 0
    apples_agent-4_max: 17
    apples_agent-4_mean: 1.64
    apples_agent-4_min: 0
    apples_agent-5_max: 8
    apples_agent-5_mean: 1.53
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 153
    cleaning_beam_agent-0_mean: 73.95
    cleaning_beam_agent-0_min: 27
    cleaning_beam_agent-1_max: 383
    cleaning_beam_agent-1_mean: 230.12
    cleaning_beam_agent-1_min: 121
    cleaning_beam_agent-2_max: 80
    cleaning_beam_agent-2_mean: 15.64
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 176
    cleaning_beam_agent-3_mean: 70.35
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 196
    cleaning_beam_agent-4_mean: 38.59
    cleaning_beam_agent-4_min: 4
    cleaning_beam_agent-5_max: 24
    cleaning_beam_agent-5_mean: 6.25
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 26
    fire_beam_agent-5_mean: 0.38
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-22-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 67.0
  episode_reward_mean: 18.52
  episode_reward_min: -344.0
  episodes_this_iter: 96
  episodes_total: 26688
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11506.834
    learner:
      agent-0:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5325518846511841
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012116923462599516
        model: {}
        policy_loss: -0.0031046858057379723
        total_loss: -0.004010359290987253
        vf_explained_var: -0.0002375096082687378
        vf_loss: 0.3161734938621521
      agent-1:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4660555422306061
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011748697143048048
        model: {}
        policy_loss: -0.0020613358356058598
        total_loss: 0.001852736808359623
        vf_explained_var: 0.0035118907690048218
        vf_loss: 47.34331130981445
      agent-2:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43965673446655273
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021601628977805376
        model: {}
        policy_loss: -0.002448475919663906
        total_loss: -0.0019093696027994156
        vf_explained_var: -0.0005535483360290527
        vf_loss: 13.129030227661133
      agent-3:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5856447219848633
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009811241179704666
        model: {}
        policy_loss: -0.0026857415214180946
        total_loss: -0.0036807390861213207
        vf_explained_var: 0.0016590803861618042
        vf_loss: 0.35739055275917053
      agent-4:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6635845303535461
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011961190029978752
        model: {}
        policy_loss: -0.0038153708446770906
        total_loss: -0.004941228777170181
        vf_explained_var: 0.012874051928520203
        vf_loss: 0.4204951822757721
      agent-5:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8324509859085083
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013280333951115608
        model: {}
        policy_loss: -0.0029004039242863655
        total_loss: -0.004302702844142914
        vf_explained_var: 0.017090320587158203
        vf_loss: 0.628119945526123
    load_time_ms: 13549.271
    num_steps_sampled: 26688000
    num_steps_trained: 26688000
    sample_time_ms: 97230.497
    update_time_ms: 16.399
  iterations_since_restore: 118
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.63542857142857
    ram_util_percent: 11.444571428571427
  pid: 13408
  policy_reward_max:
    agent-0: 13.0
    agent-1: 10.0
    agent-2: 18.0
    agent-3: 10.0
    agent-4: 18.0
    agent-5: 19.0
  policy_reward_mean:
    agent-0: 4.17
    agent-1: -1.78
    agent-2: 3.51
    agent-3: 4.31
    agent-4: 5.51
    agent-5: 2.8
  policy_reward_min:
    agent-0: 0.0
    agent-1: -347.0
    agent-2: -147.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -26.0
  sampler_perf:
    mean_env_wait_ms: 22.859428665966757
    mean_inference_ms: 12.76580836604662
    mean_processing_ms: 57.77484943993175
  time_since_restore: 14590.567646503448
  time_this_iter_s: 123.11037731170654
  time_total_s: 37100.850888967514
  timestamp: 1637234562
  timesteps_since_restore: 11328000
  timesteps_this_iter: 96000
  timesteps_total: 26688000
  training_iteration: 278
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    278 |          37100.9 | 26688000 |    18.52 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 3.02
    apples_agent-0_min: 0
    apples_agent-1_max: 22
    apples_agent-1_mean: 1.41
    apples_agent-1_min: 0
    apples_agent-2_max: 12
    apples_agent-2_mean: 2.1
    apples_agent-2_min: 0
    apples_agent-3_max: 17
    apples_agent-3_mean: 3.1
    apples_agent-3_min: 0
    apples_agent-4_max: 9
    apples_agent-4_mean: 1.07
    apples_agent-4_min: 0
    apples_agent-5_max: 9
    apples_agent-5_mean: 2.09
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 166
    cleaning_beam_agent-0_mean: 73.1
    cleaning_beam_agent-0_min: 20
    cleaning_beam_agent-1_max: 348
    cleaning_beam_agent-1_mean: 223.83
    cleaning_beam_agent-1_min: 136
    cleaning_beam_agent-2_max: 42
    cleaning_beam_agent-2_mean: 10.81
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 160
    cleaning_beam_agent-3_mean: 72.83
    cleaning_beam_agent-3_min: 20
    cleaning_beam_agent-4_max: 196
    cleaning_beam_agent-4_mean: 42.29
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 23
    cleaning_beam_agent-5_mean: 5.73
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-24-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 61.0
  episode_reward_mean: 25.04
  episode_reward_min: 9.0
  episodes_this_iter: 96
  episodes_total: 26784
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11494.457
    learner:
      agent-0:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5343538522720337
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010291336802765727
        model: {}
        policy_loss: -0.0033042500726878643
        total_loss: -0.004210162907838821
        vf_explained_var: -0.006089359521865845
        vf_loss: 0.345508337020874
      agent-1:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4631360173225403
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008727172389626503
        model: {}
        policy_loss: -0.0027108071371912956
        total_loss: -0.003515278920531273
        vf_explained_var: 0.024630993604660034
        vf_loss: 0.10647838562726974
      agent-2:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43096935749053955
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015719890361651778
        model: {}
        policy_loss: -0.0032864429522305727
        total_loss: -0.004001629538834095
        vf_explained_var: 0.0029765963554382324
        vf_loss: 0.4332183599472046
      agent-3:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.587967574596405
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011450846213847399
        model: {}
        policy_loss: -0.0026679448783397675
        total_loss: -0.003665839321911335
        vf_explained_var: -0.006205469369888306
        vf_loss: 0.36929088830947876
      agent-4:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6604599952697754
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015886934706941247
        model: {}
        policy_loss: -0.0039033740758895874
        total_loss: -0.005014980677515268
        vf_explained_var: 0.012814700603485107
        vf_loss: 0.5080246925354004
      agent-5:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8154722452163696
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015482651069760323
        model: {}
        policy_loss: -0.002663885708898306
        total_loss: -0.004069117363542318
        vf_explained_var: 0.01879076659679413
        vf_loss: 0.3000318706035614
    load_time_ms: 13522.304
    num_steps_sampled: 26784000
    num_steps_trained: 26784000
    sample_time_ms: 97360.135
    update_time_ms: 16.536
  iterations_since_restore: 119
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.42
    ram_util_percent: 11.516571428571424
  pid: 13408
  policy_reward_max:
    agent-0: 11.0
    agent-1: 6.0
    agent-2: 14.0
    agent-3: 13.0
    agent-4: 16.0
    agent-5: 14.0
  policy_reward_mean:
    agent-0: 4.4
    agent-1: 1.74
    agent-2: 5.32
    agent-3: 4.35
    agent-4: 5.79
    agent-5: 3.44
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.85636558593678
    mean_inference_ms: 12.76467519523795
    mean_processing_ms: 57.77425887551973
  time_since_restore: 14713.374794960022
  time_this_iter_s: 122.80714845657349
  time_total_s: 37223.65803742409
  timestamp: 1637234685
  timesteps_since_restore: 11424000
  timesteps_this_iter: 96000
  timesteps_total: 26784000
  training_iteration: 279
  trial_id: '00000'
  
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
== Status ==
Memory usage on this node: 21.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    279 |          37223.7 | 26784000 |    25.04 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 39
    apples_agent-0_mean: 3.59
    apples_agent-0_min: 0
    apples_agent-1_max: 15
    apples_agent-1_mean: 1.31
    apples_agent-1_min: 0
    apples_agent-2_max: 30
    apples_agent-2_mean: 2.39
    apples_agent-2_min: 0
    apples_agent-3_max: 9
    apples_agent-3_mean: 3.19
    apples_agent-3_min: 0
    apples_agent-4_max: 23
    apples_agent-4_mean: 1.55
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 1.83
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 152
    cleaning_beam_agent-0_mean: 69.77
    cleaning_beam_agent-0_min: 24
    cleaning_beam_agent-1_max: 313
    cleaning_beam_agent-1_mean: 232.16
    cleaning_beam_agent-1_min: 83
    cleaning_beam_agent-2_max: 68
    cleaning_beam_agent-2_mean: 13.03
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 186
    cleaning_beam_agent-3_mean: 78.68
    cleaning_beam_agent-3_min: 32
    cleaning_beam_agent-4_max: 117
    cleaning_beam_agent-4_mean: 38.14
    cleaning_beam_agent-4_min: 7
    cleaning_beam_agent-5_max: 24
    cleaning_beam_agent-5_mean: 6.63
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-26-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 58.0
  episode_reward_mean: 24.55
  episode_reward_min: 0.0
  episodes_this_iter: 96
  episodes_total: 26880
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11501.92
    learner:
      agent-0:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5320462584495544
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016970097785815597
        model: {}
        policy_loss: -0.003679180284962058
        total_loss: -0.0045838505029678345
        vf_explained_var: -0.003786742687225342
        vf_loss: 0.3173173666000366
      agent-1:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46545982360839844
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011832575546577573
        model: {}
        policy_loss: -0.00265749404206872
        total_loss: -0.0034633383620530367
        vf_explained_var: 0.0011374205350875854
        vf_loss: 0.1336236596107483
      agent-2:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43073877692222595
        entropy_coeff: 0.0017600000137463212
        kl: 0.001219023484736681
        model: {}
        policy_loss: -0.00330986431799829
        total_loss: -0.004017791710793972
        vf_explained_var: -0.002154439687728882
        vf_loss: 0.5017350912094116
      agent-3:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5811795592308044
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010152081958949566
        model: {}
        policy_loss: -0.002609245479106903
        total_loss: -0.003594235982745886
        vf_explained_var: 0.001998618245124817
        vf_loss: 0.3788638710975647
      agent-4:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6619492173194885
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015333101619035006
        model: {}
        policy_loss: -0.0038495156913995743
        total_loss: -0.004970535635948181
        vf_explained_var: 0.020532771944999695
        vf_loss: 0.44012272357940674
      agent-5:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8460712432861328
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015813971403986216
        model: {}
        policy_loss: -0.0029039434157311916
        total_loss: -0.004368694964796305
        vf_explained_var: 0.0024273544549942017
        vf_loss: 0.24330437183380127
    load_time_ms: 13536.278
    num_steps_sampled: 26880000
    num_steps_trained: 26880000
    sample_time_ms: 97229.08
    update_time_ms: 16.485
  iterations_since_restore: 120
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.382758620689657
    ram_util_percent: 11.507471264367814
  pid: 13408
  policy_reward_max:
    agent-0: 10.0
    agent-1: 12.0
    agent-2: 15.0
    agent-3: 15.0
    agent-4: 15.0
    agent-5: 14.0
  policy_reward_mean:
    agent-0: 4.18
    agent-1: 1.78
    agent-2: 5.34
    agent-3: 4.49
    agent-4: 5.36
    agent-5: 3.4
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.853112148032313
    mean_inference_ms: 12.763945856673235
    mean_processing_ms: 57.772972921375214
  time_since_restore: 14835.396803379059
  time_this_iter_s: 122.02200841903687
  time_total_s: 37345.680045843124
  timestamp: 1637234807
  timesteps_since_restore: 11520000
  timesteps_this_iter: 96000
  timesteps_total: 26880000
  training_iteration: 280
  trial_id: '00000'
  
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
== Status ==
Memory usage on this node: 20.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    280 |          37345.7 | 26880000 |    24.55 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 2.81
    apples_agent-0_min: 0
    apples_agent-1_max: 9
    apples_agent-1_mean: 1.14
    apples_agent-1_min: 0
    apples_agent-2_max: 40
    apples_agent-2_mean: 2.53
    apples_agent-2_min: 0
    apples_agent-3_max: 13
    apples_agent-3_mean: 2.71
    apples_agent-3_min: 0
    apples_agent-4_max: 171
    apples_agent-4_mean: 3.16
    apples_agent-4_min: 0
    apples_agent-5_max: 20
    apples_agent-5_mean: 2.04
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 145
    cleaning_beam_agent-0_mean: 67.26
    cleaning_beam_agent-0_min: 25
    cleaning_beam_agent-1_max: 400
    cleaning_beam_agent-1_mean: 236.75
    cleaning_beam_agent-1_min: 131
    cleaning_beam_agent-2_max: 53
    cleaning_beam_agent-2_mean: 10.72
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 213
    cleaning_beam_agent-3_mean: 70.79
    cleaning_beam_agent-3_min: 24
    cleaning_beam_agent-4_max: 172
    cleaning_beam_agent-4_mean: 39.33
    cleaning_beam_agent-4_min: 7
    cleaning_beam_agent-5_max: 22
    cleaning_beam_agent-5_mean: 6.51
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-28-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 76.0
  episode_reward_mean: 23.84
  episode_reward_min: -27.0
  episodes_this_iter: 96
  episodes_total: 26976
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11505.085
    learner:
      agent-0:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5230317115783691
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015317341312766075
        model: {}
        policy_loss: -0.003554182592779398
        total_loss: -0.0044415914453566074
        vf_explained_var: 0.0031910240650177
        vf_loss: 0.3312629461288452
      agent-1:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46299469470977783
        entropy_coeff: 0.0017600000137463212
        kl: 0.000993036082945764
        model: {}
        policy_loss: -0.0024485718458890915
        total_loss: -0.0032468484714627266
        vf_explained_var: 0.01765543222427368
        vf_loss: 0.1659601777791977
      agent-2:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4187484383583069
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010425024665892124
        model: {}
        policy_loss: -0.0032698074355721474
        total_loss: -0.003957989625632763
        vf_explained_var: 0.002534732222557068
        vf_loss: 0.4881686568260193
      agent-3:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5691993832588196
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016529830172657967
        model: {}
        policy_loss: -0.0018477444536983967
        total_loss: -0.002679541241377592
        vf_explained_var: -0.001015305519104004
        vf_loss: 1.6999397277832031
      agent-4:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6658698320388794
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017587658949196339
        model: {}
        policy_loss: -0.0038891909644007683
        total_loss: -0.005014415364712477
        vf_explained_var: 0.014413326978683472
        vf_loss: 0.4670718014240265
      agent-5:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8247619271278381
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014090699842199683
        model: {}
        policy_loss: -0.00256279855966568
        total_loss: -0.003981600049883127
        vf_explained_var: 0.012940958142280579
        vf_loss: 0.3277849555015564
    load_time_ms: 13543.617
    num_steps_sampled: 26976000
    num_steps_trained: 26976000
    sample_time_ms: 97367.911
    update_time_ms: 16.963
  iterations_since_restore: 121
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.175000000000008
    ram_util_percent: 11.264772727272726
  pid: 13408
  policy_reward_max:
    agent-0: 13.0
    agent-1: 12.0
    agent-2: 18.0
    agent-3: 14.0
    agent-4: 14.0
    agent-5: 17.0
  policy_reward_mean:
    agent-0: 3.96
    agent-1: 1.81
    agent-2: 5.47
    agent-3: 3.69
    agent-4: 5.38
    agent-5: 3.53
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -44.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.85069934566313
    mean_inference_ms: 12.762883791117947
    mean_processing_ms: 57.773701368627634
  time_since_restore: 14958.745770931244
  time_this_iter_s: 123.34896755218506
  time_total_s: 37469.02901339531
  timestamp: 1637234931
  timesteps_since_restore: 11616000
  timesteps_this_iter: 96000
  timesteps_total: 26976000
  training_iteration: 281
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    281 |            37469 | 26976000 |    23.84 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 35
    apples_agent-0_mean: 2.96
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 1.15
    apples_agent-1_min: 0
    apples_agent-2_max: 21
    apples_agent-2_mean: 1.81
    apples_agent-2_min: 0
    apples_agent-3_max: 16
    apples_agent-3_mean: 2.98
    apples_agent-3_min: 0
    apples_agent-4_max: 114
    apples_agent-4_mean: 2.28
    apples_agent-4_min: 0
    apples_agent-5_max: 20
    apples_agent-5_mean: 1.76
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 130
    cleaning_beam_agent-0_mean: 68.46
    cleaning_beam_agent-0_min: 28
    cleaning_beam_agent-1_max: 350
    cleaning_beam_agent-1_mean: 230.21
    cleaning_beam_agent-1_min: 133
    cleaning_beam_agent-2_max: 51
    cleaning_beam_agent-2_mean: 13.18
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 213
    cleaning_beam_agent-3_mean: 74.85
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 183
    cleaning_beam_agent-4_mean: 37.95
    cleaning_beam_agent-4_min: 6
    cleaning_beam_agent-5_max: 22
    cleaning_beam_agent-5_mean: 6.31
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-30-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 65.0
  episode_reward_mean: 23.97
  episode_reward_min: 8.0
  episodes_this_iter: 96
  episodes_total: 27072
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11502.911
    learner:
      agent-0:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5180317759513855
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017271931283175945
        model: {}
        policy_loss: -0.003512054216116667
        total_loss: -0.004396094940602779
        vf_explained_var: -0.0016859471797943115
        vf_loss: 0.27695518732070923
      agent-1:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46294867992401123
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008161296136677265
        model: {}
        policy_loss: -0.0023216335102915764
        total_loss: -0.0031241453252732754
        vf_explained_var: 0.021053388714790344
        vf_loss: 0.12280871719121933
      agent-2:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43679147958755493
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012461512815207243
        model: {}
        policy_loss: -0.003314051078632474
        total_loss: -0.004039985127747059
        vf_explained_var: 0.004040807485580444
        vf_loss: 0.4281640648841858
      agent-3:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5779337882995605
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010336200939491391
        model: {}
        policy_loss: -0.002384977415204048
        total_loss: -0.003353242762386799
        vf_explained_var: -0.00035546720027923584
        vf_loss: 0.4889966547489166
      agent-4:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6713067293167114
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012730868766084313
        model: {}
        policy_loss: -0.003815241390839219
        total_loss: -0.004953084513545036
        vf_explained_var: 0.018292412161827087
        vf_loss: 0.4365898370742798
      agent-5:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8454515337944031
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017012250609695911
        model: {}
        policy_loss: -0.0023917322978377342
        total_loss: -0.0038535771891474724
        vf_explained_var: 0.004637613892555237
        vf_loss: 0.2614879608154297
    load_time_ms: 13542.522
    num_steps_sampled: 27072000
    num_steps_trained: 27072000
    sample_time_ms: 97209.658
    update_time_ms: 16.762
  iterations_since_restore: 122
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.943352601156068
    ram_util_percent: 11.530057803468207
  pid: 13408
  policy_reward_max:
    agent-0: 10.0
    agent-1: 6.0
    agent-2: 18.0
    agent-3: 25.0
    agent-4: 16.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 3.9
    agent-1: 1.95
    agent-2: 4.99
    agent-3: 4.45
    agent-4: 5.25
    agent-5: 3.43
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.847682862457596
    mean_inference_ms: 12.762227987710638
    mean_processing_ms: 57.77126522771274
  time_since_restore: 15080.171639442444
  time_this_iter_s: 121.42586851119995
  time_total_s: 37590.45488190651
  timestamp: 1637235053
  timesteps_since_restore: 11712000
  timesteps_this_iter: 96000
  timesteps_total: 27072000
  training_iteration: 282
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    282 |          37590.5 | 27072000 |    23.97 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 2.8
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 1.06
    apples_agent-1_min: 0
    apples_agent-2_max: 21
    apples_agent-2_mean: 2.34
    apples_agent-2_min: 0
    apples_agent-3_max: 10
    apples_agent-3_mean: 2.81
    apples_agent-3_min: 0
    apples_agent-4_max: 10
    apples_agent-4_mean: 1.38
    apples_agent-4_min: 0
    apples_agent-5_max: 68
    apples_agent-5_mean: 2.87
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 146
    cleaning_beam_agent-0_mean: 67.08
    cleaning_beam_agent-0_min: 22
    cleaning_beam_agent-1_max: 410
    cleaning_beam_agent-1_mean: 225.3
    cleaning_beam_agent-1_min: 127
    cleaning_beam_agent-2_max: 67
    cleaning_beam_agent-2_mean: 10.33
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 196
    cleaning_beam_agent-3_mean: 73.5
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 131
    cleaning_beam_agent-4_mean: 38.91
    cleaning_beam_agent-4_min: 10
    cleaning_beam_agent-5_max: 24
    cleaning_beam_agent-5_mean: 6.13
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-32-56
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 58.0
  episode_reward_mean: 22.76
  episode_reward_min: -20.0
  episodes_this_iter: 96
  episodes_total: 27168
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11521.307
    learner:
      agent-0:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5201036334037781
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012317730579525232
        model: {}
        policy_loss: -0.0034384317696094513
        total_loss: -0.004326843190938234
        vf_explained_var: -0.005471155047416687
        vf_loss: 0.2696770429611206
      agent-1:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4670533239841461
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008914355421438813
        model: {}
        policy_loss: -0.002330000512301922
        total_loss: -0.003140287008136511
        vf_explained_var: 0.023858889937400818
        vf_loss: 0.11728152632713318
      agent-2:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42754656076431274
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011584528256207705
        model: {}
        policy_loss: -0.0032130409963428974
        total_loss: -0.003912567626684904
        vf_explained_var: -0.00520738959312439
        vf_loss: 0.5295385718345642
      agent-3:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5786576271057129
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012666181428357959
        model: {}
        policy_loss: -0.002727783052250743
        total_loss: -0.0037168492563068867
        vf_explained_var: 0.003823027014732361
        vf_loss: 0.2937120199203491
      agent-4:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6687686443328857
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013465274823829532
        model: {}
        policy_loss: -0.003596421331167221
        total_loss: -0.0047236159443855286
        vf_explained_var: 0.02185530960559845
        vf_loss: 0.49836796522140503
      agent-5:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8252919912338257
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022741181310266256
        model: {}
        policy_loss: -0.0020402283407747746
        total_loss: -0.003374430350959301
        vf_explained_var: 0.008508622646331787
        vf_loss: 1.183098316192627
    load_time_ms: 13566.006
    num_steps_sampled: 27168000
    num_steps_trained: 27168000
    sample_time_ms: 97295.878
    update_time_ms: 16.529
  iterations_since_restore: 123
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.23085714285714
    ram_util_percent: 11.42857142857143
  pid: 13408
  policy_reward_max:
    agent-0: 11.0
    agent-1: 7.0
    agent-2: 19.0
    agent-3: 13.0
    agent-4: 16.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 3.63
    agent-1: 1.71
    agent-2: 5.16
    agent-3: 4.1
    agent-4: 5.35
    agent-5: 2.81
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -44.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -46.0
  sampler_perf:
    mean_env_wait_ms: 22.844210843156265
    mean_inference_ms: 12.761319473221548
    mean_processing_ms: 57.76914460826763
  time_since_restore: 15203.027738571167
  time_this_iter_s: 122.85609912872314
  time_total_s: 37713.31098103523
  timestamp: 1637235176
  timesteps_since_restore: 11808000
  timesteps_this_iter: 96000
  timesteps_total: 27168000
  training_iteration: 283
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    283 |          37713.3 | 27168000 |    22.76 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 2.71
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.78
    apples_agent-1_min: 0
    apples_agent-2_max: 19
    apples_agent-2_mean: 2.54
    apples_agent-2_min: 0
    apples_agent-3_max: 20
    apples_agent-3_mean: 2.8
    apples_agent-3_min: 0
    apples_agent-4_max: 47
    apples_agent-4_mean: 2.15
    apples_agent-4_min: 0
    apples_agent-5_max: 8
    apples_agent-5_mean: 1.59
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 126
    cleaning_beam_agent-0_mean: 65.92
    cleaning_beam_agent-0_min: 26
    cleaning_beam_agent-1_max: 355
    cleaning_beam_agent-1_mean: 226.2
    cleaning_beam_agent-1_min: 128
    cleaning_beam_agent-2_max: 62
    cleaning_beam_agent-2_mean: 13.27
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 151
    cleaning_beam_agent-3_mean: 74.38
    cleaning_beam_agent-3_min: 25
    cleaning_beam_agent-4_max: 85
    cleaning_beam_agent-4_mean: 39.26
    cleaning_beam_agent-4_min: 3
    cleaning_beam_agent-5_max: 51
    cleaning_beam_agent-5_mean: 7.21
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-34-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 60.0
  episode_reward_mean: 22.42
  episode_reward_min: -13.0
  episodes_this_iter: 96
  episodes_total: 27264
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11521.12
    learner:
      agent-0:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5286403894424438
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014477253425866365
        model: {}
        policy_loss: -0.003109411336481571
        total_loss: -0.00400458462536335
        vf_explained_var: 0.0007290095090866089
        vf_loss: 0.3523164391517639
      agent-1:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47022995352745056
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011507112067192793
        model: {}
        policy_loss: -0.0025591847952455282
        total_loss: -0.0033769402652978897
        vf_explained_var: 0.01986744999885559
        vf_loss: 0.09846039116382599
      agent-2:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4285610020160675
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013332205126062036
        model: {}
        policy_loss: -0.0032472126185894012
        total_loss: -0.003946440294384956
        vf_explained_var: -0.0018038153648376465
        vf_loss: 0.5504071116447449
      agent-3:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5747545957565308
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010566195705905557
        model: {}
        policy_loss: -0.002658146433532238
        total_loss: -0.003640049370005727
        vf_explained_var: -0.0010340064764022827
        vf_loss: 0.2967008352279663
      agent-4:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6747118234634399
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022690591868013144
        model: {}
        policy_loss: -0.004172180313616991
        total_loss: -0.005317033268511295
        vf_explained_var: 0.01067352294921875
        vf_loss: 0.42639046907424927
      agent-5:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8495540618896484
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016751273069530725
        model: {}
        policy_loss: -0.002998628653585911
        total_loss: -0.004470271989703178
        vf_explained_var: 0.0002384781837463379
        vf_loss: 0.2357175350189209
    load_time_ms: 13566.697
    num_steps_sampled: 27264000
    num_steps_trained: 27264000
    sample_time_ms: 97344.597
    update_time_ms: 16.153
  iterations_since_restore: 124
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.258857142857146
    ram_util_percent: 11.418857142857146
  pid: 13408
  policy_reward_max:
    agent-0: 14.0
    agent-1: 10.0
    agent-2: 18.0
    agent-3: 11.0
    agent-4: 13.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 3.47
    agent-1: 1.56
    agent-2: 5.31
    agent-3: 3.83
    agent-4: 5.08
    agent-5: 3.17
  policy_reward_min:
    agent-0: -47.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.841210871492986
    mean_inference_ms: 12.760837453686781
    mean_processing_ms: 57.767733049398636
  time_since_restore: 15325.906397104263
  time_this_iter_s: 122.87865853309631
  time_total_s: 37836.18963956833
  timestamp: 1637235299
  timesteps_since_restore: 11904000
  timesteps_this_iter: 96000
  timesteps_total: 27264000
  training_iteration: 284
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    284 |          37836.2 | 27264000 |    22.42 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 2.6
    apples_agent-0_min: 0
    apples_agent-1_max: 15
    apples_agent-1_mean: 1.46
    apples_agent-1_min: 0
    apples_agent-2_max: 11
    apples_agent-2_mean: 2.33
    apples_agent-2_min: 0
    apples_agent-3_max: 15
    apples_agent-3_mean: 3.73
    apples_agent-3_min: 0
    apples_agent-4_max: 24
    apples_agent-4_mean: 2.13
    apples_agent-4_min: 0
    apples_agent-5_max: 23
    apples_agent-5_mean: 2.37
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 147
    cleaning_beam_agent-0_mean: 66.66
    cleaning_beam_agent-0_min: 13
    cleaning_beam_agent-1_max: 451
    cleaning_beam_agent-1_mean: 237.45
    cleaning_beam_agent-1_min: 127
    cleaning_beam_agent-2_max: 53
    cleaning_beam_agent-2_mean: 13.29
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 172
    cleaning_beam_agent-3_mean: 73.27
    cleaning_beam_agent-3_min: 26
    cleaning_beam_agent-4_max: 144
    cleaning_beam_agent-4_mean: 42.88
    cleaning_beam_agent-4_min: 5
    cleaning_beam_agent-5_max: 19
    cleaning_beam_agent-5_mean: 6.38
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-37-01
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 60.0
  episode_reward_mean: 25.67
  episode_reward_min: -35.0
  episodes_this_iter: 96
  episodes_total: 27360
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11518.758
    learner:
      agent-0:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5196800231933594
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014431901508942246
        model: {}
        policy_loss: -0.003470227587968111
        total_loss: -0.004350506234914064
        vf_explained_var: -0.0014928728342056274
        vf_loss: 0.34362125396728516
      agent-1:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45968392491340637
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011912116315215826
        model: {}
        policy_loss: -0.0025933142751455307
        total_loss: -0.003386078868061304
        vf_explained_var: 0.027329325675964355
        vf_loss: 0.1628136932849884
      agent-2:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43596985936164856
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010915392776951194
        model: {}
        policy_loss: -0.003503110259771347
        total_loss: -0.004208658821880817
        vf_explained_var: -0.006278276443481445
        vf_loss: 0.6175599098205566
      agent-3:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5862295627593994
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009666718542575836
        model: {}
        policy_loss: -0.0027670571580529213
        total_loss: -0.0037455279380083084
        vf_explained_var: 0.004158884286880493
        vf_loss: 0.5329127907752991
      agent-4:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6669434905052185
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014014819171279669
        model: {}
        policy_loss: -0.003769775852560997
        total_loss: -0.004886531736701727
        vf_explained_var: 0.02223069965839386
        vf_loss: 0.5706332921981812
      agent-5:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.843032717704773
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015331635950133204
        model: {}
        policy_loss: -0.0026735803112387657
        total_loss: -0.004125063307583332
        vf_explained_var: 0.0013788938522338867
        vf_loss: 0.3225392997264862
    load_time_ms: 13543.68
    num_steps_sampled: 27360000
    num_steps_trained: 27360000
    sample_time_ms: 97317.818
    update_time_ms: 16.109
  iterations_since_restore: 125
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.543678160919537
    ram_util_percent: 11.431609195402299
  pid: 13408
  policy_reward_max:
    agent-0: 13.0
    agent-1: 9.0
    agent-2: 20.0
    agent-3: 17.0
    agent-4: 19.0
    agent-5: 17.0
  policy_reward_mean:
    agent-0: 3.91
    agent-1: 2.17
    agent-2: 5.88
    agent-3: 5.01
    agent-4: 5.07
    agent-5: 3.63
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -47.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.838485247645664
    mean_inference_ms: 12.759401359649422
    mean_processing_ms: 57.76574924883269
  time_since_restore: 15448.036367177963
  time_this_iter_s: 122.12997007369995
  time_total_s: 37958.31960964203
  timestamp: 1637235421
  timesteps_since_restore: 12000000
  timesteps_this_iter: 96000
  timesteps_total: 27360000
  training_iteration: 285
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    285 |          37958.3 | 27360000 |    25.67 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 21
    apples_agent-0_mean: 2.89
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 1.07
    apples_agent-1_min: 0
    apples_agent-2_max: 16
    apples_agent-2_mean: 2.27
    apples_agent-2_min: 0
    apples_agent-3_max: 29
    apples_agent-3_mean: 3.14
    apples_agent-3_min: 0
    apples_agent-4_max: 11
    apples_agent-4_mean: 1.87
    apples_agent-4_min: 0
    apples_agent-5_max: 14
    apples_agent-5_mean: 1.87
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 127
    cleaning_beam_agent-0_mean: 63.64
    cleaning_beam_agent-0_min: 23
    cleaning_beam_agent-1_max: 385
    cleaning_beam_agent-1_mean: 232.91
    cleaning_beam_agent-1_min: 150
    cleaning_beam_agent-2_max: 51
    cleaning_beam_agent-2_mean: 9.5
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 131
    cleaning_beam_agent-3_mean: 68.1
    cleaning_beam_agent-3_min: 26
    cleaning_beam_agent-4_max: 104
    cleaning_beam_agent-4_mean: 37.48
    cleaning_beam_agent-4_min: 10
    cleaning_beam_agent-5_max: 30
    cleaning_beam_agent-5_mean: 7.18
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-39-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 64.0
  episode_reward_mean: 26.04
  episode_reward_min: -23.0
  episodes_this_iter: 96
  episodes_total: 27456
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11502.48
    learner:
      agent-0:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5219290256500244
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010972253512591124
        model: {}
        policy_loss: -0.002972301095724106
        total_loss: -0.003847281914204359
        vf_explained_var: 0.003337755799293518
        vf_loss: 0.43614456057548523
      agent-1:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.462471067905426
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013966407859697938
        model: {}
        policy_loss: -0.0026376191526651382
        total_loss: -0.003437954233959317
        vf_explained_var: 0.021311581134796143
        vf_loss: 0.13615909218788147
      agent-2:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4270106554031372
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012291001621633768
        model: {}
        policy_loss: -0.0032526985742151737
        total_loss: -0.003942626528441906
        vf_explained_var: 0.0001321732997894287
        vf_loss: 0.6161155700683594
      agent-3:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5922421813011169
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015907473862171173
        model: {}
        policy_loss: -0.0027885260060429573
        total_loss: -0.003793485462665558
        vf_explained_var: -0.001367524266242981
        vf_loss: 0.37386637926101685
      agent-4:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6687575578689575
        entropy_coeff: 0.0017600000137463212
        kl: 0.001173220807686448
        model: {}
        policy_loss: -0.003921849653124809
        total_loss: -0.005044583696871996
        vf_explained_var: 0.01480276882648468
        vf_loss: 0.5427968502044678
      agent-5:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8322170972824097
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016554805915802717
        model: {}
        policy_loss: -0.0024958846624940634
        total_loss: -0.003929982427507639
        vf_explained_var: 0.002123311161994934
        vf_loss: 0.3060750961303711
    load_time_ms: 13548.283
    num_steps_sampled: 27456000
    num_steps_trained: 27456000
    sample_time_ms: 97267.033
    update_time_ms: 15.739
  iterations_since_restore: 126
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.620689655172413
    ram_util_percent: 11.500574712643678
  pid: 13408
  policy_reward_max:
    agent-0: 16.0
    agent-1: 7.0
    agent-2: 18.0
    agent-3: 14.0
    agent-4: 18.0
    agent-5: 14.0
  policy_reward_mean:
    agent-0: 4.35
    agent-1: 1.96
    agent-2: 6.03
    agent-3: 4.32
    agent-4: 6.26
    agent-5: 3.12
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -47.0
  sampler_perf:
    mean_env_wait_ms: 22.83588031010431
    mean_inference_ms: 12.758307300645876
    mean_processing_ms: 57.761363342564
  time_since_restore: 15570.187294244766
  time_this_iter_s: 122.15092706680298
  time_total_s: 38080.47053670883
  timestamp: 1637235543
  timesteps_since_restore: 12096000
  timesteps_this_iter: 96000
  timesteps_total: 27456000
  training_iteration: 286
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    286 |          38080.5 | 27456000 |    26.04 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 2.82
    apples_agent-0_min: 0
    apples_agent-1_max: 10
    apples_agent-1_mean: 1.17
    apples_agent-1_min: 0
    apples_agent-2_max: 16
    apples_agent-2_mean: 2.19
    apples_agent-2_min: 0
    apples_agent-3_max: 12
    apples_agent-3_mean: 2.66
    apples_agent-3_min: 0
    apples_agent-4_max: 8
    apples_agent-4_mean: 1.33
    apples_agent-4_min: 0
    apples_agent-5_max: 73
    apples_agent-5_mean: 2.98
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 135
    cleaning_beam_agent-0_mean: 63.67
    cleaning_beam_agent-0_min: 11
    cleaning_beam_agent-1_max: 372
    cleaning_beam_agent-1_mean: 224.23
    cleaning_beam_agent-1_min: 135
    cleaning_beam_agent-2_max: 63
    cleaning_beam_agent-2_mean: 12.52
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 160
    cleaning_beam_agent-3_mean: 72.57
    cleaning_beam_agent-3_min: 29
    cleaning_beam_agent-4_max: 148
    cleaning_beam_agent-4_mean: 37.19
    cleaning_beam_agent-4_min: 8
    cleaning_beam_agent-5_max: 42
    cleaning_beam_agent-5_mean: 7.22
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-41-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 56.0
  episode_reward_mean: 23.52
  episode_reward_min: 3.0
  episodes_this_iter: 96
  episodes_total: 27552
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11499.265
    learner:
      agent-0:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5118826627731323
        entropy_coeff: 0.0017600000137463212
        kl: 0.001275685615837574
        model: {}
        policy_loss: -0.0034721167758107185
        total_loss: -0.004345906898379326
        vf_explained_var: -0.004584789276123047
        vf_loss: 0.27122512459754944
      agent-1:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4554155170917511
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008300527697429061
        model: {}
        policy_loss: -0.0024178866297006607
        total_loss: -0.0032053114846348763
        vf_explained_var: 0.012166723608970642
        vf_loss: 0.14107590913772583
      agent-2:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4228302836418152
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010278495028614998
        model: {}
        policy_loss: -0.0032992856577038765
        total_loss: -0.00400503259152174
        vf_explained_var: -0.00361594557762146
        vf_loss: 0.38435304164886475
      agent-3:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5966290235519409
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010913258884102106
        model: {}
        policy_loss: -0.0029035634361207485
        total_loss: -0.003923872951418161
        vf_explained_var: 0.002431422472000122
        vf_loss: 0.29757553339004517
      agent-4:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6747006177902222
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019226230215281248
        model: {}
        policy_loss: -0.00429775845259428
        total_loss: -0.005436247680336237
        vf_explained_var: 0.011523067951202393
        vf_loss: 0.4898306727409363
      agent-5:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8419986963272095
        entropy_coeff: 0.0017600000137463212
        kl: 0.001494389958679676
        model: {}
        policy_loss: -0.002460076939314604
        total_loss: -0.003919702954590321
        vf_explained_var: 0.013275817036628723
        vf_loss: 0.22293099761009216
    load_time_ms: 13542.229
    num_steps_sampled: 27552000
    num_steps_trained: 27552000
    sample_time_ms: 97365.424
    update_time_ms: 15.692
  iterations_since_restore: 127
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.54228571428571
    ram_util_percent: 11.42742857142857
  pid: 13408
  policy_reward_max:
    agent-0: 9.0
    agent-1: 8.0
    agent-2: 17.0
    agent-3: 11.0
    agent-4: 20.0
    agent-5: 14.0
  policy_reward_mean:
    agent-0: 3.96
    agent-1: 2.04
    agent-2: 4.91
    agent-3: 4.0
    agent-4: 5.27
    agent-5: 3.34
  policy_reward_min:
    agent-0: 0.0
    agent-1: -1.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.83306243050939
    mean_inference_ms: 12.757306922865984
    mean_processing_ms: 57.76098982239098
  time_since_restore: 15692.675689935684
  time_this_iter_s: 122.48839569091797
  time_total_s: 38202.95893239975
  timestamp: 1637235666
  timesteps_since_restore: 12192000
  timesteps_this_iter: 96000
  timesteps_total: 27552000
  training_iteration: 287
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    287 |            38203 | 27552000 |    23.52 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 35
    apples_agent-0_mean: 3.25
    apples_agent-0_min: 0
    apples_agent-1_max: 11
    apples_agent-1_mean: 1.21
    apples_agent-1_min: 0
    apples_agent-2_max: 10
    apples_agent-2_mean: 2.29
    apples_agent-2_min: 0
    apples_agent-3_max: 24
    apples_agent-3_mean: 3.17
    apples_agent-3_min: 0
    apples_agent-4_max: 19
    apples_agent-4_mean: 1.52
    apples_agent-4_min: 0
    apples_agent-5_max: 39
    apples_agent-5_mean: 2.26
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 148
    cleaning_beam_agent-0_mean: 61.5
    cleaning_beam_agent-0_min: 17
    cleaning_beam_agent-1_max: 330
    cleaning_beam_agent-1_mean: 212.66
    cleaning_beam_agent-1_min: 131
    cleaning_beam_agent-2_max: 81
    cleaning_beam_agent-2_mean: 11.47
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 145
    cleaning_beam_agent-3_mean: 74.51
    cleaning_beam_agent-3_min: 30
    cleaning_beam_agent-4_max: 207
    cleaning_beam_agent-4_mean: 44.83
    cleaning_beam_agent-4_min: 6
    cleaning_beam_agent-5_max: 25
    cleaning_beam_agent-5_mean: 6.99
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-43-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 70.0
  episode_reward_mean: 25.53
  episode_reward_min: 7.0
  episodes_this_iter: 96
  episodes_total: 27648
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11495.67
    learner:
      agent-0:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.513543426990509
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013767860364168882
        model: {}
        policy_loss: -0.0035063435789197683
        total_loss: -0.004374325275421143
        vf_explained_var: 0.0031634867191314697
        vf_loss: 0.3585295081138611
      agent-1:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45212990045547485
        entropy_coeff: 0.0017600000137463212
        kl: 0.001366264303214848
        model: {}
        policy_loss: -0.002585691399872303
        total_loss: -0.0033676191233098507
        vf_explained_var: 0.022252246737480164
        vf_loss: 0.13822321593761444
      agent-2:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4310420751571655
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018742209067568183
        model: {}
        policy_loss: -0.0034626005217432976
        total_loss: -0.004159001633524895
        vf_explained_var: -2.516806125640869e-05
        vf_loss: 0.6223239898681641
      agent-3:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5947858095169067
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010832513216882944
        model: {}
        policy_loss: -0.002249006647616625
        total_loss: -0.0032535435166209936
        vf_explained_var: 0.0018602609634399414
        vf_loss: 0.42287346720695496
      agent-4:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.675352156162262
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013807873474434018
        model: {}
        policy_loss: -0.003907991107553244
        total_loss: -0.005048682447522879
        vf_explained_var: 0.025477886199951172
        vf_loss: 0.47929155826568604
      agent-5:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8232328295707703
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016452902927994728
        model: {}
        policy_loss: -0.0027422215789556503
        total_loss: -0.004162108991295099
        vf_explained_var: 0.014346092939376831
        vf_loss: 0.2900378704071045
    load_time_ms: 13519.347
    num_steps_sampled: 27648000
    num_steps_trained: 27648000
    sample_time_ms: 97355.984
    update_time_ms: 14.948
  iterations_since_restore: 128
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.31896551724138
    ram_util_percent: 11.492528735632183
  pid: 13408
  policy_reward_max:
    agent-0: 15.0
    agent-1: 9.0
    agent-2: 19.0
    agent-3: 17.0
    agent-4: 16.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 4.01
    agent-1: 2.06
    agent-2: 6.13
    agent-3: 4.41
    agent-4: 5.52
    agent-5: 3.4
  policy_reward_min:
    agent-0: 0.0
    agent-1: -1.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.829628277363295
    mean_inference_ms: 12.756405009973424
    mean_processing_ms: 57.75999040722269
  time_since_restore: 15815.366029500961
  time_this_iter_s: 122.6903395652771
  time_total_s: 38325.64927196503
  timestamp: 1637235789
  timesteps_since_restore: 12288000
  timesteps_this_iter: 96000
  timesteps_total: 27648000
  training_iteration: 288
  trial_id: '00000'
  
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
== Status ==
Memory usage on this node: 20.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    288 |          38325.6 | 27648000 |    25.53 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 2.59
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 0.97
    apples_agent-1_min: 0
    apples_agent-2_max: 93
    apples_agent-2_mean: 3.17
    apples_agent-2_min: 0
    apples_agent-3_max: 15
    apples_agent-3_mean: 3.76
    apples_agent-3_min: 0
    apples_agent-4_max: 16
    apples_agent-4_mean: 1.22
    apples_agent-4_min: 0
    apples_agent-5_max: 12
    apples_agent-5_mean: 1.84
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 130
    cleaning_beam_agent-0_mean: 59.2
    cleaning_beam_agent-0_min: 18
    cleaning_beam_agent-1_max: 326
    cleaning_beam_agent-1_mean: 220.84
    cleaning_beam_agent-1_min: 124
    cleaning_beam_agent-2_max: 56
    cleaning_beam_agent-2_mean: 12.23
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 207
    cleaning_beam_agent-3_mean: 81.67
    cleaning_beam_agent-3_min: 35
    cleaning_beam_agent-4_max: 190
    cleaning_beam_agent-4_mean: 44.97
    cleaning_beam_agent-4_min: 10
    cleaning_beam_agent-5_max: 38
    cleaning_beam_agent-5_mean: 7.35
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-45-10
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 54.0
  episode_reward_mean: 24.55
  episode_reward_min: 8.0
  episodes_this_iter: 96
  episodes_total: 27744
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11493.137
    learner:
      agent-0:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5235092639923096
        entropy_coeff: 0.0017600000137463212
        kl: 0.001196323661133647
        model: {}
        policy_loss: -0.0034850887022912502
        total_loss: -0.004371640272438526
        vf_explained_var: -0.004494458436965942
        vf_loss: 0.3482556939125061
      agent-1:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4555264711380005
        entropy_coeff: 0.0017600000137463212
        kl: 0.001020776922814548
        model: {}
        policy_loss: -0.0024204468354582787
        total_loss: -0.003209994174540043
        vf_explained_var: 0.020484894514083862
        vf_loss: 0.12179060280323029
      agent-2:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4197857677936554
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014032002072781324
        model: {}
        policy_loss: -0.003644850105047226
        total_loss: -0.004340221174061298
        vf_explained_var: -0.0014078617095947266
        vf_loss: 0.4345170855522156
      agent-3:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5960140228271484
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011628333013504744
        model: {}
        policy_loss: -0.0026673506945371628
        total_loss: -0.0036692223511636257
        vf_explained_var: -0.0004943162202835083
        vf_loss: 0.4711342453956604
      agent-4:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6655135154724121
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021018758416175842
        model: {}
        policy_loss: -0.004092338494956493
        total_loss: -0.005222647916525602
        vf_explained_var: 0.007577687501907349
        vf_loss: 0.4099073112010956
      agent-5:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.858445405960083
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017706525977700949
        model: {}
        policy_loss: -0.0028738416731357574
        total_loss: -0.004355468321591616
        vf_explained_var: 0.0026140064001083374
        vf_loss: 0.29235729575157166
    load_time_ms: 13532.01
    num_steps_sampled: 27744000
    num_steps_trained: 27744000
    sample_time_ms: 97179.431
    update_time_ms: 14.872
  iterations_since_restore: 129
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.471676300578036
    ram_util_percent: 11.41791907514451
  pid: 13408
  policy_reward_max:
    agent-0: 15.0
    agent-1: 8.0
    agent-2: 15.0
    agent-3: 17.0
    agent-4: 14.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 4.1
    agent-1: 1.74
    agent-2: 5.1
    agent-3: 4.85
    agent-4: 5.07
    agent-5: 3.69
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.826656083757474
    mean_inference_ms: 12.755614764717798
    mean_processing_ms: 57.75535240550872
  time_since_restore: 15936.545931816101
  time_this_iter_s: 121.17990231513977
  time_total_s: 38446.82917428017
  timestamp: 1637235910
  timesteps_since_restore: 12384000
  timesteps_this_iter: 96000
  timesteps_total: 27744000
  training_iteration: 289
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    289 |          38446.8 | 27744000 |    24.55 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 2.64
    apples_agent-0_min: 0
    apples_agent-1_max: 12
    apples_agent-1_mean: 1.2
    apples_agent-1_min: 0
    apples_agent-2_max: 15
    apples_agent-2_mean: 1.92
    apples_agent-2_min: 0
    apples_agent-3_max: 20
    apples_agent-3_mean: 3.0
    apples_agent-3_min: 0
    apples_agent-4_max: 39
    apples_agent-4_mean: 2.29
    apples_agent-4_min: 0
    apples_agent-5_max: 30
    apples_agent-5_mean: 2.04
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 155
    cleaning_beam_agent-0_mean: 64.26
    cleaning_beam_agent-0_min: 18
    cleaning_beam_agent-1_max: 346
    cleaning_beam_agent-1_mean: 213.05
    cleaning_beam_agent-1_min: 82
    cleaning_beam_agent-2_max: 89
    cleaning_beam_agent-2_mean: 13.25
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 257
    cleaning_beam_agent-3_mean: 83.52
    cleaning_beam_agent-3_min: 26
    cleaning_beam_agent-4_max: 110
    cleaning_beam_agent-4_mean: 41.29
    cleaning_beam_agent-4_min: 7
    cleaning_beam_agent-5_max: 29
    cleaning_beam_agent-5_mean: 7.55
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-47-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 53.0
  episode_reward_mean: 23.67
  episode_reward_min: 9.0
  episodes_this_iter: 96
  episodes_total: 27840
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11499.675
    learner:
      agent-0:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5096291303634644
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014721510233357549
        model: {}
        policy_loss: -0.0032386775128543377
        total_loss: -0.004100724123418331
        vf_explained_var: -0.0019277483224868774
        vf_loss: 0.3489992618560791
      agent-1:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4484569728374481
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007956401677802205
        model: {}
        policy_loss: -0.002332678996026516
        total_loss: -0.0031070373952388763
        vf_explained_var: 0.027986273169517517
        vf_loss: 0.14922980964183807
      agent-2:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4165695905685425
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011004029074683785
        model: {}
        policy_loss: -0.003421916626393795
        total_loss: -0.0041159167885780334
        vf_explained_var: -0.003965482115745544
        vf_loss: 0.39162689447402954
      agent-3:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5975366830825806
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018407650059089065
        model: {}
        policy_loss: -0.0028193085454404354
        total_loss: -0.0038346911314874887
        vf_explained_var: -0.0009619444608688354
        vf_loss: 0.3628094494342804
      agent-4:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6706234812736511
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013691885396838188
        model: {}
        policy_loss: -0.0037150969728827477
        total_loss: -0.004850404337048531
        vf_explained_var: 0.010206416249275208
        vf_loss: 0.4499199390411377
      agent-5:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8348560333251953
        entropy_coeff: 0.0017600000137463212
        kl: 0.00180086272303015
        model: {}
        policy_loss: -0.0029280995950102806
        total_loss: -0.004366658627986908
        vf_explained_var: 0.006355553865432739
        vf_loss: 0.3078433871269226
    load_time_ms: 13529.182
    num_steps_sampled: 27840000
    num_steps_trained: 27840000
    sample_time_ms: 97175.684
    update_time_ms: 14.952
  iterations_since_restore: 130
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.371098265895952
    ram_util_percent: 11.424277456647399
  pid: 13408
  policy_reward_max:
    agent-0: 10.0
    agent-1: 9.0
    agent-2: 15.0
    agent-3: 11.0
    agent-4: 14.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 3.91
    agent-1: 1.9
    agent-2: 4.82
    agent-3: 4.27
    agent-4: 5.37
    agent-5: 3.4
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.822600585312372
    mean_inference_ms: 12.754759682806782
    mean_processing_ms: 57.749878909342186
  time_since_restore: 16058.552815914154
  time_this_iter_s: 122.00688409805298
  time_total_s: 38568.83605837822
  timestamp: 1637236032
  timesteps_since_restore: 12480000
  timesteps_this_iter: 96000
  timesteps_total: 27840000
  training_iteration: 290
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    290 |          38568.8 | 27840000 |    23.67 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 3.1
    apples_agent-0_min: 0
    apples_agent-1_max: 32
    apples_agent-1_mean: 1.37
    apples_agent-1_min: 0
    apples_agent-2_max: 14
    apples_agent-2_mean: 2.52
    apples_agent-2_min: 0
    apples_agent-3_max: 13
    apples_agent-3_mean: 3.33
    apples_agent-3_min: 0
    apples_agent-4_max: 7
    apples_agent-4_mean: 1.36
    apples_agent-4_min: 0
    apples_agent-5_max: 8
    apples_agent-5_mean: 1.94
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 129
    cleaning_beam_agent-0_mean: 62.07
    cleaning_beam_agent-0_min: 16
    cleaning_beam_agent-1_max: 328
    cleaning_beam_agent-1_mean: 218.84
    cleaning_beam_agent-1_min: 125
    cleaning_beam_agent-2_max: 44
    cleaning_beam_agent-2_mean: 10.57
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 177
    cleaning_beam_agent-3_mean: 87.71
    cleaning_beam_agent-3_min: 35
    cleaning_beam_agent-4_max: 108
    cleaning_beam_agent-4_mean: 39.26
    cleaning_beam_agent-4_min: 6
    cleaning_beam_agent-5_max: 21
    cleaning_beam_agent-5_mean: 6.76
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-49-15
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 49.0
  episode_reward_mean: 24.56
  episode_reward_min: -26.0
  episodes_this_iter: 96
  episodes_total: 27936
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11476.505
    learner:
      agent-0:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49218833446502686
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011339429765939713
        model: {}
        policy_loss: -0.0030845007859170437
        total_loss: -0.003908798564225435
        vf_explained_var: -1.3530254364013672e-05
        vf_loss: 0.4195351302623749
      agent-1:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45464837551116943
        entropy_coeff: 0.0017600000137463212
        kl: 0.001224234001711011
        model: {}
        policy_loss: -0.0025513721629977226
        total_loss: -0.003341046627610922
        vf_explained_var: 0.023335948586463928
        vf_loss: 0.1050531417131424
      agent-2:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4108503460884094
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011730561964213848
        model: {}
        policy_loss: -0.002009429968893528
        total_loss: -0.0025535670574754477
        vf_explained_var: 0.004012048244476318
        vf_loss: 1.7896151542663574
      agent-3:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5878010988235474
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012493333779275417
        model: {}
        policy_loss: -0.0027416758239269257
        total_loss: -0.0037375688552856445
        vf_explained_var: 0.0021205246448516846
        vf_loss: 0.3863869309425354
      agent-4:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6708756685256958
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017640802543610334
        model: {}
        policy_loss: -0.0038877646438777447
        total_loss: -0.00502423383295536
        vf_explained_var: 0.012306630611419678
        vf_loss: 0.442748486995697
      agent-5:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8543431758880615
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019192321924492717
        model: {}
        policy_loss: -0.0027241273783147335
        total_loss: -0.004200404509902
        vf_explained_var: 0.002939775586128235
        vf_loss: 0.2737160325050354
    load_time_ms: 13527.179
    num_steps_sampled: 27936000
    num_steps_trained: 27936000
    sample_time_ms: 97054.486
    update_time_ms: 14.383
  iterations_since_restore: 131
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.312
    ram_util_percent: 11.414857142857144
  pid: 13408
  policy_reward_max:
    agent-0: 15.0
    agent-1: 6.0
    agent-2: 14.0
    agent-3: 15.0
    agent-4: 13.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 3.69
    agent-1: 1.72
    agent-2: 5.5
    agent-3: 4.46
    agent-4: 5.57
    agent-5: 3.62
  policy_reward_min:
    agent-0: -42.0
    agent-1: 0.0
    agent-2: -45.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.819369743306055
    mean_inference_ms: 12.753530127295779
    mean_processing_ms: 57.74920907049853
  time_since_restore: 16180.430578231812
  time_this_iter_s: 121.87776231765747
  time_total_s: 38690.71382069588
  timestamp: 1637236155
  timesteps_since_restore: 12576000
  timesteps_this_iter: 96000
  timesteps_total: 27936000
  training_iteration: 291
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    291 |          38690.7 | 27936000 |    24.56 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 3.04
    apples_agent-0_min: 0
    apples_agent-1_max: 19
    apples_agent-1_mean: 1.28
    apples_agent-1_min: 0
    apples_agent-2_max: 7
    apples_agent-2_mean: 2.03
    apples_agent-2_min: 0
    apples_agent-3_max: 19
    apples_agent-3_mean: 3.2
    apples_agent-3_min: 0
    apples_agent-4_max: 10
    apples_agent-4_mean: 1.27
    apples_agent-4_min: 0
    apples_agent-5_max: 9
    apples_agent-5_mean: 1.87
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 110
    cleaning_beam_agent-0_mean: 65.12
    cleaning_beam_agent-0_min: 22
    cleaning_beam_agent-1_max: 362
    cleaning_beam_agent-1_mean: 225.2
    cleaning_beam_agent-1_min: 123
    cleaning_beam_agent-2_max: 51
    cleaning_beam_agent-2_mean: 9.66
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 184
    cleaning_beam_agent-3_mean: 87.71
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 246
    cleaning_beam_agent-4_mean: 42.6
    cleaning_beam_agent-4_min: 6
    cleaning_beam_agent-5_max: 32
    cleaning_beam_agent-5_mean: 6.82
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-51-17
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 60.0
  episode_reward_mean: 24.19
  episode_reward_min: -35.0
  episodes_this_iter: 96
  episodes_total: 28032
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11477.641
    learner:
      agent-0:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5056983232498169
        entropy_coeff: 0.0017600000137463212
        kl: 0.001637800713069737
        model: {}
        policy_loss: -0.003321389202028513
        total_loss: -0.004180740099400282
        vf_explained_var: 0.0010614991188049316
        vf_loss: 0.30682557821273804
      agent-1:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46294763684272766
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009327817242592573
        model: {}
        policy_loss: -0.0026754364371299744
        total_loss: -0.003478563390672207
        vf_explained_var: 0.026968449354171753
        vf_loss: 0.11660043895244598
      agent-2:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4072168469429016
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012815207010135055
        model: {}
        policy_loss: -0.003420109860599041
        total_loss: -0.004082517698407173
        vf_explained_var: -0.0018436163663864136
        vf_loss: 0.5429849624633789
      agent-3:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5873854160308838
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015806944575160742
        model: {}
        policy_loss: -0.002406972460448742
        total_loss: -0.003398813772946596
        vf_explained_var: -0.0011707991361618042
        vf_loss: 0.41956913471221924
      agent-4:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.673854410648346
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011401406954973936
        model: {}
        policy_loss: -0.003936321008950472
        total_loss: -0.005080265924334526
        vf_explained_var: 0.019016340374946594
        vf_loss: 0.4203782081604004
      agent-5:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8370156288146973
        entropy_coeff: 0.0017600000137463212
        kl: 0.001996068749576807
        model: {}
        policy_loss: -0.0030196798034012318
        total_loss: -0.004465017933398485
        vf_explained_var: 0.006066679954528809
        vf_loss: 0.2780906856060028
    load_time_ms: 13535.629
    num_steps_sampled: 28032000
    num_steps_trained: 28032000
    sample_time_ms: 97073.315
    update_time_ms: 14.59
  iterations_since_restore: 132
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.273988439306358
    ram_util_percent: 11.424855491329481
  pid: 13408
  policy_reward_max:
    agent-0: 11.0
    agent-1: 7.0
    agent-2: 17.0
    agent-3: 25.0
    agent-4: 14.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 4.1
    agent-1: 1.88
    agent-2: 5.61
    agent-3: 4.4
    agent-4: 5.26
    agent-5: 2.94
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -49.0
  sampler_perf:
    mean_env_wait_ms: 22.81613277049272
    mean_inference_ms: 12.752117520077142
    mean_processing_ms: 57.74379009071247
  time_since_restore: 16302.080485105515
  time_this_iter_s: 121.649906873703
  time_total_s: 38812.36372756958
  timestamp: 1637236277
  timesteps_since_restore: 12672000
  timesteps_this_iter: 96000
  timesteps_total: 28032000
  training_iteration: 292
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    292 |          38812.4 | 28032000 |    24.19 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 35
    apples_agent-0_mean: 3.48
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 1.17
    apples_agent-1_min: 0
    apples_agent-2_max: 12
    apples_agent-2_mean: 2.06
    apples_agent-2_min: 0
    apples_agent-3_max: 32
    apples_agent-3_mean: 3.37
    apples_agent-3_min: 0
    apples_agent-4_max: 21
    apples_agent-4_mean: 1.55
    apples_agent-4_min: 0
    apples_agent-5_max: 9
    apples_agent-5_mean: 1.98
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 125
    cleaning_beam_agent-0_mean: 61.86
    cleaning_beam_agent-0_min: 20
    cleaning_beam_agent-1_max: 337
    cleaning_beam_agent-1_mean: 225.18
    cleaning_beam_agent-1_min: 139
    cleaning_beam_agent-2_max: 48
    cleaning_beam_agent-2_mean: 8.69
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 209
    cleaning_beam_agent-3_mean: 89.39
    cleaning_beam_agent-3_min: 34
    cleaning_beam_agent-4_max: 184
    cleaning_beam_agent-4_mean: 43.6
    cleaning_beam_agent-4_min: 8
    cleaning_beam_agent-5_max: 20
    cleaning_beam_agent-5_mean: 6.56
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-53-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 78.0
  episode_reward_mean: 24.39
  episode_reward_min: -36.0
  episodes_this_iter: 96
  episodes_total: 28128
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11458.022
    learner:
      agent-0:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5013636350631714
        entropy_coeff: 0.0017600000137463212
        kl: 0.001088167424313724
        model: {}
        policy_loss: -0.002750837244093418
        total_loss: -0.0035832244902849197
        vf_explained_var: 0.0017011165618896484
        vf_loss: 0.5001043081283569
      agent-1:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.457841694355011
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012673361925408244
        model: {}
        policy_loss: -0.0023491797037422657
        total_loss: -0.0031433654949069023
        vf_explained_var: 0.014036118984222412
        vf_loss: 0.11616349965333939
      agent-2:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39252805709838867
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015742236282676458
        model: {}
        policy_loss: -0.0033643916249275208
        total_loss: -0.004009753465652466
        vf_explained_var: -0.006439030170440674
        vf_loss: 0.45485401153564453
      agent-3:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5837084650993347
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009150949772447348
        model: {}
        policy_loss: -0.0024276883341372013
        total_loss: -0.003414405509829521
        vf_explained_var: -0.00038826465606689453
        vf_loss: 0.4061087667942047
      agent-4:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6645609140396118
        entropy_coeff: 0.0017600000137463212
        kl: 0.001814855495467782
        model: {}
        policy_loss: -0.003904362441971898
        total_loss: -0.0050310478545725346
        vf_explained_var: -0.002536594867706299
        vf_loss: 0.4294230341911316
      agent-5:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8255636096000671
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013247124152258039
        model: {}
        policy_loss: -0.002314904937520623
        total_loss: -0.0037336009554564953
        vf_explained_var: 0.008414700627326965
        vf_loss: 0.3429565131664276
    load_time_ms: 13527.537
    num_steps_sampled: 28128000
    num_steps_trained: 28128000
    sample_time_ms: 97010.321
    update_time_ms: 15.017
  iterations_since_restore: 133
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.542528735632185
    ram_util_percent: 11.510344827586206
  pid: 13408
  policy_reward_max:
    agent-0: 29.0
    agent-1: 10.0
    agent-2: 13.0
    agent-3: 15.0
    agent-4: 13.0
    agent-5: 16.0
  policy_reward_mean:
    agent-0: 4.36
    agent-1: 1.79
    agent-2: 5.74
    agent-3: 3.71
    agent-4: 5.21
    agent-5: 3.58
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -48.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.81393576651085
    mean_inference_ms: 12.750683847010697
    mean_processing_ms: 57.740999133878155
  time_since_restore: 16424.005136489868
  time_this_iter_s: 121.92465138435364
  time_total_s: 38934.288378953934
  timestamp: 1637236399
  timesteps_since_restore: 12768000
  timesteps_this_iter: 96000
  timesteps_total: 28128000
  training_iteration: 293
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    293 |          38934.3 | 28128000 |    24.39 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 28
    apples_agent-0_mean: 3.06
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 1.0
    apples_agent-1_min: 0
    apples_agent-2_max: 15
    apples_agent-2_mean: 2.29
    apples_agent-2_min: 0
    apples_agent-3_max: 22
    apples_agent-3_mean: 3.35
    apples_agent-3_min: 0
    apples_agent-4_max: 9
    apples_agent-4_mean: 1.16
    apples_agent-4_min: 0
    apples_agent-5_max: 10
    apples_agent-5_mean: 2.03
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 142
    cleaning_beam_agent-0_mean: 65.0
    cleaning_beam_agent-0_min: 25
    cleaning_beam_agent-1_max: 364
    cleaning_beam_agent-1_mean: 210.77
    cleaning_beam_agent-1_min: 111
    cleaning_beam_agent-2_max: 48
    cleaning_beam_agent-2_mean: 8.86
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 177
    cleaning_beam_agent-3_mean: 84.35
    cleaning_beam_agent-3_min: 41
    cleaning_beam_agent-4_max: 118
    cleaning_beam_agent-4_mean: 42.17
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 30
    cleaning_beam_agent-5_mean: 8.04
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-55-21
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 50.0
  episode_reward_mean: 24.17
  episode_reward_min: 3.0
  episodes_this_iter: 96
  episodes_total: 28224
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11455.99
    learner:
      agent-0:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5083224773406982
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015732868341729045
        model: {}
        policy_loss: -0.0033999159932136536
        total_loss: -0.004263241775333881
        vf_explained_var: -0.0005659312009811401
        vf_loss: 0.31321170926094055
      agent-1:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45672881603240967
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008039310923777521
        model: {}
        policy_loss: -0.0025409068912267685
        total_loss: -0.0033335513435304165
        vf_explained_var: 0.006713688373565674
        vf_loss: 0.11198873072862625
      agent-2:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3890814185142517
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010635789949446917
        model: {}
        policy_loss: -0.0032627456821501255
        total_loss: -0.003890623804181814
        vf_explained_var: -0.005445122718811035
        vf_loss: 0.5690311193466187
      agent-3:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5797578692436218
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009724811534397304
        model: {}
        policy_loss: -0.0025014709681272507
        total_loss: -0.003492495510727167
        vf_explained_var: 0.0012898892164230347
        vf_loss: 0.29348015785217285
      agent-4:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6711681485176086
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019860677421092987
        model: {}
        policy_loss: -0.004126382060348988
        total_loss: -0.005265482701361179
        vf_explained_var: 0.009879842400550842
        vf_loss: 0.42154690623283386
      agent-5:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8476285338401794
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017116086091846228
        model: {}
        policy_loss: -0.0027261569630354643
        total_loss: -0.004195636138319969
        vf_explained_var: 0.0034755170345306396
        vf_loss: 0.22346815466880798
    load_time_ms: 13548.069
    num_steps_sampled: 28224000
    num_steps_trained: 28224000
    sample_time_ms: 96952.46
    update_time_ms: 14.801
  iterations_since_restore: 134
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.616091954022984
    ram_util_percent: 11.440229885057471
  pid: 13408
  policy_reward_max:
    agent-0: 11.0
    agent-1: 10.0
    agent-2: 17.0
    agent-3: 11.0
    agent-4: 17.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 3.94
    agent-1: 1.85
    agent-2: 5.79
    agent-3: 4.16
    agent-4: 5.21
    agent-5: 3.22
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.811188665310603
    mean_inference_ms: 12.749596652074306
    mean_processing_ms: 57.73784945607959
  time_since_restore: 16546.489048480988
  time_this_iter_s: 122.48391199111938
  time_total_s: 39056.77229094505
  timestamp: 1637236521
  timesteps_since_restore: 12864000
  timesteps_this_iter: 96000
  timesteps_total: 28224000
  training_iteration: 294
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    294 |          39056.8 | 28224000 |    24.17 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 3.09
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 1.11
    apples_agent-1_min: 0
    apples_agent-2_max: 54
    apples_agent-2_mean: 3.43
    apples_agent-2_min: 0
    apples_agent-3_max: 22
    apples_agent-3_mean: 3.41
    apples_agent-3_min: 0
    apples_agent-4_max: 7
    apples_agent-4_mean: 1.11
    apples_agent-4_min: 0
    apples_agent-5_max: 18
    apples_agent-5_mean: 2.1
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 117
    cleaning_beam_agent-0_mean: 60.79
    cleaning_beam_agent-0_min: 8
    cleaning_beam_agent-1_max: 344
    cleaning_beam_agent-1_mean: 215.04
    cleaning_beam_agent-1_min: 125
    cleaning_beam_agent-2_max: 64
    cleaning_beam_agent-2_mean: 10.58
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 182
    cleaning_beam_agent-3_mean: 88.35
    cleaning_beam_agent-3_min: 29
    cleaning_beam_agent-4_max: 140
    cleaning_beam_agent-4_mean: 42.49
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 36
    cleaning_beam_agent-5_mean: 7.48
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-57-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 64.0
  episode_reward_mean: 24.53
  episode_reward_min: -41.0
  episodes_this_iter: 96
  episodes_total: 28320
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11454.387
    learner:
      agent-0:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5125346779823303
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013133520260453224
        model: {}
        policy_loss: -0.0029709353111684322
        total_loss: -0.003847745945677161
        vf_explained_var: 0.0015018582344055176
        vf_loss: 0.2524842321872711
      agent-1:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45137858390808105
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014533239882439375
        model: {}
        policy_loss: -0.0027230435516685247
        total_loss: -0.0035011337604373693
        vf_explained_var: 0.01710781455039978
        vf_loss: 0.16336220502853394
      agent-2:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4007369577884674
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009638668852858245
        model: {}
        policy_loss: -0.003080589696764946
        total_loss: -0.0037363364826887846
        vf_explained_var: -0.0040932297706604
        vf_loss: 0.4954993724822998
      agent-3:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5820379853248596
        entropy_coeff: 0.0017600000137463212
        kl: 0.001016799476929009
        model: {}
        policy_loss: -0.001195400021970272
        total_loss: -0.002049514092504978
        vf_explained_var: -0.0012612640857696533
        vf_loss: 1.7027517557144165
      agent-4:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6741738319396973
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015344582498073578
        model: {}
        policy_loss: -0.00392208993434906
        total_loss: -0.00505298376083374
        vf_explained_var: 0.019121915102005005
        vf_loss: 0.5565222501754761
      agent-5:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8600980043411255
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016863138880580664
        model: {}
        policy_loss: -0.0029478762298822403
        total_loss: -0.004433751106262207
        vf_explained_var: 0.008162960410118103
        vf_loss: 0.2789838910102844
    load_time_ms: 13557.111
    num_steps_sampled: 28320000
    num_steps_trained: 28320000
    sample_time_ms: 97008.469
    update_time_ms: 14.885
  iterations_since_restore: 135
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.59085714285714
    ram_util_percent: 11.508571428571425
  pid: 13408
  policy_reward_max:
    agent-0: 12.0
    agent-1: 12.0
    agent-2: 21.0
    agent-3: 16.0
    agent-4: 14.0
    agent-5: 17.0
  policy_reward_mean:
    agent-0: 3.79
    agent-1: 1.95
    agent-2: 5.47
    agent-3: 4.17
    agent-4: 5.73
    agent-5: 3.42
  policy_reward_min:
    agent-0: 1.0
    agent-1: 0.0
    agent-2: 1.0
    agent-3: -49.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.80998282114065
    mean_inference_ms: 12.749167808973645
    mean_processing_ms: 57.73851462866853
  time_since_restore: 16669.25254225731
  time_this_iter_s: 122.76349377632141
  time_total_s: 39179.535784721375
  timestamp: 1637236644
  timesteps_since_restore: 12960000
  timesteps_this_iter: 96000
  timesteps_total: 28320000
  training_iteration: 295
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    295 |          39179.5 | 28320000 |    24.53 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 21
    apples_agent-0_mean: 2.95
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 1.22
    apples_agent-1_min: 0
    apples_agent-2_max: 32
    apples_agent-2_mean: 2.4
    apples_agent-2_min: 0
    apples_agent-3_max: 12
    apples_agent-3_mean: 2.97
    apples_agent-3_min: 0
    apples_agent-4_max: 8
    apples_agent-4_mean: 1.02
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 1.61
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 119
    cleaning_beam_agent-0_mean: 62.82
    cleaning_beam_agent-0_min: 10
    cleaning_beam_agent-1_max: 340
    cleaning_beam_agent-1_mean: 211.44
    cleaning_beam_agent-1_min: 116
    cleaning_beam_agent-2_max: 36
    cleaning_beam_agent-2_mean: 8.1
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 156
    cleaning_beam_agent-3_mean: 87.21
    cleaning_beam_agent-3_min: 32
    cleaning_beam_agent-4_max: 97
    cleaning_beam_agent-4_mean: 34.73
    cleaning_beam_agent-4_min: 11
    cleaning_beam_agent-5_max: 31
    cleaning_beam_agent-5_mean: 7.19
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-59-27
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 54.0
  episode_reward_mean: 22.63
  episode_reward_min: -37.0
  episodes_this_iter: 96
  episodes_total: 28416
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11469.602
    learner:
      agent-0:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5240074396133423
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017716913716867566
        model: {}
        policy_loss: -0.0034830975346267223
        total_loss: -0.004378289449959993
        vf_explained_var: 0.0031594783067703247
        vf_loss: 0.27063021063804626
      agent-1:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4583855867385864
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009633417939767241
        model: {}
        policy_loss: -0.002434674184769392
        total_loss: -0.0032291109673678875
        vf_explained_var: 0.020588144659996033
        vf_loss: 0.12317647784948349
      agent-2:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3981890082359314
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012321234680712223
        model: {}
        policy_loss: -0.0027816155925393105
        total_loss: -0.0034346822649240494
        vf_explained_var: -0.0010111033916473389
        vf_loss: 0.4774401783943176
      agent-3:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5684353113174438
        entropy_coeff: 0.0017600000137463212
        kl: 0.00151306320913136
        model: {}
        policy_loss: -0.0028145387768745422
        total_loss: -0.0037798737175762653
        vf_explained_var: 0.0013514310121536255
        vf_loss: 0.351102739572525
      agent-4:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6726378202438354
        entropy_coeff: 0.0017600000137463212
        kl: 0.001420124201104045
        model: {}
        policy_loss: -0.004080161452293396
        total_loss: -0.005225436761975288
        vf_explained_var: 0.015481039881706238
        vf_loss: 0.3857063055038452
      agent-5:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8434227705001831
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018462506122887135
        model: {}
        policy_loss: -0.0025233030319213867
        total_loss: -0.003981093410402536
        vf_explained_var: 0.014386266469955444
        vf_loss: 0.26633694767951965
    load_time_ms: 13564.523
    num_steps_sampled: 28416000
    num_steps_trained: 28416000
    sample_time_ms: 97064.348
    update_time_ms: 14.903
  iterations_since_restore: 136
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.170285714285722
    ram_util_percent: 11.413714285714283
  pid: 13408
  policy_reward_max:
    agent-0: 12.0
    agent-1: 9.0
    agent-2: 16.0
    agent-3: 14.0
    agent-4: 13.0
    agent-5: 17.0
  policy_reward_mean:
    agent-0: 3.64
    agent-1: 1.82
    agent-2: 5.18
    agent-3: 4.34
    agent-4: 4.8
    agent-5: 2.85
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -48.0
  sampler_perf:
    mean_env_wait_ms: 22.807663151274298
    mean_inference_ms: 12.748407021083123
    mean_processing_ms: 57.73828004772915
  time_since_restore: 16792.1926343441
  time_this_iter_s: 122.94009208679199
  time_total_s: 39302.47587680817
  timestamp: 1637236767
  timesteps_since_restore: 13056000
  timesteps_this_iter: 96000
  timesteps_total: 28416000
  training_iteration: 296
  trial_id: '00000'
  
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
== Status ==
Memory usage on this node: 20.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    296 |          39302.5 | 28416000 |    22.63 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 2.63
    apples_agent-0_min: 0
    apples_agent-1_max: 41
    apples_agent-1_mean: 1.71
    apples_agent-1_min: 0
    apples_agent-2_max: 10
    apples_agent-2_mean: 1.77
    apples_agent-2_min: 0
    apples_agent-3_max: 24
    apples_agent-3_mean: 3.41
    apples_agent-3_min: 0
    apples_agent-4_max: 25
    apples_agent-4_mean: 1.7
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 1.98
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 119
    cleaning_beam_agent-0_mean: 61.15
    cleaning_beam_agent-0_min: 18
    cleaning_beam_agent-1_max: 333
    cleaning_beam_agent-1_mean: 214.35
    cleaning_beam_agent-1_min: 115
    cleaning_beam_agent-2_max: 70
    cleaning_beam_agent-2_mean: 10.05
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 185
    cleaning_beam_agent-3_mean: 82.41
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 146
    cleaning_beam_agent-4_mean: 46.2
    cleaning_beam_agent-4_min: 11
    cleaning_beam_agent-5_max: 26
    cleaning_beam_agent-5_mean: 6.98
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-01-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 76.0
  episode_reward_mean: 26.2
  episode_reward_min: 7.0
  episodes_this_iter: 96
  episodes_total: 28512
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11487.928
    learner:
      agent-0:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5104860067367554
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014147378969937563
        model: {}
        policy_loss: -0.0035055866464972496
        total_loss: -0.004373639822006226
        vf_explained_var: 0.003514200448989868
        vf_loss: 0.30399879813194275
      agent-1:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45573580265045166
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008423728868365288
        model: {}
        policy_loss: -0.002273509744554758
        total_loss: -0.0030585560016334057
        vf_explained_var: 0.018415987491607666
        vf_loss: 0.17049989104270935
      agent-2:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4097837209701538
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010622047120705247
        model: {}
        policy_loss: -0.0031136851757764816
        total_loss: -0.003784874454140663
        vf_explained_var: 0.007146403193473816
        vf_loss: 0.5003230571746826
      agent-3:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5757281184196472
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010389827657490969
        model: {}
        policy_loss: -0.0023965565487742424
        total_loss: -0.003371170721948147
        vf_explained_var: 0.004970088601112366
        vf_loss: 0.38665392994880676
      agent-4:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6711997985839844
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018480264116078615
        model: {}
        policy_loss: -0.004243309609591961
        total_loss: -0.005376290995627642
        vf_explained_var: 0.020401552319526672
        vf_loss: 0.4833250641822815
      agent-5:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8583123087882996
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017551509663462639
        model: {}
        policy_loss: -0.0027227795217186213
        total_loss: -0.004203497432172298
        vf_explained_var: -0.004631668329238892
        vf_loss: 0.29915332794189453
    load_time_ms: 13563.363
    num_steps_sampled: 28512000
    num_steps_trained: 28512000
    sample_time_ms: 97117.413
    update_time_ms: 15.0
  iterations_since_restore: 137
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.855113636363637
    ram_util_percent: 11.465909090909092
  pid: 13408
  policy_reward_max:
    agent-0: 14.0
    agent-1: 10.0
    agent-2: 15.0
    agent-3: 18.0
    agent-4: 15.0
    agent-5: 18.0
  policy_reward_mean:
    agent-0: 3.91
    agent-1: 2.03
    agent-2: 6.06
    agent-3: 4.83
    agent-4: 5.74
    agent-5: 3.63
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.806078409481746
    mean_inference_ms: 12.748100161382416
    mean_processing_ms: 57.740233033149686
  time_since_restore: 16915.352344751358
  time_this_iter_s: 123.15971040725708
  time_total_s: 39425.63558721542
  timestamp: 1637236891
  timesteps_since_restore: 13152000
  timesteps_this_iter: 96000
  timesteps_total: 28512000
  training_iteration: 297
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    297 |          39425.6 | 28512000 |     26.2 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 2.96
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 1.13
    apples_agent-1_min: 0
    apples_agent-2_max: 13
    apples_agent-2_mean: 2.07
    apples_agent-2_min: 0
    apples_agent-3_max: 9
    apples_agent-3_mean: 3.25
    apples_agent-3_min: 0
    apples_agent-4_max: 25
    apples_agent-4_mean: 1.62
    apples_agent-4_min: 0
    apples_agent-5_max: 14
    apples_agent-5_mean: 1.85
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 190
    cleaning_beam_agent-0_mean: 67.61
    cleaning_beam_agent-0_min: 23
    cleaning_beam_agent-1_max: 386
    cleaning_beam_agent-1_mean: 215.73
    cleaning_beam_agent-1_min: 140
    cleaning_beam_agent-2_max: 47
    cleaning_beam_agent-2_mean: 10.08
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 202
    cleaning_beam_agent-3_mean: 87.73
    cleaning_beam_agent-3_min: 33
    cleaning_beam_agent-4_max: 118
    cleaning_beam_agent-4_mean: 39.29
    cleaning_beam_agent-4_min: 6
    cleaning_beam_agent-5_max: 28
    cleaning_beam_agent-5_mean: 7.17
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-03-35
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 62.0
  episode_reward_mean: 24.24
  episode_reward_min: -61.0
  episodes_this_iter: 96
  episodes_total: 28608
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11491.204
    learner:
      agent-0:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5105045437812805
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016543616075068712
        model: {}
        policy_loss: -0.0033150319941341877
        total_loss: -0.00417662225663662
        vf_explained_var: 0.0007863938808441162
        vf_loss: 0.3689398467540741
      agent-1:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4479188919067383
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009344448917545378
        model: {}
        policy_loss: -0.002144140424206853
        total_loss: -0.0029152072966098785
        vf_explained_var: 0.018904417753219604
        vf_loss: 0.17270681262016296
      agent-2:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4034584164619446
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015077212592586875
        model: {}
        policy_loss: -0.0033456292003393173
        total_loss: -0.004012893885374069
        vf_explained_var: 0.003442898392677307
        vf_loss: 0.4282514452934265
      agent-3:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5767034292221069
        entropy_coeff: 0.0017600000137463212
        kl: 0.001537454780191183
        model: {}
        policy_loss: -0.0018831375055015087
        total_loss: -0.002729622647166252
        vf_explained_var: 0.00256308913230896
        vf_loss: 1.685142993927002
      agent-4:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6689490079879761
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013628940796479583
        model: {}
        policy_loss: -0.0036139744333922863
        total_loss: -0.0047386870719492435
        vf_explained_var: 0.02448943257331848
        vf_loss: 0.5263460278511047
      agent-5:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8083314895629883
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013300315476953983
        model: {}
        policy_loss: -0.0022777363192290068
        total_loss: -0.0036733709275722504
        vf_explained_var: 0.017712250351905823
        vf_loss: 0.2702733278274536
    load_time_ms: 13575.575
    num_steps_sampled: 28608000
    num_steps_trained: 28608000
    sample_time_ms: 97194.423
    update_time_ms: 15.726
  iterations_since_restore: 138
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.18409090909091
    ram_util_percent: 11.448295454545454
  pid: 13408
  policy_reward_max:
    agent-0: 17.0
    agent-1: 11.0
    agent-2: 15.0
    agent-3: 12.0
    agent-4: 20.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 4.44
    agent-1: 1.57
    agent-2: 5.43
    agent-3: 3.51
    agent-4: 5.89
    agent-5: 3.4
  policy_reward_min:
    agent-0: 0.0
    agent-1: -48.0
    agent-2: 0.0
    agent-3: -46.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.80487881991201
    mean_inference_ms: 12.747775508327276
    mean_processing_ms: 57.74099119446486
  time_since_restore: 17039.018838882446
  time_this_iter_s: 123.66649413108826
  time_total_s: 39549.30208134651
  timestamp: 1637237015
  timesteps_since_restore: 13248000
  timesteps_this_iter: 96000
  timesteps_total: 28608000
  training_iteration: 298
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    298 |          39549.3 | 28608000 |    24.24 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 2.99
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 1.15
    apples_agent-1_min: 0
    apples_agent-2_max: 18
    apples_agent-2_mean: 2.37
    apples_agent-2_min: 0
    apples_agent-3_max: 11
    apples_agent-3_mean: 3.2
    apples_agent-3_min: 0
    apples_agent-4_max: 11
    apples_agent-4_mean: 1.43
    apples_agent-4_min: 0
    apples_agent-5_max: 20
    apples_agent-5_mean: 2.35
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 145
    cleaning_beam_agent-0_mean: 60.98
    cleaning_beam_agent-0_min: 7
    cleaning_beam_agent-1_max: 399
    cleaning_beam_agent-1_mean: 223.38
    cleaning_beam_agent-1_min: 145
    cleaning_beam_agent-2_max: 61
    cleaning_beam_agent-2_mean: 9.0
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 201
    cleaning_beam_agent-3_mean: 92.66
    cleaning_beam_agent-3_min: 34
    cleaning_beam_agent-4_max: 113
    cleaning_beam_agent-4_mean: 39.37
    cleaning_beam_agent-4_min: 3
    cleaning_beam_agent-5_max: 27
    cleaning_beam_agent-5_mean: 6.21
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-05-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 59.0
  episode_reward_mean: 26.21
  episode_reward_min: 5.0
  episodes_this_iter: 96
  episodes_total: 28704
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11474.544
    learner:
      agent-0:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.506403923034668
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014943715650588274
        model: {}
        policy_loss: -0.003398377913981676
        total_loss: -0.004257681779563427
        vf_explained_var: -0.007460057735443115
        vf_loss: 0.3197025954723358
      agent-1:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45026183128356934
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007525681285187602
        model: {}
        policy_loss: -0.002389643806964159
        total_loss: -0.00316507276147604
        vf_explained_var: 0.0068467408418655396
        vf_loss: 0.17031797766685486
      agent-2:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3992505967617035
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014037905493751168
        model: {}
        policy_loss: -0.003056728281080723
        total_loss: -0.00370680820196867
        vf_explained_var: -0.00709649920463562
        vf_loss: 0.5260215997695923
      agent-3:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5794240832328796
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014585030730813742
        model: {}
        policy_loss: -0.0024996940046548843
        total_loss: -0.00348578579723835
        vf_explained_var: -0.000948682427406311
        vf_loss: 0.3369257152080536
      agent-4:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6698364019393921
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017008509021252394
        model: {}
        policy_loss: -0.004044618457555771
        total_loss: -0.005180537700653076
        vf_explained_var: 0.008345410227775574
        vf_loss: 0.4299122095108032
      agent-5:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8516006469726562
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015955122653394938
        model: {}
        policy_loss: -0.0028279058169573545
        total_loss: -0.004291543271392584
        vf_explained_var: 0.0009285509586334229
        vf_loss: 0.35180944204330444
    load_time_ms: 13570.545
    num_steps_sampled: 28704000
    num_steps_trained: 28704000
    sample_time_ms: 97346.059
    update_time_ms: 16.498
  iterations_since_restore: 139
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.378735632183908
    ram_util_percent: 11.418390804597703
  pid: 13408
  policy_reward_max:
    agent-0: 14.0
    agent-1: 10.0
    agent-2: 15.0
    agent-3: 12.0
    agent-4: 17.0
    agent-5: 16.0
  policy_reward_mean:
    agent-0: 4.09
    agent-1: 2.32
    agent-2: 5.93
    agent-3: 4.6
    agent-4: 5.3
    agent-5: 3.97
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.80313537448863
    mean_inference_ms: 12.747424229977474
    mean_processing_ms: 57.739866550667806
  time_since_restore: 17161.46767950058
  time_this_iter_s: 122.44884061813354
  time_total_s: 39671.750921964645
  timestamp: 1637237137
  timesteps_since_restore: 13344000
  timesteps_this_iter: 96000
  timesteps_total: 28704000
  training_iteration: 299
  trial_id: '00000'
  
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
== Status ==
Memory usage on this node: 20.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    299 |          39671.8 | 28704000 |    26.21 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 8
    apples_agent-0_mean: 2.36
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.89
    apples_agent-1_min: 0
    apples_agent-2_max: 44
    apples_agent-2_mean: 2.74
    apples_agent-2_min: 0
    apples_agent-3_max: 11
    apples_agent-3_mean: 3.33
    apples_agent-3_min: 0
    apples_agent-4_max: 11
    apples_agent-4_mean: 1.7
    apples_agent-4_min: 0
    apples_agent-5_max: 100
    apples_agent-5_mean: 2.58
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 113
    cleaning_beam_agent-0_mean: 64.69
    cleaning_beam_agent-0_min: 30
    cleaning_beam_agent-1_max: 327
    cleaning_beam_agent-1_mean: 212.05
    cleaning_beam_agent-1_min: 107
    cleaning_beam_agent-2_max: 70
    cleaning_beam_agent-2_mean: 9.56
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 220
    cleaning_beam_agent-3_mean: 93.51
    cleaning_beam_agent-3_min: 32
    cleaning_beam_agent-4_max: 98
    cleaning_beam_agent-4_mean: 39.16
    cleaning_beam_agent-4_min: 7
    cleaning_beam_agent-5_max: 22
    cleaning_beam_agent-5_mean: 6.79
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-07-39
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 62.0
  episode_reward_mean: 23.42
  episode_reward_min: -81.0
  episodes_this_iter: 96
  episodes_total: 28800
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11463.118
    learner:
      agent-0:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5127602219581604
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012826340971514583
        model: {}
        policy_loss: -0.00335720949806273
        total_loss: -0.004234721418470144
        vf_explained_var: -0.00022698938846588135
        vf_loss: 0.24945509433746338
      agent-1:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4645675718784332
        entropy_coeff: 0.0017600000137463212
        kl: 0.000872506236191839
        model: {}
        policy_loss: -0.00251535652205348
        total_loss: -0.003321476746350527
        vf_explained_var: 0.022395387291908264
        vf_loss: 0.1152196153998375
      agent-2:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.401247501373291
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010801574680954218
        model: {}
        policy_loss: -0.0030647455714643
        total_loss: -0.0037070270627737045
        vf_explained_var: 0.0075490474700927734
        vf_loss: 0.6391490697860718
      agent-3:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5911880731582642
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011118135880678892
        model: {}
        policy_loss: -0.001977500505745411
        total_loss: -0.002841861452907324
        vf_explained_var: -0.00011126697063446045
        vf_loss: 1.761290192604065
      agent-4:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6645960807800293
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014133042423054576
        model: {}
        policy_loss: -0.003034500405192375
        total_loss: -0.004022651351988316
        vf_explained_var: 0.005448475480079651
        vf_loss: 1.8153926134109497
      agent-5:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8718971014022827
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016599325463175774
        model: {}
        policy_loss: -0.0026072831824421883
        total_loss: -0.004116520285606384
        vf_explained_var: -7.3909759521484375e-06
        vf_loss: 0.2530076503753662
    load_time_ms: 13597.124
    num_steps_sampled: 28800000
    num_steps_trained: 28800000
    sample_time_ms: 97342.319
    update_time_ms: 16.353
  iterations_since_restore: 140
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.03793103448276
    ram_util_percent: 11.42816091954023
  pid: 13408
  policy_reward_max:
    agent-0: 10.0
    agent-1: 7.0
    agent-2: 21.0
    agent-3: 17.0
    agent-4: 17.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 3.53
    agent-1: 1.81
    agent-2: 5.35
    agent-3: 4.18
    agent-4: 5.23
    agent-5: 3.32
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -44.0
    agent-3: -48.0
    agent-4: -48.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 22.801042520159168
    mean_inference_ms: 12.746708841639322
    mean_processing_ms: 57.736434400868895
  time_since_restore: 17283.58912873268
  time_this_iter_s: 122.12144923210144
  time_total_s: 39793.87237119675
  timestamp: 1637237259
  timesteps_since_restore: 13440000
  timesteps_this_iter: 96000
  timesteps_total: 28800000
  training_iteration: 300
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    300 |          39793.9 | 28800000 |    23.42 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 7
    apples_agent-0_mean: 2.58
    apples_agent-0_min: 0
    apples_agent-1_max: 11
    apples_agent-1_mean: 1.08
    apples_agent-1_min: 0
    apples_agent-2_max: 18
    apples_agent-2_mean: 3.01
    apples_agent-2_min: 0
    apples_agent-3_max: 11
    apples_agent-3_mean: 3.01
    apples_agent-3_min: 0
    apples_agent-4_max: 6
    apples_agent-4_mean: 1.19
    apples_agent-4_min: 0
    apples_agent-5_max: 36
    apples_agent-5_mean: 2.38
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 156
    cleaning_beam_agent-0_mean: 63.68
    cleaning_beam_agent-0_min: 22
    cleaning_beam_agent-1_max: 412
    cleaning_beam_agent-1_mean: 228.81
    cleaning_beam_agent-1_min: 126
    cleaning_beam_agent-2_max: 59
    cleaning_beam_agent-2_mean: 10.24
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 233
    cleaning_beam_agent-3_mean: 99.82
    cleaning_beam_agent-3_min: 31
    cleaning_beam_agent-4_max: 132
    cleaning_beam_agent-4_mean: 44.39
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 28
    cleaning_beam_agent-5_mean: 6.58
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-09-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 56.0
  episode_reward_mean: 26.18
  episode_reward_min: 8.0
  episodes_this_iter: 96
  episodes_total: 28896
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11472.333
    learner:
      agent-0:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5020221471786499
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013211844488978386
        model: {}
        policy_loss: -0.0032004297245293856
        total_loss: -0.0040560620836913586
        vf_explained_var: -0.003295809030532837
        vf_loss: 0.2792382538318634
      agent-1:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4536333978176117
        entropy_coeff: 0.0017600000137463212
        kl: 0.001100210240110755
        model: {}
        policy_loss: -0.002609425224363804
        total_loss: -0.0033950498327612877
        vf_explained_var: 0.011401712894439697
        vf_loss: 0.12768177688121796
      agent-2:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41747093200683594
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015666252002120018
        model: {}
        policy_loss: -0.003490342292934656
        total_loss: -0.00417268555611372
        vf_explained_var: -0.00037294626235961914
        vf_loss: 0.5240947008132935
      agent-3:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5705785751342773
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013375660637393594
        model: {}
        policy_loss: -0.002807208336889744
        total_loss: -0.0037833983078598976
        vf_explained_var: 0.002853766083717346
        vf_loss: 0.2802845239639282
      agent-4:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6718245148658752
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012656624894589186
        model: {}
        policy_loss: -0.0036724559031426907
        total_loss: -0.004809225909411907
        vf_explained_var: 0.01395978033542633
        vf_loss: 0.4564107656478882
      agent-5:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8634666204452515
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016198004595935345
        model: {}
        policy_loss: -0.0026239054277539253
        total_loss: -0.004114809911698103
        vf_explained_var: 0.0033585280179977417
        vf_loss: 0.28793948888778687
    load_time_ms: 13585.253
    num_steps_sampled: 28896000
    num_steps_trained: 28896000
    sample_time_ms: 97482.986
    update_time_ms: 16.571
  iterations_since_restore: 141
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.31581920903955
    ram_util_percent: 11.432768361581921
  pid: 13408
  policy_reward_max:
    agent-0: 10.0
    agent-1: 7.0
    agent-2: 17.0
    agent-3: 17.0
    agent-4: 16.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 3.92
    agent-1: 1.93
    agent-2: 6.3
    agent-3: 4.46
    agent-4: 5.69
    agent-5: 3.88
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.79971760823967
    mean_inference_ms: 12.745862690126746
    mean_processing_ms: 57.73643244589702
  time_since_restore: 17406.85182785988
  time_this_iter_s: 123.26269912719727
  time_total_s: 39917.135070323944
  timestamp: 1637237383
  timesteps_since_restore: 13536000
  timesteps_this_iter: 96000
  timesteps_total: 28896000
  training_iteration: 301
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    301 |          39917.1 | 28896000 |    26.18 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 2.87
    apples_agent-0_min: 0
    apples_agent-1_max: 153
    apples_agent-1_mean: 2.55
    apples_agent-1_min: 0
    apples_agent-2_max: 29
    apples_agent-2_mean: 2.43
    apples_agent-2_min: 0
    apples_agent-3_max: 15
    apples_agent-3_mean: 3.08
    apples_agent-3_min: 0
    apples_agent-4_max: 13
    apples_agent-4_mean: 1.71
    apples_agent-4_min: 0
    apples_agent-5_max: 21
    apples_agent-5_mean: 2.26
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 144
    cleaning_beam_agent-0_mean: 60.72
    cleaning_beam_agent-0_min: 27
    cleaning_beam_agent-1_max: 353
    cleaning_beam_agent-1_mean: 214.81
    cleaning_beam_agent-1_min: 113
    cleaning_beam_agent-2_max: 95
    cleaning_beam_agent-2_mean: 11.83
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 181
    cleaning_beam_agent-3_mean: 87.31
    cleaning_beam_agent-3_min: 30
    cleaning_beam_agent-4_max: 214
    cleaning_beam_agent-4_mean: 44.72
    cleaning_beam_agent-4_min: 5
    cleaning_beam_agent-5_max: 26
    cleaning_beam_agent-5_mean: 7.66
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-11-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 59.0
  episode_reward_mean: 23.51
  episode_reward_min: -68.0
  episodes_this_iter: 96
  episodes_total: 28992
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11481.239
    learner:
      agent-0:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5050691366195679
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010750690707936883
        model: {}
        policy_loss: -0.0030672959983348846
        total_loss: -0.003923187032341957
        vf_explained_var: -0.0035857409238815308
        vf_loss: 0.33031415939331055
      agent-1:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45070958137512207
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009279178339056671
        model: {}
        policy_loss: -0.002193401800468564
        total_loss: -0.0029726296197623014
        vf_explained_var: 0.02174420654773712
        vf_loss: 0.14021266996860504
      agent-2:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41277363896369934
        entropy_coeff: 0.0017600000137463212
        kl: 0.001007383456453681
        model: {}
        policy_loss: -0.003334362991154194
        total_loss: -0.004012542311102152
        vf_explained_var: -0.00946575403213501
        vf_loss: 0.48301076889038086
      agent-3:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5740578770637512
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006967082154005766
        model: {}
        policy_loss: -0.0013113541062921286
        total_loss: -0.002169820247218013
        vf_explained_var: -9.034574031829834e-05
        vf_loss: 1.518767237663269
      agent-4:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6665118932723999
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013224034337326884
        model: {}
        policy_loss: -0.0028217253275215626
        total_loss: -0.0036837360821664333
        vf_explained_var: 0.00034911930561065674
        vf_loss: 3.110494613647461
      agent-5:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8608134984970093
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014523931313306093
        model: {}
        policy_loss: -0.002706038299947977
        total_loss: -0.0041947439312934875
        vf_explained_var: 0.012197151780128479
        vf_loss: 0.26322194933891296
    load_time_ms: 13582.327
    num_steps_sampled: 28992000
    num_steps_trained: 28992000
    sample_time_ms: 97649.336
    update_time_ms: 16.539
  iterations_since_restore: 142
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.557386363636365
    ram_util_percent: 11.50340909090909
  pid: 13408
  policy_reward_max:
    agent-0: 14.0
    agent-1: 9.0
    agent-2: 14.0
    agent-3: 11.0
    agent-4: 15.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 4.18
    agent-1: 1.97
    agent-2: 5.58
    agent-3: 3.63
    agent-4: 4.37
    agent-5: 3.78
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -44.0
    agent-4: -47.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.79837857119375
    mean_inference_ms: 12.745037400382408
    mean_processing_ms: 57.73723074266845
  time_since_restore: 17530.27295064926
  time_this_iter_s: 123.42112278938293
  time_total_s: 40040.55619311333
  timestamp: 1637237507
  timesteps_since_restore: 13632000
  timesteps_this_iter: 96000
  timesteps_total: 28992000
  training_iteration: 302
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    302 |          40040.6 | 28992000 |    23.51 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 2.73
    apples_agent-0_min: 0
    apples_agent-1_max: 30
    apples_agent-1_mean: 1.57
    apples_agent-1_min: 0
    apples_agent-2_max: 75
    apples_agent-2_mean: 3.0
    apples_agent-2_min: 0
    apples_agent-3_max: 11
    apples_agent-3_mean: 2.97
    apples_agent-3_min: 0
    apples_agent-4_max: 16
    apples_agent-4_mean: 1.46
    apples_agent-4_min: 0
    apples_agent-5_max: 20
    apples_agent-5_mean: 2.15
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 113
    cleaning_beam_agent-0_mean: 61.86
    cleaning_beam_agent-0_min: 18
    cleaning_beam_agent-1_max: 458
    cleaning_beam_agent-1_mean: 214.47
    cleaning_beam_agent-1_min: 129
    cleaning_beam_agent-2_max: 74
    cleaning_beam_agent-2_mean: 9.33
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 197
    cleaning_beam_agent-3_mean: 95.0
    cleaning_beam_agent-3_min: 27
    cleaning_beam_agent-4_max: 182
    cleaning_beam_agent-4_mean: 44.09
    cleaning_beam_agent-4_min: 8
    cleaning_beam_agent-5_max: 30
    cleaning_beam_agent-5_mean: 7.01
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-13-50
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 72.0
  episode_reward_mean: 24.24
  episode_reward_min: -76.0
  episodes_this_iter: 96
  episodes_total: 29088
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11498.837
    learner:
      agent-0:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5001331567764282
        entropy_coeff: 0.0017600000137463212
        kl: 0.001084514195099473
        model: {}
        policy_loss: -0.0030954210087656975
        total_loss: -0.003942822106182575
        vf_explained_var: -0.0016528517007827759
        vf_loss: 0.32834747433662415
      agent-1:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45805537700653076
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011898484081029892
        model: {}
        policy_loss: -0.0015607697423547506
        total_loss: -0.0022206492722034454
        vf_explained_var: 0.001034766435623169
        vf_loss: 1.4629870653152466
      agent-2:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4065811038017273
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017070933245122433
        model: {}
        policy_loss: -0.003354161512106657
        total_loss: -0.0040208203718066216
        vf_explained_var: -0.0024208426475524902
        vf_loss: 0.48921629786491394
      agent-3:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5876752138137817
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017859531799331307
        model: {}
        policy_loss: -0.0019458108581602573
        total_loss: -0.002825744217261672
        vf_explained_var: -0.0003313720226287842
        vf_loss: 1.5437726974487305
      agent-4:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6640710234642029
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014811488799750805
        model: {}
        policy_loss: -0.00398832093924284
        total_loss: -0.005110330879688263
        vf_explained_var: 0.018998607993125916
        vf_loss: 0.46757128834724426
      agent-5:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8690683841705322
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018378087552264333
        model: {}
        policy_loss: -0.0027123726904392242
        total_loss: -0.0042145736515522
        vf_explained_var: 0.0010551214218139648
        vf_loss: 0.27356913685798645
    load_time_ms: 13580.379
    num_steps_sampled: 29088000
    num_steps_trained: 29088000
    sample_time_ms: 97736.331
    update_time_ms: 16.112
  iterations_since_restore: 143
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.670857142857145
    ram_util_percent: 11.434857142857146
  pid: 13408
  policy_reward_max:
    agent-0: 11.0
    agent-1: 8.0
    agent-2: 17.0
    agent-3: 11.0
    agent-4: 20.0
    agent-5: 15.0
  policy_reward_mean:
    agent-0: 4.32
    agent-1: 1.42
    agent-2: 5.56
    agent-3: 3.68
    agent-4: 5.74
    agent-5: 3.52
  policy_reward_min:
    agent-0: 0.0
    agent-1: -50.0
    agent-2: 0.0
    agent-3: -46.0
    agent-4: 1.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.796300389468662
    mean_inference_ms: 12.74439414090803
    mean_processing_ms: 57.736274414669296
  time_since_restore: 17653.28157019615
  time_this_iter_s: 123.00861954689026
  time_total_s: 40163.56481266022
  timestamp: 1637237630
  timesteps_since_restore: 13728000
  timesteps_this_iter: 96000
  timesteps_total: 29088000
  training_iteration: 303
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    303 |          40163.6 | 29088000 |    24.24 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 2.87
    apples_agent-0_min: 0
    apples_agent-1_max: 14
    apples_agent-1_mean: 1.17
    apples_agent-1_min: 0
    apples_agent-2_max: 21
    apples_agent-2_mean: 2.24
    apples_agent-2_min: 0
    apples_agent-3_max: 14
    apples_agent-3_mean: 2.92
    apples_agent-3_min: 0
    apples_agent-4_max: 10
    apples_agent-4_mean: 1.28
    apples_agent-4_min: 0
    apples_agent-5_max: 8
    apples_agent-5_mean: 1.92
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 149
    cleaning_beam_agent-0_mean: 61.58
    cleaning_beam_agent-0_min: 24
    cleaning_beam_agent-1_max: 358
    cleaning_beam_agent-1_mean: 208.49
    cleaning_beam_agent-1_min: 128
    cleaning_beam_agent-2_max: 66
    cleaning_beam_agent-2_mean: 10.92
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 219
    cleaning_beam_agent-3_mean: 97.21
    cleaning_beam_agent-3_min: 33
    cleaning_beam_agent-4_max: 88
    cleaning_beam_agent-4_mean: 38.3
    cleaning_beam_agent-4_min: 10
    cleaning_beam_agent-5_max: 54
    cleaning_beam_agent-5_mean: 8.68
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-15-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 74.0
  episode_reward_mean: 22.97
  episode_reward_min: -90.0
  episodes_this_iter: 96
  episodes_total: 29184
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11494.427
    learner:
      agent-0:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5138866305351257
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011017192155122757
        model: {}
        policy_loss: -0.0033894022926688194
        total_loss: -0.0042570242658257484
        vf_explained_var: -0.0020352303981781006
        vf_loss: 0.3681725859642029
      agent-1:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4518677592277527
        entropy_coeff: 0.0017600000137463212
        kl: 0.001364657306112349
        model: {}
        policy_loss: -0.002351241186261177
        total_loss: -0.003135383129119873
        vf_explained_var: 0.01859511435031891
        vf_loss: 0.11145387589931488
      agent-2:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41265133023262024
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010572276078164577
        model: {}
        policy_loss: -0.003148794872686267
        total_loss: -0.0038323006592690945
        vf_explained_var: -0.0033790618181228638
        vf_loss: 0.4276256859302521
      agent-3:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5995009541511536
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013628043234348297
        model: {}
        policy_loss: -0.0028295209631323814
        total_loss: -0.0038508535362780094
        vf_explained_var: 0.0035537034273147583
        vf_loss: 0.3378923535346985
      agent-4:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6778254508972168
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008263234631158412
        model: {}
        policy_loss: -0.0019198410445824265
        total_loss: -0.00257086637429893
        vf_explained_var: 0.0042581260204315186
        vf_loss: 5.419488430023193
      agent-5:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8658362030982971
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014134332304820418
        model: {}
        policy_loss: -0.002434795256704092
        total_loss: -0.003932654857635498
        vf_explained_var: 0.007698580622673035
        vf_loss: 0.26014047861099243
    load_time_ms: 13560.657
    num_steps_sampled: 29184000
    num_steps_trained: 29184000
    sample_time_ms: 97829.01
    update_time_ms: 16.306
  iterations_since_restore: 144
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.298863636363635
    ram_util_percent: 11.411363636363637
  pid: 13408
  policy_reward_max:
    agent-0: 19.0
    agent-1: 6.0
    agent-2: 19.0
    agent-3: 16.0
    agent-4: 16.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 4.21
    agent-1: 1.71
    agent-2: 5.44
    agent-3: 4.01
    agent-4: 4.42
    agent-5: 3.18
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 1.0
    agent-3: 0.0
    agent-4: -97.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.794635361610094
    mean_inference_ms: 12.744070687492718
    mean_processing_ms: 57.73670565007482
  time_since_restore: 17776.45670247078
  time_this_iter_s: 123.17513227462769
  time_total_s: 40286.739944934845
  timestamp: 1637237753
  timesteps_since_restore: 13824000
  timesteps_this_iter: 96000
  timesteps_total: 29184000
  training_iteration: 304
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    304 |          40286.7 | 29184000 |    22.97 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 3.17
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 1.09
    apples_agent-1_min: 0
    apples_agent-2_max: 11
    apples_agent-2_mean: 2.11
    apples_agent-2_min: 0
    apples_agent-3_max: 29
    apples_agent-3_mean: 3.55
    apples_agent-3_min: 0
    apples_agent-4_max: 7
    apples_agent-4_mean: 1.55
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 1.88
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 122
    cleaning_beam_agent-0_mean: 63.81
    cleaning_beam_agent-0_min: 20
    cleaning_beam_agent-1_max: 353
    cleaning_beam_agent-1_mean: 216.95
    cleaning_beam_agent-1_min: 113
    cleaning_beam_agent-2_max: 39
    cleaning_beam_agent-2_mean: 10.16
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 227
    cleaning_beam_agent-3_mean: 88.34
    cleaning_beam_agent-3_min: 30
    cleaning_beam_agent-4_max: 199
    cleaning_beam_agent-4_mean: 44.85
    cleaning_beam_agent-4_min: 14
    cleaning_beam_agent-5_max: 40
    cleaning_beam_agent-5_mean: 8.04
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-17-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 58.0
  episode_reward_mean: 25.59
  episode_reward_min: -33.0
  episodes_this_iter: 96
  episodes_total: 29280
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11501.89
    learner:
      agent-0:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5086758732795715
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014564157463610172
        model: {}
        policy_loss: -0.0032873426098376513
        total_loss: -0.0041477615013718605
        vf_explained_var: -0.0002546757459640503
        vf_loss: 0.3485077917575836
      agent-1:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4490872919559479
        entropy_coeff: 0.0017600000137463212
        kl: 0.001060805981978774
        model: {}
        policy_loss: -0.0024148388765752316
        total_loss: -0.00318816676735878
        vf_explained_var: 0.022091060876846313
        vf_loss: 0.17064905166625977
      agent-2:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4072592854499817
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010374487610533834
        model: {}
        policy_loss: -0.003201995510607958
        total_loss: -0.0038701086305081844
        vf_explained_var: -0.001106947660446167
        vf_loss: 0.4866238832473755
      agent-3:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5963721871376038
        entropy_coeff: 0.0017600000137463212
        kl: 0.001073545659892261
        model: {}
        policy_loss: -0.00271578598767519
        total_loss: -0.0037291105836629868
        vf_explained_var: 0.0021225661039352417
        vf_loss: 0.36289912462234497
      agent-4:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6638888120651245
        entropy_coeff: 0.0017600000137463212
        kl: 0.001397341606207192
        model: {}
        policy_loss: -0.0038789515383541584
        total_loss: -0.0050003016367554665
        vf_explained_var: 0.01692233979701996
        vf_loss: 0.4709276556968689
      agent-5:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8653308153152466
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021169993560761213
        model: {}
        policy_loss: -0.002429400570690632
        total_loss: -0.003803870640695095
        vf_explained_var: 0.002405807375907898
        vf_loss: 1.4851151704788208
    load_time_ms: 13545.979
    num_steps_sampled: 29280000
    num_steps_trained: 29280000
    sample_time_ms: 97679.161
    update_time_ms: 16.284
  iterations_since_restore: 145
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.44825581395349
    ram_util_percent: 11.347674418604653
  pid: 13408
  policy_reward_max:
    agent-0: 12.0
    agent-1: 10.0
    agent-2: 14.0
    agent-3: 15.0
    agent-4: 15.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 4.39
    agent-1: 2.2
    agent-2: 5.61
    agent-3: 4.52
    agent-4: 5.85
    agent-5: 3.02
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -45.0
  sampler_perf:
    mean_env_wait_ms: 22.792048213686467
    mean_inference_ms: 12.743375628480564
    mean_processing_ms: 57.733766432002284
  time_since_restore: 17897.68017220497
  time_this_iter_s: 121.2234697341919
  time_total_s: 40407.96341466904
  timestamp: 1637237875
  timesteps_since_restore: 13920000
  timesteps_this_iter: 96000
  timesteps_total: 29280000
  training_iteration: 305
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    305 |            40408 | 29280000 |    25.59 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 3.02
    apples_agent-0_min: 0
    apples_agent-1_max: 15
    apples_agent-1_mean: 1.33
    apples_agent-1_min: 0
    apples_agent-2_max: 12
    apples_agent-2_mean: 2.57
    apples_agent-2_min: 0
    apples_agent-3_max: 36
    apples_agent-3_mean: 3.79
    apples_agent-3_min: 0
    apples_agent-4_max: 20
    apples_agent-4_mean: 1.87
    apples_agent-4_min: 0
    apples_agent-5_max: 9
    apples_agent-5_mean: 2.02
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 116
    cleaning_beam_agent-0_mean: 59.09
    cleaning_beam_agent-0_min: 23
    cleaning_beam_agent-1_max: 407
    cleaning_beam_agent-1_mean: 214.44
    cleaning_beam_agent-1_min: 122
    cleaning_beam_agent-2_max: 57
    cleaning_beam_agent-2_mean: 10.18
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 261
    cleaning_beam_agent-3_mean: 90.3
    cleaning_beam_agent-3_min: 35
    cleaning_beam_agent-4_max: 114
    cleaning_beam_agent-4_mean: 40.14
    cleaning_beam_agent-4_min: 8
    cleaning_beam_agent-5_max: 27
    cleaning_beam_agent-5_mean: 7.54
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-19-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 59.0
  episode_reward_mean: 25.99
  episode_reward_min: -28.0
  episodes_this_iter: 96
  episodes_total: 29376
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11500.346
    learner:
      agent-0:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5015236735343933
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012273435713723302
        model: {}
        policy_loss: -0.0031552251894026995
        total_loss: -0.00400198670104146
        vf_explained_var: 0.0006418675184249878
        vf_loss: 0.359177827835083
      agent-1:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4508676528930664
        entropy_coeff: 0.0017600000137463212
        kl: 0.001177340978756547
        model: {}
        policy_loss: -0.0024493513628840446
        total_loss: -0.0032303491607308388
        vf_explained_var: 0.026520848274230957
        vf_loss: 0.12528324127197266
      agent-2:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4064963459968567
        entropy_coeff: 0.0017600000137463212
        kl: 0.000796312524471432
        model: {}
        policy_loss: -0.0026532132178545
        total_loss: -0.0032063950784504414
        vf_explained_var: 0.0002865791320800781
        vf_loss: 1.6225192546844482
      agent-3:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5943424701690674
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014470805181190372
        model: {}
        policy_loss: -0.002879924373701215
        total_loss: -0.003884469624608755
        vf_explained_var: 0.002962157130241394
        vf_loss: 0.41500458121299744
      agent-4:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6656437516212463
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014197928830981255
        model: {}
        policy_loss: -0.0029230930376797915
        total_loss: -0.0039429934695363045
        vf_explained_var: 0.0019460171461105347
        vf_loss: 1.516363263130188
      agent-5:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8481174111366272
        entropy_coeff: 0.0017600000137463212
        kl: 0.002143316436558962
        model: {}
        policy_loss: -0.002670151414349675
        total_loss: -0.004131093621253967
        vf_explained_var: 0.013962432742118835
        vf_loss: 0.31741639971733093
    load_time_ms: 13550.367
    num_steps_sampled: 29376000
    num_steps_trained: 29376000
    sample_time_ms: 97684.943
    update_time_ms: 16.144
  iterations_since_restore: 146
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.26590909090909
    ram_util_percent: 11.432954545454544
  pid: 13408
  policy_reward_max:
    agent-0: 18.0
    agent-1: 7.0
    agent-2: 18.0
    agent-3: 16.0
    agent-4: 14.0
    agent-5: 17.0
  policy_reward_mean:
    agent-0: 4.35
    agent-1: 1.94
    agent-2: 5.57
    agent-3: 4.8
    agent-4: 5.55
    agent-5: 3.78
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -35.0
    agent-3: 0.0
    agent-4: -40.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.79100657102137
    mean_inference_ms: 12.742879604651643
    mean_processing_ms: 57.73256544528805
  time_since_restore: 18020.738997220993
  time_this_iter_s: 123.05882501602173
  time_total_s: 40531.02223968506
  timestamp: 1637237998
  timesteps_since_restore: 14016000
  timesteps_this_iter: 96000
  timesteps_total: 29376000
  training_iteration: 306
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    306 |            40531 | 29376000 |    25.99 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 2.79
    apples_agent-0_min: 0
    apples_agent-1_max: 12
    apples_agent-1_mean: 1.33
    apples_agent-1_min: 0
    apples_agent-2_max: 36
    apples_agent-2_mean: 2.8
    apples_agent-2_min: 0
    apples_agent-3_max: 20
    apples_agent-3_mean: 3.7
    apples_agent-3_min: 0
    apples_agent-4_max: 15
    apples_agent-4_mean: 1.47
    apples_agent-4_min: 0
    apples_agent-5_max: 18
    apples_agent-5_mean: 2.07
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 132
    cleaning_beam_agent-0_mean: 62.24
    cleaning_beam_agent-0_min: 18
    cleaning_beam_agent-1_max: 361
    cleaning_beam_agent-1_mean: 212.94
    cleaning_beam_agent-1_min: 121
    cleaning_beam_agent-2_max: 44
    cleaning_beam_agent-2_mean: 10.0
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 153
    cleaning_beam_agent-3_mean: 79.02
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 146
    cleaning_beam_agent-4_mean: 46.46
    cleaning_beam_agent-4_min: 8
    cleaning_beam_agent-5_max: 53
    cleaning_beam_agent-5_mean: 7.63
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-22-00
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 57.0
  episode_reward_mean: 26.63
  episode_reward_min: 7.0
  episodes_this_iter: 96
  episodes_total: 29472
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11483.995
    learner:
      agent-0:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5100571513175964
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013773395912721753
        model: {}
        policy_loss: -0.003111886093392968
        total_loss: -0.00397476414218545
        vf_explained_var: 0.0015456974506378174
        vf_loss: 0.34822192788124084
      agent-1:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.457186222076416
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012963487533852458
        model: {}
        policy_loss: -0.0025576993357390165
        total_loss: -0.0033491612412035465
        vf_explained_var: 0.019666746258735657
        vf_loss: 0.1318480372428894
      agent-2:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40365689992904663
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013054681476205587
        model: {}
        policy_loss: -0.0031064017675817013
        total_loss: -0.003770569572225213
        vf_explained_var: 0.00966787338256836
        vf_loss: 0.462671160697937
      agent-3:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.565179705619812
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017690726090222597
        model: {}
        policy_loss: -0.002861747518181801
        total_loss: -0.0038105277344584465
        vf_explained_var: 0.0021848678588867188
        vf_loss: 0.45936155319213867
      agent-4:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6714595556259155
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016013839049264789
        model: {}
        policy_loss: -0.0037652188912034035
        total_loss: -0.0048865219578146935
        vf_explained_var: 0.021126016974449158
        vf_loss: 0.6046739220619202
      agent-5:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8495579361915588
        entropy_coeff: 0.0017600000137463212
        kl: 0.001855173846706748
        model: {}
        policy_loss: -0.002802332630380988
        total_loss: -0.004271418787539005
        vf_explained_var: 0.008650049567222595
        vf_loss: 0.2613685727119446
    load_time_ms: 13541.896
    num_steps_sampled: 29472000
    num_steps_trained: 29472000
    sample_time_ms: 97605.655
    update_time_ms: 16.236
  iterations_since_restore: 147
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.39597701149425
    ram_util_percent: 11.42528735632184
  pid: 13408
  policy_reward_max:
    agent-0: 12.0
    agent-1: 9.0
    agent-2: 16.0
    agent-3: 15.0
    agent-4: 15.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 4.27
    agent-1: 2.11
    agent-2: 5.53
    agent-3: 5.14
    agent-4: 6.21
    agent-5: 3.37
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 1.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.78823525348275
    mean_inference_ms: 12.742380443237364
    mean_processing_ms: 57.7309592987334
  time_since_restore: 18142.85670042038
  time_this_iter_s: 122.1177031993866
  time_total_s: 40653.139942884445
  timestamp: 1637238120
  timesteps_since_restore: 14112000
  timesteps_this_iter: 96000
  timesteps_total: 29472000
  training_iteration: 307
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    307 |          40653.1 | 29472000 |    26.63 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 14
    apples_agent-0_mean: 3.01
    apples_agent-0_min: 0
    apples_agent-1_max: 24
    apples_agent-1_mean: 1.55
    apples_agent-1_min: 0
    apples_agent-2_max: 17
    apples_agent-2_mean: 2.17
    apples_agent-2_min: 0
    apples_agent-3_max: 20
    apples_agent-3_mean: 3.99
    apples_agent-3_min: 0
    apples_agent-4_max: 10
    apples_agent-4_mean: 1.54
    apples_agent-4_min: 0
    apples_agent-5_max: 17
    apples_agent-5_mean: 2.43
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 124
    cleaning_beam_agent-0_mean: 55.16
    cleaning_beam_agent-0_min: 23
    cleaning_beam_agent-1_max: 353
    cleaning_beam_agent-1_mean: 199.17
    cleaning_beam_agent-1_min: 88
    cleaning_beam_agent-2_max: 89
    cleaning_beam_agent-2_mean: 11.46
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 139
    cleaning_beam_agent-3_mean: 71.32
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 134
    cleaning_beam_agent-4_mean: 45.64
    cleaning_beam_agent-4_min: 11
    cleaning_beam_agent-5_max: 25
    cleaning_beam_agent-5_mean: 6.92
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-24-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 63.0
  episode_reward_mean: 25.44
  episode_reward_min: -70.0
  episodes_this_iter: 96
  episodes_total: 29568
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11483.031
    learner:
      agent-0:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5058934092521667
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011918845120817423
        model: {}
        policy_loss: -0.0031372662633657455
        total_loss: -0.003995654173195362
        vf_explained_var: -0.0013875365257263184
        vf_loss: 0.31983983516693115
      agent-1:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45321816205978394
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010000062175095081
        model: {}
        policy_loss: -0.0016133652534335852
        total_loss: -0.0022704170551151037
        vf_explained_var: 0.010369986295700073
        vf_loss: 1.4060982465744019
      agent-2:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.409360408782959
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011549113551154733
        model: {}
        policy_loss: -0.0016384301707148552
        total_loss: -0.0021768338046967983
        vf_explained_var: 0.000385090708732605
        vf_loss: 1.8207530975341797
      agent-3:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5621500015258789
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011912616901099682
        model: {}
        policy_loss: -0.0025808215141296387
        total_loss: -0.0035288503859192133
        vf_explained_var: -0.003400862216949463
        vf_loss: 0.4135269522666931
      agent-4:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6742832660675049
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014802139485254884
        model: {}
        policy_loss: -0.0040541114285588264
        total_loss: -0.005195644684135914
        vf_explained_var: 0.02196323871612549
        vf_loss: 0.4520709812641144
      agent-5:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8611921072006226
        entropy_coeff: 0.0017600000137463212
        kl: 0.001267909538000822
        model: {}
        policy_loss: -0.0026376827154308558
        total_loss: -0.004121208097785711
        vf_explained_var: 0.004973888397216797
        vf_loss: 0.3217259645462036
    load_time_ms: 13540.167
    num_steps_sampled: 29568000
    num_steps_trained: 29568000
    sample_time_ms: 97468.085
    update_time_ms: 15.59
  iterations_since_restore: 148
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.96551724137931
    ram_util_percent: 11.444827586206895
  pid: 13408
  policy_reward_max:
    agent-0: 12.0
    agent-1: 8.0
    agent-2: 14.0
    agent-3: 15.0
    agent-4: 14.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 4.08
    agent-1: 1.44
    agent-2: 5.17
    agent-3: 4.92
    agent-4: 5.75
    agent-5: 4.08
  policy_reward_min:
    agent-0: 0.0
    agent-1: -47.0
    agent-2: -44.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.785186781995193
    mean_inference_ms: 12.74162297438536
    mean_processing_ms: 57.7293619097174
  time_since_restore: 18265.126633882523
  time_this_iter_s: 122.26993346214294
  time_total_s: 40775.40987634659
  timestamp: 1637238243
  timesteps_since_restore: 14208000
  timesteps_this_iter: 96000
  timesteps_total: 29568000
  training_iteration: 308
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    308 |          40775.4 | 29568000 |    25.44 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 2.55
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 1.02
    apples_agent-1_min: 0
    apples_agent-2_max: 11
    apples_agent-2_mean: 2.01
    apples_agent-2_min: 0
    apples_agent-3_max: 102
    apples_agent-3_mean: 3.84
    apples_agent-3_min: 0
    apples_agent-4_max: 30
    apples_agent-4_mean: 1.8
    apples_agent-4_min: 0
    apples_agent-5_max: 27
    apples_agent-5_mean: 2.3
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 132
    cleaning_beam_agent-0_mean: 53.53
    cleaning_beam_agent-0_min: 20
    cleaning_beam_agent-1_max: 312
    cleaning_beam_agent-1_mean: 202.38
    cleaning_beam_agent-1_min: 103
    cleaning_beam_agent-2_max: 58
    cleaning_beam_agent-2_mean: 9.4
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 144
    cleaning_beam_agent-3_mean: 70.99
    cleaning_beam_agent-3_min: 25
    cleaning_beam_agent-4_max: 119
    cleaning_beam_agent-4_mean: 43.38
    cleaning_beam_agent-4_min: 10
    cleaning_beam_agent-5_max: 32
    cleaning_beam_agent-5_mean: 7.87
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-26-05
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 46.0
  episode_reward_mean: 24.14
  episode_reward_min: 4.0
  episodes_this_iter: 96
  episodes_total: 29664
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11495.287
    learner:
      agent-0:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48849809169769287
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012005064636468887
        model: {}
        policy_loss: -0.003121647983789444
        total_loss: -0.0039523011073470116
        vf_explained_var: 0.0010293126106262207
        vf_loss: 0.2910499572753906
      agent-1:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45902350544929504
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007041994831524789
        model: {}
        policy_loss: -0.0025066067464649677
        total_loss: -0.0033023578580468893
        vf_explained_var: 0.017100244760513306
        vf_loss: 0.12133681029081345
      agent-2:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.405705064535141
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013581572566181421
        model: {}
        policy_loss: -0.0032045720145106316
        total_loss: -0.0038733219262212515
        vf_explained_var: 0.003728032112121582
        vf_loss: 0.45290955901145935
      agent-3:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5702685117721558
        entropy_coeff: 0.0017600000137463212
        kl: 0.001060264534316957
        model: {}
        policy_loss: -0.002688429318368435
        total_loss: -0.00365848490037024
        vf_explained_var: 0.0014006197452545166
        vf_loss: 0.3361678719520569
      agent-4:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6707106828689575
        entropy_coeff: 0.0017600000137463212
        kl: 0.002177864545956254
        model: {}
        policy_loss: -0.004097448196262121
        total_loss: -0.005230185575783253
        vf_explained_var: 0.0019184648990631104
        vf_loss: 0.47709399461746216
      agent-5:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8554946780204773
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016009435057640076
        model: {}
        policy_loss: -0.002771481405943632
        total_loss: -0.0042518856935203075
        vf_explained_var: -0.0005260258913040161
        vf_loss: 0.2526577413082123
    load_time_ms: 13546.53
    num_steps_sampled: 29664000
    num_steps_trained: 29664000
    sample_time_ms: 97442.723
    update_time_ms: 14.807
  iterations_since_restore: 149
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.49080459770115
    ram_util_percent: 11.50287356321839
  pid: 13408
  policy_reward_max:
    agent-0: 15.0
    agent-1: 10.0
    agent-2: 17.0
    agent-3: 11.0
    agent-4: 17.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 4.0
    agent-1: 1.74
    agent-2: 5.43
    agent-3: 4.43
    agent-4: 5.27
    agent-5: 3.27
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.78194035939539
    mean_inference_ms: 12.740751982670604
    mean_processing_ms: 57.726792177115456
  time_since_restore: 18387.49995303154
  time_this_iter_s: 122.37331914901733
  time_total_s: 40897.783195495605
  timestamp: 1637238365
  timesteps_since_restore: 14304000
  timesteps_this_iter: 96000
  timesteps_total: 29664000
  training_iteration: 309
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    309 |          40897.8 | 29664000 |    24.14 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 3.25
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 1.01
    apples_agent-1_min: 0
    apples_agent-2_max: 27
    apples_agent-2_mean: 2.56
    apples_agent-2_min: 0
    apples_agent-3_max: 12
    apples_agent-3_mean: 3.18
    apples_agent-3_min: 0
    apples_agent-4_max: 21
    apples_agent-4_mean: 1.44
    apples_agent-4_min: 0
    apples_agent-5_max: 10
    apples_agent-5_mean: 1.78
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 133
    cleaning_beam_agent-0_mean: 50.43
    cleaning_beam_agent-0_min: 16
    cleaning_beam_agent-1_max: 365
    cleaning_beam_agent-1_mean: 197.25
    cleaning_beam_agent-1_min: 99
    cleaning_beam_agent-2_max: 38
    cleaning_beam_agent-2_mean: 9.66
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 187
    cleaning_beam_agent-3_mean: 66.93
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 217
    cleaning_beam_agent-4_mean: 40.57
    cleaning_beam_agent-4_min: 12
    cleaning_beam_agent-5_max: 27
    cleaning_beam_agent-5_mean: 8.49
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-28-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 63.0
  episode_reward_mean: 23.82
  episode_reward_min: 8.0
  episodes_this_iter: 96
  episodes_total: 29760
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11499.78
    learner:
      agent-0:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4941628873348236
        entropy_coeff: 0.0017600000137463212
        kl: 0.001351080252788961
        model: {}
        policy_loss: -0.003256864845752716
        total_loss: -0.004089750349521637
        vf_explained_var: -0.005145221948623657
        vf_loss: 0.3684048056602478
      agent-1:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4647127389907837
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008975313976407051
        model: {}
        policy_loss: -0.002304640132933855
        total_loss: -0.003110713791102171
        vf_explained_var: 0.008845120668411255
        vf_loss: 0.11824354529380798
      agent-2:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3949851095676422
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014534532092511654
        model: {}
        policy_loss: -0.003070421051234007
        total_loss: -0.0037035695277154446
        vf_explained_var: 0.0010834187269210815
        vf_loss: 0.6202411651611328
      agent-3:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5698335766792297
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014473031042143703
        model: {}
        policy_loss: -0.002745433710515499
        total_loss: -0.0037104138173162937
        vf_explained_var: 0.006796360015869141
        vf_loss: 0.37926265597343445
      agent-4:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6828555464744568
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018999690655618906
        model: {}
        policy_loss: -0.004187441430985928
        total_loss: -0.005348382517695427
        vf_explained_var: 0.018238261342048645
        vf_loss: 0.408852756023407
      agent-5:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8826801180839539
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015312915202230215
        model: {}
        policy_loss: -0.002855260856449604
        total_loss: -0.004384361673146486
        vf_explained_var: 0.0023384541273117065
        vf_loss: 0.24418044090270996
    load_time_ms: 13516.307
    num_steps_sampled: 29760000
    num_steps_trained: 29760000
    sample_time_ms: 97559.865
    update_time_ms: 14.963
  iterations_since_restore: 150
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.64
    ram_util_percent: 11.446857142857143
  pid: 13408
  policy_reward_max:
    agent-0: 12.0
    agent-1: 8.0
    agent-2: 21.0
    agent-3: 13.0
    agent-4: 13.0
    agent-5: 16.0
  policy_reward_mean:
    agent-0: 4.08
    agent-1: 1.62
    agent-2: 5.43
    agent-3: 4.49
    agent-4: 5.09
    agent-5: 3.11
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.779535679927662
    mean_inference_ms: 12.740841982822582
    mean_processing_ms: 57.72869384211816
  time_since_restore: 18510.51782464981
  time_this_iter_s: 123.01787161827087
  time_total_s: 41020.801067113876
  timestamp: 1637238488
  timesteps_since_restore: 14400000
  timesteps_this_iter: 96000
  timesteps_total: 29760000
  training_iteration: 310
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    310 |          41020.8 | 29760000 |    23.82 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 2.76
    apples_agent-0_min: 0
    apples_agent-1_max: 14
    apples_agent-1_mean: 1.21
    apples_agent-1_min: 0
    apples_agent-2_max: 35
    apples_agent-2_mean: 2.47
    apples_agent-2_min: 0
    apples_agent-3_max: 34
    apples_agent-3_mean: 3.97
    apples_agent-3_min: 0
    apples_agent-4_max: 85
    apples_agent-4_mean: 2.17
    apples_agent-4_min: 0
    apples_agent-5_max: 10
    apples_agent-5_mean: 1.79
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 95
    cleaning_beam_agent-0_mean: 45.64
    cleaning_beam_agent-0_min: 10
    cleaning_beam_agent-1_max: 346
    cleaning_beam_agent-1_mean: 197.12
    cleaning_beam_agent-1_min: 91
    cleaning_beam_agent-2_max: 63
    cleaning_beam_agent-2_mean: 8.19
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 198
    cleaning_beam_agent-3_mean: 72.02
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 162
    cleaning_beam_agent-4_mean: 43.71
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 39
    cleaning_beam_agent-5_mean: 8.35
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-30-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 102.0
  episode_reward_mean: 26.2
  episode_reward_min: 4.0
  episodes_this_iter: 96
  episodes_total: 29856
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11539.75
    learner:
      agent-0:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4951840043067932
        entropy_coeff: 0.0017600000137463212
        kl: 0.00122356740757823
        model: {}
        policy_loss: -0.003354838117957115
        total_loss: -0.004196694120764732
        vf_explained_var: 0.0037494897842407227
        vf_loss: 0.2966734766960144
      agent-1:
        cur_kl_coeff: 6.018531165893214e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45487821102142334
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007641444681212306
        model: {}
        policy_loss: -0.0020526284351944923
        total_loss: -0.002835768274962902
        vf_explained_var: 0.014852240681648254
        vf_loss: 0.17443598806858063
      agent-2:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3858787417411804
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011138698318973184
        model: {}
        policy_loss: -0.003121756250038743
        total_loss: -0.00374291162006557
        vf_explained_var: 0.0036614984273910522
        vf_loss: 0.5799096822738647
      agent-3:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5738843679428101
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011167952325195074
        model: {}
        policy_loss: -0.0025534844025969505
        total_loss: -0.0035099945962429047
        vf_explained_var: -0.0014953911304473877
        vf_loss: 0.535253643989563
      agent-4:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6738872528076172
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012460146099328995
        model: {}
        policy_loss: -0.0035317568108439445
        total_loss: -0.004667951725423336
        vf_explained_var: 0.006124615669250488
        vf_loss: 0.4984831213951111
      agent-5:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8687126040458679
        entropy_coeff: 0.0017600000137463212
        kl: 0.001666465774178505
        model: {}
        policy_loss: -0.002580335596576333
        total_loss: -0.004071498289704323
        vf_explained_var: 0.008109554648399353
        vf_loss: 0.3777245283126831
    load_time_ms: 13552.211
    num_steps_sampled: 29856000
    num_steps_trained: 29856000
    sample_time_ms: 97446.611
    update_time_ms: 14.998
  iterations_since_restore: 151
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.002272727272725
    ram_util_percent: 11.467045454545454
  pid: 13408
  policy_reward_max:
    agent-0: 15.0
    agent-1: 17.0
    agent-2: 22.0
    agent-3: 27.0
    agent-4: 14.0
    agent-5: 25.0
  policy_reward_mean:
    agent-0: 3.88
    agent-1: 1.83
    agent-2: 6.16
    agent-3: 4.87
    agent-4: 5.77
    agent-5: 3.69
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -1.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.776341438726345
    mean_inference_ms: 12.740029198309522
    mean_processing_ms: 57.727572886311485
  time_since_restore: 18633.46127796173
  time_this_iter_s: 122.94345331192017
  time_total_s: 41143.7445204258
  timestamp: 1637238612
  timesteps_since_restore: 14496000
  timesteps_this_iter: 96000
  timesteps_total: 29856000
  training_iteration: 311
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    311 |          41143.7 | 29856000 |     26.2 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 2.62
    apples_agent-0_min: 0
    apples_agent-1_max: 9
    apples_agent-1_mean: 1.2
    apples_agent-1_min: 0
    apples_agent-2_max: 23
    apples_agent-2_mean: 2.37
    apples_agent-2_min: 0
    apples_agent-3_max: 12
    apples_agent-3_mean: 3.0
    apples_agent-3_min: 0
    apples_agent-4_max: 14
    apples_agent-4_mean: 1.63
    apples_agent-4_min: 0
    apples_agent-5_max: 10
    apples_agent-5_mean: 1.74
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 125
    cleaning_beam_agent-0_mean: 45.98
    cleaning_beam_agent-0_min: 12
    cleaning_beam_agent-1_max: 348
    cleaning_beam_agent-1_mean: 208.36
    cleaning_beam_agent-1_min: 92
    cleaning_beam_agent-2_max: 58
    cleaning_beam_agent-2_mean: 8.89
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 144
    cleaning_beam_agent-3_mean: 74.66
    cleaning_beam_agent-3_min: 24
    cleaning_beam_agent-4_max: 114
    cleaning_beam_agent-4_mean: 42.85
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 25
    cleaning_beam_agent-5_mean: 8.29
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-32-15
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 61.0
  episode_reward_mean: 25.58
  episode_reward_min: 3.0
  episodes_this_iter: 96
  episodes_total: 29952
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11544.876
    learner:
      agent-0:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48753273487091064
        entropy_coeff: 0.0017600000137463212
        kl: 0.001360561465844512
        model: {}
        policy_loss: -0.00299988710321486
        total_loss: -0.0038224910385906696
        vf_explained_var: -0.0023661255836486816
        vf_loss: 0.3545442819595337
      agent-1:
        cur_kl_coeff: 3.009265582946607e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.445808082818985
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009457727428525686
        model: {}
        policy_loss: -0.0025469346437603235
        total_loss: -0.003317903960123658
        vf_explained_var: 0.0062750279903411865
        vf_loss: 0.13653579354286194
      agent-2:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3925252854824066
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015468760393559933
        model: {}
        policy_loss: -0.003282652236521244
        total_loss: -0.0039258114993572235
        vf_explained_var: 0.0043348222970962524
        vf_loss: 0.47688591480255127
      agent-3:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5850347280502319
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011689369566738605
        model: {}
        policy_loss: -0.002546298783272505
        total_loss: -0.0035443580709397793
        vf_explained_var: 0.0007911622524261475
        vf_loss: 0.31602033972740173
      agent-4:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6689196825027466
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013282370055094361
        model: {}
        policy_loss: -0.003768559079617262
        total_loss: -0.004897384438663721
        vf_explained_var: 0.024944737553596497
        vf_loss: 0.48473435640335083
      agent-5:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8562670946121216
        entropy_coeff: 0.0017600000137463212
        kl: 0.00123903201892972
        model: {}
        policy_loss: -0.002591107040643692
        total_loss: -0.004071277566254139
        vf_explained_var: 0.008242174983024597
        vf_loss: 0.2685827314853668
    load_time_ms: 13560.377
    num_steps_sampled: 29952000
    num_steps_trained: 29952000
    sample_time_ms: 97377.494
    update_time_ms: 15.096
  iterations_since_restore: 152
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.186857142857143
    ram_util_percent: 11.42114285714286
  pid: 13408
  policy_reward_max:
    agent-0: 12.0
    agent-1: 9.0
    agent-2: 20.0
    agent-3: 13.0
    agent-4: 18.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 4.25
    agent-1: 1.9
    agent-2: 5.84
    agent-3: 4.19
    agent-4: 6.0
    agent-5: 3.4
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.77358730368242
    mean_inference_ms: 12.739426079604259
    mean_processing_ms: 57.72554886301149
  time_since_restore: 18756.275013446808
  time_this_iter_s: 122.8137354850769
  time_total_s: 41266.55825591087
  timestamp: 1637238735
  timesteps_since_restore: 14592000
  timesteps_this_iter: 96000
  timesteps_total: 29952000
  training_iteration: 312
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.43 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.5:13408 |    312 |          41266.6 | 29952000 |    25.58 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=13408)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbe4ed9f588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 14
    apples_agent-0_mean: 2.62
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 1.04
    apples_agent-1_min: 0
    apples_agent-2_max: 20
    apples_agent-2_mean: 2.62
    apples_agent-2_min: 0
    apples_agent-3_max: 14
    apples_agent-3_mean: 3.2
    apples_agent-3_min: 0
    apples_agent-4_max: 20
    apples_agent-4_mean: 1.38
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 1.82
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 103
    cleaning_beam_agent-0_mean: 44.03
    cleaning_beam_agent-0_min: 12
    cleaning_beam_agent-1_max: 432
    cleaning_beam_agent-1_mean: 197.72
    cleaning_beam_agent-1_min: 82
    cleaning_beam_agent-2_max: 52
    cleaning_beam_agent-2_mean: 9.92
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 163
    cleaning_beam_agent-3_mean: 77.88
    cleaning_beam_agent-3_min: 23
    cleaning_beam_agent-4_max: 99
    cleaning_beam_agent-4_mean: 35.49
    cleaning_beam_agent-4_min: 10
    cleaning_beam_agent-5_max: 22
    cleaning_beam_agent-5_mean: 8.27
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-34-17
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 55.0
  episode_reward_mean: 23.54
  episode_reward_min: -70.0
  episodes_this_iter: 96
  episodes_total: 30048
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu005
  info:
    grad_time_ms: 11534.692
    learner:
      agent-0:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4894546866416931
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008623707108199596
        model: {}
        policy_loss: -0.0025550550781190395
        total_loss: -0.0033025378361344337
        vf_explained_var: 0.0003047436475753784
        vf_loss: 1.1395639181137085
      agent-1:
        cur_kl_coeff: 1.5046327914733034e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4425092339515686
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010848904494196177
        model: {}
        policy_loss: -0.002559262327849865
        total_loss: -0.0033261720091104507
        vf_explained_var: 0.02097196877002716
        vf_loss: 0.11907695978879929
      agent-2:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39247724413871765
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012121954932808876
        model: {}
        policy_loss: -0.0016253553330898285
        total_loss: -0.0021453953813761473
        vf_explained_var: -0.0011030882596969604
        vf_loss: 1.7071853876113892
      agent-3:
        cur_kl_coeff: 6.018531165893214e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5864614248275757
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012243175879120827
        model: {}
        policy_loss: -0.002389889443293214
        total_loss: -0.0033868392929434776
        vf_explained_var: -0.0031702816486358643
        vf_loss: 0.3521949052810669
      agent-4:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6777807474136353
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018070614896714687
        model: {}
        policy_loss: -0.003945114556699991
        total_loss: -0.005091838072985411
        vf_explained_var: 0.00993308424949646
        vf_loss: 0.46171247959136963
      agent-5:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8745108842849731
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020185415633022785
        model: {}
        policy_loss: -0.001776853809133172
        total_loss: -0.003161100670695305
        vf_explained_var: -0.00030490756034851074
        vf_loss: 1.5489150285720825
    load_time_ms: 13539.581
    num_steps_sampled: 30048000
    num_steps_trained: 30048000
    sample_time_ms: 97288.682
    update_time_ms: 15.131
  iterations_since_restore: 153
  node_ip: 172.17.8.5
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.320114942528736
    ram_util_percent: 11.285632183908048
  pid: 13408
  policy_reward_max:
    agent-0: 15.0
    agent-1: 9.0
    agent-2: 14.0
    agent-3: 15.0
    agent-4: 14.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 3.84
    agent-1: 1.83
    agent-2: 4.93
    agent-3: 4.33
    agent-4: 5.77
    agent-5: 2.84
  policy_reward_min:
    agent-0: -46.0
    agent-1: 0.0
    agent-2: -46.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -49.0
  sampler_perf:
    mean_env_wait_ms: 22.77053956876835
    mean_inference_ms: 12.73856322981343
    mean_processing_ms: 57.72306323379887
  time_since_restore: 18878.025923490524
  time_this_iter_s: 121.75091004371643
  time_total_s: 41388.30916595459
  timestamp: 1637238857
  timesteps_since_restore: 14688000
  timesteps_this_iter: 96000
  timesteps_total: 30048000
  training_iteration: 313
  trial_id: '00000'
  