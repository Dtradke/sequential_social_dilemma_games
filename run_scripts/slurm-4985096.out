>>> RESTORING FROM:  /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics/BaselinePPOTrainer_cleanup_env_0_2021-11-15_17-00-45_bf59qjr/checkpoint_20
== Status ==
Memory usage on this node: 15.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------+--------+------------------+---------+----------+
| Trial name                           | status   | loc   |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  |       |     20 |          3211.06 | 1920000 |    18.95 |
+--------------------------------------+----------+-------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m 2021-11-15 18:03:03,628	INFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution
[2m[36m(pid=4061)[0m 2021-11-15 18:03:03,643	INFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=4061)[0m 2021-11-15 18:04:48,852	INFO trainable.py:180 -- _setup took 105.223 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(pid=4061)[0m 2021-11-15 18:04:48,852	INFO trainable.py:217 -- Getting current IP.
[2m[36m(pid=4061)[0m 2021-11-15 18:04:48,852	WARNING util.py:37 -- Install gputil for GPU system monitoring.
[2m[36m(pid=4061)[0m 2021-11-15 18:04:51,772	INFO trainable.py:217 -- Getting current IP.
[2m[36m(pid=4061)[0m 2021-11-15 18:04:51,772	INFO trainable.py:423 -- Restored on 172.17.8.124 from checkpoint: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics/BaselinePPOTrainer_cleanup_env_0_2021-11-15_17-00-45_bf59qjr/tmp68359u75restore_from_object/checkpoint-20
[2m[36m(pid=4061)[0m 2021-11-15 18:04:51,772	INFO trainable.py:430 -- Current state after restoring: {'_iteration': 20, '_timesteps_total': 1920000, '_time_total': 3211.063686132431, '_episodes_total': 1920}
== Status ==
Memory usage on this node: 24.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------+--------+------------------+---------+----------+
| Trial name                           | status   | loc   |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  |       |     20 |          3211.06 | 1920000 |    18.95 |
+--------------------------------------+----------+-------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 27
    apples_agent-0_mean: 5.145833333333333
    apples_agent-0_min: 0
    apples_agent-1_max: 20
    apples_agent-1_mean: 4.21875
    apples_agent-1_min: 0
    apples_agent-2_max: 17
    apples_agent-2_mean: 5.28125
    apples_agent-2_min: 0
    apples_agent-3_max: 39
    apples_agent-3_mean: 6.333333333333333
    apples_agent-3_min: 0
    apples_agent-4_max: 25
    apples_agent-4_mean: 4.125
    apples_agent-4_min: 0
    apples_agent-5_max: 40
    apples_agent-5_mean: 7.802083333333333
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 159
    cleaning_beam_agent-0_mean: 130.21875
    cleaning_beam_agent-0_min: 106
    cleaning_beam_agent-1_max: 224
    cleaning_beam_agent-1_mean: 182.69791666666666
    cleaning_beam_agent-1_min: 153
    cleaning_beam_agent-2_max: 291
    cleaning_beam_agent-2_mean: 241.67708333333334
    cleaning_beam_agent-2_min: 204
    cleaning_beam_agent-3_max: 116
    cleaning_beam_agent-3_mean: 92.47916666666667
    cleaning_beam_agent-3_min: 72
    cleaning_beam_agent-4_max: 172
    cleaning_beam_agent-4_mean: 133.91666666666666
    cleaning_beam_agent-4_min: 109
    cleaning_beam_agent-5_max: 117
    cleaning_beam_agent-5_mean: 94.26041666666667
    cleaning_beam_agent-5_min: 71
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.16666666666666666
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.13541666666666666
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 3
    fire_beam_agent-2_mean: 0.3229166666666667
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 3
    fire_beam_agent-3_mean: 0.4791666666666667
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.2708333333333333
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.16666666666666666
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_18-08-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 124.00000000000138
  episode_reward_mean: 32.51041666666668
  episode_reward_min: -137.00000000000034
  episodes_this_iter: 96
  episodes_total: 2016
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 31469.419
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0011401920346543193
        entropy: 1.8605456352233887
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012954367557540536
        model: {}
        policy_loss: -0.0008486416190862656
        total_loss: -0.0037101786583662033
        vf_explained_var: -0.0015935897827148438
        vf_loss: 1.539335012435913
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0011401920346543193
        entropy: 2.0150516033172607
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019622324034571648
        model: {}
        policy_loss: -0.0012434376403689384
        total_loss: -0.004244908690452576
        vf_explained_var: 0.0010368674993515015
        vf_loss: 1.5257481336593628
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0011401920346543193
        entropy: 1.8831274509429932
        entropy_coeff: 0.0017600000137463212
        kl: 0.004943071398884058
        model: {}
        policy_loss: -0.005248939618468285
        total_loss: -0.007421096321195364
        vf_explained_var: 0.0043481141328811646
        vf_loss: 1.5353562831878662
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0011401920346543193
        entropy: 1.9607901573181152
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019111785804852843
        model: {}
        policy_loss: -0.0012153754942119122
        total_loss: -0.0041298335418105125
        vf_explained_var: 0.0023707151412963867
        vf_loss: 1.5429723262786865
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0011401920346543193
        entropy: 1.8227348327636719
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021387820597738028
        model: {}
        policy_loss: -0.0017837835475802422
        total_loss: -0.004414969123899937
        vf_explained_var: 0.002913042902946472
        vf_loss: 1.490714430809021
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0011401920346543193
        entropy: 1.8690834045410156
        entropy_coeff: 0.0017600000137463212
        kl: 0.002231467515230179
        model: {}
        policy_loss: -0.0014693900011479855
        total_loss: -0.00416040513664484
        vf_explained_var: 0.0020844191312789917
        vf_loss: 1.522787094116211
    load_time_ms: 47207.094
    num_steps_sampled: 2016000
    num_steps_trained: 2016000
    sample_time_ms: 129967.233
    update_time_ms: 3019.663
  iterations_since_restore: 1
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 18.35465838509317
    ram_util_percent: 14.995652173913044
  pid: 4061
  policy_reward_max:
    agent-0: 20.66666666666667
    agent-1: 20.66666666666667
    agent-2: 20.66666666666667
    agent-3: 20.66666666666667
    agent-4: 20.66666666666667
    agent-5: 20.66666666666667
  policy_reward_mean:
    agent-0: 5.418402777777778
    agent-1: 5.418402777777778
    agent-2: 5.418402777777778
    agent-3: 5.418402777777778
    agent-4: 5.418402777777778
    agent-5: 5.418402777777778
  policy_reward_min:
    agent-0: -22.833333333333318
    agent-1: -22.833333333333318
    agent-2: -22.833333333333318
    agent-3: -22.833333333333318
    agent-4: -22.833333333333318
    agent-5: -22.833333333333318
  sampler_perf:
    mean_env_wait_ms: 22.605887818566732
    mean_inference_ms: 14.060025408868986
    mean_processing_ms: 53.3573053457163
  time_since_restore: 216.63467049598694
  time_this_iter_s: 216.63467049598694
  time_total_s: 3427.698356628418
  timestamp: 1637017714
  timesteps_since_restore: 96000
  timesteps_this_iter: 96000
  timesteps_total: 2016000
  training_iteration: 21
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     21 |           3427.7 | 2016000 |  32.5104 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 5.55
    apples_agent-0_min: 0
    apples_agent-1_max: 25
    apples_agent-1_mean: 3.32
    apples_agent-1_min: 0
    apples_agent-2_max: 31
    apples_agent-2_mean: 5.68
    apples_agent-2_min: 0
    apples_agent-3_max: 40
    apples_agent-3_mean: 7.09
    apples_agent-3_min: 0
    apples_agent-4_max: 22
    apples_agent-4_mean: 5.67
    apples_agent-4_min: 0
    apples_agent-5_max: 48
    apples_agent-5_mean: 8.12
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 185
    cleaning_beam_agent-0_mean: 155.73
    cleaning_beam_agent-0_min: 126
    cleaning_beam_agent-1_max: 277
    cleaning_beam_agent-1_mean: 239.62
    cleaning_beam_agent-1_min: 170
    cleaning_beam_agent-2_max: 285
    cleaning_beam_agent-2_mean: 241.53
    cleaning_beam_agent-2_min: 200
    cleaning_beam_agent-3_max: 114
    cleaning_beam_agent-3_mean: 93.96
    cleaning_beam_agent-3_min: 74
    cleaning_beam_agent-4_max: 157
    cleaning_beam_agent-4_mean: 120.77
    cleaning_beam_agent-4_min: 95
    cleaning_beam_agent-5_max: 128
    cleaning_beam_agent-5_mean: 100.96
    cleaning_beam_agent-5_min: 74
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.07
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.11
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.21
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.37
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.16
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.09
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_18-11-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 143.0000000000006
  episode_reward_mean: 41.85999999999994
  episode_reward_min: -169.00000000000193
  episodes_this_iter: 96
  episodes_total: 2112
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 26053.711
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0011342016514390707
        entropy: 1.8546847105026245
        entropy_coeff: 0.0017600000137463212
        kl: 0.0061659011989831924
        model: {}
        policy_loss: -0.002919556573033333
        total_loss: -0.005445435177534819
        vf_explained_var: -0.006688803434371948
        vf_loss: 1.217774748802185
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0011342016514390707
        entropy: 2.0030221939086914
        entropy_coeff: 0.0017600000137463212
        kl: 0.007749214768409729
        model: {}
        policy_loss: -0.0038934580516070127
        total_loss: -0.006523342803120613
        vf_explained_var: 0.0010106861591339111
        vf_loss: 1.2051197290420532
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0011342016514390707
        entropy: 1.8474289178848267
        entropy_coeff: 0.0017600000137463212
        kl: 0.009699126705527306
        model: {}
        policy_loss: -0.006734413560479879
        total_loss: -0.00889590010046959
        vf_explained_var: 0.004744231700897217
        vf_loss: 1.2007520198822021
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0011342016514390707
        entropy: 1.938385248184204
        entropy_coeff: 0.0017600000137463212
        kl: 0.006138836964964867
        model: {}
        policy_loss: -0.0028119892813265324
        total_loss: -0.005489223171025515
        vf_explained_var: 0.002404451370239258
        vf_loss: 1.2044048309326172
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0011342016514390707
        entropy: 1.8095951080322266
        entropy_coeff: 0.0017600000137463212
        kl: 0.005180727690458298
        model: {}
        policy_loss: -0.0025421734899282455
        total_loss: -0.005086896941065788
        vf_explained_var: -0.011730670928955078
        vf_loss: 1.220889925956726
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0011342016514390707
        entropy: 1.928562045097351
        entropy_coeff: 0.0017600000137463212
        kl: 0.006047683302313089
        model: {}
        policy_loss: -0.0022061108611524105
        total_loss: -0.004875789862126112
        vf_explained_var: 0.006290853023529053
        vf_loss: 1.1982061862945557
    load_time_ms: 48163.237
    num_steps_sampled: 2112000
    num_steps_trained: 2112000
    sample_time_ms: 123789.201
    update_time_ms: 1601.047
  iterations_since_restore: 2
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 18.708759124087592
    ram_util_percent: 15.562043795620438
  pid: 4061
  policy_reward_max:
    agent-0: 23.833333333333364
    agent-1: 23.833333333333364
    agent-2: 23.833333333333364
    agent-3: 23.833333333333364
    agent-4: 23.833333333333364
    agent-5: 23.833333333333364
  policy_reward_mean:
    agent-0: 6.976666666666665
    agent-1: 6.976666666666665
    agent-2: 6.976666666666665
    agent-3: 6.976666666666665
    agent-4: 6.976666666666665
    agent-5: 6.976666666666665
  policy_reward_min:
    agent-0: -28.166666666666647
    agent-1: -28.166666666666647
    agent-2: -28.166666666666647
    agent-3: -28.166666666666647
    agent-4: -28.166666666666647
    agent-5: -28.166666666666647
  sampler_perf:
    mean_env_wait_ms: 23.004269688904266
    mean_inference_ms: 13.553351822413891
    mean_processing_ms: 53.02259457137777
  time_since_restore: 404.4222764968872
  time_this_iter_s: 187.78760600090027
  time_total_s: 3615.4859626293182
  timestamp: 1637017907
  timesteps_since_restore: 192000
  timesteps_this_iter: 96000
  timesteps_total: 2112000
  training_iteration: 22
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     22 |          3615.49 | 2112000 |    41.86 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 27
    apples_agent-0_mean: 5.58
    apples_agent-0_min: 0
    apples_agent-1_max: 23
    apples_agent-1_mean: 5.92
    apples_agent-1_min: 0
    apples_agent-2_max: 49
    apples_agent-2_mean: 7.72
    apples_agent-2_min: 0
    apples_agent-3_max: 51
    apples_agent-3_mean: 9.4
    apples_agent-3_min: 0
    apples_agent-4_max: 32
    apples_agent-4_mean: 7.39
    apples_agent-4_min: 0
    apples_agent-5_max: 58
    apples_agent-5_mean: 10.69
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 287
    cleaning_beam_agent-0_mean: 240.62
    cleaning_beam_agent-0_min: 150
    cleaning_beam_agent-1_max: 256
    cleaning_beam_agent-1_mean: 214.35
    cleaning_beam_agent-1_min: 176
    cleaning_beam_agent-2_max: 298
    cleaning_beam_agent-2_mean: 256.66
    cleaning_beam_agent-2_min: 205
    cleaning_beam_agent-3_max: 162
    cleaning_beam_agent-3_mean: 129.87
    cleaning_beam_agent-3_min: 89
    cleaning_beam_agent-4_max: 127
    cleaning_beam_agent-4_mean: 101.01
    cleaning_beam_agent-4_min: 66
    cleaning_beam_agent-5_max: 146
    cleaning_beam_agent-5_mean: 123.02
    cleaning_beam_agent-5_min: 86
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.12
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.17
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.06
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_18-14-48
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 165.9999999999991
  episode_reward_mean: 66.5800000000001
  episode_reward_min: -115.00000000000082
  episodes_this_iter: 96
  episodes_total: 2208
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 24203.867
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0011282111518085003
        entropy: 1.881791114807129
        entropy_coeff: 0.0017600000137463212
        kl: 0.006870262324810028
        model: {}
        policy_loss: -0.004374874755740166
        total_loss: -0.006896809674799442
        vf_explained_var: -0.01781710982322693
        vf_loss: 1.0298932790756226
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0011282111518085003
        entropy: 2.005859851837158
        entropy_coeff: 0.0017600000137463212
        kl: 0.009429916739463806
        model: {}
        policy_loss: -0.005464638117700815
        total_loss: -0.007951725274324417
        vf_explained_var: 0.007019832730293274
        vf_loss: 1.0023332834243774
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0011282111518085003
        entropy: 1.8376022577285767
        entropy_coeff: 0.0017600000137463212
        kl: 0.009712649509310722
        model: {}
        policy_loss: -0.008816398680210114
        total_loss: -0.010980358347296715
        vf_explained_var: 0.020229056477546692
        vf_loss: 0.9895716905593872
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0011282111518085003
        entropy: 1.9375615119934082
        entropy_coeff: 0.0017600000137463212
        kl: 0.008770589716732502
        model: {}
        policy_loss: -0.0033338582143187523
        total_loss: -0.005766112357378006
        vf_explained_var: 0.0020757466554641724
        vf_loss: 1.0079421997070312
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0011282111518085003
        entropy: 1.7858352661132812
        entropy_coeff: 0.0017600000137463212
        kl: 0.008333183825016022
        model: {}
        policy_loss: -0.005699602887034416
        total_loss: -0.007900713011622429
        vf_explained_var: -0.0743473470211029
        vf_loss: 1.086415410041809
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0011282111518085003
        entropy: 1.9131375551223755
        entropy_coeff: 0.0017600000137463212
        kl: 0.00997929833829403
        model: {}
        policy_loss: -0.004564931616187096
        total_loss: -0.0068343086168169975
        vf_explained_var: 0.011820465326309204
        vf_loss: 0.9981589317321777
    load_time_ms: 47291.983
    num_steps_sampled: 2208000
    num_steps_trained: 2208000
    sample_time_ms: 120547.94
    update_time_ms: 1089.733
  iterations_since_restore: 3
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 19.18217054263566
    ram_util_percent: 15.57248062015504
  pid: 4061
  policy_reward_max:
    agent-0: 27.666666666666718
    agent-1: 27.666666666666718
    agent-2: 27.666666666666718
    agent-3: 27.666666666666718
    agent-4: 27.666666666666718
    agent-5: 27.666666666666718
  policy_reward_mean:
    agent-0: 11.096666666666668
    agent-1: 11.096666666666668
    agent-2: 11.096666666666668
    agent-3: 11.096666666666668
    agent-4: 11.096666666666668
    agent-5: 11.096666666666668
  policy_reward_min:
    agent-0: -19.166666666666625
    agent-1: -19.166666666666625
    agent-2: -19.166666666666625
    agent-3: -19.166666666666625
    agent-4: -19.166666666666625
    agent-5: -19.166666666666625
  sampler_perf:
    mean_env_wait_ms: 23.198550319355096
    mean_inference_ms: 13.361952784756163
    mean_processing_ms: 53.097583997892855
  time_since_restore: 584.7396199703217
  time_this_iter_s: 180.31734347343445
  time_total_s: 3795.8033061027527
  timestamp: 1637018088
  timesteps_since_restore: 288000
  timesteps_this_iter: 96000
  timesteps_total: 2208000
  training_iteration: 23
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     23 |           3795.8 | 2208000 |    66.58 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 37
    apples_agent-0_mean: 4.56
    apples_agent-0_min: 0
    apples_agent-1_max: 39
    apples_agent-1_mean: 6.47
    apples_agent-1_min: 0
    apples_agent-2_max: 39
    apples_agent-2_mean: 10.25
    apples_agent-2_min: 0
    apples_agent-3_max: 61
    apples_agent-3_mean: 13.32
    apples_agent-3_min: 0
    apples_agent-4_max: 66
    apples_agent-4_mean: 9.8
    apples_agent-4_min: 0
    apples_agent-5_max: 51
    apples_agent-5_mean: 12.21
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 339
    cleaning_beam_agent-0_mean: 290.14
    cleaning_beam_agent-0_min: 241
    cleaning_beam_agent-1_max: 384
    cleaning_beam_agent-1_mean: 319.48
    cleaning_beam_agent-1_min: 182
    cleaning_beam_agent-2_max: 334
    cleaning_beam_agent-2_mean: 287.64
    cleaning_beam_agent-2_min: 233
    cleaning_beam_agent-3_max: 175
    cleaning_beam_agent-3_mean: 146.77
    cleaning_beam_agent-3_min: 121
    cleaning_beam_agent-4_max: 120
    cleaning_beam_agent-4_mean: 88.12
    cleaning_beam_agent-4_min: 50
    cleaning_beam_agent-5_max: 215
    cleaning_beam_agent-5_mean: 170.86
    cleaning_beam_agent-5_min: 104
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.08
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.09
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.22
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.17
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.29
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_18-17-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 176.99999999999892
  episode_reward_mean: 77.77000000000012
  episode_reward_min: -40.99999999999964
  episodes_this_iter: 96
  episodes_total: 2304
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 23238.956
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0011222207685932517
        entropy: 1.8329260349273682
        entropy_coeff: 0.0017600000137463212
        kl: 0.010435916483402252
        model: {}
        policy_loss: -0.006114828400313854
        total_loss: -0.008105211891233921
        vf_explained_var: -0.012027770280838013
        vf_loss: 1.9197427034378052
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0011222207685932517
        entropy: 1.9080079793930054
        entropy_coeff: 0.0017600000137463212
        kl: 0.009357478469610214
        model: {}
        policy_loss: -0.007272916845977306
        total_loss: -0.009506471455097198
        vf_explained_var: 0.005289122462272644
        vf_loss: 1.8879022598266602
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0011222207685932517
        entropy: 1.8035049438476562
        entropy_coeff: 0.0017600000137463212
        kl: 0.008719297125935555
        model: {}
        policy_loss: -0.008452958427369595
        total_loss: -0.010568605735898018
        vf_explained_var: 0.01651422679424286
        vf_loss: 1.8659131526947021
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0011222207685932517
        entropy: 1.9290332794189453
        entropy_coeff: 0.0017600000137463212
        kl: 0.0057294052094221115
        model: {}
        policy_loss: -0.0036296548787504435
        total_loss: -0.00626165559515357
        vf_explained_var: -0.0026417970657348633
        vf_loss: 1.9015967845916748
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0011222207685932517
        entropy: 1.8236266374588013
        entropy_coeff: 0.0017600000137463212
        kl: 0.009058257564902306
        model: {}
        policy_loss: -0.007987278513610363
        total_loss: -0.010097010992467403
        vf_explained_var: -0.02328750491142273
        vf_loss: 1.940256118774414
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0011222207685932517
        entropy: 1.9665331840515137
        entropy_coeff: 0.0017600000137463212
        kl: 0.00781602505594492
        model: {}
        policy_loss: -0.004527701996266842
        total_loss: -0.0070188771933317184
        vf_explained_var: 0.007398486137390137
        vf_loss: 1.8832002878189087
    load_time_ms: 45057.296
    num_steps_sampled: 2304000
    num_steps_trained: 2304000
    sample_time_ms: 118218.306
    update_time_ms: 832.78
  iterations_since_restore: 4
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 13.076954732510288
    ram_util_percent: 11.460493827160493
  pid: 4061
  policy_reward_max:
    agent-0: 29.500000000000053
    agent-1: 29.500000000000053
    agent-2: 29.500000000000053
    agent-3: 29.500000000000053
    agent-4: 29.500000000000053
    agent-5: 29.500000000000053
  policy_reward_mean:
    agent-0: 12.961666666666668
    agent-1: 12.961666666666668
    agent-2: 12.961666666666668
    agent-3: 12.961666666666668
    agent-4: 12.961666666666668
    agent-5: 12.961666666666668
  policy_reward_min:
    agent-0: -6.833333333333356
    agent-1: -6.833333333333356
    agent-2: -6.833333333333356
    agent-3: -6.833333333333356
    agent-4: -6.833333333333356
    agent-5: -6.833333333333356
  sampler_perf:
    mean_env_wait_ms: 23.34589726414084
    mean_inference_ms: 13.150962500814563
    mean_processing_ms: 52.71729359378096
  time_since_restore: 754.8679299354553
  time_this_iter_s: 170.12830996513367
  time_total_s: 3965.9316160678864
  timestamp: 1637018258
  timesteps_since_restore: 384000
  timesteps_this_iter: 96000
  timesteps_total: 2304000
  training_iteration: 24
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     24 |          3965.93 | 2304000 |    77.77 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 31
    apples_agent-0_mean: 2.67
    apples_agent-0_min: 0
    apples_agent-1_max: 48
    apples_agent-1_mean: 6.13
    apples_agent-1_min: 0
    apples_agent-2_max: 46
    apples_agent-2_mean: 11.33
    apples_agent-2_min: 0
    apples_agent-3_max: 44
    apples_agent-3_mean: 13.89
    apples_agent-3_min: 0
    apples_agent-4_max: 49
    apples_agent-4_mean: 11.06
    apples_agent-4_min: 0
    apples_agent-5_max: 53
    apples_agent-5_mean: 14.14
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 324
    cleaning_beam_agent-0_mean: 247.99
    cleaning_beam_agent-0_min: 155
    cleaning_beam_agent-1_max: 357
    cleaning_beam_agent-1_mean: 283.95
    cleaning_beam_agent-1_min: 236
    cleaning_beam_agent-2_max: 368
    cleaning_beam_agent-2_mean: 292.4
    cleaning_beam_agent-2_min: 222
    cleaning_beam_agent-3_max: 190
    cleaning_beam_agent-3_mean: 136.1
    cleaning_beam_agent-3_min: 106
    cleaning_beam_agent-4_max: 157
    cleaning_beam_agent-4_mean: 98.73
    cleaning_beam_agent-4_min: 56
    cleaning_beam_agent-5_max: 166
    cleaning_beam_agent-5_mean: 94.55
    cleaning_beam_agent-5_min: 68
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.07
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.07
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.09
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.13
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.09
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_18-20-33
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 171.9999999999988
  episode_reward_mean: 80.90000000000026
  episode_reward_min: -29.999999999999464
  episodes_this_iter: 96
  episodes_total: 2400
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 22618.446
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0011162303853780031
        entropy: 1.8180311918258667
        entropy_coeff: 0.0017600000137463212
        kl: 0.009536243975162506
        model: {}
        policy_loss: -0.0045386021956801414
        total_loss: -0.00664700660854578
        vf_explained_var: -0.0009339004755020142
        vf_loss: 1.3770647048950195
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0011162303853780031
        entropy: 1.9093326330184937
        entropy_coeff: 0.0017600000137463212
        kl: 0.00878327339887619
        model: {}
        policy_loss: -0.007786995731294155
        total_loss: -0.010133394040167332
        vf_explained_var: 0.013776659965515137
        vf_loss: 1.3569573163986206
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0011162303853780031
        entropy: 1.80390465259552
        entropy_coeff: 0.0017600000137463212
        kl: 0.010729135014116764
        model: {}
        policy_loss: -0.01008913666009903
        total_loss: -0.012056203559041023
        vf_explained_var: 0.019427135586738586
        vf_loss: 1.3489270210266113
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0011162303853780031
        entropy: 1.9333821535110474
        entropy_coeff: 0.0017600000137463212
        kl: 0.007814642041921616
        model: {}
        policy_loss: -0.005144665949046612
        total_loss: -0.007627825252711773
        vf_explained_var: -0.003818035125732422
        vf_loss: 1.3812907934188843
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0011162303853780031
        entropy: 1.796743392944336
        entropy_coeff: 0.0017600000137463212
        kl: 0.010191828943789005
        model: {}
        policy_loss: -0.009252556599676609
        total_loss: -0.01125291083008051
        vf_explained_var: -0.03599551320075989
        vf_loss: 1.4273335933685303
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0011162303853780031
        entropy: 1.93226158618927
        entropy_coeff: 0.0017600000137463212
        kl: 0.00850885733962059
        model: {}
        policy_loss: -0.006796909496188164
        total_loss: -0.009211547672748566
        vf_explained_var: 0.01722276210784912
        vf_loss: 1.3525669574737549
    load_time_ms: 43938.304
    num_steps_sampled: 2400000
    num_steps_trained: 2400000
    sample_time_ms: 117476.582
    update_time_ms: 681.946
  iterations_since_restore: 5
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 12.224899598393574
    ram_util_percent: 10.842168674698794
  pid: 4061
  policy_reward_max:
    agent-0: 28.66666666666672
    agent-1: 28.66666666666672
    agent-2: 28.66666666666672
    agent-3: 28.66666666666672
    agent-4: 28.66666666666672
    agent-5: 28.66666666666672
  policy_reward_mean:
    agent-0: 13.483333333333333
    agent-1: 13.483333333333333
    agent-2: 13.483333333333333
    agent-3: 13.483333333333333
    agent-4: 13.483333333333333
    agent-5: 13.483333333333333
  policy_reward_min:
    agent-0: -5.000000000000016
    agent-1: -5.000000000000016
    agent-2: -5.000000000000016
    agent-3: -5.000000000000016
    agent-4: -5.000000000000016
    agent-5: -5.000000000000016
  sampler_perf:
    mean_env_wait_ms: 23.265568645893918
    mean_inference_ms: 12.979972080898783
    mean_processing_ms: 52.35869628285058
  time_since_restore: 929.193665266037
  time_this_iter_s: 174.32573533058167
  time_total_s: 4140.257351398468
  timestamp: 1637018433
  timesteps_since_restore: 480000
  timesteps_this_iter: 96000
  timesteps_total: 2400000
  training_iteration: 25
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     25 |          4140.26 | 2400000 |     80.9 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 25
    apples_agent-0_mean: 5.68
    apples_agent-0_min: 0
    apples_agent-1_max: 34
    apples_agent-1_mean: 6.02
    apples_agent-1_min: 0
    apples_agent-2_max: 44
    apples_agent-2_mean: 12.81
    apples_agent-2_min: 0
    apples_agent-3_max: 66
    apples_agent-3_mean: 16.98
    apples_agent-3_min: 0
    apples_agent-4_max: 39
    apples_agent-4_mean: 7.28
    apples_agent-4_min: 0
    apples_agent-5_max: 46
    apples_agent-5_mean: 13.08
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 288
    cleaning_beam_agent-0_mean: 220.04
    cleaning_beam_agent-0_min: 155
    cleaning_beam_agent-1_max: 319
    cleaning_beam_agent-1_mean: 242.65
    cleaning_beam_agent-1_min: 192
    cleaning_beam_agent-2_max: 405
    cleaning_beam_agent-2_mean: 345.33
    cleaning_beam_agent-2_min: 248
    cleaning_beam_agent-3_max: 163
    cleaning_beam_agent-3_mean: 118.86
    cleaning_beam_agent-3_min: 84
    cleaning_beam_agent-4_max: 190
    cleaning_beam_agent-4_mean: 128.52
    cleaning_beam_agent-4_min: 80
    cleaning_beam_agent-5_max: 119
    cleaning_beam_agent-5_mean: 96.74
    cleaning_beam_agent-5_min: 77
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.09
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.12
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.12
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.07
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_18-23-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 189.99999999999807
  episode_reward_mean: 89.19000000000027
  episode_reward_min: -27.000000000000234
  episodes_this_iter: 96
  episodes_total: 2496
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 22205.878
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0011102400021627545
        entropy: 1.8236433267593384
        entropy_coeff: 0.0017600000137463212
        kl: 0.00820715632289648
        model: {}
        policy_loss: -0.005975673906505108
        total_loss: -0.00823119468986988
        vf_explained_var: -0.001132875680923462
        vf_loss: 1.3337887525558472
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0011102400021627545
        entropy: 1.9231648445129395
        entropy_coeff: 0.0017600000137463212
        kl: 0.008733504451811314
        model: {}
        policy_loss: -0.007157099433243275
        total_loss: -0.00953557901084423
        vf_explained_var: 0.0022079795598983765
        vf_loss: 1.329392910003662
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0011102400021627545
        entropy: 1.7150897979736328
        entropy_coeff: 0.0017600000137463212
        kl: 0.010954358614981174
        model: {}
        policy_loss: -0.010171027854084969
        total_loss: -0.011962777003645897
        vf_explained_var: 0.014328032732009888
        vf_loss: 1.313747763633728
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0011102400021627545
        entropy: 1.8834326267242432
        entropy_coeff: 0.0017600000137463212
        kl: 0.008796989917755127
        model: {}
        policy_loss: -0.007231458090245724
        total_loss: -0.009534195065498352
        vf_explained_var: 0.006013825535774231
        vf_loss: 1.3240396976470947
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0011102400021627545
        entropy: 1.841720461845398
        entropy_coeff: 0.0017600000137463212
        kl: 0.009371001273393631
        model: {}
        policy_loss: -0.009815587662160397
        total_loss: -0.011982208117842674
        vf_explained_var: -0.032168447971343994
        vf_loss: 1.377086877822876
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0011102400021627545
        entropy: 1.979556918144226
        entropy_coeff: 0.0017600000137463212
        kl: 0.0100236926227808
        model: {}
        policy_loss: -0.009114616550505161
        total_loss: -0.011463734321296215
        vf_explained_var: 0.009317293763160706
        vf_loss: 1.3252867460250854
    load_time_ms: 42938.8
    num_steps_sampled: 2496000
    num_steps_trained: 2496000
    sample_time_ms: 116613.293
    update_time_ms: 580.148
  iterations_since_restore: 6
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 11.800000000000002
    ram_util_percent: 10.782304526748973
  pid: 4061
  policy_reward_max:
    agent-0: 31.66666666666673
    agent-1: 31.66666666666673
    agent-2: 31.66666666666673
    agent-3: 31.66666666666673
    agent-4: 31.66666666666673
    agent-5: 31.66666666666673
  policy_reward_mean:
    agent-0: 14.864999999999998
    agent-1: 14.864999999999998
    agent-2: 14.864999999999998
    agent-3: 14.864999999999998
    agent-4: 14.864999999999998
    agent-5: 14.864999999999998
  policy_reward_min:
    agent-0: -4.499999999999996
    agent-1: -4.499999999999996
    agent-2: -4.499999999999996
    agent-3: -4.499999999999996
    agent-4: -4.499999999999996
    agent-5: -4.499999999999996
  sampler_perf:
    mean_env_wait_ms: 23.163970169701752
    mean_inference_ms: 12.856261110537229
    mean_processing_ms: 52.08549816873587
  time_since_restore: 1099.7710616588593
  time_this_iter_s: 170.57739639282227
  time_total_s: 4310.83474779129
  timestamp: 1637018603
  timesteps_since_restore: 576000
  timesteps_this_iter: 96000
  timesteps_total: 2496000
  training_iteration: 26
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     26 |          4310.83 | 2496000 |    89.19 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 29
    apples_agent-0_mean: 4.79
    apples_agent-0_min: 0
    apples_agent-1_max: 45
    apples_agent-1_mean: 5.51
    apples_agent-1_min: 0
    apples_agent-2_max: 53
    apples_agent-2_mean: 13.19
    apples_agent-2_min: 0
    apples_agent-3_max: 90
    apples_agent-3_mean: 21.51
    apples_agent-3_min: 0
    apples_agent-4_max: 55
    apples_agent-4_mean: 7.47
    apples_agent-4_min: 0
    apples_agent-5_max: 75
    apples_agent-5_mean: 14.49
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 294
    cleaning_beam_agent-0_mean: 237.75
    cleaning_beam_agent-0_min: 167
    cleaning_beam_agent-1_max: 283
    cleaning_beam_agent-1_mean: 193.84
    cleaning_beam_agent-1_min: 150
    cleaning_beam_agent-2_max: 430
    cleaning_beam_agent-2_mean: 343.37
    cleaning_beam_agent-2_min: 257
    cleaning_beam_agent-3_max: 156
    cleaning_beam_agent-3_mean: 118.25
    cleaning_beam_agent-3_min: 87
    cleaning_beam_agent-4_max: 228
    cleaning_beam_agent-4_mean: 159.74
    cleaning_beam_agent-4_min: 85
    cleaning_beam_agent-5_max: 166
    cleaning_beam_agent-5_mean: 115.67
    cleaning_beam_agent-5_min: 62
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.05
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.11
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_18-26-11
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 220.99999999999707
  episode_reward_mean: 96.60000000000018
  episode_reward_min: 6.000000000000063
  episodes_this_iter: 96
  episodes_total: 2592
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 21929.14
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.001104249618947506
        entropy: 1.868422508239746
        entropy_coeff: 0.0017600000137463212
        kl: 0.006561706308275461
        model: {}
        policy_loss: -0.005564200691878796
        total_loss: -0.008044137619435787
        vf_explained_var: -0.007468312978744507
        vf_loss: 1.5232013463974
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.001104249618947506
        entropy: 1.9007132053375244
        entropy_coeff: 0.0017600000137463212
        kl: 0.010285614989697933
        model: {}
        policy_loss: -0.009213201701641083
        total_loss: -0.011380350217223167
        vf_explained_var: 0.00913432240486145
        vf_loss: 1.4954655170440674
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.001104249618947506
        entropy: 1.702161431312561
        entropy_coeff: 0.0017600000137463212
        kl: 0.01129099540412426
        model: {}
        policy_loss: -0.011111361905932426
        total_loss: -0.012830018065869808
        vf_explained_var: 0.0191991925239563
        vf_loss: 1.480478286743164
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.001104249618947506
        entropy: 1.8676893711090088
        entropy_coeff: 0.0017600000137463212
        kl: 0.010636573657393456
        model: {}
        policy_loss: -0.009120993316173553
        total_loss: -0.011195922270417213
        vf_explained_var: 0.015792176127433777
        vf_loss: 1.4854531288146973
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.001104249618947506
        entropy: 1.8313220739364624
        entropy_coeff: 0.0017600000137463212
        kl: 0.010842014104127884
        model: {}
        policy_loss: -0.01065829023718834
        total_loss: -0.012645107693970203
        vf_explained_var: -0.0067183226346969604
        vf_loss: 1.5210788249969482
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.001104249618947506
        entropy: 1.9742696285247803
        entropy_coeff: 0.0017600000137463212
        kl: 0.011949351988732815
        model: {}
        policy_loss: -0.01115939486771822
        total_loss: -0.013284402899444103
        vf_explained_var: -0.022962987422943115
        vf_loss: 1.547752022743225
    load_time_ms: 42317.315
    num_steps_sampled: 2592000
    num_steps_trained: 2592000
    sample_time_ms: 115340.479
    update_time_ms: 507.979
  iterations_since_restore: 7
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 12.006302521008404
    ram_util_percent: 10.719327731092438
  pid: 4061
  policy_reward_max:
    agent-0: 36.83333333333332
    agent-1: 36.83333333333332
    agent-2: 36.83333333333332
    agent-3: 36.83333333333332
    agent-4: 36.83333333333332
    agent-5: 36.83333333333332
  policy_reward_mean:
    agent-0: 16.1
    agent-1: 16.1
    agent-2: 16.1
    agent-3: 16.1
    agent-4: 16.1
    agent-5: 16.1
  policy_reward_min:
    agent-0: 1.0000000000000002
    agent-1: 1.0000000000000002
    agent-2: 1.0000000000000002
    agent-3: 1.0000000000000002
    agent-4: 1.0000000000000002
    agent-5: 1.0000000000000002
  sampler_perf:
    mean_env_wait_ms: 23.099344285960193
    mean_inference_ms: 12.781737562451925
    mean_processing_ms: 51.82159359593428
  time_since_restore: 1266.582943201065
  time_this_iter_s: 166.8118815422058
  time_total_s: 4477.646629333496
  timestamp: 1637018771
  timesteps_since_restore: 672000
  timesteps_this_iter: 96000
  timesteps_total: 2592000
  training_iteration: 27
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     27 |          4477.65 | 2592000 |     96.6 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 39
    apples_agent-0_mean: 4.58
    apples_agent-0_min: 0
    apples_agent-1_max: 38
    apples_agent-1_mean: 4.62
    apples_agent-1_min: 0
    apples_agent-2_max: 49
    apples_agent-2_mean: 11.78
    apples_agent-2_min: 0
    apples_agent-3_max: 88
    apples_agent-3_mean: 22.33
    apples_agent-3_min: 1
    apples_agent-4_max: 59
    apples_agent-4_mean: 6.91
    apples_agent-4_min: 0
    apples_agent-5_max: 75
    apples_agent-5_mean: 13.24
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 301
    cleaning_beam_agent-0_mean: 246.62
    cleaning_beam_agent-0_min: 194
    cleaning_beam_agent-1_max: 228
    cleaning_beam_agent-1_mean: 164.84
    cleaning_beam_agent-1_min: 126
    cleaning_beam_agent-2_max: 464
    cleaning_beam_agent-2_mean: 379.35
    cleaning_beam_agent-2_min: 295
    cleaning_beam_agent-3_max: 139
    cleaning_beam_agent-3_mean: 114.65
    cleaning_beam_agent-3_min: 83
    cleaning_beam_agent-4_max: 183
    cleaning_beam_agent-4_mean: 119.87
    cleaning_beam_agent-4_min: 80
    cleaning_beam_agent-5_max: 143
    cleaning_beam_agent-5_mean: 108.45
    cleaning_beam_agent-5_min: 57
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.04
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.06
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.07
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_18-29-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 212.9999999999967
  episode_reward_mean: 91.55000000000011
  episode_reward_min: 16.000000000000053
  episodes_this_iter: 96
  episodes_total: 2688
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 21716.192
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010982592357322574
        entropy: 1.8676036596298218
        entropy_coeff: 0.0017600000137463212
        kl: 0.008724130690097809
        model: {}
        policy_loss: -0.006681645754724741
        total_loss: -0.008957535028457642
        vf_explained_var: -0.003451049327850342
        vf_loss: 1.3868061304092407
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010982592357322574
        entropy: 1.879827857017517
        entropy_coeff: 0.0017600000137463212
        kl: 0.009990837424993515
        model: {}
        policy_loss: -0.009098364971578121
        total_loss: -0.011270858347415924
        vf_explained_var: 0.00823695957660675
        vf_loss: 1.3692007064819336
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010982592357322574
        entropy: 1.6534709930419922
        entropy_coeff: 0.0017600000137463212
        kl: 0.009454933926463127
        model: {}
        policy_loss: -0.010861054994165897
        total_loss: -0.012690558098256588
        vf_explained_var: 0.020723938941955566
        vf_loss: 1.3511099815368652
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010982592357322574
        entropy: 1.8517974615097046
        entropy_coeff: 0.0017600000137463212
        kl: 0.01055574044585228
        model: {}
        policy_loss: -0.008898667991161346
        total_loss: -0.010966966859996319
        vf_explained_var: 0.019472837448120117
        vf_loss: 1.3529101610183716
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010982592357322574
        entropy: 1.8389346599578857
        entropy_coeff: 0.0017600000137463212
        kl: 0.010315900668501854
        model: {}
        policy_loss: -0.01102231815457344
        total_loss: -0.013091634958982468
        vf_explained_var: 0.0166843980550766
        vf_loss: 1.3561797142028809
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010982592357322574
        entropy: 1.9701894521713257
        entropy_coeff: 0.0017600000137463212
        kl: 0.010305543430149555
        model: {}
        policy_loss: -0.011808726005256176
        total_loss: -0.01410276535898447
        vf_explained_var: -0.03589162230491638
        vf_loss: 1.4293882846832275
    load_time_ms: 41087.487
    num_steps_sampled: 2688000
    num_steps_trained: 2688000
    sample_time_ms: 115785.604
    update_time_ms: 454.454
  iterations_since_restore: 8
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 12.02326530612245
    ram_util_percent: 10.620816326530615
  pid: 4061
  policy_reward_max:
    agent-0: 35.50000000000001
    agent-1: 35.50000000000001
    agent-2: 35.50000000000001
    agent-3: 35.50000000000001
    agent-4: 35.50000000000001
    agent-5: 35.50000000000001
  policy_reward_mean:
    agent-0: 15.25833333333334
    agent-1: 15.25833333333334
    agent-2: 15.25833333333334
    agent-3: 15.25833333333334
    agent-4: 15.25833333333334
    agent-5: 15.25833333333334
  policy_reward_min:
    agent-0: 2.666666666666665
    agent-1: 2.666666666666665
    agent-2: 2.666666666666665
    agent-3: 2.666666666666665
    agent-4: 2.666666666666665
    agent-5: 2.666666666666665
  sampler_perf:
    mean_env_wait_ms: 23.035466993068216
    mean_inference_ms: 12.714109895760476
    mean_processing_ms: 51.630860691788136
  time_since_restore: 1438.3833603858948
  time_this_iter_s: 171.8004171848297
  time_total_s: 4649.447046518326
  timestamp: 1637018943
  timesteps_since_restore: 768000
  timesteps_this_iter: 96000
  timesteps_total: 2688000
  training_iteration: 28
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     28 |          4649.45 | 2688000 |    91.55 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 38
    apples_agent-0_mean: 4.96
    apples_agent-0_min: 0
    apples_agent-1_max: 32
    apples_agent-1_mean: 5.05
    apples_agent-1_min: 0
    apples_agent-2_max: 44
    apples_agent-2_mean: 10.43
    apples_agent-2_min: 0
    apples_agent-3_max: 69
    apples_agent-3_mean: 22.27
    apples_agent-3_min: 2
    apples_agent-4_max: 47
    apples_agent-4_mean: 6.77
    apples_agent-4_min: 0
    apples_agent-5_max: 47
    apples_agent-5_mean: 13.28
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 275
    cleaning_beam_agent-0_mean: 188.44
    cleaning_beam_agent-0_min: 143
    cleaning_beam_agent-1_max: 188
    cleaning_beam_agent-1_mean: 154.9
    cleaning_beam_agent-1_min: 115
    cleaning_beam_agent-2_max: 434
    cleaning_beam_agent-2_mean: 350.33
    cleaning_beam_agent-2_min: 230
    cleaning_beam_agent-3_max: 150
    cleaning_beam_agent-3_mean: 111.22
    cleaning_beam_agent-3_min: 69
    cleaning_beam_agent-4_max: 184
    cleaning_beam_agent-4_mean: 124.45
    cleaning_beam_agent-4_min: 84
    cleaning_beam_agent-5_max: 147
    cleaning_beam_agent-5_mean: 101.57
    cleaning_beam_agent-5_min: 65
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.04
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.07
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_18-31-33
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 175.99999999999866
  episode_reward_mean: 93.7400000000002
  episode_reward_min: -28.999999999999414
  episodes_this_iter: 96
  episodes_total: 2784
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 21547.029
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010922688525170088
        entropy: 1.8844187259674072
        entropy_coeff: 0.0017600000137463212
        kl: 0.008055721409618855
        model: {}
        policy_loss: -0.007330961059778929
        total_loss: -0.009715581312775612
        vf_explained_var: 0.005510479211807251
        vf_loss: 1.2638236284255981
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010922688525170088
        entropy: 1.853622555732727
        entropy_coeff: 0.0017600000137463212
        kl: 0.010340253822505474
        model: {}
        policy_loss: -0.010306915268301964
        total_loss: -0.012410703115165234
        vf_explained_var: 0.01817403733730316
        vf_loss: 1.2456330060958862
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010922688525170088
        entropy: 1.6951866149902344
        entropy_coeff: 0.0017600000137463212
        kl: 0.008912216871976852
        model: {}
        policy_loss: -0.010986682027578354
        total_loss: -0.012953373603522778
        vf_explained_var: 0.009927570819854736
        vf_loss: 1.2561290264129639
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010922688525170088
        entropy: 1.810577392578125
        entropy_coeff: 0.0017600000137463212
        kl: 0.010592100210487843
        model: {}
        policy_loss: -0.008507328107953072
        total_loss: -0.010510630905628204
        vf_explained_var: 0.02168436348438263
        vf_loss: 1.2410671710968018
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010922688525170088
        entropy: 1.7860180139541626
        entropy_coeff: 0.0017600000137463212
        kl: 0.011030059307813644
        model: {}
        policy_loss: -0.012673725374042988
        total_loss: -0.014581489376723766
        vf_explained_var: -0.04591438174247742
        vf_loss: 1.3261897563934326
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010922688525170088
        entropy: 1.9083740711212158
        entropy_coeff: 0.0017600000137463212
        kl: 0.010568604804575443
        model: {}
        policy_loss: -0.013673827052116394
        total_loss: -0.015845056623220444
        vf_explained_var: -0.029482156038284302
        vf_loss: 1.306465744972229
    load_time_ms: 38870.244
    num_steps_sampled: 2784000
    num_steps_trained: 2784000
    sample_time_ms: 115037.159
    update_time_ms: 412.871
  iterations_since_restore: 9
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 14.01953488372093
    ram_util_percent: 10.527906976744186
  pid: 4061
  policy_reward_max:
    agent-0: 29.3333333333334
    agent-1: 29.3333333333334
    agent-2: 29.3333333333334
    agent-3: 29.3333333333334
    agent-4: 29.3333333333334
    agent-5: 29.3333333333334
  policy_reward_mean:
    agent-0: 15.623333333333333
    agent-1: 15.623333333333333
    agent-2: 15.623333333333333
    agent-3: 15.623333333333333
    agent-4: 15.623333333333333
    agent-5: 15.623333333333333
  policy_reward_min:
    agent-0: -4.83333333333335
    agent-1: -4.83333333333335
    agent-2: -4.83333333333335
    agent-3: -4.83333333333335
    agent-4: -4.83333333333335
    agent-5: -4.83333333333335
  sampler_perf:
    mean_env_wait_ms: 22.978773898579778
    mean_inference_ms: 12.670341382337062
    mean_processing_ms: 51.54134725344406
  time_since_restore: 1588.9585061073303
  time_this_iter_s: 150.57514572143555
  time_total_s: 4800.022192239761
  timestamp: 1637019093
  timesteps_since_restore: 864000
  timesteps_this_iter: 96000
  timesteps_total: 2784000
  training_iteration: 29
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     29 |          4800.02 | 2784000 |    93.74 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 23
    apples_agent-0_mean: 3.65
    apples_agent-0_min: 0
    apples_agent-1_max: 27
    apples_agent-1_mean: 3.35
    apples_agent-1_min: 0
    apples_agent-2_max: 43
    apples_agent-2_mean: 11.73
    apples_agent-2_min: 0
    apples_agent-3_max: 78
    apples_agent-3_mean: 26.59
    apples_agent-3_min: 1
    apples_agent-4_max: 63
    apples_agent-4_mean: 9.19
    apples_agent-4_min: 0
    apples_agent-5_max: 58
    apples_agent-5_mean: 16.86
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 231
    cleaning_beam_agent-0_mean: 186.13
    cleaning_beam_agent-0_min: 128
    cleaning_beam_agent-1_max: 209
    cleaning_beam_agent-1_mean: 156.65
    cleaning_beam_agent-1_min: 123
    cleaning_beam_agent-2_max: 399
    cleaning_beam_agent-2_mean: 315.5
    cleaning_beam_agent-2_min: 197
    cleaning_beam_agent-3_max: 186
    cleaning_beam_agent-3_mean: 118.63
    cleaning_beam_agent-3_min: 80
    cleaning_beam_agent-4_max: 201
    cleaning_beam_agent-4_mean: 128.45
    cleaning_beam_agent-4_min: 61
    cleaning_beam_agent-5_max: 135
    cleaning_beam_agent-5_mean: 92.0
    cleaning_beam_agent-5_min: 67
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.07
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.04
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.06
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_18-34-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 220.99999999999702
  episode_reward_mean: 111.18000000000022
  episode_reward_min: 26.999999999999808
  episodes_this_iter: 96
  episodes_total: 2880
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 21415.117
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010862783528864384
        entropy: 1.9020049571990967
        entropy_coeff: 0.0017600000137463212
        kl: 0.008930252864956856
        model: {}
        policy_loss: -0.007293952163308859
        total_loss: -0.009605368599295616
        vf_explained_var: -0.013554483652114868
        vf_loss: 1.4308823347091675
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010862783528864384
        entropy: 1.8400676250457764
        entropy_coeff: 0.0017600000137463212
        kl: 0.010105600580573082
        model: {}
        policy_loss: -0.0103308642283082
        total_loss: -0.012415358796715736
        vf_explained_var: -0.01529395580291748
        vf_loss: 1.4346106052398682
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010862783528864384
        entropy: 1.6986005306243896
        entropy_coeff: 0.0017600000137463212
        kl: 0.010861566290259361
        model: {}
        policy_loss: -0.012674499303102493
        total_loss: -0.014438236132264137
        vf_explained_var: 0.010994046926498413
        vf_loss: 1.3964552879333496
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010862783528864384
        entropy: 1.8268908262252808
        entropy_coeff: 0.0017600000137463212
        kl: 0.012810823507606983
        model: {}
        policy_loss: -0.01156030222773552
        total_loss: -0.013355173170566559
        vf_explained_var: 0.013545975089073181
        vf_loss: 1.393747091293335
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010862783528864384
        entropy: 1.7504314184188843
        entropy_coeff: 0.0017600000137463212
        kl: 0.012764406390488148
        model: {}
        policy_loss: -0.014820572920143604
        total_loss: -0.016475047916173935
        vf_explained_var: -0.06047391891479492
        vf_loss: 1.4984056949615479
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010862783528864384
        entropy: 1.8897786140441895
        entropy_coeff: 0.0017600000137463212
        kl: 0.012161336839199066
        model: {}
        policy_loss: -0.01437671110033989
        total_loss: -0.01634257100522518
        vf_explained_var: -0.020337074995040894
        vf_loss: 1.4401631355285645
    load_time_ms: 39416.844
    num_steps_sampled: 2880000
    num_steps_trained: 2880000
    sample_time_ms: 113580.667
    update_time_ms: 380.493
  iterations_since_restore: 10
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 12.01779661016949
    ram_util_percent: 10.491101694915253
  pid: 4061
  policy_reward_max:
    agent-0: 36.83333333333333
    agent-1: 36.83333333333333
    agent-2: 36.83333333333333
    agent-3: 36.83333333333333
    agent-4: 36.83333333333333
    agent-5: 36.83333333333333
  policy_reward_mean:
    agent-0: 18.530000000000005
    agent-1: 18.530000000000005
    agent-2: 18.530000000000005
    agent-3: 18.530000000000005
    agent-4: 18.530000000000005
    agent-5: 18.530000000000005
  policy_reward_min:
    agent-0: 4.499999999999999
    agent-1: 4.499999999999999
    agent-2: 4.499999999999999
    agent-3: 4.499999999999999
    agent-4: 4.499999999999999
    agent-5: 4.499999999999999
  sampler_perf:
    mean_env_wait_ms: 22.900092058015684
    mean_inference_ms: 12.63002086572866
    mean_processing_ms: 51.4297263593324
  time_since_restore: 1754.197972536087
  time_this_iter_s: 165.2394664287567
  time_total_s: 4965.261658668518
  timestamp: 1637019259
  timesteps_since_restore: 960000
  timesteps_this_iter: 96000
  timesteps_total: 2880000
  training_iteration: 30
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     30 |          4965.26 | 2880000 |   111.18 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 51
    apples_agent-0_mean: 5.88
    apples_agent-0_min: 0
    apples_agent-1_max: 69
    apples_agent-1_mean: 5.16
    apples_agent-1_min: 0
    apples_agent-2_max: 32
    apples_agent-2_mean: 8.39
    apples_agent-2_min: 0
    apples_agent-3_max: 80
    apples_agent-3_mean: 30.03
    apples_agent-3_min: 1
    apples_agent-4_max: 79
    apples_agent-4_mean: 9.58
    apples_agent-4_min: 0
    apples_agent-5_max: 55
    apples_agent-5_mean: 19.52
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 274
    cleaning_beam_agent-0_mean: 212.51
    cleaning_beam_agent-0_min: 135
    cleaning_beam_agent-1_max: 214
    cleaning_beam_agent-1_mean: 153.77
    cleaning_beam_agent-1_min: 117
    cleaning_beam_agent-2_max: 422
    cleaning_beam_agent-2_mean: 340.88
    cleaning_beam_agent-2_min: 240
    cleaning_beam_agent-3_max: 173
    cleaning_beam_agent-3_mean: 113.55
    cleaning_beam_agent-3_min: 73
    cleaning_beam_agent-4_max: 219
    cleaning_beam_agent-4_mean: 147.44
    cleaning_beam_agent-4_min: 84
    cleaning_beam_agent-5_max: 122
    cleaning_beam_agent-5_mean: 90.29
    cleaning_beam_agent-5_min: 59
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.05
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.04
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_18-37-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 247.99999999999594
  episode_reward_mean: 126.56999999999996
  episode_reward_min: -15.999999999999806
  episodes_this_iter: 96
  episodes_total: 2976
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20277.673
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010802879696711898
        entropy: 1.8786550760269165
        entropy_coeff: 0.0017600000137463212
        kl: 0.008418656885623932
        model: {}
        policy_loss: -0.008790506981313229
        total_loss: -0.011068183928728104
        vf_explained_var: -0.026973217725753784
        vf_loss: 1.8688890933990479
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010802879696711898
        entropy: 1.7664215564727783
        entropy_coeff: 0.0017600000137463212
        kl: 0.009706379845738411
        model: {}
        policy_loss: -0.01058677863329649
        total_loss: -0.012543664313852787
        vf_explained_var: 0.002419769763946533
        vf_loss: 1.813768982887268
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010802879696711898
        entropy: 1.669550895690918
        entropy_coeff: 0.0017600000137463212
        kl: 0.011931389570236206
        model: {}
        policy_loss: -0.01327924057841301
        total_loss: -0.014848781749606133
        vf_explained_var: 0.03394448757171631
        vf_loss: 1.757246494293213
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010802879696711898
        entropy: 1.8028146028518677
        entropy_coeff: 0.0017600000137463212
        kl: 0.00985771231353283
        model: {}
        policy_loss: -0.011769942007958889
        total_loss: -0.01377832144498825
        vf_explained_var: 0.018492653965950012
        vf_loss: 1.7880150079727173
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010802879696711898
        entropy: 1.7330275774002075
        entropy_coeff: 0.0017600000137463212
        kl: 0.010936607606709003
        model: {}
        policy_loss: -0.013860652223229408
        total_loss: -0.01563604548573494
        vf_explained_var: 0.004754051566123962
        vf_loss: 1.8107354640960693
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010802879696711898
        entropy: 1.8528882265090942
        entropy_coeff: 0.0017600000137463212
        kl: 0.012019194662570953
        model: {}
        policy_loss: -0.015917276963591576
        total_loss: -0.017792783677577972
        vf_explained_var: -0.009513914585113525
        vf_loss: 1.836554765701294
    load_time_ms: 38000.834
    num_steps_sampled: 2976000
    num_steps_trained: 2976000
    sample_time_ms: 111616.772
    update_time_ms: 85.548
  iterations_since_restore: 11
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 12.22782608695652
    ram_util_percent: 10.336956521739129
  pid: 4061
  policy_reward_max:
    agent-0: 41.3333333333333
    agent-1: 41.3333333333333
    agent-2: 41.3333333333333
    agent-3: 41.3333333333333
    agent-4: 41.3333333333333
    agent-5: 41.3333333333333
  policy_reward_mean:
    agent-0: 21.095000000000013
    agent-1: 21.095000000000013
    agent-2: 21.095000000000013
    agent-3: 21.095000000000013
    agent-4: 21.095000000000013
    agent-5: 21.095000000000013
  policy_reward_min:
    agent-0: -2.6666666666666434
    agent-1: -2.6666666666666434
    agent-2: -2.6666666666666434
    agent-3: -2.6666666666666434
    agent-4: -2.6666666666666434
    agent-5: -2.6666666666666434
  sampler_perf:
    mean_env_wait_ms: 22.838308599244588
    mean_inference_ms: 12.590962301010642
    mean_processing_ms: 51.292646338848535
  time_since_restore: 1917.8744509220123
  time_this_iter_s: 163.6764783859253
  time_total_s: 5128.938137054443
  timestamp: 1637019423
  timesteps_since_restore: 1056000
  timesteps_this_iter: 96000
  timesteps_total: 2976000
  training_iteration: 31
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     31 |          5128.94 | 2976000 |   126.57 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 33
    apples_agent-0_mean: 5.83
    apples_agent-0_min: 0
    apples_agent-1_max: 26
    apples_agent-1_mean: 4.09
    apples_agent-1_min: 0
    apples_agent-2_max: 39
    apples_agent-2_mean: 9.08
    apples_agent-2_min: 0
    apples_agent-3_max: 74
    apples_agent-3_mean: 27.61
    apples_agent-3_min: 2
    apples_agent-4_max: 85
    apples_agent-4_mean: 10.38
    apples_agent-4_min: 0
    apples_agent-5_max: 71
    apples_agent-5_mean: 18.4
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 333
    cleaning_beam_agent-0_mean: 257.3
    cleaning_beam_agent-0_min: 160
    cleaning_beam_agent-1_max: 209
    cleaning_beam_agent-1_mean: 148.67
    cleaning_beam_agent-1_min: 108
    cleaning_beam_agent-2_max: 425
    cleaning_beam_agent-2_mean: 311.55
    cleaning_beam_agent-2_min: 203
    cleaning_beam_agent-3_max: 178
    cleaning_beam_agent-3_mean: 128.23
    cleaning_beam_agent-3_min: 85
    cleaning_beam_agent-4_max: 161
    cleaning_beam_agent-4_mean: 112.31
    cleaning_beam_agent-4_min: 72
    cleaning_beam_agent-5_max: 110
    cleaning_beam_agent-5_mean: 65.89
    cleaning_beam_agent-5_min: 40
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.07
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.07
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.09
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_18-39-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 246.99999999999648
  episode_reward_mean: 116.20000000000009
  episode_reward_min: 7.078226893497686e-13
  episodes_this_iter: 96
  episodes_total: 3072
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20215.887
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010742975864559412
        entropy: 1.8575552701950073
        entropy_coeff: 0.0017600000137463212
        kl: 0.00988445058465004
        model: {}
        policy_loss: -0.01000873651355505
        total_loss: -0.01210010051727295
        vf_explained_var: -0.058895111083984375
        vf_loss: 1.8948451280593872
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010742975864559412
        entropy: 1.8243623971939087
        entropy_coeff: 0.0017600000137463212
        kl: 0.00911334902048111
        model: {}
        policy_loss: -0.010873740538954735
        total_loss: -0.012994934804737568
        vf_explained_var: 0.002711743116378784
        vf_loss: 1.7835001945495605
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010742975864559412
        entropy: 1.6588983535766602
        entropy_coeff: 0.0017600000137463212
        kl: 0.011819317936897278
        model: {}
        policy_loss: -0.014296304434537888
        total_loss: -0.015859529376029968
        vf_explained_var: 0.024147778749465942
        vf_loss: 1.7450377941131592
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010742975864559412
        entropy: 1.8209813833236694
        entropy_coeff: 0.0017600000137463212
        kl: 0.010730620473623276
        model: {}
        policy_loss: -0.010328024625778198
        total_loss: -0.012282740324735641
        vf_explained_var: 0.01045025885105133
        vf_loss: 1.771510362625122
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010742975864559412
        entropy: 1.7330152988433838
        entropy_coeff: 0.0017600000137463212
        kl: 0.013342220336198807
        model: {}
        policy_loss: -0.015176431275904179
        total_loss: -0.016708148643374443
        vf_explained_var: -0.029472798109054565
        vf_loss: 1.8416774272918701
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010742975864559412
        entropy: 1.8111571073532104
        entropy_coeff: 0.0017600000137463212
        kl: 0.012845202349126339
        model: {}
        policy_loss: -0.014043256640434265
        total_loss: -0.015764201059937477
        vf_explained_var: -0.018777847290039062
        vf_loss: 1.8217066526412964
    load_time_ms: 35961.991
    num_steps_sampled: 3072000
    num_steps_trained: 3072000
    sample_time_ms: 111720.35
    update_time_ms: 76.592
  iterations_since_restore: 12
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 12.371129707112972
    ram_util_percent: 10.19539748953975
  pid: 4061
  policy_reward_max:
    agent-0: 41.16666666666664
    agent-1: 41.16666666666664
    agent-2: 41.16666666666664
    agent-3: 41.16666666666664
    agent-4: 41.16666666666664
    agent-5: 41.16666666666664
  policy_reward_mean:
    agent-0: 19.366666666666678
    agent-1: 19.366666666666678
    agent-2: 19.366666666666678
    agent-3: 19.366666666666678
    agent-4: 19.366666666666678
    agent-5: 19.366666666666678
  policy_reward_min:
    agent-0: -1.1712852909795402e-14
    agent-1: -1.1712852909795402e-14
    agent-2: -1.1712852909795402e-14
    agent-3: -1.1712852909795402e-14
    agent-4: -1.1712852909795402e-14
    agent-5: -1.1712852909795402e-14
  sampler_perf:
    mean_env_wait_ms: 22.74212819122779
    mean_inference_ms: 12.620906347015518
    mean_processing_ms: 52.353809625326306
  time_since_restore: 2085.475872516632
  time_this_iter_s: 167.60142159461975
  time_total_s: 5296.539558649063
  timestamp: 1637019591
  timesteps_since_restore: 1152000
  timesteps_this_iter: 96000
  timesteps_total: 3072000
  training_iteration: 32
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     32 |          5296.54 | 3072000 |    116.2 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 25
    apples_agent-0_mean: 5.23
    apples_agent-0_min: 0
    apples_agent-1_max: 42
    apples_agent-1_mean: 4.29
    apples_agent-1_min: 0
    apples_agent-2_max: 45
    apples_agent-2_mean: 9.49
    apples_agent-2_min: 0
    apples_agent-3_max: 117
    apples_agent-3_mean: 30.35
    apples_agent-3_min: 3
    apples_agent-4_max: 47
    apples_agent-4_mean: 5.79
    apples_agent-4_min: 0
    apples_agent-5_max: 61
    apples_agent-5_mean: 18.74
    apples_agent-5_min: 1
    cleaning_beam_agent-0_max: 330
    cleaning_beam_agent-0_mean: 213.37
    cleaning_beam_agent-0_min: 130
    cleaning_beam_agent-1_max: 197
    cleaning_beam_agent-1_mean: 129.55
    cleaning_beam_agent-1_min: 85
    cleaning_beam_agent-2_max: 434
    cleaning_beam_agent-2_mean: 296.29
    cleaning_beam_agent-2_min: 150
    cleaning_beam_agent-3_max: 191
    cleaning_beam_agent-3_mean: 137.68
    cleaning_beam_agent-3_min: 104
    cleaning_beam_agent-4_max: 190
    cleaning_beam_agent-4_mean: 126.28
    cleaning_beam_agent-4_min: 66
    cleaning_beam_agent-5_max: 86
    cleaning_beam_agent-5_mean: 62.3
    cleaning_beam_agent-5_min: 38
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.04
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.12
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_18-42-27
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 280.999999999998
  episode_reward_mean: 118.21999999999991
  episode_reward_min: -6.999999999999306
  episodes_this_iter: 96
  episodes_total: 3168
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20179.924
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010683072032406926
        entropy: 1.7419331073760986
        entropy_coeff: 0.0017600000137463212
        kl: 0.009748505428433418
        model: {}
        policy_loss: -0.01211581937968731
        total_loss: -0.013994477689266205
        vf_explained_var: -0.06755727529525757
        vf_loss: 2.122953414916992
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010683072032406926
        entropy: 1.8175609111785889
        entropy_coeff: 0.0017600000137463212
        kl: 0.010242215357720852
        model: {}
        policy_loss: -0.011893924325704575
        total_loss: -0.013870902359485626
        vf_explained_var: 0.004297241568565369
        vf_loss: 1.9771058559417725
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010683072032406926
        entropy: 1.6019788980484009
        entropy_coeff: 0.0017600000137463212
        kl: 0.01198442094027996
        model: {}
        policy_loss: -0.014301612973213196
        total_loss: -0.015732837840914726
        vf_explained_var: 0.04407036304473877
        vf_loss: 1.8981765508651733
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010683072032406926
        entropy: 1.8038305044174194
        entropy_coeff: 0.0017600000137463212
        kl: 0.011291337199509144
        model: {}
        policy_loss: -0.011163362301886082
        total_loss: -0.013014719821512699
        vf_explained_var: 0.021814584732055664
        vf_loss: 1.9425134658813477
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010683072032406926
        entropy: 1.741277813911438
        entropy_coeff: 0.0017600000137463212
        kl: 0.011991963721811771
        model: {}
        policy_loss: -0.014000681228935719
        total_loss: -0.015676259994506836
        vf_explained_var: 0.043899208307266235
        vf_loss: 1.8987493515014648
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010683072032406926
        entropy: 1.7790043354034424
        entropy_coeff: 0.0017600000137463212
        kl: 0.011792260222136974
        model: {}
        policy_loss: -0.015978332608938217
        total_loss: -0.017733968794345856
        vf_explained_var: 0.012100324034690857
        vf_loss: 1.9618638753890991
    load_time_ms: 33700.13
    num_steps_sampled: 3168000
    num_steps_trained: 3168000
    sample_time_ms: 111565.847
    update_time_ms: 74.231
  iterations_since_restore: 13
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 12.603603603603604
    ram_util_percent: 10.260810810810812
  pid: 4061
  policy_reward_max:
    agent-0: 46.83333333333325
    agent-1: 46.83333333333325
    agent-2: 46.83333333333325
    agent-3: 46.83333333333325
    agent-4: 46.83333333333325
    agent-5: 46.83333333333325
  policy_reward_mean:
    agent-0: 19.703333333333344
    agent-1: 19.703333333333344
    agent-2: 19.703333333333344
    agent-3: 19.703333333333344
    agent-4: 19.703333333333344
    agent-5: 19.703333333333344
  policy_reward_min:
    agent-0: -1.1666666666666787
    agent-1: -1.1666666666666787
    agent-2: -1.1666666666666787
    agent-3: -1.1666666666666787
    agent-4: -1.1666666666666787
    agent-5: -1.1666666666666787
  sampler_perf:
    mean_env_wait_ms: 22.688655615307244
    mean_inference_ms: 12.604805303877047
    mean_processing_ms: 52.1976697361051
  time_since_restore: 2241.236696958542
  time_this_iter_s: 155.7608244419098
  time_total_s: 5452.300383090973
  timestamp: 1637019747
  timesteps_since_restore: 1248000
  timesteps_this_iter: 96000
  timesteps_total: 3168000
  training_iteration: 33
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     33 |           5452.3 | 3168000 |   118.22 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 44
    apples_agent-0_mean: 6.47
    apples_agent-0_min: 0
    apples_agent-1_max: 39
    apples_agent-1_mean: 5.88
    apples_agent-1_min: 0
    apples_agent-2_max: 39
    apples_agent-2_mean: 9.46
    apples_agent-2_min: 0
    apples_agent-3_max: 94
    apples_agent-3_mean: 38.14
    apples_agent-3_min: 1
    apples_agent-4_max: 41
    apples_agent-4_mean: 5.54
    apples_agent-4_min: 0
    apples_agent-5_max: 73
    apples_agent-5_mean: 24.14
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 277
    cleaning_beam_agent-0_mean: 225.58
    cleaning_beam_agent-0_min: 159
    cleaning_beam_agent-1_max: 180
    cleaning_beam_agent-1_mean: 128.12
    cleaning_beam_agent-1_min: 74
    cleaning_beam_agent-2_max: 522
    cleaning_beam_agent-2_mean: 338.23
    cleaning_beam_agent-2_min: 165
    cleaning_beam_agent-3_max: 200
    cleaning_beam_agent-3_mean: 149.53
    cleaning_beam_agent-3_min: 98
    cleaning_beam_agent-4_max: 190
    cleaning_beam_agent-4_mean: 127.92
    cleaning_beam_agent-4_min: 63
    cleaning_beam_agent-5_max: 87
    cleaning_beam_agent-5_mean: 61.73
    cleaning_beam_agent-5_min: 37
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.04
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_18-45-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 273.99999999999704
  episode_reward_mean: 146.81999999999948
  episode_reward_min: 21.000000000000007
  episodes_this_iter: 96
  episodes_total: 3264
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20203.513
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.001062316820025444
        entropy: 1.776451587677002
        entropy_coeff: 0.0017600000137463212
        kl: 0.009661717340350151
        model: {}
        policy_loss: -0.011836456134915352
        total_loss: -0.01376265101134777
        vf_explained_var: -0.029585808515548706
        vf_loss: 2.3418829441070557
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.001062316820025444
        entropy: 1.7680819034576416
        entropy_coeff: 0.0017600000137463212
        kl: 0.010103695094585419
        model: {}
        policy_loss: -0.012358363717794418
        total_loss: -0.014235850423574448
        vf_explained_var: 0.015594378113746643
        vf_loss: 2.239630937576294
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.001062316820025444
        entropy: 1.5544421672821045
        entropy_coeff: 0.0017600000137463212
        kl: 0.011735096573829651
        model: {}
        policy_loss: -0.01482989452779293
        total_loss: -0.01617600955069065
        vf_explained_var: 0.05010835826396942
        vf_loss: 2.161954164505005
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.001062316820025444
        entropy: 1.7854466438293457
        entropy_coeff: 0.0017600000137463212
        kl: 0.009664813987910748
        model: {}
        policy_loss: -0.011705984361469746
        total_loss: -0.01365701761096716
        vf_explained_var: 0.013236463069915771
        vf_loss: 2.248753547668457
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.001062316820025444
        entropy: 1.7157899141311646
        entropy_coeff: 0.0017600000137463212
        kl: 0.01223358977586031
        model: {}
        policy_loss: -0.015597870573401451
        total_loss: -0.017175201326608658
        vf_explained_var: 0.0359177440404892
        vf_loss: 2.191011428833008
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.001062316820025444
        entropy: 1.761857271194458
        entropy_coeff: 0.0017600000137463212
        kl: 0.011585189960896969
        model: {}
        policy_loss: -0.016420571133494377
        total_loss: -0.018129566684365273
        vf_explained_var: -0.023844748735427856
        vf_loss: 2.3335249423980713
    load_time_ms: 32570.579
    num_steps_sampled: 3264000
    num_steps_trained: 3264000
    sample_time_ms: 111363.652
    update_time_ms: 72.12
  iterations_since_restore: 14
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 12.483482142857143
    ram_util_percent: 10.238392857142857
  pid: 4061
  policy_reward_max:
    agent-0: 45.66666666666658
    agent-1: 45.66666666666658
    agent-2: 45.66666666666658
    agent-3: 45.66666666666658
    agent-4: 45.66666666666658
    agent-5: 45.66666666666658
  policy_reward_mean:
    agent-0: 24.470000000000024
    agent-1: 24.470000000000024
    agent-2: 24.470000000000024
    agent-3: 24.470000000000024
    agent-4: 24.470000000000024
    agent-5: 24.470000000000024
  policy_reward_min:
    agent-0: 3.4999999999999987
    agent-1: 3.4999999999999987
    agent-2: 3.4999999999999987
    agent-3: 3.4999999999999987
    agent-4: 3.4999999999999987
    agent-5: 3.4999999999999987
  sampler_perf:
    mean_env_wait_ms: 22.656508348194592
    mean_inference_ms: 12.574345343572295
    mean_processing_ms: 52.05195514343886
  time_since_restore: 2398.2514543533325
  time_this_iter_s: 157.01475739479065
  time_total_s: 5609.315140485764
  timestamp: 1637019904
  timesteps_since_restore: 1344000
  timesteps_this_iter: 96000
  timesteps_total: 3264000
  training_iteration: 34
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     34 |          5609.32 | 3264000 |   146.82 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 30
    apples_agent-0_mean: 5.11
    apples_agent-0_min: 0
    apples_agent-1_max: 33
    apples_agent-1_mean: 6.01
    apples_agent-1_min: 0
    apples_agent-2_max: 53
    apples_agent-2_mean: 11.41
    apples_agent-2_min: 0
    apples_agent-3_max: 114
    apples_agent-3_mean: 37.35
    apples_agent-3_min: 1
    apples_agent-4_max: 56
    apples_agent-4_mean: 6.43
    apples_agent-4_min: 0
    apples_agent-5_max: 77
    apples_agent-5_mean: 22.95
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 307
    cleaning_beam_agent-0_mean: 230.61
    cleaning_beam_agent-0_min: 160
    cleaning_beam_agent-1_max: 232
    cleaning_beam_agent-1_mean: 130.95
    cleaning_beam_agent-1_min: 74
    cleaning_beam_agent-2_max: 525
    cleaning_beam_agent-2_mean: 348.93
    cleaning_beam_agent-2_min: 175
    cleaning_beam_agent-3_max: 178
    cleaning_beam_agent-3_mean: 110.68
    cleaning_beam_agent-3_min: 72
    cleaning_beam_agent-4_max: 202
    cleaning_beam_agent-4_mean: 130.06
    cleaning_beam_agent-4_min: 70
    cleaning_beam_agent-5_max: 104
    cleaning_beam_agent-5_mean: 64.53
    cleaning_beam_agent-5_min: 41
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.04
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_18-47-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 294.9999999999992
  episode_reward_mean: 152.68999999999937
  episode_reward_min: -28.00000000000024
  episodes_this_iter: 96
  episodes_total: 3360
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20234.78
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010563264368101954
        entropy: 1.7700669765472412
        entropy_coeff: 0.0017600000137463212
        kl: 0.01192411407828331
        model: {}
        policy_loss: -0.011406145058572292
        total_loss: -0.013063115999102592
        vf_explained_var: -0.02583131194114685
        vf_loss: 2.659360885620117
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010563264368101954
        entropy: 1.7221509218215942
        entropy_coeff: 0.0017600000137463212
        kl: 0.01102378685027361
        model: {}
        policy_loss: -0.01469120942056179
        total_loss: -0.016362430527806282
        vf_explained_var: 0.006556302309036255
        vf_loss: 2.5738415718078613
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010563264368101954
        entropy: 1.5438517332077026
        entropy_coeff: 0.0017600000137463212
        kl: 0.01203897688537836
        model: {}
        policy_loss: -0.015023545362055302
        total_loss: -0.01630515046417713
        vf_explained_var: 0.10550715029239655
        vf_loss: 2.3167717456817627
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010563264368101954
        entropy: 1.7711493968963623
        entropy_coeff: 0.0017600000137463212
        kl: 0.01231381669640541
        model: {}
        policy_loss: -0.012726962566375732
        total_loss: -0.01435995101928711
        vf_explained_var: 0.024096697568893433
        vf_loss: 2.528553009033203
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010563264368101954
        entropy: 1.7044342756271362
        entropy_coeff: 0.0017600000137463212
        kl: 0.012488920241594315
        model: {}
        policy_loss: -0.014633196406066418
        total_loss: -0.01613631285727024
        vf_explained_var: 0.0434495210647583
        vf_loss: 2.477971315383911
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010563264368101954
        entropy: 1.743006706237793
        entropy_coeff: 0.0017600000137463212
        kl: 0.012545229867100716
        model: {}
        policy_loss: -0.01853765733540058
        total_loss: -0.020095262676477432
        vf_explained_var: 0.013464361429214478
        vf_loss: 2.5556368827819824
    load_time_ms: 31258.875
    num_steps_sampled: 3360000
    num_steps_trained: 3360000
    sample_time_ms: 111309.399
    update_time_ms: 73.151
  iterations_since_restore: 15
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 13.348471615720525
    ram_util_percent: 10.189956331877728
  pid: 4061
  policy_reward_max:
    agent-0: 49.16666666666653
    agent-1: 49.16666666666653
    agent-2: 49.16666666666653
    agent-3: 49.16666666666653
    agent-4: 49.16666666666653
    agent-5: 49.16666666666653
  policy_reward_mean:
    agent-0: 25.44833333333335
    agent-1: 25.44833333333335
    agent-2: 25.44833333333335
    agent-3: 25.44833333333335
    agent-4: 25.44833333333335
    agent-5: 25.44833333333335
  policy_reward_min:
    agent-0: -4.6666666666666625
    agent-1: -4.6666666666666625
    agent-2: -4.6666666666666625
    agent-3: -4.6666666666666625
    agent-4: -4.6666666666666625
    agent-5: -4.6666666666666625
  sampler_perf:
    mean_env_wait_ms: 22.61528421906246
    mean_inference_ms: 12.546487754186211
    mean_processing_ms: 51.91134899470648
  time_since_restore: 2559.2031903266907
  time_this_iter_s: 160.95173597335815
  time_total_s: 5770.266876459122
  timestamp: 1637020065
  timesteps_since_restore: 1440000
  timesteps_this_iter: 96000
  timesteps_total: 3360000
  training_iteration: 35
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     35 |          5770.27 | 3360000 |   152.69 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 45
    apples_agent-0_mean: 5.39
    apples_agent-0_min: 0
    apples_agent-1_max: 59
    apples_agent-1_mean: 6.28
    apples_agent-1_min: 0
    apples_agent-2_max: 52
    apples_agent-2_mean: 9.28
    apples_agent-2_min: 0
    apples_agent-3_max: 128
    apples_agent-3_mean: 51.03
    apples_agent-3_min: 2
    apples_agent-4_max: 56
    apples_agent-4_mean: 7.35
    apples_agent-4_min: 0
    apples_agent-5_max: 82
    apples_agent-5_mean: 31.96
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 265
    cleaning_beam_agent-0_mean: 208.99
    cleaning_beam_agent-0_min: 139
    cleaning_beam_agent-1_max: 269
    cleaning_beam_agent-1_mean: 151.53
    cleaning_beam_agent-1_min: 71
    cleaning_beam_agent-2_max: 550
    cleaning_beam_agent-2_mean: 386.32
    cleaning_beam_agent-2_min: 157
    cleaning_beam_agent-3_max: 177
    cleaning_beam_agent-3_mean: 114.75
    cleaning_beam_agent-3_min: 62
    cleaning_beam_agent-4_max: 247
    cleaning_beam_agent-4_mean: 145.47
    cleaning_beam_agent-4_min: 66
    cleaning_beam_agent-5_max: 112
    cleaning_beam_agent-5_mean: 73.32
    cleaning_beam_agent-5_min: 41
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_18-50-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 327.99999999999983
  episode_reward_mean: 176.34999999999877
  episode_reward_min: 15.9999999999998
  episodes_this_iter: 96
  episodes_total: 3456
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20245.011
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010503360535949469
        entropy: 1.7493212223052979
        entropy_coeff: 0.0017600000137463212
        kl: 0.00979037769138813
        model: {}
        policy_loss: -0.011832032352685928
        total_loss: -0.013640569522976875
        vf_explained_var: -0.023000329732894897
        vf_loss: 2.9123096466064453
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010503360535949469
        entropy: 1.7430378198623657
        entropy_coeff: 0.0017600000137463212
        kl: 0.013304252177476883
        model: {}
        policy_loss: -0.014048876240849495
        total_loss: -0.01551271416246891
        vf_explained_var: 0.03993973135948181
        vf_loss: 2.7348620891571045
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010503360535949469
        entropy: 1.4910612106323242
        entropy_coeff: 0.0017600000137463212
        kl: 0.012542509473860264
        model: {}
        policy_loss: -0.012624514289200306
        total_loss: -0.01373286359012127
        vf_explained_var: 0.08187685906887054
        vf_loss: 2.6166868209838867
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010503360535949469
        entropy: 1.724297046661377
        entropy_coeff: 0.0017600000137463212
        kl: 0.012317846529185772
        model: {}
        policy_loss: -0.010755554772913456
        total_loss: -0.012284282594919205
        vf_explained_var: 0.03717280924320221
        vf_loss: 2.742495059967041
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010503360535949469
        entropy: 1.6788264513015747
        entropy_coeff: 0.0017600000137463212
        kl: 0.013814971782267094
        model: {}
        policy_loss: -0.015225766226649284
        total_loss: -0.016527824103832245
        vf_explained_var: 0.04743552207946777
        vf_loss: 2.711794137954712
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010503360535949469
        entropy: 1.7944986820220947
        entropy_coeff: 0.0017600000137463212
        kl: 0.014108729548752308
        model: {}
        policy_loss: -0.01953725516796112
        total_loss: -0.02100534178316593
        vf_explained_var: 0.020591482520103455
        vf_loss: 2.7935750484466553
    load_time_ms: 31121.268
    num_steps_sampled: 3456000
    num_steps_trained: 3456000
    sample_time_ms: 110794.815
    update_time_ms: 72.807
  iterations_since_restore: 16
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 12.878205128205128
    ram_util_percent: 10.203846153846158
  pid: 4061
  policy_reward_max:
    agent-0: 54.66666666666655
    agent-1: 54.66666666666655
    agent-2: 54.66666666666655
    agent-3: 54.66666666666655
    agent-4: 54.66666666666655
    agent-5: 54.66666666666655
  policy_reward_mean:
    agent-0: 29.391666666666673
    agent-1: 29.391666666666673
    agent-2: 29.391666666666673
    agent-3: 29.391666666666673
    agent-4: 29.391666666666673
    agent-5: 29.391666666666673
  policy_reward_min:
    agent-0: 2.6666666666666696
    agent-1: 2.6666666666666696
    agent-2: 2.6666666666666696
    agent-3: 2.6666666666666696
    agent-4: 2.6666666666666696
    agent-5: 2.6666666666666696
  sampler_perf:
    mean_env_wait_ms: 22.624677290177647
    mean_inference_ms: 12.52843315156065
    mean_processing_ms: 51.83768512793745
  time_since_restore: 2723.421040058136
  time_this_iter_s: 164.2178497314453
  time_total_s: 5934.484726190567
  timestamp: 1637020229
  timesteps_since_restore: 1536000
  timesteps_this_iter: 96000
  timesteps_total: 3456000
  training_iteration: 36
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     36 |          5934.48 | 3456000 |   176.35 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 32
    apples_agent-0_mean: 5.68
    apples_agent-0_min: 0
    apples_agent-1_max: 39
    apples_agent-1_mean: 5.44
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 10.01
    apples_agent-2_min: 0
    apples_agent-3_max: 129
    apples_agent-3_mean: 41.91
    apples_agent-3_min: 2
    apples_agent-4_max: 35
    apples_agent-4_mean: 5.21
    apples_agent-4_min: 0
    apples_agent-5_max: 84
    apples_agent-5_mean: 31.27
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 272
    cleaning_beam_agent-0_mean: 195.31
    cleaning_beam_agent-0_min: 139
    cleaning_beam_agent-1_max: 302
    cleaning_beam_agent-1_mean: 160.05
    cleaning_beam_agent-1_min: 52
    cleaning_beam_agent-2_max: 550
    cleaning_beam_agent-2_mean: 295.67
    cleaning_beam_agent-2_min: 73
    cleaning_beam_agent-3_max: 158
    cleaning_beam_agent-3_mean: 112.45
    cleaning_beam_agent-3_min: 83
    cleaning_beam_agent-4_max: 200
    cleaning_beam_agent-4_mean: 137.54
    cleaning_beam_agent-4_min: 69
    cleaning_beam_agent-5_max: 106
    cleaning_beam_agent-5_mean: 54.55
    cleaning_beam_agent-5_min: 31
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.04
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_18-53-22
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 317.0000000000006
  episode_reward_mean: 161.82999999999925
  episode_reward_min: 9.999999999999797
  episodes_this_iter: 96
  episodes_total: 3552
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20245.155
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010443455539643764
        entropy: 1.7375797033309937
        entropy_coeff: 0.0017600000137463212
        kl: 0.011903314851224422
        model: {}
        policy_loss: -0.013385911472141743
        total_loss: -0.014997941441833973
        vf_explained_var: -0.04255160689353943
        vf_loss: 2.5577774047851562
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010443455539643764
        entropy: 1.7259678840637207
        entropy_coeff: 0.0017600000137463212
        kl: 0.011504840105772018
        model: {}
        policy_loss: -0.01672825962305069
        total_loss: -0.018377583473920822
        vf_explained_var: 0.02916647493839264
        vf_loss: 2.378955841064453
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010443455539643764
        entropy: 1.538813591003418
        entropy_coeff: 0.0017600000137463212
        kl: 0.011008144356310368
        model: {}
        policy_loss: -0.01458941400051117
        total_loss: -0.01596865989267826
        vf_explained_var: 0.06853581964969635
        vf_loss: 2.282496929168701
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010443455539643764
        entropy: 1.7068047523498535
        entropy_coeff: 0.0017600000137463212
        kl: 0.012168342247605324
        model: {}
        policy_loss: -0.013524815440177917
        total_loss: -0.015076794661581516
        vf_explained_var: 0.0399056077003479
        vf_loss: 2.351635217666626
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010443455539643764
        entropy: 1.6820945739746094
        entropy_coeff: 0.0017600000137463212
        kl: 0.012289493344724178
        model: {}
        policy_loss: -0.016763683408498764
        total_loss: -0.018266160041093826
        vf_explained_var: 0.06432411074638367
        vf_loss: 2.2905960083007812
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010443455539643764
        entropy: 1.7432341575622559
        entropy_coeff: 0.0017600000137463212
        kl: 0.012991833500564098
        model: {}
        policy_loss: -0.019983328878879547
        total_loss: -0.02152012661099434
        vf_explained_var: 0.05331231653690338
        vf_loss: 2.3210980892181396
    load_time_ms: 31654.881
    num_steps_sampled: 3552000
    num_steps_trained: 3552000
    sample_time_ms: 110839.822
    update_time_ms: 67.534
  iterations_since_restore: 17
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 18.63238866396761
    ram_util_percent: 13.596356275303645
  pid: 4061
  policy_reward_max:
    agent-0: 52.833333333333165
    agent-1: 52.833333333333165
    agent-2: 52.833333333333165
    agent-3: 52.833333333333165
    agent-4: 52.833333333333165
    agent-5: 52.833333333333165
  policy_reward_mean:
    agent-0: 26.971666666666682
    agent-1: 26.971666666666682
    agent-2: 26.971666666666682
    agent-3: 26.971666666666682
    agent-4: 26.971666666666682
    agent-5: 26.971666666666682
  policy_reward_min:
    agent-0: 1.6666666666666694
    agent-1: 1.6666666666666694
    agent-2: 1.6666666666666694
    agent-3: 1.6666666666666694
    agent-4: 1.6666666666666694
    agent-5: 1.6666666666666694
  sampler_perf:
    mean_env_wait_ms: 22.6164753931272
    mean_inference_ms: 12.516425534193289
    mean_processing_ms: 51.80493749693691
  time_since_restore: 2896.018956184387
  time_this_iter_s: 172.59791612625122
  time_total_s: 6107.082642316818
  timestamp: 1637020402
  timesteps_since_restore: 1632000
  timesteps_this_iter: 96000
  timesteps_total: 3552000
  training_iteration: 37
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     37 |          6107.08 | 3552000 |   161.83 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 36
    apples_agent-0_mean: 5.24
    apples_agent-0_min: 0
    apples_agent-1_max: 44
    apples_agent-1_mean: 7.75
    apples_agent-1_min: 0
    apples_agent-2_max: 57
    apples_agent-2_mean: 13.16
    apples_agent-2_min: 0
    apples_agent-3_max: 180
    apples_agent-3_mean: 48.6
    apples_agent-3_min: 7
    apples_agent-4_max: 41
    apples_agent-4_mean: 4.1
    apples_agent-4_min: 0
    apples_agent-5_max: 86
    apples_agent-5_mean: 34.8
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 271
    cleaning_beam_agent-0_mean: 210.35
    cleaning_beam_agent-0_min: 150
    cleaning_beam_agent-1_max: 276
    cleaning_beam_agent-1_mean: 124.42
    cleaning_beam_agent-1_min: 50
    cleaning_beam_agent-2_max: 447
    cleaning_beam_agent-2_mean: 283.72
    cleaning_beam_agent-2_min: 73
    cleaning_beam_agent-3_max: 165
    cleaning_beam_agent-3_mean: 115.64
    cleaning_beam_agent-3_min: 77
    cleaning_beam_agent-4_max: 372
    cleaning_beam_agent-4_mean: 165.63
    cleaning_beam_agent-4_min: 67
    cleaning_beam_agent-5_max: 111
    cleaning_beam_agent-5_mean: 65.7
    cleaning_beam_agent-5_min: 32
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.07
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.07
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.07
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_18-55-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 399.0000000000085
  episode_reward_mean: 186.47999999999874
  episode_reward_min: 35.0
  episodes_this_iter: 96
  episodes_total: 3648
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20250.021
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010383551707491279
        entropy: 1.7246768474578857
        entropy_coeff: 0.0017600000137463212
        kl: 0.00992578361183405
        model: {}
        policy_loss: -0.013096189126372337
        total_loss: -0.014843249693512917
        vf_explained_var: -0.03069344162940979
        vf_loss: 2.957940101623535
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010383551707491279
        entropy: 1.70116126537323
        entropy_coeff: 0.0017600000137463212
        kl: 0.011169090867042542
        model: {}
        policy_loss: -0.016552302986383438
        total_loss: -0.018154671415686607
        vf_explained_var: 0.04281546175479889
        vf_loss: 2.747690439224243
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010383551707491279
        entropy: 1.5271978378295898
        entropy_coeff: 0.0017600000137463212
        kl: 0.011963363736867905
        model: {}
        policy_loss: -0.01650131493806839
        total_loss: -0.017728017643094063
        vf_explained_var: 0.07683499157428741
        vf_loss: 2.6483311653137207
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010383551707491279
        entropy: 1.673158049583435
        entropy_coeff: 0.0017600000137463212
        kl: 0.011525014415383339
        model: {}
        policy_loss: -0.014025415293872356
        total_loss: -0.01554399449378252
        vf_explained_var: 0.046342119574546814
        vf_loss: 2.7368078231811523
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010383551707491279
        entropy: 1.6612900495529175
        entropy_coeff: 0.0017600000137463212
        kl: 0.014586875215172768
        model: {}
        policy_loss: -0.016333134844899178
        total_loss: -0.017526153475046158
        vf_explained_var: 0.051104843616485596
        vf_loss: 2.721618890762329
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010383551707491279
        entropy: 1.7482448816299438
        entropy_coeff: 0.0017600000137463212
        kl: 0.014086360111832619
        model: {}
        policy_loss: -0.01940830796957016
        total_loss: -0.020808055996894836
        vf_explained_var: 0.0650402158498764
        vf_loss: 2.6852431297302246
    load_time_ms: 30322.121
    num_steps_sampled: 3648000
    num_steps_trained: 3648000
    sample_time_ms: 109088.171
    update_time_ms: 66.995
  iterations_since_restore: 18
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.179601990049747
    ram_util_percent: 13.990049751243783
  pid: 4061
  policy_reward_max:
    agent-0: 66.49999999999979
    agent-1: 66.49999999999979
    agent-2: 66.49999999999979
    agent-3: 66.49999999999979
    agent-4: 66.49999999999979
    agent-5: 66.49999999999979
  policy_reward_mean:
    agent-0: 31.080000000000005
    agent-1: 31.080000000000005
    agent-2: 31.080000000000005
    agent-3: 31.080000000000005
    agent-4: 31.080000000000005
    agent-5: 31.080000000000005
  policy_reward_min:
    agent-0: 5.833333333333334
    agent-1: 5.833333333333334
    agent-2: 5.833333333333334
    agent-3: 5.833333333333334
    agent-4: 5.833333333333334
    agent-5: 5.833333333333334
  sampler_perf:
    mean_env_wait_ms: 22.604379226032826
    mean_inference_ms: 12.505087409588622
    mean_processing_ms: 51.7394672506843
  time_since_restore: 3037.0075063705444
  time_this_iter_s: 140.98855018615723
  time_total_s: 6248.0711925029755
  timestamp: 1637020543
  timesteps_since_restore: 1728000
  timesteps_this_iter: 96000
  timesteps_total: 3648000
  training_iteration: 38
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     38 |          6248.07 | 3648000 |   186.48 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 30
    apples_agent-0_mean: 5.27
    apples_agent-0_min: 0
    apples_agent-1_max: 71
    apples_agent-1_mean: 10.82
    apples_agent-1_min: 0
    apples_agent-2_max: 71
    apples_agent-2_mean: 10.14
    apples_agent-2_min: 0
    apples_agent-3_max: 122
    apples_agent-3_mean: 52.69
    apples_agent-3_min: 5
    apples_agent-4_max: 55
    apples_agent-4_mean: 5.1
    apples_agent-4_min: 0
    apples_agent-5_max: 84
    apples_agent-5_mean: 35.33
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 309
    cleaning_beam_agent-0_mean: 234.08
    cleaning_beam_agent-0_min: 142
    cleaning_beam_agent-1_max: 299
    cleaning_beam_agent-1_mean: 126.48
    cleaning_beam_agent-1_min: 51
    cleaning_beam_agent-2_max: 455
    cleaning_beam_agent-2_mean: 306.72
    cleaning_beam_agent-2_min: 121
    cleaning_beam_agent-3_max: 159
    cleaning_beam_agent-3_mean: 109.79
    cleaning_beam_agent-3_min: 70
    cleaning_beam_agent-4_max: 392
    cleaning_beam_agent-4_mean: 199.94
    cleaning_beam_agent-4_min: 76
    cleaning_beam_agent-5_max: 107
    cleaning_beam_agent-5_mean: 74.89
    cleaning_beam_agent-5_min: 39
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.05
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_18-58-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 354.0000000000017
  episode_reward_mean: 203.88999999999862
  episode_reward_min: 64.99999999999974
  episodes_this_iter: 96
  episodes_total: 3744
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20258.841
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010323647875338793
        entropy: 1.697279930114746
        entropy_coeff: 0.0017600000137463212
        kl: 0.011378600262105465
        model: {}
        policy_loss: -0.014846728183329105
        total_loss: -0.01636812463402748
        vf_explained_var: 0.017771288752555847
        vf_loss: 3.2795612812042236
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010323647875338793
        entropy: 1.6789724826812744
        entropy_coeff: 0.0017600000137463212
        kl: 0.012821907177567482
        model: {}
        policy_loss: -0.017981339246034622
        total_loss: -0.019336728379130363
        vf_explained_var: 0.04960349202156067
        vf_loss: 3.174116373062134
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010323647875338793
        entropy: 1.5174560546875
        entropy_coeff: 0.0017600000137463212
        kl: 0.01217359583824873
        model: {}
        policy_loss: -0.016738180071115494
        total_loss: -0.017877861857414246
        vf_explained_var: 0.06106366217136383
        vf_loss: 3.1367852687835693
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010323647875338793
        entropy: 1.6628687381744385
        entropy_coeff: 0.0017600000137463212
        kl: 0.013817435130476952
        model: {}
        policy_loss: -0.012735005468130112
        total_loss: -0.013966089114546776
        vf_explained_var: 0.0595880001783371
        vf_loss: 3.138223886489868
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010323647875338793
        entropy: 1.6412440538406372
        entropy_coeff: 0.0017600000137463212
        kl: 0.01263807900249958
        model: {}
        policy_loss: -0.018705355003476143
        total_loss: -0.020018188282847404
        vf_explained_var: 0.06586158275604248
        vf_loss: 3.1195240020751953
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010323647875338793
        entropy: 1.6907463073730469
        entropy_coeff: 0.0017600000137463212
        kl: 0.01372123509645462
        model: {}
        policy_loss: -0.020625924691557884
        total_loss: -0.021928710862994194
        vf_explained_var: 0.0991295725107193
        vf_loss: 3.008068084716797
    load_time_ms: 30223.42
    num_steps_sampled: 3744000
    num_steps_trained: 3744000
    sample_time_ms: 108176.054
    update_time_ms: 61.372
  iterations_since_restore: 19
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.22
    ram_util_percent: 14.063500000000001
  pid: 4061
  policy_reward_max:
    agent-0: 58.99999999999985
    agent-1: 58.99999999999985
    agent-2: 58.99999999999985
    agent-3: 58.99999999999985
    agent-4: 58.99999999999985
    agent-5: 58.99999999999985
  policy_reward_mean:
    agent-0: 33.98166666666666
    agent-1: 33.98166666666666
    agent-2: 33.98166666666666
    agent-3: 33.98166666666666
    agent-4: 33.98166666666666
    agent-5: 33.98166666666666
  policy_reward_min:
    agent-0: 10.833333333333327
    agent-1: 10.833333333333327
    agent-2: 10.833333333333327
    agent-3: 10.833333333333327
    agent-4: 10.833333333333327
    agent-5: 10.833333333333327
  sampler_perf:
    mean_env_wait_ms: 22.611752504365683
    mean_inference_ms: 12.494966227047986
    mean_processing_ms: 51.69993658998542
  time_since_restore: 3177.487622976303
  time_this_iter_s: 140.48011660575867
  time_total_s: 6388.551309108734
  timestamp: 1637020684
  timesteps_since_restore: 1824000
  timesteps_this_iter: 96000
  timesteps_total: 3744000
  training_iteration: 39
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     39 |          6388.55 | 3744000 |   203.89 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 25
    apples_agent-0_mean: 5.05
    apples_agent-0_min: 0
    apples_agent-1_max: 59
    apples_agent-1_mean: 9.03
    apples_agent-1_min: 0
    apples_agent-2_max: 75
    apples_agent-2_mean: 12.99
    apples_agent-2_min: 0
    apples_agent-3_max: 124
    apples_agent-3_mean: 51.51
    apples_agent-3_min: 9
    apples_agent-4_max: 86
    apples_agent-4_mean: 9.18
    apples_agent-4_min: 0
    apples_agent-5_max: 102
    apples_agent-5_mean: 45.27
    apples_agent-5_min: 2
    cleaning_beam_agent-0_max: 379
    cleaning_beam_agent-0_mean: 279.51
    cleaning_beam_agent-0_min: 191
    cleaning_beam_agent-1_max: 375
    cleaning_beam_agent-1_mean: 199.87
    cleaning_beam_agent-1_min: 70
    cleaning_beam_agent-2_max: 531
    cleaning_beam_agent-2_mean: 324.35
    cleaning_beam_agent-2_min: 139
    cleaning_beam_agent-3_max: 161
    cleaning_beam_agent-3_mean: 119.12
    cleaning_beam_agent-3_min: 77
    cleaning_beam_agent-4_max: 381
    cleaning_beam_agent-4_mean: 192.72
    cleaning_beam_agent-4_min: 71
    cleaning_beam_agent-5_max: 111
    cleaning_beam_agent-5_mean: 76.38
    cleaning_beam_agent-5_min: 51
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.05
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.08
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.04
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_19-00-17
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 379.00000000000546
  episode_reward_mean: 229.18999999999897
  episode_reward_min: 77.0
  episodes_this_iter: 96
  episodes_total: 3840
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20229.756
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010263744043186307
        entropy: 1.6281291246414185
        entropy_coeff: 0.0017600000137463212
        kl: 0.012398744001984596
        model: {}
        policy_loss: -0.014388484880328178
        total_loss: -0.015650304034352303
        vf_explained_var: 0.02917546033859253
        vf_loss: 3.6381030082702637
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010263744043186307
        entropy: 1.6639868021011353
        entropy_coeff: 0.0017600000137463212
        kl: 0.011867204681038857
        model: {}
        policy_loss: -0.019302234053611755
        total_loss: -0.02068989910185337
        vf_explained_var: 0.054558709263801575
        vf_loss: 3.54233455657959
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010263744043186307
        entropy: 1.4800512790679932
        entropy_coeff: 0.0017600000137463212
        kl: 0.013212276622653008
        model: {}
        policy_loss: -0.016857722774147987
        total_loss: -0.017796630039811134
        vf_explained_var: 0.08058927953243256
        vf_loss: 3.4475321769714355
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010263744043186307
        entropy: 1.6453895568847656
        entropy_coeff: 0.0017600000137463212
        kl: 0.010437640361487865
        model: {}
        policy_loss: -0.013255888596177101
        total_loss: -0.01476266048848629
        vf_explained_var: 0.07934786379337311
        vf_loss: 3.4534964561462402
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010263744043186307
        entropy: 1.6075024604797363
        entropy_coeff: 0.0017600000137463212
        kl: 0.01285762619227171
        model: {}
        policy_loss: -0.01961429975926876
        total_loss: -0.0207987017929554
        vf_explained_var: 0.04217180609703064
        vf_loss: 3.5903587341308594
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010263744043186307
        entropy: 1.706716775894165
        entropy_coeff: 0.0017600000137463212
        kl: 0.013108151033520699
        model: {}
        policy_loss: -0.02036525309085846
        total_loss: -0.02172250486910343
        vf_explained_var: 0.10460145771503448
        vf_loss: 3.357541084289551
    load_time_ms: 27687.957
    num_steps_sampled: 3840000
    num_steps_trained: 3840000
    sample_time_ms: 107482.91
    update_time_ms: 59.135
  iterations_since_restore: 20
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.940740740740743
    ram_util_percent: 13.973015873015873
  pid: 4061
  policy_reward_max:
    agent-0: 63.16666666666644
    agent-1: 63.16666666666644
    agent-2: 63.16666666666644
    agent-3: 63.16666666666644
    agent-4: 63.16666666666644
    agent-5: 63.16666666666644
  policy_reward_mean:
    agent-0: 38.198333333333295
    agent-1: 38.198333333333295
    agent-2: 38.198333333333295
    agent-3: 38.198333333333295
    agent-4: 38.198333333333295
    agent-5: 38.198333333333295
  policy_reward_min:
    agent-0: 12.833333333333321
    agent-1: 12.833333333333321
    agent-2: 12.833333333333321
    agent-3: 12.833333333333321
    agent-4: 12.833333333333321
    agent-5: 12.833333333333321
  sampler_perf:
    mean_env_wait_ms: 22.647918424006516
    mean_inference_ms: 12.487392524779377
    mean_processing_ms: 51.6658479227549
  time_since_restore: 3310.1136527061462
  time_this_iter_s: 132.62602972984314
  time_total_s: 6521.177338838577
  timestamp: 1637020817
  timesteps_since_restore: 1920000
  timesteps_this_iter: 96000
  timesteps_total: 3840000
  training_iteration: 40
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     40 |          6521.18 | 3840000 |   229.19 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 38
    apples_agent-0_mean: 4.82
    apples_agent-0_min: 0
    apples_agent-1_max: 46
    apples_agent-1_mean: 8.93
    apples_agent-1_min: 0
    apples_agent-2_max: 81
    apples_agent-2_mean: 10.84
    apples_agent-2_min: 0
    apples_agent-3_max: 115
    apples_agent-3_mean: 44.07
    apples_agent-3_min: 4
    apples_agent-4_max: 107
    apples_agent-4_mean: 11.36
    apples_agent-4_min: 0
    apples_agent-5_max: 125
    apples_agent-5_mean: 38.74
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 417
    cleaning_beam_agent-0_mean: 257.05
    cleaning_beam_agent-0_min: 145
    cleaning_beam_agent-1_max: 394
    cleaning_beam_agent-1_mean: 205.88
    cleaning_beam_agent-1_min: 68
    cleaning_beam_agent-2_max: 527
    cleaning_beam_agent-2_mean: 330.46
    cleaning_beam_agent-2_min: 153
    cleaning_beam_agent-3_max: 155
    cleaning_beam_agent-3_mean: 108.29
    cleaning_beam_agent-3_min: 69
    cleaning_beam_agent-4_max: 345
    cleaning_beam_agent-4_mean: 132.14
    cleaning_beam_agent-4_min: 56
    cleaning_beam_agent-5_max: 160
    cleaning_beam_agent-5_mean: 103.89
    cleaning_beam_agent-5_min: 40
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.06
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_19-02-35
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 393.0000000000056
  episode_reward_mean: 204.719999999999
  episode_reward_min: 38.99999999999994
  episodes_this_iter: 96
  episodes_total: 3936
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20239.623
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010203840211033821
        entropy: 1.645137071609497
        entropy_coeff: 0.0017600000137463212
        kl: 0.009747613221406937
        model: {}
        policy_loss: -0.014549288898706436
        total_loss: -0.016090229153633118
        vf_explained_var: 0.011543959379196167
        vf_loss: 3.797414779663086
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010203840211033821
        entropy: 1.6115471124649048
        entropy_coeff: 0.0017600000137463212
        kl: 0.01346573419868946
        model: {}
        policy_loss: -0.020053936168551445
        total_loss: -0.021182456985116005
        vf_explained_var: 0.05981843173503876
        vf_loss: 3.612301826477051
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010203840211033821
        entropy: 1.470410704612732
        entropy_coeff: 0.0017600000137463212
        kl: 0.012165588326752186
        model: {}
        policy_loss: -0.01627359353005886
        total_loss: -0.017285168170928955
        vf_explained_var: 0.06467068195343018
        vf_loss: 3.597914695739746
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010203840211033821
        entropy: 1.5932602882385254
        entropy_coeff: 0.0017600000137463212
        kl: 0.011595616117119789
        model: {}
        policy_loss: -0.016118938103318214
        total_loss: -0.01740901730954647
        vf_explained_var: 0.07737725973129272
        vf_loss: 3.54498291015625
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010203840211033821
        entropy: 1.567622423171997
        entropy_coeff: 0.0017600000137463212
        kl: 0.012268687598407269
        model: {}
        policy_loss: -0.02114713191986084
        total_loss: -0.022324025630950928
        vf_explained_var: 0.07492507994174957
        vf_loss: 3.552522659301758
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010203840211033821
        entropy: 1.7111533880233765
        entropy_coeff: 0.0017600000137463212
        kl: 0.016294479370117188
        model: {}
        policy_loss: -0.018882492557168007
        total_loss: -0.01992058753967285
        vf_explained_var: 0.1047162264585495
        vf_loss: 3.4408299922943115
    load_time_ms: 26303.293
    num_steps_sampled: 3936000
    num_steps_trained: 3936000
    sample_time_ms: 106173.523
    update_time_ms: 57.936
  iterations_since_restore: 21
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.46446700507614
    ram_util_percent: 13.948730964467007
  pid: 4061
  policy_reward_max:
    agent-0: 65.49999999999976
    agent-1: 65.49999999999976
    agent-2: 65.49999999999976
    agent-3: 65.49999999999976
    agent-4: 65.49999999999976
    agent-5: 65.49999999999976
  policy_reward_mean:
    agent-0: 34.119999999999976
    agent-1: 34.119999999999976
    agent-2: 34.119999999999976
    agent-3: 34.119999999999976
    agent-4: 34.119999999999976
    agent-5: 34.119999999999976
  policy_reward_min:
    agent-0: 6.500000000000001
    agent-1: 6.500000000000001
    agent-2: 6.500000000000001
    agent-3: 6.500000000000001
    agent-4: 6.500000000000001
    agent-5: 6.500000000000001
  sampler_perf:
    mean_env_wait_ms: 22.669395440572952
    mean_inference_ms: 12.48048979845481
    mean_processing_ms: 51.638912980346255
  time_since_restore: 3446.8763031959534
  time_this_iter_s: 136.76265048980713
  time_total_s: 6657.939989328384
  timestamp: 1637020955
  timesteps_since_restore: 2016000
  timesteps_this_iter: 96000
  timesteps_total: 3936000
  training_iteration: 41
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     41 |          6657.94 | 3936000 |   204.72 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 46
    apples_agent-0_mean: 5.68
    apples_agent-0_min: 0
    apples_agent-1_max: 71
    apples_agent-1_mean: 10.45
    apples_agent-1_min: 0
    apples_agent-2_max: 54
    apples_agent-2_mean: 9.85
    apples_agent-2_min: 0
    apples_agent-3_max: 103
    apples_agent-3_mean: 45.14
    apples_agent-3_min: 3
    apples_agent-4_max: 60
    apples_agent-4_mean: 8.63
    apples_agent-4_min: 0
    apples_agent-5_max: 103
    apples_agent-5_mean: 42.73
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 381
    cleaning_beam_agent-0_mean: 289.83
    cleaning_beam_agent-0_min: 166
    cleaning_beam_agent-1_max: 315
    cleaning_beam_agent-1_mean: 152.22
    cleaning_beam_agent-1_min: 68
    cleaning_beam_agent-2_max: 560
    cleaning_beam_agent-2_mean: 338.7
    cleaning_beam_agent-2_min: 160
    cleaning_beam_agent-3_max: 174
    cleaning_beam_agent-3_mean: 111.5
    cleaning_beam_agent-3_min: 75
    cleaning_beam_agent-4_max: 395
    cleaning_beam_agent-4_mean: 136.56
    cleaning_beam_agent-4_min: 59
    cleaning_beam_agent-5_max: 146
    cleaning_beam_agent-5_mean: 99.35
    cleaning_beam_agent-5_min: 57
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.1
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.05
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.13
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_19-04-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 373.00000000000483
  episode_reward_mean: 223.97999999999894
  episode_reward_min: 74.99999999999983
  episodes_this_iter: 96
  episodes_total: 4032
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20257.069
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010143936378881335
        entropy: 1.5449106693267822
        entropy_coeff: 0.0017600000137463212
        kl: 0.009498311206698418
        model: {}
        policy_loss: -0.013874726369976997
        total_loss: -0.015275022014975548
        vf_explained_var: 0.01013992726802826
        vf_loss: 3.689143657684326
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010143936378881335
        entropy: 1.6285847425460815
        entropy_coeff: 0.0017600000137463212
        kl: 0.012887479737401009
        model: {}
        policy_loss: -0.020569905638694763
        total_loss: -0.021795127540826797
        vf_explained_var: 0.05468808114528656
        vf_loss: 3.523378610610962
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010143936378881335
        entropy: 1.4631643295288086
        entropy_coeff: 0.0017600000137463212
        kl: 0.012904424220323563
        model: {}
        policy_loss: -0.01800449937582016
        total_loss: -0.01894674450159073
        vf_explained_var: 0.08033540844917297
        vf_loss: 3.4248223304748535
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010143936378881335
        entropy: 1.5728133916854858
        entropy_coeff: 0.0017600000137463212
        kl: 0.011504915542900562
        model: {}
        policy_loss: -0.014641404151916504
        total_loss: -0.01591634191572666
        vf_explained_var: 0.0811963677406311
        vf_loss: 3.4272165298461914
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010143936378881335
        entropy: 1.5718560218811035
        entropy_coeff: 0.0017600000137463212
        kl: 0.012903056107461452
        model: {}
        policy_loss: -0.021302565932273865
        total_loss: -0.022437024861574173
        vf_explained_var: 0.08214125037193298
        vf_loss: 3.4169747829437256
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010143936378881335
        entropy: 1.6980384588241577
        entropy_coeff: 0.0017600000137463212
        kl: 0.01296934299170971
        model: {}
        policy_loss: -0.02197743020951748
        total_loss: -0.023324724286794662
        vf_explained_var: 0.07622653245925903
        vf_loss: 3.4432015419006348
    load_time_ms: 25122.579
    num_steps_sampled: 4032000
    num_steps_trained: 4032000
    sample_time_ms: 103682.498
    update_time_ms: 51.453
  iterations_since_restore: 22
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.088709677419356
    ram_util_percent: 13.911290322580648
  pid: 4061
  policy_reward_max:
    agent-0: 62.16666666666643
    agent-1: 62.16666666666643
    agent-2: 62.16666666666643
    agent-3: 62.16666666666643
    agent-4: 62.16666666666643
    agent-5: 62.16666666666643
  policy_reward_mean:
    agent-0: 37.32999999999996
    agent-1: 37.32999999999996
    agent-2: 37.32999999999996
    agent-3: 37.32999999999996
    agent-4: 37.32999999999996
    agent-5: 37.32999999999996
  policy_reward_min:
    agent-0: 12.499999999999995
    agent-1: 12.499999999999995
    agent-2: 12.499999999999995
    agent-3: 12.499999999999995
    agent-4: 12.499999999999995
    agent-5: 12.499999999999995
  sampler_perf:
    mean_env_wait_ms: 22.683844588943085
    mean_inference_ms: 12.473385752332668
    mean_processing_ms: 51.610114585192676
  time_since_restore: 3577.866050720215
  time_this_iter_s: 130.98974752426147
  time_total_s: 6788.929736852646
  timestamp: 1637021086
  timesteps_since_restore: 2112000
  timesteps_this_iter: 96000
  timesteps_total: 4032000
  training_iteration: 42
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     42 |          6788.93 | 4032000 |   223.98 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 35
    apples_agent-0_mean: 5.61
    apples_agent-0_min: 0
    apples_agent-1_max: 89
    apples_agent-1_mean: 11.4
    apples_agent-1_min: 0
    apples_agent-2_max: 42
    apples_agent-2_mean: 8.95
    apples_agent-2_min: 0
    apples_agent-3_max: 108
    apples_agent-3_mean: 45.34
    apples_agent-3_min: 8
    apples_agent-4_max: 113
    apples_agent-4_mean: 10.53
    apples_agent-4_min: 0
    apples_agent-5_max: 136
    apples_agent-5_mean: 43.29
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 379
    cleaning_beam_agent-0_mean: 300.03
    cleaning_beam_agent-0_min: 204
    cleaning_beam_agent-1_max: 269
    cleaning_beam_agent-1_mean: 133.29
    cleaning_beam_agent-1_min: 47
    cleaning_beam_agent-2_max: 543
    cleaning_beam_agent-2_mean: 327.46
    cleaning_beam_agent-2_min: 136
    cleaning_beam_agent-3_max: 187
    cleaning_beam_agent-3_mean: 111.26
    cleaning_beam_agent-3_min: 71
    cleaning_beam_agent-4_max: 375
    cleaning_beam_agent-4_mean: 170.94
    cleaning_beam_agent-4_min: 67
    cleaning_beam_agent-5_max: 139
    cleaning_beam_agent-5_mean: 89.65
    cleaning_beam_agent-5_min: 43
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.05
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.06
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 5
    fire_beam_agent-4_mean: 0.14
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.07
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_19-07-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 388.0000000000081
  episode_reward_mean: 232.34999999999854
  episode_reward_min: 88.00000000000044
  episodes_this_iter: 96
  episodes_total: 4128
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20268.189
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.001008403254672885
        entropy: 1.5555434226989746
        entropy_coeff: 0.0017600000137463212
        kl: 0.009381572715938091
        model: {}
        policy_loss: -0.013690978288650513
        total_loss: -0.015102623030543327
        vf_explained_var: 0.017421066761016846
        vf_loss: 3.8795368671417236
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.001008403254672885
        entropy: 1.5953987836837769
        entropy_coeff: 0.0017600000137463212
        kl: 0.014386809431016445
        model: {}
        policy_loss: -0.021472742781043053
        total_loss: -0.022466696798801422
        vf_explained_var: 0.0494706928730011
        vf_loss: 3.75266432762146
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.001008403254672885
        entropy: 1.4589382410049438
        entropy_coeff: 0.0017600000137463212
        kl: 0.013521470129489899
        model: {}
        policy_loss: -0.018635723739862442
        total_loss: -0.019493239000439644
        vf_explained_var: 0.09306569397449493
        vf_loss: 3.5806593894958496
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.001008403254672885
        entropy: 1.5473811626434326
        entropy_coeff: 0.0017600000137463212
        kl: 0.012651844881474972
        model: {}
        policy_loss: -0.014713134616613388
        total_loss: -0.01580602303147316
        vf_explained_var: 0.07618819177150726
        vf_loss: 3.65313982963562
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.001008403254672885
        entropy: 1.5207784175872803
        entropy_coeff: 0.0017600000137463212
        kl: 0.012866426259279251
        model: {}
        policy_loss: -0.02084013819694519
        total_loss: -0.021867096424102783
        vf_explained_var: 0.08008742332458496
        vf_loss: 3.6297073364257812
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.001008403254672885
        entropy: 1.6574617624282837
        entropy_coeff: 0.0017600000137463212
        kl: 0.013592751696705818
        model: {}
        policy_loss: -0.02230047434568405
        total_loss: -0.023501673713326454
        vf_explained_var: 0.09590208530426025
        vf_loss: 3.5666027069091797
    load_time_ms: 25865.128
    num_steps_sampled: 4128000
    num_steps_trained: 4128000
    sample_time_ms: 101920.806
    update_time_ms: 52.213
  iterations_since_restore: 23
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.907692307692308
    ram_util_percent: 14.109615384615388
  pid: 4061
  policy_reward_max:
    agent-0: 64.66666666666647
    agent-1: 64.66666666666647
    agent-2: 64.66666666666647
    agent-3: 64.66666666666647
    agent-4: 64.66666666666647
    agent-5: 64.66666666666647
  policy_reward_mean:
    agent-0: 38.72499999999997
    agent-1: 38.72499999999997
    agent-2: 38.72499999999997
    agent-3: 38.72499999999997
    agent-4: 38.72499999999997
    agent-5: 38.72499999999997
  policy_reward_min:
    agent-0: 14.66666666666665
    agent-1: 14.66666666666665
    agent-2: 14.66666666666665
    agent-3: 14.66666666666665
    agent-4: 14.66666666666665
    agent-5: 14.66666666666665
  sampler_perf:
    mean_env_wait_ms: 22.698218595513794
    mean_inference_ms: 12.469736394750363
    mean_processing_ms: 51.63068454713662
  time_since_restore: 3723.5499069690704
  time_this_iter_s: 145.6838562488556
  time_total_s: 6934.6135931015015
  timestamp: 1637021232
  timesteps_since_restore: 2208000
  timesteps_this_iter: 96000
  timesteps_total: 4128000
  training_iteration: 43
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     43 |          6934.61 | 4128000 |   232.35 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 35
    apples_agent-0_mean: 5.64
    apples_agent-0_min: 0
    apples_agent-1_max: 66
    apples_agent-1_mean: 12.31
    apples_agent-1_min: 0
    apples_agent-2_max: 56
    apples_agent-2_mean: 9.9
    apples_agent-2_min: 0
    apples_agent-3_max: 103
    apples_agent-3_mean: 45.11
    apples_agent-3_min: 2
    apples_agent-4_max: 102
    apples_agent-4_mean: 11.49
    apples_agent-4_min: 0
    apples_agent-5_max: 99
    apples_agent-5_mean: 42.69
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 401
    cleaning_beam_agent-0_mean: 321.04
    cleaning_beam_agent-0_min: 205
    cleaning_beam_agent-1_max: 386
    cleaning_beam_agent-1_mean: 174.31
    cleaning_beam_agent-1_min: 57
    cleaning_beam_agent-2_max: 633
    cleaning_beam_agent-2_mean: 342.55
    cleaning_beam_agent-2_min: 137
    cleaning_beam_agent-3_max: 162
    cleaning_beam_agent-3_mean: 103.35
    cleaning_beam_agent-3_min: 66
    cleaning_beam_agent-4_max: 357
    cleaning_beam_agent-4_mean: 144.89
    cleaning_beam_agent-4_min: 58
    cleaning_beam_agent-5_max: 124
    cleaning_beam_agent-5_mean: 82.92
    cleaning_beam_agent-5_min: 47
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.06
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 3
    fire_beam_agent-4_mean: 0.3
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_19-09-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 394.0000000000053
  episode_reward_mean: 250.1599999999993
  episode_reward_min: 63.999999999999716
  episodes_this_iter: 96
  episodes_total: 4224
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20233.635
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010024127550423145
        entropy: 1.5539178848266602
        entropy_coeff: 0.0017600000137463212
        kl: 0.010180859826505184
        model: {}
        policy_loss: -0.014594842679798603
        total_loss: -0.01583290286362171
        vf_explained_var: 0.05529406666755676
        vf_loss: 4.7874603271484375
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010024127550423145
        entropy: 1.5689902305603027
        entropy_coeff: 0.0017600000137463212
        kl: 0.014126104302704334
        model: {}
        policy_loss: -0.02225193753838539
        total_loss: -0.023128774017095566
        vf_explained_var: 0.06879588961601257
        vf_loss: 4.719770431518555
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010024127550423145
        entropy: 1.405801773071289
        entropy_coeff: 0.0017600000137463212
        kl: 0.012971164658665657
        model: {}
        policy_loss: -0.017691388726234436
        total_loss: -0.018395699560642242
        vf_explained_var: 0.06757110357284546
        vf_loss: 4.72783088684082
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010024127550423145
        entropy: 1.5133792161941528
        entropy_coeff: 0.0017600000137463212
        kl: 0.011844629421830177
        model: {}
        policy_loss: -0.016663840040564537
        total_loss: -0.01768467202782631
        vf_explained_var: 0.09634120762348175
        vf_loss: 4.582531452178955
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010024127550423145
        entropy: 1.4925240278244019
        entropy_coeff: 0.0017600000137463212
        kl: 0.01242339238524437
        model: {}
        policy_loss: -0.022235747426748276
        total_loss: -0.02315758354961872
        vf_explained_var: 0.08666543662548065
        vf_loss: 4.626653671264648
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0010024127550423145
        entropy: 1.6289159059524536
        entropy_coeff: 0.0017600000137463212
        kl: 0.015179743990302086
        model: {}
        policy_loss: -0.021005284041166306
        total_loss: -0.021890398114919662
        vf_explained_var: 0.08676864206790924
        vf_loss: 4.638067245483398
    load_time_ms: 25614.874
    num_steps_sampled: 4224000
    num_steps_trained: 4224000
    sample_time_ms: 100221.428
    update_time_ms: 50.176
  iterations_since_restore: 24
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.601025641025643
    ram_util_percent: 14.013846153846158
  pid: 4061
  policy_reward_max:
    agent-0: 65.66666666666643
    agent-1: 65.66666666666643
    agent-2: 65.66666666666643
    agent-3: 65.66666666666643
    agent-4: 65.66666666666643
    agent-5: 65.66666666666643
  policy_reward_mean:
    agent-0: 41.69333333333328
    agent-1: 41.69333333333328
    agent-2: 41.69333333333328
    agent-3: 41.69333333333328
    agent-4: 41.69333333333328
    agent-5: 41.69333333333328
  policy_reward_min:
    agent-0: 10.666666666666663
    agent-1: 10.666666666666663
    agent-2: 10.666666666666663
    agent-3: 10.666666666666663
    agent-4: 10.666666666666663
    agent-5: 10.666666666666663
  sampler_perf:
    mean_env_wait_ms: 22.717387139932494
    mean_inference_ms: 12.463167204817253
    mean_processing_ms: 51.65361775574259
  time_since_restore: 3860.6488087177277
  time_this_iter_s: 137.09890174865723
  time_total_s: 7071.712494850159
  timestamp: 1637021369
  timesteps_since_restore: 2304000
  timesteps_this_iter: 96000
  timesteps_total: 4224000
  training_iteration: 44
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     44 |          7071.71 | 4224000 |   250.16 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 58
    apples_agent-0_mean: 5.07
    apples_agent-0_min: 0
    apples_agent-1_max: 58
    apples_agent-1_mean: 12.36
    apples_agent-1_min: 0
    apples_agent-2_max: 58
    apples_agent-2_mean: 10.49
    apples_agent-2_min: 0
    apples_agent-3_max: 111
    apples_agent-3_mean: 42.85
    apples_agent-3_min: 1
    apples_agent-4_max: 78
    apples_agent-4_mean: 9.64
    apples_agent-4_min: 0
    apples_agent-5_max: 95
    apples_agent-5_mean: 43.6
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 431
    cleaning_beam_agent-0_mean: 325.17
    cleaning_beam_agent-0_min: 184
    cleaning_beam_agent-1_max: 355
    cleaning_beam_agent-1_mean: 163.38
    cleaning_beam_agent-1_min: 53
    cleaning_beam_agent-2_max: 689
    cleaning_beam_agent-2_mean: 406.16
    cleaning_beam_agent-2_min: 188
    cleaning_beam_agent-3_max: 151
    cleaning_beam_agent-3_mean: 91.14
    cleaning_beam_agent-3_min: 58
    cleaning_beam_agent-4_max: 311
    cleaning_beam_agent-4_mean: 123.9
    cleaning_beam_agent-4_min: 49
    cleaning_beam_agent-5_max: 133
    cleaning_beam_agent-5_mean: 82.36
    cleaning_beam_agent-5_min: 47
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.3
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.07
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_19-11-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 416.00000000000443
  episode_reward_mean: 243.76999999999873
  episode_reward_min: 80.0000000000001
  episodes_this_iter: 96
  episodes_total: 4320
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20223.212
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000996422371827066
        entropy: 1.4916160106658936
        entropy_coeff: 0.0017600000137463212
        kl: 0.010306628420948982
        model: {}
        policy_loss: -0.014552334323525429
        total_loss: -0.01570911519229412
        vf_explained_var: 0.03864896297454834
        vf_loss: 4.377988815307617
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000996422371827066
        entropy: 1.5532346963882446
        entropy_coeff: 0.0017600000137463212
        kl: 0.013672497123479843
        model: {}
        policy_loss: -0.023389924317598343
        total_loss: -0.024312077090144157
        vf_explained_var: 0.02374100685119629
        vf_loss: 4.442913055419922
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000996422371827066
        entropy: 1.3413124084472656
        entropy_coeff: 0.0017600000137463212
        kl: 0.011930756270885468
        model: {}
        policy_loss: -0.018473340198397636
        total_loss: -0.019228529185056686
        vf_explained_var: 0.09390994906425476
        vf_loss: 4.124457359313965
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000996422371827066
        entropy: 1.494133710861206
        entropy_coeff: 0.0017600000137463212
        kl: 0.011662794277071953
        model: {}
        policy_loss: -0.01629463955760002
        total_loss: -0.017337678000330925
        vf_explained_var: 0.07810446619987488
        vf_loss: 4.203592300415039
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000996422371827066
        entropy: 1.4879255294799805
        entropy_coeff: 0.0017600000137463212
        kl: 0.015059014782309532
        model: {}
        policy_loss: -0.020621061325073242
        total_loss: -0.02129782922565937
        vf_explained_var: 0.04240494966506958
        vf_loss: 4.360795021057129
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000996422371827066
        entropy: 1.6439969539642334
        entropy_coeff: 0.0017600000137463212
        kl: 0.014351080171763897
        model: {}
        policy_loss: -0.01906057819724083
        total_loss: -0.020119648426771164
        vf_explained_var: 0.12400433421134949
        vf_loss: 3.992544174194336
    load_time_ms: 25766.076
    num_steps_sampled: 4320000
    num_steps_trained: 4320000
    sample_time_ms: 97717.258
    update_time_ms: 43.278
  iterations_since_restore: 25
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.474999999999998
    ram_util_percent: 14.029591836734696
  pid: 4061
  policy_reward_max:
    agent-0: 69.33333333333327
    agent-1: 69.33333333333327
    agent-2: 69.33333333333327
    agent-3: 69.33333333333327
    agent-4: 69.33333333333327
    agent-5: 69.33333333333327
  policy_reward_mean:
    agent-0: 40.62833333333329
    agent-1: 40.62833333333329
    agent-2: 40.62833333333329
    agent-3: 40.62833333333329
    agent-4: 40.62833333333329
    agent-5: 40.62833333333329
  policy_reward_min:
    agent-0: 13.333333333333323
    agent-1: 13.333333333333323
    agent-2: 13.333333333333323
    agent-3: 13.333333333333323
    agent-4: 13.333333333333323
    agent-5: 13.333333333333323
  sampler_perf:
    mean_env_wait_ms: 22.739911702696325
    mean_inference_ms: 12.456139945698016
    mean_processing_ms: 51.62015594994091
  time_since_restore: 3997.9011175632477
  time_this_iter_s: 137.25230884552002
  time_total_s: 7208.964803695679
  timestamp: 1637021507
  timesteps_since_restore: 2400000
  timesteps_this_iter: 96000
  timesteps_total: 4320000
  training_iteration: 45
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     45 |          7208.96 | 4320000 |   243.77 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 49
    apples_agent-0_mean: 5.63
    apples_agent-0_min: 0
    apples_agent-1_max: 74
    apples_agent-1_mean: 14.15
    apples_agent-1_min: 0
    apples_agent-2_max: 81
    apples_agent-2_mean: 9.05
    apples_agent-2_min: 0
    apples_agent-3_max: 132
    apples_agent-3_mean: 50.41
    apples_agent-3_min: 0
    apples_agent-4_max: 102
    apples_agent-4_mean: 8.25
    apples_agent-4_min: 0
    apples_agent-5_max: 95
    apples_agent-5_mean: 44.46
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 424
    cleaning_beam_agent-0_mean: 329.11
    cleaning_beam_agent-0_min: 210
    cleaning_beam_agent-1_max: 327
    cleaning_beam_agent-1_mean: 148.3
    cleaning_beam_agent-1_min: 52
    cleaning_beam_agent-2_max: 734
    cleaning_beam_agent-2_mean: 428.18
    cleaning_beam_agent-2_min: 124
    cleaning_beam_agent-3_max: 120
    cleaning_beam_agent-3_mean: 77.71
    cleaning_beam_agent-3_min: 46
    cleaning_beam_agent-4_max: 316
    cleaning_beam_agent-4_mean: 151.57
    cleaning_beam_agent-4_min: 49
    cleaning_beam_agent-5_max: 109
    cleaning_beam_agent-5_mean: 67.53
    cleaning_beam_agent-5_min: 32
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.05
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.13
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.16
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_19-13-52
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 416.00000000000875
  episode_reward_mean: 273.3500000000006
  episode_reward_min: 18.00000000000072
  episodes_this_iter: 96
  episodes_total: 4416
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20238.535
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009904319886118174
        entropy: 1.4912803173065186
        entropy_coeff: 0.0017600000137463212
        kl: 0.01125776395201683
        model: {}
        policy_loss: -0.014097906649112701
        total_loss: -0.01507861353456974
        vf_explained_var: 0.034558385610580444
        vf_loss: 5.1817169189453125
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009904319886118174
        entropy: 1.5663994550704956
        entropy_coeff: 0.0017600000137463212
        kl: 0.014209211803972721
        model: {}
        policy_loss: -0.025562651455402374
        total_loss: -0.026391245424747467
        vf_explained_var: 0.055084988474845886
        vf_loss: 5.073464870452881
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009904319886118174
        entropy: 1.3180875778198242
        entropy_coeff: 0.0017600000137463212
        kl: 0.012690015137195587
        model: {}
        policy_loss: -0.020878396928310394
        total_loss: -0.021412964910268784
        vf_explained_var: 0.038524478673934937
        vf_loss: 5.162625789642334
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009904319886118174
        entropy: 1.4392361640930176
        entropy_coeff: 0.0017600000137463212
        kl: 0.017809346318244934
        model: {}
        policy_loss: -0.01368577592074871
        total_loss: -0.013952268287539482
        vf_explained_var: 0.09577421844005585
        vf_loss: 4.8562774658203125
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009904319886118174
        entropy: 1.4747474193572998
        entropy_coeff: 0.0017600000137463212
        kl: 0.01293893251568079
        model: {}
        policy_loss: -0.021149365231394768
        total_loss: -0.0219649076461792
        vf_explained_var: 0.09409137070178986
        vf_loss: 4.861203193664551
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009904319886118174
        entropy: 1.5900473594665527
        entropy_coeff: 0.0017600000137463212
        kl: 0.01299977395683527
        model: {}
        policy_loss: -0.023168809711933136
        total_loss: -0.024181419983506203
        vf_explained_var: 0.094898521900177
        vf_loss: 4.858979225158691
    load_time_ms: 23520.396
    num_steps_sampled: 4416000
    num_steps_trained: 4416000
    sample_time_ms: 96089.288
    update_time_ms: 40.757
  iterations_since_restore: 26
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.57094972067039
    ram_util_percent: 13.859776536312852
  pid: 4061
  policy_reward_max:
    agent-0: 69.33333333333327
    agent-1: 69.33333333333327
    agent-2: 69.33333333333327
    agent-3: 69.33333333333327
    agent-4: 69.33333333333327
    agent-5: 69.33333333333327
  policy_reward_mean:
    agent-0: 45.55833333333324
    agent-1: 45.55833333333324
    agent-2: 45.55833333333324
    agent-3: 45.55833333333324
    agent-4: 45.55833333333324
    agent-5: 45.55833333333324
  policy_reward_min:
    agent-0: 2.999999999999987
    agent-1: 2.999999999999987
    agent-2: 2.999999999999987
    agent-3: 2.999999999999987
    agent-4: 2.999999999999987
    agent-5: 2.999999999999987
  sampler_perf:
    mean_env_wait_ms: 22.76428273531469
    mean_inference_ms: 12.451398144666323
    mean_processing_ms: 51.599979394134614
  time_since_restore: 4123.436188459396
  time_this_iter_s: 125.53507089614868
  time_total_s: 7334.499874591827
  timestamp: 1637021632
  timesteps_since_restore: 2496000
  timesteps_this_iter: 96000
  timesteps_total: 4416000
  training_iteration: 46
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 25.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     46 |           7334.5 | 4416000 |   273.35 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 35
    apples_agent-0_mean: 4.27
    apples_agent-0_min: 0
    apples_agent-1_max: 110
    apples_agent-1_mean: 15.41
    apples_agent-1_min: 0
    apples_agent-2_max: 56
    apples_agent-2_mean: 10.51
    apples_agent-2_min: 0
    apples_agent-3_max: 90
    apples_agent-3_mean: 48.44
    apples_agent-3_min: 3
    apples_agent-4_max: 92
    apples_agent-4_mean: 8.65
    apples_agent-4_min: 0
    apples_agent-5_max: 99
    apples_agent-5_mean: 43.85
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 415
    cleaning_beam_agent-0_mean: 329.03
    cleaning_beam_agent-0_min: 223
    cleaning_beam_agent-1_max: 426
    cleaning_beam_agent-1_mean: 187.75
    cleaning_beam_agent-1_min: 48
    cleaning_beam_agent-2_max: 697
    cleaning_beam_agent-2_mean: 353.59
    cleaning_beam_agent-2_min: 163
    cleaning_beam_agent-3_max: 136
    cleaning_beam_agent-3_mean: 79.9
    cleaning_beam_agent-3_min: 36
    cleaning_beam_agent-4_max: 428
    cleaning_beam_agent-4_mean: 162.91
    cleaning_beam_agent-4_min: 47
    cleaning_beam_agent-5_max: 149
    cleaning_beam_agent-5_mean: 61.27
    cleaning_beam_agent-5_min: 21
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.06
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.11
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 8
    fire_beam_agent-5_mean: 0.19
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_19-15-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 470.00000000001205
  episode_reward_mean: 285.71000000000095
  episode_reward_min: 89.00000000000044
  episodes_this_iter: 96
  episodes_total: 4512
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20212.695
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009844416053965688
        entropy: 1.5011212825775146
        entropy_coeff: 0.0017600000137463212
        kl: 0.010228305123746395
        model: {}
        policy_loss: -0.015290413051843643
        total_loss: -0.016305197030305862
        vf_explained_var: 0.026280760765075684
        vf_loss: 6.043542861938477
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009844416053965688
        entropy: 1.5568256378173828
        entropy_coeff: 0.0017600000137463212
        kl: 0.013021089136600494
        model: {}
        policy_loss: -0.024442477151751518
        total_loss: -0.025300299748778343
        vf_explained_var: 0.06536020338535309
        vf_loss: 5.800826549530029
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009844416053965688
        entropy: 1.4171802997589111
        entropy_coeff: 0.0017600000137463212
        kl: 0.012591032311320305
        model: {}
        policy_loss: -0.018283942714333534
        total_loss: -0.01894756406545639
        vf_explained_var: 0.07990117371082306
        vf_loss: 5.715137481689453
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009844416053965688
        entropy: 1.4401837587356567
        entropy_coeff: 0.0017600000137463212
        kl: 0.012894954532384872
        model: {}
        policy_loss: -0.017117539420723915
        total_loss: -0.017800338566303253
        vf_explained_var: 0.0938626229763031
        vf_loss: 5.624299049377441
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009844416053965688
        entropy: 1.4417171478271484
        entropy_coeff: 0.0017600000137463212
        kl: 0.013162593357264996
        model: {}
        policy_loss: -0.0214381143450737
        total_loss: -0.022121421992778778
        vf_explained_var: 0.13278347253799438
        vf_loss: 5.378549098968506
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009844416053965688
        entropy: 1.5001184940338135
        entropy_coeff: 0.0017600000137463212
        kl: 0.013368272222578526
        model: {}
        policy_loss: -0.023074600845575333
        total_loss: -0.023823197931051254
        vf_explained_var: 0.10618937015533447
        vf_loss: 5.547891139984131
    load_time_ms: 20650.456
    num_steps_sampled: 4512000
    num_steps_trained: 4512000
    sample_time_ms: 94248.98
    update_time_ms: 40.577
  iterations_since_restore: 27
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.625280898876404
    ram_util_percent: 13.873033707865172
  pid: 4061
  policy_reward_max:
    agent-0: 78.33333333333346
    agent-1: 78.33333333333346
    agent-2: 78.33333333333346
    agent-3: 78.33333333333346
    agent-4: 78.33333333333346
    agent-5: 78.33333333333346
  policy_reward_mean:
    agent-0: 47.618333333333254
    agent-1: 47.618333333333254
    agent-2: 47.618333333333254
    agent-3: 47.618333333333254
    agent-4: 47.618333333333254
    agent-5: 47.618333333333254
  policy_reward_min:
    agent-0: 14.833333333333314
    agent-1: 14.833333333333314
    agent-2: 14.833333333333314
    agent-3: 14.833333333333314
    agent-4: 14.833333333333314
    agent-5: 14.833333333333314
  sampler_perf:
    mean_env_wait_ms: 22.786075592521584
    mean_inference_ms: 12.447698174043875
    mean_processing_ms: 51.57850570258065
  time_since_restore: 4248.571710586548
  time_this_iter_s: 125.13552212715149
  time_total_s: 7459.635396718979
  timestamp: 1637021758
  timesteps_since_restore: 2592000
  timesteps_this_iter: 96000
  timesteps_total: 4512000
  training_iteration: 47
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     47 |          7459.64 | 4512000 |   285.71 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 34
    apples_agent-0_mean: 4.5
    apples_agent-0_min: 0
    apples_agent-1_max: 107
    apples_agent-1_mean: 14.81
    apples_agent-1_min: 0
    apples_agent-2_max: 66
    apples_agent-2_mean: 10.79
    apples_agent-2_min: 0
    apples_agent-3_max: 125
    apples_agent-3_mean: 55.59
    apples_agent-3_min: 11
    apples_agent-4_max: 107
    apples_agent-4_mean: 10.81
    apples_agent-4_min: 0
    apples_agent-5_max: 84
    apples_agent-5_mean: 44.06
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 389
    cleaning_beam_agent-0_mean: 308.5
    cleaning_beam_agent-0_min: 210
    cleaning_beam_agent-1_max: 416
    cleaning_beam_agent-1_mean: 166.39
    cleaning_beam_agent-1_min: 54
    cleaning_beam_agent-2_max: 678
    cleaning_beam_agent-2_mean: 348.0
    cleaning_beam_agent-2_min: 116
    cleaning_beam_agent-3_max: 100
    cleaning_beam_agent-3_mean: 62.46
    cleaning_beam_agent-3_min: 32
    cleaning_beam_agent-4_max: 386
    cleaning_beam_agent-4_mean: 198.82
    cleaning_beam_agent-4_min: 64
    cleaning_beam_agent-5_max: 114
    cleaning_beam_agent-5_mean: 66.11
    cleaning_beam_agent-5_min: 22
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.04
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.14
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 13
    fire_beam_agent-5_mean: 0.18
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_19-18-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 509.0000000000178
  episode_reward_mean: 291.34000000000106
  episode_reward_min: 72.99999999999994
  episodes_this_iter: 96
  episodes_total: 4608
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20233.572
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009784512221813202
        entropy: 1.471497893333435
        entropy_coeff: 0.0017600000137463212
        kl: 0.011312060058116913
        model: {}
        policy_loss: -0.016358613967895508
        total_loss: -0.017233124002814293
        vf_explained_var: 0.04169021546840668
        vf_loss: 5.841217994689941
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009784512221813202
        entropy: 1.497040867805481
        entropy_coeff: 0.0017600000137463212
        kl: 0.015791887417435646
        model: {}
        policy_loss: -0.024900689721107483
        total_loss: -0.025370394811034203
        vf_explained_var: 0.0392298549413681
        vf_loss: 5.859017848968506
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009784512221813202
        entropy: 1.3977024555206299
        entropy_coeff: 0.0017600000137463212
        kl: 0.012798359617590904
        model: {}
        policy_loss: -0.020701590925455093
        total_loss: -0.021327059715986252
        vf_explained_var: 0.09017437696456909
        vf_loss: 5.546538352966309
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009784512221813202
        entropy: 1.3569002151489258
        entropy_coeff: 0.0017600000137463212
        kl: 0.0114962849766016
        model: {}
        policy_loss: -0.018119076266884804
        total_loss: -0.018832826986908913
        vf_explained_var: 0.1392790526151657
        vf_loss: 5.24763298034668
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009784512221813202
        entropy: 1.4312090873718262
        entropy_coeff: 0.0017600000137463212
        kl: 0.012895793654024601
        model: {}
        policy_loss: -0.02400856278836727
        total_loss: -0.024700086563825607
        vf_explained_var: 0.11713621020317078
        vf_loss: 5.378273010253906
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009784512221813202
        entropy: 1.4874563217163086
        entropy_coeff: 0.0017600000137463212
        kl: 0.014485813677310944
        model: {}
        policy_loss: -0.024520499631762505
        total_loss: -0.025138206779956818
        vf_explained_var: 0.09644067287445068
        vf_loss: 5.516324996948242
    load_time_ms: 20225.399
    num_steps_sampled: 4608000
    num_steps_trained: 4608000
    sample_time_ms: 93133.119
    update_time_ms: 35.2
  iterations_since_restore: 28
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.576243093922653
    ram_util_percent: 13.852486187845304
  pid: 4061
  policy_reward_max:
    agent-0: 84.8333333333335
    agent-1: 84.8333333333335
    agent-2: 84.8333333333335
    agent-3: 84.8333333333335
    agent-4: 84.8333333333335
    agent-5: 84.8333333333335
  policy_reward_mean:
    agent-0: 48.55666666666659
    agent-1: 48.55666666666659
    agent-2: 48.55666666666659
    agent-3: 48.55666666666659
    agent-4: 48.55666666666659
    agent-5: 48.55666666666659
  policy_reward_min:
    agent-0: 12.166666666666659
    agent-1: 12.166666666666659
    agent-2: 12.166666666666659
    agent-3: 12.166666666666659
    agent-4: 12.166666666666659
    agent-5: 12.166666666666659
  sampler_perf:
    mean_env_wait_ms: 22.808713919961363
    mean_inference_ms: 12.447356285225325
    mean_processing_ms: 51.57396763564799
  time_since_restore: 4374.309707641602
  time_this_iter_s: 125.73799705505371
  time_total_s: 7585.373393774033
  timestamp: 1637021884
  timesteps_since_restore: 2688000
  timesteps_this_iter: 96000
  timesteps_total: 4608000
  training_iteration: 48
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     48 |          7585.37 | 4608000 |   291.34 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 79
    apples_agent-0_mean: 8.06
    apples_agent-0_min: 0
    apples_agent-1_max: 95
    apples_agent-1_mean: 17.98
    apples_agent-1_min: 0
    apples_agent-2_max: 89
    apples_agent-2_mean: 12.36
    apples_agent-2_min: 0
    apples_agent-3_max: 119
    apples_agent-3_mean: 53.6
    apples_agent-3_min: 6
    apples_agent-4_max: 96
    apples_agent-4_mean: 7.26
    apples_agent-4_min: 0
    apples_agent-5_max: 118
    apples_agent-5_mean: 44.59
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 368
    cleaning_beam_agent-0_mean: 262.62
    cleaning_beam_agent-0_min: 152
    cleaning_beam_agent-1_max: 468
    cleaning_beam_agent-1_mean: 188.72
    cleaning_beam_agent-1_min: 49
    cleaning_beam_agent-2_max: 600
    cleaning_beam_agent-2_mean: 356.51
    cleaning_beam_agent-2_min: 87
    cleaning_beam_agent-3_max: 118
    cleaning_beam_agent-3_mean: 61.41
    cleaning_beam_agent-3_min: 31
    cleaning_beam_agent-4_max: 426
    cleaning_beam_agent-4_mean: 239.32
    cleaning_beam_agent-4_min: 55
    cleaning_beam_agent-5_max: 136
    cleaning_beam_agent-5_mean: 68.03
    cleaning_beam_agent-5_min: 19
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.05
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.12
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.09
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_19-20-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 505.00000000001705
  episode_reward_mean: 287.560000000001
  episode_reward_min: 93.00000000000024
  episodes_this_iter: 96
  episodes_total: 4704
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20199.935
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009724607807584107
        entropy: 1.5353809595108032
        entropy_coeff: 0.0017600000137463212
        kl: 0.011056618764996529
        model: {}
        policy_loss: -0.016775526106357574
        total_loss: -0.017701534554362297
        vf_explained_var: 0.050139591097831726
        vf_loss: 6.70596170425415
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009724607807584107
        entropy: 1.4986552000045776
        entropy_coeff: 0.0017600000137463212
        kl: 0.014570111408829689
        model: {}
        policy_loss: -0.025785045698285103
        total_loss: -0.026287555694580078
        vf_explained_var: 0.03937813639640808
        vf_loss: 6.781135082244873
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009724607807584107
        entropy: 1.349552035331726
        entropy_coeff: 0.0017600000137463212
        kl: 0.013514772057533264
        model: {}
        policy_loss: -0.019971149042248726
        total_loss: -0.02032294124364853
        vf_explained_var: 0.04830484092235565
        vf_loss: 6.7194600105285645
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009724607807584107
        entropy: 1.371889352798462
        entropy_coeff: 0.0017600000137463212
        kl: 0.019576847553253174
        model: {}
        policy_loss: -0.01149323582649231
        total_loss: -0.01135230716317892
        vf_explained_var: 0.15452229976654053
        vf_loss: 5.9777116775512695
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009724607807584107
        entropy: 1.396493911743164
        entropy_coeff: 0.0017600000137463212
        kl: 0.012668970972299576
        model: {}
        policy_loss: -0.023533955216407776
        total_loss: -0.02410111017525196
        vf_explained_var: 0.11635801196098328
        vf_loss: 6.237766742706299
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009724607807584107
        entropy: 1.4879952669143677
        entropy_coeff: 0.0017600000137463212
        kl: 0.014313465915620327
        model: {}
        policy_loss: -0.02544480562210083
        total_loss: -0.025999829173088074
        vf_explained_var: 0.1046781986951828
        vf_loss: 6.325047492980957
    load_time_ms: 19683.33
    num_steps_sampled: 4704000
    num_steps_trained: 4704000
    sample_time_ms: 92064.688
    update_time_ms: 35.392
  iterations_since_restore: 29
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 19.1125
    ram_util_percent: 11.666477272727274
  pid: 4061
  policy_reward_max:
    agent-0: 84.16666666666691
    agent-1: 84.16666666666691
    agent-2: 84.16666666666691
    agent-3: 84.16666666666691
    agent-4: 84.16666666666691
    agent-5: 84.16666666666691
  policy_reward_mean:
    agent-0: 47.9266666666666
    agent-1: 47.9266666666666
    agent-2: 47.9266666666666
    agent-3: 47.9266666666666
    agent-4: 47.9266666666666
    agent-5: 47.9266666666666
  policy_reward_min:
    agent-0: 15.499999999999986
    agent-1: 15.499999999999986
    agent-2: 15.499999999999986
    agent-3: 15.499999999999986
    agent-4: 15.499999999999986
    agent-5: 15.499999999999986
  sampler_perf:
    mean_env_wait_ms: 22.826864837844763
    mean_inference_ms: 12.442497458658233
    mean_processing_ms: 51.5473839338061
  time_since_restore: 4498.337728977203
  time_this_iter_s: 124.0280213356018
  time_total_s: 7709.401415109634
  timestamp: 1637022009
  timesteps_since_restore: 2784000
  timesteps_this_iter: 96000
  timesteps_total: 4704000
  training_iteration: 49
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     49 |           7709.4 | 4704000 |   287.56 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 43
    apples_agent-0_mean: 3.06
    apples_agent-0_min: 0
    apples_agent-1_max: 99
    apples_agent-1_mean: 19.8
    apples_agent-1_min: 0
    apples_agent-2_max: 135
    apples_agent-2_mean: 11.56
    apples_agent-2_min: 0
    apples_agent-3_max: 107
    apples_agent-3_mean: 59.73
    apples_agent-3_min: 11
    apples_agent-4_max: 118
    apples_agent-4_mean: 9.5
    apples_agent-4_min: 0
    apples_agent-5_max: 89
    apples_agent-5_mean: 48.45
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 381
    cleaning_beam_agent-0_mean: 287.17
    cleaning_beam_agent-0_min: 170
    cleaning_beam_agent-1_max: 498
    cleaning_beam_agent-1_mean: 222.56
    cleaning_beam_agent-1_min: 59
    cleaning_beam_agent-2_max: 806
    cleaning_beam_agent-2_mean: 404.41
    cleaning_beam_agent-2_min: 143
    cleaning_beam_agent-3_max: 121
    cleaning_beam_agent-3_mean: 65.38
    cleaning_beam_agent-3_min: 35
    cleaning_beam_agent-4_max: 432
    cleaning_beam_agent-4_mean: 204.9
    cleaning_beam_agent-4_min: 34
    cleaning_beam_agent-5_max: 119
    cleaning_beam_agent-5_mean: 56.28
    cleaning_beam_agent-5_min: 18
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.05
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 3
    fire_beam_agent-4_mean: 0.17
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.16
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_19-22-11
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 465.0000000000116
  episode_reward_mean: 299.7500000000006
  episode_reward_min: 144.00000000000063
  episodes_this_iter: 96
  episodes_total: 4800
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20259.813
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009664703975431621
        entropy: 1.5380394458770752
        entropy_coeff: 0.0017600000137463212
        kl: 0.010937212966382504
        model: {}
        policy_loss: -0.015815313905477524
        total_loss: -0.01692572422325611
        vf_explained_var: 0.036975085735321045
        vf_loss: 5.02819299697876
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009664703975431621
        entropy: 1.486586093902588
        entropy_coeff: 0.0017600000137463212
        kl: 0.014866936951875687
        model: {}
        policy_loss: -0.026481667533516884
        total_loss: -0.027097705751657486
        vf_explained_var: 0.016881927847862244
        vf_loss: 5.136611461639404
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009664703975431621
        entropy: 1.2794052362442017
        entropy_coeff: 0.0017600000137463212
        kl: 0.013297698460519314
        model: {}
        policy_loss: -0.020359743386507034
        total_loss: -0.02079147659242153
        vf_explained_var: 0.0612713098526001
        vf_loss: 4.902491569519043
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009664703975431621
        entropy: 1.3771214485168457
        entropy_coeff: 0.0017600000137463212
        kl: 0.011127139441668987
        model: {}
        policy_loss: -0.019225822761654854
        total_loss: -0.020062660798430443
        vf_explained_var: 0.09208303689956665
        vf_loss: 4.741830825805664
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009664703975431621
        entropy: 1.399820327758789
        entropy_coeff: 0.0017600000137463212
        kl: 0.013868013396859169
        model: {}
        policy_loss: -0.02140767313539982
        total_loss: -0.022014614194631577
        vf_explained_var: 0.10062231123447418
        vf_loss: 4.6994194984436035
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009664703975431621
        entropy: 1.3858647346496582
        entropy_coeff: 0.0017600000137463212
        kl: 0.01283290982246399
        model: {}
        policy_loss: -0.02419319748878479
        total_loss: -0.02487529255449772
        vf_explained_var: 0.09359502792358398
        vf_loss: 4.737332820892334
    load_time_ms: 19135.755
    num_steps_sampled: 4800000
    num_steps_trained: 4800000
    sample_time_ms: 91521.295
    update_time_ms: 32.559
  iterations_since_restore: 30
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.263428571428568
    ram_util_percent: 9.633142857142857
  pid: 4061
  policy_reward_max:
    agent-0: 77.50000000000004
    agent-1: 77.50000000000004
    agent-2: 77.50000000000004
    agent-3: 77.50000000000004
    agent-4: 77.50000000000004
    agent-5: 77.50000000000004
  policy_reward_mean:
    agent-0: 49.95833333333323
    agent-1: 49.95833333333323
    agent-2: 49.95833333333323
    agent-3: 49.95833333333323
    agent-4: 49.95833333333323
    agent-5: 49.95833333333323
  policy_reward_min:
    agent-0: 24.00000000000003
    agent-1: 24.00000000000003
    agent-2: 24.00000000000003
    agent-3: 24.00000000000003
    agent-4: 24.00000000000003
    agent-5: 24.00000000000003
  sampler_perf:
    mean_env_wait_ms: 22.836258309437017
    mean_inference_ms: 12.43165060199723
    mean_processing_ms: 51.5002018981607
  time_since_restore: 4620.688128709793
  time_this_iter_s: 122.35039973258972
  time_total_s: 7831.751814842224
  timestamp: 1637022131
  timesteps_since_restore: 2880000
  timesteps_this_iter: 96000
  timesteps_total: 4800000
  training_iteration: 50
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     50 |          7831.75 | 4800000 |   299.75 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 62
    apples_agent-0_mean: 5.54
    apples_agent-0_min: 0
    apples_agent-1_max: 118
    apples_agent-1_mean: 19.17
    apples_agent-1_min: 0
    apples_agent-2_max: 161
    apples_agent-2_mean: 17.68
    apples_agent-2_min: 0
    apples_agent-3_max: 111
    apples_agent-3_mean: 56.94
    apples_agent-3_min: 8
    apples_agent-4_max: 84
    apples_agent-4_mean: 6.28
    apples_agent-4_min: 0
    apples_agent-5_max: 105
    apples_agent-5_mean: 49.14
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 379
    cleaning_beam_agent-0_mean: 276.66
    cleaning_beam_agent-0_min: 139
    cleaning_beam_agent-1_max: 561
    cleaning_beam_agent-1_mean: 245.66
    cleaning_beam_agent-1_min: 73
    cleaning_beam_agent-2_max: 726
    cleaning_beam_agent-2_mean: 363.39
    cleaning_beam_agent-2_min: 130
    cleaning_beam_agent-3_max: 105
    cleaning_beam_agent-3_mean: 60.46
    cleaning_beam_agent-3_min: 24
    cleaning_beam_agent-4_max: 467
    cleaning_beam_agent-4_mean: 230.93
    cleaning_beam_agent-4_min: 55
    cleaning_beam_agent-5_max: 166
    cleaning_beam_agent-5_mean: 60.41
    cleaning_beam_agent-5_min: 16
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.1
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 86
    fire_beam_agent-5_mean: 1.29
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_19-24-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 484.00000000000813
  episode_reward_mean: 296.48000000000104
  episode_reward_min: -341.0000000000007
  episodes_this_iter: 96
  episodes_total: 4896
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20261.13
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009604800143279135
        entropy: 1.5436127185821533
        entropy_coeff: 0.0017600000137463212
        kl: 0.010248759761452675
        model: {}
        policy_loss: -0.012896869331598282
        total_loss: -0.013543955981731415
        vf_explained_var: 0.01754772663116455
        vf_loss: 10.44796085357666
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009604800143279135
        entropy: 1.4277639389038086
        entropy_coeff: 0.0017600000137463212
        kl: 0.014389663934707642
        model: {}
        policy_loss: -0.021082494407892227
        total_loss: -0.021124504506587982
        vf_explained_var: 0.030436888337135315
        vf_loss: 10.31887435913086
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009604800143279135
        entropy: 1.308513879776001
        entropy_coeff: 0.0017600000137463212
        kl: 0.012870993465185165
        model: {}
        policy_loss: -0.016097771003842354
        total_loss: -0.01607174053788185
        vf_explained_var: 0.019959881901741028
        vf_loss: 10.419137001037598
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009604800143279135
        entropy: 1.351303219795227
        entropy_coeff: 0.0017600000137463212
        kl: 0.010392623022198677
        model: {}
        policy_loss: -0.01587572693824768
        total_loss: -0.016205331310629845
        vf_explained_var: 0.05308161675930023
        vf_loss: 10.094255447387695
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009604800143279135
        entropy: 1.3917734622955322
        entropy_coeff: 0.0017600000137463212
        kl: 0.012145631946623325
        model: {}
        policy_loss: -0.01922369748353958
        total_loss: -0.01941857673227787
        vf_explained_var: 0.03405711054801941
        vf_loss: 10.400768280029297
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009604800143279135
        entropy: 1.3848834037780762
        entropy_coeff: 0.0017600000137463212
        kl: 0.012828964740037918
        model: {}
        policy_loss: -0.023296568542718887
        total_loss: -0.02349426969885826
        vf_explained_var: 0.09466935694217682
        vf_loss: 9.567975997924805
    load_time_ms: 18606.103
    num_steps_sampled: 4896000
    num_steps_trained: 4896000
    sample_time_ms: 90614.144
    update_time_ms: 29.288
  iterations_since_restore: 31
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.406321839080462
    ram_util_percent: 9.598275862068968
  pid: 4061
  policy_reward_max:
    agent-0: 80.66666666666666
    agent-1: 80.66666666666666
    agent-2: 80.66666666666666
    agent-3: 80.66666666666666
    agent-4: 80.66666666666666
    agent-5: 80.66666666666666
  policy_reward_mean:
    agent-0: 49.41333333333324
    agent-1: 49.41333333333324
    agent-2: 49.41333333333324
    agent-3: 49.41333333333324
    agent-4: 49.41333333333324
    agent-5: 49.41333333333324
  policy_reward_min:
    agent-0: -56.833333333332774
    agent-1: -56.833333333332774
    agent-2: -56.833333333332774
    agent-3: -56.833333333332774
    agent-4: -56.833333333332774
    agent-5: -56.833333333332774
  sampler_perf:
    mean_env_wait_ms: 22.85051192932207
    mean_inference_ms: 12.42431728767924
    mean_processing_ms: 51.46319342177312
  time_since_restore: 4743.074294805527
  time_this_iter_s: 122.38616609573364
  time_total_s: 7954.137980937958
  timestamp: 1637022254
  timesteps_since_restore: 2976000
  timesteps_this_iter: 96000
  timesteps_total: 4896000
  training_iteration: 51
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     51 |          7954.14 | 4896000 |   296.48 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 59
    apples_agent-0_mean: 3.09
    apples_agent-0_min: 0
    apples_agent-1_max: 94
    apples_agent-1_mean: 20.2
    apples_agent-1_min: 0
    apples_agent-2_max: 100
    apples_agent-2_mean: 15.1
    apples_agent-2_min: 0
    apples_agent-3_max: 118
    apples_agent-3_mean: 61.45
    apples_agent-3_min: 0
    apples_agent-4_max: 70
    apples_agent-4_mean: 6.43
    apples_agent-4_min: 0
    apples_agent-5_max: 115
    apples_agent-5_mean: 57.53
    apples_agent-5_min: 7
    cleaning_beam_agent-0_max: 371
    cleaning_beam_agent-0_mean: 275.51
    cleaning_beam_agent-0_min: 139
    cleaning_beam_agent-1_max: 462
    cleaning_beam_agent-1_mean: 209.83
    cleaning_beam_agent-1_min: 70
    cleaning_beam_agent-2_max: 703
    cleaning_beam_agent-2_mean: 378.35
    cleaning_beam_agent-2_min: 161
    cleaning_beam_agent-3_max: 104
    cleaning_beam_agent-3_mean: 64.46
    cleaning_beam_agent-3_min: 31
    cleaning_beam_agent-4_max: 414
    cleaning_beam_agent-4_mean: 234.12
    cleaning_beam_agent-4_min: 48
    cleaning_beam_agent-5_max: 154
    cleaning_beam_agent-5_mean: 64.81
    cleaning_beam_agent-5_min: 18
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.04
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.09
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.12
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_19-26-17
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 507.0000000000123
  episode_reward_mean: 329.82000000000255
  episode_reward_min: 58.99999999999974
  episodes_this_iter: 96
  episodes_total: 4992
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20247.77
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000954489572905004
        entropy: 1.5329830646514893
        entropy_coeff: 0.0017600000137463212
        kl: 0.01148645207285881
        model: {}
        policy_loss: -0.01762409880757332
        total_loss: -0.018609002232551575
        vf_explained_var: 0.037681132555007935
        vf_loss: 5.645047664642334
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000954489572905004
        entropy: 1.3925013542175293
        entropy_coeff: 0.0017600000137463212
        kl: 0.017792174592614174
        model: {}
        policy_loss: -0.02543460763990879
        total_loss: -0.02552245743572712
        vf_explained_var: 0.007378935813903809
        vf_loss: 5.837334632873535
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000954489572905004
        entropy: 1.3231446743011475
        entropy_coeff: 0.0017600000137463212
        kl: 0.013492775149643421
        model: {}
        policy_loss: -0.021619806066155434
        total_loss: -0.022066237404942513
        vf_explained_var: 0.091814324259758
        vf_loss: 5.33024787902832
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000954489572905004
        entropy: 1.322619080543518
        entropy_coeff: 0.0017600000137463212
        kl: 0.012330441735684872
        model: {}
        policy_loss: -0.020602108910679817
        total_loss: -0.021193545311689377
        vf_explained_var: 0.14265194535255432
        vf_loss: 5.033273220062256
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000954489572905004
        entropy: 1.3599406480789185
        entropy_coeff: 0.0017600000137463212
        kl: 0.013397911563515663
        model: {}
        policy_loss: -0.023736482486128807
        total_loss: -0.02426120452582836
        vf_explained_var: 0.09807965159416199
        vf_loss: 5.289829254150391
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000954489572905004
        entropy: 1.415507197380066
        entropy_coeff: 0.0017600000137463212
        kl: 0.01571611501276493
        model: {}
        policy_loss: -0.02592669241130352
        total_loss: -0.02631276287138462
        vf_explained_var: 0.09315480291843414
        vf_loss: 5.336141109466553
    load_time_ms: 18345.151
    num_steps_sampled: 4992000
    num_steps_trained: 4992000
    sample_time_ms: 90084.827
    update_time_ms: 29.23
  iterations_since_restore: 32
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.26875
    ram_util_percent: 9.665340909090908
  pid: 4061
  policy_reward_max:
    agent-0: 84.50000000000017
    agent-1: 84.50000000000017
    agent-2: 84.50000000000017
    agent-3: 84.50000000000017
    agent-4: 84.50000000000017
    agent-5: 84.50000000000017
  policy_reward_mean:
    agent-0: 54.96999999999989
    agent-1: 54.96999999999989
    agent-2: 54.96999999999989
    agent-3: 54.96999999999989
    agent-4: 54.96999999999989
    agent-5: 54.96999999999989
  policy_reward_min:
    agent-0: 9.833333333333334
    agent-1: 9.833333333333334
    agent-2: 9.833333333333334
    agent-3: 9.833333333333334
    agent-4: 9.833333333333334
    agent-5: 9.833333333333334
  sampler_perf:
    mean_env_wait_ms: 22.863311417988744
    mean_inference_ms: 12.413928720980136
    mean_processing_ms: 51.426416276477724
  time_since_restore: 4866.039041042328
  time_this_iter_s: 122.96474623680115
  time_total_s: 8077.102727174759
  timestamp: 1637022377
  timesteps_since_restore: 3072000
  timesteps_this_iter: 96000
  timesteps_total: 4992000
  training_iteration: 52
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     52 |           8077.1 | 4992000 |   329.82 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 59
    apples_agent-0_mean: 6.38
    apples_agent-0_min: 0
    apples_agent-1_max: 92
    apples_agent-1_mean: 14.2
    apples_agent-1_min: 0
    apples_agent-2_max: 76
    apples_agent-2_mean: 11.52
    apples_agent-2_min: 0
    apples_agent-3_max: 126
    apples_agent-3_mean: 56.72
    apples_agent-3_min: 0
    apples_agent-4_max: 79
    apples_agent-4_mean: 6.17
    apples_agent-4_min: 0
    apples_agent-5_max: 115
    apples_agent-5_mean: 56.39
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 308
    cleaning_beam_agent-0_mean: 236.23
    cleaning_beam_agent-0_min: 147
    cleaning_beam_agent-1_max: 653
    cleaning_beam_agent-1_mean: 233.6
    cleaning_beam_agent-1_min: 63
    cleaning_beam_agent-2_max: 691
    cleaning_beam_agent-2_mean: 399.74
    cleaning_beam_agent-2_min: 150
    cleaning_beam_agent-3_max: 100
    cleaning_beam_agent-3_mean: 66.47
    cleaning_beam_agent-3_min: 33
    cleaning_beam_agent-4_max: 528
    cleaning_beam_agent-4_mean: 272.16
    cleaning_beam_agent-4_min: 60
    cleaning_beam_agent-5_max: 123
    cleaning_beam_agent-5_mean: 50.59
    cleaning_beam_agent-5_min: 15
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.05
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 3
    fire_beam_agent-4_mean: 0.11
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_19-29-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 508.00000000000983
  episode_reward_mean: 343.76000000000334
  episode_reward_min: 126.00000000000054
  episodes_this_iter: 96
  episodes_total: 5088
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20249.751
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009484991896897554
        entropy: 1.5530271530151367
        entropy_coeff: 0.0017600000137463212
        kl: 0.010448113083839417
        model: {}
        policy_loss: -0.017818527296185493
        total_loss: -0.01883799023926258
        vf_explained_var: 0.05322258174419403
        vf_loss: 6.6905364990234375
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009484991896897554
        entropy: 1.4172574281692505
        entropy_coeff: 0.0017600000137463212
        kl: 0.016442591324448586
        model: {}
        policy_loss: -0.024691246449947357
        total_loss: -0.02484656311571598
        vf_explained_var: 0.01723533868789673
        vf_loss: 6.947964191436768
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009484991896897554
        entropy: 1.2729463577270508
        entropy_coeff: 0.0017600000137463212
        kl: 0.013461221940815449
        model: {}
        policy_loss: -0.022063633427023888
        total_loss: -0.022317443042993546
        vf_explained_var: 0.09333357214927673
        vf_loss: 6.404542446136475
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009484991896897554
        entropy: 1.3178722858428955
        entropy_coeff: 0.0017600000137463212
        kl: 0.012240098789334297
        model: {}
        policy_loss: -0.022460319101810455
        total_loss: -0.022928712889552116
        vf_explained_var: 0.11316229403018951
        vf_loss: 6.270488739013672
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009484991896897554
        entropy: 1.3317651748657227
        entropy_coeff: 0.0017600000137463212
        kl: 0.013749927282333374
        model: {}
        policy_loss: -0.026972755789756775
        total_loss: -0.02732846513390541
        vf_explained_var: 0.13210506737232208
        vf_loss: 6.132034778594971
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009484991896897554
        entropy: 1.320200800895691
        entropy_coeff: 0.0017600000137463212
        kl: 0.014084549620747566
        model: {}
        policy_loss: -0.024564452469348907
        total_loss: -0.024865897372364998
        vf_explained_var: 0.13221493363380432
        vf_loss: 6.136534690856934
    load_time_ms: 16820.492
    num_steps_sampled: 5088000
    num_steps_trained: 5088000
    sample_time_ms: 94148.427
    update_time_ms: 26.24
  iterations_since_restore: 33
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.167039106145252
    ram_util_percent: 9.639664804469273
  pid: 4061
  policy_reward_max:
    agent-0: 84.66666666666694
    agent-1: 84.66666666666694
    agent-2: 84.66666666666694
    agent-3: 84.66666666666694
    agent-4: 84.66666666666694
    agent-5: 84.66666666666694
  policy_reward_mean:
    agent-0: 57.29333333333323
    agent-1: 57.29333333333323
    agent-2: 57.29333333333323
    agent-3: 57.29333333333323
    agent-4: 57.29333333333323
    agent-5: 57.29333333333323
  policy_reward_min:
    agent-0: 20.999999999999993
    agent-1: 20.999999999999993
    agent-2: 20.999999999999993
    agent-3: 20.999999999999993
    agent-4: 20.999999999999993
    agent-5: 20.999999999999993
  sampler_perf:
    mean_env_wait_ms: 22.87594144795044
    mean_inference_ms: 12.405823447612786
    mean_processing_ms: 51.387388450782716
  time_since_restore: 5037.067874908447
  time_this_iter_s: 171.02883386611938
  time_total_s: 8248.131561040878
  timestamp: 1637022548
  timesteps_since_restore: 3168000
  timesteps_this_iter: 96000
  timesteps_total: 5088000
  training_iteration: 53
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     53 |          8248.13 | 5088000 |   343.76 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 69
    apples_agent-0_mean: 7.76
    apples_agent-0_min: 0
    apples_agent-1_max: 88
    apples_agent-1_mean: 14.29
    apples_agent-1_min: 0
    apples_agent-2_max: 100
    apples_agent-2_mean: 10.3
    apples_agent-2_min: 0
    apples_agent-3_max: 120
    apples_agent-3_mean: 60.55
    apples_agent-3_min: 10
    apples_agent-4_max: 91
    apples_agent-4_mean: 5.26
    apples_agent-4_min: 0
    apples_agent-5_max: 105
    apples_agent-5_mean: 59.42
    apples_agent-5_min: 6
    cleaning_beam_agent-0_max: 312
    cleaning_beam_agent-0_mean: 236.99
    cleaning_beam_agent-0_min: 128
    cleaning_beam_agent-1_max: 485
    cleaning_beam_agent-1_mean: 256.07
    cleaning_beam_agent-1_min: 71
    cleaning_beam_agent-2_max: 645
    cleaning_beam_agent-2_mean: 382.32
    cleaning_beam_agent-2_min: 119
    cleaning_beam_agent-3_max: 142
    cleaning_beam_agent-3_mean: 65.41
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 590
    cleaning_beam_agent-4_mean: 313.48
    cleaning_beam_agent-4_min: 94
    cleaning_beam_agent-5_max: 148
    cleaning_beam_agent-5_mean: 57.03
    cleaning_beam_agent-5_min: 19
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.06
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 3
    fire_beam_agent-4_mean: 0.13
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_19-31-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 559.000000000004
  episode_reward_mean: 367.6800000000047
  episode_reward_min: 128.00000000000134
  episodes_this_iter: 96
  episodes_total: 5184
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20238.615
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009425088064745069
        entropy: 1.5200296640396118
        entropy_coeff: 0.0017600000137463212
        kl: 0.01124644372612238
        model: {}
        policy_loss: -0.01765367016196251
        total_loss: -0.01853358745574951
        vf_explained_var: 0.0377175509929657
        vf_loss: 6.706930160522461
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009425088064745069
        entropy: 1.4132506847381592
        entropy_coeff: 0.0017600000137463212
        kl: 0.013754552230238914
        model: {}
        policy_loss: -0.026581117883324623
        total_loss: -0.027015354484319687
        vf_explained_var: 0.030085071921348572
        vf_loss: 6.776336669921875
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009425088064745069
        entropy: 1.2988057136535645
        entropy_coeff: 0.0017600000137463212
        kl: 0.013486362993717194
        model: {}
        policy_loss: -0.023197416216135025
        total_loss: -0.023477276787161827
        vf_explained_var: 0.05856350064277649
        vf_loss: 6.574010848999023
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009425088064745069
        entropy: 1.2525253295898438
        entropy_coeff: 0.0017600000137463212
        kl: 0.011981528252363205
        model: {}
        policy_loss: -0.020565494894981384
        total_loss: -0.020940449088811874
        vf_explained_var: 0.09561361372470856
        vf_loss: 6.313340663909912
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009425088064745069
        entropy: 1.304526686668396
        entropy_coeff: 0.0017600000137463212
        kl: 0.013056071475148201
        model: {}
        policy_loss: -0.027055315673351288
        total_loss: -0.027372512966394424
        vf_explained_var: 0.03544732928276062
        vf_loss: 6.731630325317383
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009425088064745069
        entropy: 1.3492710590362549
        entropy_coeff: 0.0017600000137463212
        kl: 0.01491981279104948
        model: {}
        policy_loss: -0.026486266404390335
        total_loss: -0.02675376832485199
        vf_explained_var: 0.12030528485774994
        vf_loss: 6.152335166931152
    load_time_ms: 15846.499
    num_steps_sampled: 5184000
    num_steps_trained: 5184000
    sample_time_ms: 93822.894
    update_time_ms: 30.664
  iterations_since_restore: 34
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 16.658757062146893
    ram_util_percent: 9.781920903954802
  pid: 4061
  policy_reward_max:
    agent-0: 93.16666666666693
    agent-1: 93.16666666666693
    agent-2: 93.16666666666693
    agent-3: 93.16666666666693
    agent-4: 93.16666666666693
    agent-5: 93.16666666666693
  policy_reward_mean:
    agent-0: 61.279999999999916
    agent-1: 61.279999999999916
    agent-2: 61.279999999999916
    agent-3: 61.279999999999916
    agent-4: 61.279999999999916
    agent-5: 61.279999999999916
  policy_reward_min:
    agent-0: 21.33333333333334
    agent-1: 21.33333333333334
    agent-2: 21.33333333333334
    agent-3: 21.33333333333334
    agent-4: 21.33333333333334
    agent-5: 21.33333333333334
  sampler_perf:
    mean_env_wait_ms: 22.89676464102087
    mean_inference_ms: 12.400179950429626
    mean_processing_ms: 51.36191151738598
  time_since_restore: 5161.145706415176
  time_this_iter_s: 124.07783150672913
  time_total_s: 8372.209392547607
  timestamp: 1637022672
  timesteps_since_restore: 3264000
  timesteps_this_iter: 96000
  timesteps_total: 5184000
  training_iteration: 54
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     54 |          8372.21 | 5184000 |   367.68 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 34
    apples_agent-0_mean: 5.38
    apples_agent-0_min: 0
    apples_agent-1_max: 74
    apples_agent-1_mean: 13.86
    apples_agent-1_min: 0
    apples_agent-2_max: 237
    apples_agent-2_mean: 27.54
    apples_agent-2_min: 0
    apples_agent-3_max: 121
    apples_agent-3_mean: 57.96
    apples_agent-3_min: 11
    apples_agent-4_max: 110
    apples_agent-4_mean: 7.67
    apples_agent-4_min: 0
    apples_agent-5_max: 121
    apples_agent-5_mean: 58.31
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 346
    cleaning_beam_agent-0_mean: 266.29
    cleaning_beam_agent-0_min: 163
    cleaning_beam_agent-1_max: 550
    cleaning_beam_agent-1_mean: 278.75
    cleaning_beam_agent-1_min: 111
    cleaning_beam_agent-2_max: 553
    cleaning_beam_agent-2_mean: 325.35
    cleaning_beam_agent-2_min: 64
    cleaning_beam_agent-3_max: 102
    cleaning_beam_agent-3_mean: 65.54
    cleaning_beam_agent-3_min: 36
    cleaning_beam_agent-4_max: 591
    cleaning_beam_agent-4_mean: 307.88
    cleaning_beam_agent-4_min: 97
    cleaning_beam_agent-5_max: 165
    cleaning_beam_agent-5_mean: 59.93
    cleaning_beam_agent-5_min: 23
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.08
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.04
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 3
    fire_beam_agent-4_mean: 0.12
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_19-33-21
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 554.0000000000119
  episode_reward_mean: 360.1800000000044
  episode_reward_min: 22.000000000000814
  episodes_this_iter: 96
  episodes_total: 5280
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20248.076
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009365184232592583
        entropy: 1.4690709114074707
        entropy_coeff: 0.0017600000137463212
        kl: 0.011073211207985878
        model: {}
        policy_loss: -0.01908097043633461
        total_loss: -0.019818972796201706
        vf_explained_var: 0.06006534397602081
        vf_loss: 7.402430534362793
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009365184232592583
        entropy: 1.4018129110336304
        entropy_coeff: 0.0017600000137463212
        kl: 0.01802125945687294
        model: {}
        policy_loss: -0.02293054200708866
        total_loss: -0.02279778942465782
        vf_explained_var: -0.013275831937789917
        vf_loss: 7.978168964385986
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009365184232592583
        entropy: 1.2540186643600464
        entropy_coeff: 0.0017600000137463212
        kl: 0.014537138864398003
        model: {}
        policy_loss: -0.0245533250272274
        total_loss: -0.02461339719593525
        vf_explained_var: 0.11987964808940887
        vf_loss: 6.932875633239746
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009365184232592583
        entropy: 1.2435929775238037
        entropy_coeff: 0.0017600000137463212
        kl: 0.014764603227376938
        model: {}
        policy_loss: -0.019240722060203552
        total_loss: -0.019255174323916435
        vf_explained_var: 0.11357463896274567
        vf_loss: 6.9780731201171875
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009365184232592583
        entropy: 1.288851261138916
        entropy_coeff: 0.0017600000137463212
        kl: 0.016982540488243103
        model: {}
        policy_loss: -0.024529699236154556
        total_loss: -0.024359680712223053
        vf_explained_var: 0.06076635420322418
        vf_loss: 7.401454448699951
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009365184232592583
        entropy: 1.3650562763214111
        entropy_coeff: 0.0017600000137463212
        kl: 0.013534262776374817
        model: {}
        policy_loss: -0.02661069855093956
        total_loss: -0.026975132524967194
        vf_explained_var: 0.13081170618534088
        vf_loss: 6.846399784088135
    load_time_ms: 14671.464
    num_steps_sampled: 5280000
    num_steps_trained: 5280000
    sample_time_ms: 94117.576
    update_time_ms: 30.692
  iterations_since_restore: 35
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 14.86393442622951
    ram_util_percent: 9.717486338797812
  pid: 4061
  policy_reward_max:
    agent-0: 92.33333333333371
    agent-1: 92.33333333333371
    agent-2: 92.33333333333371
    agent-3: 92.33333333333371
    agent-4: 92.33333333333371
    agent-5: 92.33333333333371
  policy_reward_mean:
    agent-0: 60.0299999999999
    agent-1: 60.0299999999999
    agent-2: 60.0299999999999
    agent-3: 60.0299999999999
    agent-4: 60.0299999999999
    agent-5: 60.0299999999999
  policy_reward_min:
    agent-0: 3.666666666666649
    agent-1: 3.666666666666649
    agent-2: 3.666666666666649
    agent-3: 3.666666666666649
    agent-4: 3.666666666666649
    agent-5: 3.666666666666649
  sampler_perf:
    mean_env_wait_ms: 22.912257234448155
    mean_inference_ms: 12.410747922700061
    mean_processing_ms: 51.37110860948721
  time_since_restore: 5289.661789417267
  time_this_iter_s: 128.51608300209045
  time_total_s: 8500.725475549698
  timestamp: 1637022801
  timesteps_since_restore: 3360000
  timesteps_this_iter: 96000
  timesteps_total: 5280000
  training_iteration: 55
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     55 |          8500.73 | 5280000 |   360.18 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 104
    apples_agent-0_mean: 10.16
    apples_agent-0_min: 0
    apples_agent-1_max: 110
    apples_agent-1_mean: 15.47
    apples_agent-1_min: 0
    apples_agent-2_max: 151
    apples_agent-2_mean: 25.55
    apples_agent-2_min: 0
    apples_agent-3_max: 157
    apples_agent-3_mean: 66.63
    apples_agent-3_min: 18
    apples_agent-4_max: 89
    apples_agent-4_mean: 5.01
    apples_agent-4_min: 0
    apples_agent-5_max: 123
    apples_agent-5_mean: 61.22
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 398
    cleaning_beam_agent-0_mean: 296.51
    cleaning_beam_agent-0_min: 204
    cleaning_beam_agent-1_max: 572
    cleaning_beam_agent-1_mean: 286.26
    cleaning_beam_agent-1_min: 132
    cleaning_beam_agent-2_max: 608
    cleaning_beam_agent-2_mean: 327.5
    cleaning_beam_agent-2_min: 108
    cleaning_beam_agent-3_max: 147
    cleaning_beam_agent-3_mean: 66.46
    cleaning_beam_agent-3_min: 32
    cleaning_beam_agent-4_max: 609
    cleaning_beam_agent-4_mean: 340.73
    cleaning_beam_agent-4_min: 106
    cleaning_beam_agent-5_max: 195
    cleaning_beam_agent-5_mean: 54.86
    cleaning_beam_agent-5_min: 21
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.05
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.13
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_19-35-28
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 646.0000000000011
  episode_reward_mean: 405.7700000000052
  episode_reward_min: 194.99999999999787
  episodes_this_iter: 96
  episodes_total: 5376
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20218.537
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009305279818363488
        entropy: 1.3835227489471436
        entropy_coeff: 0.0017600000137463212
        kl: 0.014279564842581749
        model: {}
        policy_loss: -0.016171293333172798
        total_loss: -0.016377989202737808
        vf_explained_var: 0.025374680757522583
        vf_loss: 8.003464698791504
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009305279818363488
        entropy: 1.370774745941162
        entropy_coeff: 0.0017600000137463212
        kl: 0.014158714562654495
        model: {}
        policy_loss: -0.025578759610652924
        total_loss: -0.025795046240091324
        vf_explained_var: 0.0500679612159729
        vf_loss: 7.804054260253906
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009305279818363488
        entropy: 1.2854219675064087
        entropy_coeff: 0.0017600000137463212
        kl: 0.014439292252063751
        model: {}
        policy_loss: -0.02445838414132595
        total_loss: -0.024514244869351387
        vf_explained_var: 0.07112617790699005
        vf_loss: 7.625499725341797
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009305279818363488
        entropy: 1.2149238586425781
        entropy_coeff: 0.0017600000137463212
        kl: 0.012722671963274479
        model: {}
        policy_loss: -0.020350227132439613
        total_loss: -0.020482074469327927
        vf_explained_var: 0.10600489377975464
        vf_loss: 7.3415141105651855
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009305279818363488
        entropy: 1.2462143898010254
        entropy_coeff: 0.0017600000137463212
        kl: 0.014190446585416794
        model: {}
        policy_loss: -0.027828939259052277
        total_loss: -0.027851175516843796
        vf_explained_var: 0.08218680322170258
        vf_loss: 7.520562171936035
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009305279818363488
        entropy: 1.3251739740371704
        entropy_coeff: 0.0017600000137463212
        kl: 0.014232836663722992
        model: {}
        policy_loss: -0.02662467770278454
        total_loss: -0.026821356266736984
        vf_explained_var: 0.13258391618728638
        vf_loss: 7.123444557189941
    load_time_ms: 15012.138
    num_steps_sampled: 5376000
    num_steps_trained: 5376000
    sample_time_ms: 93946.944
    update_time_ms: 29.415
  iterations_since_restore: 36
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 14.958011049723757
    ram_util_percent: 9.74696132596685
  pid: 4061
  policy_reward_max:
    agent-0: 107.6666666666671
    agent-1: 107.6666666666671
    agent-2: 107.6666666666671
    agent-3: 107.6666666666671
    agent-4: 107.6666666666671
    agent-5: 107.6666666666671
  policy_reward_mean:
    agent-0: 67.62833333333327
    agent-1: 67.62833333333327
    agent-2: 67.62833333333327
    agent-3: 67.62833333333327
    agent-4: 67.62833333333327
    agent-5: 67.62833333333327
  policy_reward_min:
    agent-0: 32.50000000000008
    agent-1: 32.50000000000008
    agent-2: 32.50000000000008
    agent-3: 32.50000000000008
    agent-4: 32.50000000000008
    agent-5: 32.50000000000008
  sampler_perf:
    mean_env_wait_ms: 22.933810247385406
    mean_inference_ms: 12.40441665355273
    mean_processing_ms: 51.337639412325814
  time_since_restore: 5416.6139171123505
  time_this_iter_s: 126.95212769508362
  time_total_s: 8627.677603244781
  timestamp: 1637022928
  timesteps_since_restore: 3456000
  timesteps_this_iter: 96000
  timesteps_total: 5376000
  training_iteration: 56
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     56 |          8627.68 | 5376000 |   405.77 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 104
    apples_agent-0_mean: 13.35
    apples_agent-0_min: 0
    apples_agent-1_max: 63
    apples_agent-1_mean: 11.72
    apples_agent-1_min: 0
    apples_agent-2_max: 244
    apples_agent-2_mean: 20.07
    apples_agent-2_min: 0
    apples_agent-3_max: 118
    apples_agent-3_mean: 68.02
    apples_agent-3_min: 13
    apples_agent-4_max: 98
    apples_agent-4_mean: 4.34
    apples_agent-4_min: 0
    apples_agent-5_max: 134
    apples_agent-5_mean: 60.29
    apples_agent-5_min: 17
    cleaning_beam_agent-0_max: 446
    cleaning_beam_agent-0_mean: 272.05
    cleaning_beam_agent-0_min: 149
    cleaning_beam_agent-1_max: 588
    cleaning_beam_agent-1_mean: 313.32
    cleaning_beam_agent-1_min: 118
    cleaning_beam_agent-2_max: 659
    cleaning_beam_agent-2_mean: 331.74
    cleaning_beam_agent-2_min: 111
    cleaning_beam_agent-3_max: 108
    cleaning_beam_agent-3_mean: 63.02
    cleaning_beam_agent-3_min: 29
    cleaning_beam_agent-4_max: 583
    cleaning_beam_agent-4_mean: 327.39
    cleaning_beam_agent-4_min: 71
    cleaning_beam_agent-5_max: 156
    cleaning_beam_agent-5_mean: 57.6
    cleaning_beam_agent-5_min: 17
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 5
    fire_beam_agent-1_mean: 0.1
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 4
    fire_beam_agent-4_mean: 0.1
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.07
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_19-37-33
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 598.0000000000028
  episode_reward_mean: 405.0500000000061
  episode_reward_min: 161.9999999999976
  episodes_this_iter: 96
  episodes_total: 5472
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20225.915
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009245375986211002
        entropy: 1.3516974449157715
        entropy_coeff: 0.0017600000137463212
        kl: 0.011784186586737633
        model: {}
        policy_loss: -0.019370023161172867
        total_loss: -0.019676752388477325
        vf_explained_var: 0.04311691224575043
        vf_loss: 8.938419342041016
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009245375986211002
        entropy: 1.3533021211624146
        entropy_coeff: 0.0017600000137463212
        kl: 0.0161137618124485
        model: {}
        policy_loss: -0.025685414671897888
        total_loss: -0.025570061057806015
        vf_explained_var: 0.052830636501312256
        vf_loss: 8.857882499694824
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009245375986211002
        entropy: 1.299504280090332
        entropy_coeff: 0.0017600000137463212
        kl: 0.015195276588201523
        model: {}
        policy_loss: -0.025276483967900276
        total_loss: -0.025193121284246445
        vf_explained_var: 0.08948308229446411
        vf_loss: 8.509629249572754
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009245375986211002
        entropy: 1.2118312120437622
        entropy_coeff: 0.0017600000137463212
        kl: 0.012449841946363449
        model: {}
        policy_loss: -0.022609591484069824
        total_loss: -0.02270400896668434
        vf_explained_var: 0.15114347636699677
        vf_loss: 7.934212684631348
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009245375986211002
        entropy: 1.2738995552062988
        entropy_coeff: 0.0017600000137463212
        kl: 0.01541886292397976
        model: {}
        policy_loss: -0.030613891780376434
        total_loss: -0.03044014610350132
        vf_explained_var: 0.06449602544307709
        vf_loss: 8.73924446105957
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009245375986211002
        entropy: 1.3430970907211304
        entropy_coeff: 0.0017600000137463212
        kl: 0.015395100228488445
        model: {}
        policy_loss: -0.028727805241942406
        total_loss: -0.02874688245356083
        vf_explained_var: 0.139115571975708
        vf_loss: 8.052643775939941
    load_time_ms: 15034.352
    num_steps_sampled: 5472000
    num_steps_trained: 5472000
    sample_time_ms: 93889.547
    update_time_ms: 29.223
  iterations_since_restore: 37
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.24943820224719
    ram_util_percent: 9.6938202247191
  pid: 4061
  policy_reward_max:
    agent-0: 99.66666666666686
    agent-1: 99.66666666666686
    agent-2: 99.66666666666686
    agent-3: 99.66666666666686
    agent-4: 99.66666666666686
    agent-5: 99.66666666666686
  policy_reward_mean:
    agent-0: 67.50833333333328
    agent-1: 67.50833333333328
    agent-2: 67.50833333333328
    agent-3: 67.50833333333328
    agent-4: 67.50833333333328
    agent-5: 67.50833333333328
  policy_reward_min:
    agent-0: 27.00000000000005
    agent-1: 27.00000000000005
    agent-2: 27.00000000000005
    agent-3: 27.00000000000005
    agent-4: 27.00000000000005
    agent-5: 27.00000000000005
  sampler_perf:
    mean_env_wait_ms: 22.955169325808978
    mean_inference_ms: 12.397934109310599
    mean_processing_ms: 51.30791148010555
  time_since_restore: 5541.417150020599
  time_this_iter_s: 124.8032329082489
  time_total_s: 8752.48083615303
  timestamp: 1637023053
  timesteps_since_restore: 3552000
  timesteps_this_iter: 96000
  timesteps_total: 5472000
  training_iteration: 57
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     57 |          8752.48 | 5472000 |   405.05 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 40
    apples_agent-0_mean: 6.02
    apples_agent-0_min: 0
    apples_agent-1_max: 72
    apples_agent-1_mean: 16.26
    apples_agent-1_min: 0
    apples_agent-2_max: 153
    apples_agent-2_mean: 18.61
    apples_agent-2_min: 0
    apples_agent-3_max: 148
    apples_agent-3_mean: 72.83
    apples_agent-3_min: 8
    apples_agent-4_max: 91
    apples_agent-4_mean: 7.02
    apples_agent-4_min: 0
    apples_agent-5_max: 95
    apples_agent-5_mean: 53.55
    apples_agent-5_min: 6
    cleaning_beam_agent-0_max: 427
    cleaning_beam_agent-0_mean: 298.44
    cleaning_beam_agent-0_min: 173
    cleaning_beam_agent-1_max: 585
    cleaning_beam_agent-1_mean: 280.72
    cleaning_beam_agent-1_min: 74
    cleaning_beam_agent-2_max: 491
    cleaning_beam_agent-2_mean: 296.47
    cleaning_beam_agent-2_min: 126
    cleaning_beam_agent-3_max: 128
    cleaning_beam_agent-3_mean: 66.29
    cleaning_beam_agent-3_min: 27
    cleaning_beam_agent-4_max: 522
    cleaning_beam_agent-4_mean: 331.86
    cleaning_beam_agent-4_min: 95
    cleaning_beam_agent-5_max: 192
    cleaning_beam_agent-5_mean: 64.38
    cleaning_beam_agent-5_min: 16
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 7
    fire_beam_agent-1_mean: 0.1
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_19-39-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 641.9999999999937
  episode_reward_mean: 409.56000000000574
  episode_reward_min: 73.99999999999997
  episodes_this_iter: 96
  episodes_total: 5568
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20217.905
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009185472154058516
        entropy: 1.4281649589538574
        entropy_coeff: 0.0017600000137463212
        kl: 0.011428913101553917
        model: {}
        policy_loss: -0.018603945150971413
        total_loss: -0.01903252862393856
        vf_explained_var: 0.05650131404399872
        vf_loss: 9.42092227935791
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009185472154058516
        entropy: 1.3361554145812988
        entropy_coeff: 0.0017600000137463212
        kl: 0.013852667063474655
        model: {}
        policy_loss: -0.02997605875134468
        total_loss: -0.02998356521129608
        vf_explained_var: 0.04198770225048065
        vf_loss: 9.588580131530762
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009185472154058516
        entropy: 1.340277910232544
        entropy_coeff: 0.0017600000137463212
        kl: 0.014921966940164566
        model: {}
        policy_loss: -0.026910709217190742
        total_loss: -0.026879653334617615
        vf_explained_var: 0.10016860067844391
        vf_loss: 8.977508544921875
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009185472154058516
        entropy: 1.1996122598648071
        entropy_coeff: 0.0017600000137463212
        kl: 0.012113256379961967
        model: {}
        policy_loss: -0.02158423885703087
        total_loss: -0.021642394363880157
        vf_explained_var: 0.1568962037563324
        vf_loss: 8.4183349609375
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009185472154058516
        entropy: 1.2415262460708618
        entropy_coeff: 0.0017600000137463212
        kl: 0.015946216881275177
        model: {}
        policy_loss: -0.02996646985411644
        total_loss: -0.029701225459575653
        vf_explained_var: 0.14259667694568634
        vf_loss: 8.55705451965332
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009185472154058516
        entropy: 1.3279705047607422
        entropy_coeff: 0.0017600000137463212
        kl: 0.016661185771226883
        model: {}
        policy_loss: -0.027047861367464066
        total_loss: -0.02685713768005371
        vf_explained_var: 0.13850267231464386
        vf_loss: 8.618342399597168
    load_time_ms: 15172.567
    num_steps_sampled: 5568000
    num_steps_trained: 5568000
    sample_time_ms: 93847.079
    update_time_ms: 29.963
  iterations_since_restore: 38
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.811602209944752
    ram_util_percent: 9.709944751381215
  pid: 4061
  policy_reward_max:
    agent-0: 107.00000000000043
    agent-1: 107.00000000000043
    agent-2: 107.00000000000043
    agent-3: 107.00000000000043
    agent-4: 107.00000000000043
    agent-5: 107.00000000000043
  policy_reward_mean:
    agent-0: 68.25999999999999
    agent-1: 68.25999999999999
    agent-2: 68.25999999999999
    agent-3: 68.25999999999999
    agent-4: 68.25999999999999
    agent-5: 68.25999999999999
  policy_reward_min:
    agent-0: 12.333333333333321
    agent-1: 12.333333333333321
    agent-2: 12.333333333333321
    agent-3: 12.333333333333321
    agent-4: 12.333333333333321
    agent-5: 12.333333333333321
  sampler_perf:
    mean_env_wait_ms: 22.978501997215474
    mean_inference_ms: 12.39269896905098
    mean_processing_ms: 51.28985359835084
  time_since_restore: 5668.033278465271
  time_this_iter_s: 126.61612844467163
  time_total_s: 8879.096964597702
  timestamp: 1637023180
  timesteps_since_restore: 3648000
  timesteps_this_iter: 96000
  timesteps_total: 5568000
  training_iteration: 58
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     58 |           8879.1 | 5568000 |   409.56 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 58
    apples_agent-0_mean: 9.03
    apples_agent-0_min: 0
    apples_agent-1_max: 95
    apples_agent-1_mean: 14.83
    apples_agent-1_min: 0
    apples_agent-2_max: 118
    apples_agent-2_mean: 20.38
    apples_agent-2_min: 0
    apples_agent-3_max: 131
    apples_agent-3_mean: 75.75
    apples_agent-3_min: 16
    apples_agent-4_max: 68
    apples_agent-4_mean: 3.49
    apples_agent-4_min: 0
    apples_agent-5_max: 110
    apples_agent-5_mean: 59.74
    apples_agent-5_min: 20
    cleaning_beam_agent-0_max: 443
    cleaning_beam_agent-0_mean: 287.01
    cleaning_beam_agent-0_min: 173
    cleaning_beam_agent-1_max: 566
    cleaning_beam_agent-1_mean: 311.68
    cleaning_beam_agent-1_min: 144
    cleaning_beam_agent-2_max: 514
    cleaning_beam_agent-2_mean: 306.78
    cleaning_beam_agent-2_min: 102
    cleaning_beam_agent-3_max: 128
    cleaning_beam_agent-3_mean: 64.21
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 543
    cleaning_beam_agent-4_mean: 365.38
    cleaning_beam_agent-4_min: 133
    cleaning_beam_agent-5_max: 126
    cleaning_beam_agent-5_mean: 62.97
    cleaning_beam_agent-5_min: 21
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.06
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.04
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.07
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_19-41-44
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 687.9999999999867
  episode_reward_mean: 436.1000000000067
  episode_reward_min: 208.99999999999918
  episodes_this_iter: 96
  episodes_total: 5664
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20217.78
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009125567739829421
        entropy: 1.3826372623443604
        entropy_coeff: 0.0017600000137463212
        kl: 0.010954800061881542
        model: {}
        policy_loss: -0.02047407254576683
        total_loss: -0.02094978839159012
        vf_explained_var: 0.060177505016326904
        vf_loss: 8.622440338134766
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009125567739829421
        entropy: 1.30595862865448
        entropy_coeff: 0.0017600000137463212
        kl: 0.015342055819928646
        model: {}
        policy_loss: -0.03160851076245308
        total_loss: -0.03145138546824455
        vf_explained_var: -0.0030063986778259277
        vf_loss: 9.214034080505371
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009125567739829421
        entropy: 1.318711280822754
        entropy_coeff: 0.0017600000137463212
        kl: 0.014106753282248974
        model: {}
        policy_loss: -0.027350418269634247
        total_loss: -0.02741030976176262
        vf_explained_var: 0.07242082059383392
        vf_loss: 8.50364875793457
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009125567739829421
        entropy: 1.1518404483795166
        entropy_coeff: 0.0017600000137463212
        kl: 0.01300230622291565
        model: {}
        policy_loss: -0.021652938798069954
        total_loss: -0.02159452997148037
        vf_explained_var: 0.14379823207855225
        vf_loss: 7.854184150695801
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009125567739829421
        entropy: 1.2218056917190552
        entropy_coeff: 0.0017600000137463212
        kl: 0.015185344964265823
        model: {}
        policy_loss: -0.02934657409787178
        total_loss: -0.02914799004793167
        vf_explained_var: 0.09325563907623291
        vf_loss: 8.304254531860352
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009125567739829421
        entropy: 1.294954538345337
        entropy_coeff: 0.0017600000137463212
        kl: 0.014019947499036789
        model: {}
        policy_loss: -0.028284940868616104
        total_loss: -0.028377749025821686
        vf_explained_var: 0.14424414932727814
        vf_loss: 7.843197345733643
    load_time_ms: 15127.323
    num_steps_sampled: 5664000
    num_steps_trained: 5664000
    sample_time_ms: 93860.539
    update_time_ms: 29.42
  iterations_since_restore: 39
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.238983050847459
    ram_util_percent: 9.6819209039548
  pid: 4061
  policy_reward_max:
    agent-0: 114.66666666666723
    agent-1: 114.66666666666723
    agent-2: 114.66666666666723
    agent-3: 114.66666666666723
    agent-4: 114.66666666666723
    agent-5: 114.66666666666723
  policy_reward_mean:
    agent-0: 72.68333333333334
    agent-1: 72.68333333333334
    agent-2: 72.68333333333334
    agent-3: 72.68333333333334
    agent-4: 72.68333333333334
    agent-5: 72.68333333333334
  policy_reward_min:
    agent-0: 34.83333333333339
    agent-1: 34.83333333333339
    agent-2: 34.83333333333339
    agent-3: 34.83333333333339
    agent-4: 34.83333333333339
    agent-5: 34.83333333333339
  sampler_perf:
    mean_env_wait_ms: 23.000307693993637
    mean_inference_ms: 12.386227290795288
    mean_processing_ms: 51.26026380196832
  time_since_restore: 5791.734532117844
  time_this_iter_s: 123.70125365257263
  time_total_s: 9002.798218250275
  timestamp: 1637023304
  timesteps_since_restore: 3744000
  timesteps_this_iter: 96000
  timesteps_total: 5664000
  training_iteration: 59
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     59 |           9002.8 | 5664000 |    436.1 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 30
    apples_agent-0_mean: 5.85
    apples_agent-0_min: 0
    apples_agent-1_max: 138
    apples_agent-1_mean: 16.82
    apples_agent-1_min: 0
    apples_agent-2_max: 172
    apples_agent-2_mean: 23.94
    apples_agent-2_min: 0
    apples_agent-3_max: 169
    apples_agent-3_mean: 80.33
    apples_agent-3_min: 7
    apples_agent-4_max: 100
    apples_agent-4_mean: 5.9
    apples_agent-4_min: 0
    apples_agent-5_max: 146
    apples_agent-5_mean: 59.31
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 476
    cleaning_beam_agent-0_mean: 305.55
    cleaning_beam_agent-0_min: 159
    cleaning_beam_agent-1_max: 519
    cleaning_beam_agent-1_mean: 299.23
    cleaning_beam_agent-1_min: 97
    cleaning_beam_agent-2_max: 554
    cleaning_beam_agent-2_mean: 303.54
    cleaning_beam_agent-2_min: 57
    cleaning_beam_agent-3_max: 123
    cleaning_beam_agent-3_mean: 63.93
    cleaning_beam_agent-3_min: 20
    cleaning_beam_agent-4_max: 493
    cleaning_beam_agent-4_mean: 333.16
    cleaning_beam_agent-4_min: 129
    cleaning_beam_agent-5_max: 135
    cleaning_beam_agent-5_mean: 58.92
    cleaning_beam_agent-5_min: 27
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.06
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.08
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.09
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.09
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_19-43-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 680.9999999999905
  episode_reward_mean: 431.86000000000627
  episode_reward_min: 124.00000000000122
  episodes_this_iter: 96
  episodes_total: 5760
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20178.263
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009065663907676935
        entropy: 1.3301938772201538
        entropy_coeff: 0.0017600000137463212
        kl: 0.011625188402831554
        model: {}
        policy_loss: -0.0209849551320076
        total_loss: -0.021189009770751
        vf_explained_var: 0.0757434219121933
        vf_loss: 9.745689392089844
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009065663907676935
        entropy: 1.3140214681625366
        entropy_coeff: 0.0017600000137463212
        kl: 0.01645481400191784
        model: {}
        policy_loss: -0.030119609087705612
        total_loss: -0.02974803000688553
        vf_explained_var: 0.015240296721458435
        vf_loss: 10.387763977050781
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009065663907676935
        entropy: 1.2967331409454346
        entropy_coeff: 0.0017600000137463212
        kl: 0.014447316527366638
        model: {}
        policy_loss: -0.025142040103673935
        total_loss: -0.02496117725968361
        vf_explained_var: 0.03509338200092316
        vf_loss: 10.183853149414062
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009065663907676935
        entropy: 1.1524603366851807
        entropy_coeff: 0.0017600000137463212
        kl: 0.011752096936106682
        model: {}
        policy_loss: -0.02266320213675499
        total_loss: -0.022632788866758347
        vf_explained_var: 0.162740096449852
        vf_loss: 8.835319519042969
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009065663907676935
        entropy: 1.2322616577148438
        entropy_coeff: 0.0017600000137463212
        kl: 0.015010823495686054
        model: {}
        policy_loss: -0.03095533326268196
        total_loss: -0.030658088624477386
        vf_explained_var: 0.08479365706443787
        vf_loss: 9.649471282958984
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009065663907676935
        entropy: 1.2943661212921143
        entropy_coeff: 0.0017600000137463212
        kl: 0.014771128073334694
        model: {}
        policy_loss: -0.029475081712007523
        total_loss: -0.029386527836322784
        vf_explained_var: 0.15674889087677002
        vf_loss: 8.895224571228027
    load_time_ms: 15203.986
    num_steps_sampled: 5760000
    num_steps_trained: 5760000
    sample_time_ms: 93917.258
    update_time_ms: 27.635
  iterations_since_restore: 40
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.301714285714281
    ram_util_percent: 9.667428571428573
  pid: 4061
  policy_reward_max:
    agent-0: 113.50000000000053
    agent-1: 113.50000000000053
    agent-2: 113.50000000000053
    agent-3: 113.50000000000053
    agent-4: 113.50000000000053
    agent-5: 113.50000000000053
  policy_reward_mean:
    agent-0: 71.9766666666667
    agent-1: 71.9766666666667
    agent-2: 71.9766666666667
    agent-3: 71.9766666666667
    agent-4: 71.9766666666667
    agent-5: 71.9766666666667
  policy_reward_min:
    agent-0: 20.666666666666668
    agent-1: 20.666666666666668
    agent-2: 20.666666666666668
    agent-3: 20.666666666666668
    agent-4: 20.666666666666668
    agent-5: 20.666666666666668
  sampler_perf:
    mean_env_wait_ms: 23.020679623013276
    mean_inference_ms: 12.379759658781795
    mean_processing_ms: 51.232463519759
  time_since_restore: 5914.948127746582
  time_this_iter_s: 123.2135956287384
  time_total_s: 9126.011813879013
  timestamp: 1637023427
  timesteps_since_restore: 3840000
  timesteps_this_iter: 96000
  timesteps_total: 5760000
  training_iteration: 60
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     60 |          9126.01 | 5760000 |   431.86 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 42
    apples_agent-0_mean: 6.88
    apples_agent-0_min: 0
    apples_agent-1_max: 75
    apples_agent-1_mean: 19.07
    apples_agent-1_min: 0
    apples_agent-2_max: 127
    apples_agent-2_mean: 14.68
    apples_agent-2_min: 0
    apples_agent-3_max: 157
    apples_agent-3_mean: 83.04
    apples_agent-3_min: 33
    apples_agent-4_max: 104
    apples_agent-4_mean: 4.48
    apples_agent-4_min: 0
    apples_agent-5_max: 93
    apples_agent-5_mean: 62.09
    apples_agent-5_min: 25
    cleaning_beam_agent-0_max: 436
    cleaning_beam_agent-0_mean: 265.28
    cleaning_beam_agent-0_min: 139
    cleaning_beam_agent-1_max: 534
    cleaning_beam_agent-1_mean: 311.39
    cleaning_beam_agent-1_min: 92
    cleaning_beam_agent-2_max: 521
    cleaning_beam_agent-2_mean: 312.94
    cleaning_beam_agent-2_min: 85
    cleaning_beam_agent-3_max: 140
    cleaning_beam_agent-3_mean: 69.78
    cleaning_beam_agent-3_min: 30
    cleaning_beam_agent-4_max: 528
    cleaning_beam_agent-4_mean: 342.67
    cleaning_beam_agent-4_min: 94
    cleaning_beam_agent-5_max: 163
    cleaning_beam_agent-5_mean: 59.09
    cleaning_beam_agent-5_min: 26
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.05
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.04
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_19-45-52
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 671.9999999999914
  episode_reward_mean: 453.21000000000606
  episode_reward_min: 199.99999999999807
  episodes_this_iter: 96
  episodes_total: 5856
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20193.582
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009005760075524449
        entropy: 1.3915162086486816
        entropy_coeff: 0.0017600000137463212
        kl: 0.0139010576531291
        model: {}
        policy_loss: -0.017792057245969772
        total_loss: -0.017893167212605476
        vf_explained_var: 0.0904645174741745
        vf_loss: 9.578540802001953
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009005760075524449
        entropy: 1.2859442234039307
        entropy_coeff: 0.0017600000137463212
        kl: 0.01465800404548645
        model: {}
        policy_loss: -0.03086518868803978
        total_loss: -0.030641566962003708
        vf_explained_var: 0.03232032060623169
        vf_loss: 10.210834503173828
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009005760075524449
        entropy: 1.3220844268798828
        entropy_coeff: 0.0017600000137463212
        kl: 0.014251350425183773
        model: {}
        policy_loss: -0.027041003108024597
        total_loss: -0.026977594941854477
        vf_explained_var: 0.08394977450370789
        vf_loss: 9.651390075683594
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009005760075524449
        entropy: 1.149832844734192
        entropy_coeff: 0.0017600000137463212
        kl: 0.012131785973906517
        model: {}
        policy_loss: -0.022745903581380844
        total_loss: -0.02270815707743168
        vf_explained_var: 0.19584451615810394
        vf_loss: 8.482711791992188
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009005760075524449
        entropy: 1.2272729873657227
        entropy_coeff: 0.0017600000137463212
        kl: 0.015272452495992184
        model: {}
        policy_loss: -0.030234230682253838
        total_loss: -0.029950203374028206
        vf_explained_var: 0.1295059472322464
        vf_loss: 9.167823791503906
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0009005760075524449
        entropy: 1.2754430770874023
        entropy_coeff: 0.0017600000137463212
        kl: 0.014579817652702332
        model: {}
        policy_loss: -0.03142513334751129
        total_loss: -0.0313660129904747
        vf_explained_var: 0.19752676784992218
        vf_loss: 8.459153175354004
    load_time_ms: 15248.087
    num_steps_sampled: 5856000
    num_steps_trained: 5856000
    sample_time_ms: 94013.751
    update_time_ms: 27.347
  iterations_since_restore: 41
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.156741573033708
    ram_util_percent: 9.609550561797754
  pid: 4061
  policy_reward_max:
    agent-0: 112.00000000000058
    agent-1: 112.00000000000058
    agent-2: 112.00000000000058
    agent-3: 112.00000000000058
    agent-4: 112.00000000000058
    agent-5: 112.00000000000058
  policy_reward_mean:
    agent-0: 75.53500000000005
    agent-1: 75.53500000000005
    agent-2: 75.53500000000005
    agent-3: 75.53500000000005
    agent-4: 75.53500000000005
    agent-5: 75.53500000000005
  policy_reward_min:
    agent-0: 33.333333333333385
    agent-1: 33.333333333333385
    agent-2: 33.333333333333385
    agent-3: 33.333333333333385
    agent-4: 33.333333333333385
    agent-5: 33.333333333333385
  sampler_perf:
    mean_env_wait_ms: 23.036107668765286
    mean_inference_ms: 12.37370445675273
    mean_processing_ms: 51.2059879658795
  time_since_restore: 6038.907345533371
  time_this_iter_s: 123.95921778678894
  time_total_s: 9249.971031665802
  timestamp: 1637023552
  timesteps_since_restore: 3936000
  timesteps_this_iter: 96000
  timesteps_total: 5856000
  training_iteration: 61
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     61 |          9249.97 | 5856000 |   453.21 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 39
    apples_agent-0_mean: 7.14
    apples_agent-0_min: 0
    apples_agent-1_max: 86
    apples_agent-1_mean: 21.36
    apples_agent-1_min: 0
    apples_agent-2_max: 165
    apples_agent-2_mean: 15.37
    apples_agent-2_min: 0
    apples_agent-3_max: 161
    apples_agent-3_mean: 87.39
    apples_agent-3_min: 23
    apples_agent-4_max: 79
    apples_agent-4_mean: 3.44
    apples_agent-4_min: 0
    apples_agent-5_max: 129
    apples_agent-5_mean: 68.42
    apples_agent-5_min: 19
    cleaning_beam_agent-0_max: 396
    cleaning_beam_agent-0_mean: 299.84
    cleaning_beam_agent-0_min: 165
    cleaning_beam_agent-1_max: 625
    cleaning_beam_agent-1_mean: 324.76
    cleaning_beam_agent-1_min: 123
    cleaning_beam_agent-2_max: 573
    cleaning_beam_agent-2_mean: 321.07
    cleaning_beam_agent-2_min: 128
    cleaning_beam_agent-3_max: 122
    cleaning_beam_agent-3_mean: 69.79
    cleaning_beam_agent-3_min: 31
    cleaning_beam_agent-4_max: 453
    cleaning_beam_agent-4_mean: 336.72
    cleaning_beam_agent-4_min: 130
    cleaning_beam_agent-5_max: 120
    cleaning_beam_agent-5_mean: 54.46
    cleaning_beam_agent-5_min: 21
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.07
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.04
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.09
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_19-47-56
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 706.9999999999923
  episode_reward_mean: 493.88000000000534
  episode_reward_min: 253.99999999999562
  episodes_this_iter: 96
  episodes_total: 5952
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20203.376
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008945856243371964
        entropy: 1.4153962135314941
        entropy_coeff: 0.0017600000137463212
        kl: 0.010465089231729507
        model: {}
        policy_loss: -0.01948530226945877
        total_loss: -0.019990339875221252
        vf_explained_var: 0.110672727227211
        vf_loss: 9.395488739013672
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008945856243371964
        entropy: 1.302248477935791
        entropy_coeff: 0.0017600000137463212
        kl: 0.015917392447590828
        model: {}
        policy_loss: -0.030743980780243874
        total_loss: -0.030377916991710663
        vf_explained_var: -0.007467493414878845
        vf_loss: 10.662824630737305
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008945856243371964
        entropy: 1.300360918045044
        entropy_coeff: 0.0017600000137463212
        kl: 0.015007605776190758
        model: {}
        policy_loss: -0.026362843811511993
        total_loss: -0.02616078406572342
        vf_explained_var: 0.06393714249134064
        vf_loss: 9.899333000183105
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008945856243371964
        entropy: 1.086930274963379
        entropy_coeff: 0.0017600000137463212
        kl: 0.0157378651201725
        model: {}
        policy_loss: -0.018018046393990517
        total_loss: -0.017463814467191696
        vf_explained_var: 0.15541508793830872
        vf_loss: 8.93442153930664
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008945856243371964
        entropy: 1.2011611461639404
        entropy_coeff: 0.0017600000137463212
        kl: 0.01631082221865654
        model: {}
        policy_loss: -0.028837665915489197
        total_loss: -0.028329873457551003
        vf_explained_var: 0.062258437275886536
        vf_loss: 9.90754508972168
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008945856243371964
        entropy: 1.2706879377365112
        entropy_coeff: 0.0017600000137463212
        kl: 0.015556024387478828
        model: {}
        policy_loss: -0.03117922693490982
        total_loss: -0.0309089757502079
        vf_explained_var: 0.10013441741466522
        vf_loss: 9.510584831237793
    load_time_ms: 15245.177
    num_steps_sampled: 5952000
    num_steps_trained: 5952000
    sample_time_ms: 94119.66
    update_time_ms: 26.879
  iterations_since_restore: 42
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.138068181818182
    ram_util_percent: 9.663636363636362
  pid: 4061
  policy_reward_max:
    agent-0: 117.83333333333378
    agent-1: 117.83333333333378
    agent-2: 117.83333333333378
    agent-3: 117.83333333333378
    agent-4: 117.83333333333378
    agent-5: 117.83333333333378
  policy_reward_mean:
    agent-0: 82.31333333333343
    agent-1: 82.31333333333343
    agent-2: 82.31333333333343
    agent-3: 82.31333333333343
    agent-4: 82.31333333333343
    agent-5: 82.31333333333343
  policy_reward_min:
    agent-0: 42.333333333333314
    agent-1: 42.333333333333314
    agent-2: 42.333333333333314
    agent-3: 42.333333333333314
    agent-4: 42.333333333333314
    agent-5: 42.333333333333314
  sampler_perf:
    mean_env_wait_ms: 23.05505433259843
    mean_inference_ms: 12.36735901380089
    mean_processing_ms: 51.17557460239763
  time_since_restore: 6162.956519842148
  time_this_iter_s: 124.04917430877686
  time_total_s: 9374.020205974579
  timestamp: 1637023676
  timesteps_since_restore: 4032000
  timesteps_this_iter: 96000
  timesteps_total: 5952000
  training_iteration: 62
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     62 |          9374.02 | 5952000 |   493.88 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 59
    apples_agent-0_mean: 5.26
    apples_agent-0_min: 0
    apples_agent-1_max: 90
    apples_agent-1_mean: 17.54
    apples_agent-1_min: 0
    apples_agent-2_max: 126
    apples_agent-2_mean: 16.24
    apples_agent-2_min: 0
    apples_agent-3_max: 184
    apples_agent-3_mean: 95.9
    apples_agent-3_min: 12
    apples_agent-4_max: 74
    apples_agent-4_mean: 4.53
    apples_agent-4_min: 0
    apples_agent-5_max: 119
    apples_agent-5_mean: 70.83
    apples_agent-5_min: 7
    cleaning_beam_agent-0_max: 396
    cleaning_beam_agent-0_mean: 299.14
    cleaning_beam_agent-0_min: 195
    cleaning_beam_agent-1_max: 619
    cleaning_beam_agent-1_mean: 338.22
    cleaning_beam_agent-1_min: 53
    cleaning_beam_agent-2_max: 493
    cleaning_beam_agent-2_mean: 315.75
    cleaning_beam_agent-2_min: 65
    cleaning_beam_agent-3_max: 132
    cleaning_beam_agent-3_mean: 64.78
    cleaning_beam_agent-3_min: 28
    cleaning_beam_agent-4_max: 519
    cleaning_beam_agent-4_mean: 357.1
    cleaning_beam_agent-4_min: 179
    cleaning_beam_agent-5_max: 171
    cleaning_beam_agent-5_mean: 53.67
    cleaning_beam_agent-5_min: 17
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.05
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 4
    fire_beam_agent-4_mean: 0.07
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 6
    fire_beam_agent-5_mean: 0.12
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_19-50-01
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 716.9999999999784
  episode_reward_mean: 501.8200000000045
  episode_reward_min: 125.0000000000014
  episodes_this_iter: 96
  episodes_total: 6048
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20194.157
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008885951829142869
        entropy: 1.4856770038604736
        entropy_coeff: 0.0017600000137463212
        kl: 0.01103985495865345
        model: {}
        policy_loss: -0.020108819007873535
        total_loss: -0.020501084625720978
        vf_explained_var: 0.06654337048530579
        vf_loss: 11.185375213623047
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008885951829142869
        entropy: 1.255549430847168
        entropy_coeff: 0.0017600000137463212
        kl: 0.015801681205630302
        model: {}
        policy_loss: -0.031031467020511627
        total_loss: -0.030524007976055145
        vf_explained_var: 0.051352158188819885
        vf_loss: 11.370561599731445
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008885951829142869
        entropy: 1.304154396057129
        entropy_coeff: 0.0017600000137463212
        kl: 0.014549622312188148
        model: {}
        policy_loss: -0.02816689945757389
        total_loss: -0.02793922647833824
        vf_explained_var: 0.10891574621200562
        vf_loss: 10.68022346496582
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008885951829142869
        entropy: 1.0839778184890747
        entropy_coeff: 0.0017600000137463212
        kl: 0.011331051588058472
        model: {}
        policy_loss: -0.022287538275122643
        total_loss: -0.02207442745566368
        vf_explained_var: 0.17751444876194
        vf_loss: 9.878070831298828
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008885951829142869
        entropy: 1.1972057819366455
        entropy_coeff: 0.0017600000137463212
        kl: 0.014371831901371479
        model: {}
        policy_loss: -0.03225407749414444
        total_loss: -0.031861815601587296
        vf_explained_var: 0.11358210444450378
        vf_loss: 10.621551513671875
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008885951829142869
        entropy: 1.2477989196777344
        entropy_coeff: 0.0017600000137463212
        kl: 0.014846972189843655
        model: {}
        policy_loss: -0.03237418457865715
        total_loss: -0.032108649611473083
        vf_explained_var: 0.18525093793869019
        vf_loss: 9.769644737243652
    load_time_ms: 15210.173
    num_steps_sampled: 6048000
    num_steps_trained: 6048000
    sample_time_ms: 89509.925
    update_time_ms: 26.874
  iterations_since_restore: 43
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.891573033707864
    ram_util_percent: 9.67134831460674
  pid: 4061
  policy_reward_max:
    agent-0: 119.50000000000095
    agent-1: 119.50000000000095
    agent-2: 119.50000000000095
    agent-3: 119.50000000000095
    agent-4: 119.50000000000095
    agent-5: 119.50000000000095
  policy_reward_mean:
    agent-0: 83.63666666666683
    agent-1: 83.63666666666683
    agent-2: 83.63666666666683
    agent-3: 83.63666666666683
    agent-4: 83.63666666666683
    agent-5: 83.63666666666683
  policy_reward_min:
    agent-0: 20.833333333333346
    agent-1: 20.833333333333346
    agent-2: 20.833333333333346
    agent-3: 20.833333333333346
    agent-4: 20.833333333333346
    agent-5: 20.833333333333346
  sampler_perf:
    mean_env_wait_ms: 23.074535531383148
    mean_inference_ms: 12.362630694179536
    mean_processing_ms: 51.152271267894214
  time_since_restore: 6287.489276170731
  time_this_iter_s: 124.53275632858276
  time_total_s: 9498.552962303162
  timestamp: 1637023801
  timesteps_since_restore: 4128000
  timesteps_this_iter: 96000
  timesteps_total: 6048000
  training_iteration: 63
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     63 |          9498.55 | 6048000 |   501.82 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 59
    apples_agent-0_mean: 8.18
    apples_agent-0_min: 0
    apples_agent-1_max: 72
    apples_agent-1_mean: 14.38
    apples_agent-1_min: 0
    apples_agent-2_max: 126
    apples_agent-2_mean: 18.51
    apples_agent-2_min: 0
    apples_agent-3_max: 195
    apples_agent-3_mean: 92.55
    apples_agent-3_min: 24
    apples_agent-4_max: 85
    apples_agent-4_mean: 5.99
    apples_agent-4_min: 0
    apples_agent-5_max: 116
    apples_agent-5_mean: 68.33
    apples_agent-5_min: 21
    cleaning_beam_agent-0_max: 421
    cleaning_beam_agent-0_mean: 293.69
    cleaning_beam_agent-0_min: 195
    cleaning_beam_agent-1_max: 643
    cleaning_beam_agent-1_mean: 375.85
    cleaning_beam_agent-1_min: 53
    cleaning_beam_agent-2_max: 459
    cleaning_beam_agent-2_mean: 321.73
    cleaning_beam_agent-2_min: 121
    cleaning_beam_agent-3_max: 122
    cleaning_beam_agent-3_mean: 65.46
    cleaning_beam_agent-3_min: 23
    cleaning_beam_agent-4_max: 512
    cleaning_beam_agent-4_mean: 358.62
    cleaning_beam_agent-4_min: 109
    cleaning_beam_agent-5_max: 139
    cleaning_beam_agent-5_mean: 52.21
    cleaning_beam_agent-5_min: 19
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.09
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.1
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_19-52-07
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 719.9999999999814
  episode_reward_mean: 514.5100000000033
  episode_reward_min: 129.00000000000122
  episodes_this_iter: 96
  episodes_total: 6144
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20198.302
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008826047996990383
        entropy: 1.4249597787857056
        entropy_coeff: 0.0017600000137463212
        kl: 0.011550539173185825
        model: {}
        policy_loss: -0.020907096564769745
        total_loss: -0.021214235574007034
        vf_explained_var: 0.10108679533004761
        vf_loss: 10.457388877868652
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008826047996990383
        entropy: 1.2377227544784546
        entropy_coeff: 0.0017600000137463212
        kl: 0.014491192996501923
        model: {}
        policy_loss: -0.030574513599276543
        total_loss: -0.030166534706950188
        vf_explained_var: 0.02320176362991333
        vf_loss: 11.372533798217773
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008826047996990383
        entropy: 1.303977370262146
        entropy_coeff: 0.0017600000137463212
        kl: 0.015077756717801094
        model: {}
        policy_loss: -0.026447154581546783
        total_loss: -0.02616460621356964
        vf_explained_var: 0.07992392778396606
        vf_loss: 10.697771072387695
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008826047996990383
        entropy: 1.0801465511322021
        entropy_coeff: 0.0017600000137463212
        kl: 0.01257266104221344
        model: {}
        policy_loss: -0.020974207669496536
        total_loss: -0.020664406940340996
        vf_explained_var: 0.18050391972064972
        vf_loss: 9.535945892333984
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008826047996990383
        entropy: 1.178970456123352
        entropy_coeff: 0.0017600000137463212
        kl: 0.015577762387692928
        model: {}
        policy_loss: -0.03176743909716606
        total_loss: -0.031279079616069794
        vf_explained_var: 0.13523158431053162
        vf_loss: 10.055689811706543
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008826047996990383
        entropy: 1.221928596496582
        entropy_coeff: 0.0017600000137463212
        kl: 0.013959730975329876
        model: {}
        policy_loss: -0.0325174517929554
        total_loss: -0.0322803370654583
        vf_explained_var: 0.1470165103673935
        vf_loss: 9.9173583984375
    load_time_ms: 15355.287
    num_steps_sampled: 6144000
    num_steps_trained: 6144000
    sample_time_ms: 89581.679
    update_time_ms: 23.368
  iterations_since_restore: 44
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.057777777777776
    ram_util_percent: 9.638888888888891
  pid: 4061
  policy_reward_max:
    agent-0: 120.00000000000054
    agent-1: 120.00000000000054
    agent-2: 120.00000000000054
    agent-3: 120.00000000000054
    agent-4: 120.00000000000054
    agent-5: 120.00000000000054
  policy_reward_mean:
    agent-0: 85.75166666666681
    agent-1: 85.75166666666681
    agent-2: 85.75166666666681
    agent-3: 85.75166666666681
    agent-4: 85.75166666666681
    agent-5: 85.75166666666681
  policy_reward_min:
    agent-0: 21.500000000000014
    agent-1: 21.500000000000014
    agent-2: 21.500000000000014
    agent-3: 21.500000000000014
    agent-4: 21.500000000000014
    agent-5: 21.500000000000014
  sampler_perf:
    mean_env_wait_ms: 23.09591874608788
    mean_inference_ms: 12.357187679469826
    mean_processing_ms: 51.127653851543336
  time_since_restore: 6413.741598367691
  time_this_iter_s: 126.25232219696045
  time_total_s: 9624.805284500122
  timestamp: 1637023927
  timesteps_since_restore: 4224000
  timesteps_this_iter: 96000
  timesteps_total: 6144000
  training_iteration: 64
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     64 |          9624.81 | 6144000 |   514.51 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 43
    apples_agent-0_mean: 5.74
    apples_agent-0_min: 0
    apples_agent-1_max: 114
    apples_agent-1_mean: 19.15
    apples_agent-1_min: 0
    apples_agent-2_max: 133
    apples_agent-2_mean: 11.54
    apples_agent-2_min: 0
    apples_agent-3_max: 214
    apples_agent-3_mean: 104.98
    apples_agent-3_min: 28
    apples_agent-4_max: 53
    apples_agent-4_mean: 3.84
    apples_agent-4_min: 0
    apples_agent-5_max: 122
    apples_agent-5_mean: 73.84
    apples_agent-5_min: 15
    cleaning_beam_agent-0_max: 395
    cleaning_beam_agent-0_mean: 290.04
    cleaning_beam_agent-0_min: 178
    cleaning_beam_agent-1_max: 592
    cleaning_beam_agent-1_mean: 359.67
    cleaning_beam_agent-1_min: 64
    cleaning_beam_agent-2_max: 519
    cleaning_beam_agent-2_mean: 370.25
    cleaning_beam_agent-2_min: 177
    cleaning_beam_agent-3_max: 127
    cleaning_beam_agent-3_mean: 61.74
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 639
    cleaning_beam_agent-4_mean: 383.42
    cleaning_beam_agent-4_min: 138
    cleaning_beam_agent-5_max: 151
    cleaning_beam_agent-5_mean: 47.49
    cleaning_beam_agent-5_min: 20
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_19-54-10
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 752.9999999999902
  episode_reward_mean: 533.6700000000037
  episode_reward_min: 226.99999999999636
  episodes_this_iter: 96
  episodes_total: 6240
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20164.536
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008766144164837897
        entropy: 1.4497965574264526
        entropy_coeff: 0.0017600000137463212
        kl: 0.011742867529392242
        model: {}
        policy_loss: -0.020877931267023087
        total_loss: -0.02109561488032341
        vf_explained_var: 0.06366336345672607
        vf_loss: 11.596735000610352
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008766144164837897
        entropy: 1.253909945487976
        entropy_coeff: 0.0017600000137463212
        kl: 0.019959675148129463
        model: {}
        policy_loss: -0.027870148420333862
        total_loss: -0.026889562606811523
        vf_explained_var: 0.03890605270862579
        vf_loss: 11.914976119995117
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008766144164837897
        entropy: 1.2826776504516602
        entropy_coeff: 0.0017600000137463212
        kl: 0.01469515636563301
        model: {}
        policy_loss: -0.027209429070353508
        total_loss: -0.026881573721766472
        vf_explained_var: 0.0991377979516983
        vf_loss: 11.158541679382324
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008766144164837897
        entropy: 1.070202350616455
        entropy_coeff: 0.0017600000137463212
        kl: 0.011401522904634476
        model: {}
        policy_loss: -0.021979713812470436
        total_loss: -0.021693620830774307
        vf_explained_var: 0.16928093135356903
        vf_loss: 10.294958114624023
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008766144164837897
        entropy: 1.1321825981140137
        entropy_coeff: 0.0017600000137463212
        kl: 0.013828862458467484
        model: {}
        policy_loss: -0.029756831005215645
        total_loss: -0.029226697981357574
        vf_explained_var: 0.07972067594528198
        vf_loss: 11.398871421813965
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008766144164837897
        entropy: 1.2084159851074219
        entropy_coeff: 0.0017600000137463212
        kl: 0.014669030904769897
        model: {}
        policy_loss: -0.030475938692688942
        total_loss: -0.030087025836110115
        vf_explained_var: 0.15386292338371277
        vf_loss: 10.488283157348633
    load_time_ms: 15075.881
    num_steps_sampled: 6240000
    num_steps_trained: 6240000
    sample_time_ms: 89310.821
    update_time_ms: 24.402
  iterations_since_restore: 45
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.364571428571427
    ram_util_percent: 9.639428571428573
  pid: 4061
  policy_reward_max:
    agent-0: 125.50000000000054
    agent-1: 125.50000000000054
    agent-2: 125.50000000000054
    agent-3: 125.50000000000054
    agent-4: 125.50000000000054
    agent-5: 125.50000000000054
  policy_reward_mean:
    agent-0: 88.9450000000002
    agent-1: 88.9450000000002
    agent-2: 88.9450000000002
    agent-3: 88.9450000000002
    agent-4: 88.9450000000002
    agent-5: 88.9450000000002
  policy_reward_min:
    agent-0: 37.833333333333336
    agent-1: 37.833333333333336
    agent-2: 37.833333333333336
    agent-3: 37.833333333333336
    agent-4: 37.833333333333336
    agent-5: 37.833333333333336
  sampler_perf:
    mean_env_wait_ms: 23.12255273464736
    mean_inference_ms: 12.352045853586594
    mean_processing_ms: 51.10595891075821
  time_since_restore: 6536.444118261337
  time_this_iter_s: 122.70251989364624
  time_total_s: 9747.507804393768
  timestamp: 1637024050
  timesteps_since_restore: 4320000
  timesteps_this_iter: 96000
  timesteps_total: 6240000
  training_iteration: 65
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     65 |          9747.51 | 6240000 |   533.67 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 77
    apples_agent-0_mean: 8.18
    apples_agent-0_min: 0
    apples_agent-1_max: 156
    apples_agent-1_mean: 19.49
    apples_agent-1_min: 0
    apples_agent-2_max: 162
    apples_agent-2_mean: 16.05
    apples_agent-2_min: 0
    apples_agent-3_max: 191
    apples_agent-3_mean: 108.1
    apples_agent-3_min: 44
    apples_agent-4_max: 149
    apples_agent-4_mean: 7.19
    apples_agent-4_min: 0
    apples_agent-5_max: 109
    apples_agent-5_mean: 67.6
    apples_agent-5_min: 21
    cleaning_beam_agent-0_max: 420
    cleaning_beam_agent-0_mean: 283.82
    cleaning_beam_agent-0_min: 153
    cleaning_beam_agent-1_max: 575
    cleaning_beam_agent-1_mean: 360.47
    cleaning_beam_agent-1_min: 178
    cleaning_beam_agent-2_max: 519
    cleaning_beam_agent-2_mean: 351.86
    cleaning_beam_agent-2_min: 156
    cleaning_beam_agent-3_max: 150
    cleaning_beam_agent-3_mean: 67.97
    cleaning_beam_agent-3_min: 25
    cleaning_beam_agent-4_max: 639
    cleaning_beam_agent-4_mean: 387.76
    cleaning_beam_agent-4_min: 148
    cleaning_beam_agent-5_max: 146
    cleaning_beam_agent-5_mean: 50.49
    cleaning_beam_agent-5_min: 20
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_19-56-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 746.9999999999836
  episode_reward_mean: 528.050000000004
  episode_reward_min: 264.9999999999961
  episodes_this_iter: 96
  episodes_total: 6336
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20197.086
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008706239750608802
        entropy: 1.4142286777496338
        entropy_coeff: 0.0017600000137463212
        kl: 0.011587628163397312
        model: {}
        policy_loss: -0.021232187747955322
        total_loss: -0.02151137962937355
        vf_explained_var: 0.07894870638847351
        vf_loss: 10.510885238647461
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008706239750608802
        entropy: 1.2584419250488281
        entropy_coeff: 0.0017600000137463212
        kl: 0.016529271379113197
        model: {}
        policy_loss: -0.03120335005223751
        total_loss: -0.03066365420818329
        vf_explained_var: 0.03536058962345123
        vf_loss: 11.016266822814941
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008706239750608802
        entropy: 1.2638108730316162
        entropy_coeff: 0.0017600000137463212
        kl: 0.014827711507678032
        model: {}
        policy_loss: -0.02569166198372841
        total_loss: -0.025339309126138687
        vf_explained_var: 0.041190892457962036
        vf_loss: 10.938929557800293
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008706239750608802
        entropy: 1.063795804977417
        entropy_coeff: 0.0017600000137463212
        kl: 0.011415989138185978
        model: {}
        policy_loss: -0.02171163819730282
        total_loss: -0.021513184532523155
        vf_explained_var: 0.18605230748653412
        vf_loss: 9.291364669799805
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008706239750608802
        entropy: 1.1187989711761475
        entropy_coeff: 0.0017600000137463212
        kl: 0.014904793351888657
        model: {}
        policy_loss: -0.02874068170785904
        total_loss: -0.02818833664059639
        vf_explained_var: 0.09700584411621094
        vf_loss: 10.309492111206055
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008706239750608802
        entropy: 1.1737215518951416
        entropy_coeff: 0.0017600000137463212
        kl: 0.013944783248007298
        model: {}
        policy_loss: -0.03167225047945976
        total_loss: -0.03139127418398857
        vf_explained_var: 0.16565771400928497
        vf_loss: 9.522467613220215
    load_time_ms: 14655.588
    num_steps_sampled: 6336000
    num_steps_trained: 6336000
    sample_time_ms: 89366.926
    update_time_ms: 23.3
  iterations_since_restore: 46
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.288068181818183
    ram_util_percent: 9.601704545454545
  pid: 4061
  policy_reward_max:
    agent-0: 124.50000000000067
    agent-1: 124.50000000000067
    agent-2: 124.50000000000067
    agent-3: 124.50000000000067
    agent-4: 124.50000000000067
    agent-5: 124.50000000000067
  policy_reward_mean:
    agent-0: 88.00833333333354
    agent-1: 88.00833333333354
    agent-2: 88.00833333333354
    agent-3: 88.00833333333354
    agent-4: 88.00833333333354
    agent-5: 88.00833333333354
  policy_reward_min:
    agent-0: 44.16666666666655
    agent-1: 44.16666666666655
    agent-2: 44.16666666666655
    agent-3: 44.16666666666655
    agent-4: 44.16666666666655
    agent-5: 44.16666666666655
  sampler_perf:
    mean_env_wait_ms: 23.144315263679964
    mean_inference_ms: 12.361890971264701
    mean_processing_ms: 51.07810690892852
  time_since_restore: 6660.062817335129
  time_this_iter_s: 123.6186990737915
  time_total_s: 9871.12650346756
  timestamp: 1637024174
  timesteps_since_restore: 4416000
  timesteps_this_iter: 96000
  timesteps_total: 6336000
  training_iteration: 66
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 17.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     66 |          9871.13 | 6336000 |   528.05 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 43
    apples_agent-0_mean: 7.63
    apples_agent-0_min: 0
    apples_agent-1_max: 102
    apples_agent-1_mean: 16.56
    apples_agent-1_min: 0
    apples_agent-2_max: 199
    apples_agent-2_mean: 15.55
    apples_agent-2_min: 0
    apples_agent-3_max: 203
    apples_agent-3_mean: 102.4
    apples_agent-3_min: 31
    apples_agent-4_max: 84
    apples_agent-4_mean: 6.32
    apples_agent-4_min: 0
    apples_agent-5_max: 96
    apples_agent-5_mean: 63.48
    apples_agent-5_min: 22
    cleaning_beam_agent-0_max: 397
    cleaning_beam_agent-0_mean: 284.91
    cleaning_beam_agent-0_min: 166
    cleaning_beam_agent-1_max: 519
    cleaning_beam_agent-1_mean: 350.31
    cleaning_beam_agent-1_min: 208
    cleaning_beam_agent-2_max: 519
    cleaning_beam_agent-2_mean: 325.85
    cleaning_beam_agent-2_min: 70
    cleaning_beam_agent-3_max: 146
    cleaning_beam_agent-3_mean: 70.77
    cleaning_beam_agent-3_min: 24
    cleaning_beam_agent-4_max: 542
    cleaning_beam_agent-4_mean: 347.41
    cleaning_beam_agent-4_min: 171
    cleaning_beam_agent-5_max: 152
    cleaning_beam_agent-5_mean: 46.91
    cleaning_beam_agent-5_min: 12
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.04
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_19-58-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 751.9999999999936
  episode_reward_mean: 527.0100000000047
  episode_reward_min: 266.9999999999971
  episodes_this_iter: 96
  episodes_total: 6432
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20189.044
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008646335918456316
        entropy: 1.3844735622406006
        entropy_coeff: 0.0017600000137463212
        kl: 0.01169094629585743
        model: {}
        policy_loss: -0.020340247079730034
        total_loss: -0.02055119164288044
        vf_explained_var: 0.09930086135864258
        vf_loss: 10.56633186340332
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008646335918456316
        entropy: 1.2699960470199585
        entropy_coeff: 0.0017600000137463212
        kl: 0.015447797253727913
        model: {}
        policy_loss: -0.03203883022069931
        total_loss: -0.03158816322684288
        vf_explained_var: 0.027119576930999756
        vf_loss: 11.410845756530762
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008646335918456316
        entropy: 1.2700473070144653
        entropy_coeff: 0.0017600000137463212
        kl: 0.014559134840965271
        model: {}
        policy_loss: -0.026151061058044434
        total_loss: -0.02585289254784584
        vf_explained_var: 0.08102254569530487
        vf_loss: 10.775386810302734
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008646335918456316
        entropy: 1.0492360591888428
        entropy_coeff: 0.0017600000137463212
        kl: 0.012349363416433334
        model: {}
        policy_loss: -0.023942409083247185
        total_loss: -0.02358892560005188
        vf_explained_var: 0.1768093705177307
        vf_loss: 9.65206527709961
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008646335918456316
        entropy: 1.141934871673584
        entropy_coeff: 0.0017600000137463212
        kl: 0.014941016212105751
        model: {}
        policy_loss: -0.029533278197050095
        total_loss: -0.028962545096874237
        vf_explained_var: 0.0745302140712738
        vf_loss: 10.864368438720703
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008646335918456316
        entropy: 1.1294102668762207
        entropy_coeff: 0.0017600000137463212
        kl: 0.013716993853449821
        model: {}
        policy_loss: -0.03022681549191475
        total_loss: -0.02985793724656105
        vf_explained_var: 0.1608172506093979
        vf_loss: 9.849386215209961
    load_time_ms: 14639.195
    num_steps_sampled: 6432000
    num_steps_trained: 6432000
    sample_time_ms: 89425.663
    update_time_ms: 23.813
  iterations_since_restore: 47
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.314606741573034
    ram_util_percent: 9.617977528089888
  pid: 4061
  policy_reward_max:
    agent-0: 125.33333333333385
    agent-1: 125.33333333333385
    agent-2: 125.33333333333385
    agent-3: 125.33333333333385
    agent-4: 125.33333333333385
    agent-5: 125.33333333333385
  policy_reward_mean:
    agent-0: 87.83500000000012
    agent-1: 87.83500000000012
    agent-2: 87.83500000000012
    agent-3: 87.83500000000012
    agent-4: 87.83500000000012
    agent-5: 87.83500000000012
  policy_reward_min:
    agent-0: 44.499999999999936
    agent-1: 44.499999999999936
    agent-2: 44.499999999999936
    agent-3: 44.499999999999936
    agent-4: 44.499999999999936
    agent-5: 44.499999999999936
  sampler_perf:
    mean_env_wait_ms: 23.16294393277841
    mean_inference_ms: 12.360336294208283
    mean_processing_ms: 51.065098537047824
  time_since_restore: 6785.216152906418
  time_this_iter_s: 125.15333557128906
  time_total_s: 9996.279839038849
  timestamp: 1637024299
  timesteps_since_restore: 4512000
  timesteps_this_iter: 96000
  timesteps_total: 6432000
  training_iteration: 67
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 17.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     67 |          9996.28 | 6432000 |   527.01 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 37
    apples_agent-0_mean: 4.95
    apples_agent-0_min: 0
    apples_agent-1_max: 93
    apples_agent-1_mean: 13.13
    apples_agent-1_min: 0
    apples_agent-2_max: 262
    apples_agent-2_mean: 18.68
    apples_agent-2_min: 0
    apples_agent-3_max: 260
    apples_agent-3_mean: 103.98
    apples_agent-3_min: 38
    apples_agent-4_max: 85
    apples_agent-4_mean: 5.55
    apples_agent-4_min: 0
    apples_agent-5_max: 105
    apples_agent-5_mean: 67.13
    apples_agent-5_min: 13
    cleaning_beam_agent-0_max: 426
    cleaning_beam_agent-0_mean: 292.82
    cleaning_beam_agent-0_min: 169
    cleaning_beam_agent-1_max: 559
    cleaning_beam_agent-1_mean: 333.97
    cleaning_beam_agent-1_min: 165
    cleaning_beam_agent-2_max: 476
    cleaning_beam_agent-2_mean: 309.23
    cleaning_beam_agent-2_min: 76
    cleaning_beam_agent-3_max: 194
    cleaning_beam_agent-3_mean: 67.86
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 483
    cleaning_beam_agent-4_mean: 327.67
    cleaning_beam_agent-4_min: 128
    cleaning_beam_agent-5_max: 105
    cleaning_beam_agent-5_mean: 39.26
    cleaning_beam_agent-5_min: 14
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-00-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 794.9999999999885
  episode_reward_mean: 531.0400000000044
  episode_reward_min: 241.99999999999676
  episodes_this_iter: 96
  episodes_total: 6528
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20197.985
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000858643208630383
        entropy: 1.3923299312591553
        entropy_coeff: 0.0017600000137463212
        kl: 0.012026576325297356
        model: {}
        policy_loss: -0.022282741963863373
        total_loss: -0.02239277958869934
        vf_explained_var: 0.07224974036216736
        vf_loss: 11.378069877624512
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000858643208630383
        entropy: 1.2836319208145142
        entropy_coeff: 0.0017600000137463212
        kl: 0.015295208431780338
        model: {}
        policy_loss: -0.032889146357774734
        total_loss: -0.03247765079140663
        vf_explained_var: 0.06888334453105927
        vf_loss: 11.411705017089844
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000858643208630383
        entropy: 1.2456563711166382
        entropy_coeff: 0.0017600000137463212
        kl: 0.014893271960318089
        model: {}
        policy_loss: -0.02637367695569992
        total_loss: -0.025949541479349136
        vf_explained_var: 0.08044648170471191
        vf_loss: 11.27163314819336
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000858643208630383
        entropy: 1.0332330465316772
        entropy_coeff: 0.0017600000137463212
        kl: 0.01184934563934803
        model: {}
        policy_loss: -0.023476574569940567
        total_loss: -0.02311583049595356
        vf_explained_var: 0.1889505684375763
        vf_loss: 9.943009376525879
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000858643208630383
        entropy: 1.1443496942520142
        entropy_coeff: 0.0017600000137463212
        kl: 0.014355799183249474
        model: {}
        policy_loss: -0.03023245558142662
        total_loss: -0.02972213178873062
        vf_explained_var: 0.11129532754421234
        vf_loss: 10.888039588928223
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000858643208630383
        entropy: 1.1449847221374512
        entropy_coeff: 0.0017600000137463212
        kl: 0.015870045870542526
        model: {}
        policy_loss: -0.03066347911953926
        total_loss: -0.030084438621997833
        vf_explained_var: 0.17849525809288025
        vf_loss: 10.072084426879883
    load_time_ms: 14432.314
    num_steps_sampled: 6528000
    num_steps_trained: 6528000
    sample_time_ms: 89344.879
    update_time_ms: 22.95
  iterations_since_restore: 48
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 16.53728813559322
    ram_util_percent: 9.699435028248587
  pid: 4061
  policy_reward_max:
    agent-0: 132.5000000000005
    agent-1: 132.5000000000005
    agent-2: 132.5000000000005
    agent-3: 132.5000000000005
    agent-4: 132.5000000000005
    agent-5: 132.5000000000005
  policy_reward_mean:
    agent-0: 88.50666666666683
    agent-1: 88.50666666666683
    agent-2: 88.50666666666683
    agent-3: 88.50666666666683
    agent-4: 88.50666666666683
    agent-5: 88.50666666666683
  policy_reward_min:
    agent-0: 40.3333333333333
    agent-1: 40.3333333333333
    agent-2: 40.3333333333333
    agent-3: 40.3333333333333
    agent-4: 40.3333333333333
    agent-5: 40.3333333333333
  sampler_perf:
    mean_env_wait_ms: 23.17721452037074
    mean_inference_ms: 12.356354781010152
    mean_processing_ms: 51.04968256557531
  time_since_restore: 6909.087160110474
  time_this_iter_s: 123.87100720405579
  time_total_s: 10120.150846242905
  timestamp: 1637024423
  timesteps_since_restore: 4608000
  timesteps_this_iter: 96000
  timesteps_total: 6528000
  training_iteration: 68
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     68 |          10120.2 | 6528000 |   531.04 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 48
    apples_agent-0_mean: 5.0
    apples_agent-0_min: 0
    apples_agent-1_max: 168
    apples_agent-1_mean: 19.28
    apples_agent-1_min: 0
    apples_agent-2_max: 139
    apples_agent-2_mean: 13.88
    apples_agent-2_min: 0
    apples_agent-3_max: 183
    apples_agent-3_mean: 101.76
    apples_agent-3_min: 31
    apples_agent-4_max: 95
    apples_agent-4_mean: 6.91
    apples_agent-4_min: 0
    apples_agent-5_max: 102
    apples_agent-5_mean: 62.59
    apples_agent-5_min: 13
    cleaning_beam_agent-0_max: 546
    cleaning_beam_agent-0_mean: 317.61
    cleaning_beam_agent-0_min: 184
    cleaning_beam_agent-1_max: 483
    cleaning_beam_agent-1_mean: 315.25
    cleaning_beam_agent-1_min: 160
    cleaning_beam_agent-2_max: 507
    cleaning_beam_agent-2_mean: 324.41
    cleaning_beam_agent-2_min: 95
    cleaning_beam_agent-3_max: 141
    cleaning_beam_agent-3_mean: 60.64
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 558
    cleaning_beam_agent-4_mean: 323.73
    cleaning_beam_agent-4_min: 125
    cleaning_beam_agent-5_max: 161
    cleaning_beam_agent-5_mean: 44.92
    cleaning_beam_agent-5_min: 12
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.08
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-02-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 726.9999999999973
  episode_reward_mean: 534.2200000000036
  episode_reward_min: 265.9999999999976
  episodes_this_iter: 96
  episodes_total: 6624
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20213.205
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008526528254151344
        entropy: 1.3651618957519531
        entropy_coeff: 0.0017600000137463212
        kl: 0.012107627466320992
        model: {}
        policy_loss: -0.022191956639289856
        total_loss: -0.02231064811348915
        vf_explained_var: 0.07461662590503693
        vf_loss: 10.732309341430664
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008526528254151344
        entropy: 1.300155758857727
        entropy_coeff: 0.0017600000137463212
        kl: 0.016409857198596
        model: {}
        policy_loss: -0.033973708748817444
        total_loss: -0.033500172197818756
        vf_explained_var: 0.033884331583976746
        vf_loss: 11.208250045776367
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008526528254151344
        entropy: 1.276568055152893
        entropy_coeff: 0.0017600000137463212
        kl: 0.017080381512641907
        model: {}
        policy_loss: -0.024702558293938637
        total_loss: -0.02416030317544937
        vf_explained_var: 0.06797812879085541
        vf_loss: 10.80977725982666
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008526528254151344
        entropy: 1.0082814693450928
        entropy_coeff: 0.0017600000137463212
        kl: 0.010938928462564945
        model: {}
        policy_loss: -0.021568140015006065
        total_loss: -0.02127847447991371
        vf_explained_var: 0.16307273507118225
        vf_loss: 9.703499794006348
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008526528254151344
        entropy: 1.1440602540969849
        entropy_coeff: 0.0017600000137463212
        kl: 0.01531311683356762
        model: {}
        policy_loss: -0.030328551307320595
        total_loss: -0.02975766360759735
        vf_explained_var: 0.09204784035682678
        vf_loss: 10.531238555908203
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008526528254151344
        entropy: 1.1371675729751587
        entropy_coeff: 0.0017600000137463212
        kl: 0.015523415058851242
        model: {}
        policy_loss: -0.029933635145425797
        total_loss: -0.029393676668405533
        vf_explained_var: 0.14783817529678345
        vf_loss: 9.890349388122559
    load_time_ms: 14543.351
    num_steps_sampled: 6624000
    num_steps_trained: 6624000
    sample_time_ms: 89460.882
    update_time_ms: 22.87
  iterations_since_restore: 49
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.795555555555557
    ram_util_percent: 9.612222222222224
  pid: 4061
  policy_reward_max:
    agent-0: 121.16666666666703
    agent-1: 121.16666666666703
    agent-2: 121.16666666666703
    agent-3: 121.16666666666703
    agent-4: 121.16666666666703
    agent-5: 121.16666666666703
  policy_reward_mean:
    agent-0: 89.03666666666685
    agent-1: 89.03666666666685
    agent-2: 89.03666666666685
    agent-3: 89.03666666666685
    agent-4: 89.03666666666685
    agent-5: 89.03666666666685
  policy_reward_min:
    agent-0: 44.333333333333336
    agent-1: 44.333333333333336
    agent-2: 44.333333333333336
    agent-3: 44.333333333333336
    agent-4: 44.333333333333336
    agent-5: 44.333333333333336
  sampler_perf:
    mean_env_wait_ms: 23.198404578793248
    mean_inference_ms: 12.355434552532607
    mean_processing_ms: 51.0467864525235
  time_since_restore: 7035.201948404312
  time_this_iter_s: 126.1147882938385
  time_total_s: 10246.265634536743
  timestamp: 1637024549
  timesteps_since_restore: 4704000
  timesteps_this_iter: 96000
  timesteps_total: 6624000
  training_iteration: 69
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     69 |          10246.3 | 6624000 |   534.22 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 48
    apples_agent-0_mean: 2.91
    apples_agent-0_min: 0
    apples_agent-1_max: 66
    apples_agent-1_mean: 15.12
    apples_agent-1_min: 0
    apples_agent-2_max: 139
    apples_agent-2_mean: 13.87
    apples_agent-2_min: 0
    apples_agent-3_max: 217
    apples_agent-3_mean: 103.04
    apples_agent-3_min: 31
    apples_agent-4_max: 91
    apples_agent-4_mean: 8.08
    apples_agent-4_min: 0
    apples_agent-5_max: 108
    apples_agent-5_mean: 65.49
    apples_agent-5_min: 24
    cleaning_beam_agent-0_max: 440
    cleaning_beam_agent-0_mean: 323.29
    cleaning_beam_agent-0_min: 184
    cleaning_beam_agent-1_max: 523
    cleaning_beam_agent-1_mean: 335.84
    cleaning_beam_agent-1_min: 138
    cleaning_beam_agent-2_max: 487
    cleaning_beam_agent-2_mean: 317.11
    cleaning_beam_agent-2_min: 95
    cleaning_beam_agent-3_max: 139
    cleaning_beam_agent-3_mean: 61.53
    cleaning_beam_agent-3_min: 25
    cleaning_beam_agent-4_max: 587
    cleaning_beam_agent-4_mean: 350.79
    cleaning_beam_agent-4_min: 138
    cleaning_beam_agent-5_max: 133
    cleaning_beam_agent-5_mean: 49.45
    cleaning_beam_agent-5_min: 17
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-04-33
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 769.9999999999814
  episode_reward_mean: 543.9500000000037
  episode_reward_min: 249.99999999999798
  episodes_this_iter: 96
  episodes_total: 6720
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20204.899
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008466623839922249
        entropy: 1.359302043914795
        entropy_coeff: 0.0017600000137463212
        kl: 0.013014555908739567
        model: {}
        policy_loss: -0.023098688572645187
        total_loss: -0.023016199469566345
        vf_explained_var: 0.07055552303791046
        vf_loss: 11.734068870544434
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008466623839922249
        entropy: 1.282879114151001
        entropy_coeff: 0.0017600000137463212
        kl: 0.01672692410647869
        model: {}
        policy_loss: -0.03263234347105026
        total_loss: -0.032006267458200455
        vf_explained_var: 0.041303008794784546
        vf_loss: 12.112548828125
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008466623839922249
        entropy: 1.2903046607971191
        entropy_coeff: 0.0017600000137463212
        kl: 0.015825534239411354
        model: {}
        policy_loss: -0.027468286454677582
        total_loss: -0.027008652687072754
        vf_explained_var: 0.0902319997549057
        vf_loss: 11.480165481567383
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008466623839922249
        entropy: 1.0096349716186523
        entropy_coeff: 0.0017600000137463212
        kl: 0.012548593804240227
        model: {}
        policy_loss: -0.021022634580731392
        total_loss: -0.0205070860683918
        vf_explained_var: 0.17856824398040771
        vf_loss: 10.376469612121582
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008466623839922249
        entropy: 1.144845724105835
        entropy_coeff: 0.0017600000137463212
        kl: 0.015412095002830029
        model: {}
        policy_loss: -0.03142425790429115
        total_loss: -0.0308059211820364
        vf_explained_var: 0.13434576988220215
        vf_loss: 10.920585632324219
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008466623839922249
        entropy: 1.1432856321334839
        entropy_coeff: 0.0017600000137463212
        kl: 0.014337247237563133
        model: {}
        policy_loss: -0.03191593289375305
        total_loss: -0.031473495066165924
        vf_explained_var: 0.19088387489318848
        vf_loss: 10.208945274353027
    load_time_ms: 14486.882
    num_steps_sampled: 6720000
    num_steps_trained: 6720000
    sample_time_ms: 89552.134
    update_time_ms: 23.743
  iterations_since_restore: 50
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.33409090909091
    ram_util_percent: 9.659090909090908
  pid: 4061
  policy_reward_max:
    agent-0: 128.3333333333339
    agent-1: 128.3333333333339
    agent-2: 128.3333333333339
    agent-3: 128.3333333333339
    agent-4: 128.3333333333339
    agent-5: 128.3333333333339
  policy_reward_mean:
    agent-0: 90.65833333333352
    agent-1: 90.65833333333352
    agent-2: 90.65833333333352
    agent-3: 90.65833333333352
    agent-4: 90.65833333333352
    agent-5: 90.65833333333352
  policy_reward_min:
    agent-0: 41.66666666666661
    agent-1: 41.66666666666661
    agent-2: 41.66666666666661
    agent-3: 41.66666666666661
    agent-4: 41.66666666666661
    agent-5: 41.66666666666661
  sampler_perf:
    mean_env_wait_ms: 23.21531156240136
    mean_inference_ms: 12.351273539303003
    mean_processing_ms: 51.02710910376302
  time_since_restore: 7158.7212998867035
  time_this_iter_s: 123.51935148239136
  time_total_s: 10369.784986019135
  timestamp: 1637024673
  timesteps_since_restore: 4800000
  timesteps_this_iter: 96000
  timesteps_total: 6720000
  training_iteration: 70
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     70 |          10369.8 | 6720000 |   543.95 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 66
    apples_agent-0_mean: 3.89
    apples_agent-0_min: 0
    apples_agent-1_max: 104
    apples_agent-1_mean: 19.87
    apples_agent-1_min: 0
    apples_agent-2_max: 123
    apples_agent-2_mean: 11.83
    apples_agent-2_min: 0
    apples_agent-3_max: 166
    apples_agent-3_mean: 104.67
    apples_agent-3_min: 32
    apples_agent-4_max: 81
    apples_agent-4_mean: 8.15
    apples_agent-4_min: 0
    apples_agent-5_max: 163
    apples_agent-5_mean: 66.99
    apples_agent-5_min: 15
    cleaning_beam_agent-0_max: 499
    cleaning_beam_agent-0_mean: 330.85
    cleaning_beam_agent-0_min: 154
    cleaning_beam_agent-1_max: 515
    cleaning_beam_agent-1_mean: 337.98
    cleaning_beam_agent-1_min: 163
    cleaning_beam_agent-2_max: 476
    cleaning_beam_agent-2_mean: 300.75
    cleaning_beam_agent-2_min: 86
    cleaning_beam_agent-3_max: 180
    cleaning_beam_agent-3_mean: 65.3
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 484
    cleaning_beam_agent-4_mean: 325.33
    cleaning_beam_agent-4_min: 127
    cleaning_beam_agent-5_max: 124
    cleaning_beam_agent-5_mean: 54.64
    cleaning_beam_agent-5_min: 21
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-06-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 794.9999999999852
  episode_reward_mean: 528.2400000000025
  episode_reward_min: 226.99999999999682
  episodes_this_iter: 96
  episodes_total: 6816
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20173.084
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008406720007769763
        entropy: 1.3758467435836792
        entropy_coeff: 0.0017600000137463212
        kl: 0.013543104752898216
        model: {}
        policy_loss: -0.02422063983976841
        total_loss: -0.024108998477458954
        vf_explained_var: 0.063382089138031
        vf_loss: 11.788174629211426
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008406720007769763
        entropy: 1.2618213891983032
        entropy_coeff: 0.0017600000137463212
        kl: 0.01567351073026657
        model: {}
        policy_loss: -0.03322397172451019
        total_loss: -0.032689280807971954
        vf_explained_var: 0.05676867067813873
        vf_loss: 11.881465911865234
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008406720007769763
        entropy: 1.2881464958190918
        entropy_coeff: 0.0017600000137463212
        kl: 0.015529796481132507
        model: {}
        policy_loss: -0.02966153435409069
        total_loss: -0.02921198680996895
        vf_explained_var: 0.07568530738353729
        vf_loss: 11.637066841125488
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008406720007769763
        entropy: 1.0245556831359863
        entropy_coeff: 0.0017600000137463212
        kl: 0.012222925201058388
        model: {}
        policy_loss: -0.02433905005455017
        total_loss: -0.023914122954010963
        vf_explained_var: 0.201635479927063
        vf_loss: 10.058549880981445
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008406720007769763
        entropy: 1.1589018106460571
        entropy_coeff: 0.0017600000137463212
        kl: 0.015427111648023129
        model: {}
        policy_loss: -0.03356419876217842
        total_loss: -0.03299877420067787
        vf_explained_var: 0.15621152520179749
        vf_loss: 10.623822212219238
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008406720007769763
        entropy: 1.1563305854797363
        entropy_coeff: 0.0017600000137463212
        kl: 0.01433385070413351
        model: {}
        policy_loss: -0.03301938250660896
        total_loss: -0.03259019926190376
        vf_explained_var: 0.1807878464460373
        vf_loss: 10.309407234191895
    load_time_ms: 14428.659
    num_steps_sampled: 6816000
    num_steps_trained: 6816000
    sample_time_ms: 89558.84
    update_time_ms: 23.521
  iterations_since_restore: 51
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.432000000000002
    ram_util_percent: 9.656571428571429
  pid: 4061
  policy_reward_max:
    agent-0: 132.50000000000048
    agent-1: 132.50000000000048
    agent-2: 132.50000000000048
    agent-3: 132.50000000000048
    agent-4: 132.50000000000048
    agent-5: 132.50000000000048
  policy_reward_mean:
    agent-0: 88.0400000000002
    agent-1: 88.0400000000002
    agent-2: 88.0400000000002
    agent-3: 88.0400000000002
    agent-4: 88.0400000000002
    agent-5: 88.0400000000002
  policy_reward_min:
    agent-0: 37.83333333333336
    agent-1: 37.83333333333336
    agent-2: 37.83333333333336
    agent-3: 37.83333333333336
    agent-4: 37.83333333333336
    agent-5: 37.83333333333336
  sampler_perf:
    mean_env_wait_ms: 23.22844194443342
    mean_inference_ms: 12.346982832546308
    mean_processing_ms: 51.004653934364576
  time_since_restore: 7281.82462477684
  time_this_iter_s: 123.10332489013672
  time_total_s: 10492.888310909271
  timestamp: 1637024796
  timesteps_since_restore: 4896000
  timesteps_this_iter: 96000
  timesteps_total: 6816000
  training_iteration: 71
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     71 |          10492.9 | 6816000 |   528.24 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 47
    apples_agent-0_mean: 2.94
    apples_agent-0_min: 0
    apples_agent-1_max: 119
    apples_agent-1_mean: 17.42
    apples_agent-1_min: 0
    apples_agent-2_max: 76
    apples_agent-2_mean: 9.12
    apples_agent-2_min: 0
    apples_agent-3_max: 186
    apples_agent-3_mean: 102.47
    apples_agent-3_min: 27
    apples_agent-4_max: 93
    apples_agent-4_mean: 9.04
    apples_agent-4_min: 0
    apples_agent-5_max: 104
    apples_agent-5_mean: 63.53
    apples_agent-5_min: 17
    cleaning_beam_agent-0_max: 499
    cleaning_beam_agent-0_mean: 328.44
    cleaning_beam_agent-0_min: 148
    cleaning_beam_agent-1_max: 459
    cleaning_beam_agent-1_mean: 307.71
    cleaning_beam_agent-1_min: 141
    cleaning_beam_agent-2_max: 504
    cleaning_beam_agent-2_mean: 320.41
    cleaning_beam_agent-2_min: 111
    cleaning_beam_agent-3_max: 121
    cleaning_beam_agent-3_mean: 65.54
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 518
    cleaning_beam_agent-4_mean: 330.62
    cleaning_beam_agent-4_min: 129
    cleaning_beam_agent-5_max: 124
    cleaning_beam_agent-5_mean: 50.49
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.07
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-08-39
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 766.9999999999928
  episode_reward_mean: 535.7000000000037
  episode_reward_min: 202.9999999999984
  episodes_this_iter: 96
  episodes_total: 6912
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20167.46
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008346816175617278
        entropy: 1.372877597808838
        entropy_coeff: 0.0017600000137463212
        kl: 0.011608957313001156
        model: {}
        policy_loss: -0.02413472905755043
        total_loss: -0.024267572909593582
        vf_explained_var: 0.08677530288696289
        vf_loss: 11.225211143493652
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008346816175617278
        entropy: 1.2630921602249146
        entropy_coeff: 0.0017600000137463212
        kl: 0.01582406274974346
        model: {}
        policy_loss: -0.031947579234838486
        total_loss: -0.031443241983652115
        vf_explained_var: 0.06803163886070251
        vf_loss: 11.449714660644531
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008346816175617278
        entropy: 1.299330711364746
        entropy_coeff: 0.0017600000137463212
        kl: 0.018478114157915115
        model: {}
        policy_loss: -0.02630976215004921
        total_loss: -0.02563367784023285
        vf_explained_var: 0.09225422143936157
        vf_loss: 11.150968551635742
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008346816175617278
        entropy: 1.0361452102661133
        entropy_coeff: 0.0017600000137463212
        kl: 0.012174423784017563
        model: {}
        policy_loss: -0.025066222995519638
        total_loss: -0.024665575474500656
        vf_explained_var: 0.1821008324623108
        vf_loss: 10.068217277526855
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008346816175617278
        entropy: 1.1446987390518188
        entropy_coeff: 0.0017600000137463212
        kl: 0.01571717858314514
        model: {}
        policy_loss: -0.033831726759672165
        total_loss: -0.033190831542015076
        vf_explained_var: 0.11808806657791138
        vf_loss: 10.838457107543945
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008346816175617278
        entropy: 1.1060283184051514
        entropy_coeff: 0.0017600000137463212
        kl: 0.0147069301456213
        model: {}
        policy_loss: -0.03163887560367584
        total_loss: -0.031108595430850983
        vf_explained_var: 0.18083438277244568
        vf_loss: 10.061973571777344
    load_time_ms: 14360.616
    num_steps_sampled: 6912000
    num_steps_trained: 6912000
    sample_time_ms: 89519.541
    update_time_ms: 23.177
  iterations_since_restore: 52
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.268181818181818
    ram_util_percent: 9.592045454545456
  pid: 4061
  policy_reward_max:
    agent-0: 127.83333333333402
    agent-1: 127.83333333333402
    agent-2: 127.83333333333402
    agent-3: 127.83333333333402
    agent-4: 127.83333333333402
    agent-5: 127.83333333333402
  policy_reward_mean:
    agent-0: 89.2833333333335
    agent-1: 89.2833333333335
    agent-2: 89.2833333333335
    agent-3: 89.2833333333335
    agent-4: 89.2833333333335
    agent-5: 89.2833333333335
  policy_reward_min:
    agent-0: 33.83333333333334
    agent-1: 33.83333333333334
    agent-2: 33.83333333333334
    agent-3: 33.83333333333334
    agent-4: 33.83333333333334
    agent-5: 33.83333333333334
  sampler_perf:
    mean_env_wait_ms: 23.241920005143914
    mean_inference_ms: 12.343409465955506
    mean_processing_ms: 50.98475165947193
  time_since_restore: 7404.757656574249
  time_this_iter_s: 122.93303179740906
  time_total_s: 10615.82134270668
  timestamp: 1637024919
  timesteps_since_restore: 4992000
  timesteps_this_iter: 96000
  timesteps_total: 6912000
  training_iteration: 72
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     72 |          10615.8 | 6912000 |    535.7 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 72
    apples_agent-0_mean: 5.23
    apples_agent-0_min: 0
    apples_agent-1_max: 85
    apples_agent-1_mean: 18.97
    apples_agent-1_min: 0
    apples_agent-2_max: 84
    apples_agent-2_mean: 9.74
    apples_agent-2_min: 0
    apples_agent-3_max: 193
    apples_agent-3_mean: 107.2
    apples_agent-3_min: 9
    apples_agent-4_max: 89
    apples_agent-4_mean: 5.88
    apples_agent-4_min: 0
    apples_agent-5_max: 111
    apples_agent-5_mean: 66.13
    apples_agent-5_min: 12
    cleaning_beam_agent-0_max: 548
    cleaning_beam_agent-0_mean: 308.82
    cleaning_beam_agent-0_min: 108
    cleaning_beam_agent-1_max: 518
    cleaning_beam_agent-1_mean: 299.3
    cleaning_beam_agent-1_min: 140
    cleaning_beam_agent-2_max: 525
    cleaning_beam_agent-2_mean: 359.92
    cleaning_beam_agent-2_min: 114
    cleaning_beam_agent-3_max: 122
    cleaning_beam_agent-3_mean: 64.21
    cleaning_beam_agent-3_min: 23
    cleaning_beam_agent-4_max: 458
    cleaning_beam_agent-4_mean: 321.31
    cleaning_beam_agent-4_min: 143
    cleaning_beam_agent-5_max: 176
    cleaning_beam_agent-5_mean: 50.2
    cleaning_beam_agent-5_min: 12
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.05
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-10-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 725.9999999999827
  episode_reward_mean: 536.9200000000029
  episode_reward_min: 202.9999999999984
  episodes_this_iter: 96
  episodes_total: 7008
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20173.753
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008286911761388183
        entropy: 1.3956528902053833
        entropy_coeff: 0.0017600000137463212
        kl: 0.0125966165214777
        model: {}
        policy_loss: -0.025431470945477486
        total_loss: -0.02544524520635605
        vf_explained_var: 0.07954436540603638
        vf_loss: 11.829147338867188
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008286911761388183
        entropy: 1.2599653005599976
        entropy_coeff: 0.0017600000137463212
        kl: 0.015769483521580696
        model: {}
        policy_loss: -0.03278880566358566
        total_loss: -0.03221850097179413
        vf_explained_var: 0.05821433663368225
        vf_loss: 12.108898162841797
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008286911761388183
        entropy: 1.2486848831176758
        entropy_coeff: 0.0017600000137463212
        kl: 0.014716077595949173
        model: {}
        policy_loss: -0.026902493089437485
        total_loss: -0.02643594518303871
        vf_explained_var: 0.07186838984489441
        vf_loss: 11.926265716552734
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008286911761388183
        entropy: 1.0031194686889648
        entropy_coeff: 0.0017600000137463212
        kl: 0.012377235107123852
        model: {}
        policy_loss: -0.023594915866851807
        total_loss: -0.023111294955015182
        vf_explained_var: 0.21274201571941376
        vf_loss: 10.113859176635742
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008286911761388183
        entropy: 1.1649188995361328
        entropy_coeff: 0.0017600000137463212
        kl: 0.016774391755461693
        model: {}
        policy_loss: -0.03423047065734863
        total_loss: -0.033452022820711136
        vf_explained_var: 0.10373285412788391
        vf_loss: 11.512648582458496
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008286911761388183
        entropy: 1.1223870515823364
        entropy_coeff: 0.0017600000137463212
        kl: 0.015746472403407097
        model: {}
        policy_loss: -0.03306225314736366
        total_loss: -0.03237408772110939
        vf_explained_var: 0.15271201729774475
        vf_loss: 10.889184951782227
    load_time_ms: 14272.813
    num_steps_sampled: 7008000
    num_steps_trained: 7008000
    sample_time_ms: 89545.792
    update_time_ms: 23.173
  iterations_since_restore: 53
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 16.22824858757062
    ram_util_percent: 9.597740112994352
  pid: 4061
  policy_reward_max:
    agent-0: 121.00000000000045
    agent-1: 121.00000000000045
    agent-2: 121.00000000000045
    agent-3: 121.00000000000045
    agent-4: 121.00000000000045
    agent-5: 121.00000000000045
  policy_reward_mean:
    agent-0: 89.48666666666686
    agent-1: 89.48666666666686
    agent-2: 89.48666666666686
    agent-3: 89.48666666666686
    agent-4: 89.48666666666686
    agent-5: 89.48666666666686
  policy_reward_min:
    agent-0: 33.83333333333334
    agent-1: 33.83333333333334
    agent-2: 33.83333333333334
    agent-3: 33.83333333333334
    agent-4: 33.83333333333334
    agent-5: 33.83333333333334
  sampler_perf:
    mean_env_wait_ms: 23.25594791677315
    mean_inference_ms: 12.34100188081183
    mean_processing_ms: 50.97018477834922
  time_since_restore: 7528.687494277954
  time_this_iter_s: 123.92983770370483
  time_total_s: 10739.751180410385
  timestamp: 1637025043
  timesteps_since_restore: 5088000
  timesteps_this_iter: 96000
  timesteps_total: 7008000
  training_iteration: 73
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     73 |          10739.8 | 7008000 |   536.92 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 67
    apples_agent-0_mean: 6.3
    apples_agent-0_min: 0
    apples_agent-1_max: 89
    apples_agent-1_mean: 18.89
    apples_agent-1_min: 0
    apples_agent-2_max: 74
    apples_agent-2_mean: 7.96
    apples_agent-2_min: 0
    apples_agent-3_max: 184
    apples_agent-3_mean: 97.66
    apples_agent-3_min: 24
    apples_agent-4_max: 120
    apples_agent-4_mean: 6.16
    apples_agent-4_min: 0
    apples_agent-5_max: 131
    apples_agent-5_mean: 68.22
    apples_agent-5_min: 16
    cleaning_beam_agent-0_max: 464
    cleaning_beam_agent-0_mean: 311.52
    cleaning_beam_agent-0_min: 129
    cleaning_beam_agent-1_max: 542
    cleaning_beam_agent-1_mean: 310.87
    cleaning_beam_agent-1_min: 158
    cleaning_beam_agent-2_max: 522
    cleaning_beam_agent-2_mean: 355.55
    cleaning_beam_agent-2_min: 101
    cleaning_beam_agent-3_max: 144
    cleaning_beam_agent-3_mean: 59.87
    cleaning_beam_agent-3_min: 26
    cleaning_beam_agent-4_max: 481
    cleaning_beam_agent-4_mean: 320.29
    cleaning_beam_agent-4_min: 94
    cleaning_beam_agent-5_max: 176
    cleaning_beam_agent-5_mean: 53.73
    cleaning_beam_agent-5_min: 12
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-12-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 806.9999999999885
  episode_reward_mean: 544.7800000000019
  episode_reward_min: 269.9999999999971
  episodes_this_iter: 96
  episodes_total: 7104
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20172.537
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008227007929235697
        entropy: 1.4198086261749268
        entropy_coeff: 0.0017600000137463212
        kl: 0.014427943155169487
        model: {}
        policy_loss: -0.026163550093770027
        total_loss: -0.025975937023758888
        vf_explained_var: 0.10241295397281647
        vf_loss: 12.436817169189453
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008227007929235697
        entropy: 1.2501713037490845
        entropy_coeff: 0.0017600000137463212
        kl: 0.016955753788352013
        model: {}
        policy_loss: -0.03342308849096298
        total_loss: -0.0326543003320694
        vf_explained_var: 0.08059169352054596
        vf_loss: 12.735172271728516
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008227007929235697
        entropy: 1.236313819885254
        entropy_coeff: 0.0017600000137463212
        kl: 0.014156267046928406
        model: {}
        policy_loss: -0.027728725224733353
        total_loss: -0.027251388877630234
        vf_explained_var: 0.10666273534297943
        vf_loss: 12.376206398010254
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008227007929235697
        entropy: 0.9973693490028381
        entropy_coeff: 0.0017600000137463212
        kl: 0.013139945454895496
        model: {}
        policy_loss: -0.023912256583571434
        total_loss: -0.023281579837203026
        vf_explained_var: 0.22686739265918732
        vf_loss: 10.720566749572754
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008227007929235697
        entropy: 1.1623631715774536
        entropy_coeff: 0.0017600000137463212
        kl: 0.016456281766295433
        model: {}
        policy_loss: -0.033781133592128754
        total_loss: -0.032992228865623474
        vf_explained_var: 0.14180397987365723
        vf_loss: 11.890368461608887
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008227007929235697
        entropy: 1.1295721530914307
        entropy_coeff: 0.0017600000137463212
        kl: 0.015476660802960396
        model: {}
        policy_loss: -0.03402186185121536
        total_loss: -0.0333414264023304
        vf_explained_var: 0.19121383130550385
        vf_loss: 11.20813274383545
    load_time_ms: 14045.882
    num_steps_sampled: 7104000
    num_steps_trained: 7104000
    sample_time_ms: 89401.198
    update_time_ms: 22.036
  iterations_since_restore: 54
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.423563218390804
    ram_util_percent: 9.603448275862071
  pid: 4061
  policy_reward_max:
    agent-0: 134.5000000000005
    agent-1: 134.5000000000005
    agent-2: 134.5000000000005
    agent-3: 134.5000000000005
    agent-4: 134.5000000000005
    agent-5: 134.5000000000005
  policy_reward_mean:
    agent-0: 90.79666666666687
    agent-1: 90.79666666666687
    agent-2: 90.79666666666687
    agent-3: 90.79666666666687
    agent-4: 90.79666666666687
    agent-5: 90.79666666666687
  policy_reward_min:
    agent-0: 44.999999999999915
    agent-1: 44.999999999999915
    agent-2: 44.999999999999915
    agent-3: 44.999999999999915
    agent-4: 44.999999999999915
    agent-5: 44.999999999999915
  sampler_perf:
    mean_env_wait_ms: 23.26889594208066
    mean_inference_ms: 12.336976540027944
    mean_processing_ms: 50.950156647072625
  time_since_restore: 7651.18057179451
  time_this_iter_s: 122.49307751655579
  time_total_s: 10862.244257926941
  timestamp: 1637025166
  timesteps_since_restore: 5184000
  timesteps_this_iter: 96000
  timesteps_total: 7104000
  training_iteration: 74
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     74 |          10862.2 | 7104000 |   544.78 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 48
    apples_agent-0_mean: 5.23
    apples_agent-0_min: 0
    apples_agent-1_max: 120
    apples_agent-1_mean: 17.06
    apples_agent-1_min: 0
    apples_agent-2_max: 144
    apples_agent-2_mean: 6.48
    apples_agent-2_min: 0
    apples_agent-3_max: 170
    apples_agent-3_mean: 92.44
    apples_agent-3_min: 10
    apples_agent-4_max: 99
    apples_agent-4_mean: 7.21
    apples_agent-4_min: 0
    apples_agent-5_max: 151
    apples_agent-5_mean: 67.47
    apples_agent-5_min: 19
    cleaning_beam_agent-0_max: 471
    cleaning_beam_agent-0_mean: 308.45
    cleaning_beam_agent-0_min: 128
    cleaning_beam_agent-1_max: 562
    cleaning_beam_agent-1_mean: 290.63
    cleaning_beam_agent-1_min: 135
    cleaning_beam_agent-2_max: 577
    cleaning_beam_agent-2_mean: 365.33
    cleaning_beam_agent-2_min: 105
    cleaning_beam_agent-3_max: 160
    cleaning_beam_agent-3_mean: 55.72
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 498
    cleaning_beam_agent-4_mean: 324.13
    cleaning_beam_agent-4_min: 61
    cleaning_beam_agent-5_max: 131
    cleaning_beam_agent-5_mean: 51.09
    cleaning_beam_agent-5_min: 15
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-14-50
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 768.9999999999857
  episode_reward_mean: 537.3900000000007
  episode_reward_min: 110.00000000000082
  episodes_this_iter: 96
  episodes_total: 7200
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20163.466
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008167104097083211
        entropy: 1.4095547199249268
        entropy_coeff: 0.0017600000137463212
        kl: 0.012357698753476143
        model: {}
        policy_loss: -0.026422472670674324
        total_loss: -0.026376627385616302
        vf_explained_var: 0.09789952635765076
        vf_loss: 12.908927917480469
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008167104097083211
        entropy: 1.2501051425933838
        entropy_coeff: 0.0017600000137463212
        kl: 0.016112973913550377
        model: {}
        policy_loss: -0.032741568982601166
        total_loss: -0.031977586448192596
        vf_explained_var: 0.05463187396526337
        vf_loss: 13.528722763061523
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008167104097083211
        entropy: 1.2224316596984863
        entropy_coeff: 0.0017600000137463212
        kl: 0.014825754798948765
        model: {}
        policy_loss: -0.027983784675598145
        total_loss: -0.02734443172812462
        vf_explained_var: 0.08589254319667816
        vf_loss: 13.082595825195312
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008167104097083211
        entropy: 0.9750972390174866
        entropy_coeff: 0.0017600000137463212
        kl: 0.012136107310652733
        model: {}
        policy_loss: -0.024833815172314644
        total_loss: -0.024231934919953346
        vf_explained_var: 0.22785276174545288
        vf_loss: 11.044380187988281
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008167104097083211
        entropy: 1.1664445400238037
        entropy_coeff: 0.0017600000137463212
        kl: 0.016450310125947
        model: {}
        policy_loss: -0.0364544540643692
        total_loss: -0.035686615854501724
        vf_explained_var: 0.17838551104068756
        vf_loss: 11.757493019104004
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008167104097083211
        entropy: 1.1292856931686401
        entropy_coeff: 0.0017600000137463212
        kl: 0.015886005014181137
        model: {}
        policy_loss: -0.033628541976213455
        total_loss: -0.03285641968250275
        vf_explained_var: 0.18166840076446533
        vf_loss: 11.710662841796875
    load_time_ms: 14079.765
    num_steps_sampled: 7200000
    num_steps_trained: 7200000
    sample_time_ms: 89432.542
    update_time_ms: 20.903
  iterations_since_restore: 55
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.169886363636364
    ram_util_percent: 9.648863636363636
  pid: 4061
  policy_reward_max:
    agent-0: 128.16666666666734
    agent-1: 128.16666666666734
    agent-2: 128.16666666666734
    agent-3: 128.16666666666734
    agent-4: 128.16666666666734
    agent-5: 128.16666666666734
  policy_reward_mean:
    agent-0: 89.5650000000002
    agent-1: 89.5650000000002
    agent-2: 89.5650000000002
    agent-3: 89.5650000000002
    agent-4: 89.5650000000002
    agent-5: 89.5650000000002
  policy_reward_min:
    agent-0: 18.33333333333333
    agent-1: 18.33333333333333
    agent-2: 18.33333333333333
    agent-3: 18.33333333333333
    agent-4: 18.33333333333333
    agent-5: 18.33333333333333
  sampler_perf:
    mean_env_wait_ms: 23.27859567792609
    mean_inference_ms: 12.333454779420492
    mean_processing_ms: 50.930712520615096
  time_since_restore: 7774.466638326645
  time_this_iter_s: 123.28606653213501
  time_total_s: 10985.530324459076
  timestamp: 1637025290
  timesteps_since_restore: 5280000
  timesteps_this_iter: 96000
  timesteps_total: 7200000
  training_iteration: 75
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     75 |          10985.5 | 7200000 |   537.39 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 74
    apples_agent-0_mean: 5.4
    apples_agent-0_min: 0
    apples_agent-1_max: 64
    apples_agent-1_mean: 16.51
    apples_agent-1_min: 0
    apples_agent-2_max: 108
    apples_agent-2_mean: 6.73
    apples_agent-2_min: 0
    apples_agent-3_max: 168
    apples_agent-3_mean: 93.65
    apples_agent-3_min: 29
    apples_agent-4_max: 95
    apples_agent-4_mean: 6.54
    apples_agent-4_min: 0
    apples_agent-5_max: 117
    apples_agent-5_mean: 67.96
    apples_agent-5_min: 21
    cleaning_beam_agent-0_max: 440
    cleaning_beam_agent-0_mean: 321.13
    cleaning_beam_agent-0_min: 181
    cleaning_beam_agent-1_max: 456
    cleaning_beam_agent-1_mean: 285.16
    cleaning_beam_agent-1_min: 166
    cleaning_beam_agent-2_max: 643
    cleaning_beam_agent-2_mean: 388.05
    cleaning_beam_agent-2_min: 152
    cleaning_beam_agent-3_max: 130
    cleaning_beam_agent-3_mean: 54.27
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 533
    cleaning_beam_agent-4_mean: 343.14
    cleaning_beam_agent-4_min: 141
    cleaning_beam_agent-5_max: 146
    cleaning_beam_agent-5_mean: 50.98
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-16-54
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 768.9999999999857
  episode_reward_mean: 548.580000000003
  episode_reward_min: 196.99999999999747
  episodes_this_iter: 96
  episodes_total: 7296
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20127.793
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008107200264930725
        entropy: 1.4201759099960327
        entropy_coeff: 0.0017600000137463212
        kl: 0.014228147454559803
        model: {}
        policy_loss: -0.026540402323007584
        total_loss: -0.026390187442302704
        vf_explained_var: 0.0642165094614029
        vf_loss: 12.269095420837402
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008107200264930725
        entropy: 1.2562403678894043
        entropy_coeff: 0.0017600000137463212
        kl: 0.01688079535961151
        model: {}
        policy_loss: -0.03339346498250961
        total_loss: -0.03268767148256302
        vf_explained_var: 0.06217062473297119
        vf_loss: 12.286977767944336
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008107200264930725
        entropy: 1.2071919441223145
        entropy_coeff: 0.0017600000137463212
        kl: 0.015239368192851543
        model: {}
        policy_loss: -0.028291864320635796
        total_loss: -0.027710597962141037
        vf_explained_var: 0.09741105139255524
        vf_loss: 11.819854736328125
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008107200264930725
        entropy: 0.958407461643219
        entropy_coeff: 0.0017600000137463212
        kl: 0.012098964303731918
        model: {}
        policy_loss: -0.02399476058781147
        total_loss: -0.023417597636580467
        vf_explained_var: 0.19627730548381805
        vf_loss: 10.540632247924805
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008107200264930725
        entropy: 1.1483876705169678
        entropy_coeff: 0.0017600000137463212
        kl: 0.016316261142492294
        model: {}
        policy_loss: -0.037029266357421875
        total_loss: -0.036189571022987366
        vf_explained_var: 0.06103311479091644
        vf_loss: 12.292351722717285
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0008107200264930725
        entropy: 1.107383131980896
        entropy_coeff: 0.0017600000137463212
        kl: 0.01616625115275383
        model: {}
        policy_loss: -0.033497124910354614
        total_loss: -0.03273694962263107
        vf_explained_var: 0.1654786616563797
        vf_loss: 10.925445556640625
    load_time_ms: 14121.918
    num_steps_sampled: 7296000
    num_steps_trained: 7296000
    sample_time_ms: 89467.059
    update_time_ms: 21.286
  iterations_since_restore: 56
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.238418079096045
    ram_util_percent: 9.522033898305086
  pid: 4061
  policy_reward_max:
    agent-0: 128.16666666666734
    agent-1: 128.16666666666734
    agent-2: 128.16666666666734
    agent-3: 128.16666666666734
    agent-4: 128.16666666666734
    agent-5: 128.16666666666734
  policy_reward_mean:
    agent-0: 91.43000000000022
    agent-1: 91.43000000000022
    agent-2: 91.43000000000022
    agent-3: 91.43000000000022
    agent-4: 91.43000000000022
    agent-5: 91.43000000000022
  policy_reward_min:
    agent-0: 32.83333333333339
    agent-1: 32.83333333333339
    agent-2: 32.83333333333339
    agent-3: 32.83333333333339
    agent-4: 32.83333333333339
    agent-5: 32.83333333333339
  sampler_perf:
    mean_env_wait_ms: 23.292775159017616
    mean_inference_ms: 12.330616010237959
    mean_processing_ms: 50.91716113611763
  time_since_restore: 7898.43802690506
  time_this_iter_s: 123.97138857841492
  time_total_s: 11109.50171303749
  timestamp: 1637025414
  timesteps_since_restore: 5376000
  timesteps_this_iter: 96000
  timesteps_total: 7296000
  training_iteration: 76
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     76 |          11109.5 | 7296000 |   548.58 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 35
    apples_agent-0_mean: 1.98
    apples_agent-0_min: 0
    apples_agent-1_max: 74
    apples_agent-1_mean: 15.62
    apples_agent-1_min: 0
    apples_agent-2_max: 97
    apples_agent-2_mean: 8.36
    apples_agent-2_min: 0
    apples_agent-3_max: 165
    apples_agent-3_mean: 95.72
    apples_agent-3_min: 32
    apples_agent-4_max: 111
    apples_agent-4_mean: 8.77
    apples_agent-4_min: 0
    apples_agent-5_max: 119
    apples_agent-5_mean: 68.18
    apples_agent-5_min: 6
    cleaning_beam_agent-0_max: 440
    cleaning_beam_agent-0_mean: 313.02
    cleaning_beam_agent-0_min: 184
    cleaning_beam_agent-1_max: 468
    cleaning_beam_agent-1_mean: 300.85
    cleaning_beam_agent-1_min: 157
    cleaning_beam_agent-2_max: 517
    cleaning_beam_agent-2_mean: 378.57
    cleaning_beam_agent-2_min: 118
    cleaning_beam_agent-3_max: 153
    cleaning_beam_agent-3_mean: 49.67
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 556
    cleaning_beam_agent-4_mean: 340.32
    cleaning_beam_agent-4_min: 128
    cleaning_beam_agent-5_max: 215
    cleaning_beam_agent-5_mean: 62.04
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 8
    fire_beam_agent-4_mean: 0.09
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-18-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 754.9999999999964
  episode_reward_mean: 565.2100000000007
  episode_reward_min: 262.9999999999987
  episodes_this_iter: 96
  episodes_total: 7392
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20150.403
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000804729585070163
        entropy: 1.4156458377838135
        entropy_coeff: 0.0017600000137463212
        kl: 0.014000821858644485
        model: {}
        policy_loss: -0.027269871905446053
        total_loss: -0.02698243409395218
        vf_explained_var: 0.07947935163974762
        vf_loss: 13.788928031921387
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000804729585070163
        entropy: 1.2301127910614014
        entropy_coeff: 0.0017600000137463212
        kl: 0.018442334607243538
        model: {}
        policy_loss: -0.033590350300073624
        total_loss: -0.03250621259212494
        vf_explained_var: 0.06181518733501434
        vf_loss: 14.049013137817383
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000804729585070163
        entropy: 1.203158974647522
        entropy_coeff: 0.0017600000137463212
        kl: 0.015275227837264538
        model: {}
        policy_loss: -0.028384044766426086
        total_loss: -0.027604805305600166
        vf_explained_var: 0.08607639372348785
        vf_loss: 13.692808151245117
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000804729585070163
        entropy: 0.9403551816940308
        entropy_coeff: 0.0017600000137463212
        kl: 0.011662162840366364
        model: {}
        policy_loss: -0.024275824427604675
        total_loss: -0.023674707859754562
        vf_explained_var: 0.2721974849700928
        vf_loss: 10.899271965026855
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000804729585070163
        entropy: 1.1467019319534302
        entropy_coeff: 0.0017600000137463212
        kl: 0.01735643669962883
        model: {}
        policy_loss: -0.03692664951086044
        total_loss: -0.03592390567064285
        vf_explained_var: 0.14154581725597382
        vf_loss: 12.852958679199219
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000804729585070163
        entropy: 1.10542893409729
        entropy_coeff: 0.0017600000137463212
        kl: 0.015775423496961594
        model: {}
        policy_loss: -0.033870212733745575
        total_loss: -0.033042699098587036
        vf_explained_var: 0.20162449777126312
        vf_loss: 11.955286026000977
    load_time_ms: 14003.657
    num_steps_sampled: 7392000
    num_steps_trained: 7392000
    sample_time_ms: 89334.462
    update_time_ms: 20.864
  iterations_since_restore: 57
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.536571428571426
    ram_util_percent: 9.662285714285714
  pid: 4061
  policy_reward_max:
    agent-0: 125.83333333333398
    agent-1: 125.83333333333398
    agent-2: 125.83333333333398
    agent-3: 125.83333333333398
    agent-4: 125.83333333333398
    agent-5: 125.83333333333398
  policy_reward_mean:
    agent-0: 94.20166666666694
    agent-1: 94.20166666666694
    agent-2: 94.20166666666694
    agent-3: 94.20166666666694
    agent-4: 94.20166666666694
    agent-5: 94.20166666666694
  policy_reward_min:
    agent-0: 43.83333333333333
    agent-1: 43.83333333333333
    agent-2: 43.83333333333333
    agent-3: 43.83333333333333
    agent-4: 43.83333333333333
    agent-5: 43.83333333333333
  sampler_perf:
    mean_env_wait_ms: 23.305631326369976
    mean_inference_ms: 12.327032914073001
    mean_processing_ms: 50.90226345116135
  time_since_restore: 8021.355047225952
  time_this_iter_s: 122.91702032089233
  time_total_s: 11232.418733358383
  timestamp: 1637025537
  timesteps_since_restore: 5472000
  timesteps_this_iter: 96000
  timesteps_total: 7392000
  training_iteration: 77
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     77 |          11232.4 | 7392000 |   565.21 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 57
    apples_agent-0_mean: 6.07
    apples_agent-0_min: 0
    apples_agent-1_max: 61
    apples_agent-1_mean: 15.24
    apples_agent-1_min: 0
    apples_agent-2_max: 161
    apples_agent-2_mean: 11.03
    apples_agent-2_min: 0
    apples_agent-3_max: 157
    apples_agent-3_mean: 89.48
    apples_agent-3_min: 32
    apples_agent-4_max: 114
    apples_agent-4_mean: 9.16
    apples_agent-4_min: 0
    apples_agent-5_max: 120
    apples_agent-5_mean: 65.75
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 467
    cleaning_beam_agent-0_mean: 318.99
    cleaning_beam_agent-0_min: 158
    cleaning_beam_agent-1_max: 436
    cleaning_beam_agent-1_mean: 283.57
    cleaning_beam_agent-1_min: 161
    cleaning_beam_agent-2_max: 480
    cleaning_beam_agent-2_mean: 331.07
    cleaning_beam_agent-2_min: 155
    cleaning_beam_agent-3_max: 94
    cleaning_beam_agent-3_mean: 50.65
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 477
    cleaning_beam_agent-4_mean: 330.97
    cleaning_beam_agent-4_min: 100
    cleaning_beam_agent-5_max: 238
    cleaning_beam_agent-5_mean: 76.26
    cleaning_beam_agent-5_min: 24
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.07
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-21-00
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 720.9999999999894
  episode_reward_mean: 546.2900000000034
  episode_reward_min: 229.99999999999707
  episodes_this_iter: 96
  episodes_total: 7488
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20123.816
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007987392018549144
        entropy: 1.3962138891220093
        entropy_coeff: 0.0017600000137463212
        kl: 0.014676693826913834
        model: {}
        policy_loss: -0.028941074386239052
        total_loss: -0.028698785230517387
        vf_explained_var: 0.0760343074798584
        vf_loss: 12.319517135620117
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007987392018549144
        entropy: 1.2531987428665161
        entropy_coeff: 0.0017600000137463212
        kl: 0.016310561448335648
        model: {}
        policy_loss: -0.034361764788627625
        total_loss: -0.03368847072124481
        vf_explained_var: 0.06334628164768219
        vf_loss: 12.478668212890625
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007987392018549144
        entropy: 1.2397315502166748
        entropy_coeff: 0.0017600000137463212
        kl: 0.015868326649069786
        model: {}
        policy_loss: -0.0297451950609684
        total_loss: -0.029103126376867294
        vf_explained_var: 0.07185551524162292
        vf_loss: 12.37165355682373
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007987392018549144
        entropy: 0.9157414436340332
        entropy_coeff: 0.0017600000137463212
        kl: 0.011766715906560421
        model: {}
        policy_loss: -0.02464546263217926
        total_loss: -0.02403651922941208
        vf_explained_var: 0.2174326330423355
        vf_loss: 10.439823150634766
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007987392018549144
        entropy: 1.1463115215301514
        entropy_coeff: 0.0017600000137463212
        kl: 0.017072513699531555
        model: {}
        policy_loss: -0.03478831797838211
        total_loss: -0.03392603248357773
        vf_explained_var: 0.11969739198684692
        vf_loss: 11.725419998168945
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007987392018549144
        entropy: 1.1448185443878174
        entropy_coeff: 0.0017600000137463212
        kl: 0.018537793308496475
        model: {}
        policy_loss: -0.030758410692214966
        total_loss: -0.02982175350189209
        vf_explained_var: 0.17752262949943542
        vf_loss: 10.97757339477539
    load_time_ms: 13986.493
    num_steps_sampled: 7488000
    num_steps_trained: 7488000
    sample_time_ms: 89336.947
    update_time_ms: 21.099
  iterations_since_restore: 58
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.703977272727274
    ram_util_percent: 9.59375
  pid: 4061
  policy_reward_max:
    agent-0: 120.16666666666704
    agent-1: 120.16666666666704
    agent-2: 120.16666666666704
    agent-3: 120.16666666666704
    agent-4: 120.16666666666704
    agent-5: 120.16666666666704
  policy_reward_mean:
    agent-0: 91.04833333333357
    agent-1: 91.04833333333357
    agent-2: 91.04833333333357
    agent-3: 91.04833333333357
    agent-4: 91.04833333333357
    agent-5: 91.04833333333357
  policy_reward_min:
    agent-0: 38.333333333333364
    agent-1: 38.333333333333364
    agent-2: 38.333333333333364
    agent-3: 38.333333333333364
    agent-4: 38.333333333333364
    agent-5: 38.333333333333364
  sampler_perf:
    mean_env_wait_ms: 23.317349744100056
    mean_inference_ms: 12.325784011641769
    mean_processing_ms: 50.893166869262416
  time_since_restore: 8144.738215684891
  time_this_iter_s: 123.3831684589386
  time_total_s: 11355.801901817322
  timestamp: 1637025660
  timesteps_since_restore: 5568000
  timesteps_this_iter: 96000
  timesteps_total: 7488000
  training_iteration: 78
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     78 |          11355.8 | 7488000 |   546.29 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 48
    apples_agent-0_mean: 3.61
    apples_agent-0_min: 0
    apples_agent-1_max: 67
    apples_agent-1_mean: 16.85
    apples_agent-1_min: 0
    apples_agent-2_max: 187
    apples_agent-2_mean: 9.79
    apples_agent-2_min: 0
    apples_agent-3_max: 159
    apples_agent-3_mean: 87.43
    apples_agent-3_min: 42
    apples_agent-4_max: 90
    apples_agent-4_mean: 6.22
    apples_agent-4_min: 0
    apples_agent-5_max: 140
    apples_agent-5_mean: 66.25
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 475
    cleaning_beam_agent-0_mean: 331.97
    cleaning_beam_agent-0_min: 142
    cleaning_beam_agent-1_max: 410
    cleaning_beam_agent-1_mean: 276.4
    cleaning_beam_agent-1_min: 147
    cleaning_beam_agent-2_max: 556
    cleaning_beam_agent-2_mean: 313.74
    cleaning_beam_agent-2_min: 125
    cleaning_beam_agent-3_max: 149
    cleaning_beam_agent-3_mean: 49.78
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 489
    cleaning_beam_agent-4_mean: 323.93
    cleaning_beam_agent-4_min: 159
    cleaning_beam_agent-5_max: 399
    cleaning_beam_agent-5_mean: 98.82
    cleaning_beam_agent-5_min: 19
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.05
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 8
    fire_beam_agent-4_mean: 0.21
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 22
    fire_beam_agent-5_mean: 0.27
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-23-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 769.999999999978
  episode_reward_mean: 537.6400000000023
  episode_reward_min: 219.9999999999992
  episodes_this_iter: 96
  episodes_total: 7584
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20139.685
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007927488186396658
        entropy: 1.3643231391906738
        entropy_coeff: 0.0017600000137463212
        kl: 0.01321110688149929
        model: {}
        policy_loss: -0.027915336191654205
        total_loss: -0.027693182229995728
        vf_explained_var: 0.03843033313751221
        vf_loss: 13.022510528564453
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007927488186396658
        entropy: 1.2407915592193604
        entropy_coeff: 0.0017600000137463212
        kl: 0.017008967697620392
        model: {}
        policy_loss: -0.03488536179065704
        total_loss: -0.03408202901482582
        vf_explained_var: 0.05027702450752258
        vf_loss: 12.862332344055176
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007927488186396658
        entropy: 1.2519761323928833
        entropy_coeff: 0.0017600000137463212
        kl: 0.016137415543198586
        model: {}
        policy_loss: -0.03129652515053749
        total_loss: -0.030623281374573708
        vf_explained_var: 0.0667182058095932
        vf_loss: 12.62980842590332
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007927488186396658
        entropy: 0.9162297248840332
        entropy_coeff: 0.0017600000137463212
        kl: 0.012960118241608143
        model: {}
        policy_loss: -0.02471289038658142
        total_loss: -0.023987838998436928
        vf_explained_var: 0.2306726723909378
        vf_loss: 10.416050910949707
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007927488186396658
        entropy: 1.1504251956939697
        entropy_coeff: 0.0017600000137463212
        kl: 0.016483191400766373
        model: {}
        policy_loss: -0.0337505079805851
        total_loss: -0.03295304998755455
        vf_explained_var: 0.1326826512813568
        vf_loss: 11.73880672454834
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007927488186396658
        entropy: 1.1103601455688477
        entropy_coeff: 0.0017600000137463212
        kl: 0.0177549310028553
        model: {}
        policy_loss: -0.03387928381562233
        total_loss: -0.03293430432677269
        vf_explained_var: 0.16937848925590515
        vf_loss: 11.237239837646484
    load_time_ms: 13813.259
    num_steps_sampled: 7584000
    num_steps_trained: 7584000
    sample_time_ms: 89179.653
    update_time_ms: 21.499
  iterations_since_restore: 59
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.3125
    ram_util_percent: 9.58125
  pid: 4061
  policy_reward_max:
    agent-0: 128.33333333333408
    agent-1: 128.33333333333408
    agent-2: 128.33333333333408
    agent-3: 128.33333333333408
    agent-4: 128.33333333333408
    agent-5: 128.33333333333408
  policy_reward_mean:
    agent-0: 89.60666666666688
    agent-1: 89.60666666666688
    agent-2: 89.60666666666688
    agent-3: 89.60666666666688
    agent-4: 89.60666666666688
    agent-5: 89.60666666666688
  policy_reward_min:
    agent-0: 36.66666666666666
    agent-1: 36.66666666666666
    agent-2: 36.66666666666666
    agent-3: 36.66666666666666
    agent-4: 36.66666666666666
    agent-5: 36.66666666666666
  sampler_perf:
    mean_env_wait_ms: 23.32790331931346
    mean_inference_ms: 12.322373722562432
    mean_processing_ms: 50.88344775476875
  time_since_restore: 8267.801998853683
  time_this_iter_s: 123.06378316879272
  time_total_s: 11478.865684986115
  timestamp: 1637025783
  timesteps_since_restore: 5664000
  timesteps_this_iter: 96000
  timesteps_total: 7584000
  training_iteration: 79
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     79 |          11478.9 | 7584000 |   537.64 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 55
    apples_agent-0_mean: 4.21
    apples_agent-0_min: 0
    apples_agent-1_max: 72
    apples_agent-1_mean: 20.49
    apples_agent-1_min: 0
    apples_agent-2_max: 151
    apples_agent-2_mean: 12.11
    apples_agent-2_min: 0
    apples_agent-3_max: 194
    apples_agent-3_mean: 93.9
    apples_agent-3_min: 32
    apples_agent-4_max: 76
    apples_agent-4_mean: 4.46
    apples_agent-4_min: 0
    apples_agent-5_max: 199
    apples_agent-5_mean: 69.31
    apples_agent-5_min: 3
    cleaning_beam_agent-0_max: 490
    cleaning_beam_agent-0_mean: 329.17
    cleaning_beam_agent-0_min: 163
    cleaning_beam_agent-1_max: 451
    cleaning_beam_agent-1_mean: 291.32
    cleaning_beam_agent-1_min: 136
    cleaning_beam_agent-2_max: 516
    cleaning_beam_agent-2_mean: 302.85
    cleaning_beam_agent-2_min: 132
    cleaning_beam_agent-3_max: 117
    cleaning_beam_agent-3_mean: 51.26
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 497
    cleaning_beam_agent-4_mean: 352.0
    cleaning_beam_agent-4_min: 179
    cleaning_beam_agent-5_max: 317
    cleaning_beam_agent-5_mean: 98.12
    cleaning_beam_agent-5_min: 22
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.06
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 9
    fire_beam_agent-4_mean: 0.11
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 7
    fire_beam_agent-5_mean: 0.1
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-25-07
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 769.9999999999815
  episode_reward_mean: 552.5600000000028
  episode_reward_min: 259.9999999999963
  episodes_this_iter: 96
  episodes_total: 7680
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20158.525
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007867583772167563
        entropy: 1.3631088733673096
        entropy_coeff: 0.0017600000137463212
        kl: 0.013366909697651863
        model: {}
        policy_loss: -0.02888190746307373
        total_loss: -0.028683818876743317
        vf_explained_var: 0.04951448738574982
        vf_loss: 12.60470962524414
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007867583772167563
        entropy: 1.2179250717163086
        entropy_coeff: 0.0017600000137463212
        kl: 0.018018042668700218
        model: {}
        policy_loss: -0.033713992685079575
        total_loss: -0.03282318264245987
        vf_explained_var: 0.07091611623764038
        vf_loss: 12.32555103302002
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007867583772167563
        entropy: 1.259127140045166
        entropy_coeff: 0.0017600000137463212
        kl: 0.01611272245645523
        model: {}
        policy_loss: -0.030644237995147705
        total_loss: -0.030018337070941925
        vf_explained_var: 0.07190187275409698
        vf_loss: 12.306939125061035
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007867583772167563
        entropy: 0.9038702249526978
        entropy_coeff: 0.0017600000137463212
        kl: 0.011825342662632465
        model: {}
        policy_loss: -0.024565566331148148
        total_loss: -0.02391422539949417
        vf_explained_var: 0.20105962455272675
        vf_loss: 10.59617805480957
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007867583772167563
        entropy: 1.1232203245162964
        entropy_coeff: 0.0017600000137463212
        kl: 0.017856525257229805
        model: {}
        policy_loss: -0.03407225385308266
        total_loss: -0.03310522437095642
        vf_explained_var: 0.12627270817756653
        vf_loss: 11.582464218139648
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007867583772167563
        entropy: 1.1341640949249268
        entropy_coeff: 0.0017600000137463212
        kl: 0.019001498818397522
        model: {}
        policy_loss: -0.03475995361804962
        total_loss: -0.033745333552360535
        vf_explained_var: 0.16264601051807404
        vf_loss: 11.106013298034668
    load_time_ms: 13797.004
    num_steps_sampled: 7680000
    num_steps_trained: 7680000
    sample_time_ms: 89206.955
    update_time_ms: 21.011
  iterations_since_restore: 60
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.413068181818183
    ram_util_percent: 9.590909090909092
  pid: 4061
  policy_reward_max:
    agent-0: 128.333333333334
    agent-1: 128.333333333334
    agent-2: 128.333333333334
    agent-3: 128.333333333334
    agent-4: 128.333333333334
    agent-5: 128.333333333334
  policy_reward_mean:
    agent-0: 92.09333333333353
    agent-1: 92.09333333333353
    agent-2: 92.09333333333353
    agent-3: 92.09333333333353
    agent-4: 92.09333333333353
    agent-5: 92.09333333333353
  policy_reward_min:
    agent-0: 43.333333333333265
    agent-1: 43.333333333333265
    agent-2: 43.333333333333265
    agent-3: 43.333333333333265
    agent-4: 43.333333333333265
    agent-5: 43.333333333333265
  sampler_perf:
    mean_env_wait_ms: 23.340777300702648
    mean_inference_ms: 12.320750796589923
    mean_processing_ms: 50.8769259837758
  time_since_restore: 8391.582073688507
  time_this_iter_s: 123.78007483482361
  time_total_s: 11602.645759820938
  timestamp: 1637025907
  timesteps_since_restore: 5760000
  timesteps_this_iter: 96000
  timesteps_total: 7680000
  training_iteration: 80
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     80 |          11602.6 | 7680000 |   552.56 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 25
    apples_agent-0_mean: 1.77
    apples_agent-0_min: 0
    apples_agent-1_max: 70
    apples_agent-1_mean: 17.44
    apples_agent-1_min: 0
    apples_agent-2_max: 116
    apples_agent-2_mean: 12.44
    apples_agent-2_min: 0
    apples_agent-3_max: 175
    apples_agent-3_mean: 99.43
    apples_agent-3_min: 19
    apples_agent-4_max: 92
    apples_agent-4_mean: 5.61
    apples_agent-4_min: 0
    apples_agent-5_max: 174
    apples_agent-5_mean: 74.83
    apples_agent-5_min: 22
    cleaning_beam_agent-0_max: 504
    cleaning_beam_agent-0_mean: 338.94
    cleaning_beam_agent-0_min: 175
    cleaning_beam_agent-1_max: 440
    cleaning_beam_agent-1_mean: 293.55
    cleaning_beam_agent-1_min: 146
    cleaning_beam_agent-2_max: 494
    cleaning_beam_agent-2_mean: 303.75
    cleaning_beam_agent-2_min: 95
    cleaning_beam_agent-3_max: 107
    cleaning_beam_agent-3_mean: 51.75
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 482
    cleaning_beam_agent-4_mean: 330.39
    cleaning_beam_agent-4_min: 144
    cleaning_beam_agent-5_max: 385
    cleaning_beam_agent-5_mean: 111.18
    cleaning_beam_agent-5_min: 20
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 23
    fire_beam_agent-5_mean: 0.34
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-27-10
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 837.9999999999872
  episode_reward_mean: 560.4600000000015
  episode_reward_min: 164.9999999999998
  episodes_this_iter: 96
  episodes_total: 7776
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20153.172
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007807679940015078
        entropy: 1.3426225185394287
        entropy_coeff: 0.0017600000137463212
        kl: 0.01314842700958252
        model: {}
        policy_loss: -0.02859402447938919
        total_loss: -0.028269782662391663
        vf_explained_var: 0.03874467313289642
        vf_loss: 13.724126815795898
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007807679940015078
        entropy: 1.2288776636123657
        entropy_coeff: 0.0017600000137463212
        kl: 0.01677367091178894
        model: {}
        policy_loss: -0.033784523606300354
        total_loss: -0.03297213837504387
        vf_explained_var: 0.09091305732727051
        vf_loss: 12.978397369384766
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007807679940015078
        entropy: 1.2523391246795654
        entropy_coeff: 0.0017600000137463212
        kl: 0.016194695606827736
        model: {}
        policy_loss: -0.03245115280151367
        total_loss: -0.031677909195423126
        vf_explained_var: 0.049625739455223083
        vf_loss: 13.578917503356934
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007807679940015078
        entropy: 0.9022565484046936
        entropy_coeff: 0.0017600000137463212
        kl: 0.01383163407444954
        model: {}
        policy_loss: -0.022980621084570885
        total_loss: -0.02204333059489727
        vf_explained_var: 0.20033332705497742
        vf_loss: 11.421016693115234
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007807679940015078
        entropy: 1.1397687196731567
        entropy_coeff: 0.0017600000137463212
        kl: 0.016828378662467003
        model: {}
        policy_loss: -0.03671342134475708
        total_loss: -0.03581997752189636
        vf_explained_var: 0.14850419759750366
        vf_loss: 12.166055679321289
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007807679940015078
        entropy: 1.1309475898742676
        entropy_coeff: 0.0017600000137463212
        kl: 0.016781730577349663
        model: {}
        policy_loss: -0.0363960787653923
        total_loss: -0.03549975901842117
        vf_explained_var: 0.15343649685382843
        vf_loss: 12.086149215698242
    load_time_ms: 13789.449
    num_steps_sampled: 7776000
    num_steps_trained: 7776000
    sample_time_ms: 89130.272
    update_time_ms: 20.787
  iterations_since_restore: 61
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.479885057471265
    ram_util_percent: 9.5816091954023
  pid: 4061
  policy_reward_max:
    agent-0: 139.66666666666703
    agent-1: 139.66666666666703
    agent-2: 139.66666666666703
    agent-3: 139.66666666666703
    agent-4: 139.66666666666703
    agent-5: 139.66666666666703
  policy_reward_mean:
    agent-0: 93.41000000000024
    agent-1: 93.41000000000024
    agent-2: 93.41000000000024
    agent-3: 93.41000000000024
    agent-4: 93.41000000000024
    agent-5: 93.41000000000024
  policy_reward_min:
    agent-0: 27.50000000000002
    agent-1: 27.50000000000002
    agent-2: 27.50000000000002
    agent-3: 27.50000000000002
    agent-4: 27.50000000000002
    agent-5: 27.50000000000002
  sampler_perf:
    mean_env_wait_ms: 23.35062065555889
    mean_inference_ms: 12.31794648052644
    mean_processing_ms: 50.86428160880294
  time_since_restore: 8513.777613162994
  time_this_iter_s: 122.1955394744873
  time_total_s: 11724.841299295425
  timestamp: 1637026030
  timesteps_since_restore: 5856000
  timesteps_this_iter: 96000
  timesteps_total: 7776000
  training_iteration: 81
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     81 |          11724.8 | 7776000 |   560.46 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 39
    apples_agent-0_mean: 3.75
    apples_agent-0_min: 0
    apples_agent-1_max: 83
    apples_agent-1_mean: 17.78
    apples_agent-1_min: 0
    apples_agent-2_max: 174
    apples_agent-2_mean: 14.84
    apples_agent-2_min: 0
    apples_agent-3_max: 196
    apples_agent-3_mean: 102.57
    apples_agent-3_min: 39
    apples_agent-4_max: 83
    apples_agent-4_mean: 5.04
    apples_agent-4_min: 0
    apples_agent-5_max: 141
    apples_agent-5_mean: 68.94
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 504
    cleaning_beam_agent-0_mean: 288.5
    cleaning_beam_agent-0_min: 154
    cleaning_beam_agent-1_max: 399
    cleaning_beam_agent-1_mean: 301.29
    cleaning_beam_agent-1_min: 161
    cleaning_beam_agent-2_max: 442
    cleaning_beam_agent-2_mean: 291.93
    cleaning_beam_agent-2_min: 91
    cleaning_beam_agent-3_max: 150
    cleaning_beam_agent-3_mean: 55.06
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 489
    cleaning_beam_agent-4_mean: 333.08
    cleaning_beam_agent-4_min: 71
    cleaning_beam_agent-5_max: 430
    cleaning_beam_agent-5_mean: 108.1
    cleaning_beam_agent-5_min: 26
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 32
    fire_beam_agent-5_mean: 0.41
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-29-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 756.9999999999928
  episode_reward_mean: 551.0000000000015
  episode_reward_min: 126.00000000000057
  episodes_this_iter: 96
  episodes_total: 7872
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20185.879
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007747776107862592
        entropy: 1.3697463274002075
        entropy_coeff: 0.0017600000137463212
        kl: 0.015092581510543823
        model: {}
        policy_loss: -0.0309230238199234
        total_loss: -0.03060626983642578
        vf_explained_var: 0.07079392671585083
        vf_loss: 12.182476997375488
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007747776107862592
        entropy: 1.226552963256836
        entropy_coeff: 0.0017600000137463212
        kl: 0.016010690480470657
        model: {}
        policy_loss: -0.03293560445308685
        total_loss: -0.032230131328105927
        vf_explained_var: 0.03671056032180786
        vf_loss: 12.631393432617188
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007747776107862592
        entropy: 1.260162115097046
        entropy_coeff: 0.0017600000137463212
        kl: 0.015277154743671417
        model: {}
        policy_loss: -0.03146683797240257
        total_loss: -0.0309734046459198
        vf_explained_var: 0.09727679193019867
        vf_loss: 11.83602523803711
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007747776107862592
        entropy: 0.917356014251709
        entropy_coeff: 0.0017600000137463212
        kl: 0.013425738550722599
        model: {}
        policy_loss: -0.02447957545518875
        total_loss: -0.02373690903186798
        vf_explained_var: 0.2265857458114624
        vf_loss: 10.146373748779297
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007747776107862592
        entropy: 1.116052508354187
        entropy_coeff: 0.0017600000137463212
        kl: 0.01689254492521286
        model: {}
        policy_loss: -0.03480825573205948
        total_loss: -0.0339154414832592
        vf_explained_var: 0.11096231639385223
        vf_loss: 11.678154945373535
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007747776107862592
        entropy: 1.1232001781463623
        entropy_coeff: 0.0017600000137463212
        kl: 0.01783256232738495
        model: {}
        policy_loss: -0.03600503131747246
        total_loss: -0.03511100262403488
        vf_explained_var: 0.17001360654830933
        vf_loss: 10.876012802124023
    load_time_ms: 13836.831
    num_steps_sampled: 7872000
    num_steps_trained: 7872000
    sample_time_ms: 89289.831
    update_time_ms: 21.271
  iterations_since_restore: 62
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 19.855307262569834
    ram_util_percent: 11.802234636871505
  pid: 4061
  policy_reward_max:
    agent-0: 126.16666666666713
    agent-1: 126.16666666666713
    agent-2: 126.16666666666713
    agent-3: 126.16666666666713
    agent-4: 126.16666666666713
    agent-5: 126.16666666666713
  policy_reward_mean:
    agent-0: 91.83333333333356
    agent-1: 91.83333333333356
    agent-2: 91.83333333333356
    agent-3: 91.83333333333356
    agent-4: 91.83333333333356
    agent-5: 91.83333333333356
  policy_reward_min:
    agent-0: 21.000000000000014
    agent-1: 21.000000000000014
    agent-2: 21.000000000000014
    agent-3: 21.000000000000014
    agent-4: 21.000000000000014
    agent-5: 21.000000000000014
  sampler_perf:
    mean_env_wait_ms: 23.360396765572364
    mean_inference_ms: 12.316166285854672
    mean_processing_ms: 50.862778065364864
  time_since_restore: 8639.171561479568
  time_this_iter_s: 125.3939483165741
  time_total_s: 11850.235247612
  timestamp: 1637026156
  timesteps_since_restore: 5952000
  timesteps_this_iter: 96000
  timesteps_total: 7872000
  training_iteration: 82
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     82 |          11850.2 | 7872000 |      551 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 43
    apples_agent-0_mean: 4.19
    apples_agent-0_min: 0
    apples_agent-1_max: 77
    apples_agent-1_mean: 18.52
    apples_agent-1_min: 0
    apples_agent-2_max: 119
    apples_agent-2_mean: 10.39
    apples_agent-2_min: 0
    apples_agent-3_max: 161
    apples_agent-3_mean: 97.37
    apples_agent-3_min: 27
    apples_agent-4_max: 122
    apples_agent-4_mean: 10.15
    apples_agent-4_min: 0
    apples_agent-5_max: 132
    apples_agent-5_mean: 66.62
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 481
    cleaning_beam_agent-0_mean: 287.29
    cleaning_beam_agent-0_min: 98
    cleaning_beam_agent-1_max: 423
    cleaning_beam_agent-1_mean: 292.73
    cleaning_beam_agent-1_min: 165
    cleaning_beam_agent-2_max: 450
    cleaning_beam_agent-2_mean: 292.74
    cleaning_beam_agent-2_min: 124
    cleaning_beam_agent-3_max: 132
    cleaning_beam_agent-3_mean: 50.91
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 508
    cleaning_beam_agent-4_mean: 317.16
    cleaning_beam_agent-4_min: 71
    cleaning_beam_agent-5_max: 356
    cleaning_beam_agent-5_mean: 103.93
    cleaning_beam_agent-5_min: 14
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 63
    fire_beam_agent-5_mean: 1.15
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-31-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 789.999999999992
  episode_reward_mean: 541.2000000000019
  episode_reward_min: 167.99999999999875
  episodes_this_iter: 96
  episodes_total: 7968
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20194.329
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007687872275710106
        entropy: 1.3395514488220215
        entropy_coeff: 0.0017600000137463212
        kl: 0.01457757968455553
        model: {}
        policy_loss: -0.030639484524726868
        total_loss: -0.030209431424736977
        vf_explained_var: 0.02609136700630188
        vf_loss: 13.299020767211914
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007687872275710106
        entropy: 1.2273918390274048
        entropy_coeff: 0.0017600000137463212
        kl: 0.01720612868666649
        model: {}
        policy_loss: -0.033576518297195435
        total_loss: -0.03270803391933441
        vf_explained_var: 0.041554421186447144
        vf_loss: 13.080819129943848
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007687872275710106
        entropy: 1.2552937269210815
        entropy_coeff: 0.0017600000137463212
        kl: 0.014818141236901283
        model: {}
        policy_loss: -0.03101145289838314
        total_loss: -0.030439432710409164
        vf_explained_var: 0.04838860034942627
        vf_loss: 12.995269775390625
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007687872275710106
        entropy: 0.9297676086425781
        entropy_coeff: 0.0017600000137463212
        kl: 0.012736737728118896
        model: {}
        policy_loss: -0.02528025023639202
        total_loss: -0.02459127828478813
        vf_explained_var: 0.22918513417243958
        vf_loss: 10.516900062561035
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007687872275710106
        entropy: 1.1315209865570068
        entropy_coeff: 0.0017600000137463212
        kl: 0.01693316176533699
        model: {}
        policy_loss: -0.035710278898477554
        total_loss: -0.03485775366425514
        vf_explained_var: 0.15682350099086761
        vf_loss: 11.506848335266113
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007687872275710106
        entropy: 1.0954391956329346
        entropy_coeff: 0.0017600000137463212
        kl: 0.016201239079236984
        model: {}
        policy_loss: -0.03811819851398468
        total_loss: -0.03720603138208389
        vf_explained_var: 0.10633862018585205
        vf_loss: 12.200122833251953
    load_time_ms: 13888.697
    num_steps_sampled: 7968000
    num_steps_trained: 7968000
    sample_time_ms: 89551.414
    update_time_ms: 23.444
  iterations_since_restore: 63
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.68516483516484
    ram_util_percent: 13.4489010989011
  pid: 4061
  policy_reward_max:
    agent-0: 131.66666666666717
    agent-1: 131.66666666666717
    agent-2: 131.66666666666717
    agent-3: 131.66666666666717
    agent-4: 131.66666666666717
    agent-5: 131.66666666666717
  policy_reward_mean:
    agent-0: 90.2000000000002
    agent-1: 90.2000000000002
    agent-2: 90.2000000000002
    agent-3: 90.2000000000002
    agent-4: 90.2000000000002
    agent-5: 90.2000000000002
  policy_reward_min:
    agent-0: 28.000000000000053
    agent-1: 28.000000000000053
    agent-2: 28.000000000000053
    agent-3: 28.000000000000053
    agent-4: 28.000000000000053
    agent-5: 28.000000000000053
  sampler_perf:
    mean_env_wait_ms: 23.377920958016016
    mean_inference_ms: 12.318690103329716
    mean_processing_ms: 50.87513768744011
  time_since_restore: 8766.342928886414
  time_this_iter_s: 127.17136740684509
  time_total_s: 11977.406615018845
  timestamp: 1637026283
  timesteps_since_restore: 6048000
  timesteps_this_iter: 96000
  timesteps_total: 7968000
  training_iteration: 83
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     83 |          11977.4 | 7968000 |    541.2 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 56
    apples_agent-0_mean: 4.72
    apples_agent-0_min: 0
    apples_agent-1_max: 101
    apples_agent-1_mean: 15.07
    apples_agent-1_min: 0
    apples_agent-2_max: 96
    apples_agent-2_mean: 10.81
    apples_agent-2_min: 0
    apples_agent-3_max: 159
    apples_agent-3_mean: 98.52
    apples_agent-3_min: 20
    apples_agent-4_max: 89
    apples_agent-4_mean: 7.16
    apples_agent-4_min: 0
    apples_agent-5_max: 163
    apples_agent-5_mean: 73.09
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 449
    cleaning_beam_agent-0_mean: 262.78
    cleaning_beam_agent-0_min: 125
    cleaning_beam_agent-1_max: 401
    cleaning_beam_agent-1_mean: 296.35
    cleaning_beam_agent-1_min: 123
    cleaning_beam_agent-2_max: 514
    cleaning_beam_agent-2_mean: 308.51
    cleaning_beam_agent-2_min: 98
    cleaning_beam_agent-3_max: 110
    cleaning_beam_agent-3_mean: 51.8
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 528
    cleaning_beam_agent-4_mean: 329.68
    cleaning_beam_agent-4_min: 189
    cleaning_beam_agent-5_max: 431
    cleaning_beam_agent-5_mean: 99.89
    cleaning_beam_agent-5_min: 28
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.05
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.04
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 9
    fire_beam_agent-5_mean: 0.2
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-33-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 761.9999999999817
  episode_reward_mean: 541.0900000000041
  episode_reward_min: 207.9999999999986
  episodes_this_iter: 96
  episodes_total: 8064
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20205.552
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007627967861481011
        entropy: 1.350003957748413
        entropy_coeff: 0.0017600000137463212
        kl: 0.014615504071116447
        model: {}
        policy_loss: -0.030519602820277214
        total_loss: -0.030155718326568604
        vf_explained_var: 0.033710822463035583
        vf_loss: 12.783411026000977
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007627967861481011
        entropy: 1.2338367700576782
        entropy_coeff: 0.0017600000137463212
        kl: 0.017087161540985107
        model: {}
        policy_loss: -0.03600211441516876
        total_loss: -0.03516020625829697
        vf_explained_var: 0.013616487383842468
        vf_loss: 13.047505378723145
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007627967861481011
        entropy: 1.2434957027435303
        entropy_coeff: 0.0017600000137463212
        kl: 0.015845920890569687
        model: {}
        policy_loss: -0.03172079101204872
        total_loss: -0.031117333099246025
        vf_explained_var: 0.08693145215511322
        vf_loss: 12.074177742004395
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007627967861481011
        entropy: 0.9244738817214966
        entropy_coeff: 0.0017600000137463212
        kl: 0.012243736535310745
        model: {}
        policy_loss: -0.025022437795996666
        total_loss: -0.024349234998226166
        vf_explained_var: 0.18571870028972626
        vf_loss: 10.759038925170898
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007627967861481011
        entropy: 1.1477091312408447
        entropy_coeff: 0.0017600000137463212
        kl: 0.017321467399597168
        model: {}
        policy_loss: -0.034963808953762054
        total_loss: -0.034137919545173645
        vf_explained_var: 0.15726886689662933
        vf_loss: 11.137121200561523
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007627967861481011
        entropy: 1.1150054931640625
        entropy_coeff: 0.0017600000137463212
        kl: 0.017594624310731888
        model: {}
        policy_loss: -0.03709122911095619
        total_loss: -0.036171913146972656
        vf_explained_var: 0.1517667919397354
        vf_loss: 11.222692489624023
    load_time_ms: 13947.303
    num_steps_sampled: 8064000
    num_steps_trained: 8064000
    sample_time_ms: 89863.673
    update_time_ms: 23.976
  iterations_since_restore: 64
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.73388888888889
    ram_util_percent: 13.328888888888887
  pid: 4061
  policy_reward_max:
    agent-0: 127.00000000000068
    agent-1: 127.00000000000068
    agent-2: 127.00000000000068
    agent-3: 127.00000000000068
    agent-4: 127.00000000000068
    agent-5: 127.00000000000068
  policy_reward_mean:
    agent-0: 90.18166666666689
    agent-1: 90.18166666666689
    agent-2: 90.18166666666689
    agent-3: 90.18166666666689
    agent-4: 90.18166666666689
    agent-5: 90.18166666666689
  policy_reward_min:
    agent-0: 34.66666666666671
    agent-1: 34.66666666666671
    agent-2: 34.66666666666671
    agent-3: 34.66666666666671
    agent-4: 34.66666666666671
    agent-5: 34.66666666666671
  sampler_perf:
    mean_env_wait_ms: 23.392865249274827
    mean_inference_ms: 12.320473344893964
    mean_processing_ms: 50.886147502038995
  time_since_restore: 8892.675924062729
  time_this_iter_s: 126.33299517631531
  time_total_s: 12103.73961019516
  timestamp: 1637026409
  timesteps_since_restore: 6144000
  timesteps_this_iter: 96000
  timesteps_total: 8064000
  training_iteration: 84
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     84 |          12103.7 | 8064000 |   541.09 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 58
    apples_agent-0_mean: 5.17
    apples_agent-0_min: 0
    apples_agent-1_max: 65
    apples_agent-1_mean: 17.58
    apples_agent-1_min: 0
    apples_agent-2_max: 110
    apples_agent-2_mean: 11.28
    apples_agent-2_min: 0
    apples_agent-3_max: 164
    apples_agent-3_mean: 94.69
    apples_agent-3_min: 41
    apples_agent-4_max: 75
    apples_agent-4_mean: 6.15
    apples_agent-4_min: 0
    apples_agent-5_max: 138
    apples_agent-5_mean: 70.0
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 468
    cleaning_beam_agent-0_mean: 273.08
    cleaning_beam_agent-0_min: 120
    cleaning_beam_agent-1_max: 409
    cleaning_beam_agent-1_mean: 284.57
    cleaning_beam_agent-1_min: 149
    cleaning_beam_agent-2_max: 485
    cleaning_beam_agent-2_mean: 282.68
    cleaning_beam_agent-2_min: 101
    cleaning_beam_agent-3_max: 107
    cleaning_beam_agent-3_mean: 55.65
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 502
    cleaning_beam_agent-4_mean: 296.35
    cleaning_beam_agent-4_min: 86
    cleaning_beam_agent-5_max: 703
    cleaning_beam_agent-5_mean: 124.24
    cleaning_beam_agent-5_min: 36
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 29
    fire_beam_agent-5_mean: 0.61
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-35-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 735.9999999999906
  episode_reward_mean: 520.250000000005
  episode_reward_min: -26.99999999998833
  episodes_this_iter: 96
  episodes_total: 8160
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20228.115
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007568064029328525
        entropy: 1.3540080785751343
        entropy_coeff: 0.0017600000137463212
        kl: 0.013736367225646973
        model: {}
        policy_loss: -0.027735374867916107
        total_loss: -0.027074482291936874
        vf_explained_var: 0.07815681397914886
        vf_loss: 16.703086853027344
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007568064029328525
        entropy: 1.2242823839187622
        entropy_coeff: 0.0017600000137463212
        kl: 0.015484366565942764
        model: {}
        policy_loss: -0.03209462761878967
        total_loss: -0.031006891280412674
        vf_explained_var: 0.06366994976997375
        vf_loss: 16.94039535522461
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007568064029328525
        entropy: 1.2558655738830566
        entropy_coeff: 0.0017600000137463212
        kl: 0.015412569977343082
        model: {}
        policy_loss: -0.028895333409309387
        total_loss: -0.027906127274036407
        vf_explained_var: 0.0834675282239914
        vf_loss: 16.58272933959961
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007568064029328525
        entropy: 0.9414756298065186
        entropy_coeff: 0.0017600000137463212
        kl: 0.01322997361421585
        model: {}
        policy_loss: -0.023534178733825684
        total_loss: -0.022360779345035553
        vf_explained_var: 0.16820259392261505
        vf_loss: 15.074007034301758
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007568064029328525
        entropy: 1.1547446250915527
        entropy_coeff: 0.0017600000137463212
        kl: 0.016243092715740204
        model: {}
        policy_loss: -0.0321284718811512
        total_loss: -0.03096068650484085
        vf_explained_var: 0.13035713136196136
        vf_loss: 15.758268356323242
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007568064029328525
        entropy: 1.0921368598937988
        entropy_coeff: 0.0017600000137463212
        kl: 0.015457081608474255
        model: {}
        policy_loss: -0.035314757376909256
        total_loss: -0.034191809594631195
        vf_explained_var: 0.17155896127223969
        vf_loss: 14.994022369384766
    load_time_ms: 13959.193
    num_steps_sampled: 8160000
    num_steps_trained: 8160000
    sample_time_ms: 90320.791
    update_time_ms: 24.657
  iterations_since_restore: 65
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.67103825136612
    ram_util_percent: 13.41748633879782
  pid: 4061
  policy_reward_max:
    agent-0: 122.66666666666723
    agent-1: 122.66666666666723
    agent-2: 122.66666666666723
    agent-3: 122.66666666666723
    agent-4: 122.66666666666723
    agent-5: 122.66666666666723
  policy_reward_mean:
    agent-0: 86.7083333333335
    agent-1: 86.7083333333335
    agent-2: 86.7083333333335
    agent-3: 86.7083333333335
    agent-4: 86.7083333333335
    agent-5: 86.7083333333335
  policy_reward_min:
    agent-0: -4.499999999999701
    agent-1: -4.499999999999701
    agent-2: -4.499999999999701
    agent-3: -4.499999999999701
    agent-4: -4.499999999999701
    agent-5: -4.499999999999701
  sampler_perf:
    mean_env_wait_ms: 23.407527699408192
    mean_inference_ms: 12.32220848893418
    mean_processing_ms: 50.89926190580053
  time_since_restore: 9020.875066280365
  time_this_iter_s: 128.1991422176361
  time_total_s: 12231.938752412796
  timestamp: 1637026538
  timesteps_since_restore: 6240000
  timesteps_this_iter: 96000
  timesteps_total: 8160000
  training_iteration: 85
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     85 |          12231.9 | 8160000 |   520.25 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 56
    apples_agent-0_mean: 3.41
    apples_agent-0_min: 0
    apples_agent-1_max: 47
    apples_agent-1_mean: 14.84
    apples_agent-1_min: 0
    apples_agent-2_max: 74
    apples_agent-2_mean: 9.78
    apples_agent-2_min: 0
    apples_agent-3_max: 175
    apples_agent-3_mean: 90.1
    apples_agent-3_min: 28
    apples_agent-4_max: 99
    apples_agent-4_mean: 6.69
    apples_agent-4_min: 0
    apples_agent-5_max: 132
    apples_agent-5_mean: 66.92
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 475
    cleaning_beam_agent-0_mean: 280.44
    cleaning_beam_agent-0_min: 145
    cleaning_beam_agent-1_max: 460
    cleaning_beam_agent-1_mean: 287.86
    cleaning_beam_agent-1_min: 177
    cleaning_beam_agent-2_max: 527
    cleaning_beam_agent-2_mean: 273.3
    cleaning_beam_agent-2_min: 114
    cleaning_beam_agent-3_max: 119
    cleaning_beam_agent-3_mean: 52.44
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 423
    cleaning_beam_agent-4_mean: 296.64
    cleaning_beam_agent-4_min: 128
    cleaning_beam_agent-5_max: 703
    cleaning_beam_agent-5_mean: 99.22
    cleaning_beam_agent-5_min: 15
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 51
    fire_beam_agent-5_mean: 0.57
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-37-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 819.999999999978
  episode_reward_mean: 532.200000000003
  episode_reward_min: 192.99999999999866
  episodes_this_iter: 96
  episodes_total: 8256
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20224.456
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007508160197176039
        entropy: 1.3449504375457764
        entropy_coeff: 0.0017600000137463212
        kl: 0.013902129605412483
        model: {}
        policy_loss: -0.030371444299817085
        total_loss: -0.030088750645518303
        vf_explained_var: 0.062110915780067444
        vf_loss: 12.595929145812988
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007508160197176039
        entropy: 1.199444055557251
        entropy_coeff: 0.0017600000137463212
        kl: 0.0166621096432209
        model: {}
        policy_loss: -0.03464093431830406
        total_loss: -0.033792704343795776
        vf_explained_var: 0.0368245393037796
        vf_loss: 12.930455207824707
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007508160197176039
        entropy: 1.2605764865875244
        entropy_coeff: 0.0017600000137463212
        kl: 0.016187967732548714
        model: {}
        policy_loss: -0.03358202427625656
        total_loss: -0.03293155878782272
        vf_explained_var: 0.06882727146148682
        vf_loss: 12.502792358398438
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007508160197176039
        entropy: 0.9638054370880127
        entropy_coeff: 0.0017600000137463212
        kl: 0.01468225009739399
        model: {}
        policy_loss: -0.025470681488513947
        total_loss: -0.024587316438555717
        vf_explained_var: 0.1728193163871765
        vf_loss: 11.114357948303223
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007508160197176039
        entropy: 1.1604111194610596
        entropy_coeff: 0.0017600000137463212
        kl: 0.016784902662038803
        model: {}
        policy_loss: -0.03521987050771713
        total_loss: -0.03439020738005638
        vf_explained_var: 0.11075867712497711
        vf_loss: 11.934999465942383
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007508160197176039
        entropy: 1.0825189352035522
        entropy_coeff: 0.0017600000137463212
        kl: 0.016605395823717117
        model: {}
        policy_loss: -0.03717346861958504
        total_loss: -0.03630761057138443
        vf_explained_var: 0.17270605266094208
        vf_loss: 11.105497360229492
    load_time_ms: 13991.139
    num_steps_sampled: 8256000
    num_steps_trained: 8256000
    sample_time_ms: 90568.17
    update_time_ms: 26.953
  iterations_since_restore: 66
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.5817679558011
    ram_util_percent: 13.252486187845303
  pid: 4061
  policy_reward_max:
    agent-0: 136.66666666666717
    agent-1: 136.66666666666717
    agent-2: 136.66666666666717
    agent-3: 136.66666666666717
    agent-4: 136.66666666666717
    agent-5: 136.66666666666717
  policy_reward_mean:
    agent-0: 88.70000000000016
    agent-1: 88.70000000000016
    agent-2: 88.70000000000016
    agent-3: 88.70000000000016
    agent-4: 88.70000000000016
    agent-5: 88.70000000000016
  policy_reward_min:
    agent-0: 32.166666666666735
    agent-1: 32.166666666666735
    agent-2: 32.166666666666735
    agent-3: 32.166666666666735
    agent-4: 32.166666666666735
    agent-5: 32.166666666666735
  sampler_perf:
    mean_env_wait_ms: 23.418012999819783
    mean_inference_ms: 12.324815630008091
    mean_processing_ms: 50.90475880678446
  time_since_restore: 9147.632252454758
  time_this_iter_s: 126.7571861743927
  time_total_s: 12358.695938587189
  timestamp: 1637026665
  timesteps_since_restore: 6336000
  timesteps_this_iter: 96000
  timesteps_total: 8256000
  training_iteration: 86
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 24.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     86 |          12358.7 | 8256000 |    532.2 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 30
    apples_agent-0_mean: 3.74
    apples_agent-0_min: 0
    apples_agent-1_max: 55
    apples_agent-1_mean: 16.63
    apples_agent-1_min: 0
    apples_agent-2_max: 167
    apples_agent-2_mean: 10.83
    apples_agent-2_min: 0
    apples_agent-3_max: 161
    apples_agent-3_mean: 92.27
    apples_agent-3_min: 33
    apples_agent-4_max: 114
    apples_agent-4_mean: 7.02
    apples_agent-4_min: 0
    apples_agent-5_max: 238
    apples_agent-5_mean: 76.33
    apples_agent-5_min: 7
    cleaning_beam_agent-0_max: 479
    cleaning_beam_agent-0_mean: 290.73
    cleaning_beam_agent-0_min: 136
    cleaning_beam_agent-1_max: 452
    cleaning_beam_agent-1_mean: 291.44
    cleaning_beam_agent-1_min: 126
    cleaning_beam_agent-2_max: 471
    cleaning_beam_agent-2_mean: 274.28
    cleaning_beam_agent-2_min: 132
    cleaning_beam_agent-3_max: 121
    cleaning_beam_agent-3_mean: 56.63
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 450
    cleaning_beam_agent-4_mean: 305.31
    cleaning_beam_agent-4_min: 127
    cleaning_beam_agent-5_max: 325
    cleaning_beam_agent-5_mean: 84.94
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.11
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-39-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 781.9999999999836
  episode_reward_mean: 535.7200000000029
  episode_reward_min: 222.99999999999665
  episodes_this_iter: 96
  episodes_total: 8352
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20203.878
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007448255782946944
        entropy: 1.3316693305969238
        entropy_coeff: 0.0017600000137463212
        kl: 0.013676276430487633
        model: {}
        policy_loss: -0.02908199466764927
        total_loss: -0.028717443346977234
        vf_explained_var: 0.052718713879585266
        vf_loss: 13.406660079956055
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007448255782946944
        entropy: 1.210600733757019
        entropy_coeff: 0.0017600000137463212
        kl: 0.016405699774622917
        model: {}
        policy_loss: -0.03631071373820305
        total_loss: -0.035457901656627655
        vf_explained_var: 0.051262423396110535
        vf_loss: 13.428950309753418
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007448255782946944
        entropy: 1.2427725791931152
        entropy_coeff: 0.0017600000137463212
        kl: 0.01620345190167427
        model: {}
        policy_loss: -0.03375377506017685
        total_loss: -0.03303200751543045
        vf_explained_var: 0.08869132399559021
        vf_loss: 12.88700008392334
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007448255782946944
        entropy: 0.9469835758209229
        entropy_coeff: 0.0017600000137463212
        kl: 0.012556216679513454
        model: {}
        policy_loss: -0.025749467313289642
        total_loss: -0.025039812549948692
        vf_explained_var: 0.20774276554584503
        vf_loss: 11.207249641418457
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007448255782946944
        entropy: 1.1507211923599243
        entropy_coeff: 0.0017600000137463212
        kl: 0.01932862401008606
        model: {}
        policy_loss: -0.031921908259391785
        total_loss: -0.03082396648824215
        vf_explained_var: 0.15919797122478485
        vf_loss: 11.903440475463867
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007448255782946944
        entropy: 1.0891326665878296
        entropy_coeff: 0.0017600000137463212
        kl: 0.017801649868488312
        model: {}
        policy_loss: -0.03666451573371887
        total_loss: -0.035633884370326996
        vf_explained_var: 0.1766333431005478
        vf_loss: 11.673407554626465
    load_time_ms: 13960.1
    num_steps_sampled: 8352000
    num_steps_trained: 8352000
    sample_time_ms: 91159.854
    update_time_ms: 27.222
  iterations_since_restore: 67
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.599453551912568
    ram_util_percent: 13.406557377049184
  pid: 4061
  policy_reward_max:
    agent-0: 130.3333333333338
    agent-1: 130.3333333333338
    agent-2: 130.3333333333338
    agent-3: 130.3333333333338
    agent-4: 130.3333333333338
    agent-5: 130.3333333333338
  policy_reward_mean:
    agent-0: 89.28666666666682
    agent-1: 89.28666666666682
    agent-2: 89.28666666666682
    agent-3: 89.28666666666682
    agent-4: 89.28666666666682
    agent-5: 89.28666666666682
  policy_reward_min:
    agent-0: 37.16666666666669
    agent-1: 37.16666666666669
    agent-2: 37.16666666666669
    agent-3: 37.16666666666669
    agent-4: 37.16666666666669
    agent-5: 37.16666666666669
  sampler_perf:
    mean_env_wait_ms: 23.432999194587122
    mean_inference_ms: 12.32809723360699
    mean_processing_ms: 50.923050181498276
  time_since_restore: 9275.99530005455
  time_this_iter_s: 128.36304759979248
  time_total_s: 12487.058986186981
  timestamp: 1637026793
  timesteps_since_restore: 6432000
  timesteps_this_iter: 96000
  timesteps_total: 8352000
  training_iteration: 87
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     87 |          12487.1 | 8352000 |   535.72 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 48
    apples_agent-0_mean: 3.75
    apples_agent-0_min: 0
    apples_agent-1_max: 69
    apples_agent-1_mean: 17.31
    apples_agent-1_min: 0
    apples_agent-2_max: 126
    apples_agent-2_mean: 9.65
    apples_agent-2_min: 0
    apples_agent-3_max: 205
    apples_agent-3_mean: 91.16
    apples_agent-3_min: 31
    apples_agent-4_max: 83
    apples_agent-4_mean: 5.1
    apples_agent-4_min: 0
    apples_agent-5_max: 136
    apples_agent-5_mean: 73.16
    apples_agent-5_min: 17
    cleaning_beam_agent-0_max: 518
    cleaning_beam_agent-0_mean: 299.53
    cleaning_beam_agent-0_min: 179
    cleaning_beam_agent-1_max: 423
    cleaning_beam_agent-1_mean: 282.98
    cleaning_beam_agent-1_min: 86
    cleaning_beam_agent-2_max: 454
    cleaning_beam_agent-2_mean: 273.89
    cleaning_beam_agent-2_min: 102
    cleaning_beam_agent-3_max: 128
    cleaning_beam_agent-3_mean: 61.73
    cleaning_beam_agent-3_min: 29
    cleaning_beam_agent-4_max: 509
    cleaning_beam_agent-4_mean: 303.66
    cleaning_beam_agent-4_min: 71
    cleaning_beam_agent-5_max: 359
    cleaning_beam_agent-5_mean: 83.09
    cleaning_beam_agent-5_min: 20
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 12
    fire_beam_agent-5_mean: 0.18
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-42-02
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 755.9999999999889
  episode_reward_mean: 531.5400000000029
  episode_reward_min: 206.9999999999985
  episodes_this_iter: 96
  episodes_total: 8448
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20206.46
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007388351950794458
        entropy: 1.318190097808838
        entropy_coeff: 0.0017600000137463212
        kl: 0.014198310673236847
        model: {}
        policy_loss: -0.029495535418391228
        total_loss: -0.02904978021979332
        vf_explained_var: 0.03308592736721039
        vf_loss: 13.4593505859375
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007388351950794458
        entropy: 1.2191669940948486
        entropy_coeff: 0.0017600000137463212
        kl: 0.018838826566934586
        model: {}
        policy_loss: -0.03491687402129173
        total_loss: -0.033855266869068146
        vf_explained_var: 0.04931412637233734
        vf_loss: 13.234639167785645
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007388351950794458
        entropy: 1.2482235431671143
        entropy_coeff: 0.0017600000137463212
        kl: 0.017342248931527138
        model: {}
        policy_loss: -0.03334873914718628
        total_loss: -0.03248923271894455
        vf_explained_var: 0.05016802251338959
        vf_loss: 13.2215576171875
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007388351950794458
        entropy: 0.95697420835495
        entropy_coeff: 0.0017600000137463212
        kl: 0.012883586809039116
        model: {}
        policy_loss: -0.02692067064344883
        total_loss: -0.02622211165726185
        vf_explained_var: 0.21344637870788574
        vf_loss: 10.94472599029541
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007388351950794458
        entropy: 1.1396843194961548
        entropy_coeff: 0.0017600000137463212
        kl: 0.01548643410205841
        model: {}
        policy_loss: -0.034314822405576706
        total_loss: -0.03359429910778999
        vf_explained_var: 0.1538432538509369
        vf_loss: 11.77720832824707
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007388351950794458
        entropy: 1.0772297382354736
        entropy_coeff: 0.0017600000137463212
        kl: 0.017529865726828575
        model: {}
        policy_loss: -0.038480259478092194
        total_loss: -0.03745032474398613
        vf_explained_var: 0.15729527175426483
        vf_loss: 11.728680610656738
    load_time_ms: 13844.642
    num_steps_sampled: 8448000
    num_steps_trained: 8448000
    sample_time_ms: 91779.847
    update_time_ms: 26.916
  iterations_since_restore: 68
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.621857923497263
    ram_util_percent: 13.324043715846996
  pid: 4061
  policy_reward_max:
    agent-0: 126.00000000000051
    agent-1: 126.00000000000051
    agent-2: 126.00000000000051
    agent-3: 126.00000000000051
    agent-4: 126.00000000000051
    agent-5: 126.00000000000051
  policy_reward_mean:
    agent-0: 88.59000000000019
    agent-1: 88.59000000000019
    agent-2: 88.59000000000019
    agent-3: 88.59000000000019
    agent-4: 88.59000000000019
    agent-5: 88.59000000000019
  policy_reward_min:
    agent-0: 34.50000000000003
    agent-1: 34.50000000000003
    agent-2: 34.50000000000003
    agent-3: 34.50000000000003
    agent-4: 34.50000000000003
    agent-5: 34.50000000000003
  sampler_perf:
    mean_env_wait_ms: 23.45190581087806
    mean_inference_ms: 12.330547211685671
    mean_processing_ms: 50.94437269499937
  time_since_restore: 9404.463958263397
  time_this_iter_s: 128.46865820884705
  time_total_s: 12615.527644395828
  timestamp: 1637026922
  timesteps_since_restore: 6528000
  timesteps_this_iter: 96000
  timesteps_total: 8448000
  training_iteration: 88
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     88 |          12615.5 | 8448000 |   531.54 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 76
    apples_agent-0_mean: 5.12
    apples_agent-0_min: 0
    apples_agent-1_max: 71
    apples_agent-1_mean: 18.19
    apples_agent-1_min: 0
    apples_agent-2_max: 90
    apples_agent-2_mean: 8.86
    apples_agent-2_min: 0
    apples_agent-3_max: 184
    apples_agent-3_mean: 91.71
    apples_agent-3_min: 32
    apples_agent-4_max: 124
    apples_agent-4_mean: 6.49
    apples_agent-4_min: 0
    apples_agent-5_max: 177
    apples_agent-5_mean: 74.16
    apples_agent-5_min: 5
    cleaning_beam_agent-0_max: 469
    cleaning_beam_agent-0_mean: 281.86
    cleaning_beam_agent-0_min: 118
    cleaning_beam_agent-1_max: 399
    cleaning_beam_agent-1_mean: 267.62
    cleaning_beam_agent-1_min: 137
    cleaning_beam_agent-2_max: 396
    cleaning_beam_agent-2_mean: 267.38
    cleaning_beam_agent-2_min: 105
    cleaning_beam_agent-3_max: 114
    cleaning_beam_agent-3_mean: 52.51
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 473
    cleaning_beam_agent-4_mean: 322.03
    cleaning_beam_agent-4_min: 92
    cleaning_beam_agent-5_max: 422
    cleaning_beam_agent-5_mean: 93.07
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.08
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-44-11
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 791.9999999999895
  episode_reward_mean: 550.3400000000026
  episode_reward_min: 229.9999999999972
  episodes_this_iter: 96
  episodes_total: 8544
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20227.153
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007328448118641973
        entropy: 1.3159518241882324
        entropy_coeff: 0.0017600000137463212
        kl: 0.016061192378401756
        model: {}
        policy_loss: -0.031091269105672836
        total_loss: -0.030547354370355606
        vf_explained_var: 0.08718669414520264
        vf_loss: 12.538684844970703
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007328448118641973
        entropy: 1.2267343997955322
        entropy_coeff: 0.0017600000137463212
        kl: 0.019824128597974777
        model: {}
        policy_loss: -0.0321493037045002
        total_loss: -0.03102504462003708
        vf_explained_var: 0.053339987993240356
        vf_loss: 13.008964538574219
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007328448118641973
        entropy: 1.255235195159912
        entropy_coeff: 0.0017600000137463212
        kl: 0.015842216089367867
        model: {}
        policy_loss: -0.03321453556418419
        total_loss: -0.03256988525390625
        vf_explained_var: 0.07555676996707916
        vf_loss: 12.696416854858398
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007328448118641973
        entropy: 0.9271793365478516
        entropy_coeff: 0.0017600000137463212
        kl: 0.01217055507004261
        model: {}
        policy_loss: -0.02482619509100914
        total_loss: -0.024133970960974693
        vf_explained_var: 0.1935291886329651
        vf_loss: 11.07004165649414
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007328448118641973
        entropy: 1.1401629447937012
        entropy_coeff: 0.0017600000137463212
        kl: 0.016099222004413605
        model: {}
        policy_loss: -0.03438530117273331
        total_loss: -0.03356187045574188
        vf_explained_var: 0.11136490106582642
        vf_loss: 12.201972961425781
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007328448118641973
        entropy: 1.0990139245986938
        entropy_coeff: 0.0017600000137463212
        kl: 0.01680842973291874
        model: {}
        policy_loss: -0.03825123608112335
        total_loss: -0.03734118491411209
        vf_explained_var: 0.15337227284908295
        vf_loss: 11.634747505187988
    load_time_ms: 13864.772
    num_steps_sampled: 8544000
    num_steps_trained: 8544000
    sample_time_ms: 92283.323
    update_time_ms: 26.728
  iterations_since_restore: 69
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.61857923497268
    ram_util_percent: 13.34972677595629
  pid: 4061
  policy_reward_max:
    agent-0: 132.0000000000004
    agent-1: 132.0000000000004
    agent-2: 132.0000000000004
    agent-3: 132.0000000000004
    agent-4: 132.0000000000004
    agent-5: 132.0000000000004
  policy_reward_mean:
    agent-0: 91.72333333333353
    agent-1: 91.72333333333353
    agent-2: 91.72333333333353
    agent-3: 91.72333333333353
    agent-4: 91.72333333333353
    agent-5: 91.72333333333353
  policy_reward_min:
    agent-0: 38.33333333333333
    agent-1: 38.33333333333333
    agent-2: 38.33333333333333
    agent-3: 38.33333333333333
    agent-4: 38.33333333333333
    agent-5: 38.33333333333333
  sampler_perf:
    mean_env_wait_ms: 23.467052724248287
    mean_inference_ms: 12.333133473750284
    mean_processing_ms: 50.96180181454652
  time_since_restore: 9532.910426616669
  time_this_iter_s: 128.44646835327148
  time_total_s: 12743.9741127491
  timestamp: 1637027051
  timesteps_since_restore: 6624000
  timesteps_this_iter: 96000
  timesteps_total: 8544000
  training_iteration: 89
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     89 |            12744 | 8544000 |   550.34 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 91
    apples_agent-0_mean: 5.94
    apples_agent-0_min: 0
    apples_agent-1_max: 81
    apples_agent-1_mean: 22.49
    apples_agent-1_min: 0
    apples_agent-2_max: 70
    apples_agent-2_mean: 10.27
    apples_agent-2_min: 0
    apples_agent-3_max: 149
    apples_agent-3_mean: 79.73
    apples_agent-3_min: 16
    apples_agent-4_max: 97
    apples_agent-4_mean: 6.9
    apples_agent-4_min: 0
    apples_agent-5_max: 177
    apples_agent-5_mean: 77.75
    apples_agent-5_min: 2
    cleaning_beam_agent-0_max: 432
    cleaning_beam_agent-0_mean: 268.43
    cleaning_beam_agent-0_min: 108
    cleaning_beam_agent-1_max: 400
    cleaning_beam_agent-1_mean: 261.69
    cleaning_beam_agent-1_min: 141
    cleaning_beam_agent-2_max: 444
    cleaning_beam_agent-2_mean: 266.0
    cleaning_beam_agent-2_min: 110
    cleaning_beam_agent-3_max: 144
    cleaning_beam_agent-3_mean: 54.92
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 562
    cleaning_beam_agent-4_mean: 321.06
    cleaning_beam_agent-4_min: 67
    cleaning_beam_agent-5_max: 459
    cleaning_beam_agent-5_mean: 105.38
    cleaning_beam_agent-5_min: 16
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.08
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-46-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 793.9999999999905
  episode_reward_mean: 539.3300000000027
  episode_reward_min: 230.99999999999676
  episodes_this_iter: 96
  episodes_total: 8640
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20211.412
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007268544286489487
        entropy: 1.2907911539077759
        entropy_coeff: 0.0017600000137463212
        kl: 0.014151793904602528
        model: {}
        policy_loss: -0.03185370936989784
        total_loss: -0.03148370608687401
        vf_explained_var: 0.0756990909576416
        vf_loss: 12.266183853149414
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007268544286489487
        entropy: 1.2074556350708008
        entropy_coeff: 0.0017600000137463212
        kl: 0.016171306371688843
        model: {}
        policy_loss: -0.03461570292711258
        total_loss: -0.03386727720499039
        vf_explained_var: 0.05298618972301483
        vf_loss: 12.56413459777832
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007268544286489487
        entropy: 1.244195580482483
        entropy_coeff: 0.0017600000137463212
        kl: 0.017982982099056244
        model: {}
        policy_loss: -0.03221840783953667
        total_loss: -0.03139577805995941
        vf_explained_var: 0.08570623397827148
        vf_loss: 12.141145706176758
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007268544286489487
        entropy: 0.9444279670715332
        entropy_coeff: 0.0017600000137463212
        kl: 0.012183362618088722
        model: {}
        policy_loss: -0.025355027988553047
        total_loss: -0.02473415806889534
        vf_explained_var: 0.19694921374320984
        vf_loss: 10.647242546081543
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007268544286489487
        entropy: 1.1023499965667725
        entropy_coeff: 0.0017600000137463212
        kl: 0.017910297960042953
        model: {}
        policy_loss: -0.03244903311133385
        total_loss: -0.031447723507881165
        vf_explained_var: 0.131937175989151
        vf_loss: 11.504165649414062
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007268544286489487
        entropy: 1.0851285457611084
        entropy_coeff: 0.0017600000137463212
        kl: 0.01612519845366478
        model: {}
        policy_loss: -0.038019657135009766
        total_loss: -0.037164561450481415
        vf_explained_var: 0.13290148973464966
        vf_loss: 11.524023056030273
    load_time_ms: 13979.046
    num_steps_sampled: 8640000
    num_steps_trained: 8640000
    sample_time_ms: 92773.207
    update_time_ms: 26.63
  iterations_since_restore: 70
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 19.17783783783784
    ram_util_percent: 11.68108108108108
  pid: 4061
  policy_reward_max:
    agent-0: 132.33333333333377
    agent-1: 132.33333333333377
    agent-2: 132.33333333333377
    agent-3: 132.33333333333377
    agent-4: 132.33333333333377
    agent-5: 132.33333333333377
  policy_reward_mean:
    agent-0: 89.88833333333353
    agent-1: 89.88833333333353
    agent-2: 89.88833333333353
    agent-3: 89.88833333333353
    agent-4: 89.88833333333353
    agent-5: 89.88833333333353
  policy_reward_min:
    agent-0: 38.500000000000014
    agent-1: 38.500000000000014
    agent-2: 38.500000000000014
    agent-3: 38.500000000000014
    agent-4: 38.500000000000014
    agent-5: 38.500000000000014
  sampler_perf:
    mean_env_wait_ms: 23.480354136665966
    mean_inference_ms: 12.336289927716457
    mean_processing_ms: 50.98066468820427
  time_since_restore: 9662.55921959877
  time_this_iter_s: 129.64879298210144
  time_total_s: 12873.622905731201
  timestamp: 1637027180
  timesteps_since_restore: 6720000
  timesteps_this_iter: 96000
  timesteps_total: 8640000
  training_iteration: 90
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     90 |          12873.6 | 8640000 |   539.33 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 85
    apples_agent-0_mean: 6.45
    apples_agent-0_min: 0
    apples_agent-1_max: 64
    apples_agent-1_mean: 18.29
    apples_agent-1_min: 0
    apples_agent-2_max: 99
    apples_agent-2_mean: 11.75
    apples_agent-2_min: 0
    apples_agent-3_max: 191
    apples_agent-3_mean: 87.57
    apples_agent-3_min: 33
    apples_agent-4_max: 75
    apples_agent-4_mean: 4.73
    apples_agent-4_min: 0
    apples_agent-5_max: 172
    apples_agent-5_mean: 78.98
    apples_agent-5_min: 16
    cleaning_beam_agent-0_max: 463
    cleaning_beam_agent-0_mean: 276.69
    cleaning_beam_agent-0_min: 99
    cleaning_beam_agent-1_max: 422
    cleaning_beam_agent-1_mean: 271.94
    cleaning_beam_agent-1_min: 135
    cleaning_beam_agent-2_max: 480
    cleaning_beam_agent-2_mean: 262.32
    cleaning_beam_agent-2_min: 117
    cleaning_beam_agent-3_max: 121
    cleaning_beam_agent-3_mean: 54.85
    cleaning_beam_agent-3_min: 20
    cleaning_beam_agent-4_max: 474
    cleaning_beam_agent-4_mean: 298.26
    cleaning_beam_agent-4_min: 100
    cleaning_beam_agent-5_max: 439
    cleaning_beam_agent-5_mean: 101.0
    cleaning_beam_agent-5_min: 22
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 23
    fire_beam_agent-5_mean: 0.39
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-48-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 740.9999999999953
  episode_reward_mean: 539.2000000000024
  episode_reward_min: 228.9999999999971
  episodes_this_iter: 96
  episodes_total: 8736
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20220.612
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007208639872260392
        entropy: 1.3047008514404297
        entropy_coeff: 0.0017600000137463212
        kl: 0.01425891648977995
        model: {}
        policy_loss: -0.030993342399597168
        total_loss: -0.030617350712418556
        vf_explained_var: 0.07003733515739441
        vf_loss: 12.4637451171875
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007208639872260392
        entropy: 1.1965209245681763
        entropy_coeff: 0.0017600000137463212
        kl: 0.016957754269242287
        model: {}
        policy_loss: -0.032908711582422256
        total_loss: -0.03201378509402275
        vf_explained_var: 0.02590779960155487
        vf_loss: 13.050271034240723
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007208639872260392
        entropy: 1.238976240158081
        entropy_coeff: 0.0017600000137463212
        kl: 0.01685323193669319
        model: {}
        policy_loss: -0.03299893066287041
        total_loss: -0.03221697732806206
        vf_explained_var: 0.046871185302734375
        vf_loss: 12.772302627563477
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007208639872260392
        entropy: 0.9540195465087891
        entropy_coeff: 0.0017600000137463212
        kl: 0.012709561735391617
        model: {}
        policy_loss: -0.02487000823020935
        total_loss: -0.024186013266444206
        vf_explained_var: 0.1843147873878479
        vf_loss: 10.921135902404785
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007208639872260392
        entropy: 1.1225872039794922
        entropy_coeff: 0.0017600000137463212
        kl: 0.01660877838730812
        model: {}
        policy_loss: -0.035607293248176575
        total_loss: -0.03474286198616028
        vf_explained_var: 0.11916948854923248
        vf_loss: 11.79308032989502
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007208639872260392
        entropy: 1.1115503311157227
        entropy_coeff: 0.0017600000137463212
        kl: 0.016492072492837906
        model: {}
        policy_loss: -0.038742270320653915
        total_loss: -0.03788881003856659
        vf_explained_var: 0.13428886234760284
        vf_loss: 11.605802536010742
    load_time_ms: 14012.072
    num_steps_sampled: 8736000
    num_steps_trained: 8736000
    sample_time_ms: 93029.738
    update_time_ms: 26.707
  iterations_since_restore: 71
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.223463687150838
    ram_util_percent: 9.589944134078214
  pid: 4061
  policy_reward_max:
    agent-0: 123.5000000000004
    agent-1: 123.5000000000004
    agent-2: 123.5000000000004
    agent-3: 123.5000000000004
    agent-4: 123.5000000000004
    agent-5: 123.5000000000004
  policy_reward_mean:
    agent-0: 89.86666666666684
    agent-1: 89.86666666666684
    agent-2: 89.86666666666684
    agent-3: 89.86666666666684
    agent-4: 89.86666666666684
    agent-5: 89.86666666666684
  policy_reward_min:
    agent-0: 38.16666666666667
    agent-1: 38.16666666666667
    agent-2: 38.16666666666667
    agent-3: 38.16666666666667
    agent-4: 38.16666666666667
    agent-5: 38.16666666666667
  sampler_perf:
    mean_env_wait_ms: 23.48567439576503
    mean_inference_ms: 12.337339366023707
    mean_processing_ms: 50.981428280966995
  time_since_restore: 9787.774664878845
  time_this_iter_s: 125.21544528007507
  time_total_s: 12998.838351011276
  timestamp: 1637027306
  timesteps_since_restore: 6816000
  timesteps_this_iter: 96000
  timesteps_total: 8736000
  training_iteration: 91
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     91 |          12998.8 | 8736000 |    539.2 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 41
    apples_agent-0_mean: 4.08
    apples_agent-0_min: 0
    apples_agent-1_max: 99
    apples_agent-1_mean: 19.23
    apples_agent-1_min: 0
    apples_agent-2_max: 79
    apples_agent-2_mean: 10.86
    apples_agent-2_min: 0
    apples_agent-3_max: 198
    apples_agent-3_mean: 93.03
    apples_agent-3_min: 35
    apples_agent-4_max: 78
    apples_agent-4_mean: 3.96
    apples_agent-4_min: 0
    apples_agent-5_max: 163
    apples_agent-5_mean: 81.28
    apples_agent-5_min: 7
    cleaning_beam_agent-0_max: 490
    cleaning_beam_agent-0_mean: 284.82
    cleaning_beam_agent-0_min: 84
    cleaning_beam_agent-1_max: 440
    cleaning_beam_agent-1_mean: 281.4
    cleaning_beam_agent-1_min: 157
    cleaning_beam_agent-2_max: 407
    cleaning_beam_agent-2_mean: 279.7
    cleaning_beam_agent-2_min: 64
    cleaning_beam_agent-3_max: 111
    cleaning_beam_agent-3_mean: 50.04
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 497
    cleaning_beam_agent-4_mean: 323.71
    cleaning_beam_agent-4_min: 123
    cleaning_beam_agent-5_max: 335
    cleaning_beam_agent-5_mean: 103.77
    cleaning_beam_agent-5_min: 21
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 4
    fire_beam_agent-5_mean: 0.15
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-50-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 787.9999999999749
  episode_reward_mean: 567.2000000000012
  episode_reward_min: 235.99999999999767
  episodes_this_iter: 96
  episodes_total: 8832
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20187.642
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007148736040107906
        entropy: 1.2879421710968018
        entropy_coeff: 0.0017600000137463212
        kl: 0.014518070966005325
        model: {}
        policy_loss: -0.03008377179503441
        total_loss: -0.029632478952407837
        vf_explained_var: 0.08506378531455994
        vf_loss: 12.662607192993164
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007148736040107906
        entropy: 1.184514045715332
        entropy_coeff: 0.0017600000137463212
        kl: 0.016879167407751083
        model: {}
        policy_loss: -0.034213632345199585
        total_loss: -0.03331097960472107
        vf_explained_var: 0.061743319034576416
        vf_loss: 12.99482536315918
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007148736040107906
        entropy: 1.2303898334503174
        entropy_coeff: 0.0017600000137463212
        kl: 0.01722622662782669
        model: {}
        policy_loss: -0.03427422419190407
        total_loss: -0.033391766250133514
        vf_explained_var: 0.04276268184185028
        vf_loss: 13.253255844116211
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007148736040107906
        entropy: 0.9144287109375
        entropy_coeff: 0.0017600000137463212
        kl: 0.012769000604748726
        model: {}
        policy_loss: -0.02622896060347557
        total_loss: -0.025431524962186813
        vf_explained_var: 0.18374070525169373
        vf_loss: 11.29928207397461
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007148736040107906
        entropy: 1.105266809463501
        entropy_coeff: 0.0017600000137463212
        kl: 0.015251369215548038
        model: {}
        policy_loss: -0.03388248383998871
        total_loss: -0.03313519060611725
        vf_explained_var: 0.15646353363990784
        vf_loss: 11.674293518066406
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007148736040107906
        entropy: 1.1037228107452393
        entropy_coeff: 0.0017600000137463212
        kl: 0.017282281070947647
        model: {}
        policy_loss: -0.0395054928958416
        total_loss: -0.03852237015962601
        vf_explained_var: 0.1352655440568924
        vf_loss: 11.974544525146484
    load_time_ms: 13991.12
    num_steps_sampled: 8832000
    num_steps_trained: 8832000
    sample_time_ms: 93120.391
    update_time_ms: 26.457
  iterations_since_restore: 72
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 16.150837988826815
    ram_util_percent: 9.58491620111732
  pid: 4061
  policy_reward_max:
    agent-0: 131.33333333333388
    agent-1: 131.33333333333388
    agent-2: 131.33333333333388
    agent-3: 131.33333333333388
    agent-4: 131.33333333333388
    agent-5: 131.33333333333388
  policy_reward_mean:
    agent-0: 94.53333333333357
    agent-1: 94.53333333333357
    agent-2: 94.53333333333357
    agent-3: 94.53333333333357
    agent-4: 94.53333333333357
    agent-5: 94.53333333333357
  policy_reward_min:
    agent-0: 39.33333333333329
    agent-1: 39.33333333333329
    agent-2: 39.33333333333329
    agent-3: 39.33333333333329
    agent-4: 39.33333333333329
    agent-5: 39.33333333333329
  sampler_perf:
    mean_env_wait_ms: 23.492155274953184
    mean_inference_ms: 12.338490743227414
    mean_processing_ms: 50.98126322358094
  time_since_restore: 9913.456922531128
  time_this_iter_s: 125.68225765228271
  time_total_s: 13124.520608663559
  timestamp: 1637027432
  timesteps_since_restore: 6912000
  timesteps_this_iter: 96000
  timesteps_total: 8832000
  training_iteration: 92
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     92 |          13124.5 | 8832000 |    567.2 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 62
    apples_agent-0_mean: 5.09
    apples_agent-0_min: 0
    apples_agent-1_max: 82
    apples_agent-1_mean: 17.74
    apples_agent-1_min: 0
    apples_agent-2_max: 127
    apples_agent-2_mean: 12.12
    apples_agent-2_min: 0
    apples_agent-3_max: 168
    apples_agent-3_mean: 90.15
    apples_agent-3_min: 21
    apples_agent-4_max: 45
    apples_agent-4_mean: 2.65
    apples_agent-4_min: 0
    apples_agent-5_max: 163
    apples_agent-5_mean: 75.47
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 467
    cleaning_beam_agent-0_mean: 265.98
    cleaning_beam_agent-0_min: 124
    cleaning_beam_agent-1_max: 380
    cleaning_beam_agent-1_mean: 275.6
    cleaning_beam_agent-1_min: 133
    cleaning_beam_agent-2_max: 401
    cleaning_beam_agent-2_mean: 267.75
    cleaning_beam_agent-2_min: 57
    cleaning_beam_agent-3_max: 173
    cleaning_beam_agent-3_mean: 51.39
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 497
    cleaning_beam_agent-4_mean: 318.63
    cleaning_beam_agent-4_min: 113
    cleaning_beam_agent-5_max: 434
    cleaning_beam_agent-5_mean: 113.19
    cleaning_beam_agent-5_min: 18
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 21
    fire_beam_agent-5_mean: 0.4
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-52-39
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 787.9999999999749
  episode_reward_mean: 545.6500000000027
  episode_reward_min: 161.9999999999999
  episodes_this_iter: 96
  episodes_total: 8928
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20190.041
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000708883220795542
        entropy: 1.2769895792007446
        entropy_coeff: 0.0017600000137463212
        kl: 0.013540051877498627
        model: {}
        policy_loss: -0.03049338236451149
        total_loss: -0.030078131705522537
        vf_explained_var: 0.09978395700454712
        vf_loss: 13.0874605178833
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000708883220795542
        entropy: 1.1735719442367554
        entropy_coeff: 0.0017600000137463212
        kl: 0.017319489270448685
        model: {}
        policy_loss: -0.0328117199242115
        total_loss: -0.031727295368909836
        vf_explained_var: 0.025321975350379944
        vf_loss: 14.179591178894043
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000708883220795542
        entropy: 1.2336400747299194
        entropy_coeff: 0.0017600000137463212
        kl: 0.016771916300058365
        model: {}
        policy_loss: -0.03429326415061951
        total_loss: -0.033407799899578094
        vf_explained_var: 0.051013797521591187
        vf_loss: 13.794809341430664
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000708883220795542
        entropy: 0.9230788350105286
        entropy_coeff: 0.0017600000137463212
        kl: 0.01248969417065382
        model: {}
        policy_loss: -0.026253731921315193
        total_loss: -0.025450551882386208
        vf_explained_var: 0.1891600489616394
        vf_loss: 11.788326263427734
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000708883220795542
        entropy: 1.10431706905365
        entropy_coeff: 0.0017600000137463212
        kl: 0.01590532623231411
        model: {}
        policy_loss: -0.036079131066799164
        total_loss: -0.035172805190086365
        vf_explained_var: 0.13417448103427887
        vf_loss: 12.593883514404297
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000708883220795542
        entropy: 1.1075975894927979
        entropy_coeff: 0.0017600000137463212
        kl: 0.017355747520923615
        model: {}
        policy_loss: -0.03974480181932449
        total_loss: -0.03870602697134018
        vf_explained_var: 0.13877999782562256
        vf_loss: 12.525716781616211
    load_time_ms: 13945.576
    num_steps_sampled: 8928000
    num_steps_trained: 8928000
    sample_time_ms: 93127.546
    update_time_ms: 24.35
  iterations_since_restore: 73
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.134806629834257
    ram_util_percent: 9.64254143646409
  pid: 4061
  policy_reward_max:
    agent-0: 131.33333333333388
    agent-1: 131.33333333333388
    agent-2: 131.33333333333388
    agent-3: 131.33333333333388
    agent-4: 131.33333333333388
    agent-5: 131.33333333333388
  policy_reward_mean:
    agent-0: 90.94166666666689
    agent-1: 90.94166666666689
    agent-2: 90.94166666666689
    agent-3: 90.94166666666689
    agent-4: 90.94166666666689
    agent-5: 90.94166666666689
  policy_reward_min:
    agent-0: 27.00000000000003
    agent-1: 27.00000000000003
    agent-2: 27.00000000000003
    agent-3: 27.00000000000003
    agent-4: 27.00000000000003
    agent-5: 27.00000000000003
  sampler_perf:
    mean_env_wait_ms: 23.49680561881834
    mean_inference_ms: 12.340022867454893
    mean_processing_ms: 50.982436927100245
  time_since_restore: 10040.289299964905
  time_this_iter_s: 126.83237743377686
  time_total_s: 13251.352986097336
  timestamp: 1637027559
  timesteps_since_restore: 7008000
  timesteps_this_iter: 96000
  timesteps_total: 8928000
  training_iteration: 93
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     93 |          13251.4 | 8928000 |   545.65 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 59
    apples_agent-0_mean: 6.15
    apples_agent-0_min: 0
    apples_agent-1_max: 66
    apples_agent-1_mean: 19.14
    apples_agent-1_min: 0
    apples_agent-2_max: 109
    apples_agent-2_mean: 10.51
    apples_agent-2_min: 0
    apples_agent-3_max: 160
    apples_agent-3_mean: 86.39
    apples_agent-3_min: 38
    apples_agent-4_max: 80
    apples_agent-4_mean: 3.88
    apples_agent-4_min: 0
    apples_agent-5_max: 152
    apples_agent-5_mean: 77.46
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 372
    cleaning_beam_agent-0_mean: 257.16
    cleaning_beam_agent-0_min: 133
    cleaning_beam_agent-1_max: 397
    cleaning_beam_agent-1_mean: 267.12
    cleaning_beam_agent-1_min: 128
    cleaning_beam_agent-2_max: 443
    cleaning_beam_agent-2_mean: 283.19
    cleaning_beam_agent-2_min: 102
    cleaning_beam_agent-3_max: 173
    cleaning_beam_agent-3_mean: 55.23
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 448
    cleaning_beam_agent-4_mean: 315.19
    cleaning_beam_agent-4_min: 164
    cleaning_beam_agent-5_max: 359
    cleaning_beam_agent-5_mean: 92.42
    cleaning_beam_agent-5_min: 14
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.12
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-54-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 797.9999999999911
  episode_reward_mean: 547.0400000000024
  episode_reward_min: 240.99999999999653
  episodes_this_iter: 96
  episodes_total: 9024
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20155.366
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007028927793726325
        entropy: 1.2898914813995361
        entropy_coeff: 0.0017600000137463212
        kl: 0.016169285401701927
        model: {}
        policy_loss: -0.03037021867930889
        total_loss: -0.02967333421111107
        vf_explained_var: 0.06155115365982056
        vf_loss: 13.501676559448242
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007028927793726325
        entropy: 1.1715511083602905
        entropy_coeff: 0.0017600000137463212
        kl: 0.01679355464875698
        model: {}
        policy_loss: -0.03282935917377472
        total_loss: -0.03184257447719574
        vf_explained_var: 0.048862114548683167
        vf_loss: 13.693573951721191
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007028927793726325
        entropy: 1.2155418395996094
        entropy_coeff: 0.0017600000137463212
        kl: 0.016978882253170013
        model: {}
        policy_loss: -0.033457063138484955
        total_loss: -0.03255544975399971
        vf_explained_var: 0.06697463989257812
        vf_loss: 13.430740356445312
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007028927793726325
        entropy: 0.950817346572876
        entropy_coeff: 0.0017600000137463212
        kl: 0.014766774140298367
        model: {}
        policy_loss: -0.0262946505099535
        total_loss: -0.025332119315862656
        vf_explained_var: 0.19453702867031097
        vf_loss: 11.59290885925293
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007028927793726325
        entropy: 1.1100994348526
        entropy_coeff: 0.0017600000137463212
        kl: 0.016785698011517525
        model: {}
        policy_loss: -0.03458014130592346
        total_loss: -0.033676862716674805
        vf_explained_var: 0.1809731125831604
        vf_loss: 11.784858703613281
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0007028927793726325
        entropy: 1.0965991020202637
        entropy_coeff: 0.0017600000137463212
        kl: 0.018139522522687912
        model: {}
        policy_loss: -0.039287105202674866
        total_loss: -0.038188789039850235
        vf_explained_var: 0.15676479041576385
        vf_loss: 12.143808364868164
    load_time_ms: 13867.163
    num_steps_sampled: 9024000
    num_steps_trained: 9024000
    sample_time_ms: 93362.628
    update_time_ms: 24.115
  iterations_since_restore: 74
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.137569060773478
    ram_util_percent: 9.572375690607737
  pid: 4061
  policy_reward_max:
    agent-0: 133.00000000000014
    agent-1: 133.00000000000014
    agent-2: 133.00000000000014
    agent-3: 133.00000000000014
    agent-4: 133.00000000000014
    agent-5: 133.00000000000014
  policy_reward_mean:
    agent-0: 91.17333333333355
    agent-1: 91.17333333333355
    agent-2: 91.17333333333355
    agent-3: 91.17333333333355
    agent-4: 91.17333333333355
    agent-5: 91.17333333333355
  policy_reward_min:
    agent-0: 40.166666666666636
    agent-1: 40.166666666666636
    agent-2: 40.166666666666636
    agent-3: 40.166666666666636
    agent-4: 40.166666666666636
    agent-5: 40.166666666666636
  sampler_perf:
    mean_env_wait_ms: 23.50195488063186
    mean_inference_ms: 12.341261075294645
    mean_processing_ms: 50.98931523809236
  time_since_restore: 10167.820920467377
  time_this_iter_s: 127.53162050247192
  time_total_s: 13378.884606599808
  timestamp: 1637027686
  timesteps_since_restore: 7104000
  timesteps_this_iter: 96000
  timesteps_total: 9024000
  training_iteration: 94
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     94 |          13378.9 | 9024000 |   547.04 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 66
    apples_agent-0_mean: 7.14
    apples_agent-0_min: 0
    apples_agent-1_max: 74
    apples_agent-1_mean: 21.52
    apples_agent-1_min: 0
    apples_agent-2_max: 103
    apples_agent-2_mean: 9.86
    apples_agent-2_min: 0
    apples_agent-3_max: 186
    apples_agent-3_mean: 90.82
    apples_agent-3_min: 22
    apples_agent-4_max: 107
    apples_agent-4_mean: 4.49
    apples_agent-4_min: 0
    apples_agent-5_max: 166
    apples_agent-5_mean: 80.44
    apples_agent-5_min: 9
    cleaning_beam_agent-0_max: 376
    cleaning_beam_agent-0_mean: 270.8
    cleaning_beam_agent-0_min: 131
    cleaning_beam_agent-1_max: 380
    cleaning_beam_agent-1_mean: 274.96
    cleaning_beam_agent-1_min: 174
    cleaning_beam_agent-2_max: 380
    cleaning_beam_agent-2_mean: 263.67
    cleaning_beam_agent-2_min: 127
    cleaning_beam_agent-3_max: 119
    cleaning_beam_agent-3_mean: 52.57
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 565
    cleaning_beam_agent-4_mean: 335.54
    cleaning_beam_agent-4_min: 142
    cleaning_beam_agent-5_max: 320
    cleaning_beam_agent-5_mean: 99.34
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 19
    fire_beam_agent-5_mean: 0.39
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-56-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 793.9999999999791
  episode_reward_mean: 571.9300000000004
  episode_reward_min: 157.99999999999977
  episodes_this_iter: 96
  episodes_total: 9120
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20143.414
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006969023961573839
        entropy: 1.266600251197815
        entropy_coeff: 0.0017600000137463212
        kl: 0.015264114364981651
        model: {}
        policy_loss: -0.029169432818889618
        total_loss: -0.02842804603278637
        vf_explained_var: 0.0955992192029953
        vf_loss: 14.441938400268555
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006969023961573839
        entropy: 1.1630737781524658
        entropy_coeff: 0.0017600000137463212
        kl: 0.01586579903960228
        model: {}
        policy_loss: -0.03334289416670799
        total_loss: -0.03231098875403404
        vf_explained_var: 0.06545910239219666
        vf_loss: 14.92332649230957
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006969023961573839
        entropy: 1.2259719371795654
        entropy_coeff: 0.0017600000137463212
        kl: 0.016594281420111656
        model: {}
        policy_loss: -0.03378625214099884
        total_loss: -0.03274908289313316
        vf_explained_var: 0.038411930203437805
        vf_loss: 15.354524612426758
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006969023961573839
        entropy: 0.8903607130050659
        entropy_coeff: 0.0017600000137463212
        kl: 0.012614361010491848
        model: {}
        policy_loss: -0.024781862273812294
        total_loss: -0.02372576855123043
        vf_explained_var: 0.1482773721218109
        vf_loss: 13.61690902709961
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006969023961573839
        entropy: 1.113707184791565
        entropy_coeff: 0.0017600000137463212
        kl: 0.01588292233645916
        model: {}
        policy_loss: -0.03365599736571312
        total_loss: -0.0326191745698452
        vf_explained_var: 0.11828655004501343
        vf_loss: 14.08657455444336
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006969023961573839
        entropy: 1.0811518430709839
        entropy_coeff: 0.0017600000137463212
        kl: 0.017207521945238113
        model: {}
        policy_loss: -0.038868069648742676
        total_loss: -0.03771552816033363
        vf_explained_var: 0.1638789176940918
        vf_loss: 13.346147537231445
    load_time_ms: 13833.527
    num_steps_sampled: 9120000
    num_steps_trained: 9120000
    sample_time_ms: 93035.566
    update_time_ms: 23.418
  iterations_since_restore: 75
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.22752808988764
    ram_util_percent: 9.538202247191013
  pid: 4061
  policy_reward_max:
    agent-0: 132.33333333333366
    agent-1: 132.33333333333366
    agent-2: 132.33333333333366
    agent-3: 132.33333333333366
    agent-4: 132.33333333333366
    agent-5: 132.33333333333366
  policy_reward_mean:
    agent-0: 95.3216666666669
    agent-1: 95.3216666666669
    agent-2: 95.3216666666669
    agent-3: 95.3216666666669
    agent-4: 95.3216666666669
    agent-5: 95.3216666666669
  policy_reward_min:
    agent-0: 26.333333333333368
    agent-1: 26.333333333333368
    agent-2: 26.333333333333368
    agent-3: 26.333333333333368
    agent-4: 26.333333333333368
    agent-5: 26.333333333333368
  sampler_perf:
    mean_env_wait_ms: 23.505734301485294
    mean_inference_ms: 12.341078561685844
    mean_processing_ms: 50.98414076903486
  time_since_restore: 10292.270641326904
  time_this_iter_s: 124.44972085952759
  time_total_s: 13503.334327459335
  timestamp: 1637027811
  timesteps_since_restore: 7200000
  timesteps_this_iter: 96000
  timesteps_total: 9120000
  training_iteration: 95
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     95 |          13503.3 | 9120000 |   571.93 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 60
    apples_agent-0_mean: 6.2
    apples_agent-0_min: 0
    apples_agent-1_max: 81
    apples_agent-1_mean: 18.65
    apples_agent-1_min: 0
    apples_agent-2_max: 214
    apples_agent-2_mean: 17.85
    apples_agent-2_min: 0
    apples_agent-3_max: 181
    apples_agent-3_mean: 87.46
    apples_agent-3_min: 31
    apples_agent-4_max: 74
    apples_agent-4_mean: 5.41
    apples_agent-4_min: 0
    apples_agent-5_max: 162
    apples_agent-5_mean: 74.83
    apples_agent-5_min: 6
    cleaning_beam_agent-0_max: 427
    cleaning_beam_agent-0_mean: 292.51
    cleaning_beam_agent-0_min: 139
    cleaning_beam_agent-1_max: 388
    cleaning_beam_agent-1_mean: 277.6
    cleaning_beam_agent-1_min: 141
    cleaning_beam_agent-2_max: 383
    cleaning_beam_agent-2_mean: 235.6
    cleaning_beam_agent-2_min: 88
    cleaning_beam_agent-3_max: 146
    cleaning_beam_agent-3_mean: 54.3
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 484
    cleaning_beam_agent-4_mean: 332.57
    cleaning_beam_agent-4_min: 153
    cleaning_beam_agent-5_max: 439
    cleaning_beam_agent-5_mean: 113.71
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.07
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 40
    fire_beam_agent-5_mean: 0.79
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-58-54
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 875.9999999999807
  episode_reward_mean: 557.1900000000014
  episode_reward_min: 214.0000000000046
  episodes_this_iter: 96
  episodes_total: 9216
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20152.544
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006909120129421353
        entropy: 1.2369760274887085
        entropy_coeff: 0.0017600000137463212
        kl: 0.014194919727742672
        model: {}
        policy_loss: -0.031176548451185226
        total_loss: -0.03038674406707287
        vf_explained_var: 0.0525105744600296
        vf_loss: 15.473941802978516
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006909120129421353
        entropy: 1.145871877670288
        entropy_coeff: 0.0017600000137463212
        kl: 0.015946492552757263
        model: {}
        policy_loss: -0.03137347102165222
        total_loss: -0.03018660843372345
        vf_explained_var: 0.01601986587047577
        vf_loss: 16.089439392089844
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006909120129421353
        entropy: 1.1904881000518799
        entropy_coeff: 0.0017600000137463212
        kl: 0.016238801181316376
        model: {}
        policy_loss: -0.0340866893529892
        total_loss: -0.033007871359586716
        vf_explained_var: 0.052418917417526245
        vf_loss: 15.501986503601074
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006909120129421353
        entropy: 0.8849141001701355
        entropy_coeff: 0.0017600000137463212
        kl: 0.01229154784232378
        model: {}
        policy_loss: -0.02481389418244362
        total_loss: -0.023807156831026077
        vf_explained_var: 0.18376660346984863
        vf_loss: 13.350257873535156
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006909120129421353
        entropy: 1.1067520380020142
        entropy_coeff: 0.0017600000137463212
        kl: 0.016915464773774147
        model: {}
        policy_loss: -0.0347139909863472
        total_loss: -0.03355153650045395
        vf_explained_var: 0.1314331591129303
        vf_loss: 14.187896728515625
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006909120129421353
        entropy: 1.0769208669662476
        entropy_coeff: 0.0017600000137463212
        kl: 0.017036262899637222
        model: {}
        policy_loss: -0.03935352340340614
        total_loss: -0.0381874181330204
        vf_explained_var: 0.16840195655822754
        vf_loss: 13.57864761352539
    load_time_ms: 13813.043
    num_steps_sampled: 9216000
    num_steps_trained: 9216000
    sample_time_ms: 92714.9
    update_time_ms: 20.743
  iterations_since_restore: 76
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.2625
    ram_util_percent: 9.66022727272727
  pid: 4061
  policy_reward_max:
    agent-0: 146.00000000000006
    agent-1: 146.00000000000006
    agent-2: 146.00000000000006
    agent-3: 146.00000000000006
    agent-4: 146.00000000000006
    agent-5: 146.00000000000006
  policy_reward_mean:
    agent-0: 92.86500000000017
    agent-1: 92.86500000000017
    agent-2: 92.86500000000017
    agent-3: 92.86500000000017
    agent-4: 92.86500000000017
    agent-5: 92.86500000000017
  policy_reward_min:
    agent-0: 35.6666666666667
    agent-1: 35.6666666666667
    agent-2: 35.6666666666667
    agent-3: 35.6666666666667
    agent-4: 35.6666666666667
    agent-5: 35.6666666666667
  sampler_perf:
    mean_env_wait_ms: 23.506346418912017
    mean_inference_ms: 12.339829705163048
    mean_processing_ms: 50.97382883607772
  time_since_restore: 10415.68420124054
  time_this_iter_s: 123.41355991363525
  time_total_s: 13626.74788737297
  timestamp: 1637027934
  timesteps_since_restore: 7296000
  timesteps_this_iter: 96000
  timesteps_total: 9216000
  training_iteration: 96
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     96 |          13626.7 | 9216000 |   557.19 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 48
    apples_agent-0_mean: 5.89
    apples_agent-0_min: 0
    apples_agent-1_max: 97
    apples_agent-1_mean: 20.94
    apples_agent-1_min: 0
    apples_agent-2_max: 135
    apples_agent-2_mean: 15.06
    apples_agent-2_min: 0
    apples_agent-3_max: 224
    apples_agent-3_mean: 90.08
    apples_agent-3_min: 36
    apples_agent-4_max: 47
    apples_agent-4_mean: 3.67
    apples_agent-4_min: 0
    apples_agent-5_max: 153
    apples_agent-5_mean: 75.04
    apples_agent-5_min: 13
    cleaning_beam_agent-0_max: 565
    cleaning_beam_agent-0_mean: 308.42
    cleaning_beam_agent-0_min: 127
    cleaning_beam_agent-1_max: 390
    cleaning_beam_agent-1_mean: 270.04
    cleaning_beam_agent-1_min: 167
    cleaning_beam_agent-2_max: 376
    cleaning_beam_agent-2_mean: 256.23
    cleaning_beam_agent-2_min: 125
    cleaning_beam_agent-3_max: 146
    cleaning_beam_agent-3_mean: 55.16
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 440
    cleaning_beam_agent-4_mean: 325.57
    cleaning_beam_agent-4_min: 146
    cleaning_beam_agent-5_max: 351
    cleaning_beam_agent-5_mean: 92.77
    cleaning_beam_agent-5_min: 14
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.08
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-00-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 804.9999999999886
  episode_reward_mean: 569.770000000001
  episode_reward_min: 287.9999999999973
  episodes_this_iter: 96
  episodes_total: 9312
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20180.757
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006849215715192258
        entropy: 1.2176085710525513
        entropy_coeff: 0.0017600000137463212
        kl: 0.0137903718277812
        model: {}
        policy_loss: -0.0310162752866745
        total_loss: -0.030406542122364044
        vf_explained_var: 0.05171497166156769
        vf_loss: 13.736886978149414
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006849215715192258
        entropy: 1.1244876384735107
        entropy_coeff: 0.0017600000137463212
        kl: 0.016428142786026
        model: {}
        policy_loss: -0.03276824578642845
        total_loss: -0.031706564128398895
        vf_explained_var: 0.03452162444591522
        vf_loss: 13.9796724319458
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006849215715192258
        entropy: 1.2114355564117432
        entropy_coeff: 0.0017600000137463212
        kl: 0.017848925665020943
        model: {}
        policy_loss: -0.03516343608498573
        total_loss: -0.034163329750299454
        vf_explained_var: 0.06983084976673126
        vf_loss: 13.473388671875
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006849215715192258
        entropy: 0.886817216873169
        entropy_coeff: 0.0017600000137463212
        kl: 0.012412995100021362
        model: {}
        policy_loss: -0.024699576199054718
        total_loss: -0.02379426173865795
        vf_explained_var: 0.15446344017982483
        vf_loss: 12.248133659362793
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006849215715192258
        entropy: 1.1188850402832031
        entropy_coeff: 0.0017600000137463212
        kl: 0.016274500638246536
        model: {}
        policy_loss: -0.034227654337882996
        total_loss: -0.03332061320543289
        vf_explained_var: 0.1373913735151291
        vf_loss: 12.48830795288086
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006849215715192258
        entropy: 1.0619877576828003
        entropy_coeff: 0.0017600000137463212
        kl: 0.016625307500362396
        model: {}
        policy_loss: -0.03948476165533066
        total_loss: -0.03842440992593765
        vf_explained_var: 0.12511657178401947
        vf_loss: 12.669175148010254
    load_time_ms: 13924.816
    num_steps_sampled: 9312000
    num_steps_trained: 9312000
    sample_time_ms: 92155.843
    update_time_ms: 20.443
  iterations_since_restore: 77
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 17.124858757062146
    ram_util_percent: 9.68587570621469
  pid: 4061
  policy_reward_max:
    agent-0: 134.16666666666694
    agent-1: 134.16666666666694
    agent-2: 134.16666666666694
    agent-3: 134.16666666666694
    agent-4: 134.16666666666694
    agent-5: 134.16666666666694
  policy_reward_mean:
    agent-0: 94.96166666666687
    agent-1: 94.96166666666687
    agent-2: 94.96166666666687
    agent-3: 94.96166666666687
    agent-4: 94.96166666666687
    agent-5: 94.96166666666687
  policy_reward_min:
    agent-0: 47.99999999999993
    agent-1: 47.99999999999993
    agent-2: 47.99999999999993
    agent-3: 47.99999999999993
    agent-4: 47.99999999999993
    agent-5: 47.99999999999993
  sampler_perf:
    mean_env_wait_ms: 23.509415830698767
    mean_inference_ms: 12.338382557156796
    mean_processing_ms: 50.96725007319917
  time_since_restore: 10539.812848567963
  time_this_iter_s: 124.1286473274231
  time_total_s: 13750.876534700394
  timestamp: 1637028059
  timesteps_since_restore: 7392000
  timesteps_this_iter: 96000
  timesteps_total: 9312000
  training_iteration: 97
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     97 |          13750.9 | 9312000 |   569.77 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 69
    apples_agent-0_mean: 6.75
    apples_agent-0_min: 0
    apples_agent-1_max: 118
    apples_agent-1_mean: 18.49
    apples_agent-1_min: 0
    apples_agent-2_max: 77
    apples_agent-2_mean: 14.05
    apples_agent-2_min: 0
    apples_agent-3_max: 163
    apples_agent-3_mean: 91.86
    apples_agent-3_min: 22
    apples_agent-4_max: 107
    apples_agent-4_mean: 4.45
    apples_agent-4_min: 0
    apples_agent-5_max: 211
    apples_agent-5_mean: 83.25
    apples_agent-5_min: 22
    cleaning_beam_agent-0_max: 481
    cleaning_beam_agent-0_mean: 298.25
    cleaning_beam_agent-0_min: 139
    cleaning_beam_agent-1_max: 478
    cleaning_beam_agent-1_mean: 270.12
    cleaning_beam_agent-1_min: 119
    cleaning_beam_agent-2_max: 353
    cleaning_beam_agent-2_mean: 228.56
    cleaning_beam_agent-2_min: 60
    cleaning_beam_agent-3_max: 111
    cleaning_beam_agent-3_mean: 51.33
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 464
    cleaning_beam_agent-4_mean: 318.87
    cleaning_beam_agent-4_min: 141
    cleaning_beam_agent-5_max: 306
    cleaning_beam_agent-5_mean: 99.65
    cleaning_beam_agent-5_min: 16
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.07
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-03-01
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 793.9999999999824
  episode_reward_mean: 578.4000000000005
  episode_reward_min: 196.99999999999906
  episodes_this_iter: 96
  episodes_total: 9408
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20172.896
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006789311883039773
        entropy: 1.2014944553375244
        entropy_coeff: 0.0017600000137463212
        kl: 0.013929798267781734
        model: {}
        policy_loss: -0.03188449889421463
        total_loss: -0.03126349672675133
        vf_explained_var: 0.06624580919742584
        vf_loss: 13.426525115966797
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006789311883039773
        entropy: 1.137122631072998
        entropy_coeff: 0.0017600000137463212
        kl: 0.015343117527663708
        model: {}
        policy_loss: -0.03252062201499939
        total_loss: -0.03160784766077995
        vf_explained_var: 0.03981195390224457
        vf_loss: 13.797998428344727
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006789311883039773
        entropy: 1.216862678527832
        entropy_coeff: 0.0017600000137463212
        kl: 0.017723947763442993
        model: {}
        policy_loss: -0.03707809001207352
        total_loss: -0.036098625510931015
        vf_explained_var: 0.061426132917404175
        vf_loss: 13.487438201904297
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006789311883039773
        entropy: 0.8598695993423462
        entropy_coeff: 0.0017600000137463212
        kl: 0.012448888272047043
        model: {}
        policy_loss: -0.0260063074529171
        total_loss: -0.025094540789723396
        vf_explained_var: 0.17841491103172302
        vf_loss: 11.80247974395752
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006789311883039773
        entropy: 1.1079884767532349
        entropy_coeff: 0.0017600000137463212
        kl: 0.015256144106388092
        model: {}
        policy_loss: -0.035058505833148956
        total_loss: -0.03428713604807854
        vf_explained_var: 0.16750265657901764
        vf_loss: 11.958169937133789
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006789311883039773
        entropy: 1.0554393529891968
        entropy_coeff: 0.0017600000137463212
        kl: 0.017382116988301277
        model: {}
        policy_loss: -0.03859483450651169
        total_loss: -0.037441086024045944
        vf_explained_var: 0.11454951763153076
        vf_loss: 12.731101036071777
    load_time_ms: 14054.995
    num_steps_sampled: 9408000
    num_steps_trained: 9408000
    sample_time_ms: 91399.537
    update_time_ms: 21.216
  iterations_since_restore: 78
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.373563218390805
    ram_util_percent: 9.609195402298853
  pid: 4061
  policy_reward_max:
    agent-0: 132.33333333333405
    agent-1: 132.33333333333405
    agent-2: 132.33333333333405
    agent-3: 132.33333333333405
    agent-4: 132.33333333333405
    agent-5: 132.33333333333405
  policy_reward_mean:
    agent-0: 96.40000000000026
    agent-1: 96.40000000000026
    agent-2: 96.40000000000026
    agent-3: 96.40000000000026
    agent-4: 96.40000000000026
    agent-5: 96.40000000000026
  policy_reward_min:
    agent-0: 32.8333333333334
    agent-1: 32.8333333333334
    agent-2: 32.8333333333334
    agent-3: 32.8333333333334
    agent-4: 32.8333333333334
    agent-5: 32.8333333333334
  sampler_perf:
    mean_env_wait_ms: 23.508006424970407
    mean_inference_ms: 12.33536331478678
    mean_processing_ms: 50.951690099341
  time_since_restore: 10661.939725637436
  time_this_iter_s: 122.12687706947327
  time_total_s: 13873.003411769867
  timestamp: 1637028181
  timesteps_since_restore: 7488000
  timesteps_this_iter: 96000
  timesteps_total: 9408000
  training_iteration: 98
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     98 |            13873 | 9408000 |    578.4 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 50
    apples_agent-0_mean: 4.64
    apples_agent-0_min: 0
    apples_agent-1_max: 103
    apples_agent-1_mean: 19.76
    apples_agent-1_min: 0
    apples_agent-2_max: 118
    apples_agent-2_mean: 15.12
    apples_agent-2_min: 0
    apples_agent-3_max: 225
    apples_agent-3_mean: 88.43
    apples_agent-3_min: 18
    apples_agent-4_max: 76
    apples_agent-4_mean: 2.18
    apples_agent-4_min: 0
    apples_agent-5_max: 172
    apples_agent-5_mean: 83.09
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 488
    cleaning_beam_agent-0_mean: 301.62
    cleaning_beam_agent-0_min: 129
    cleaning_beam_agent-1_max: 421
    cleaning_beam_agent-1_mean: 269.27
    cleaning_beam_agent-1_min: 132
    cleaning_beam_agent-2_max: 370
    cleaning_beam_agent-2_mean: 234.42
    cleaning_beam_agent-2_min: 103
    cleaning_beam_agent-3_max: 140
    cleaning_beam_agent-3_mean: 45.13
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 511
    cleaning_beam_agent-4_mean: 321.71
    cleaning_beam_agent-4_min: 89
    cleaning_beam_agent-5_max: 499
    cleaning_beam_agent-5_mean: 109.23
    cleaning_beam_agent-5_min: 21
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.05
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-05-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 817.9999999999859
  episode_reward_mean: 597.2099999999994
  episode_reward_min: 95.00000000000006
  episodes_this_iter: 96
  episodes_total: 9504
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20137.345
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006729408050887287
        entropy: 1.2218974828720093
        entropy_coeff: 0.0017600000137463212
        kl: 0.014269636943936348
        model: {}
        policy_loss: -0.033419616520404816
        total_loss: -0.03275613486766815
        vf_explained_var: 0.059311628341674805
        vf_loss: 13.870594024658203
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006729408050887287
        entropy: 1.1325876712799072
        entropy_coeff: 0.0017600000137463212
        kl: 0.019359314814209938
        model: {}
        policy_loss: -0.029233884066343307
        total_loss: -0.027897696942090988
        vf_explained_var: 0.05478069186210632
        vf_loss: 13.93612289428711
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006729408050887287
        entropy: 1.2062113285064697
        entropy_coeff: 0.0017600000137463212
        kl: 0.016678588464856148
        model: {}
        policy_loss: -0.036990128457546234
        total_loss: -0.03611734136939049
        vf_explained_var: 0.09892848134040833
        vf_loss: 13.278603553771973
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006729408050887287
        entropy: 0.8426524996757507
        entropy_coeff: 0.0017600000137463212
        kl: 0.012526002712547779
        model: {}
        policy_loss: -0.025214698165655136
        total_loss: -0.02426893264055252
        vf_explained_var: 0.20249886810779572
        vf_loss: 11.76236629486084
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006729408050887287
        entropy: 1.1033562421798706
        entropy_coeff: 0.0017600000137463212
        kl: 0.01581640914082527
        model: {}
        policy_loss: -0.03466011583805084
        total_loss: -0.033746909350156784
        vf_explained_var: 0.13684870302677155
        vf_loss: 12.73466968536377
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006729408050887287
        entropy: 1.0727202892303467
        entropy_coeff: 0.0017600000137463212
        kl: 0.017475971952080727
        model: {}
        policy_loss: -0.03909950703382492
        total_loss: -0.03797518461942673
        vf_explained_var: 0.14286775887012482
        vf_loss: 12.64716625213623
    load_time_ms: 14120.537
    num_steps_sampled: 9504000
    num_steps_trained: 9504000
    sample_time_ms: 90838.384
    update_time_ms: 21.432
  iterations_since_restore: 79
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.322285714285714
    ram_util_percent: 9.665714285714287
  pid: 4061
  policy_reward_max:
    agent-0: 136.33333333333354
    agent-1: 136.33333333333354
    agent-2: 136.33333333333354
    agent-3: 136.33333333333354
    agent-4: 136.33333333333354
    agent-5: 136.33333333333354
  policy_reward_mean:
    agent-0: 99.53500000000031
    agent-1: 99.53500000000031
    agent-2: 99.53500000000031
    agent-3: 99.53500000000031
    agent-4: 99.53500000000031
    agent-5: 99.53500000000031
  policy_reward_min:
    agent-0: 15.833333333333318
    agent-1: 15.833333333333318
    agent-2: 15.833333333333318
    agent-3: 15.833333333333318
    agent-4: 15.833333333333318
    agent-5: 15.833333333333318
  sampler_perf:
    mean_env_wait_ms: 23.507704223984227
    mean_inference_ms: 12.33362091206411
    mean_processing_ms: 50.941800677967734
  time_since_restore: 10785.100882053375
  time_this_iter_s: 123.16115641593933
  time_total_s: 13996.164568185806
  timestamp: 1637028304
  timesteps_since_restore: 7584000
  timesteps_this_iter: 96000
  timesteps_total: 9504000
  training_iteration: 99
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |     99 |          13996.2 | 9504000 |   597.21 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 80
    apples_agent-0_mean: 5.16
    apples_agent-0_min: 0
    apples_agent-1_max: 87
    apples_agent-1_mean: 18.25
    apples_agent-1_min: 0
    apples_agent-2_max: 99
    apples_agent-2_mean: 12.84
    apples_agent-2_min: 0
    apples_agent-3_max: 141
    apples_agent-3_mean: 74.5
    apples_agent-3_min: 10
    apples_agent-4_max: 98
    apples_agent-4_mean: 5.79
    apples_agent-4_min: 0
    apples_agent-5_max: 189
    apples_agent-5_mean: 73.39
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 443
    cleaning_beam_agent-0_mean: 273.55
    cleaning_beam_agent-0_min: 80
    cleaning_beam_agent-1_max: 425
    cleaning_beam_agent-1_mean: 259.49
    cleaning_beam_agent-1_min: 118
    cleaning_beam_agent-2_max: 378
    cleaning_beam_agent-2_mean: 225.83
    cleaning_beam_agent-2_min: 94
    cleaning_beam_agent-3_max: 111
    cleaning_beam_agent-3_mean: 48.09
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 482
    cleaning_beam_agent-4_mean: 290.0
    cleaning_beam_agent-4_min: 130
    cleaning_beam_agent-5_max: 545
    cleaning_beam_agent-5_mean: 114.37
    cleaning_beam_agent-5_min: 23
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-07-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 810.9999999999769
  episode_reward_mean: 531.5300000000029
  episode_reward_min: 183.99999999999832
  episodes_this_iter: 96
  episodes_total: 9600
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20128.771
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006669504218734801
        entropy: 1.2391510009765625
        entropy_coeff: 0.0017600000137463212
        kl: 0.016587650403380394
        model: {}
        policy_loss: -0.032590873539447784
        total_loss: -0.031783685088157654
        vf_explained_var: 0.06378832459449768
        vf_loss: 13.2932767868042
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006669504218734801
        entropy: 1.1403088569641113
        entropy_coeff: 0.0017600000137463212
        kl: 0.015712391585111618
        model: {}
        policy_loss: -0.031347449868917465
        total_loss: -0.030413072556257248
        vf_explained_var: 0.035844504833221436
        vf_loss: 13.700820922851562
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006669504218734801
        entropy: 1.211146593093872
        entropy_coeff: 0.0017600000137463212
        kl: 0.017029115930199623
        model: {}
        policy_loss: -0.034849926829338074
        total_loss: -0.03395550698041916
        vf_explained_var: 0.06897729635238647
        vf_loss: 13.231181144714355
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006669504218734801
        entropy: 0.905829668045044
        entropy_coeff: 0.0017600000137463212
        kl: 0.012685736641287804
        model: {}
        policy_loss: -0.026838034391403198
        total_loss: -0.02601620741188526
        vf_explained_var: 0.19177861511707306
        vf_loss: 11.475137710571289
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006669504218734801
        entropy: 1.1208674907684326
        entropy_coeff: 0.0017600000137463212
        kl: 0.016870776191353798
        model: {}
        policy_loss: -0.03545185551047325
        total_loss: -0.03458767384290695
        vf_explained_var: 0.19032199680805206
        vf_loss: 11.498284339904785
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006669504218734801
        entropy: 1.0809237957000732
        entropy_coeff: 0.0017600000137463212
        kl: 0.018324337899684906
        model: {}
        policy_loss: -0.038481224328279495
        total_loss: -0.03733403980731964
        vf_explained_var: 0.14355461299419403
        vf_loss: 12.17170238494873
    load_time_ms: 14102.874
    num_steps_sampled: 9600000
    num_steps_trained: 9600000
    sample_time_ms: 90188.111
    update_time_ms: 21.002
  iterations_since_restore: 80
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.376571428571426
    ram_util_percent: 9.669714285714287
  pid: 4061
  policy_reward_max:
    agent-0: 135.166666666667
    agent-1: 135.166666666667
    agent-2: 135.166666666667
    agent-3: 135.166666666667
    agent-4: 135.166666666667
    agent-5: 135.166666666667
  policy_reward_mean:
    agent-0: 88.58833333333352
    agent-1: 88.58833333333352
    agent-2: 88.58833333333352
    agent-3: 88.58833333333352
    agent-4: 88.58833333333352
    agent-5: 88.58833333333352
  policy_reward_min:
    agent-0: 30.666666666666742
    agent-1: 30.666666666666742
    agent-2: 30.666666666666742
    agent-3: 30.666666666666742
    agent-4: 30.666666666666742
    agent-5: 30.666666666666742
  sampler_perf:
    mean_env_wait_ms: 23.50393975240927
    mean_inference_ms: 12.331776574935912
    mean_processing_ms: 50.93080807729212
  time_since_restore: 10907.972661495209
  time_this_iter_s: 122.8717794418335
  time_total_s: 14119.03634762764
  timestamp: 1637028428
  timesteps_since_restore: 7680000
  timesteps_this_iter: 96000
  timesteps_total: 9600000
  training_iteration: 100
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    100 |            14119 | 9600000 |   531.53 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 70
    apples_agent-0_mean: 7.16
    apples_agent-0_min: 0
    apples_agent-1_max: 95
    apples_agent-1_mean: 17.96
    apples_agent-1_min: 0
    apples_agent-2_max: 70
    apples_agent-2_mean: 10.57
    apples_agent-2_min: 0
    apples_agent-3_max: 146
    apples_agent-3_mean: 76.87
    apples_agent-3_min: 24
    apples_agent-4_max: 86
    apples_agent-4_mean: 4.08
    apples_agent-4_min: 0
    apples_agent-5_max: 152
    apples_agent-5_mean: 73.45
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 478
    cleaning_beam_agent-0_mean: 267.52
    cleaning_beam_agent-0_min: 127
    cleaning_beam_agent-1_max: 444
    cleaning_beam_agent-1_mean: 259.62
    cleaning_beam_agent-1_min: 118
    cleaning_beam_agent-2_max: 354
    cleaning_beam_agent-2_mean: 237.05
    cleaning_beam_agent-2_min: 53
    cleaning_beam_agent-3_max: 133
    cleaning_beam_agent-3_mean: 46.91
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 441
    cleaning_beam_agent-4_mean: 308.12
    cleaning_beam_agent-4_min: 148
    cleaning_beam_agent-5_max: 649
    cleaning_beam_agent-5_mean: 103.18
    cleaning_beam_agent-5_min: 15
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-09-10
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 781.99999999999
  episode_reward_mean: 546.500000000003
  episode_reward_min: 281.99999999999864
  episodes_this_iter: 96
  episodes_total: 9696
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20138.94
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006609599804505706
        entropy: 1.2079566717147827
        entropy_coeff: 0.0017600000137463212
        kl: 0.015114000998437405
        model: {}
        policy_loss: -0.032410379499197006
        total_loss: -0.03166715428233147
        vf_explained_var: 0.06820672750473022
        vf_loss: 13.578287124633789
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006609599804505706
        entropy: 1.123796820640564
        entropy_coeff: 0.0017600000137463212
        kl: 0.0166655033826828
        model: {}
        policy_loss: -0.03089236095547676
        total_loss: -0.029847558587789536
        vf_explained_var: 0.06825423240661621
        vf_loss: 13.56131649017334
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006609599804505706
        entropy: 1.215197205543518
        entropy_coeff: 0.0017600000137463212
        kl: 0.017207251861691475
        model: {}
        policy_loss: -0.03420466184616089
        total_loss: -0.0332581102848053
        vf_explained_var: 0.06281553208827972
        vf_loss: 13.645773887634277
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006609599804505706
        entropy: 0.8632014989852905
        entropy_coeff: 0.0017600000137463212
        kl: 0.012437553144991398
        model: {}
        policy_loss: -0.025753479450941086
        total_loss: -0.02486823871731758
        vf_explained_var: 0.20282398164272308
        vf_loss: 11.607234954833984
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006609599804505706
        entropy: 1.1055611371994019
        entropy_coeff: 0.0017600000137463212
        kl: 0.01656457968056202
        model: {}
        policy_loss: -0.03481394797563553
        total_loss: -0.03386034816503525
        vf_explained_var: 0.14575161039829254
        vf_loss: 12.429302215576172
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006609599804505706
        entropy: 1.073122262954712
        entropy_coeff: 0.0017600000137463212
        kl: 0.01890573278069496
        model: {}
        policy_loss: -0.03697602078318596
        total_loss: -0.035743597894907
        vf_explained_var: 0.15407882630825043
        vf_loss: 12.305414199829102
    load_time_ms: 14105.992
    num_steps_sampled: 9696000
    num_steps_trained: 9696000
    sample_time_ms: 89877.039
    update_time_ms: 21.036
  iterations_since_restore: 81
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.587428571428573
    ram_util_percent: 9.584000000000001
  pid: 4061
  policy_reward_max:
    agent-0: 130.33333333333377
    agent-1: 130.33333333333377
    agent-2: 130.33333333333377
    agent-3: 130.33333333333377
    agent-4: 130.33333333333377
    agent-5: 130.33333333333377
  policy_reward_mean:
    agent-0: 91.08333333333354
    agent-1: 91.08333333333354
    agent-2: 91.08333333333354
    agent-3: 91.08333333333354
    agent-4: 91.08333333333354
    agent-5: 91.08333333333354
  policy_reward_min:
    agent-0: 46.99999999999992
    agent-1: 46.99999999999992
    agent-2: 46.99999999999992
    agent-3: 46.99999999999992
    agent-4: 46.99999999999992
    agent-5: 46.99999999999992
  sampler_perf:
    mean_env_wait_ms: 23.50075783220909
    mean_inference_ms: 12.32951066299243
    mean_processing_ms: 50.923705312809716
  time_since_restore: 11030.231208324432
  time_this_iter_s: 122.25854682922363
  time_total_s: 14241.294894456863
  timestamp: 1637028550
  timesteps_since_restore: 7776000
  timesteps_this_iter: 96000
  timesteps_total: 9696000
  training_iteration: 101
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    101 |          14241.3 | 9696000 |    546.5 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 47
    apples_agent-0_mean: 6.67
    apples_agent-0_min: 0
    apples_agent-1_max: 107
    apples_agent-1_mean: 20.77
    apples_agent-1_min: 0
    apples_agent-2_max: 77
    apples_agent-2_mean: 13.95
    apples_agent-2_min: 0
    apples_agent-3_max: 148
    apples_agent-3_mean: 70.81
    apples_agent-3_min: 21
    apples_agent-4_max: 116
    apples_agent-4_mean: 5.92
    apples_agent-4_min: 0
    apples_agent-5_max: 166
    apples_agent-5_mean: 72.37
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 466
    cleaning_beam_agent-0_mean: 275.22
    cleaning_beam_agent-0_min: 90
    cleaning_beam_agent-1_max: 461
    cleaning_beam_agent-1_mean: 269.92
    cleaning_beam_agent-1_min: 136
    cleaning_beam_agent-2_max: 454
    cleaning_beam_agent-2_mean: 235.25
    cleaning_beam_agent-2_min: 112
    cleaning_beam_agent-3_max: 155
    cleaning_beam_agent-3_mean: 52.54
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 509
    cleaning_beam_agent-4_mean: 318.39
    cleaning_beam_agent-4_min: 123
    cleaning_beam_agent-5_max: 679
    cleaning_beam_agent-5_mean: 118.12
    cleaning_beam_agent-5_min: 23
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-11-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 789.999999999981
  episode_reward_mean: 550.7100000000011
  episode_reward_min: 160.99999999999946
  episodes_this_iter: 96
  episodes_total: 9792
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20133.611
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000654969597235322
        entropy: 1.2295637130737305
        entropy_coeff: 0.0017600000137463212
        kl: 0.01409117877483368
        model: {}
        policy_loss: -0.033492110669612885
        total_loss: -0.03282350301742554
        vf_explained_var: 0.07188279926776886
        vf_loss: 14.235228538513184
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000654969597235322
        entropy: 1.1269906759262085
        entropy_coeff: 0.0017600000137463212
        kl: 0.014899211004376411
        model: {}
        policy_loss: -0.03049507923424244
        total_loss: -0.029550626873970032
        vf_explained_var: 0.06239704787731171
        vf_loss: 14.380349159240723
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000654969597235322
        entropy: 1.210822582244873
        entropy_coeff: 0.0017600000137463212
        kl: 0.01667001284658909
        model: {}
        policy_loss: -0.03533152863383293
        total_loss: -0.03437398746609688
        vf_explained_var: 0.0725841075181961
        vf_loss: 14.215910911560059
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000654969597235322
        entropy: 0.8691450953483582
        entropy_coeff: 0.0017600000137463212
        kl: 0.012407416477799416
        model: {}
        policy_loss: -0.026528626680374146
        total_loss: -0.02566128596663475
        vf_explained_var: 0.24635827541351318
        vf_loss: 11.562944412231445
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000654969597235322
        entropy: 1.0996109247207642
        entropy_coeff: 0.0017600000137463212
        kl: 0.016617726534605026
        model: {}
        policy_loss: -0.036755334585905075
        total_loss: -0.035761427134275436
        vf_explained_var: 0.17323851585388184
        vf_loss: 12.674470901489258
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000654969597235322
        entropy: 1.0632245540618896
        entropy_coeff: 0.0017600000137463212
        kl: 0.016810333356261253
        model: {}
        policy_loss: -0.03855569660663605
        total_loss: -0.03750712797045708
        vf_explained_var: 0.1904062032699585
        vf_loss: 12.38808536529541
    load_time_ms: 13999.901
    num_steps_sampled: 9792000
    num_steps_trained: 9792000
    sample_time_ms: 89621.148
    update_time_ms: 21.014
  iterations_since_restore: 82
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 16.23103448275862
    ram_util_percent: 9.643678160919542
  pid: 4061
  policy_reward_max:
    agent-0: 131.66666666666723
    agent-1: 131.66666666666723
    agent-2: 131.66666666666723
    agent-3: 131.66666666666723
    agent-4: 131.66666666666723
    agent-5: 131.66666666666723
  policy_reward_mean:
    agent-0: 91.78500000000022
    agent-1: 91.78500000000022
    agent-2: 91.78500000000022
    agent-3: 91.78500000000022
    agent-4: 91.78500000000022
    agent-5: 91.78500000000022
  policy_reward_min:
    agent-0: 26.833333333333368
    agent-1: 26.833333333333368
    agent-2: 26.833333333333368
    agent-3: 26.833333333333368
    agent-4: 26.833333333333368
    agent-5: 26.833333333333368
  sampler_perf:
    mean_env_wait_ms: 23.502016468772904
    mean_inference_ms: 12.328789173894094
    mean_processing_ms: 50.917959054609554
  time_since_restore: 11152.277789831161
  time_this_iter_s: 122.04658150672913
  time_total_s: 14363.341475963593
  timestamp: 1637028672
  timesteps_since_restore: 7872000
  timesteps_this_iter: 96000
  timesteps_total: 9792000
  training_iteration: 102
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    102 |          14363.3 | 9792000 |   550.71 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 55
    apples_agent-0_mean: 6.29
    apples_agent-0_min: 0
    apples_agent-1_max: 87
    apples_agent-1_mean: 22.29
    apples_agent-1_min: 0
    apples_agent-2_max: 75
    apples_agent-2_mean: 13.46
    apples_agent-2_min: 0
    apples_agent-3_max: 145
    apples_agent-3_mean: 72.69
    apples_agent-3_min: 20
    apples_agent-4_max: 55
    apples_agent-4_mean: 2.97
    apples_agent-4_min: 0
    apples_agent-5_max: 155
    apples_agent-5_mean: 73.7
    apples_agent-5_min: 6
    cleaning_beam_agent-0_max: 455
    cleaning_beam_agent-0_mean: 283.45
    cleaning_beam_agent-0_min: 94
    cleaning_beam_agent-1_max: 465
    cleaning_beam_agent-1_mean: 256.76
    cleaning_beam_agent-1_min: 100
    cleaning_beam_agent-2_max: 459
    cleaning_beam_agent-2_mean: 229.11
    cleaning_beam_agent-2_min: 98
    cleaning_beam_agent-3_max: 103
    cleaning_beam_agent-3_mean: 52.08
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 482
    cleaning_beam_agent-4_mean: 312.01
    cleaning_beam_agent-4_min: 125
    cleaning_beam_agent-5_max: 408
    cleaning_beam_agent-5_mean: 114.03
    cleaning_beam_agent-5_min: 23
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.05
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 5
    fire_beam_agent-4_mean: 0.07
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-13-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 781.9999999999842
  episode_reward_mean: 561.1000000000015
  episode_reward_min: 171.99999999999878
  episodes_this_iter: 96
  episodes_total: 9888
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20083.848
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006489792140200734
        entropy: 1.2294849157333374
        entropy_coeff: 0.0017600000137463212
        kl: 0.01439593080431223
        model: {}
        policy_loss: -0.033425454050302505
        total_loss: -0.03277003765106201
        vf_explained_var: 0.07394659519195557
        vf_loss: 13.797179222106934
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006489792140200734
        entropy: 1.1096172332763672
        entropy_coeff: 0.0017600000137463212
        kl: 0.01443423144519329
        model: {}
        policy_loss: -0.031656358391046524
        total_loss: -0.030764849856495857
        vf_explained_var: 0.05915951728820801
        vf_loss: 14.010162353515625
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006489792140200734
        entropy: 1.2001420259475708
        entropy_coeff: 0.0017600000137463212
        kl: 0.016253113746643066
        model: {}
        policy_loss: -0.034025393426418304
        total_loss: -0.03310191631317139
        vf_explained_var: 0.05295999348163605
        vf_loss: 14.104154586791992
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006489792140200734
        entropy: 0.8373450040817261
        entropy_coeff: 0.0017600000137463212
        kl: 0.012280771508812904
        model: {}
        policy_loss: -0.02558806724846363
        total_loss: -0.02465759962797165
        vf_explained_var: 0.2108139544725418
        vf_loss: 11.761190414428711
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006489792140200734
        entropy: 1.096745252609253
        entropy_coeff: 0.0017600000137463212
        kl: 0.015065697953104973
        model: {}
        policy_loss: -0.03657740354537964
        total_loss: -0.03574219346046448
        vf_explained_var: 0.1545039713382721
        vf_loss: 12.589088439941406
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006489792140200734
        entropy: 1.05996835231781
        entropy_coeff: 0.0017600000137463212
        kl: 0.01835276186466217
        model: {}
        policy_loss: -0.03754892945289612
        total_loss: -0.03629079461097717
        vf_explained_var: 0.1343867927789688
        vf_loss: 12.884048461914062
    load_time_ms: 14016.595
    num_steps_sampled: 9888000
    num_steps_trained: 9888000
    sample_time_ms: 89161.387
    update_time_ms: 21.192
  iterations_since_restore: 83
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.40977011494253
    ram_util_percent: 9.600574712643681
  pid: 4061
  policy_reward_max:
    agent-0: 130.33333333333385
    agent-1: 130.33333333333385
    agent-2: 130.33333333333385
    agent-3: 130.33333333333385
    agent-4: 130.33333333333385
    agent-5: 130.33333333333385
  policy_reward_mean:
    agent-0: 93.51666666666692
    agent-1: 93.51666666666692
    agent-2: 93.51666666666692
    agent-3: 93.51666666666692
    agent-4: 93.51666666666692
    agent-5: 93.51666666666692
  policy_reward_min:
    agent-0: 28.666666666666714
    agent-1: 28.666666666666714
    agent-2: 28.666666666666714
    agent-3: 28.666666666666714
    agent-4: 28.666666666666714
    agent-5: 28.666666666666714
  sampler_perf:
    mean_env_wait_ms: 23.50045685930228
    mean_inference_ms: 12.326018820484586
    mean_processing_ms: 50.90796191913924
  time_since_restore: 11274.163248062134
  time_this_iter_s: 121.88545823097229
  time_total_s: 14485.226934194565
  timestamp: 1637028794
  timesteps_since_restore: 7968000
  timesteps_this_iter: 96000
  timesteps_total: 9888000
  training_iteration: 103
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    103 |          14485.2 | 9888000 |    561.1 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 45
    apples_agent-0_mean: 6.92
    apples_agent-0_min: 0
    apples_agent-1_max: 104
    apples_agent-1_mean: 22.57
    apples_agent-1_min: 0
    apples_agent-2_max: 169
    apples_agent-2_mean: 12.07
    apples_agent-2_min: 0
    apples_agent-3_max: 161
    apples_agent-3_mean: 77.38
    apples_agent-3_min: 27
    apples_agent-4_max: 92
    apples_agent-4_mean: 2.87
    apples_agent-4_min: 0
    apples_agent-5_max: 150
    apples_agent-5_mean: 76.81
    apples_agent-5_min: 6
    cleaning_beam_agent-0_max: 455
    cleaning_beam_agent-0_mean: 266.51
    cleaning_beam_agent-0_min: 104
    cleaning_beam_agent-1_max: 414
    cleaning_beam_agent-1_mean: 258.08
    cleaning_beam_agent-1_min: 108
    cleaning_beam_agent-2_max: 436
    cleaning_beam_agent-2_mean: 236.52
    cleaning_beam_agent-2_min: 91
    cleaning_beam_agent-3_max: 123
    cleaning_beam_agent-3_mean: 43.52
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 457
    cleaning_beam_agent-4_mean: 328.73
    cleaning_beam_agent-4_min: 160
    cleaning_beam_agent-5_max: 349
    cleaning_beam_agent-5_mean: 105.59
    cleaning_beam_agent-5_min: 20
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.05
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.04
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.14
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-15-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 859.9999999999764
  episode_reward_mean: 571.3100000000011
  episode_reward_min: 255.99999999999477
  episodes_this_iter: 96
  episodes_total: 9984
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20080.919
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006429887725971639
        entropy: 1.2148494720458984
        entropy_coeff: 0.0017600000137463212
        kl: 0.013641057536005974
        model: {}
        policy_loss: -0.03330003097653389
        total_loss: -0.03276275470852852
        vf_explained_var: 0.052372366189956665
        vf_loss: 13.113061904907227
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006429887725971639
        entropy: 1.1173065900802612
        entropy_coeff: 0.0017600000137463212
        kl: 0.014428926631808281
        model: {}
        policy_loss: -0.03233414143323898
        total_loss: -0.0315767265856266
        vf_explained_var: 0.07339973747730255
        vf_loss: 12.809846878051758
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006429887725971639
        entropy: 1.2045683860778809
        entropy_coeff: 0.0017600000137463212
        kl: 0.016311122104525566
        model: {}
        policy_loss: -0.035165801644325256
        total_loss: -0.03434270992875099
        vf_explained_var: 0.05090592801570892
        vf_loss: 13.120213508605957
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006429887725971639
        entropy: 0.8238468766212463
        entropy_coeff: 0.0017600000137463212
        kl: 0.011841497384011745
        model: {}
        policy_loss: -0.02572540193796158
        total_loss: -0.024849899113178253
        vf_explained_var: 0.17427754402160645
        vf_loss: 11.413208961486816
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006429887725971639
        entropy: 1.0658977031707764
        entropy_coeff: 0.0017600000137463212
        kl: 0.015264837071299553
        model: {}
        policy_loss: -0.03486931324005127
        total_loss: -0.034022025763988495
        vf_explained_var: 0.1354151964187622
        vf_loss: 11.967903137207031
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006429887725971639
        entropy: 1.0545644760131836
        entropy_coeff: 0.0017600000137463212
        kl: 0.018104538321495056
        model: {}
        policy_loss: -0.03872314840555191
        total_loss: -0.03748564049601555
        vf_explained_var: 0.07221721112728119
        vf_loss: 12.830904006958008
    load_time_ms: 14730.627
    num_steps_sampled: 9984000
    num_steps_trained: 9984000
    sample_time_ms: 88576.292
    update_time_ms: 21.521
  iterations_since_restore: 84
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 14.743169398907105
    ram_util_percent: 9.646994535519129
  pid: 4061
  policy_reward_max:
    agent-0: 143.33333333333383
    agent-1: 143.33333333333383
    agent-2: 143.33333333333383
    agent-3: 143.33333333333383
    agent-4: 143.33333333333383
    agent-5: 143.33333333333383
  policy_reward_mean:
    agent-0: 95.21833333333358
    agent-1: 95.21833333333358
    agent-2: 95.21833333333358
    agent-3: 95.21833333333358
    agent-4: 95.21833333333358
    agent-5: 95.21833333333358
  policy_reward_min:
    agent-0: 42.66666666666663
    agent-1: 42.66666666666663
    agent-2: 42.66666666666663
    agent-3: 42.66666666666663
    agent-4: 42.66666666666663
    agent-5: 42.66666666666663
  sampler_perf:
    mean_env_wait_ms: 23.49815167405693
    mean_inference_ms: 12.323612219694624
    mean_processing_ms: 50.89640322676858
  time_since_restore: 11402.938074588776
  time_this_iter_s: 128.77482652664185
  time_total_s: 14614.001760721207
  timestamp: 1637028923
  timesteps_since_restore: 8064000
  timesteps_this_iter: 96000
  timesteps_total: 9984000
  training_iteration: 104
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    104 |            14614 | 9984000 |   571.31 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 54
    apples_agent-0_mean: 6.74
    apples_agent-0_min: 0
    apples_agent-1_max: 104
    apples_agent-1_mean: 19.96
    apples_agent-1_min: 0
    apples_agent-2_max: 74
    apples_agent-2_mean: 12.89
    apples_agent-2_min: 0
    apples_agent-3_max: 131
    apples_agent-3_mean: 75.75
    apples_agent-3_min: 15
    apples_agent-4_max: 51
    apples_agent-4_mean: 1.75
    apples_agent-4_min: 0
    apples_agent-5_max: 156
    apples_agent-5_mean: 78.86
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 450
    cleaning_beam_agent-0_mean: 269.32
    cleaning_beam_agent-0_min: 112
    cleaning_beam_agent-1_max: 438
    cleaning_beam_agent-1_mean: 269.5
    cleaning_beam_agent-1_min: 135
    cleaning_beam_agent-2_max: 373
    cleaning_beam_agent-2_mean: 233.09
    cleaning_beam_agent-2_min: 86
    cleaning_beam_agent-3_max: 119
    cleaning_beam_agent-3_mean: 42.22
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 511
    cleaning_beam_agent-4_mean: 327.04
    cleaning_beam_agent-4_min: 169
    cleaning_beam_agent-5_max: 377
    cleaning_beam_agent-5_mean: 105.19
    cleaning_beam_agent-5_min: 16
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 10
    fire_beam_agent-5_mean: 0.13
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-17-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 796.9999999999868
  episode_reward_mean: 574.3400000000005
  episode_reward_min: 190.999999999998
  episodes_this_iter: 96
  episodes_total: 10080
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20103.76
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006369983893819153
        entropy: 1.1951478719711304
        entropy_coeff: 0.0017600000137463212
        kl: 0.014457509852945805
        model: {}
        policy_loss: -0.032789405435323715
        total_loss: -0.03198014199733734
        vf_explained_var: 0.07311409711837769
        vf_loss: 14.669767379760742
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006369983893819153
        entropy: 1.1292256116867065
        entropy_coeff: 0.0017600000137463212
        kl: 0.01636536419391632
        model: {}
        policy_loss: -0.03247649595141411
        total_loss: -0.03140002116560936
        vf_explained_var: 0.09702341258525848
        vf_loss: 14.273757934570312
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006369983893819153
        entropy: 1.1993696689605713
        entropy_coeff: 0.0017600000137463212
        kl: 0.016401682049036026
        model: {}
        policy_loss: -0.03469638153910637
        total_loss: -0.033652715384960175
        vf_explained_var: 0.042835503816604614
        vf_loss: 15.143892288208008
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006369983893819153
        entropy: 0.8034000396728516
        entropy_coeff: 0.0017600000137463212
        kl: 0.012181410565972328
        model: {}
        policy_loss: -0.02606443502008915
        total_loss: -0.025025274604558945
        vf_explained_var: 0.21936768293380737
        vf_loss: 12.350028038024902
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006369983893819153
        entropy: 1.069619059562683
        entropy_coeff: 0.0017600000137463212
        kl: 0.01732473447918892
        model: {}
        policy_loss: -0.03363848105072975
        total_loss: -0.03242463245987892
        vf_explained_var: 0.1368718445301056
        vf_loss: 13.639029502868652
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006369983893819153
        entropy: 1.0489944219589233
        entropy_coeff: 0.0017600000137463212
        kl: 0.016885414719581604
        model: {}
        policy_loss: -0.0387895405292511
        total_loss: -0.03761106729507446
        vf_explained_var: 0.1545233428478241
        vf_loss: 13.361591339111328
    load_time_ms: 14769.257
    num_steps_sampled: 10080000
    num_steps_trained: 10080000
    sample_time_ms: 88329.725
    update_time_ms: 25.809
  iterations_since_restore: 85
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.308
    ram_util_percent: 9.657142857142858
  pid: 4061
  policy_reward_max:
    agent-0: 132.83333333333394
    agent-1: 132.83333333333394
    agent-2: 132.83333333333394
    agent-3: 132.83333333333394
    agent-4: 132.83333333333394
    agent-5: 132.83333333333394
  policy_reward_mean:
    agent-0: 95.7233333333336
    agent-1: 95.7233333333336
    agent-2: 95.7233333333336
    agent-3: 95.7233333333336
    agent-4: 95.7233333333336
    agent-5: 95.7233333333336
  policy_reward_min:
    agent-0: 31.833333333333393
    agent-1: 31.833333333333393
    agent-2: 31.833333333333393
    agent-3: 31.833333333333393
    agent-4: 31.833333333333393
    agent-5: 31.833333333333393
  sampler_perf:
    mean_env_wait_ms: 23.496469561808464
    mean_inference_ms: 12.321498450394072
    mean_processing_ms: 50.88910862508932
  time_since_restore: 11525.576943159103
  time_this_iter_s: 122.63886857032776
  time_total_s: 14736.640629291534
  timestamp: 1637029046
  timesteps_since_restore: 8160000
  timesteps_this_iter: 96000
  timesteps_total: 10080000
  training_iteration: 105
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    105 |          14736.6 | 10080000 |   574.34 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 45
    apples_agent-0_mean: 5.35
    apples_agent-0_min: 0
    apples_agent-1_max: 78
    apples_agent-1_mean: 20.45
    apples_agent-1_min: 0
    apples_agent-2_max: 89
    apples_agent-2_mean: 14.07
    apples_agent-2_min: 0
    apples_agent-3_max: 147
    apples_agent-3_mean: 75.03
    apples_agent-3_min: 15
    apples_agent-4_max: 85
    apples_agent-4_mean: 5.87
    apples_agent-4_min: 0
    apples_agent-5_max: 136
    apples_agent-5_mean: 69.75
    apples_agent-5_min: 2
    cleaning_beam_agent-0_max: 526
    cleaning_beam_agent-0_mean: 273.01
    cleaning_beam_agent-0_min: 138
    cleaning_beam_agent-1_max: 516
    cleaning_beam_agent-1_mean: 263.48
    cleaning_beam_agent-1_min: 86
    cleaning_beam_agent-2_max: 373
    cleaning_beam_agent-2_mean: 229.05
    cleaning_beam_agent-2_min: 90
    cleaning_beam_agent-3_max: 101
    cleaning_beam_agent-3_mean: 46.05
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 513
    cleaning_beam_agent-4_mean: 329.63
    cleaning_beam_agent-4_min: 145
    cleaning_beam_agent-5_max: 710
    cleaning_beam_agent-5_mean: 123.04
    cleaning_beam_agent-5_min: 25
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 13
    fire_beam_agent-5_mean: 0.16
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-19-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 803.9999999999862
  episode_reward_mean: 562.2600000000006
  episode_reward_min: 190.999999999998
  episodes_this_iter: 96
  episodes_total: 10176
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20102.828
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006310080061666667
        entropy: 1.1917511224746704
        entropy_coeff: 0.0017600000137463212
        kl: 0.013799312524497509
        model: {}
        policy_loss: -0.03207394480705261
        total_loss: -0.031442318111658096
        vf_explained_var: 0.07180105149745941
        vf_loss: 13.49178695678711
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006310080061666667
        entropy: 1.1276144981384277
        entropy_coeff: 0.0017600000137463212
        kl: 0.015281153842806816
        model: {}
        policy_loss: -0.03300415351986885
        total_loss: -0.03208840265870094
        vf_explained_var: 0.056140586733818054
        vf_loss: 13.722414016723633
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006310080061666667
        entropy: 1.196731448173523
        entropy_coeff: 0.0017600000137463212
        kl: 0.0170498788356781
        model: {}
        policy_loss: -0.03678901121020317
        total_loss: -0.035867027938365936
        vf_explained_var: 0.09008175134658813
        vf_loss: 13.232366561889648
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006310080061666667
        entropy: 0.8342055678367615
        entropy_coeff: 0.0017600000137463212
        kl: 0.011864501982927322
        model: {}
        policy_loss: -0.025328760966658592
        total_loss: -0.024458279833197594
        vf_explained_var: 0.20747968554496765
        vf_loss: 11.522300720214844
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006310080061666667
        entropy: 1.0576348304748535
        entropy_coeff: 0.0017600000137463212
        kl: 0.016061007976531982
        model: {}
        policy_loss: -0.03483989089727402
        total_loss: -0.0338532030582428
        vf_explained_var: 0.14571170508861542
        vf_loss: 12.420219421386719
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006310080061666667
        entropy: 1.046295404434204
        entropy_coeff: 0.0017600000137463212
        kl: 0.01746917888522148
        model: {}
        policy_loss: -0.03749256953597069
        total_loss: -0.03629350662231445
        vf_explained_var: 0.11113724112510681
        vf_loss: 12.936283111572266
    load_time_ms: 14802.537
    num_steps_sampled: 10176000
    num_steps_trained: 10176000
    sample_time_ms: 88222.224
    update_time_ms: 25.816
  iterations_since_restore: 86
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.812571428571427
    ram_util_percent: 9.598285714285716
  pid: 4061
  policy_reward_max:
    agent-0: 134.00000000000034
    agent-1: 134.00000000000034
    agent-2: 134.00000000000034
    agent-3: 134.00000000000034
    agent-4: 134.00000000000034
    agent-5: 134.00000000000034
  policy_reward_mean:
    agent-0: 93.71000000000024
    agent-1: 93.71000000000024
    agent-2: 93.71000000000024
    agent-3: 93.71000000000024
    agent-4: 93.71000000000024
    agent-5: 93.71000000000024
  policy_reward_min:
    agent-0: 31.833333333333393
    agent-1: 31.833333333333393
    agent-2: 31.833333333333393
    agent-3: 31.833333333333393
    agent-4: 31.833333333333393
    agent-5: 31.833333333333393
  sampler_perf:
    mean_env_wait_ms: 23.495037300366285
    mean_inference_ms: 12.31894738911928
    mean_processing_ms: 50.8800673373217
  time_since_restore: 11648.241270542145
  time_this_iter_s: 122.66432738304138
  time_total_s: 14859.304956674576
  timestamp: 1637029169
  timesteps_since_restore: 8256000
  timesteps_this_iter: 96000
  timesteps_total: 10176000
  training_iteration: 106
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    106 |          14859.3 | 10176000 |   562.26 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 35
    apples_agent-0_mean: 7.34
    apples_agent-0_min: 0
    apples_agent-1_max: 67
    apples_agent-1_mean: 21.08
    apples_agent-1_min: 0
    apples_agent-2_max: 239
    apples_agent-2_mean: 13.58
    apples_agent-2_min: 0
    apples_agent-3_max: 160
    apples_agent-3_mean: 75.49
    apples_agent-3_min: 28
    apples_agent-4_max: 62
    apples_agent-4_mean: 2.3
    apples_agent-4_min: 0
    apples_agent-5_max: 129
    apples_agent-5_mean: 75.64
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 440
    cleaning_beam_agent-0_mean: 274.6
    cleaning_beam_agent-0_min: 123
    cleaning_beam_agent-1_max: 482
    cleaning_beam_agent-1_mean: 262.58
    cleaning_beam_agent-1_min: 117
    cleaning_beam_agent-2_max: 470
    cleaning_beam_agent-2_mean: 234.6
    cleaning_beam_agent-2_min: 100
    cleaning_beam_agent-3_max: 92
    cleaning_beam_agent-3_mean: 42.38
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 491
    cleaning_beam_agent-4_mean: 343.82
    cleaning_beam_agent-4_min: 158
    cleaning_beam_agent-5_max: 510
    cleaning_beam_agent-5_mean: 109.72
    cleaning_beam_agent-5_min: 17
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.04
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-21-33
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 859.9999999999786
  episode_reward_mean: 588.6600000000011
  episode_reward_min: 207.99999999999827
  episodes_this_iter: 96
  episodes_total: 10272
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20107.961
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006250176229514182
        entropy: 1.1936132907867432
        entropy_coeff: 0.0017600000137463212
        kl: 0.015229204669594765
        model: {}
        policy_loss: -0.030250145122408867
        total_loss: -0.029415059834718704
        vf_explained_var: 0.07525411248207092
        vf_loss: 14.129239082336426
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006250176229514182
        entropy: 1.1108185052871704
        entropy_coeff: 0.0017600000137463212
        kl: 0.01525361929088831
        model: {}
        policy_loss: -0.03308884799480438
        total_loss: -0.03208635002374649
        vf_explained_var: 0.06297490000724792
        vf_loss: 14.321800231933594
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006250176229514182
        entropy: 1.1888322830200195
        entropy_coeff: 0.0017600000137463212
        kl: 0.01689906418323517
        model: {}
        policy_loss: -0.03658527880907059
        total_loss: -0.0355859249830246
        vf_explained_var: 0.08231966197490692
        vf_loss: 14.017931938171387
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006250176229514182
        entropy: 0.8041490316390991
        entropy_coeff: 0.0017600000137463212
        kl: 0.011953880079090595
        model: {}
        policy_loss: -0.026043104007840157
        total_loss: -0.025044817477464676
        vf_explained_var: 0.2033584862947464
        vf_loss: 12.182001113891602
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006250176229514182
        entropy: 1.0505385398864746
        entropy_coeff: 0.0017600000137463212
        kl: 0.01588059775531292
        model: {}
        policy_loss: -0.033880021423101425
        total_loss: -0.03281935676932335
        vf_explained_var: 0.13545945286750793
        vf_loss: 13.215497016906738
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006250176229514182
        entropy: 1.01365327835083
        entropy_coeff: 0.0017600000137463212
        kl: 0.01731502264738083
        model: {}
        policy_loss: -0.03811105713248253
        total_loss: -0.03680707514286041
        vf_explained_var: 0.11240355670452118
        vf_loss: 13.56511116027832
    load_time_ms: 14729.279
    num_steps_sampled: 10272000
    num_steps_trained: 10272000
    sample_time_ms: 88221.469
    update_time_ms: 25.997
  iterations_since_restore: 87
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.780113636363637
    ram_util_percent: 9.66931818181818
  pid: 4061
  policy_reward_max:
    agent-0: 143.3333333333334
    agent-1: 143.3333333333334
    agent-2: 143.3333333333334
    agent-3: 143.3333333333334
    agent-4: 143.3333333333334
    agent-5: 143.3333333333334
  policy_reward_mean:
    agent-0: 98.11000000000027
    agent-1: 98.11000000000027
    agent-2: 98.11000000000027
    agent-3: 98.11000000000027
    agent-4: 98.11000000000027
    agent-5: 98.11000000000027
  policy_reward_min:
    agent-0: 34.6666666666667
    agent-1: 34.6666666666667
    agent-2: 34.6666666666667
    agent-3: 34.6666666666667
    agent-4: 34.6666666666667
    agent-5: 34.6666666666667
  sampler_perf:
    mean_env_wait_ms: 23.497059921073316
    mean_inference_ms: 12.317697011807825
    mean_processing_ms: 50.87496765650198
  time_since_restore: 11771.659242868423
  time_this_iter_s: 123.41797232627869
  time_total_s: 14982.722929000854
  timestamp: 1637029293
  timesteps_since_restore: 8352000
  timesteps_this_iter: 96000
  timesteps_total: 10272000
  training_iteration: 107
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    107 |          14982.7 | 10272000 |   588.66 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 49
    apples_agent-0_mean: 5.77
    apples_agent-0_min: 0
    apples_agent-1_max: 79
    apples_agent-1_mean: 22.21
    apples_agent-1_min: 0
    apples_agent-2_max: 134
    apples_agent-2_mean: 15.09
    apples_agent-2_min: 0
    apples_agent-3_max: 141
    apples_agent-3_mean: 78.64
    apples_agent-3_min: 27
    apples_agent-4_max: 93
    apples_agent-4_mean: 2.73
    apples_agent-4_min: 0
    apples_agent-5_max: 146
    apples_agent-5_mean: 76.17
    apples_agent-5_min: 17
    cleaning_beam_agent-0_max: 443
    cleaning_beam_agent-0_mean: 282.77
    cleaning_beam_agent-0_min: 120
    cleaning_beam_agent-1_max: 482
    cleaning_beam_agent-1_mean: 253.44
    cleaning_beam_agent-1_min: 119
    cleaning_beam_agent-2_max: 408
    cleaning_beam_agent-2_mean: 233.58
    cleaning_beam_agent-2_min: 101
    cleaning_beam_agent-3_max: 119
    cleaning_beam_agent-3_mean: 43.83
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 622
    cleaning_beam_agent-4_mean: 350.01
    cleaning_beam_agent-4_min: 176
    cleaning_beam_agent-5_max: 422
    cleaning_beam_agent-5_mean: 117.86
    cleaning_beam_agent-5_min: 20
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-23-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 857.9999999999847
  episode_reward_mean: 606.2899999999996
  episode_reward_min: 243.99999999999622
  episodes_this_iter: 96
  episodes_total: 10368
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20092.96
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006190271815285087
        entropy: 1.1865630149841309
        entropy_coeff: 0.0017600000137463212
        kl: 0.013795195147395134
        model: {}
        policy_loss: -0.032624751329422
        total_loss: -0.03192363679409027
        vf_explained_var: 0.08874240517616272
        vf_loss: 14.09950065612793
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006190271815285087
        entropy: 1.1301742792129517
        entropy_coeff: 0.0017600000137463212
        kl: 0.01595679670572281
        model: {}
        policy_loss: -0.03381771594285965
        total_loss: -0.03276783972978592
        vf_explained_var: 0.06694135069847107
        vf_loss: 14.433058738708496
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006190271815285087
        entropy: 1.1943321228027344
        entropy_coeff: 0.0017600000137463212
        kl: 0.016358867287635803
        model: {}
        policy_loss: -0.0360436774790287
        total_loss: -0.03509088605642319
        vf_explained_var: 0.08321043848991394
        vf_loss: 14.18929386138916
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006190271815285087
        entropy: 0.7689832448959351
        entropy_coeff: 0.0017600000137463212
        kl: 0.01171205285936594
        model: {}
        policy_loss: -0.025187578052282333
        total_loss: -0.024136565625667572
        vf_explained_var: 0.2043009102344513
        vf_loss: 12.332180976867676
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006190271815285087
        entropy: 1.046704888343811
        entropy_coeff: 0.0017600000137463212
        kl: 0.015772603452205658
        model: {}
        policy_loss: -0.03394484519958496
        total_loss: -0.032881319522857666
        vf_explained_var: 0.14201128482818604
        vf_loss: 13.284669876098633
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006190271815285087
        entropy: 1.013681411743164
        entropy_coeff: 0.0017600000137463212
        kl: 0.017981020733714104
        model: {}
        policy_loss: -0.03564707189798355
        total_loss: -0.034290067851543427
        vf_explained_var: 0.13147902488708496
        vf_loss: 13.429733276367188
    load_time_ms: 14705.651
    num_steps_sampled: 10368000
    num_steps_trained: 10368000
    sample_time_ms: 88337.2
    update_time_ms: 25.019
  iterations_since_restore: 88
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.432571428571428
    ram_util_percent: 9.594857142857144
  pid: 4061
  policy_reward_max:
    agent-0: 143.0000000000002
    agent-1: 143.0000000000002
    agent-2: 143.0000000000002
    agent-3: 143.0000000000002
    agent-4: 143.0000000000002
    agent-5: 143.0000000000002
  policy_reward_mean:
    agent-0: 101.04833333333362
    agent-1: 101.04833333333362
    agent-2: 101.04833333333362
    agent-3: 101.04833333333362
    agent-4: 101.04833333333362
    agent-5: 101.04833333333362
  policy_reward_min:
    agent-0: 40.66666666666662
    agent-1: 40.66666666666662
    agent-2: 40.66666666666662
    agent-3: 40.66666666666662
    agent-4: 40.66666666666662
    agent-5: 40.66666666666662
  sampler_perf:
    mean_env_wait_ms: 23.498448012415025
    mean_inference_ms: 12.315733105186778
    mean_processing_ms: 50.86678963236195
  time_since_restore: 11894.545305728912
  time_this_iter_s: 122.88606286048889
  time_total_s: 15105.608991861343
  timestamp: 1637029416
  timesteps_since_restore: 8448000
  timesteps_this_iter: 96000
  timesteps_total: 10368000
  training_iteration: 108
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    108 |          15105.6 | 10368000 |   606.29 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 54
    apples_agent-0_mean: 5.21
    apples_agent-0_min: 0
    apples_agent-1_max: 165
    apples_agent-1_mean: 23.12
    apples_agent-1_min: 0
    apples_agent-2_max: 149
    apples_agent-2_mean: 14.13
    apples_agent-2_min: 0
    apples_agent-3_max: 138
    apples_agent-3_mean: 74.99
    apples_agent-3_min: 26
    apples_agent-4_max: 48
    apples_agent-4_mean: 1.93
    apples_agent-4_min: 0
    apples_agent-5_max: 196
    apples_agent-5_mean: 81.26
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 480
    cleaning_beam_agent-0_mean: 289.42
    cleaning_beam_agent-0_min: 137
    cleaning_beam_agent-1_max: 453
    cleaning_beam_agent-1_mean: 255.44
    cleaning_beam_agent-1_min: 109
    cleaning_beam_agent-2_max: 438
    cleaning_beam_agent-2_mean: 224.55
    cleaning_beam_agent-2_min: 77
    cleaning_beam_agent-3_max: 115
    cleaning_beam_agent-3_mean: 43.62
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 671
    cleaning_beam_agent-4_mean: 350.59
    cleaning_beam_agent-4_min: 152
    cleaning_beam_agent-5_max: 720
    cleaning_beam_agent-5_mean: 111.48
    cleaning_beam_agent-5_min: 13
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.05
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-25-39
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 843.999999999992
  episode_reward_mean: 606.7999999999994
  episode_reward_min: 220.99999999999733
  episodes_this_iter: 96
  episodes_total: 10464
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20120.112
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006130367983132601
        entropy: 1.1906471252441406
        entropy_coeff: 0.0017600000137463212
        kl: 0.013655523769557476
        model: {}
        policy_loss: -0.03304900974035263
        total_loss: -0.03237198293209076
        vf_explained_var: 0.06806330382823944
        vf_loss: 14.070101737976074
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006130367983132601
        entropy: 1.1179112195968628
        entropy_coeff: 0.0017600000137463212
        kl: 0.015671823173761368
        model: {}
        policy_loss: -0.033402781933546066
        total_loss: -0.03237304836511612
        vf_explained_var: 0.053019583225250244
        vf_loss: 14.300751686096191
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006130367983132601
        entropy: 1.1901772022247314
        entropy_coeff: 0.0017600000137463212
        kl: 0.017232302576303482
        model: {}
        policy_loss: -0.03665918484330177
        total_loss: -0.035589247941970825
        vf_explained_var: 0.04557925462722778
        vf_loss: 14.414214134216309
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006130367983132601
        entropy: 0.7594937682151794
        entropy_coeff: 0.0017600000137463212
        kl: 0.011778756976127625
        model: {}
        policy_loss: -0.023573290556669235
        total_loss: -0.022514378651976585
        vf_explained_var: 0.19375085830688477
        vf_loss: 12.177454948425293
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006130367983132601
        entropy: 1.0526270866394043
        entropy_coeff: 0.0017600000137463212
        kl: 0.015105696395039558
        model: {}
        policy_loss: -0.03388620540499687
        total_loss: -0.03291752189397812
        vf_explained_var: 0.13141372799873352
        vf_loss: 13.10738754272461
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006130367983132601
        entropy: 1.0084129571914673
        entropy_coeff: 0.0017600000137463212
        kl: 0.017996685579419136
        model: {}
        policy_loss: -0.03820032253861427
        total_loss: -0.03680107370018959
        vf_explained_var: 0.0897284597158432
        vf_loss: 13.743880271911621
    load_time_ms: 14687.224
    num_steps_sampled: 10464000
    num_steps_trained: 10464000
    sample_time_ms: 88289.233
    update_time_ms: 24.83
  iterations_since_restore: 89
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.321590909090911
    ram_util_percent: 9.66875
  pid: 4061
  policy_reward_max:
    agent-0: 140.66666666666708
    agent-1: 140.66666666666708
    agent-2: 140.66666666666708
    agent-3: 140.66666666666708
    agent-4: 140.66666666666708
    agent-5: 140.66666666666708
  policy_reward_mean:
    agent-0: 101.13333333333361
    agent-1: 101.13333333333361
    agent-2: 101.13333333333361
    agent-3: 101.13333333333361
    agent-4: 101.13333333333361
    agent-5: 101.13333333333361
  policy_reward_min:
    agent-0: 36.83333333333336
    agent-1: 36.83333333333336
    agent-2: 36.83333333333336
    agent-3: 36.83333333333336
    agent-4: 36.83333333333336
    agent-5: 36.83333333333336
  sampler_perf:
    mean_env_wait_ms: 23.499146498543062
    mean_inference_ms: 12.313826851651802
    mean_processing_ms: 50.85826866662753
  time_since_restore: 12017.325021743774
  time_this_iter_s: 122.77971601486206
  time_total_s: 15228.388707876205
  timestamp: 1637029539
  timesteps_since_restore: 8544000
  timesteps_this_iter: 96000
  timesteps_total: 10464000
  training_iteration: 109
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    109 |          15228.4 | 10464000 |    606.8 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 56
    apples_agent-0_mean: 6.87
    apples_agent-0_min: 0
    apples_agent-1_max: 89
    apples_agent-1_mean: 22.18
    apples_agent-1_min: 0
    apples_agent-2_max: 152
    apples_agent-2_mean: 12.69
    apples_agent-2_min: 0
    apples_agent-3_max: 247
    apples_agent-3_mean: 73.02
    apples_agent-3_min: 20
    apples_agent-4_max: 65
    apples_agent-4_mean: 3.23
    apples_agent-4_min: 0
    apples_agent-5_max: 156
    apples_agent-5_mean: 79.54
    apples_agent-5_min: 1
    cleaning_beam_agent-0_max: 451
    cleaning_beam_agent-0_mean: 279.32
    cleaning_beam_agent-0_min: 139
    cleaning_beam_agent-1_max: 453
    cleaning_beam_agent-1_mean: 251.46
    cleaning_beam_agent-1_min: 128
    cleaning_beam_agent-2_max: 347
    cleaning_beam_agent-2_mean: 229.07
    cleaning_beam_agent-2_min: 89
    cleaning_beam_agent-3_max: 105
    cleaning_beam_agent-3_mean: 42.49
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 559
    cleaning_beam_agent-4_mean: 333.24
    cleaning_beam_agent-4_min: 177
    cleaning_beam_agent-5_max: 369
    cleaning_beam_agent-5_mean: 99.21
    cleaning_beam_agent-5_min: 15
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 3
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.1
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-27-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 842.9999999999804
  episode_reward_mean: 584.8100000000001
  episode_reward_min: 180.99999999999895
  episodes_this_iter: 96
  episodes_total: 10560
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20138.273
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006070464150980115
        entropy: 1.199201226234436
        entropy_coeff: 0.0017600000137463212
        kl: 0.01492384448647499
        model: {}
        policy_loss: -0.031983695924282074
        total_loss: -0.031118545681238174
        vf_explained_var: 0.06665956974029541
        vf_loss: 14.833617210388184
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006070464150980115
        entropy: 1.1473026275634766
        entropy_coeff: 0.0017600000137463212
        kl: 0.015902966260910034
        model: {}
        policy_loss: -0.034924283623695374
        total_loss: -0.03385213762521744
        vf_explained_var: 0.055108413100242615
        vf_loss: 15.011024475097656
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006070464150980115
        entropy: 1.2065620422363281
        entropy_coeff: 0.0017600000137463212
        kl: 0.016592003405094147
        model: {}
        policy_loss: -0.036917537450790405
        total_loss: -0.035935696214437485
        vf_explained_var: 0.08935016393661499
        vf_loss: 14.461872100830078
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006070464150980115
        entropy: 0.7821197509765625
        entropy_coeff: 0.0017600000137463212
        kl: 0.012042393907904625
        model: {}
        policy_loss: -0.024796137586236
        total_loss: -0.023700999096035957
        vf_explained_var: 0.20202475786209106
        vf_loss: 12.674304962158203
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006070464150980115
        entropy: 1.042742133140564
        entropy_coeff: 0.0017600000137463212
        kl: 0.016294322907924652
        model: {}
        policy_loss: -0.03438573703169823
        total_loss: -0.033175669610500336
        vf_explained_var: 0.10901413857936859
        vf_loss: 14.158576965332031
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0006070464150980115
        entropy: 1.0331815481185913
        entropy_coeff: 0.0017600000137463212
        kl: 0.017542457208037376
        model: {}
        policy_loss: -0.03930313512682915
        total_loss: -0.03800256550312042
        vf_explained_var: 0.1418718546628952
        vf_loss: 13.647226333618164
    load_time_ms: 14669.118
    num_steps_sampled: 10560000
    num_steps_trained: 10560000
    sample_time_ms: 88365.841
    update_time_ms: 24.888
  iterations_since_restore: 90
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.438068181818181
    ram_util_percent: 9.596590909090908
  pid: 4061
  policy_reward_max:
    agent-0: 140.50000000000043
    agent-1: 140.50000000000043
    agent-2: 140.50000000000043
    agent-3: 140.50000000000043
    agent-4: 140.50000000000043
    agent-5: 140.50000000000043
  policy_reward_mean:
    agent-0: 97.46833333333358
    agent-1: 97.46833333333358
    agent-2: 97.46833333333358
    agent-3: 97.46833333333358
    agent-4: 97.46833333333358
    agent-5: 97.46833333333358
  policy_reward_min:
    agent-0: 30.16666666666673
    agent-1: 30.16666666666673
    agent-2: 30.16666666666673
    agent-3: 30.16666666666673
    agent-4: 30.16666666666673
    agent-5: 30.16666666666673
  sampler_perf:
    mean_env_wait_ms: 23.49809305397014
    mean_inference_ms: 12.312410429457147
    mean_processing_ms: 50.8512400966167
  time_since_restore: 12140.956888198853
  time_this_iter_s: 123.63186645507812
  time_total_s: 15352.020574331284
  timestamp: 1637029662
  timesteps_since_restore: 8640000
  timesteps_this_iter: 96000
  timesteps_total: 10560000
  training_iteration: 110
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    110 |            15352 | 10560000 |   584.81 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 34
    apples_agent-0_mean: 6.87
    apples_agent-0_min: 0
    apples_agent-1_max: 92
    apples_agent-1_mean: 21.74
    apples_agent-1_min: 0
    apples_agent-2_max: 80
    apples_agent-2_mean: 14.78
    apples_agent-2_min: 0
    apples_agent-3_max: 141
    apples_agent-3_mean: 71.22
    apples_agent-3_min: 26
    apples_agent-4_max: 76
    apples_agent-4_mean: 3.74
    apples_agent-4_min: 0
    apples_agent-5_max: 156
    apples_agent-5_mean: 78.92
    apples_agent-5_min: 8
    cleaning_beam_agent-0_max: 475
    cleaning_beam_agent-0_mean: 297.21
    cleaning_beam_agent-0_min: 119
    cleaning_beam_agent-1_max: 510
    cleaning_beam_agent-1_mean: 275.85
    cleaning_beam_agent-1_min: 142
    cleaning_beam_agent-2_max: 321
    cleaning_beam_agent-2_mean: 215.6
    cleaning_beam_agent-2_min: 66
    cleaning_beam_agent-3_max: 104
    cleaning_beam_agent-3_mean: 42.45
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 545
    cleaning_beam_agent-4_mean: 330.14
    cleaning_beam_agent-4_min: 161
    cleaning_beam_agent-5_max: 441
    cleaning_beam_agent-5_mean: 104.13
    cleaning_beam_agent-5_min: 14
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-29-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 817.9999999999868
  episode_reward_mean: 590.0400000000003
  episode_reward_min: 239.9999999999955
  episodes_this_iter: 96
  episodes_total: 10656
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20143.297
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000601055973675102
        entropy: 1.1871678829193115
        entropy_coeff: 0.0017600000137463212
        kl: 0.014238880947232246
        model: {}
        policy_loss: -0.033691272139549255
        total_loss: -0.03293314576148987
        vf_explained_var: 0.09271998703479767
        vf_loss: 14.236536979675293
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000601055973675102
        entropy: 1.113131046295166
        entropy_coeff: 0.0017600000137463212
        kl: 0.015993118286132812
        model: {}
        policy_loss: -0.03320302069187164
        total_loss: -0.032080285251140594
        vf_explained_var: 0.055343568325042725
        vf_loss: 14.825371742248535
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000601055973675102
        entropy: 1.1759980916976929
        entropy_coeff: 0.0017600000137463212
        kl: 0.016833286732435226
        model: {}
        policy_loss: -0.03627569228410721
        total_loss: -0.03514258563518524
        vf_explained_var: 0.031074196100234985
        vf_loss: 15.195367813110352
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000601055973675102
        entropy: 0.7758875489234924
        entropy_coeff: 0.0017600000137463212
        kl: 0.011470917612314224
        model: {}
        policy_loss: -0.024033866822719574
        total_loss: -0.023026322945952415
        vf_explained_var: 0.21837368607521057
        vf_loss: 12.260154724121094
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000601055973675102
        entropy: 1.0359753370285034
        entropy_coeff: 0.0017600000137463212
        kl: 0.015647603198885918
        model: {}
        policy_loss: -0.03378431871533394
        total_loss: -0.032673779875040054
        vf_explained_var: 0.12766778469085693
        vf_loss: 13.69088363647461
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000601055973675102
        entropy: 1.014723539352417
        entropy_coeff: 0.0017600000137463212
        kl: 0.01784392073750496
        model: {}
        policy_loss: -0.03820926323533058
        total_loss: -0.036852285265922546
        vf_explained_var: 0.1342039704322815
        vf_loss: 13.585005760192871
    load_time_ms: 14665.715
    num_steps_sampled: 10656000
    num_steps_trained: 10656000
    sample_time_ms: 88402.431
    update_time_ms: 24.815
  iterations_since_restore: 91
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.74457142857143
    ram_util_percent: 9.644000000000002
  pid: 4061
  policy_reward_max:
    agent-0: 136.33333333333377
    agent-1: 136.33333333333377
    agent-2: 136.33333333333377
    agent-3: 136.33333333333377
    agent-4: 136.33333333333377
    agent-5: 136.33333333333377
  policy_reward_mean:
    agent-0: 98.34000000000026
    agent-1: 98.34000000000026
    agent-2: 98.34000000000026
    agent-3: 98.34000000000026
    agent-4: 98.34000000000026
    agent-5: 98.34000000000026
  policy_reward_min:
    agent-0: 39.99999999999998
    agent-1: 39.99999999999998
    agent-2: 39.99999999999998
    agent-3: 39.99999999999998
    agent-4: 39.99999999999998
    agent-5: 39.99999999999998
  sampler_perf:
    mean_env_wait_ms: 23.49764928739957
    mean_inference_ms: 12.310632060417406
    mean_processing_ms: 50.842897791363875
  time_since_restore: 12263.604335069656
  time_this_iter_s: 122.64744687080383
  time_total_s: 15474.668021202087
  timestamp: 1637029785
  timesteps_since_restore: 8736000
  timesteps_this_iter: 96000
  timesteps_total: 10656000
  training_iteration: 111
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    111 |          15474.7 | 10656000 |   590.04 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 60
    apples_agent-0_mean: 6.03
    apples_agent-0_min: 0
    apples_agent-1_max: 109
    apples_agent-1_mean: 24.07
    apples_agent-1_min: 0
    apples_agent-2_max: 155
    apples_agent-2_mean: 15.99
    apples_agent-2_min: 0
    apples_agent-3_max: 136
    apples_agent-3_mean: 70.9
    apples_agent-3_min: 27
    apples_agent-4_max: 57
    apples_agent-4_mean: 2.17
    apples_agent-4_min: 0
    apples_agent-5_max: 153
    apples_agent-5_mean: 71.51
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 466
    cleaning_beam_agent-0_mean: 273.02
    cleaning_beam_agent-0_min: 149
    cleaning_beam_agent-1_max: 484
    cleaning_beam_agent-1_mean: 249.17
    cleaning_beam_agent-1_min: 158
    cleaning_beam_agent-2_max: 340
    cleaning_beam_agent-2_mean: 223.9
    cleaning_beam_agent-2_min: 109
    cleaning_beam_agent-3_max: 100
    cleaning_beam_agent-3_mean: 39.16
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 632
    cleaning_beam_agent-4_mean: 343.06
    cleaning_beam_agent-4_min: 193
    cleaning_beam_agent-5_max: 709
    cleaning_beam_agent-5_mean: 124.84
    cleaning_beam_agent-5_min: 15
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.04
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-31-49
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 836.9999999999825
  episode_reward_mean: 596.6800000000002
  episode_reward_min: 315.00000000000097
  episodes_this_iter: 96
  episodes_total: 10752
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20127.622
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005950655904598534
        entropy: 1.1957557201385498
        entropy_coeff: 0.0017600000137463212
        kl: 0.014292242005467415
        model: {}
        policy_loss: -0.030475575476884842
        total_loss: -0.02975926175713539
        vf_explained_var: 0.06193952262401581
        vf_loss: 13.916186332702637
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005950655904598534
        entropy: 1.1299011707305908
        entropy_coeff: 0.0017600000137463212
        kl: 0.015865718945860863
        model: {}
        policy_loss: -0.03434166684746742
        total_loss: -0.033373743295669556
        vf_explained_var: 0.0763489305973053
        vf_loss: 13.699731826782227
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005950655904598534
        entropy: 1.1662429571151733
        entropy_coeff: 0.0017600000137463212
        kl: 0.0171416774392128
        model: {}
        policy_loss: -0.03397396579384804
        total_loss: -0.032936036586761475
        vf_explained_var: 0.07246969640254974
        vf_loss: 13.76348876953125
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005950655904598534
        entropy: 0.7396818399429321
        entropy_coeff: 0.0017600000137463212
        kl: 0.011136023327708244
        model: {}
        policy_loss: -0.02420988492667675
        total_loss: -0.023237232118844986
        vf_explained_var: 0.2167978435754776
        vf_loss: 11.608881950378418
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005950655904598534
        entropy: 1.0213232040405273
        entropy_coeff: 0.0017600000137463212
        kl: 0.015631042420864105
        model: {}
        policy_loss: -0.03217326104640961
        total_loss: -0.031092040240764618
        vf_explained_var: 0.11318039894104004
        vf_loss: 13.156465530395508
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005950655904598534
        entropy: 1.0066438913345337
        entropy_coeff: 0.0017600000137463212
        kl: 0.01688186638057232
        model: {}
        policy_loss: -0.0379486046731472
        total_loss: -0.03667568042874336
        vf_explained_var: 0.08581371605396271
        vf_loss: 13.564302444458008
    load_time_ms: 14812.643
    num_steps_sampled: 10752000
    num_steps_trained: 10752000
    sample_time_ms: 88412.157
    update_time_ms: 24.883
  iterations_since_restore: 92
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 16.329545454545453
    ram_util_percent: 9.692613636363637
  pid: 4061
  policy_reward_max:
    agent-0: 139.50000000000045
    agent-1: 139.50000000000045
    agent-2: 139.50000000000045
    agent-3: 139.50000000000045
    agent-4: 139.50000000000045
    agent-5: 139.50000000000045
  policy_reward_mean:
    agent-0: 99.44666666666696
    agent-1: 99.44666666666696
    agent-2: 99.44666666666696
    agent-3: 99.44666666666696
    agent-4: 99.44666666666696
    agent-5: 99.44666666666696
  policy_reward_min:
    agent-0: 52.49999999999985
    agent-1: 52.49999999999985
    agent-2: 52.49999999999985
    agent-3: 52.49999999999985
    agent-4: 52.49999999999985
    agent-5: 52.49999999999985
  sampler_perf:
    mean_env_wait_ms: 23.49903425191143
    mean_inference_ms: 12.311222570437224
    mean_processing_ms: 50.83898326916448
  time_since_restore: 12387.02319264412
  time_this_iter_s: 123.41885757446289
  time_total_s: 15598.08687877655
  timestamp: 1637029909
  timesteps_since_restore: 8832000
  timesteps_this_iter: 96000
  timesteps_total: 10752000
  training_iteration: 112
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    112 |          15598.1 | 10752000 |   596.68 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 73
    apples_agent-0_mean: 5.86
    apples_agent-0_min: 0
    apples_agent-1_max: 70
    apples_agent-1_mean: 20.59
    apples_agent-1_min: 0
    apples_agent-2_max: 63
    apples_agent-2_mean: 11.11
    apples_agent-2_min: 0
    apples_agent-3_max: 123
    apples_agent-3_mean: 70.22
    apples_agent-3_min: 26
    apples_agent-4_max: 51
    apples_agent-4_mean: 1.81
    apples_agent-4_min: 0
    apples_agent-5_max: 156
    apples_agent-5_mean: 73.48
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 503
    cleaning_beam_agent-0_mean: 295.83
    cleaning_beam_agent-0_min: 139
    cleaning_beam_agent-1_max: 399
    cleaning_beam_agent-1_mean: 258.58
    cleaning_beam_agent-1_min: 133
    cleaning_beam_agent-2_max: 350
    cleaning_beam_agent-2_mean: 223.48
    cleaning_beam_agent-2_min: 62
    cleaning_beam_agent-3_max: 131
    cleaning_beam_agent-3_mean: 43.95
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 570
    cleaning_beam_agent-4_mean: 316.98
    cleaning_beam_agent-4_min: 130
    cleaning_beam_agent-5_max: 548
    cleaning_beam_agent-5_mean: 111.04
    cleaning_beam_agent-5_min: 16
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-33-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 885.999999999995
  episode_reward_mean: 593.7899999999992
  episode_reward_min: 249.99999999999653
  episodes_this_iter: 96
  episodes_total: 10848
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20146.245
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005890752072446048
        entropy: 1.189005732536316
        entropy_coeff: 0.0017600000137463212
        kl: 0.014206591062247753
        model: {}
        policy_loss: -0.03068377450108528
        total_loss: -0.029858052730560303
        vf_explained_var: 0.08883093297481537
        vf_loss: 14.977119445800781
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005890752072446048
        entropy: 1.1088746786117554
        entropy_coeff: 0.0017600000137463212
        kl: 0.015611378476023674
        model: {}
        policy_loss: -0.03331947326660156
        total_loss: -0.032183580100536346
        vf_explained_var: 0.07192303240299225
        vf_loss: 15.263731002807617
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005890752072446048
        entropy: 1.1663178205490112
        entropy_coeff: 0.0017600000137463212
        kl: 0.01852758601307869
        model: {}
        policy_loss: -0.03439448028802872
        total_loss: -0.0330561026930809
        vf_explained_var: 0.0646713525056839
        vf_loss: 15.383377075195312
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005890752072446048
        entropy: 0.7428404092788696
        entropy_coeff: 0.0017600000137463212
        kl: 0.012149045243859291
        model: {}
        policy_loss: -0.024434827268123627
        total_loss: -0.023255271837115288
        vf_explained_var: 0.22625109553337097
        vf_loss: 12.720541000366211
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005890752072446048
        entropy: 1.0127466917037964
        entropy_coeff: 0.0017600000137463212
        kl: 0.014682209119200706
        model: {}
        policy_loss: -0.031731341034173965
        total_loss: -0.03065534494817257
        vf_explained_var: 0.15431924164295197
        vf_loss: 13.902130126953125
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005890752072446048
        entropy: 1.0210587978363037
        entropy_coeff: 0.0017600000137463212
        kl: 0.016104862093925476
        model: {}
        policy_loss: -0.038594797253608704
        total_loss: -0.03740422800183296
        vf_explained_var: 0.1625162959098816
        vf_loss: 13.77147102355957
    load_time_ms: 14821.523
    num_steps_sampled: 10848000
    num_steps_trained: 10848000
    sample_time_ms: 88453.09
    update_time_ms: 24.506
  iterations_since_restore: 93
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.413142857142859
    ram_util_percent: 9.669142857142859
  pid: 4061
  policy_reward_max:
    agent-0: 147.66666666666686
    agent-1: 147.66666666666686
    agent-2: 147.66666666666686
    agent-3: 147.66666666666686
    agent-4: 147.66666666666686
    agent-5: 147.66666666666686
  policy_reward_mean:
    agent-0: 98.96500000000027
    agent-1: 98.96500000000027
    agent-2: 98.96500000000027
    agent-3: 98.96500000000027
    agent-4: 98.96500000000027
    agent-5: 98.96500000000027
  policy_reward_min:
    agent-0: 41.66666666666659
    agent-1: 41.66666666666659
    agent-2: 41.66666666666659
    agent-3: 41.66666666666659
    agent-4: 41.66666666666659
    agent-5: 41.66666666666659
  sampler_perf:
    mean_env_wait_ms: 23.499658903274835
    mean_inference_ms: 12.310299936147238
    mean_processing_ms: 50.832810326498084
  time_since_restore: 12509.593392372131
  time_this_iter_s: 122.57019972801208
  time_total_s: 15720.657078504562
  timestamp: 1637030031
  timesteps_since_restore: 8928000
  timesteps_this_iter: 96000
  timesteps_total: 10848000
  training_iteration: 113
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    113 |          15720.7 | 10848000 |   593.79 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 58
    apples_agent-0_mean: 5.56
    apples_agent-0_min: 0
    apples_agent-1_max: 112
    apples_agent-1_mean: 20.7
    apples_agent-1_min: 0
    apples_agent-2_max: 111
    apples_agent-2_mean: 13.11
    apples_agent-2_min: 0
    apples_agent-3_max: 112
    apples_agent-3_mean: 68.5
    apples_agent-3_min: 18
    apples_agent-4_max: 101
    apples_agent-4_mean: 4.34
    apples_agent-4_min: 0
    apples_agent-5_max: 148
    apples_agent-5_mean: 76.45
    apples_agent-5_min: 9
    cleaning_beam_agent-0_max: 506
    cleaning_beam_agent-0_mean: 292.95
    cleaning_beam_agent-0_min: 152
    cleaning_beam_agent-1_max: 433
    cleaning_beam_agent-1_mean: 245.84
    cleaning_beam_agent-1_min: 130
    cleaning_beam_agent-2_max: 351
    cleaning_beam_agent-2_mean: 226.15
    cleaning_beam_agent-2_min: 60
    cleaning_beam_agent-3_max: 100
    cleaning_beam_agent-3_mean: 39.54
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 625
    cleaning_beam_agent-4_mean: 332.48
    cleaning_beam_agent-4_min: 101
    cleaning_beam_agent-5_max: 569
    cleaning_beam_agent-5_mean: 110.82
    cleaning_beam_agent-5_min: 22
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-35-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 853.9999999999876
  episode_reward_mean: 598.3199999999998
  episode_reward_min: 222.9999999999962
  episodes_this_iter: 96
  episodes_total: 10944
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20185.586
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005830848240293562
        entropy: 1.1721363067626953
        entropy_coeff: 0.0017600000137463212
        kl: 0.013404380530118942
        model: {}
        policy_loss: -0.031514979898929596
        total_loss: -0.030706971883773804
        vf_explained_var: 0.08377350866794586
        vf_loss: 15.305234909057617
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005830848240293562
        entropy: 1.1238359212875366
        entropy_coeff: 0.0017600000137463212
        kl: 0.016290683299303055
        model: {}
        policy_loss: -0.03363797068595886
        total_loss: -0.03241245076060295
        vf_explained_var: 0.057404398918151855
        vf_loss: 15.744044303894043
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005830848240293562
        entropy: 1.1621602773666382
        entropy_coeff: 0.0017600000137463212
        kl: 0.01784096285700798
        model: {}
        policy_loss: -0.03409992903470993
        total_loss: -0.03278490900993347
        vf_explained_var: 0.056456029415130615
        vf_loss: 15.763229370117188
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005830848240293562
        entropy: 0.7101386189460754
        entropy_coeff: 0.0017600000137463212
        kl: 0.010931840166449547
        model: {}
        policy_loss: -0.024431586265563965
        total_loss: -0.023270469158887863
        vf_explained_var: 0.21136166155338287
        vf_loss: 13.177777290344238
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005830848240293562
        entropy: 1.0330021381378174
        entropy_coeff: 0.0017600000137463212
        kl: 0.01607782021164894
        model: {}
        policy_loss: -0.03285577893257141
        total_loss: -0.03164629265666008
        vf_explained_var: 0.1497568041086197
        vf_loss: 14.197941780090332
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005830848240293562
        entropy: 1.010656714439392
        entropy_coeff: 0.0017600000137463212
        kl: 0.017324453219771385
        model: {}
        policy_loss: -0.039081647992134094
        total_loss: -0.0377228669822216
        vf_explained_var: 0.1586037427186966
        vf_loss: 14.050894737243652
    load_time_ms: 14153.168
    num_steps_sampled: 10944000
    num_steps_trained: 10944000
    sample_time_ms: 88543.86
    update_time_ms: 24.108
  iterations_since_restore: 94
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.403999999999998
    ram_util_percent: 9.532000000000002
  pid: 4061
  policy_reward_max:
    agent-0: 142.33333333333385
    agent-1: 142.33333333333385
    agent-2: 142.33333333333385
    agent-3: 142.33333333333385
    agent-4: 142.33333333333385
    agent-5: 142.33333333333385
  policy_reward_mean:
    agent-0: 99.7200000000003
    agent-1: 99.7200000000003
    agent-2: 99.7200000000003
    agent-3: 99.7200000000003
    agent-4: 99.7200000000003
    agent-5: 99.7200000000003
  policy_reward_min:
    agent-0: 37.16666666666668
    agent-1: 37.16666666666668
    agent-2: 37.16666666666668
    agent-3: 37.16666666666668
    agent-4: 37.16666666666668
    agent-5: 37.16666666666668
  sampler_perf:
    mean_env_wait_ms: 23.497847429691205
    mean_inference_ms: 12.308316980274867
    mean_processing_ms: 50.824649322807616
  time_since_restore: 12632.990094900131
  time_this_iter_s: 123.39670252799988
  time_total_s: 15844.053781032562
  timestamp: 1637030155
  timesteps_since_restore: 9024000
  timesteps_this_iter: 96000
  timesteps_total: 10944000
  training_iteration: 114
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    114 |          15844.1 | 10944000 |   598.32 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 70
    apples_agent-0_mean: 3.89
    apples_agent-0_min: 0
    apples_agent-1_max: 162
    apples_agent-1_mean: 23.08
    apples_agent-1_min: 0
    apples_agent-2_max: 133
    apples_agent-2_mean: 16.3
    apples_agent-2_min: 0
    apples_agent-3_max: 125
    apples_agent-3_mean: 68.35
    apples_agent-3_min: 26
    apples_agent-4_max: 71
    apples_agent-4_mean: 4.7
    apples_agent-4_min: 0
    apples_agent-5_max: 193
    apples_agent-5_mean: 84.18
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 479
    cleaning_beam_agent-0_mean: 303.53
    cleaning_beam_agent-0_min: 98
    cleaning_beam_agent-1_max: 472
    cleaning_beam_agent-1_mean: 241.66
    cleaning_beam_agent-1_min: 73
    cleaning_beam_agent-2_max: 345
    cleaning_beam_agent-2_mean: 214.2
    cleaning_beam_agent-2_min: 86
    cleaning_beam_agent-3_max: 124
    cleaning_beam_agent-3_mean: 38.8
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 531
    cleaning_beam_agent-4_mean: 324.1
    cleaning_beam_agent-4_min: 111
    cleaning_beam_agent-5_max: 826
    cleaning_beam_agent-5_mean: 99.17
    cleaning_beam_agent-5_min: 15
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-37-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 817.9999999999844
  episode_reward_mean: 596.2799999999987
  episode_reward_min: 165.9999999999991
  episodes_this_iter: 96
  episodes_total: 11040
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20177.517
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005770943826064467
        entropy: 1.1717945337295532
        entropy_coeff: 0.0017600000137463212
        kl: 0.013857079669833183
        model: {}
        policy_loss: -0.03208805248141289
        total_loss: -0.031297434121370316
        vf_explained_var: 0.09917654097080231
        vf_loss: 14.672718048095703
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005770943826064467
        entropy: 1.1286298036575317
        entropy_coeff: 0.0017600000137463212
        kl: 0.015560341998934746
        model: {}
        policy_loss: -0.03432255983352661
        total_loss: -0.03320269286632538
        vf_explained_var: 0.048424139618873596
        vf_loss: 15.502199172973633
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005770943826064467
        entropy: 1.1579382419586182
        entropy_coeff: 0.0017600000137463212
        kl: 0.015958774834871292
        model: {}
        policy_loss: -0.03494078293442726
        total_loss: -0.033855073153972626
        vf_explained_var: 0.0621153861284256
        vf_loss: 15.278047561645508
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005770943826064467
        entropy: 0.7157167196273804
        entropy_coeff: 0.0017600000137463212
        kl: 0.01108078844845295
        model: {}
        policy_loss: -0.02451344020664692
        total_loss: -0.023409370332956314
        vf_explained_var: 0.2291634976863861
        vf_loss: 12.556496620178223
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005770943826064467
        entropy: 1.0370674133300781
        entropy_coeff: 0.0017600000137463212
        kl: 0.01716502383351326
        model: {}
        policy_loss: -0.029948392882943153
        total_loss: -0.02865356020629406
        vf_explained_var: 0.13791108131408691
        vf_loss: 14.035690307617188
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005770943826064467
        entropy: 1.028066873550415
        entropy_coeff: 0.0017600000137463212
        kl: 0.017451107501983643
        model: {}
        policy_loss: -0.04007694497704506
        total_loss: -0.038742050528526306
        vf_explained_var: 0.14112821221351624
        vf_loss: 13.991772651672363
    load_time_ms: 14188.714
    num_steps_sampled: 11040000
    num_steps_trained: 11040000
    sample_time_ms: 88433.561
    update_time_ms: 20.116
  iterations_since_restore: 95
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.50057471264368
    ram_util_percent: 9.674137931034483
  pid: 4061
  policy_reward_max:
    agent-0: 136.33333333333394
    agent-1: 136.33333333333394
    agent-2: 136.33333333333394
    agent-3: 136.33333333333394
    agent-4: 136.33333333333394
    agent-5: 136.33333333333394
  policy_reward_mean:
    agent-0: 99.38000000000028
    agent-1: 99.38000000000028
    agent-2: 99.38000000000028
    agent-3: 99.38000000000028
    agent-4: 99.38000000000028
    agent-5: 99.38000000000028
  policy_reward_min:
    agent-0: 27.666666666666735
    agent-1: 27.666666666666735
    agent-2: 27.666666666666735
    agent-3: 27.666666666666735
    agent-4: 27.666666666666735
    agent-5: 27.666666666666735
  sampler_perf:
    mean_env_wait_ms: 23.49556595260823
    mean_inference_ms: 12.306434541525393
    mean_processing_ms: 50.81548068139899
  time_since_restore: 12754.776918172836
  time_this_iter_s: 121.78682327270508
  time_total_s: 15965.840604305267
  timestamp: 1637030277
  timesteps_since_restore: 9120000
  timesteps_this_iter: 96000
  timesteps_total: 11040000
  training_iteration: 115
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    115 |          15965.8 | 11040000 |   596.28 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 46
    apples_agent-0_mean: 5.98
    apples_agent-0_min: 0
    apples_agent-1_max: 96
    apples_agent-1_mean: 18.96
    apples_agent-1_min: 0
    apples_agent-2_max: 159
    apples_agent-2_mean: 10.14
    apples_agent-2_min: 0
    apples_agent-3_max: 121
    apples_agent-3_mean: 73.91
    apples_agent-3_min: 26
    apples_agent-4_max: 113
    apples_agent-4_mean: 5.38
    apples_agent-4_min: 0
    apples_agent-5_max: 189
    apples_agent-5_mean: 82.12
    apples_agent-5_min: 11
    cleaning_beam_agent-0_max: 460
    cleaning_beam_agent-0_mean: 285.6
    cleaning_beam_agent-0_min: 133
    cleaning_beam_agent-1_max: 393
    cleaning_beam_agent-1_mean: 248.1
    cleaning_beam_agent-1_min: 139
    cleaning_beam_agent-2_max: 340
    cleaning_beam_agent-2_mean: 229.96
    cleaning_beam_agent-2_min: 86
    cleaning_beam_agent-3_max: 121
    cleaning_beam_agent-3_mean: 38.8
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 511
    cleaning_beam_agent-4_mean: 334.08
    cleaning_beam_agent-4_min: 140
    cleaning_beam_agent-5_max: 486
    cleaning_beam_agent-5_mean: 105.11
    cleaning_beam_agent-5_min: 20
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.09
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-40-00
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 891.9999999999948
  episode_reward_mean: 625.9099999999971
  episode_reward_min: 218.999999999997
  episodes_this_iter: 96
  episodes_total: 11136
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20179.992
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005711039993911982
        entropy: 1.1891287565231323
        entropy_coeff: 0.0017600000137463212
        kl: 0.01366659626364708
        model: {}
        policy_loss: -0.03288862481713295
        total_loss: -0.03213421627879143
        vf_explained_var: 0.07470884919166565
        vf_loss: 14.806129455566406
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005711039993911982
        entropy: 1.1061846017837524
        entropy_coeff: 0.0017600000137463212
        kl: 0.014537119306623936
        model: {}
        policy_loss: -0.033417753875255585
        total_loss: -0.03238028660416603
        vf_explained_var: 0.04298684000968933
        vf_loss: 15.30642318725586
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005711039993911982
        entropy: 1.156907558441162
        entropy_coeff: 0.0017600000137463212
        kl: 0.019132647663354874
        model: {}
        policy_loss: -0.03261779621243477
        total_loss: -0.031186366453766823
        vf_explained_var: 0.02831532061100006
        vf_loss: 15.543194770812988
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005711039993911982
        entropy: 0.6968685388565063
        entropy_coeff: 0.0017600000137463212
        kl: 0.010650476440787315
        model: {}
        policy_loss: -0.023712536320090294
        total_loss: -0.022586947306990623
        vf_explained_var: 0.19633542001247406
        vf_loss: 12.870307922363281
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005711039993911982
        entropy: 1.0317299365997314
        entropy_coeff: 0.0017600000137463212
        kl: 0.01711784116923809
        model: {}
        policy_loss: -0.02836383879184723
        total_loss: -0.02706347592175007
        vf_explained_var: 0.12268860638141632
        vf_loss: 14.04422664642334
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005711039993911982
        entropy: 0.9953290224075317
        entropy_coeff: 0.0017600000137463212
        kl: 0.01714569702744484
        model: {}
        policy_loss: -0.03995801880955696
        total_loss: -0.03863883018493652
        vf_explained_var: 0.15328046679496765
        vf_loss: 13.56395149230957
    load_time_ms: 14181.813
    num_steps_sampled: 11136000
    num_steps_trained: 11136000
    sample_time_ms: 88445.529
    update_time_ms: 20.194
  iterations_since_restore: 96
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.787999999999998
    ram_util_percent: 9.605714285714289
  pid: 4061
  policy_reward_max:
    agent-0: 148.66666666666674
    agent-1: 148.66666666666674
    agent-2: 148.66666666666674
    agent-3: 148.66666666666674
    agent-4: 148.66666666666674
    agent-5: 148.66666666666674
  policy_reward_mean:
    agent-0: 104.31833333333365
    agent-1: 104.31833333333365
    agent-2: 104.31833333333365
    agent-3: 104.31833333333365
    agent-4: 104.31833333333365
    agent-5: 104.31833333333365
  policy_reward_min:
    agent-0: 36.500000000000014
    agent-1: 36.500000000000014
    agent-2: 36.500000000000014
    agent-3: 36.500000000000014
    agent-4: 36.500000000000014
    agent-5: 36.500000000000014
  sampler_perf:
    mean_env_wait_ms: 23.494993791332867
    mean_inference_ms: 12.305058350985473
    mean_processing_ms: 50.807450238260756
  time_since_restore: 12877.521822929382
  time_this_iter_s: 122.74490475654602
  time_total_s: 16088.585509061813
  timestamp: 1637030400
  timesteps_since_restore: 9216000
  timesteps_this_iter: 96000
  timesteps_total: 11136000
  training_iteration: 116
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    116 |          16088.6 | 11136000 |   625.91 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 56
    apples_agent-0_mean: 5.22
    apples_agent-0_min: 0
    apples_agent-1_max: 101
    apples_agent-1_mean: 24.31
    apples_agent-1_min: 0
    apples_agent-2_max: 85
    apples_agent-2_mean: 10.63
    apples_agent-2_min: 0
    apples_agent-3_max: 142
    apples_agent-3_mean: 74.11
    apples_agent-3_min: 38
    apples_agent-4_max: 113
    apples_agent-4_mean: 2.14
    apples_agent-4_min: 0
    apples_agent-5_max: 185
    apples_agent-5_mean: 78.02
    apples_agent-5_min: 8
    cleaning_beam_agent-0_max: 432
    cleaning_beam_agent-0_mean: 277.34
    cleaning_beam_agent-0_min: 134
    cleaning_beam_agent-1_max: 446
    cleaning_beam_agent-1_mean: 231.4
    cleaning_beam_agent-1_min: 71
    cleaning_beam_agent-2_max: 369
    cleaning_beam_agent-2_mean: 232.44
    cleaning_beam_agent-2_min: 72
    cleaning_beam_agent-3_max: 110
    cleaning_beam_agent-3_mean: 41.87
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 510
    cleaning_beam_agent-4_mean: 342.14
    cleaning_beam_agent-4_min: 181
    cleaning_beam_agent-5_max: 384
    cleaning_beam_agent-5_mean: 110.05
    cleaning_beam_agent-5_min: 18
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.07
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-42-01
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 891.9999999999948
  episode_reward_mean: 624.8299999999982
  episode_reward_min: 308.0000000000005
  episodes_this_iter: 96
  episodes_total: 11232
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20136.59
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005651136161759496
        entropy: 1.1934362649917603
        entropy_coeff: 0.0017600000137463212
        kl: 0.013617878779768944
        model: {}
        policy_loss: -0.03292698413133621
        total_loss: -0.03221341595053673
        vf_explained_var: 0.055517569184303284
        vf_loss: 14.522256851196289
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005651136161759496
        entropy: 1.1004356145858765
        entropy_coeff: 0.0017600000137463212
        kl: 0.014650130644440651
        model: {}
        policy_loss: -0.03300632908940315
        total_loss: -0.032048873603343964
        vf_explained_var: 0.07065914571285248
        vf_loss: 14.292098045349121
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005651136161759496
        entropy: 1.1639217138290405
        entropy_coeff: 0.0017600000137463212
        kl: 0.02245548740029335
        model: {}
        policy_loss: -0.028905265033245087
        total_loss: -0.027278803288936615
        vf_explained_var: 0.07028119266033173
        vf_loss: 14.294180870056152
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005651136161759496
        entropy: 0.6988783478736877
        entropy_coeff: 0.0017600000137463212
        kl: 0.011024398729205132
        model: {}
        policy_loss: -0.022706413641572
        total_loss: -0.021586783230304718
        vf_explained_var: 0.18806570768356323
        vf_loss: 12.472138404846191
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005651136161759496
        entropy: 1.0323909521102905
        entropy_coeff: 0.0017600000137463212
        kl: 0.015676358714699745
        model: {}
        policy_loss: -0.0314103402197361
        total_loss: -0.030246926471590996
        vf_explained_var: 0.08153413236141205
        vf_loss: 14.12785530090332
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005651136161759496
        entropy: 0.9891940951347351
        entropy_coeff: 0.0017600000137463212
        kl: 0.01727527379989624
        model: {}
        policy_loss: -0.03812440112233162
        total_loss: -0.036751776933670044
        vf_explained_var: 0.09899261593818665
        vf_loss: 13.86077880859375
    load_time_ms: 14069.224
    num_steps_sampled: 11232000
    num_steps_trained: 11232000
    sample_time_ms: 88308.717
    update_time_ms: 20.104
  iterations_since_restore: 97
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.537209302325579
    ram_util_percent: 9.651162790697674
  pid: 4061
  policy_reward_max:
    agent-0: 148.66666666666674
    agent-1: 148.66666666666674
    agent-2: 148.66666666666674
    agent-3: 148.66666666666674
    agent-4: 148.66666666666674
    agent-5: 148.66666666666674
  policy_reward_mean:
    agent-0: 104.13833333333363
    agent-1: 104.13833333333363
    agent-2: 104.13833333333363
    agent-3: 104.13833333333363
    agent-4: 104.13833333333363
    agent-5: 104.13833333333363
  policy_reward_min:
    agent-0: 51.333333333333144
    agent-1: 51.333333333333144
    agent-2: 51.333333333333144
    agent-3: 51.333333333333144
    agent-4: 51.333333333333144
    agent-5: 51.333333333333144
  sampler_perf:
    mean_env_wait_ms: 23.493665217382837
    mean_inference_ms: 12.30329095177429
    mean_processing_ms: 50.7985425451956
  time_since_restore: 12998.082553863525
  time_this_iter_s: 120.56073093414307
  time_total_s: 16209.146239995956
  timestamp: 1637030521
  timesteps_since_restore: 9312000
  timesteps_this_iter: 96000
  timesteps_total: 11232000
  training_iteration: 117
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    117 |          16209.1 | 11232000 |   624.83 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 52
    apples_agent-0_mean: 4.99
    apples_agent-0_min: 0
    apples_agent-1_max: 76
    apples_agent-1_mean: 25.23
    apples_agent-1_min: 0
    apples_agent-2_max: 120
    apples_agent-2_mean: 13.96
    apples_agent-2_min: 0
    apples_agent-3_max: 152
    apples_agent-3_mean: 70.73
    apples_agent-3_min: 33
    apples_agent-4_max: 46
    apples_agent-4_mean: 1.71
    apples_agent-4_min: 0
    apples_agent-5_max: 145
    apples_agent-5_mean: 77.02
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 456
    cleaning_beam_agent-0_mean: 284.46
    cleaning_beam_agent-0_min: 115
    cleaning_beam_agent-1_max: 413
    cleaning_beam_agent-1_mean: 223.84
    cleaning_beam_agent-1_min: 78
    cleaning_beam_agent-2_max: 424
    cleaning_beam_agent-2_mean: 241.23
    cleaning_beam_agent-2_min: 71
    cleaning_beam_agent-3_max: 96
    cleaning_beam_agent-3_mean: 39.52
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 603
    cleaning_beam_agent-4_mean: 346.48
    cleaning_beam_agent-4_min: 154
    cleaning_beam_agent-5_max: 679
    cleaning_beam_agent-5_mean: 115.58
    cleaning_beam_agent-5_min: 19
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-44-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 863.9999999999822
  episode_reward_mean: 619.1999999999986
  episode_reward_min: 240.9999999999985
  episodes_this_iter: 96
  episodes_total: 11328
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20124.368
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005591231747530401
        entropy: 1.1813883781433105
        entropy_coeff: 0.0017600000137463212
        kl: 0.013537692837417126
        model: {}
        policy_loss: -0.03185490146279335
        total_loss: -0.031159313395619392
        vf_explained_var: 0.06063348054885864
        vf_loss: 14.21064281463623
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005591231747530401
        entropy: 1.094039797782898
        entropy_coeff: 0.0017600000137463212
        kl: 0.014541922137141228
        model: {}
        policy_loss: -0.03397535905241966
        total_loss: -0.032948173582553864
        vf_explained_var: 0.009569048881530762
        vf_loss: 14.984970092773438
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.0005591231747530401
        entropy: 1.1452125310897827
        entropy_coeff: 0.0017600000137463212
        kl: 0.015275415033102036
        model: {}
        policy_loss: -0.03694041073322296
        total_loss: -0.03526042401790619
        vf_explained_var: 0.07106302678585052
        vf_loss: 14.042498588562012
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005591231747530401
        entropy: 0.6819591522216797
        entropy_coeff: 0.0017600000137463212
        kl: 0.010922489687800407
        model: {}
        policy_loss: -0.022652234882116318
        total_loss: -0.02156320959329605
        vf_explained_var: 0.20862363278865814
        vf_loss: 11.970247268676758
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005591231747530401
        entropy: 1.0363973379135132
        entropy_coeff: 0.0017600000137463212
        kl: 0.014830533415079117
        model: {}
        policy_loss: -0.03307133913040161
        total_loss: -0.03207625076174736
        vf_explained_var: 0.11702375113964081
        vf_loss: 13.360937118530273
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005591231747530401
        entropy: 1.015213131904602
        entropy_coeff: 0.0017600000137463212
        kl: 0.0194561667740345
        model: {}
        policy_loss: -0.03751274198293686
        total_loss: -0.03604048490524292
        vf_explained_var: 0.13151302933692932
        vf_loss: 13.134126663208008
    load_time_ms: 14102.527
    num_steps_sampled: 11328000
    num_steps_trained: 11328000
    sample_time_ms: 88301.61
    update_time_ms: 20.692
  iterations_since_restore: 98
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.23977272727273
    ram_util_percent: 9.66022727272727
  pid: 4061
  policy_reward_max:
    agent-0: 144.0000000000005
    agent-1: 144.0000000000005
    agent-2: 144.0000000000005
    agent-3: 144.0000000000005
    agent-4: 144.0000000000005
    agent-5: 144.0000000000005
  policy_reward_mean:
    agent-0: 103.20000000000032
    agent-1: 103.20000000000032
    agent-2: 103.20000000000032
    agent-3: 103.20000000000032
    agent-4: 103.20000000000032
    agent-5: 103.20000000000032
  policy_reward_min:
    agent-0: 40.16666666666662
    agent-1: 40.16666666666662
    agent-2: 40.16666666666662
    agent-3: 40.16666666666662
    agent-4: 40.16666666666662
    agent-5: 40.16666666666662
  sampler_perf:
    mean_env_wait_ms: 23.493126158567364
    mean_inference_ms: 12.301338227442802
    mean_processing_ms: 50.79291293895037
  time_since_restore: 13121.116074562073
  time_this_iter_s: 123.03352069854736
  time_total_s: 16332.179760694504
  timestamp: 1637030644
  timesteps_since_restore: 9408000
  timesteps_this_iter: 96000
  timesteps_total: 11328000
  training_iteration: 118
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    118 |          16332.2 | 11328000 |    619.2 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 49
    apples_agent-0_mean: 6.24
    apples_agent-0_min: 0
    apples_agent-1_max: 121
    apples_agent-1_mean: 23.9
    apples_agent-1_min: 0
    apples_agent-2_max: 109
    apples_agent-2_mean: 10.56
    apples_agent-2_min: 0
    apples_agent-3_max: 143
    apples_agent-3_mean: 70.31
    apples_agent-3_min: 33
    apples_agent-4_max: 48
    apples_agent-4_mean: 2.59
    apples_agent-4_min: 0
    apples_agent-5_max: 143
    apples_agent-5_mean: 74.29
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 532
    cleaning_beam_agent-0_mean: 289.32
    cleaning_beam_agent-0_min: 115
    cleaning_beam_agent-1_max: 454
    cleaning_beam_agent-1_mean: 237.96
    cleaning_beam_agent-1_min: 96
    cleaning_beam_agent-2_max: 414
    cleaning_beam_agent-2_mean: 248.85
    cleaning_beam_agent-2_min: 98
    cleaning_beam_agent-3_max: 242
    cleaning_beam_agent-3_mean: 45.59
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 605
    cleaning_beam_agent-4_mean: 362.67
    cleaning_beam_agent-4_min: 146
    cleaning_beam_agent-5_max: 703
    cleaning_beam_agent-5_mean: 139.84
    cleaning_beam_agent-5_min: 30
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-46-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 800.9999999999798
  episode_reward_mean: 630.8799999999968
  episode_reward_min: 232.99999999999676
  episodes_this_iter: 96
  episodes_total: 11424
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20113.658
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005531327915377915
        entropy: 1.1792380809783936
        entropy_coeff: 0.0017600000137463212
        kl: 0.01350498665124178
        model: {}
        policy_loss: -0.032277222722768784
        total_loss: -0.031576309353113174
        vf_explained_var: 0.06796902418136597
        vf_loss: 14.25877857208252
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005531327915377915
        entropy: 1.1336091756820679
        entropy_coeff: 0.0017600000137463212
        kl: 0.014836392365396023
        model: {}
        policy_loss: -0.03305398300290108
        total_loss: -0.03217324987053871
        vf_explained_var: 0.0898972898721695
        vf_loss: 13.92247486114502
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.0005531327915377915
        entropy: 1.1257297992706299
        entropy_coeff: 0.0017600000137463212
        kl: 0.016456427052617073
        model: {}
        policy_loss: -0.03234991803765297
        total_loss: -0.03041275590658188
        vf_explained_var: 0.05201295018196106
        vf_loss: 14.499820709228516
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005531327915377915
        entropy: 0.6657095551490784
        entropy_coeff: 0.0017600000137463212
        kl: 0.009842438623309135
        model: {}
        policy_loss: -0.02260958030819893
        total_loss: -0.02158486098051071
        vf_explained_var: 0.20741866528987885
        vf_loss: 12.121223449707031
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005531327915377915
        entropy: 1.0372474193572998
        entropy_coeff: 0.0017600000137463212
        kl: 0.014695366844534874
        model: {}
        policy_loss: -0.033704087138175964
        total_loss: -0.03272053971886635
        vf_explained_var: 0.1241031289100647
        vf_loss: 13.395586013793945
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005531327915377915
        entropy: 1.003105640411377
        entropy_coeff: 0.0017600000137463212
        kl: 0.017549265176057816
        model: {}
        policy_loss: -0.03892437741160393
        total_loss: -0.03752483054995537
        vf_explained_var: 0.0775124728679657
        vf_loss: 14.100866317749023
    load_time_ms: 14016.691
    num_steps_sampled: 11424000
    num_steps_trained: 11424000
    sample_time_ms: 88318.288
    update_time_ms: 20.517
  iterations_since_restore: 99
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.531791907514451
    ram_util_percent: 9.588439306358383
  pid: 4061
  policy_reward_max:
    agent-0: 133.50000000000026
    agent-1: 133.50000000000026
    agent-2: 133.50000000000026
    agent-3: 133.50000000000026
    agent-4: 133.50000000000026
    agent-5: 133.50000000000026
  policy_reward_mean:
    agent-0: 105.146666666667
    agent-1: 105.146666666667
    agent-2: 105.146666666667
    agent-3: 105.146666666667
    agent-4: 105.146666666667
    agent-5: 105.146666666667
  policy_reward_min:
    agent-0: 38.833333333333286
    agent-1: 38.833333333333286
    agent-2: 38.833333333333286
    agent-3: 38.833333333333286
    agent-4: 38.833333333333286
    agent-5: 38.833333333333286
  sampler_perf:
    mean_env_wait_ms: 23.495519391156623
    mean_inference_ms: 12.299735649720203
    mean_processing_ms: 50.78581531120518
  time_since_restore: 13243.052408218384
  time_this_iter_s: 121.93633365631104
  time_total_s: 16454.116094350815
  timestamp: 1637030766
  timesteps_since_restore: 9504000
  timesteps_this_iter: 96000
  timesteps_total: 11424000
  training_iteration: 119
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    119 |          16454.1 | 11424000 |   630.88 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 47
    apples_agent-0_mean: 5.42
    apples_agent-0_min: 0
    apples_agent-1_max: 68
    apples_agent-1_mean: 19.79
    apples_agent-1_min: 0
    apples_agent-2_max: 197
    apples_agent-2_mean: 15.69
    apples_agent-2_min: 0
    apples_agent-3_max: 126
    apples_agent-3_mean: 69.42
    apples_agent-3_min: 20
    apples_agent-4_max: 91
    apples_agent-4_mean: 5.21
    apples_agent-4_min: 0
    apples_agent-5_max: 198
    apples_agent-5_mean: 76.56
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 451
    cleaning_beam_agent-0_mean: 273.66
    cleaning_beam_agent-0_min: 117
    cleaning_beam_agent-1_max: 422
    cleaning_beam_agent-1_mean: 241.96
    cleaning_beam_agent-1_min: 103
    cleaning_beam_agent-2_max: 412
    cleaning_beam_agent-2_mean: 236.05
    cleaning_beam_agent-2_min: 67
    cleaning_beam_agent-3_max: 128
    cleaning_beam_agent-3_mean: 43.27
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 534
    cleaning_beam_agent-4_mean: 342.69
    cleaning_beam_agent-4_min: 177
    cleaning_beam_agent-5_max: 691
    cleaning_beam_agent-5_mean: 127.52
    cleaning_beam_agent-5_min: 30
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.07
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-48-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 828.9999999999833
  episode_reward_mean: 597.4399999999987
  episode_reward_min: 125.00000000000125
  episodes_this_iter: 96
  episodes_total: 11520
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20111.019
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005471424083225429
        entropy: 1.1934148073196411
        entropy_coeff: 0.0017600000137463212
        kl: 0.013622095808386803
        model: {}
        policy_loss: -0.033458221703767776
        total_loss: -0.03264790400862694
        vf_explained_var: 0.05508255958557129
        vf_loss: 15.4851713180542
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005471424083225429
        entropy: 1.1613595485687256
        entropy_coeff: 0.0017600000137463212
        kl: 0.016126303002238274
        model: {}
        policy_loss: -0.034248240292072296
        total_loss: -0.03321726247668266
        vf_explained_var: 0.1080612987279892
        vf_loss: 14.623392105102539
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.0005471424083225429
        entropy: 1.113844871520996
        entropy_coeff: 0.0017600000137463212
        kl: 0.014669091440737247
        model: {}
        policy_loss: -0.035263147205114365
        total_loss: -0.0334274098277092
        vf_explained_var: 0.028042614459991455
        vf_loss: 15.957395553588867
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005471424083225429
        entropy: 0.6984806060791016
        entropy_coeff: 0.0017600000137463212
        kl: 0.01077138539403677
        model: {}
        policy_loss: -0.023469990119338036
        total_loss: -0.022388624027371407
        vf_explained_var: 0.24710950255393982
        vf_loss: 12.335565567016602
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005471424083225429
        entropy: 1.0192298889160156
        entropy_coeff: 0.0017600000137463212
        kl: 0.014977866783738136
        model: {}
        policy_loss: -0.03232986107468605
        total_loss: -0.03125649690628052
        vf_explained_var: 0.1638210266828537
        vf_loss: 13.694221496582031
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005471424083225429
        entropy: 1.0316016674041748
        entropy_coeff: 0.0017600000137463212
        kl: 0.017796063795685768
        model: {}
        policy_loss: -0.04091663658618927
        total_loss: -0.039553556591272354
        vf_explained_var: 0.14767883718013763
        vf_loss: 13.99094009399414
    load_time_ms: 13919.57
    num_steps_sampled: 11520000
    num_steps_trained: 11520000
    sample_time_ms: 88256.027
    update_time_ms: 20.971
  iterations_since_restore: 100
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.49080459770115
    ram_util_percent: 9.658620689655175
  pid: 4061
  policy_reward_max:
    agent-0: 138.1666666666669
    agent-1: 138.1666666666669
    agent-2: 138.1666666666669
    agent-3: 138.1666666666669
    agent-4: 138.1666666666669
    agent-5: 138.1666666666669
  policy_reward_mean:
    agent-0: 99.57333333333365
    agent-1: 99.57333333333365
    agent-2: 99.57333333333365
    agent-3: 99.57333333333365
    agent-4: 99.57333333333365
    agent-5: 99.57333333333365
  policy_reward_min:
    agent-0: 20.833333333333346
    agent-1: 20.833333333333346
    agent-2: 20.833333333333346
    agent-3: 20.833333333333346
    agent-4: 20.833333333333346
    agent-5: 20.833333333333346
  sampler_perf:
    mean_env_wait_ms: 23.495015902738555
    mean_inference_ms: 12.29849945990701
    mean_processing_ms: 50.779633993743126
  time_since_restore: 13365.077939033508
  time_this_iter_s: 122.02553081512451
  time_total_s: 16576.14162516594
  timestamp: 1637030888
  timesteps_since_restore: 9600000
  timesteps_this_iter: 96000
  timesteps_total: 11520000
  training_iteration: 120
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    120 |          16576.1 | 11520000 |   597.44 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 66
    apples_agent-0_mean: 5.66
    apples_agent-0_min: 0
    apples_agent-1_max: 87
    apples_agent-1_mean: 21.32
    apples_agent-1_min: 0
    apples_agent-2_max: 59
    apples_agent-2_mean: 9.69
    apples_agent-2_min: 0
    apples_agent-3_max: 116
    apples_agent-3_mean: 68.21
    apples_agent-3_min: 27
    apples_agent-4_max: 70
    apples_agent-4_mean: 3.8
    apples_agent-4_min: 0
    apples_agent-5_max: 125
    apples_agent-5_mean: 69.77
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 428
    cleaning_beam_agent-0_mean: 275.94
    cleaning_beam_agent-0_min: 126
    cleaning_beam_agent-1_max: 367
    cleaning_beam_agent-1_mean: 209.76
    cleaning_beam_agent-1_min: 103
    cleaning_beam_agent-2_max: 381
    cleaning_beam_agent-2_mean: 228.34
    cleaning_beam_agent-2_min: 24
    cleaning_beam_agent-3_max: 140
    cleaning_beam_agent-3_mean: 43.16
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 513
    cleaning_beam_agent-4_mean: 329.35
    cleaning_beam_agent-4_min: 124
    cleaning_beam_agent-5_max: 609
    cleaning_beam_agent-5_mean: 143.61
    cleaning_beam_agent-5_min: 37
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-50-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 794.9999999999794
  episode_reward_mean: 591.98
  episode_reward_min: 305.00000000000273
  episodes_this_iter: 96
  episodes_total: 11616
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20102.685
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005411520251072943
        entropy: 1.18660569190979
        entropy_coeff: 0.0017600000137463212
        kl: 0.013941483572125435
        model: {}
        policy_loss: -0.03299090266227722
        total_loss: -0.03226912394165993
        vf_explained_var: 0.07755199074745178
        vf_loss: 14.160564422607422
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005411520251072943
        entropy: 1.129878282546997
        entropy_coeff: 0.0017600000137463212
        kl: 0.014847517013549805
        model: {}
        policy_loss: -0.032277341932058334
        total_loss: -0.03136326000094414
        vf_explained_var: 0.07680273056030273
        vf_loss: 14.179167747497559
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.0005411520251072943
        entropy: 1.1351966857910156
        entropy_coeff: 0.0017600000137463212
        kl: 0.015028046444058418
        model: {}
        policy_loss: -0.03597581386566162
        total_loss: -0.03427380695939064
        vf_explained_var: 0.0593414306640625
        vf_loss: 14.457477569580078
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005411520251072943
        entropy: 0.6793513298034668
        entropy_coeff: 0.0017600000137463212
        kl: 0.009761585853993893
        model: {}
        policy_loss: -0.021701104938983917
        total_loss: -0.020726347342133522
        vf_explained_var: 0.2222198247909546
        vf_loss: 11.942567825317383
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005411520251072943
        entropy: 1.0218050479888916
        entropy_coeff: 0.0017600000137463212
        kl: 0.014457286335527897
        model: {}
        policy_loss: -0.03352594003081322
        total_loss: -0.03254462778568268
        vf_explained_var: 0.13133032619953156
        vf_loss: 13.339614868164062
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005411520251072943
        entropy: 1.0222241878509521
        entropy_coeff: 0.0017600000137463212
        kl: 0.017834611237049103
        model: {}
        policy_loss: -0.040720369666814804
        total_loss: -0.03939388319849968
        vf_explained_var: 0.1265888810157776
        vf_loss: 13.421422958374023
    load_time_ms: 13933.384
    num_steps_sampled: 11616000
    num_steps_trained: 11616000
    sample_time_ms: 88290.363
    update_time_ms: 21.018
  iterations_since_restore: 101
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.770454545454548
    ram_util_percent: 9.599431818181818
  pid: 4061
  policy_reward_max:
    agent-0: 132.50000000000074
    agent-1: 132.50000000000074
    agent-2: 132.50000000000074
    agent-3: 132.50000000000074
    agent-4: 132.50000000000074
    agent-5: 132.50000000000074
  policy_reward_mean:
    agent-0: 98.6633333333336
    agent-1: 98.6633333333336
    agent-2: 98.6633333333336
    agent-3: 98.6633333333336
    agent-4: 98.6633333333336
    agent-5: 98.6633333333336
  policy_reward_min:
    agent-0: 50.83333333333319
    agent-1: 50.83333333333319
    agent-2: 50.83333333333319
    agent-3: 50.83333333333319
    agent-4: 50.83333333333319
    agent-5: 50.83333333333319
  sampler_perf:
    mean_env_wait_ms: 23.49529592591622
    mean_inference_ms: 12.297301414265235
    mean_processing_ms: 50.77678547146948
  time_since_restore: 13488.111396074295
  time_this_iter_s: 123.03345704078674
  time_total_s: 16699.175082206726
  timestamp: 1637031012
  timesteps_since_restore: 9696000
  timesteps_this_iter: 96000
  timesteps_total: 11616000
  training_iteration: 121
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    121 |          16699.2 | 11616000 |   591.98 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 44
    apples_agent-0_mean: 6.45
    apples_agent-0_min: 0
    apples_agent-1_max: 86
    apples_agent-1_mean: 17.6
    apples_agent-1_min: 0
    apples_agent-2_max: 85
    apples_agent-2_mean: 10.27
    apples_agent-2_min: 0
    apples_agent-3_max: 186
    apples_agent-3_mean: 71.46
    apples_agent-3_min: 32
    apples_agent-4_max: 74
    apples_agent-4_mean: 1.75
    apples_agent-4_min: 0
    apples_agent-5_max: 205
    apples_agent-5_mean: 77.32
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 406
    cleaning_beam_agent-0_mean: 272.73
    cleaning_beam_agent-0_min: 131
    cleaning_beam_agent-1_max: 364
    cleaning_beam_agent-1_mean: 224.77
    cleaning_beam_agent-1_min: 82
    cleaning_beam_agent-2_max: 413
    cleaning_beam_agent-2_mean: 240.14
    cleaning_beam_agent-2_min: 62
    cleaning_beam_agent-3_max: 134
    cleaning_beam_agent-3_mean: 40.78
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 549
    cleaning_beam_agent-4_mean: 321.92
    cleaning_beam_agent-4_min: 178
    cleaning_beam_agent-5_max: 779
    cleaning_beam_agent-5_mean: 119.57
    cleaning_beam_agent-5_min: 25
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-52-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 872.99999999998
  episode_reward_mean: 619.8399999999968
  episode_reward_min: 283.9999999999971
  episodes_this_iter: 96
  episodes_total: 11712
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20124.054
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005351615836843848
        entropy: 1.1685194969177246
        entropy_coeff: 0.0017600000137463212
        kl: 0.013941171579062939
        model: {}
        policy_loss: -0.03033294342458248
        total_loss: -0.029643027111887932
        vf_explained_var: 0.09094734489917755
        vf_loss: 13.523904800415039
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005351615836843848
        entropy: 1.1219720840454102
        entropy_coeff: 0.0017600000137463212
        kl: 0.015052704140543938
        model: {}
        policy_loss: -0.03175496309995651
        total_loss: -0.030775755643844604
        vf_explained_var: 0.026301458477973938
        vf_loss: 14.486095428466797
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.0005351615836843848
        entropy: 1.1253384351730347
        entropy_coeff: 0.0017600000137463212
        kl: 0.015164786949753761
        model: {}
        policy_loss: -0.034920986741781235
        total_loss: -0.03326445817947388
        vf_explained_var: 0.08400432765483856
        vf_loss: 13.624072074890137
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005351615836843848
        entropy: 0.6496970057487488
        entropy_coeff: 0.0017600000137463212
        kl: 0.010007034987211227
        model: {}
        policy_loss: -0.021728597581386566
        total_loss: -0.02068057283759117
        vf_explained_var: 0.19950412213802338
        vf_loss: 11.907842636108398
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005351615836843848
        entropy: 1.0264008045196533
        entropy_coeff: 0.0017600000137463212
        kl: 0.014608388766646385
        model: {}
        policy_loss: -0.033991631120443344
        total_loss: -0.032994695007801056
        vf_explained_var: 0.09711749851703644
        vf_loss: 13.425649642944336
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005351615836843848
        entropy: 1.0115586519241333
        entropy_coeff: 0.0017600000137463212
        kl: 0.016980472952127457
        model: {}
        policy_loss: -0.03896854817867279
        total_loss: -0.03767130523920059
        vf_explained_var: 0.07333861291408539
        vf_loss: 13.795387268066406
    load_time_ms: 13932.043
    num_steps_sampled: 11712000
    num_steps_trained: 11712000
    sample_time_ms: 88179.484
    update_time_ms: 20.784
  iterations_since_restore: 102
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.478285714285713
    ram_util_percent: 9.669714285714287
  pid: 4061
  policy_reward_max:
    agent-0: 145.50000000000017
    agent-1: 145.50000000000017
    agent-2: 145.50000000000017
    agent-3: 145.50000000000017
    agent-4: 145.50000000000017
    agent-5: 145.50000000000017
  policy_reward_mean:
    agent-0: 103.30666666666701
    agent-1: 103.30666666666701
    agent-2: 103.30666666666701
    agent-3: 103.30666666666701
    agent-4: 103.30666666666701
    agent-5: 103.30666666666701
  policy_reward_min:
    agent-0: 47.33333333333322
    agent-1: 47.33333333333322
    agent-2: 47.33333333333322
    agent-3: 47.33333333333322
    agent-4: 47.33333333333322
    agent-5: 47.33333333333322
  sampler_perf:
    mean_env_wait_ms: 23.4944037369149
    mean_inference_ms: 12.29573516157554
    mean_processing_ms: 50.76936520284873
  time_since_restore: 13610.619542837143
  time_this_iter_s: 122.5081467628479
  time_total_s: 16821.683228969574
  timestamp: 1637031134
  timesteps_since_restore: 9792000
  timesteps_this_iter: 96000
  timesteps_total: 11712000
  training_iteration: 122
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    122 |          16821.7 | 11712000 |   619.84 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 73
    apples_agent-0_mean: 8.36
    apples_agent-0_min: 0
    apples_agent-1_max: 120
    apples_agent-1_mean: 23.41
    apples_agent-1_min: 0
    apples_agent-2_max: 90
    apples_agent-2_mean: 11.41
    apples_agent-2_min: 0
    apples_agent-3_max: 131
    apples_agent-3_mean: 68.68
    apples_agent-3_min: 22
    apples_agent-4_max: 73
    apples_agent-4_mean: 5.38
    apples_agent-4_min: 0
    apples_agent-5_max: 142
    apples_agent-5_mean: 63.81
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 426
    cleaning_beam_agent-0_mean: 255.19
    cleaning_beam_agent-0_min: 97
    cleaning_beam_agent-1_max: 380
    cleaning_beam_agent-1_mean: 215.8
    cleaning_beam_agent-1_min: 82
    cleaning_beam_agent-2_max: 403
    cleaning_beam_agent-2_mean: 229.07
    cleaning_beam_agent-2_min: 97
    cleaning_beam_agent-3_max: 241
    cleaning_beam_agent-3_mean: 44.82
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 494
    cleaning_beam_agent-4_mean: 324.46
    cleaning_beam_agent-4_min: 199
    cleaning_beam_agent-5_max: 673
    cleaning_beam_agent-5_mean: 176.84
    cleaning_beam_agent-5_min: 24
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-54-17
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 800.9999999999942
  episode_reward_mean: 572.3900000000019
  episode_reward_min: 246.99999999999596
  episodes_this_iter: 96
  episodes_total: 11808
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20148.43
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005291712004691362
        entropy: 1.1890714168548584
        entropy_coeff: 0.0017600000137463212
        kl: 0.01332861091941595
        model: {}
        policy_loss: -0.032425858080387115
        total_loss: -0.031894002109766006
        vf_explained_var: 0.07725544273853302
        vf_loss: 12.91760540008545
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005291712004691362
        entropy: 1.1162539720535278
        entropy_coeff: 0.0017600000137463212
        kl: 0.015316396951675415
        model: {}
        policy_loss: -0.03264661505818367
        total_loss: -0.03174159675836563
        vf_explained_var: 0.04452677071094513
        vf_loss: 13.37982177734375
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.0005291712004691362
        entropy: 1.117411732673645
        entropy_coeff: 0.0017600000137463212
        kl: 0.015344273298978806
        model: {}
        policy_loss: -0.03345877677202225
        total_loss: -0.03179347515106201
        vf_explained_var: 0.05112771689891815
        vf_loss: 13.303077697753906
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005291712004691362
        entropy: 0.6752302050590515
        entropy_coeff: 0.0017600000137463212
        kl: 0.010531537234783173
        model: {}
        policy_loss: -0.023217597976326942
        total_loss: -0.022256044670939445
        vf_explained_var: 0.21601922810077667
        vf_loss: 10.968034744262695
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005291712004691362
        entropy: 1.0373483896255493
        entropy_coeff: 0.0017600000137463212
        kl: 0.014430712908506393
        model: {}
        policy_loss: -0.03346506878733635
        total_loss: -0.03262431547045708
        vf_explained_var: 0.12650905549526215
        vf_loss: 12.23412799835205
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005291712004691362
        entropy: 1.0507841110229492
        entropy_coeff: 0.0017600000137463212
        kl: 0.01766812428832054
        model: {}
        policy_loss: -0.03812886029481888
        total_loss: -0.03691515326499939
        vf_explained_var: 0.07437126338481903
        vf_loss: 12.962779998779297
    load_time_ms: 13839.301
    num_steps_sampled: 11808000
    num_steps_trained: 11808000
    sample_time_ms: 88173.767
    update_time_ms: 21.34
  iterations_since_restore: 103
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.399428571428569
    ram_util_percent: 9.584571428571431
  pid: 4061
  policy_reward_max:
    agent-0: 133.50000000000026
    agent-1: 133.50000000000026
    agent-2: 133.50000000000026
    agent-3: 133.50000000000026
    agent-4: 133.50000000000026
    agent-5: 133.50000000000026
  policy_reward_mean:
    agent-0: 95.3983333333336
    agent-1: 95.3983333333336
    agent-2: 95.3983333333336
    agent-3: 95.3983333333336
    agent-4: 95.3983333333336
    agent-5: 95.3983333333336
  policy_reward_min:
    agent-0: 41.16666666666663
    agent-1: 41.16666666666663
    agent-2: 41.16666666666663
    agent-3: 41.16666666666663
    agent-4: 41.16666666666663
    agent-5: 41.16666666666663
  sampler_perf:
    mean_env_wait_ms: 23.493506963236605
    mean_inference_ms: 12.293913238465866
    mean_processing_ms: 50.76344733438373
  time_since_restore: 13732.461005210876
  time_this_iter_s: 121.84146237373352
  time_total_s: 16943.524691343307
  timestamp: 1637031257
  timesteps_since_restore: 9888000
  timesteps_this_iter: 96000
  timesteps_total: 11808000
  training_iteration: 123
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    123 |          16943.5 | 11808000 |   572.39 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 58
    apples_agent-0_mean: 7.71
    apples_agent-0_min: 0
    apples_agent-1_max: 109
    apples_agent-1_mean: 17.96
    apples_agent-1_min: 0
    apples_agent-2_max: 163
    apples_agent-2_mean: 10.61
    apples_agent-2_min: 0
    apples_agent-3_max: 128
    apples_agent-3_mean: 71.44
    apples_agent-3_min: 15
    apples_agent-4_max: 109
    apples_agent-4_mean: 6.19
    apples_agent-4_min: 0
    apples_agent-5_max: 120
    apples_agent-5_mean: 65.63
    apples_agent-5_min: 4
    cleaning_beam_agent-0_max: 418
    cleaning_beam_agent-0_mean: 251.46
    cleaning_beam_agent-0_min: 93
    cleaning_beam_agent-1_max: 383
    cleaning_beam_agent-1_mean: 215.56
    cleaning_beam_agent-1_min: 98
    cleaning_beam_agent-2_max: 431
    cleaning_beam_agent-2_mean: 231.94
    cleaning_beam_agent-2_min: 66
    cleaning_beam_agent-3_max: 207
    cleaning_beam_agent-3_mean: 49.39
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 536
    cleaning_beam_agent-4_mean: 311.43
    cleaning_beam_agent-4_min: 128
    cleaning_beam_agent-5_max: 605
    cleaning_beam_agent-5_mean: 158.15
    cleaning_beam_agent-5_min: 33
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-56-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 804.9999999999868
  episode_reward_mean: 566.2500000000014
  episode_reward_min: 159.00000000000028
  episodes_this_iter: 96
  episodes_total: 11904
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20122.144
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005231808172538877
        entropy: 1.1864140033721924
        entropy_coeff: 0.0017600000137463212
        kl: 0.013657053001224995
        model: {}
        policy_loss: -0.032141201198101044
        total_loss: -0.03145179525017738
        vf_explained_var: 0.09357774257659912
        vf_loss: 14.117897033691406
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005231808172538877
        entropy: 1.1094692945480347
        entropy_coeff: 0.0017600000137463212
        kl: 0.015426598489284515
        model: {}
        policy_loss: -0.031110573559999466
        total_loss: -0.03008616901934147
        vf_explained_var: 0.07924458384513855
        vf_loss: 14.344096183776855
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.0005231808172538877
        entropy: 1.1154136657714844
        entropy_coeff: 0.0017600000137463212
        kl: 0.013757788576185703
        model: {}
        policy_loss: -0.03299015387892723
        total_loss: -0.03146106377243996
        vf_explained_var: 0.08261293172836304
        vf_loss: 14.285504341125488
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005231808172538877
        entropy: 0.7035917043685913
        entropy_coeff: 0.0017600000137463212
        kl: 0.011190608143806458
        model: {}
        policy_loss: -0.024013817310333252
        total_loss: -0.02295542135834694
        vf_explained_var: 0.2441304326057434
        vf_loss: 11.776546478271484
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005231808172538877
        entropy: 1.0458416938781738
        entropy_coeff: 0.0017600000137463212
        kl: 0.015751473605632782
        model: {}
        policy_loss: -0.03464721143245697
        total_loss: -0.033531561493873596
        vf_explained_var: 0.11377443373203278
        vf_loss: 13.811834335327148
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005231808172538877
        entropy: 1.0399987697601318
        entropy_coeff: 0.0017600000137463212
        kl: 0.018071213737130165
        model: {}
        policy_loss: -0.03988068550825119
        total_loss: -0.03848211467266083
        vf_explained_var: 0.08740314841270447
        vf_loss: 14.218464851379395
    load_time_ms: 13727.572
    num_steps_sampled: 11904000
    num_steps_trained: 11904000
    sample_time_ms: 88142.488
    update_time_ms: 21.47
  iterations_since_restore: 104
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.591907514450867
    ram_util_percent: 9.649132947976879
  pid: 4061
  policy_reward_max:
    agent-0: 134.16666666666706
    agent-1: 134.16666666666706
    agent-2: 134.16666666666706
    agent-3: 134.16666666666706
    agent-4: 134.16666666666706
    agent-5: 134.16666666666706
  policy_reward_mean:
    agent-0: 94.37500000000024
    agent-1: 94.37500000000024
    agent-2: 94.37500000000024
    agent-3: 94.37500000000024
    agent-4: 94.37500000000024
    agent-5: 94.37500000000024
  policy_reward_min:
    agent-0: 26.50000000000003
    agent-1: 26.50000000000003
    agent-2: 26.50000000000003
    agent-3: 26.50000000000003
    agent-4: 26.50000000000003
    agent-5: 26.50000000000003
  sampler_perf:
    mean_env_wait_ms: 23.49198998786843
    mean_inference_ms: 12.292864478089141
    mean_processing_ms: 50.75913615454295
  time_since_restore: 13854.174454689026
  time_this_iter_s: 121.71344947814941
  time_total_s: 17065.238140821457
  timestamp: 1637031379
  timesteps_since_restore: 9984000
  timesteps_this_iter: 96000
  timesteps_total: 11904000
  training_iteration: 124
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    124 |          17065.2 | 11904000 |   566.25 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 96
    apples_agent-0_mean: 6.96
    apples_agent-0_min: 0
    apples_agent-1_max: 98
    apples_agent-1_mean: 22.54
    apples_agent-1_min: 0
    apples_agent-2_max: 163
    apples_agent-2_mean: 15.15
    apples_agent-2_min: 0
    apples_agent-3_max: 127
    apples_agent-3_mean: 73.42
    apples_agent-3_min: 27
    apples_agent-4_max: 50
    apples_agent-4_mean: 2.69
    apples_agent-4_min: 0
    apples_agent-5_max: 214
    apples_agent-5_mean: 72.89
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 410
    cleaning_beam_agent-0_mean: 266.3
    cleaning_beam_agent-0_min: 88
    cleaning_beam_agent-1_max: 313
    cleaning_beam_agent-1_mean: 197.43
    cleaning_beam_agent-1_min: 98
    cleaning_beam_agent-2_max: 416
    cleaning_beam_agent-2_mean: 227.2
    cleaning_beam_agent-2_min: 82
    cleaning_beam_agent-3_max: 157
    cleaning_beam_agent-3_mean: 45.02
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 514
    cleaning_beam_agent-4_mean: 318.91
    cleaning_beam_agent-4_min: 201
    cleaning_beam_agent-5_max: 605
    cleaning_beam_agent-5_mean: 148.23
    cleaning_beam_agent-5_min: 20
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-58-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 782.9999999999782
  episode_reward_mean: 584.1700000000001
  episode_reward_min: 365.00000000000347
  episodes_this_iter: 96
  episodes_total: 12000
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20092.498
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005171903758309782
        entropy: 1.1809922456741333
        entropy_coeff: 0.0017600000137463212
        kl: 0.013815885409712791
        model: {}
        policy_loss: -0.03141136094927788
        total_loss: -0.03069302812218666
        vf_explained_var: 0.07094480097293854
        vf_loss: 14.152907371520996
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005171903758309782
        entropy: 1.1105177402496338
        entropy_coeff: 0.0017600000137463212
        kl: 0.016770431771874428
        model: {}
        policy_loss: -0.03100329078733921
        total_loss: -0.029815807938575745
        vf_explained_var: 0.038138970732688904
        vf_loss: 14.649474143981934
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.0005171903758309782
        entropy: 1.1027438640594482
        entropy_coeff: 0.0017600000137463212
        kl: 0.013922035694122314
        model: {}
        policy_loss: -0.033938225358724594
        total_loss: -0.03237101435661316
        vf_explained_var: 0.06801173090934753
        vf_loss: 14.197351455688477
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005171903758309782
        entropy: 0.6835030317306519
        entropy_coeff: 0.0017600000137463212
        kl: 0.010881628841161728
        model: {}
        policy_loss: -0.0222894549369812
        total_loss: -0.0212533101439476
        vf_explained_var: 0.24453982710838318
        vf_loss: 11.509485244750977
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005171903758309782
        entropy: 1.0368833541870117
        entropy_coeff: 0.0017600000137463212
        kl: 0.015710851177573204
        model: {}
        policy_loss: -0.03461623936891556
        total_loss: -0.033505380153656006
        vf_explained_var: 0.1041431576013565
        vf_loss: 13.646944046020508
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005171903758309782
        entropy: 1.0301282405853271
        entropy_coeff: 0.0017600000137463212
        kl: 0.017082106322050095
        model: {}
        policy_loss: -0.041011955589056015
        total_loss: -0.03972847759723663
        vf_explained_var: 0.08840496838092804
        vf_loss: 13.882856369018555
    load_time_ms: 13647.322
    num_steps_sampled: 12000000
    num_steps_trained: 12000000
    sample_time_ms: 88217.218
    update_time_ms: 21.976
  iterations_since_restore: 105
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.482658959537572
    ram_util_percent: 9.585549132947978
  pid: 4061
  policy_reward_max:
    agent-0: 130.50000000000063
    agent-1: 130.50000000000063
    agent-2: 130.50000000000063
    agent-3: 130.50000000000063
    agent-4: 130.50000000000063
    agent-5: 130.50000000000063
  policy_reward_mean:
    agent-0: 97.36166666666695
    agent-1: 97.36166666666695
    agent-2: 97.36166666666695
    agent-3: 97.36166666666695
    agent-4: 97.36166666666695
    agent-5: 97.36166666666695
  policy_reward_min:
    agent-0: 60.833333333333094
    agent-1: 60.833333333333094
    agent-2: 60.833333333333094
    agent-3: 60.833333333333094
    agent-4: 60.833333333333094
    agent-5: 60.833333333333094
  sampler_perf:
    mean_env_wait_ms: 23.489971314899307
    mean_inference_ms: 12.29238212657986
    mean_processing_ms: 50.756215236318646
  time_since_restore: 13975.611992120743
  time_this_iter_s: 121.43753743171692
  time_total_s: 17186.675678253174
  timestamp: 1637031500
  timesteps_since_restore: 10080000
  timesteps_this_iter: 96000
  timesteps_total: 12000000
  training_iteration: 125
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    125 |          17186.7 | 12000000 |   584.17 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 54
    apples_agent-0_mean: 7.73
    apples_agent-0_min: 0
    apples_agent-1_max: 101
    apples_agent-1_mean: 21.93
    apples_agent-1_min: 0
    apples_agent-2_max: 80
    apples_agent-2_mean: 8.02
    apples_agent-2_min: 0
    apples_agent-3_max: 145
    apples_agent-3_mean: 71.0
    apples_agent-3_min: 30
    apples_agent-4_max: 81
    apples_agent-4_mean: 3.38
    apples_agent-4_min: 0
    apples_agent-5_max: 163
    apples_agent-5_mean: 71.01
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 503
    cleaning_beam_agent-0_mean: 270.77
    cleaning_beam_agent-0_min: 88
    cleaning_beam_agent-1_max: 333
    cleaning_beam_agent-1_mean: 199.01
    cleaning_beam_agent-1_min: 90
    cleaning_beam_agent-2_max: 400
    cleaning_beam_agent-2_mean: 263.18
    cleaning_beam_agent-2_min: 107
    cleaning_beam_agent-3_max: 126
    cleaning_beam_agent-3_mean: 40.66
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 580
    cleaning_beam_agent-4_mean: 339.05
    cleaning_beam_agent-4_min: 162
    cleaning_beam_agent-5_max: 547
    cleaning_beam_agent-5_mean: 143.34
    cleaning_beam_agent-5_min: 36
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 6
    fire_beam_agent-4_mean: 0.08
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-00-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 833.9999999999832
  episode_reward_mean: 603.5899999999998
  episode_reward_min: 271.9999999999977
  episodes_this_iter: 96
  episodes_total: 12096
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20091.152
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005111999926157296
        entropy: 1.1718648672103882
        entropy_coeff: 0.0017600000137463212
        kl: 0.013609405606985092
        model: {}
        policy_loss: -0.03256385028362274
        total_loss: -0.03191221505403519
        vf_explained_var: 0.09561848640441895
        vf_loss: 13.531736373901367
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005111999926157296
        entropy: 1.105398416519165
        entropy_coeff: 0.0017600000137463212
        kl: 0.014902329072356224
        model: {}
        policy_loss: -0.03224136680364609
        total_loss: -0.0312453955411911
        vf_explained_var: 0.029936984181404114
        vf_loss: 14.512414932250977
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.0005111999926157296
        entropy: 1.117037296295166
        entropy_coeff: 0.0017600000137463212
        kl: 0.015030592679977417
        model: {}
        policy_loss: -0.034829847514629364
        total_loss: -0.03310602530837059
        vf_explained_var: 0.040572404861450195
        vf_loss: 14.352130889892578
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005111999926157296
        entropy: 0.6662272810935974
        entropy_coeff: 0.0017600000137463212
        kl: 0.010319270193576813
        model: {}
        policy_loss: -0.021987970918416977
        total_loss: -0.02094641514122486
        vf_explained_var: 0.21018682420253754
        vf_loss: 11.821890830993652
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005111999926157296
        entropy: 1.023288607597351
        entropy_coeff: 0.0017600000137463212
        kl: 0.014637376181781292
        model: {}
        policy_loss: -0.034599047154188156
        total_loss: -0.03360453248023987
        vf_explained_var: 0.11048990488052368
        vf_loss: 13.317594528198242
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0005111999926157296
        entropy: 1.032702922821045
        entropy_coeff: 0.0017600000137463212
        kl: 0.017412248998880386
        model: {}
        policy_loss: -0.03997402638196945
        total_loss: -0.038691285997629166
        vf_explained_var: 0.0912417471408844
        vf_loss: 13.590716361999512
    load_time_ms: 13682.01
    num_steps_sampled: 12096000
    num_steps_trained: 12096000
    sample_time_ms: 88304.895
    update_time_ms: 22.833
  iterations_since_restore: 106
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.887005649717514
    ram_util_percent: 9.64406779661017
  pid: 4061
  policy_reward_max:
    agent-0: 139.00000000000026
    agent-1: 139.00000000000026
    agent-2: 139.00000000000026
    agent-3: 139.00000000000026
    agent-4: 139.00000000000026
    agent-5: 139.00000000000026
  policy_reward_mean:
    agent-0: 100.59833333333364
    agent-1: 100.59833333333364
    agent-2: 100.59833333333364
    agent-3: 100.59833333333364
    agent-4: 100.59833333333364
    agent-5: 100.59833333333364
  policy_reward_min:
    agent-0: 45.33333333333326
    agent-1: 45.33333333333326
    agent-2: 45.33333333333326
    agent-3: 45.33333333333326
    agent-4: 45.33333333333326
    agent-5: 45.33333333333326
  sampler_perf:
    mean_env_wait_ms: 23.489865469475404
    mean_inference_ms: 12.29097380140385
    mean_processing_ms: 50.75177502858376
  time_since_restore: 14099.564249038696
  time_this_iter_s: 123.95225691795349
  time_total_s: 17310.627935171127
  timestamp: 1637031624
  timesteps_since_restore: 10176000
  timesteps_this_iter: 96000
  timesteps_total: 12096000
  training_iteration: 126
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    126 |          17310.6 | 12096000 |   603.59 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 55
    apples_agent-0_mean: 7.77
    apples_agent-0_min: 0
    apples_agent-1_max: 84
    apples_agent-1_mean: 20.58
    apples_agent-1_min: 0
    apples_agent-2_max: 173
    apples_agent-2_mean: 10.07
    apples_agent-2_min: 0
    apples_agent-3_max: 122
    apples_agent-3_mean: 68.39
    apples_agent-3_min: 18
    apples_agent-4_max: 58
    apples_agent-4_mean: 4.02
    apples_agent-4_min: 0
    apples_agent-5_max: 149
    apples_agent-5_mean: 67.61
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 490
    cleaning_beam_agent-0_mean: 262.71
    cleaning_beam_agent-0_min: 110
    cleaning_beam_agent-1_max: 338
    cleaning_beam_agent-1_mean: 206.24
    cleaning_beam_agent-1_min: 81
    cleaning_beam_agent-2_max: 442
    cleaning_beam_agent-2_mean: 247.87
    cleaning_beam_agent-2_min: 81
    cleaning_beam_agent-3_max: 125
    cleaning_beam_agent-3_mean: 44.82
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 493
    cleaning_beam_agent-4_mean: 319.72
    cleaning_beam_agent-4_min: 155
    cleaning_beam_agent-5_max: 534
    cleaning_beam_agent-5_mean: 159.87
    cleaning_beam_agent-5_min: 17
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-02-28
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 815.999999999978
  episode_reward_mean: 568.0200000000011
  episode_reward_min: 224.99999999999696
  episodes_this_iter: 96
  episodes_total: 12192
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20096.294
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000505209609400481
        entropy: 1.1713954210281372
        entropy_coeff: 0.0017600000137463212
        kl: 0.013870857656002045
        model: {}
        policy_loss: -0.032501429319381714
        total_loss: -0.03179006278514862
        vf_explained_var: 0.07514388859272003
        vf_loss: 13.859400749206543
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000505209609400481
        entropy: 1.0985733270645142
        entropy_coeff: 0.0017600000137463212
        kl: 0.0150030218064785
        model: {}
        policy_loss: -0.03228156641125679
        total_loss: -0.031289029866456985
        vf_explained_var: 0.049488365650177
        vf_loss: 14.257225036621094
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.000505209609400481
        entropy: 1.091210961341858
        entropy_coeff: 0.0017600000137463212
        kl: 0.014430808834731579
        model: {}
        policy_loss: -0.03420735523104668
        total_loss: -0.03253370150923729
        vf_explained_var: 0.04694847762584686
        vf_loss: 14.295639991760254
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000505209609400481
        entropy: 0.6792022585868835
        entropy_coeff: 0.0017600000137463212
        kl: 0.010820035822689533
        model: {}
        policy_loss: -0.02272311970591545
        total_loss: -0.021725159138441086
        vf_explained_var: 0.25828927755355835
        vf_loss: 11.113517761230469
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000505209609400481
        entropy: 1.0363645553588867
        entropy_coeff: 0.0017600000137463212
        kl: 0.01564604789018631
        model: {}
        policy_loss: -0.032999925315380096
        total_loss: -0.03192498907446861
        vf_explained_var: 0.11006979644298553
        vf_loss: 13.343338012695312
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000505209609400481
        entropy: 1.0330760478973389
        entropy_coeff: 0.0017600000137463212
        kl: 0.016981732100248337
        model: {}
        policy_loss: -0.04004060849547386
        total_loss: -0.03883044794201851
        vf_explained_var: 0.113657146692276
        vf_loss: 13.302026748657227
    load_time_ms: 13809.011
    num_steps_sampled: 12192000
    num_steps_trained: 12192000
    sample_time_ms: 88437.881
    update_time_ms: 23.73
  iterations_since_restore: 107
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.89431818181818
    ram_util_percent: 9.666477272727274
  pid: 4061
  policy_reward_max:
    agent-0: 136.00000000000023
    agent-1: 136.00000000000023
    agent-2: 136.00000000000023
    agent-3: 136.00000000000023
    agent-4: 136.00000000000023
    agent-5: 136.00000000000023
  policy_reward_mean:
    agent-0: 94.67000000000024
    agent-1: 94.67000000000024
    agent-2: 94.67000000000024
    agent-3: 94.67000000000024
    agent-4: 94.67000000000024
    agent-5: 94.67000000000024
  policy_reward_min:
    agent-0: 37.499999999999986
    agent-1: 37.499999999999986
    agent-2: 37.499999999999986
    agent-3: 37.499999999999986
    agent-4: 37.499999999999986
    agent-5: 37.499999999999986
  sampler_perf:
    mean_env_wait_ms: 23.489505664952336
    mean_inference_ms: 12.290265647109404
    mean_processing_ms: 50.74970243198401
  time_since_restore: 14222.707300901413
  time_this_iter_s: 123.14305186271667
  time_total_s: 17433.770987033844
  timestamp: 1637031748
  timesteps_since_restore: 10272000
  timesteps_this_iter: 96000
  timesteps_total: 12192000
  training_iteration: 127
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    127 |          17433.8 | 12192000 |   568.02 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 66
    apples_agent-0_mean: 8.58
    apples_agent-0_min: 0
    apples_agent-1_max: 67
    apples_agent-1_mean: 19.18
    apples_agent-1_min: 0
    apples_agent-2_max: 81
    apples_agent-2_mean: 8.42
    apples_agent-2_min: 0
    apples_agent-3_max: 123
    apples_agent-3_mean: 67.42
    apples_agent-3_min: 8
    apples_agent-4_max: 111
    apples_agent-4_mean: 7.46
    apples_agent-4_min: 0
    apples_agent-5_max: 135
    apples_agent-5_mean: 69.58
    apples_agent-5_min: 3
    cleaning_beam_agent-0_max: 509
    cleaning_beam_agent-0_mean: 265.67
    cleaning_beam_agent-0_min: 116
    cleaning_beam_agent-1_max: 378
    cleaning_beam_agent-1_mean: 206.67
    cleaning_beam_agent-1_min: 115
    cleaning_beam_agent-2_max: 434
    cleaning_beam_agent-2_mean: 264.89
    cleaning_beam_agent-2_min: 77
    cleaning_beam_agent-3_max: 196
    cleaning_beam_agent-3_mean: 46.12
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 610
    cleaning_beam_agent-4_mean: 321.58
    cleaning_beam_agent-4_min: 139
    cleaning_beam_agent-5_max: 480
    cleaning_beam_agent-5_mean: 135.62
    cleaning_beam_agent-5_min: 25
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-04-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 853.9999999999882
  episode_reward_mean: 571.8500000000001
  episode_reward_min: 44.99999999999975
  episodes_this_iter: 96
  episodes_total: 12288
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20105.839
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0004992192261852324
        entropy: 1.1838352680206299
        entropy_coeff: 0.0017600000137463212
        kl: 0.01362050510942936
        model: {}
        policy_loss: -0.033732347190380096
        total_loss: -0.032906122505664825
        vf_explained_var: 0.04078073799610138
        vf_loss: 15.477252960205078
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0004992192261852324
        entropy: 1.0913331508636475
        entropy_coeff: 0.0017600000137463212
        kl: 0.015007330104708672
        model: {}
        policy_loss: -0.033072084188461304
        total_loss: -0.032037146389484406
        vf_explained_var: 0.09745019674301147
        vf_loss: 14.549542427062988
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.0004992192261852324
        entropy: 1.1090540885925293
        entropy_coeff: 0.0017600000137463212
        kl: 0.014315962791442871
        model: {}
        policy_loss: -0.03434120863676071
        total_loss: -0.03263974189758301
        vf_explained_var: 0.06505720317363739
        vf_loss: 15.060087203979492
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0004992192261852324
        entropy: 0.6763758063316345
        entropy_coeff: 0.0017600000137463212
        kl: 0.010800301097333431
        model: {}
        policy_loss: -0.023561745882034302
        total_loss: -0.02243659272789955
        vf_explained_var: 0.23419685661792755
        vf_loss: 12.355453491210938
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0004992192261852324
        entropy: 1.033251166343689
        entropy_coeff: 0.0017600000137463212
        kl: 0.015466736629605293
        model: {}
        policy_loss: -0.03363394737243652
        total_loss: -0.03250521421432495
        vf_explained_var: 0.13073022663593292
        vf_loss: 14.005799293518066
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0004992192261852324
        entropy: 1.03995680809021
        entropy_coeff: 0.0017600000137463212
        kl: 0.016984326764941216
        model: {}
        policy_loss: -0.03950872644782066
        total_loss: -0.0382319875061512
        vf_explained_var: 0.125790074467659
        vf_loss: 14.086284637451172
    load_time_ms: 13811.323
    num_steps_sampled: 12288000
    num_steps_trained: 12288000
    sample_time_ms: 88770.833
    update_time_ms: 23.455
  iterations_since_restore: 108
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.74888888888889
    ram_util_percent: 12.712222222222223
  pid: 4061
  policy_reward_max:
    agent-0: 142.33333333333366
    agent-1: 142.33333333333366
    agent-2: 142.33333333333366
    agent-3: 142.33333333333366
    agent-4: 142.33333333333366
    agent-5: 142.33333333333366
  policy_reward_mean:
    agent-0: 95.3083333333336
    agent-1: 95.3083333333336
    agent-2: 95.3083333333336
    agent-3: 95.3083333333336
    agent-4: 95.3083333333336
    agent-5: 95.3083333333336
  policy_reward_min:
    agent-0: 7.500000000000008
    agent-1: 7.500000000000008
    agent-2: 7.500000000000008
    agent-3: 7.500000000000008
    agent-4: 7.500000000000008
    agent-5: 7.500000000000008
  sampler_perf:
    mean_env_wait_ms: 23.492218607464302
    mean_inference_ms: 12.29080229098074
    mean_processing_ms: 50.75124339975913
  time_since_restore: 14349.177781105042
  time_this_iter_s: 126.47048020362854
  time_total_s: 17560.241467237473
  timestamp: 1637031874
  timesteps_since_restore: 10368000
  timesteps_this_iter: 96000
  timesteps_total: 12288000
  training_iteration: 128
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    128 |          17560.2 | 12288000 |   571.85 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 42
    apples_agent-0_mean: 7.0
    apples_agent-0_min: 0
    apples_agent-1_max: 81
    apples_agent-1_mean: 22.58
    apples_agent-1_min: 0
    apples_agent-2_max: 133
    apples_agent-2_mean: 6.9
    apples_agent-2_min: 0
    apples_agent-3_max: 150
    apples_agent-3_mean: 68.68
    apples_agent-3_min: 17
    apples_agent-4_max: 79
    apples_agent-4_mean: 4.42
    apples_agent-4_min: 0
    apples_agent-5_max: 183
    apples_agent-5_mean: 70.71
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 475
    cleaning_beam_agent-0_mean: 267.15
    cleaning_beam_agent-0_min: 134
    cleaning_beam_agent-1_max: 323
    cleaning_beam_agent-1_mean: 203.03
    cleaning_beam_agent-1_min: 96
    cleaning_beam_agent-2_max: 458
    cleaning_beam_agent-2_mean: 287.29
    cleaning_beam_agent-2_min: 62
    cleaning_beam_agent-3_max: 152
    cleaning_beam_agent-3_mean: 41.83
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 552
    cleaning_beam_agent-4_mean: 321.72
    cleaning_beam_agent-4_min: 155
    cleaning_beam_agent-5_max: 623
    cleaning_beam_agent-5_mean: 153.91
    cleaning_beam_agent-5_min: 23
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-06-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 884.9999999999872
  episode_reward_mean: 587.7599999999995
  episode_reward_min: 193.99999999999764
  episodes_this_iter: 96
  episodes_total: 12384
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20092.015
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0004932287847623229
        entropy: 1.1721574068069458
        entropy_coeff: 0.0017600000137463212
        kl: 0.014271777123212814
        model: {}
        policy_loss: -0.03419267013669014
        total_loss: -0.03333360329270363
        vf_explained_var: 0.10586336255073547
        vf_loss: 14.948853492736816
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0004932287847623229
        entropy: 1.0962181091308594
        entropy_coeff: 0.0017600000137463212
        kl: 0.01644083857536316
        model: {}
        policy_loss: -0.03092513233423233
        total_loss: -0.029655931517481804
        vf_explained_var: 0.0700092613697052
        vf_loss: 15.544624328613281
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.0004932287847623229
        entropy: 1.0885039567947388
        entropy_coeff: 0.0017600000137463212
        kl: 0.014106586575508118
        model: {}
        policy_loss: -0.03421555459499359
        total_loss: -0.03241037204861641
        vf_explained_var: 0.04026862978935242
        vf_loss: 16.049606323242188
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0004932287847623229
        entropy: 0.6722383499145508
        entropy_coeff: 0.0017600000137463212
        kl: 0.009901903569698334
        model: {}
        policy_loss: -0.02279391884803772
        total_loss: -0.021761680021882057
        vf_explained_var: 0.26740071177482605
        vf_loss: 12.251869201660156
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0004932287847623229
        entropy: 1.020993709564209
        entropy_coeff: 0.0017600000137463212
        kl: 0.014313948340713978
        model: {}
        policy_loss: -0.033572718501091
        total_loss: -0.032477691769599915
        vf_explained_var: 0.1263829618692398
        vf_loss: 14.605754852294922
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0004932287847623229
        entropy: 1.0067524909973145
        entropy_coeff: 0.0017600000137463212
        kl: 0.01649877056479454
        model: {}
        policy_loss: -0.04038648307323456
        total_loss: -0.0390157513320446
        vf_explained_var: 0.10809379816055298
        vf_loss: 14.927422523498535
    load_time_ms: 13858.497
    num_steps_sampled: 12384000
    num_steps_trained: 12384000
    sample_time_ms: 89394.257
    update_time_ms: 23.321
  iterations_since_restore: 109
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.519565217391307
    ram_util_percent: 13.877717391304348
  pid: 4061
  policy_reward_max:
    agent-0: 147.4999999999997
    agent-1: 147.4999999999997
    agent-2: 147.4999999999997
    agent-3: 147.4999999999997
    agent-4: 147.4999999999997
    agent-5: 147.4999999999997
  policy_reward_mean:
    agent-0: 97.96000000000029
    agent-1: 97.96000000000029
    agent-2: 97.96000000000029
    agent-3: 97.96000000000029
    agent-4: 97.96000000000029
    agent-5: 97.96000000000029
  policy_reward_min:
    agent-0: 32.33333333333342
    agent-1: 32.33333333333342
    agent-2: 32.33333333333342
    agent-3: 32.33333333333342
    agent-4: 32.33333333333342
    agent-5: 32.33333333333342
  sampler_perf:
    mean_env_wait_ms: 23.499409385484103
    mean_inference_ms: 12.292926008546782
    mean_processing_ms: 50.75898805118335
  time_since_restore: 14477.682387113571
  time_this_iter_s: 128.50460600852966
  time_total_s: 17688.746073246002
  timestamp: 1637032003
  timesteps_since_restore: 10464000
  timesteps_this_iter: 96000
  timesteps_total: 12384000
  training_iteration: 129
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    129 |          17688.7 | 12384000 |   587.76 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 79
    apples_agent-0_mean: 6.95
    apples_agent-0_min: 0
    apples_agent-1_max: 83
    apples_agent-1_mean: 23.9
    apples_agent-1_min: 0
    apples_agent-2_max: 69
    apples_agent-2_mean: 7.02
    apples_agent-2_min: 0
    apples_agent-3_max: 136
    apples_agent-3_mean: 72.6
    apples_agent-3_min: 28
    apples_agent-4_max: 41
    apples_agent-4_mean: 2.68
    apples_agent-4_min: 0
    apples_agent-5_max: 213
    apples_agent-5_mean: 69.81
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 481
    cleaning_beam_agent-0_mean: 280.63
    cleaning_beam_agent-0_min: 132
    cleaning_beam_agent-1_max: 431
    cleaning_beam_agent-1_mean: 209.69
    cleaning_beam_agent-1_min: 84
    cleaning_beam_agent-2_max: 444
    cleaning_beam_agent-2_mean: 271.98
    cleaning_beam_agent-2_min: 88
    cleaning_beam_agent-3_max: 119
    cleaning_beam_agent-3_mean: 41.38
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 545
    cleaning_beam_agent-4_mean: 316.56
    cleaning_beam_agent-4_min: 120
    cleaning_beam_agent-5_max: 779
    cleaning_beam_agent-5_mean: 149.92
    cleaning_beam_agent-5_min: 23
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-08-50
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 835.9999999999789
  episode_reward_mean: 584.1300000000007
  episode_reward_min: 255.99999999999582
  episodes_this_iter: 96
  episodes_total: 12480
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20061.814
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0004872384015470743
        entropy: 1.1708275079727173
        entropy_coeff: 0.0017600000137463212
        kl: 0.013557794503867626
        model: {}
        policy_loss: -0.033304229378700256
        total_loss: -0.032697372138500214
        vf_explained_var: 0.05604584515094757
        vf_loss: 13.117326736450195
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0004872384015470743
        entropy: 1.090669870376587
        entropy_coeff: 0.0017600000137463212
        kl: 0.015438564121723175
        model: {}
        policy_loss: -0.03354046866297722
        total_loss: -0.032581016421318054
        vf_explained_var: 0.03831292688846588
        vf_loss: 13.351753234863281
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.0004872384015470743
        entropy: 1.102126955986023
        entropy_coeff: 0.0017600000137463212
        kl: 0.014231720007956028
        model: {}
        policy_loss: -0.034373003989458084
        total_loss: -0.032846614718437195
        vf_explained_var: 0.041527777910232544
        vf_loss: 13.313715934753418
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0004872384015470743
        entropy: 0.6718660593032837
        entropy_coeff: 0.0017600000137463212
        kl: 0.01007935032248497
        model: {}
        policy_loss: -0.022487610578536987
        total_loss: -0.021549126133322716
        vf_explained_var: 0.1980070024728775
        vf_loss: 11.130311965942383
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0004872384015470743
        entropy: 1.00630521774292
        entropy_coeff: 0.0017600000137463212
        kl: 0.014088164083659649
        model: {}
        policy_loss: -0.03332987800240517
        total_loss: -0.03241831436753273
        vf_explained_var: 0.08373254537582397
        vf_loss: 12.738441467285156
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0004872384015470743
        entropy: 1.0098507404327393
        entropy_coeff: 0.0017600000137463212
        kl: 0.01657181978225708
        model: {}
        policy_loss: -0.03867659717798233
        total_loss: -0.037499506026506424
        vf_explained_var: 0.06676346063613892
        vf_loss: 12.972464561462402
    load_time_ms: 13903.538
    num_steps_sampled: 12480000
    num_steps_trained: 12480000
    sample_time_ms: 89828.464
    update_time_ms: 22.817
  iterations_since_restore: 110
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.79111111111111
    ram_util_percent: 13.88
  pid: 4061
  policy_reward_max:
    agent-0: 139.3333333333335
    agent-1: 139.3333333333335
    agent-2: 139.3333333333335
    agent-3: 139.3333333333335
    agent-4: 139.3333333333335
    agent-5: 139.3333333333335
  policy_reward_mean:
    agent-0: 97.35500000000027
    agent-1: 97.35500000000027
    agent-2: 97.35500000000027
    agent-3: 97.35500000000027
    agent-4: 97.35500000000027
    agent-5: 97.35500000000027
  policy_reward_min:
    agent-0: 42.666666666666615
    agent-1: 42.666666666666615
    agent-2: 42.666666666666615
    agent-3: 42.666666666666615
    agent-4: 42.666666666666615
    agent-5: 42.666666666666615
  sampler_perf:
    mean_env_wait_ms: 23.50633135853219
    mean_inference_ms: 12.29526440982716
    mean_processing_ms: 50.76853669590996
  time_since_restore: 14604.184083461761
  time_this_iter_s: 126.50169634819031
  time_total_s: 17815.247769594193
  timestamp: 1637032130
  timesteps_since_restore: 10560000
  timesteps_this_iter: 96000
  timesteps_total: 12480000
  training_iteration: 130
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    130 |          17815.2 | 12480000 |   584.13 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 65
    apples_agent-0_mean: 7.17
    apples_agent-0_min: 0
    apples_agent-1_max: 101
    apples_agent-1_mean: 19.82
    apples_agent-1_min: 0
    apples_agent-2_max: 90
    apples_agent-2_mean: 9.93
    apples_agent-2_min: 0
    apples_agent-3_max: 166
    apples_agent-3_mean: 73.06
    apples_agent-3_min: 29
    apples_agent-4_max: 55
    apples_agent-4_mean: 2.16
    apples_agent-4_min: 0
    apples_agent-5_max: 149
    apples_agent-5_mean: 76.76
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 481
    cleaning_beam_agent-0_mean: 276.44
    cleaning_beam_agent-0_min: 145
    cleaning_beam_agent-1_max: 359
    cleaning_beam_agent-1_mean: 212.89
    cleaning_beam_agent-1_min: 91
    cleaning_beam_agent-2_max: 425
    cleaning_beam_agent-2_mean: 254.87
    cleaning_beam_agent-2_min: 90
    cleaning_beam_agent-3_max: 179
    cleaning_beam_agent-3_mean: 44.92
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 502
    cleaning_beam_agent-4_mean: 312.45
    cleaning_beam_agent-4_min: 115
    cleaning_beam_agent-5_max: 799
    cleaning_beam_agent-5_mean: 123.15
    cleaning_beam_agent-5_min: 24
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-10-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 899.9999999999844
  episode_reward_mean: 579.5000000000007
  episode_reward_min: 271.9999999999974
  episodes_this_iter: 96
  episodes_total: 12576
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20053.656
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0004812479892279953
        entropy: 1.1621904373168945
        entropy_coeff: 0.0017600000137463212
        kl: 0.01430053822696209
        model: {}
        policy_loss: -0.032799214124679565
        total_loss: -0.0319632813334465
        vf_explained_var: 0.08534331619739532
        vf_loss: 14.513344764709473
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0004812479892279953
        entropy: 1.1019080877304077
        entropy_coeff: 0.0017600000137463212
        kl: 0.014088688418269157
        model: {}
        policy_loss: -0.03234381973743439
        total_loss: -0.031367164105176926
        vf_explained_var: 0.04925481975078583
        vf_loss: 15.071493148803711
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.0004812479892279953
        entropy: 1.0988658666610718
        entropy_coeff: 0.0017600000137463212
        kl: 0.015024345368146896
        model: {}
        policy_loss: -0.03569421172142029
        total_loss: -0.03386657312512398
        vf_explained_var: 0.04871177673339844
        vf_loss: 15.07984733581543
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0004812479892279953
        entropy: 0.6844388842582703
        entropy_coeff: 0.0017600000137463212
        kl: 0.010676910169422626
        model: {}
        policy_loss: -0.02325189672410488
        total_loss: -0.022149143740534782
        vf_explained_var: 0.21788205206394196
        vf_loss: 12.396800994873047
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0004812479892279953
        entropy: 1.0235239267349243
        entropy_coeff: 0.0017600000137463212
        kl: 0.015681488439440727
        model: {}
        policy_loss: -0.031119560822844505
        total_loss: -0.02995966374874115
        vf_explained_var: 0.12106367945671082
        vf_loss: 13.931510925292969
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0004812479892279953
        entropy: 1.0358504056930542
        entropy_coeff: 0.0017600000137463212
        kl: 0.017145738005638123
        model: {}
        policy_loss: -0.040079694241285324
        total_loss: -0.03879614174365997
        vf_explained_var: 0.12226051092147827
        vf_loss: 13.920757293701172
    load_time_ms: 13870.646
    num_steps_sampled: 12576000
    num_steps_trained: 12576000
    sample_time_ms: 90509.371
    update_time_ms: 23.607
  iterations_since_restore: 111
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.528648648648648
    ram_util_percent: 13.940540540540542
  pid: 4061
  policy_reward_max:
    agent-0: 150.0000000000004
    agent-1: 150.0000000000004
    agent-2: 150.0000000000004
    agent-3: 150.0000000000004
    agent-4: 150.0000000000004
    agent-5: 150.0000000000004
  policy_reward_mean:
    agent-0: 96.5833333333336
    agent-1: 96.5833333333336
    agent-2: 96.5833333333336
    agent-3: 96.5833333333336
    agent-4: 96.5833333333336
    agent-5: 96.5833333333336
  policy_reward_min:
    agent-0: 45.33333333333327
    agent-1: 45.33333333333327
    agent-2: 45.33333333333327
    agent-3: 45.33333333333327
    agent-4: 45.33333333333327
    agent-5: 45.33333333333327
  sampler_perf:
    mean_env_wait_ms: 23.513412376811512
    mean_inference_ms: 12.297414541663445
    mean_processing_ms: 50.784191054511204
  time_since_restore: 14733.60553097725
  time_this_iter_s: 129.42144751548767
  time_total_s: 17944.66921710968
  timestamp: 1637032259
  timesteps_since_restore: 10656000
  timesteps_this_iter: 96000
  timesteps_total: 12576000
  training_iteration: 131
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    131 |          17944.7 | 12576000 |    579.5 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 57
    apples_agent-0_mean: 6.87
    apples_agent-0_min: 0
    apples_agent-1_max: 89
    apples_agent-1_mean: 21.25
    apples_agent-1_min: 0
    apples_agent-2_max: 300
    apples_agent-2_mean: 10.82
    apples_agent-2_min: 0
    apples_agent-3_max: 126
    apples_agent-3_mean: 67.88
    apples_agent-3_min: 17
    apples_agent-4_max: 112
    apples_agent-4_mean: 6.58
    apples_agent-4_min: 0
    apples_agent-5_max: 153
    apples_agent-5_mean: 72.97
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 451
    cleaning_beam_agent-0_mean: 288.42
    cleaning_beam_agent-0_min: 123
    cleaning_beam_agent-1_max: 356
    cleaning_beam_agent-1_mean: 206.96
    cleaning_beam_agent-1_min: 108
    cleaning_beam_agent-2_max: 441
    cleaning_beam_agent-2_mean: 273.49
    cleaning_beam_agent-2_min: 92
    cleaning_beam_agent-3_max: 177
    cleaning_beam_agent-3_mean: 44.6
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 589
    cleaning_beam_agent-4_mean: 321.91
    cleaning_beam_agent-4_min: 129
    cleaning_beam_agent-5_max: 700
    cleaning_beam_agent-5_mean: 123.55
    cleaning_beam_agent-5_min: 27
    fire_beam_agent-0_max: 3
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-13-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 810.9999999999837
  episode_reward_mean: 590.6199999999983
  episode_reward_min: 94.00000000000044
  episodes_this_iter: 96
  episodes_total: 12672
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20052.717
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0004752576060127467
        entropy: 1.1496888399124146
        entropy_coeff: 0.0017600000137463212
        kl: 0.013437889516353607
        model: {}
        policy_loss: -0.03348254784941673
        total_loss: -0.032680585980415344
        vf_explained_var: 0.07824636995792389
        vf_loss: 14.816271781921387
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0004752576060127467
        entropy: 1.1046724319458008
        entropy_coeff: 0.0017600000137463212
        kl: 0.014771339483559132
        model: {}
        policy_loss: -0.03404545411467552
        total_loss: -0.03296219930052757
        vf_explained_var: 0.03559444844722748
        vf_loss: 15.503458023071289
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.0004752576060127467
        entropy: 1.0929232835769653
        entropy_coeff: 0.0017600000137463212
        kl: 0.01531747356057167
        model: {}
        policy_loss: -0.03532685339450836
        total_loss: -0.0334380567073822
        vf_explained_var: 0.05784609913825989
        vf_loss: 15.14715576171875
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0004752576060127467
        entropy: 0.654632031917572
        entropy_coeff: 0.0017600000137463212
        kl: 0.010110732167959213
        model: {}
        policy_loss: -0.022769074887037277
        total_loss: -0.02167385444045067
        vf_explained_var: 0.23074881732463837
        vf_loss: 12.362993240356445
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0004752576060127467
        entropy: 1.0226082801818848
        entropy_coeff: 0.0017600000137463212
        kl: 0.013770921155810356
        model: {}
        policy_loss: -0.033334385603666306
        total_loss: -0.032363951206207275
        vf_explained_var: 0.13357022404670715
        vf_loss: 13.931325912475586
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0004752576060127467
        entropy: 1.0295549631118774
        entropy_coeff: 0.0017600000137463212
        kl: 0.01749671995639801
        model: {}
        policy_loss: -0.03832489997148514
        total_loss: -0.036953747272491455
        vf_explained_var: 0.10865052044391632
        vf_loss: 14.334957122802734
    load_time_ms: 13828.229
    num_steps_sampled: 12672000
    num_steps_trained: 12672000
    sample_time_ms: 90949.329
    update_time_ms: 23.415
  iterations_since_restore: 112
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.720000000000002
    ram_util_percent: 13.954444444444446
  pid: 4061
  policy_reward_max:
    agent-0: 135.16666666666708
    agent-1: 135.16666666666708
    agent-2: 135.16666666666708
    agent-3: 135.16666666666708
    agent-4: 135.16666666666708
    agent-5: 135.16666666666708
  policy_reward_mean:
    agent-0: 98.436666666667
    agent-1: 98.436666666667
    agent-2: 98.436666666667
    agent-3: 98.436666666667
    agent-4: 98.436666666667
    agent-5: 98.436666666667
  policy_reward_min:
    agent-0: 15.666666666666648
    agent-1: 15.666666666666648
    agent-2: 15.666666666666648
    agent-3: 15.666666666666648
    agent-4: 15.666666666666648
    agent-5: 15.666666666666648
  sampler_perf:
    mean_env_wait_ms: 23.519732319288792
    mean_inference_ms: 12.298982889653757
    mean_processing_ms: 50.79233668953449
  time_since_restore: 14860.075858592987
  time_this_iter_s: 126.47032761573792
  time_total_s: 18071.139544725418
  timestamp: 1637032386
  timesteps_since_restore: 10752000
  timesteps_this_iter: 96000
  timesteps_total: 12672000
  training_iteration: 132
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    132 |          18071.1 | 12672000 |   590.62 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 37
    apples_agent-0_mean: 8.29
    apples_agent-0_min: 0
    apples_agent-1_max: 87
    apples_agent-1_mean: 20.85
    apples_agent-1_min: 0
    apples_agent-2_max: 65
    apples_agent-2_mean: 8.95
    apples_agent-2_min: 0
    apples_agent-3_max: 128
    apples_agent-3_mean: 70.78
    apples_agent-3_min: 34
    apples_agent-4_max: 101
    apples_agent-4_mean: 4.88
    apples_agent-4_min: 0
    apples_agent-5_max: 159
    apples_agent-5_mean: 73.01
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 439
    cleaning_beam_agent-0_mean: 268.54
    cleaning_beam_agent-0_min: 133
    cleaning_beam_agent-1_max: 363
    cleaning_beam_agent-1_mean: 193.38
    cleaning_beam_agent-1_min: 95
    cleaning_beam_agent-2_max: 466
    cleaning_beam_agent-2_mean: 262.35
    cleaning_beam_agent-2_min: 61
    cleaning_beam_agent-3_max: 112
    cleaning_beam_agent-3_mean: 42.73
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 467
    cleaning_beam_agent-4_mean: 320.97
    cleaning_beam_agent-4_min: 160
    cleaning_beam_agent-5_max: 706
    cleaning_beam_agent-5_mean: 142.23
    cleaning_beam_agent-5_min: 31
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-15-15
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 870.9999999999764
  episode_reward_mean: 581.2700000000008
  episode_reward_min: 220.99999999999656
  episodes_this_iter: 96
  episodes_total: 12768
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20045.948
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00046926719369366765
        entropy: 1.1469041109085083
        entropy_coeff: 0.0017600000137463212
        kl: 0.014312800951302052
        model: {}
        policy_loss: -0.034592144191265106
        total_loss: -0.03391149267554283
        vf_explained_var: 0.09139604866504669
        vf_loss: 12.679198265075684
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00046926719369366765
        entropy: 1.1080338954925537
        entropy_coeff: 0.0017600000137463212
        kl: 0.015231321565806866
        model: {}
        policy_loss: -0.03374490514397621
        total_loss: -0.03282316029071808
        vf_explained_var: 0.03436572849750519
        vf_loss: 13.487542152404785
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.00046926719369366765
        entropy: 1.0999951362609863
        entropy_coeff: 0.0017600000137463212
        kl: 0.014948018826544285
        model: {}
        policy_loss: -0.033441800624132156
        total_loss: -0.031762849539518356
        vf_explained_var: 0.01584668457508087
        vf_loss: 13.727340698242188
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00046926719369366765
        entropy: 0.667193591594696
        entropy_coeff: 0.0017600000137463212
        kl: 0.009711900725960732
        model: {}
        policy_loss: -0.02227051369845867
        total_loss: -0.02133549004793167
        vf_explained_var: 0.1839166134595871
        vf_loss: 11.380903244018555
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00046926719369366765
        entropy: 1.0168062448501587
        entropy_coeff: 0.0017600000137463212
        kl: 0.014669924974441528
        model: {}
        policy_loss: -0.03286690637469292
        total_loss: -0.03191761672496796
        vf_explained_var: 0.08832754194736481
        vf_loss: 12.71877384185791
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00046926719369366765
        entropy: 1.0097686052322388
        entropy_coeff: 0.0017600000137463212
        kl: 0.017312001436948776
        model: {}
        policy_loss: -0.03818826377391815
        total_loss: -0.03693180903792381
        vf_explained_var: 0.06594018638134003
        vf_loss: 13.024484634399414
    load_time_ms: 13914.936
    num_steps_sampled: 12768000
    num_steps_trained: 12768000
    sample_time_ms: 91561.998
    update_time_ms: 22.97
  iterations_since_restore: 113
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.520652173913046
    ram_util_percent: 13.951086956521742
  pid: 4061
  policy_reward_max:
    agent-0: 145.16666666666654
    agent-1: 145.16666666666654
    agent-2: 145.16666666666654
    agent-3: 145.16666666666654
    agent-4: 145.16666666666654
    agent-5: 145.16666666666654
  policy_reward_mean:
    agent-0: 96.87833333333363
    agent-1: 96.87833333333363
    agent-2: 96.87833333333363
    agent-3: 96.87833333333363
    agent-4: 96.87833333333363
    agent-5: 96.87833333333363
  policy_reward_min:
    agent-0: 36.83333333333334
    agent-1: 36.83333333333334
    agent-2: 36.83333333333334
    agent-3: 36.83333333333334
    agent-4: 36.83333333333334
    agent-5: 36.83333333333334
  sampler_perf:
    mean_env_wait_ms: 23.52549722653108
    mean_inference_ms: 12.30153374978756
    mean_processing_ms: 50.80435330340775
  time_since_restore: 14988.826883792877
  time_this_iter_s: 128.75102519989014
  time_total_s: 18199.89056992531
  timestamp: 1637032515
  timesteps_since_restore: 10848000
  timesteps_this_iter: 96000
  timesteps_total: 12768000
  training_iteration: 133
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    133 |          18199.9 | 12768000 |   581.27 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 68
    apples_agent-0_mean: 6.12
    apples_agent-0_min: 0
    apples_agent-1_max: 76
    apples_agent-1_mean: 19.18
    apples_agent-1_min: 0
    apples_agent-2_max: 110
    apples_agent-2_mean: 9.07
    apples_agent-2_min: 0
    apples_agent-3_max: 130
    apples_agent-3_mean: 69.08
    apples_agent-3_min: 27
    apples_agent-4_max: 39
    apples_agent-4_mean: 2.68
    apples_agent-4_min: 0
    apples_agent-5_max: 156
    apples_agent-5_mean: 79.22
    apples_agent-5_min: 15
    cleaning_beam_agent-0_max: 441
    cleaning_beam_agent-0_mean: 278.57
    cleaning_beam_agent-0_min: 149
    cleaning_beam_agent-1_max: 482
    cleaning_beam_agent-1_mean: 203.07
    cleaning_beam_agent-1_min: 86
    cleaning_beam_agent-2_max: 427
    cleaning_beam_agent-2_mean: 258.75
    cleaning_beam_agent-2_min: 140
    cleaning_beam_agent-3_max: 115
    cleaning_beam_agent-3_mean: 40.84
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 520
    cleaning_beam_agent-4_mean: 325.16
    cleaning_beam_agent-4_min: 176
    cleaning_beam_agent-5_max: 457
    cleaning_beam_agent-5_mean: 126.91
    cleaning_beam_agent-5_min: 23
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.04
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-17-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 889.9999999999903
  episode_reward_mean: 606.4599999999997
  episode_reward_min: 323.9999999999989
  episodes_this_iter: 96
  episodes_total: 12864
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20062.843
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00046327681047841907
        entropy: 1.1590152978897095
        entropy_coeff: 0.0017600000137463212
        kl: 0.014323142357170582
        model: {}
        policy_loss: -0.03322044759988785
        total_loss: -0.03240678459405899
        vf_explained_var: 0.081318199634552
        vf_loss: 14.212194442749023
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00046327681047841907
        entropy: 1.1122455596923828
        entropy_coeff: 0.0017600000137463212
        kl: 0.014362214133143425
        model: {}
        policy_loss: -0.033246420323848724
        total_loss: -0.03235550597310066
        vf_explained_var: 0.08757086098194122
        vf_loss: 14.122420310974121
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.00046327681047841907
        entropy: 1.0959422588348389
        entropy_coeff: 0.0017600000137463212
        kl: 0.015221122652292252
        model: {}
        policy_loss: -0.03262600675225258
        total_loss: -0.03081389144062996
        vf_explained_var: 0.05768100917339325
        vf_loss: 14.578068733215332
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00046327681047841907
        entropy: 0.6221818327903748
        entropy_coeff: 0.0017600000137463212
        kl: 0.00932278111577034
        model: {}
        policy_loss: -0.021189741790294647
        total_loss: -0.020183034241199493
        vf_explained_var: 0.24476680159568787
        vf_loss: 11.694656372070312
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00046327681047841907
        entropy: 1.017364740371704
        entropy_coeff: 0.0017600000137463212
        kl: 0.01519338320940733
        model: {}
        policy_loss: -0.0340995267033577
        total_loss: -0.03306075185537338
        vf_explained_var: 0.1534455567598343
        vf_loss: 13.099952697753906
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00046327681047841907
        entropy: 1.0203475952148438
        entropy_coeff: 0.0017600000137463212
        kl: 0.016674814745783806
        model: {}
        policy_loss: -0.03872200846672058
        total_loss: -0.03744269162416458
        vf_explained_var: 0.09104721248149872
        vf_loss: 14.076414108276367
    load_time_ms: 13999.421
    num_steps_sampled: 12864000
    num_steps_trained: 12864000
    sample_time_ms: 92069.565
    update_time_ms: 22.79
  iterations_since_restore: 114
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.629670329670333
    ram_util_percent: 13.87857142857143
  pid: 4061
  policy_reward_max:
    agent-0: 148.33333333333357
    agent-1: 148.33333333333357
    agent-2: 148.33333333333357
    agent-3: 148.33333333333357
    agent-4: 148.33333333333357
    agent-5: 148.33333333333357
  policy_reward_mean:
    agent-0: 101.07666666666697
    agent-1: 101.07666666666697
    agent-2: 101.07666666666697
    agent-3: 101.07666666666697
    agent-4: 101.07666666666697
    agent-5: 101.07666666666697
  policy_reward_min:
    agent-0: 53.999999999999986
    agent-1: 53.999999999999986
    agent-2: 53.999999999999986
    agent-3: 53.999999999999986
    agent-4: 53.999999999999986
    agent-5: 53.999999999999986
  sampler_perf:
    mean_env_wait_ms: 23.531895712881557
    mean_inference_ms: 12.303259592971235
    mean_processing_ms: 50.814028795604855
  time_since_restore: 15116.616472244263
  time_this_iter_s: 127.7895884513855
  time_total_s: 18327.680158376694
  timestamp: 1637032643
  timesteps_since_restore: 10944000
  timesteps_this_iter: 96000
  timesteps_total: 12864000
  training_iteration: 134
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    134 |          18327.7 | 12864000 |   606.46 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 48
    apples_agent-0_mean: 6.43
    apples_agent-0_min: 0
    apples_agent-1_max: 89
    apples_agent-1_mean: 20.05
    apples_agent-1_min: 0
    apples_agent-2_max: 77
    apples_agent-2_mean: 10.98
    apples_agent-2_min: 0
    apples_agent-3_max: 136
    apples_agent-3_mean: 78.22
    apples_agent-3_min: 28
    apples_agent-4_max: 55
    apples_agent-4_mean: 2.53
    apples_agent-4_min: 0
    apples_agent-5_max: 182
    apples_agent-5_mean: 77.46
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 471
    cleaning_beam_agent-0_mean: 285.35
    cleaning_beam_agent-0_min: 170
    cleaning_beam_agent-1_max: 308
    cleaning_beam_agent-1_mean: 191.74
    cleaning_beam_agent-1_min: 86
    cleaning_beam_agent-2_max: 411
    cleaning_beam_agent-2_mean: 234.29
    cleaning_beam_agent-2_min: 76
    cleaning_beam_agent-3_max: 172
    cleaning_beam_agent-3_mean: 46.24
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 539
    cleaning_beam_agent-4_mean: 344.22
    cleaning_beam_agent-4_min: 164
    cleaning_beam_agent-5_max: 595
    cleaning_beam_agent-5_mean: 130.4
    cleaning_beam_agent-5_min: 25
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-19-30
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 904.9999999999661
  episode_reward_mean: 603.0899999999995
  episode_reward_min: 270.99999999999807
  episodes_this_iter: 96
  episodes_total: 12960
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20084.499
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00045728639815934
        entropy: 1.1501721143722534
        entropy_coeff: 0.0017600000137463212
        kl: 0.014589729718863964
        model: {}
        policy_loss: -0.033257998526096344
        total_loss: -0.03234297037124634
        vf_explained_var: 0.04168976843357086
        vf_loss: 14.803569793701172
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00045728639815934
        entropy: 1.108170986175537
        entropy_coeff: 0.0017600000137463212
        kl: 0.014883805997669697
        model: {}
        policy_loss: -0.03240445256233215
        total_loss: -0.03136854246258736
        vf_explained_var: 0.02986796200275421
        vf_loss: 14.97911262512207
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.00045728639815934
        entropy: 1.0988647937774658
        entropy_coeff: 0.0017600000137463212
        kl: 0.01430739089846611
        model: {}
        policy_loss: -0.033612631261348724
        total_loss: -0.03202100470662117
        vf_explained_var: 0.10610179603099823
        vf_loss: 13.79521369934082
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00045728639815934
        entropy: 0.6532164812088013
        entropy_coeff: 0.0017600000137463212
        kl: 0.010623791255056858
        model: {}
        policy_loss: -0.022595105692744255
        total_loss: -0.021480239927768707
        vf_explained_var: 0.22129952907562256
        vf_loss: 12.021478652954102
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00045728639815934
        entropy: 0.9866340160369873
        entropy_coeff: 0.0017600000137463212
        kl: 0.014348390512168407
        model: {}
        policy_loss: -0.032361023128032684
        total_loss: -0.031282879412174225
        vf_explained_var: 0.10665230453014374
        vf_loss: 13.797818183898926
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00045728639815934
        entropy: 1.0197522640228271
        entropy_coeff: 0.0017600000137463212
        kl: 0.01785963587462902
        model: {}
        policy_loss: -0.039174094796180725
        total_loss: -0.03782786801457405
        vf_explained_var: 0.12348546087741852
        vf_loss: 13.550272941589355
    load_time_ms: 14054.144
    num_steps_sampled: 12960000
    num_steps_trained: 12960000
    sample_time_ms: 92615.711
    update_time_ms: 22.131
  iterations_since_restore: 115
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.56483516483516
    ram_util_percent: 13.940109890109893
  pid: 4061
  policy_reward_max:
    agent-0: 150.83333333333354
    agent-1: 150.83333333333354
    agent-2: 150.83333333333354
    agent-3: 150.83333333333354
    agent-4: 150.83333333333354
    agent-5: 150.83333333333354
  policy_reward_mean:
    agent-0: 100.51500000000027
    agent-1: 100.51500000000027
    agent-2: 100.51500000000027
    agent-3: 100.51500000000027
    agent-4: 100.51500000000027
    agent-5: 100.51500000000027
  policy_reward_min:
    agent-0: 45.16666666666654
    agent-1: 45.16666666666654
    agent-2: 45.16666666666654
    agent-3: 45.16666666666654
    agent-4: 45.16666666666654
    agent-5: 45.16666666666654
  sampler_perf:
    mean_env_wait_ms: 23.536109077117004
    mean_inference_ms: 12.304751716732213
    mean_processing_ms: 50.82309767512741
  time_since_restore: 15244.268217802048
  time_this_iter_s: 127.65174555778503
  time_total_s: 18455.33190393448
  timestamp: 1637032770
  timesteps_since_restore: 11040000
  timesteps_this_iter: 96000
  timesteps_total: 12960000
  training_iteration: 135
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    135 |          18455.3 | 12960000 |   603.09 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 62
    apples_agent-0_mean: 5.77
    apples_agent-0_min: 0
    apples_agent-1_max: 63
    apples_agent-1_mean: 19.05
    apples_agent-1_min: 0
    apples_agent-2_max: 80
    apples_agent-2_mean: 9.77
    apples_agent-2_min: 0
    apples_agent-3_max: 136
    apples_agent-3_mean: 72.27
    apples_agent-3_min: 17
    apples_agent-4_max: 42
    apples_agent-4_mean: 1.95
    apples_agent-4_min: 0
    apples_agent-5_max: 182
    apples_agent-5_mean: 73.99
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 456
    cleaning_beam_agent-0_mean: 284.73
    cleaning_beam_agent-0_min: 190
    cleaning_beam_agent-1_max: 373
    cleaning_beam_agent-1_mean: 205.28
    cleaning_beam_agent-1_min: 105
    cleaning_beam_agent-2_max: 396
    cleaning_beam_agent-2_mean: 237.88
    cleaning_beam_agent-2_min: 85
    cleaning_beam_agent-3_max: 172
    cleaning_beam_agent-3_mean: 46.43
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 479
    cleaning_beam_agent-4_mean: 329.23
    cleaning_beam_agent-4_min: 128
    cleaning_beam_agent-5_max: 765
    cleaning_beam_agent-5_mean: 149.5
    cleaning_beam_agent-5_min: 25
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-21-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 796.9999999999889
  episode_reward_mean: 586.0300000000003
  episode_reward_min: 247.99999999999855
  episodes_this_iter: 96
  episodes_total: 13056
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20087.896
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000451295985840261
        entropy: 1.1540735960006714
        entropy_coeff: 0.0017600000137463212
        kl: 0.013853703625500202
        model: {}
        policy_loss: -0.03433867543935776
        total_loss: -0.03354203701019287
        vf_explained_var: 0.06314726173877716
        vf_loss: 14.424403190612793
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000451295985840261
        entropy: 1.123128890991211
        entropy_coeff: 0.0017600000137463212
        kl: 0.01506613940000534
        model: {}
        policy_loss: -0.03364037349820137
        total_loss: -0.03259557485580444
        vf_explained_var: 0.016482427716255188
        vf_loss: 15.148893356323242
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.000451295985840261
        entropy: 1.0934809446334839
        entropy_coeff: 0.0017600000137463212
        kl: 0.01464869361370802
        model: {}
        policy_loss: -0.0344022661447525
        total_loss: -0.03264998272061348
        vf_explained_var: 0.03854738175868988
        vf_loss: 14.795047760009766
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000451295985840261
        entropy: 0.6466588377952576
        entropy_coeff: 0.0017600000137463212
        kl: 0.009662482887506485
        model: {}
        policy_loss: -0.02267753705382347
        total_loss: -0.021669596433639526
        vf_explained_var: 0.23335525393486023
        vf_loss: 11.79813003540039
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000451295985840261
        entropy: 0.9933629631996155
        entropy_coeff: 0.0017600000137463212
        kl: 0.015053988434374332
        model: {}
        policy_loss: -0.031473271548748016
        total_loss: -0.030363954603672028
        vf_explained_var: 0.12069933116436005
        vf_loss: 13.522394180297852
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000451295985840261
        entropy: 1.004160761833191
        entropy_coeff: 0.0017600000137463212
        kl: 0.016066521406173706
        model: {}
        policy_loss: -0.03851925581693649
        total_loss: -0.03723868727684021
        vf_explained_var: 0.06362175941467285
        vf_loss: 14.412400245666504
    load_time_ms: 14000.254
    num_steps_sampled: 13056000
    num_steps_trained: 13056000
    sample_time_ms: 92850.912
    update_time_ms: 21.582
  iterations_since_restore: 116
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.81508379888268
    ram_util_percent: 13.815642458100559
  pid: 4061
  policy_reward_max:
    agent-0: 132.8333333333336
    agent-1: 132.8333333333336
    agent-2: 132.8333333333336
    agent-3: 132.8333333333336
    agent-4: 132.8333333333336
    agent-5: 132.8333333333336
  policy_reward_mean:
    agent-0: 97.67166666666695
    agent-1: 97.67166666666695
    agent-2: 97.67166666666695
    agent-3: 97.67166666666695
    agent-4: 97.67166666666695
    agent-5: 97.67166666666695
  policy_reward_min:
    agent-0: 41.33333333333335
    agent-1: 41.33333333333335
    agent-2: 41.33333333333335
    agent-3: 41.33333333333335
    agent-4: 41.33333333333335
    agent-5: 41.33333333333335
  sampler_perf:
    mean_env_wait_ms: 23.540902595143844
    mean_inference_ms: 12.305935579630004
    mean_processing_ms: 50.831758000854386
  time_since_restore: 15370.063434123993
  time_this_iter_s: 125.79521632194519
  time_total_s: 18581.127120256424
  timestamp: 1637032896
  timesteps_since_restore: 11136000
  timesteps_this_iter: 96000
  timesteps_total: 13056000
  training_iteration: 136
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    136 |          18581.1 | 13056000 |   586.03 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 46
    apples_agent-0_mean: 4.46
    apples_agent-0_min: 0
    apples_agent-1_max: 76
    apples_agent-1_mean: 20.33
    apples_agent-1_min: 0
    apples_agent-2_max: 120
    apples_agent-2_mean: 7.77
    apples_agent-2_min: 0
    apples_agent-3_max: 143
    apples_agent-3_mean: 75.63
    apples_agent-3_min: 32
    apples_agent-4_max: 89
    apples_agent-4_mean: 4.23
    apples_agent-4_min: 0
    apples_agent-5_max: 159
    apples_agent-5_mean: 82.74
    apples_agent-5_min: 8
    cleaning_beam_agent-0_max: 465
    cleaning_beam_agent-0_mean: 302.96
    cleaning_beam_agent-0_min: 186
    cleaning_beam_agent-1_max: 328
    cleaning_beam_agent-1_mean: 204.02
    cleaning_beam_agent-1_min: 98
    cleaning_beam_agent-2_max: 394
    cleaning_beam_agent-2_mean: 241.05
    cleaning_beam_agent-2_min: 102
    cleaning_beam_agent-3_max: 113
    cleaning_beam_agent-3_mean: 38.83
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 663
    cleaning_beam_agent-4_mean: 358.75
    cleaning_beam_agent-4_min: 120
    cleaning_beam_agent-5_max: 567
    cleaning_beam_agent-5_mean: 110.41
    cleaning_beam_agent-5_min: 14
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-23-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 881.9999999999721
  episode_reward_mean: 614.1699999999979
  episode_reward_min: 258.9999999999956
  episodes_this_iter: 96
  episodes_total: 13152
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20095.935
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0004453056026250124
        entropy: 1.1533647775650024
        entropy_coeff: 0.0017600000137463212
        kl: 0.012862118892371655
        model: {}
        policy_loss: -0.03284330293536186
        total_loss: -0.03193877637386322
        vf_explained_var: 0.05113197863101959
        vf_loss: 16.48233413696289
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0004453056026250124
        entropy: 1.1095081567764282
        entropy_coeff: 0.0017600000137463212
        kl: 0.015156742185354233
        model: {}
        policy_loss: -0.03343015909194946
        total_loss: -0.032240353524684906
        vf_explained_var: 0.0637747198343277
        vf_loss: 16.268630981445312
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.0004453056026250124
        entropy: 1.0969009399414062
        entropy_coeff: 0.0017600000137463212
        kl: 0.01397052500396967
        model: {}
        policy_loss: -0.03254806622862816
        total_loss: -0.030738255009055138
        vf_explained_var: 0.05373114347457886
        vf_loss: 16.447776794433594
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0004453056026250124
        entropy: 0.6479145288467407
        entropy_coeff: 0.0017600000137463212
        kl: 0.00986170582473278
        model: {}
        policy_loss: -0.022379597648978233
        total_loss: -0.02122540771961212
        vf_explained_var: 0.24699057638645172
        vf_loss: 13.08349609375
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0004453056026250124
        entropy: 0.9906078577041626
        entropy_coeff: 0.0017600000137463212
        kl: 0.014563722535967827
        model: {}
        policy_loss: -0.033229172229766846
        total_loss: -0.03199145942926407
        vf_explained_var: 0.12248441576957703
        vf_loss: 15.248096466064453
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0004453056026250124
        entropy: 1.0155044794082642
        entropy_coeff: 0.0017600000137463212
        kl: 0.016212016344070435
        model: {}
        policy_loss: -0.03964491933584213
        total_loss: -0.038329675793647766
        vf_explained_var: 0.14813806116580963
        vf_loss: 14.81330680847168
    load_time_ms: 13954.308
    num_steps_sampled: 13152000
    num_steps_trained: 13152000
    sample_time_ms: 93260.365
    update_time_ms: 20.848
  iterations_since_restore: 117
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.734444444444442
    ram_util_percent: 13.946111111111112
  pid: 4061
  policy_reward_max:
    agent-0: 146.99999999999986
    agent-1: 146.99999999999986
    agent-2: 146.99999999999986
    agent-3: 146.99999999999986
    agent-4: 146.99999999999986
    agent-5: 146.99999999999986
  policy_reward_mean:
    agent-0: 102.36166666666693
    agent-1: 102.36166666666693
    agent-2: 102.36166666666693
    agent-3: 102.36166666666693
    agent-4: 102.36166666666693
    agent-5: 102.36166666666693
  policy_reward_min:
    agent-0: 43.1666666666666
    agent-1: 43.1666666666666
    agent-2: 43.1666666666666
    agent-3: 43.1666666666666
    agent-4: 43.1666666666666
    agent-5: 43.1666666666666
  sampler_perf:
    mean_env_wait_ms: 23.54709362438345
    mean_inference_ms: 12.308399741584058
    mean_processing_ms: 50.84037397910786
  time_since_restore: 15496.91197347641
  time_this_iter_s: 126.84853935241699
  time_total_s: 18707.97565960884
  timestamp: 1637033023
  timesteps_since_restore: 11232000
  timesteps_this_iter: 96000
  timesteps_total: 13152000
  training_iteration: 137
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    137 |            18708 | 13152000 |   614.17 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 51
    apples_agent-0_mean: 5.97
    apples_agent-0_min: 0
    apples_agent-1_max: 84
    apples_agent-1_mean: 19.0
    apples_agent-1_min: 0
    apples_agent-2_max: 176
    apples_agent-2_mean: 10.1
    apples_agent-2_min: 0
    apples_agent-3_max: 143
    apples_agent-3_mean: 72.7
    apples_agent-3_min: 27
    apples_agent-4_max: 63
    apples_agent-4_mean: 1.59
    apples_agent-4_min: 0
    apples_agent-5_max: 150
    apples_agent-5_mean: 81.13
    apples_agent-5_min: 8
    cleaning_beam_agent-0_max: 474
    cleaning_beam_agent-0_mean: 305.12
    cleaning_beam_agent-0_min: 177
    cleaning_beam_agent-1_max: 347
    cleaning_beam_agent-1_mean: 208.8
    cleaning_beam_agent-1_min: 94
    cleaning_beam_agent-2_max: 423
    cleaning_beam_agent-2_mean: 251.62
    cleaning_beam_agent-2_min: 57
    cleaning_beam_agent-3_max: 154
    cleaning_beam_agent-3_mean: 43.96
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 582
    cleaning_beam_agent-4_mean: 350.22
    cleaning_beam_agent-4_min: 210
    cleaning_beam_agent-5_max: 562
    cleaning_beam_agent-5_mean: 117.88
    cleaning_beam_agent-5_min: 15
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 7
    fire_beam_agent-5_mean: 0.12
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-25-49
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 883.9999999999797
  episode_reward_mean: 633.6399999999963
  episode_reward_min: 242.9999999999937
  episodes_this_iter: 96
  episodes_total: 13248
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20102.753
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00043931519030593336
        entropy: 1.1281471252441406
        entropy_coeff: 0.0017600000137463212
        kl: 0.013853230513632298
        model: {}
        policy_loss: -0.031716879457235336
        total_loss: -0.03075023740530014
        vf_explained_var: 0.107913538813591
        vf_loss: 15.668572425842285
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00043931519030593336
        entropy: 1.1196225881576538
        entropy_coeff: 0.0017600000137463212
        kl: 0.014888424426317215
        model: {}
        policy_loss: -0.032737962901592255
        total_loss: -0.031586386263370514
        vf_explained_var: 0.07009047269821167
        vf_loss: 16.33280372619629
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.00043931519030593336
        entropy: 1.1050655841827393
        entropy_coeff: 0.0017600000137463212
        kl: 0.013192255049943924
        model: {}
        policy_loss: -0.0333559513092041
        total_loss: -0.0317588746547699
        vf_explained_var: 0.10969503223896027
        vf_loss: 15.631486892700195
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00043931519030593336
        entropy: 0.6265318393707275
        entropy_coeff: 0.0017600000137463212
        kl: 0.009323781356215477
        model: {}
        policy_loss: -0.0206268522888422
        total_loss: -0.01943306252360344
        vf_explained_var: 0.2237747311592102
        vf_loss: 13.641092300415039
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00043931519030593336
        entropy: 0.9850582480430603
        entropy_coeff: 0.0017600000137463212
        kl: 0.013629862107336521
        model: {}
        policy_loss: -0.03142673522233963
        total_loss: -0.030161786824464798
        vf_explained_var: 0.06877061724662781
        vf_loss: 16.356691360473633
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00043931519030593336
        entropy: 1.0001230239868164
        entropy_coeff: 0.0017600000137463212
        kl: 0.01661793142557144
        model: {}
        policy_loss: -0.039154112339019775
        total_loss: -0.03775941580533981
        vf_explained_var: 0.1503990739583969
        vf_loss: 14.93118667602539
    load_time_ms: 13938.429
    num_steps_sampled: 13248000
    num_steps_trained: 13248000
    sample_time_ms: 93140.561
    update_time_ms: 21.269
  iterations_since_restore: 118
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.647486033519552
    ram_util_percent: 13.865921787709498
  pid: 4061
  policy_reward_max:
    agent-0: 147.33333333333312
    agent-1: 147.33333333333312
    agent-2: 147.33333333333312
    agent-3: 147.33333333333312
    agent-4: 147.33333333333312
    agent-5: 147.33333333333312
  policy_reward_mean:
    agent-0: 105.60666666666701
    agent-1: 105.60666666666701
    agent-2: 105.60666666666701
    agent-3: 105.60666666666701
    agent-4: 105.60666666666701
    agent-5: 105.60666666666701
  policy_reward_min:
    agent-0: 40.499999999999986
    agent-1: 40.499999999999986
    agent-2: 40.499999999999986
    agent-3: 40.499999999999986
    agent-4: 40.499999999999986
    agent-5: 40.499999999999986
  sampler_perf:
    mean_env_wait_ms: 23.55071331014962
    mean_inference_ms: 12.308412879814274
    mean_processing_ms: 50.84038996280304
  time_since_restore: 15622.109291553497
  time_this_iter_s: 125.1973180770874
  time_total_s: 18833.17297768593
  timestamp: 1637033149
  timesteps_since_restore: 11328000
  timesteps_this_iter: 96000
  timesteps_total: 13248000
  training_iteration: 138
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    138 |          18833.2 | 13248000 |   633.64 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 53
    apples_agent-0_mean: 6.12
    apples_agent-0_min: 0
    apples_agent-1_max: 89
    apples_agent-1_mean: 22.16
    apples_agent-1_min: 0
    apples_agent-2_max: 74
    apples_agent-2_mean: 7.09
    apples_agent-2_min: 0
    apples_agent-3_max: 140
    apples_agent-3_mean: 76.89
    apples_agent-3_min: 25
    apples_agent-4_max: 70
    apples_agent-4_mean: 1.92
    apples_agent-4_min: 0
    apples_agent-5_max: 175
    apples_agent-5_mean: 79.91
    apples_agent-5_min: 6
    cleaning_beam_agent-0_max: 469
    cleaning_beam_agent-0_mean: 315.33
    cleaning_beam_agent-0_min: 180
    cleaning_beam_agent-1_max: 363
    cleaning_beam_agent-1_mean: 215.17
    cleaning_beam_agent-1_min: 94
    cleaning_beam_agent-2_max: 447
    cleaning_beam_agent-2_mean: 248.53
    cleaning_beam_agent-2_min: 91
    cleaning_beam_agent-3_max: 140
    cleaning_beam_agent-3_mean: 45.04
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 592
    cleaning_beam_agent-4_mean: 342.93
    cleaning_beam_agent-4_min: 180
    cleaning_beam_agent-5_max: 608
    cleaning_beam_agent-5_mean: 115.27
    cleaning_beam_agent-5_min: 17
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 5
    fire_beam_agent-5_mean: 0.12
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-27-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 903.9999999999719
  episode_reward_mean: 640.5899999999966
  episode_reward_min: 231.99999999999665
  episodes_this_iter: 96
  episodes_total: 13344
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20120.2
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00043332480709068477
        entropy: 1.1330926418304443
        entropy_coeff: 0.0017600000137463212
        kl: 0.012981485575437546
        model: {}
        policy_loss: -0.03189963474869728
        total_loss: -0.03103126585483551
        vf_explained_var: 0.05192086100578308
        vf_loss: 15.64460563659668
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00043332480709068477
        entropy: 1.1107516288757324
        entropy_coeff: 0.0017600000137463212
        kl: 0.016045475378632545
        model: {}
        policy_loss: -0.03311002999544144
        total_loss: -0.03186749666929245
        vf_explained_var: 0.0355541855096817
        vf_loss: 15.929108619689941
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.00043332480709068477
        entropy: 1.1292884349822998
        entropy_coeff: 0.0017600000137463212
        kl: 0.013852129690349102
        model: {}
        policy_loss: -0.032374486327171326
        total_loss: -0.0306866355240345
        vf_explained_var: 0.03128385543823242
        vf_loss: 15.975797653198242
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00043332480709068477
        entropy: 0.6235799789428711
        entropy_coeff: 0.0017600000137463212
        kl: 0.009439068846404552
        model: {}
        policy_loss: -0.021035311743617058
        total_loss: -0.019906312227249146
        vf_explained_var: 0.22265557944774628
        vf_loss: 12.825956344604492
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00043332480709068477
        entropy: 0.9830856323242188
        entropy_coeff: 0.0017600000137463212
        kl: 0.01411272119730711
        model: {}
        policy_loss: -0.03255569934844971
        total_loss: -0.031439270824193954
        vf_explained_var: 0.12979763746261597
        vf_loss: 14.3538818359375
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00043332480709068477
        entropy: 0.9964489936828613
        entropy_coeff: 0.0017600000137463212
        kl: 0.015346400439739227
        model: {}
        policy_loss: -0.037446245551109314
        total_loss: -0.03614028915762901
        vf_explained_var: 0.07464388012886047
        vf_loss: 15.250715255737305
    load_time_ms: 13938.131
    num_steps_sampled: 13344000
    num_steps_trained: 13344000
    sample_time_ms: 92650.327
    update_time_ms: 21.573
  iterations_since_restore: 119
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.87159090909091
    ram_util_percent: 13.967613636363637
  pid: 4061
  policy_reward_max:
    agent-0: 150.666666666667
    agent-1: 150.666666666667
    agent-2: 150.666666666667
    agent-3: 150.666666666667
    agent-4: 150.666666666667
    agent-5: 150.666666666667
  policy_reward_mean:
    agent-0: 106.76500000000034
    agent-1: 106.76500000000034
    agent-2: 106.76500000000034
    agent-3: 106.76500000000034
    agent-4: 106.76500000000034
    agent-5: 106.76500000000034
  policy_reward_min:
    agent-0: 38.66666666666664
    agent-1: 38.66666666666664
    agent-2: 38.66666666666664
    agent-3: 38.66666666666664
    agent-4: 38.66666666666664
    agent-5: 38.66666666666664
  sampler_perf:
    mean_env_wait_ms: 23.554097369131807
    mean_inference_ms: 12.308572116300141
    mean_processing_ms: 50.84262293981043
  time_since_restore: 15745.888179302216
  time_this_iter_s: 123.77888774871826
  time_total_s: 18956.951865434647
  timestamp: 1637033273
  timesteps_since_restore: 11424000
  timesteps_this_iter: 96000
  timesteps_total: 13344000
  training_iteration: 139
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    139 |            18957 | 13344000 |   640.59 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 48
    apples_agent-0_mean: 4.06
    apples_agent-0_min: 0
    apples_agent-1_max: 88
    apples_agent-1_mean: 21.19
    apples_agent-1_min: 0
    apples_agent-2_max: 89
    apples_agent-2_mean: 10.0
    apples_agent-2_min: 0
    apples_agent-3_max: 162
    apples_agent-3_mean: 76.71
    apples_agent-3_min: 22
    apples_agent-4_max: 51
    apples_agent-4_mean: 2.17
    apples_agent-4_min: 0
    apples_agent-5_max: 159
    apples_agent-5_mean: 84.32
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 433
    cleaning_beam_agent-0_mean: 324.77
    cleaning_beam_agent-0_min: 213
    cleaning_beam_agent-1_max: 388
    cleaning_beam_agent-1_mean: 223.87
    cleaning_beam_agent-1_min: 120
    cleaning_beam_agent-2_max: 400
    cleaning_beam_agent-2_mean: 233.55
    cleaning_beam_agent-2_min: 60
    cleaning_beam_agent-3_max: 138
    cleaning_beam_agent-3_mean: 46.76
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 571
    cleaning_beam_agent-4_mean: 369.44
    cleaning_beam_agent-4_min: 179
    cleaning_beam_agent-5_max: 734
    cleaning_beam_agent-5_mean: 119.57
    cleaning_beam_agent-5_min: 26
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 5
    fire_beam_agent-5_mean: 0.07
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-29-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 864.9999999999845
  episode_reward_mean: 650.0799999999961
  episode_reward_min: 328.0000000000003
  episodes_this_iter: 96
  episodes_total: 13440
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20140.448
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00042733439477160573
        entropy: 1.1384949684143066
        entropy_coeff: 0.0017600000137463212
        kl: 0.01308056153357029
        model: {}
        policy_loss: -0.03120320662856102
        total_loss: -0.030388031154870987
        vf_explained_var: 0.040468379855155945
        vf_loss: 15.108678817749023
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00042733439477160573
        entropy: 1.111166000366211
        entropy_coeff: 0.0017600000137463212
        kl: 0.016203679144382477
        model: {}
        policy_loss: -0.035296354442834854
        total_loss: -0.03409760445356369
        vf_explained_var: 0.025601372122764587
        vf_loss: 15.34034538269043
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.00042733439477160573
        entropy: 1.1412875652313232
        entropy_coeff: 0.0017600000137463212
        kl: 0.014213377609848976
        model: {}
        policy_loss: -0.03164472430944443
        total_loss: -0.030113529413938522
        vf_explained_var: 0.10564860701560974
        vf_loss: 14.078559875488281
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00042733439477160573
        entropy: 0.615090012550354
        entropy_coeff: 0.0017600000137463212
        kl: 0.009507933631539345
        model: {}
        policy_loss: -0.020726267248392105
        total_loss: -0.01964426413178444
        vf_explained_var: 0.22811393439769745
        vf_loss: 12.137683868408203
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00042733439477160573
        entropy: 0.9666335582733154
        entropy_coeff: 0.0017600000137463212
        kl: 0.013397306203842163
        model: {}
        policy_loss: -0.031151864677667618
        total_loss: -0.030070293694734573
        vf_explained_var: 0.08214524388313293
        vf_loss: 14.43120288848877
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00042733439477160573
        entropy: 1.0100799798965454
        entropy_coeff: 0.0017600000137463212
        kl: 0.015930909663438797
        model: {}
        policy_loss: -0.03936255723237991
        total_loss: -0.038155697286129
        vf_explained_var: 0.11668293178081512
        vf_loss: 13.915098190307617
    load_time_ms: 13949.855
    num_steps_sampled: 13440000
    num_steps_trained: 13440000
    sample_time_ms: 92365.707
    update_time_ms: 21.917
  iterations_since_restore: 120
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.78022598870057
    ram_util_percent: 13.966101694915256
  pid: 4061
  policy_reward_max:
    agent-0: 144.16666666666688
    agent-1: 144.16666666666688
    agent-2: 144.16666666666688
    agent-3: 144.16666666666688
    agent-4: 144.16666666666688
    agent-5: 144.16666666666688
  policy_reward_mean:
    agent-0: 108.34666666666706
    agent-1: 108.34666666666706
    agent-2: 108.34666666666706
    agent-3: 108.34666666666706
    agent-4: 108.34666666666706
    agent-5: 108.34666666666706
  policy_reward_min:
    agent-0: 54.666666666666536
    agent-1: 54.666666666666536
    agent-2: 54.666666666666536
    agent-3: 54.666666666666536
    agent-4: 54.666666666666536
    agent-5: 54.666666666666536
  sampler_perf:
    mean_env_wait_ms: 23.558282190748642
    mean_inference_ms: 12.308603482787019
    mean_processing_ms: 50.841929672575354
  time_since_restore: 15869.869837999344
  time_this_iter_s: 123.9816586971283
  time_total_s: 19080.933524131775
  timestamp: 1637033397
  timesteps_since_restore: 11520000
  timesteps_this_iter: 96000
  timesteps_total: 13440000
  training_iteration: 140
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    140 |          19080.9 | 13440000 |   650.08 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 43
    apples_agent-0_mean: 4.47
    apples_agent-0_min: 0
    apples_agent-1_max: 74
    apples_agent-1_mean: 21.5
    apples_agent-1_min: 0
    apples_agent-2_max: 65
    apples_agent-2_mean: 10.53
    apples_agent-2_min: 0
    apples_agent-3_max: 147
    apples_agent-3_mean: 76.6
    apples_agent-3_min: 27
    apples_agent-4_max: 30
    apples_agent-4_mean: 1.02
    apples_agent-4_min: 0
    apples_agent-5_max: 147
    apples_agent-5_mean: 81.55
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 458
    cleaning_beam_agent-0_mean: 315.66
    cleaning_beam_agent-0_min: 198
    cleaning_beam_agent-1_max: 331
    cleaning_beam_agent-1_mean: 220.57
    cleaning_beam_agent-1_min: 120
    cleaning_beam_agent-2_max: 427
    cleaning_beam_agent-2_mean: 234.79
    cleaning_beam_agent-2_min: 60
    cleaning_beam_agent-3_max: 168
    cleaning_beam_agent-3_mean: 48.64
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 621
    cleaning_beam_agent-4_mean: 363.27
    cleaning_beam_agent-4_min: 192
    cleaning_beam_agent-5_max: 790
    cleaning_beam_agent-5_mean: 122.66
    cleaning_beam_agent-5_min: 15
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-32-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 869.9999999999906
  episode_reward_mean: 647.8299999999957
  episode_reward_min: 377.0000000000019
  episodes_this_iter: 96
  episodes_total: 13536
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20143.208
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00042134401155635715
        entropy: 1.1512093544006348
        entropy_coeff: 0.0017600000137463212
        kl: 0.012535715475678444
        model: {}
        policy_loss: -0.031642355024814606
        total_loss: -0.030870776623487473
        vf_explained_var: 0.03840853273868561
        vf_loss: 15.4413480758667
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00042134401155635715
        entropy: 1.1032819747924805
        entropy_coeff: 0.0017600000137463212
        kl: 0.015270674601197243
        model: {}
        policy_loss: -0.03381691500544548
        total_loss: -0.03265943378210068
        vf_explained_var: 0.021457821130752563
        vf_loss: 15.721875190734863
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.00042134401155635715
        entropy: 1.126330852508545
        entropy_coeff: 0.0017600000137463212
        kl: 0.014092821627855301
        model: {}
        policy_loss: -0.033404745161533356
        total_loss: -0.031841814517974854
        vf_explained_var: 0.10930807888507843
        vf_loss: 14.313481330871582
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00042134401155635715
        entropy: 0.6094238758087158
        entropy_coeff: 0.0017600000137463212
        kl: 0.009574036113917828
        model: {}
        policy_loss: -0.02164299786090851
        total_loss: -0.020521679893136024
        vf_explained_var: 0.2299642562866211
        vf_loss: 12.365002632141113
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00042134401155635715
        entropy: 0.9739379286766052
        entropy_coeff: 0.0017600000137463212
        kl: 0.013000824488699436
        model: {}
        policy_loss: -0.031106535345315933
        total_loss: -0.030066559091210365
        vf_explained_var: 0.0942506492137909
        vf_loss: 14.540226936340332
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00042134401155635715
        entropy: 0.9942013621330261
        entropy_coeff: 0.0017600000137463212
        kl: 0.01599925197660923
        model: {}
        policy_loss: -0.037651192396879196
        total_loss: -0.03634945675730705
        vf_explained_var: 0.09644126892089844
        vf_loss: 14.516033172607422
    load_time_ms: 13959.47
    num_steps_sampled: 13536000
    num_steps_trained: 13536000
    sample_time_ms: 91954.834
    update_time_ms: 21.35
  iterations_since_restore: 121
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.692737430167597
    ram_util_percent: 13.953072625698326
  pid: 4061
  policy_reward_max:
    agent-0: 145.00000000000045
    agent-1: 145.00000000000045
    agent-2: 145.00000000000045
    agent-3: 145.00000000000045
    agent-4: 145.00000000000045
    agent-5: 145.00000000000045
  policy_reward_mean:
    agent-0: 107.97166666666706
    agent-1: 107.97166666666706
    agent-2: 107.97166666666706
    agent-3: 107.97166666666706
    agent-4: 107.97166666666706
    agent-5: 107.97166666666706
  policy_reward_min:
    agent-0: 62.83333333333316
    agent-1: 62.83333333333316
    agent-2: 62.83333333333316
    agent-3: 62.83333333333316
    agent-4: 62.83333333333316
    agent-5: 62.83333333333316
  sampler_perf:
    mean_env_wait_ms: 23.56402293265197
    mean_inference_ms: 12.309292339257459
    mean_processing_ms: 50.846269300914955
  time_since_restore: 15995.303178548813
  time_this_iter_s: 125.433340549469
  time_total_s: 19206.366864681244
  timestamp: 1637033523
  timesteps_since_restore: 11616000
  timesteps_this_iter: 96000
  timesteps_total: 13536000
  training_iteration: 141
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    141 |          19206.4 | 13536000 |   647.83 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 31
    apples_agent-0_mean: 3.31
    apples_agent-0_min: 0
    apples_agent-1_max: 79
    apples_agent-1_mean: 20.25
    apples_agent-1_min: 0
    apples_agent-2_max: 88
    apples_agent-2_mean: 10.62
    apples_agent-2_min: 0
    apples_agent-3_max: 126
    apples_agent-3_mean: 78.69
    apples_agent-3_min: 25
    apples_agent-4_max: 88
    apples_agent-4_mean: 3.8
    apples_agent-4_min: 0
    apples_agent-5_max: 166
    apples_agent-5_mean: 83.6
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 469
    cleaning_beam_agent-0_mean: 335.65
    cleaning_beam_agent-0_min: 180
    cleaning_beam_agent-1_max: 401
    cleaning_beam_agent-1_mean: 212.04
    cleaning_beam_agent-1_min: 116
    cleaning_beam_agent-2_max: 395
    cleaning_beam_agent-2_mean: 223.51
    cleaning_beam_agent-2_min: 79
    cleaning_beam_agent-3_max: 183
    cleaning_beam_agent-3_mean: 45.67
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 621
    cleaning_beam_agent-4_mean: 370.64
    cleaning_beam_agent-4_min: 177
    cleaning_beam_agent-5_max: 937
    cleaning_beam_agent-5_mean: 124.18
    cleaning_beam_agent-5_min: 18
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-34-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 904.9999999999786
  episode_reward_mean: 655.6799999999955
  episode_reward_min: 194.99999999999784
  episodes_this_iter: 96
  episodes_total: 13632
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20147.839
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0004153535992372781
        entropy: 1.1110560894012451
        entropy_coeff: 0.0017600000137463212
        kl: 0.01204279437661171
        model: {}
        policy_loss: -0.030877869576215744
        total_loss: -0.030084870755672455
        vf_explained_var: 0.07573431730270386
        vf_loss: 15.441737174987793
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0004153535992372781
        entropy: 1.1073358058929443
        entropy_coeff: 0.0017600000137463212
        kl: 0.01461862027645111
        model: {}
        policy_loss: -0.03434240072965622
        total_loss: -0.033193543553352356
        vf_explained_var: 0.020581603050231934
        vf_loss: 16.35906219482422
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.0004153535992372781
        entropy: 1.1276862621307373
        entropy_coeff: 0.0017600000137463212
        kl: 0.013167805969715118
        model: {}
        policy_loss: -0.03212343156337738
        total_loss: -0.030552498996257782
        vf_explained_var: 0.05440057814121246
        vf_loss: 15.804901123046875
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0004153535992372781
        entropy: 0.6312316656112671
        entropy_coeff: 0.0017600000137463212
        kl: 0.009365512058138847
        model: {}
        policy_loss: -0.022218026220798492
        total_loss: -0.021092336624860764
        vf_explained_var: 0.2214415967464447
        vf_loss: 13.001036643981934
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0004153535992372781
        entropy: 0.9723255038261414
        entropy_coeff: 0.0017600000137463212
        kl: 0.013228485360741615
        model: {}
        policy_loss: -0.03234980255365372
        total_loss: -0.031287454068660736
        vf_explained_var: 0.13172179460525513
        vf_loss: 14.507967948913574
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0004153535992372781
        entropy: 0.9847726225852966
        entropy_coeff: 0.0017600000137463212
        kl: 0.01570175401866436
        model: {}
        policy_loss: -0.037730686366558075
        total_loss: -0.03641568869352341
        vf_explained_var: 0.11586463451385498
        vf_loss: 14.78016471862793
    load_time_ms: 13990.728
    num_steps_sampled: 13632000
    num_steps_trained: 13632000
    sample_time_ms: 91772.892
    update_time_ms: 21.882
  iterations_since_restore: 122
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.414044943820222
    ram_util_percent: 13.736516853932587
  pid: 4061
  policy_reward_max:
    agent-0: 150.8333333333336
    agent-1: 150.8333333333336
    agent-2: 150.8333333333336
    agent-3: 150.8333333333336
    agent-4: 150.8333333333336
    agent-5: 150.8333333333336
  policy_reward_mean:
    agent-0: 109.28000000000037
    agent-1: 109.28000000000037
    agent-2: 109.28000000000037
    agent-3: 109.28000000000037
    agent-4: 109.28000000000037
    agent-5: 109.28000000000037
  policy_reward_min:
    agent-0: 32.50000000000005
    agent-1: 32.50000000000005
    agent-2: 32.50000000000005
    agent-3: 32.50000000000005
    agent-4: 32.50000000000005
    agent-5: 32.50000000000005
  sampler_perf:
    mean_env_wait_ms: 23.569647587202095
    mean_inference_ms: 12.31008208794314
    mean_processing_ms: 50.849319400127925
  time_since_restore: 16120.323830604553
  time_this_iter_s: 125.02065205574036
  time_total_s: 19331.387516736984
  timestamp: 1637033648
  timesteps_since_restore: 11712000
  timesteps_this_iter: 96000
  timesteps_total: 13632000
  training_iteration: 142
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    142 |          19331.4 | 13632000 |   655.68 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 46
    apples_agent-0_mean: 3.9
    apples_agent-0_min: 0
    apples_agent-1_max: 101
    apples_agent-1_mean: 23.06
    apples_agent-1_min: 0
    apples_agent-2_max: 81
    apples_agent-2_mean: 11.52
    apples_agent-2_min: 0
    apples_agent-3_max: 158
    apples_agent-3_mean: 78.4
    apples_agent-3_min: 19
    apples_agent-4_max: 56
    apples_agent-4_mean: 1.2
    apples_agent-4_min: 0
    apples_agent-5_max: 203
    apples_agent-5_mean: 88.12
    apples_agent-5_min: 22
    cleaning_beam_agent-0_max: 477
    cleaning_beam_agent-0_mean: 340.2
    cleaning_beam_agent-0_min: 183
    cleaning_beam_agent-1_max: 349
    cleaning_beam_agent-1_mean: 206.45
    cleaning_beam_agent-1_min: 92
    cleaning_beam_agent-2_max: 418
    cleaning_beam_agent-2_mean: 220.53
    cleaning_beam_agent-2_min: 51
    cleaning_beam_agent-3_max: 91
    cleaning_beam_agent-3_mean: 43.87
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 572
    cleaning_beam_agent-4_mean: 375.43
    cleaning_beam_agent-4_min: 190
    cleaning_beam_agent-5_max: 475
    cleaning_beam_agent-5_mean: 90.83
    cleaning_beam_agent-5_min: 18
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-36-11
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 850.9999999999798
  episode_reward_mean: 656.2299999999942
  episode_reward_min: 238.99999999999665
  episodes_this_iter: 96
  episodes_total: 13728
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20148.754
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00040936318691819906
        entropy: 1.130782127380371
        entropy_coeff: 0.0017600000137463212
        kl: 0.01231022272258997
        model: {}
        policy_loss: -0.03081037849187851
        total_loss: -0.029926858842372894
        vf_explained_var: 0.047937363386154175
        vf_loss: 16.426769256591797
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00040936318691819906
        entropy: 1.1041282415390015
        entropy_coeff: 0.0017600000137463212
        kl: 0.014465877786278725
        model: {}
        policy_loss: -0.03334137052297592
        total_loss: -0.03215249627828598
        vf_explained_var: 0.02363736927509308
        vf_loss: 16.855484008789062
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.00040936318691819906
        entropy: 1.1291793584823608
        entropy_coeff: 0.0017600000137463212
        kl: 0.014277871698141098
        model: {}
        policy_loss: -0.03275873139500618
        total_loss: -0.031076304614543915
        vf_explained_var: 0.1147288978099823
        vf_loss: 15.280991554260254
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00040936318691819906
        entropy: 0.6432353854179382
        entropy_coeff: 0.0017600000137463212
        kl: 0.010265886783599854
        model: {}
        policy_loss: -0.022009680047631264
        total_loss: -0.020769191905856133
        vf_explained_var: 0.21976639330387115
        vf_loss: 13.459932327270508
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00040936318691819906
        entropy: 0.9743146300315857
        entropy_coeff: 0.0017600000137463212
        kl: 0.013494919054210186
        model: {}
        policy_loss: -0.03152154013514519
        total_loss: -0.030394427478313446
        vf_explained_var: 0.13447195291519165
        vf_loss: 14.924142837524414
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00040936318691819906
        entropy: 0.9859812259674072
        entropy_coeff: 0.0017600000137463212
        kl: 0.01528155617415905
        model: {}
        policy_loss: -0.03699995204806328
        total_loss: -0.03570418804883957
        vf_explained_var: 0.12909431755542755
        vf_loss: 15.029324531555176
    load_time_ms: 13989.677
    num_steps_sampled: 13728000
    num_steps_trained: 13728000
    sample_time_ms: 91216.381
    update_time_ms: 21.534
  iterations_since_restore: 123
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.269318181818182
    ram_util_percent: 9.671590909090908
  pid: 4061
  policy_reward_max:
    agent-0: 141.83333333333346
    agent-1: 141.83333333333346
    agent-2: 141.83333333333346
    agent-3: 141.83333333333346
    agent-4: 141.83333333333346
    agent-5: 141.83333333333346
  policy_reward_mean:
    agent-0: 109.37166666666704
    agent-1: 109.37166666666704
    agent-2: 109.37166666666704
    agent-3: 109.37166666666704
    agent-4: 109.37166666666704
    agent-5: 109.37166666666704
  policy_reward_min:
    agent-0: 39.833333333333286
    agent-1: 39.833333333333286
    agent-2: 39.833333333333286
    agent-3: 39.833333333333286
    agent-4: 39.833333333333286
    agent-5: 39.833333333333286
  sampler_perf:
    mean_env_wait_ms: 23.569666812303186
    mean_inference_ms: 12.308792552174078
    mean_processing_ms: 50.842586988782834
  time_since_restore: 16243.50714969635
  time_this_iter_s: 123.18331909179688
  time_total_s: 19454.57083582878
  timestamp: 1637033771
  timesteps_since_restore: 11808000
  timesteps_this_iter: 96000
  timesteps_total: 13728000
  training_iteration: 143
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    143 |          19454.6 | 13728000 |   656.23 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 61
    apples_agent-0_mean: 3.83
    apples_agent-0_min: 0
    apples_agent-1_max: 73
    apples_agent-1_mean: 20.83
    apples_agent-1_min: 0
    apples_agent-2_max: 297
    apples_agent-2_mean: 13.79
    apples_agent-2_min: 0
    apples_agent-3_max: 167
    apples_agent-3_mean: 81.05
    apples_agent-3_min: 31
    apples_agent-4_max: 74
    apples_agent-4_mean: 2.44
    apples_agent-4_min: 0
    apples_agent-5_max: 145
    apples_agent-5_mean: 83.27
    apples_agent-5_min: 2
    cleaning_beam_agent-0_max: 482
    cleaning_beam_agent-0_mean: 334.89
    cleaning_beam_agent-0_min: 154
    cleaning_beam_agent-1_max: 356
    cleaning_beam_agent-1_mean: 214.74
    cleaning_beam_agent-1_min: 99
    cleaning_beam_agent-2_max: 452
    cleaning_beam_agent-2_mean: 224.67
    cleaning_beam_agent-2_min: 63
    cleaning_beam_agent-3_max: 167
    cleaning_beam_agent-3_mean: 46.26
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 539
    cleaning_beam_agent-4_mean: 355.42
    cleaning_beam_agent-4_min: 157
    cleaning_beam_agent-5_max: 607
    cleaning_beam_agent-5_mean: 103.22
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-38-15
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 830.9999999999983
  episode_reward_mean: 642.8099999999961
  episode_reward_min: 305.99999999999847
  episodes_this_iter: 96
  episodes_total: 13824
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20136.654
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0004033728037029505
        entropy: 1.1332124471664429
        entropy_coeff: 0.0017600000137463212
        kl: 0.012929125688970089
        model: {}
        policy_loss: -0.030807269737124443
        total_loss: -0.029946638271212578
        vf_explained_var: 0.04131212830543518
        vf_loss: 15.621771812438965
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0004033728037029505
        entropy: 1.0958704948425293
        entropy_coeff: 0.0017600000137463212
        kl: 0.014404227025806904
        model: {}
        policy_loss: -0.03224816545844078
        total_loss: -0.031172925606369972
        vf_explained_var: 0.04009951651096344
        vf_loss: 15.635488510131836
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.0004033728037029505
        entropy: 1.128178358078003
        entropy_coeff: 0.0017600000137463212
        kl: 0.013327649794518948
        model: {}
        policy_loss: -0.030984532088041306
        total_loss: -0.029485538601875305
        vf_explained_var: 0.08830204606056213
        vf_loss: 14.854358673095703
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0004033728037029505
        entropy: 0.6459895372390747
        entropy_coeff: 0.0017600000137463212
        kl: 0.009363931603729725
        model: {}
        policy_loss: -0.020665692165493965
        total_loss: -0.019544323906302452
        vf_explained_var: 0.18813209235668182
        vf_loss: 13.219179153442383
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0004033728037029505
        entropy: 0.9871714115142822
        entropy_coeff: 0.0017600000137463212
        kl: 0.01356908492743969
        model: {}
        policy_loss: -0.0319967120885849
        total_loss: -0.03092232346534729
        vf_explained_var: 0.10596618056297302
        vf_loss: 14.548965454101562
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0004033728037029505
        entropy: 0.983241617679596
        entropy_coeff: 0.0017600000137463212
        kl: 0.016021210700273514
        model: {}
        policy_loss: -0.03572787716984749
        total_loss: -0.03437770903110504
        vf_explained_var: 0.09242002665996552
        vf_loss: 14.785579681396484
    load_time_ms: 14007.73
    num_steps_sampled: 13824000
    num_steps_trained: 13824000
    sample_time_ms: 90783.225
    update_time_ms: 21.266
  iterations_since_restore: 124
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.326704545454545
    ram_util_percent: 9.594886363636364
  pid: 4061
  policy_reward_max:
    agent-0: 138.5
    agent-1: 138.5
    agent-2: 138.5
    agent-3: 138.5
    agent-4: 138.5
    agent-5: 138.5
  policy_reward_mean:
    agent-0: 107.13500000000033
    agent-1: 107.13500000000033
    agent-2: 107.13500000000033
    agent-3: 107.13500000000033
    agent-4: 107.13500000000033
    agent-5: 107.13500000000033
  policy_reward_min:
    agent-0: 50.99999999999991
    agent-1: 50.99999999999991
    agent-2: 50.99999999999991
    agent-3: 50.99999999999991
    agent-4: 50.99999999999991
    agent-5: 50.99999999999991
  sampler_perf:
    mean_env_wait_ms: 23.569661567976812
    mean_inference_ms: 12.307490896141871
    mean_processing_ms: 50.83905378940367
  time_since_restore: 16367.026614189148
  time_this_iter_s: 123.51946449279785
  time_total_s: 19578.09030032158
  timestamp: 1637033895
  timesteps_since_restore: 11904000
  timesteps_this_iter: 96000
  timesteps_total: 13824000
  training_iteration: 144
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    144 |          19578.1 | 13824000 |   642.81 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 40
    apples_agent-0_mean: 4.5
    apples_agent-0_min: 0
    apples_agent-1_max: 69
    apples_agent-1_mean: 21.56
    apples_agent-1_min: 0
    apples_agent-2_max: 51
    apples_agent-2_mean: 6.69
    apples_agent-2_min: 0
    apples_agent-3_max: 138
    apples_agent-3_mean: 77.34
    apples_agent-3_min: 34
    apples_agent-4_max: 67
    apples_agent-4_mean: 1.61
    apples_agent-4_min: 0
    apples_agent-5_max: 421
    apples_agent-5_mean: 87.2
    apples_agent-5_min: 7
    cleaning_beam_agent-0_max: 455
    cleaning_beam_agent-0_mean: 325.02
    cleaning_beam_agent-0_min: 160
    cleaning_beam_agent-1_max: 383
    cleaning_beam_agent-1_mean: 202.87
    cleaning_beam_agent-1_min: 84
    cleaning_beam_agent-2_max: 452
    cleaning_beam_agent-2_mean: 237.76
    cleaning_beam_agent-2_min: 102
    cleaning_beam_agent-3_max: 87
    cleaning_beam_agent-3_mean: 38.69
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 550
    cleaning_beam_agent-4_mean: 359.96
    cleaning_beam_agent-4_min: 186
    cleaning_beam_agent-5_max: 679
    cleaning_beam_agent-5_mean: 113.23
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-40-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 904.9999999999881
  episode_reward_mean: 666.3799999999928
  episode_reward_min: 256.99999999999534
  episodes_this_iter: 96
  episodes_total: 13920
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20119.663
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00039738239138387144
        entropy: 1.136793851852417
        entropy_coeff: 0.0017600000137463212
        kl: 0.012954146601259708
        model: {}
        policy_loss: -0.03106522187590599
        total_loss: -0.030127812176942825
        vf_explained_var: 0.03677155077457428
        vf_loss: 16.42748260498047
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00039738239138387144
        entropy: 1.1178654432296753
        entropy_coeff: 0.0017600000137463212
        kl: 0.014233473688364029
        model: {}
        policy_loss: -0.033124327659606934
        total_loss: -0.03208816424012184
        vf_explained_var: 0.07287739217281342
        vf_loss: 15.802614212036133
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.00039738239138387144
        entropy: 1.1363294124603271
        entropy_coeff: 0.0017600000137463212
        kl: 0.012804838828742504
        model: {}
        policy_loss: -0.030968964099884033
        total_loss: -0.02943491004407406
        vf_explained_var: 0.05431060492992401
        vf_loss: 16.13272476196289
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00039738239138387144
        entropy: 0.6128539443016052
        entropy_coeff: 0.0017600000137463212
        kl: 0.008930700831115246
        model: {}
        policy_loss: -0.02005377970635891
        total_loss: -0.018875211477279663
        vf_explained_var: 0.2003725916147232
        vf_loss: 13.641206741333008
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00039738239138387144
        entropy: 0.9838207364082336
        entropy_coeff: 0.0017600000137463212
        kl: 0.013503015041351318
        model: {}
        policy_loss: -0.031682293862104416
        total_loss: -0.030486997216939926
        vf_explained_var: 0.07506939768791199
        vf_loss: 15.765203475952148
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00039738239138387144
        entropy: 0.9542533159255981
        entropy_coeff: 0.0017600000137463212
        kl: 0.014933944679796696
        model: {}
        policy_loss: -0.034575626254081726
        total_loss: -0.033277589827775955
        vf_explained_var: 0.12991730868816376
        vf_loss: 14.841299057006836
    load_time_ms: 14127.377
    num_steps_sampled: 13920000
    num_steps_trained: 13920000
    sample_time_ms: 90264.463
    update_time_ms: 22.239
  iterations_since_restore: 125
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 14.804591836734692
    ram_util_percent: 9.719897959183672
  pid: 4061
  policy_reward_max:
    agent-0: 150.83333333333326
    agent-1: 150.83333333333326
    agent-2: 150.83333333333326
    agent-3: 150.83333333333326
    agent-4: 150.83333333333326
    agent-5: 150.83333333333326
  policy_reward_mean:
    agent-0: 111.06333333333372
    agent-1: 111.06333333333372
    agent-2: 111.06333333333372
    agent-3: 111.06333333333372
    agent-4: 111.06333333333372
    agent-5: 111.06333333333372
  policy_reward_min:
    agent-0: 42.83333333333325
    agent-1: 42.83333333333325
    agent-2: 42.83333333333325
    agent-3: 42.83333333333325
    agent-4: 42.83333333333325
    agent-5: 42.83333333333325
  sampler_perf:
    mean_env_wait_ms: 23.570606085546
    mean_inference_ms: 12.306726720332126
    mean_processing_ms: 50.835761675864106
  time_since_restore: 16490.533729076385
  time_this_iter_s: 123.50711488723755
  time_total_s: 19701.597415208817
  timestamp: 1637034032
  timesteps_since_restore: 12000000
  timesteps_this_iter: 96000
  timesteps_total: 13920000
  training_iteration: 145
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    145 |          19701.6 | 13920000 |   666.38 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 48
    apples_agent-0_mean: 3.89
    apples_agent-0_min: 0
    apples_agent-1_max: 75
    apples_agent-1_mean: 20.56
    apples_agent-1_min: 0
    apples_agent-2_max: 90
    apples_agent-2_mean: 10.39
    apples_agent-2_min: 0
    apples_agent-3_max: 178
    apples_agent-3_mean: 77.7
    apples_agent-3_min: 34
    apples_agent-4_max: 67
    apples_agent-4_mean: 2.21
    apples_agent-4_min: 0
    apples_agent-5_max: 149
    apples_agent-5_mean: 79.02
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 481
    cleaning_beam_agent-0_mean: 320.98
    cleaning_beam_agent-0_min: 141
    cleaning_beam_agent-1_max: 327
    cleaning_beam_agent-1_mean: 205.57
    cleaning_beam_agent-1_min: 74
    cleaning_beam_agent-2_max: 399
    cleaning_beam_agent-2_mean: 224.52
    cleaning_beam_agent-2_min: 64
    cleaning_beam_agent-3_max: 107
    cleaning_beam_agent-3_mean: 43.56
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 548
    cleaning_beam_agent-4_mean: 366.55
    cleaning_beam_agent-4_min: 156
    cleaning_beam_agent-5_max: 665
    cleaning_beam_agent-5_mean: 119.19
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-42-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 911.9999999999854
  episode_reward_mean: 652.5999999999951
  episode_reward_min: 254.999999999997
  episodes_this_iter: 96
  episodes_total: 14016
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20127.333
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00039139200816862285
        entropy: 1.1341451406478882
        entropy_coeff: 0.0017600000137463212
        kl: 0.01323443278670311
        model: {}
        policy_loss: -0.032347358763217926
        total_loss: -0.03134146332740784
        vf_explained_var: 0.054690852761268616
        vf_loss: 16.78546714782715
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00039139200816862285
        entropy: 1.115943431854248
        entropy_coeff: 0.0017600000137463212
        kl: 0.014671441167593002
        model: {}
        policy_loss: -0.033959127962589264
        total_loss: -0.032722555100917816
        vf_explained_var: 0.02543795108795166
        vf_loss: 17.334911346435547
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.00039139200816862285
        entropy: 1.1366524696350098
        entropy_coeff: 0.0017600000137463212
        kl: 0.012902295216917992
        model: {}
        policy_loss: -0.031768880784511566
        total_loss: -0.030173711478710175
        vf_explained_var: 0.06535884737968445
        vf_loss: 16.603363037109375
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00039139200816862285
        entropy: 0.6296879649162292
        entropy_coeff: 0.0017600000137463212
        kl: 0.009193656966090202
        model: {}
        policy_loss: -0.02026527374982834
        total_loss: -0.01903538964688778
        vf_explained_var: 0.20091795921325684
        vf_loss: 14.18769359588623
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00039139200816862285
        entropy: 0.9676656723022461
        entropy_coeff: 0.0017600000137463212
        kl: 0.014212723821401596
        model: {}
        policy_loss: -0.03200036287307739
        total_loss: -0.030670445412397385
        vf_explained_var: 0.09225177764892578
        vf_loss: 16.117374420166016
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00039139200816862285
        entropy: 0.9503362774848938
        entropy_coeff: 0.0017600000137463212
        kl: 0.015450277365744114
        model: {}
        policy_loss: -0.03602982684969902
        total_loss: -0.03462354093790054
        vf_explained_var: 0.13726167380809784
        vf_loss: 15.338476181030273
    load_time_ms: 14263.282
    num_steps_sampled: 14016000
    num_steps_trained: 14016000
    sample_time_ms: 90254.909
    update_time_ms: 22.364
  iterations_since_restore: 126
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.160773480662986
    ram_util_percent: 13.304972375690609
  pid: 4061
  policy_reward_max:
    agent-0: 151.99999999999997
    agent-1: 151.99999999999997
    agent-2: 151.99999999999997
    agent-3: 151.99999999999997
    agent-4: 151.99999999999997
    agent-5: 151.99999999999997
  policy_reward_mean:
    agent-0: 108.766666666667
    agent-1: 108.766666666667
    agent-2: 108.766666666667
    agent-3: 108.766666666667
    agent-4: 108.766666666667
    agent-5: 108.766666666667
  policy_reward_min:
    agent-0: 42.49999999999994
    agent-1: 42.49999999999994
    agent-2: 42.49999999999994
    agent-3: 42.49999999999994
    agent-4: 42.49999999999994
    agent-5: 42.49999999999994
  sampler_perf:
    mean_env_wait_ms: 23.573629329784072
    mean_inference_ms: 12.307515402935586
    mean_processing_ms: 50.83744264577042
  time_since_restore: 16617.677420139313
  time_this_iter_s: 127.14369106292725
  time_total_s: 19828.741106271744
  timestamp: 1637034160
  timesteps_since_restore: 12096000
  timesteps_this_iter: 96000
  timesteps_total: 14016000
  training_iteration: 146
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 26.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    146 |          19828.7 | 14016000 |    652.6 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 58
    apples_agent-0_mean: 5.96
    apples_agent-0_min: 0
    apples_agent-1_max: 93
    apples_agent-1_mean: 20.23
    apples_agent-1_min: 0
    apples_agent-2_max: 224
    apples_agent-2_mean: 10.83
    apples_agent-2_min: 0
    apples_agent-3_max: 141
    apples_agent-3_mean: 77.7
    apples_agent-3_min: 39
    apples_agent-4_max: 35
    apples_agent-4_mean: 0.65
    apples_agent-4_min: 0
    apples_agent-5_max: 190
    apples_agent-5_mean: 87.63
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 607
    cleaning_beam_agent-0_mean: 328.22
    cleaning_beam_agent-0_min: 172
    cleaning_beam_agent-1_max: 341
    cleaning_beam_agent-1_mean: 213.27
    cleaning_beam_agent-1_min: 117
    cleaning_beam_agent-2_max: 422
    cleaning_beam_agent-2_mean: 225.09
    cleaning_beam_agent-2_min: 49
    cleaning_beam_agent-3_max: 120
    cleaning_beam_agent-3_mean: 42.25
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 548
    cleaning_beam_agent-4_mean: 355.84
    cleaning_beam_agent-4_min: 167
    cleaning_beam_agent-5_max: 801
    cleaning_beam_agent-5_mean: 114.73
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-44-50
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 898.9999999999924
  episode_reward_mean: 657.9099999999945
  episode_reward_min: 284.9999999999988
  episodes_this_iter: 96
  episodes_total: 14112
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20141.529
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0003854015958495438
        entropy: 1.1341321468353271
        entropy_coeff: 0.0017600000137463212
        kl: 0.01298863161355257
        model: {}
        policy_loss: -0.030240727588534355
        total_loss: -0.029345430433750153
        vf_explained_var: 0.06788046658039093
        vf_loss: 15.92507553100586
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0003854015958495438
        entropy: 1.1205097436904907
        entropy_coeff: 0.0017600000137463212
        kl: 0.013541007414460182
        model: {}
        policy_loss: -0.03295125067234039
        total_loss: -0.03188180550932884
        vf_explained_var: 0.012258470058441162
        vf_loss: 16.87446403503418
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.0003854015958495438
        entropy: 1.1184706687927246
        entropy_coeff: 0.0017600000137463212
        kl: 0.012228494510054588
        model: {}
        policy_loss: -0.03127849102020264
        total_loss: -0.029792606830596924
        vf_explained_var: 0.05184642970561981
        vf_loss: 16.201183319091797
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0003854015958495438
        entropy: 0.6248000264167786
        entropy_coeff: 0.0017600000137463212
        kl: 0.0091691380366683
        model: {}
        policy_loss: -0.020664013922214508
        total_loss: -0.019499339163303375
        vf_explained_var: 0.2111743539571762
        vf_loss: 13.474084854125977
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0003854015958495438
        entropy: 0.9799548387527466
        entropy_coeff: 0.0017600000137463212
        kl: 0.013379273004829884
        model: {}
        policy_loss: -0.031519465148448944
        total_loss: -0.03031136281788349
        vf_explained_var: 0.065791055560112
        vf_loss: 15.94897174835205
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0003854015958495438
        entropy: 0.9420656561851501
        entropy_coeff: 0.0017600000137463212
        kl: 0.014008057303726673
        model: {}
        policy_loss: -0.03468270227313042
        total_loss: -0.03346242755651474
        vf_explained_var: 0.135664701461792
        vf_loss: 14.775019645690918
    load_time_ms: 14690.825
    num_steps_sampled: 14112000
    num_steps_trained: 14112000
    sample_time_ms: 90145.058
    update_time_ms: 28.296
  iterations_since_restore: 127
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.315591397849463
    ram_util_percent: 14.425806451612905
  pid: 4061
  policy_reward_max:
    agent-0: 149.83333333333343
    agent-1: 149.83333333333343
    agent-2: 149.83333333333343
    agent-3: 149.83333333333343
    agent-4: 149.83333333333343
    agent-5: 149.83333333333343
  policy_reward_mean:
    agent-0: 109.65166666666701
    agent-1: 109.65166666666701
    agent-2: 109.65166666666701
    agent-3: 109.65166666666701
    agent-4: 109.65166666666701
    agent-5: 109.65166666666701
  policy_reward_min:
    agent-0: 47.499999999999865
    agent-1: 47.499999999999865
    agent-2: 47.499999999999865
    agent-3: 47.499999999999865
    agent-4: 47.499999999999865
    agent-5: 47.499999999999865
  sampler_perf:
    mean_env_wait_ms: 23.57833223504497
    mean_inference_ms: 12.309898126771497
    mean_processing_ms: 50.84229630924976
  time_since_restore: 16747.906119585037
  time_this_iter_s: 130.2286994457245
  time_total_s: 19958.96980571747
  timestamp: 1637034290
  timesteps_since_restore: 12192000
  timesteps_this_iter: 96000
  timesteps_total: 14112000
  training_iteration: 147
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 26.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    147 |            19959 | 14112000 |   657.91 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 52
    apples_agent-0_mean: 3.19
    apples_agent-0_min: 0
    apples_agent-1_max: 71
    apples_agent-1_mean: 19.73
    apples_agent-1_min: 0
    apples_agent-2_max: 61
    apples_agent-2_mean: 9.8
    apples_agent-2_min: 0
    apples_agent-3_max: 140
    apples_agent-3_mean: 71.99
    apples_agent-3_min: 16
    apples_agent-4_max: 139
    apples_agent-4_mean: 2.36
    apples_agent-4_min: 0
    apples_agent-5_max: 171
    apples_agent-5_mean: 85.58
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 470
    cleaning_beam_agent-0_mean: 317.36
    cleaning_beam_agent-0_min: 153
    cleaning_beam_agent-1_max: 371
    cleaning_beam_agent-1_mean: 215.79
    cleaning_beam_agent-1_min: 101
    cleaning_beam_agent-2_max: 383
    cleaning_beam_agent-2_mean: 217.55
    cleaning_beam_agent-2_min: 70
    cleaning_beam_agent-3_max: 189
    cleaning_beam_agent-3_mean: 44.49
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 560
    cleaning_beam_agent-4_mean: 355.61
    cleaning_beam_agent-4_min: 199
    cleaning_beam_agent-5_max: 847
    cleaning_beam_agent-5_mean: 104.48
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-47-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 878.9999999999827
  episode_reward_mean: 663.9199999999943
  episode_reward_min: 199.99999999999767
  episodes_this_iter: 96
  episodes_total: 14208
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20138.406
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0003794112126342952
        entropy: 1.131381630897522
        entropy_coeff: 0.0017600000137463212
        kl: 0.0125941913574934
        model: {}
        policy_loss: -0.03025279939174652
        total_loss: -0.029521692544221878
        vf_explained_var: 0.06606502830982208
        vf_loss: 14.629213333129883
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0003794112126342952
        entropy: 1.1166189908981323
        entropy_coeff: 0.0017600000137463212
        kl: 0.014367067255079746
        model: {}
        policy_loss: -0.03457437455654144
        total_loss: -0.033651161938905716
        vf_explained_var: 0.07347150146961212
        vf_loss: 14.517583847045898
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.0003794112126342952
        entropy: 1.149844765663147
        entropy_coeff: 0.0017600000137463212
        kl: 0.012393997050821781
        model: {}
        policy_loss: -0.031086791306734085
        total_loss: -0.02982865832746029
        vf_explained_var: 0.09173455834388733
        vf_loss: 14.227615356445312
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0003794112126342952
        entropy: 0.619026243686676
        entropy_coeff: 0.0017600000137463212
        kl: 0.009129300713539124
        model: {}
        policy_loss: -0.021483706310391426
        total_loss: -0.020418427884578705
        vf_explained_var: 0.20771928131580353
        vf_loss: 12.418327331542969
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0003794112126342952
        entropy: 0.9851375818252563
        entropy_coeff: 0.0017600000137463212
        kl: 0.012938691303133965
        model: {}
        policy_loss: -0.030967246741056442
        total_loss: -0.029942551627755165
        vf_explained_var: 0.06480343639850616
        vf_loss: 14.646645545959473
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0003794112126342952
        entropy: 0.961223840713501
        entropy_coeff: 0.0017600000137463212
        kl: 0.01487964577972889
        model: {}
        policy_loss: -0.03491475433111191
        total_loss: -0.03371856361627579
        vf_explained_var: 0.10691635310649872
        vf_loss: 13.999849319458008
    load_time_ms: 15196.311
    num_steps_sampled: 14208000
    num_steps_trained: 14208000
    sample_time_ms: 91239.976
    update_time_ms: 28.054
  iterations_since_restore: 128
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.218811881188117
    ram_util_percent: 14.410396039603963
  pid: 4061
  policy_reward_max:
    agent-0: 146.50000000000017
    agent-1: 146.50000000000017
    agent-2: 146.50000000000017
    agent-3: 146.50000000000017
    agent-4: 146.50000000000017
    agent-5: 146.50000000000017
  policy_reward_mean:
    agent-0: 110.6533333333337
    agent-1: 110.6533333333337
    agent-2: 110.6533333333337
    agent-3: 110.6533333333337
    agent-4: 110.6533333333337
    agent-5: 110.6533333333337
  policy_reward_min:
    agent-0: 33.33333333333341
    agent-1: 33.33333333333341
    agent-2: 33.33333333333341
    agent-3: 33.33333333333341
    agent-4: 33.33333333333341
    agent-5: 33.33333333333341
  sampler_perf:
    mean_env_wait_ms: 23.580341189308328
    mean_inference_ms: 12.311037126836903
    mean_processing_ms: 50.861286500774916
  time_since_restore: 16889.0705909729
  time_this_iter_s: 141.16447138786316
  time_total_s: 20100.13427710533
  timestamp: 1637034432
  timesteps_since_restore: 12288000
  timesteps_this_iter: 96000
  timesteps_total: 14208000
  training_iteration: 148
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 26.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    148 |          20100.1 | 14208000 |   663.92 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 62
    apples_agent-0_mean: 5.21
    apples_agent-0_min: 0
    apples_agent-1_max: 68
    apples_agent-1_mean: 18.54
    apples_agent-1_min: 0
    apples_agent-2_max: 111
    apples_agent-2_mean: 8.95
    apples_agent-2_min: 0
    apples_agent-3_max: 132
    apples_agent-3_mean: 75.3
    apples_agent-3_min: 27
    apples_agent-4_max: 38
    apples_agent-4_mean: 1.27
    apples_agent-4_min: 0
    apples_agent-5_max: 183
    apples_agent-5_mean: 86.11
    apples_agent-5_min: 23
    cleaning_beam_agent-0_max: 456
    cleaning_beam_agent-0_mean: 305.23
    cleaning_beam_agent-0_min: 153
    cleaning_beam_agent-1_max: 345
    cleaning_beam_agent-1_mean: 211.03
    cleaning_beam_agent-1_min: 98
    cleaning_beam_agent-2_max: 397
    cleaning_beam_agent-2_mean: 217.42
    cleaning_beam_agent-2_min: 81
    cleaning_beam_agent-3_max: 141
    cleaning_beam_agent-3_mean: 41.84
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 474
    cleaning_beam_agent-4_mean: 367.35
    cleaning_beam_agent-4_min: 208
    cleaning_beam_agent-5_max: 617
    cleaning_beam_agent-5_mean: 105.16
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-49-27
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 887.9999999999851
  episode_reward_mean: 662.2599999999936
  episode_reward_min: 303.0000000000001
  episodes_this_iter: 96
  episodes_total: 14304
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20116.363
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0003734208003152162
        entropy: 1.141920566558838
        entropy_coeff: 0.0017600000137463212
        kl: 0.012683536857366562
        model: {}
        policy_loss: -0.03109973669052124
        total_loss: -0.030236126855015755
        vf_explained_var: 0.09287816286087036
        vf_loss: 16.050395965576172
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0003734208003152162
        entropy: 1.1250923871994019
        entropy_coeff: 0.0017600000137463212
        kl: 0.01460045576095581
        model: {}
        policy_loss: -0.03357952833175659
        total_loss: -0.032381322234869
        vf_explained_var: 0.029056549072265625
        vf_loss: 17.183269500732422
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.0003734208003152162
        entropy: 1.1435205936431885
        entropy_coeff: 0.0017600000137463212
        kl: 0.012720267288386822
        model: {}
        policy_loss: -0.03086964413523674
        total_loss: -0.029376395046710968
        vf_explained_var: 0.09695278108119965
        vf_loss: 15.978059768676758
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0003734208003152162
        entropy: 0.6206446886062622
        entropy_coeff: 0.0017600000137463212
        kl: 0.009181976318359375
        model: {}
        policy_loss: -0.021663716062903404
        total_loss: -0.020514465868473053
        vf_explained_var: 0.25155508518218994
        vf_loss: 13.233901023864746
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0003734208003152162
        entropy: 0.9778784513473511
        entropy_coeff: 0.0017600000137463212
        kl: 0.012623487040400505
        model: {}
        policy_loss: -0.0313631072640419
        total_loss: -0.030177954584360123
        vf_explained_var: 0.0709025114774704
        vf_loss: 16.43870735168457
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0003734208003152162
        entropy: 0.9535024166107178
        entropy_coeff: 0.0017600000137463212
        kl: 0.014180488884449005
        model: {}
        policy_loss: -0.035464175045490265
        total_loss: -0.03424864634871483
        vf_explained_var: 0.16594523191452026
        vf_loss: 14.756508827209473
    load_time_ms: 16073.663
    num_steps_sampled: 14304000
    num_steps_trained: 14304000
    sample_time_ms: 91481.526
    update_time_ms: 27.626
  iterations_since_restore: 129
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.941666666666666
    ram_util_percent: 14.498437500000001
  pid: 4061
  policy_reward_max:
    agent-0: 148.00000000000014
    agent-1: 148.00000000000014
    agent-2: 148.00000000000014
    agent-3: 148.00000000000014
    agent-4: 148.00000000000014
    agent-5: 148.00000000000014
  policy_reward_mean:
    agent-0: 110.37666666666702
    agent-1: 110.37666666666702
    agent-2: 110.37666666666702
    agent-3: 110.37666666666702
    agent-4: 110.37666666666702
    agent-5: 110.37666666666702
  policy_reward_min:
    agent-0: 50.49999999999997
    agent-1: 50.49999999999997
    agent-2: 50.49999999999997
    agent-3: 50.49999999999997
    agent-4: 50.49999999999997
    agent-5: 50.49999999999997
  sampler_perf:
    mean_env_wait_ms: 23.585415165246367
    mean_inference_ms: 12.312022929621216
    mean_processing_ms: 50.86800935148263
  time_since_restore: 17023.812114715576
  time_this_iter_s: 134.74152374267578
  time_total_s: 20234.875800848007
  timestamp: 1637034567
  timesteps_since_restore: 12384000
  timesteps_this_iter: 96000
  timesteps_total: 14304000
  training_iteration: 149
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 26.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    149 |          20234.9 | 14304000 |   662.26 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 52
    apples_agent-0_mean: 4.47
    apples_agent-0_min: 0
    apples_agent-1_max: 96
    apples_agent-1_mean: 21.33
    apples_agent-1_min: 0
    apples_agent-2_max: 100
    apples_agent-2_mean: 10.73
    apples_agent-2_min: 0
    apples_agent-3_max: 133
    apples_agent-3_mean: 77.33
    apples_agent-3_min: 23
    apples_agent-4_max: 46
    apples_agent-4_mean: 1.61
    apples_agent-4_min: 0
    apples_agent-5_max: 186
    apples_agent-5_mean: 78.35
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 484
    cleaning_beam_agent-0_mean: 322.13
    cleaning_beam_agent-0_min: 153
    cleaning_beam_agent-1_max: 345
    cleaning_beam_agent-1_mean: 204.07
    cleaning_beam_agent-1_min: 104
    cleaning_beam_agent-2_max: 415
    cleaning_beam_agent-2_mean: 218.64
    cleaning_beam_agent-2_min: 60
    cleaning_beam_agent-3_max: 105
    cleaning_beam_agent-3_mean: 44.21
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 526
    cleaning_beam_agent-4_mean: 360.09
    cleaning_beam_agent-4_min: 72
    cleaning_beam_agent-5_max: 713
    cleaning_beam_agent-5_mean: 136.46
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-51-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 925.9999999999712
  episode_reward_mean: 650.2399999999942
  episode_reward_min: 263.99999999999585
  episodes_this_iter: 96
  episodes_total: 14400
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20144.685
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00036743038799613714
        entropy: 1.116705298423767
        entropy_coeff: 0.0017600000137463212
        kl: 0.01320567075163126
        model: {}
        policy_loss: -0.03167405724525452
        total_loss: -0.030730899423360825
        vf_explained_var: 0.08970941603183746
        vf_loss: 15.879924774169922
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00036743038799613714
        entropy: 1.1323447227478027
        entropy_coeff: 0.0017600000137463212
        kl: 0.013843605294823647
        model: {}
        policy_loss: -0.03267423063516617
        total_loss: -0.03154812008142471
        vf_explained_var: 0.006849348545074463
        vf_loss: 17.346736907958984
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.00036743038799613714
        entropy: 1.1395882368087769
        entropy_coeff: 0.0017600000137463212
        kl: 0.01284851972013712
        model: {}
        policy_loss: -0.03139710798859596
        total_loss: -0.029818007722496986
        vf_explained_var: 0.04933375120162964
        vf_loss: 16.574989318847656
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00036743038799613714
        entropy: 0.6157003045082092
        entropy_coeff: 0.0017600000137463212
        kl: 0.009357376024127007
        model: {}
        policy_loss: -0.02089405246078968
        total_loss: -0.01966814510524273
        vf_explained_var: 0.21118080615997314
        vf_loss: 13.738011360168457
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00036743038799613714
        entropy: 0.9816152453422546
        entropy_coeff: 0.0017600000137463212
        kl: 0.012983700260519981
        model: {}
        policy_loss: -0.030937407165765762
        total_loss: -0.029838621616363525
        vf_explained_var: 0.12298932671546936
        vf_loss: 15.280595779418945
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00036743038799613714
        entropy: 0.9493118524551392
        entropy_coeff: 0.0017600000137463212
        kl: 0.014676287770271301
        model: {}
        policy_loss: -0.03528266027569771
        total_loss: -0.034015413373708725
        vf_explained_var: 0.15689121186733246
        vf_loss: 14.70404052734375
    load_time_ms: 16074.822
    num_steps_sampled: 14400000
    num_steps_trained: 14400000
    sample_time_ms: 91600.46
    update_time_ms: 27.745
  iterations_since_restore: 130
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.62346368715084
    ram_util_percent: 14.288268156424584
  pid: 4061
  policy_reward_max:
    agent-0: 154.3333333333334
    agent-1: 154.3333333333334
    agent-2: 154.3333333333334
    agent-3: 154.3333333333334
    agent-4: 154.3333333333334
    agent-5: 154.3333333333334
  policy_reward_mean:
    agent-0: 108.37333333333369
    agent-1: 108.37333333333369
    agent-2: 108.37333333333369
    agent-3: 108.37333333333369
    agent-4: 108.37333333333369
    agent-5: 108.37333333333369
  policy_reward_min:
    agent-0: 43.99999999999994
    agent-1: 43.99999999999994
    agent-2: 43.99999999999994
    agent-3: 43.99999999999994
    agent-4: 43.99999999999994
    agent-5: 43.99999999999994
  sampler_perf:
    mean_env_wait_ms: 23.587683912351608
    mean_inference_ms: 12.312194925078424
    mean_processing_ms: 50.867360378228256
  time_since_restore: 17149.27470946312
  time_this_iter_s: 125.46259474754333
  time_total_s: 20360.33839559555
  timestamp: 1637034692
  timesteps_since_restore: 12480000
  timesteps_this_iter: 96000
  timesteps_total: 14400000
  training_iteration: 150
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 26.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    150 |          20360.3 | 14400000 |   650.24 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 52
    apples_agent-0_mean: 4.03
    apples_agent-0_min: 0
    apples_agent-1_max: 75
    apples_agent-1_mean: 19.09
    apples_agent-1_min: 0
    apples_agent-2_max: 82
    apples_agent-2_mean: 11.86
    apples_agent-2_min: 0
    apples_agent-3_max: 154
    apples_agent-3_mean: 73.93
    apples_agent-3_min: 32
    apples_agent-4_max: 73
    apples_agent-4_mean: 3.04
    apples_agent-4_min: 0
    apples_agent-5_max: 193
    apples_agent-5_mean: 79.12
    apples_agent-5_min: 2
    cleaning_beam_agent-0_max: 472
    cleaning_beam_agent-0_mean: 318.44
    cleaning_beam_agent-0_min: 168
    cleaning_beam_agent-1_max: 383
    cleaning_beam_agent-1_mean: 210.43
    cleaning_beam_agent-1_min: 103
    cleaning_beam_agent-2_max: 415
    cleaning_beam_agent-2_mean: 213.4
    cleaning_beam_agent-2_min: 70
    cleaning_beam_agent-3_max: 290
    cleaning_beam_agent-3_mean: 43.74
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 522
    cleaning_beam_agent-4_mean: 370.51
    cleaning_beam_agent-4_min: 94
    cleaning_beam_agent-5_max: 649
    cleaning_beam_agent-5_mean: 119.86
    cleaning_beam_agent-5_min: 12
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-53-39
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 905.9999999999721
  episode_reward_mean: 649.6099999999938
  episode_reward_min: 251.99999999999545
  episodes_this_iter: 96
  episodes_total: 14496
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20153.413
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00036144000478088856
        entropy: 1.128258228302002
        entropy_coeff: 0.0017600000137463212
        kl: 0.01217210479080677
        model: {}
        policy_loss: -0.03161792457103729
        total_loss: -0.030721215531229973
        vf_explained_var: 0.047972172498703
        vf_loss: 16.652347564697266
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00036144000478088856
        entropy: 1.112229585647583
        entropy_coeff: 0.0017600000137463212
        kl: 0.014009036123752594
        model: {}
        policy_loss: -0.033132992684841156
        total_loss: -0.03200659900903702
        vf_explained_var: 0.036393582820892334
        vf_loss: 16.83014678955078
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.00036144000478088856
        entropy: 1.1268712282180786
        entropy_coeff: 0.0017600000137463212
        kl: 0.012435533106327057
        model: {}
        policy_loss: -0.03144638240337372
        total_loss: -0.029913483187556267
        vf_explained_var: 0.05409857630729675
        vf_loss: 16.508615493774414
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00036144000478088856
        entropy: 0.6037826538085938
        entropy_coeff: 0.0017600000137463212
        kl: 0.00867006927728653
        model: {}
        policy_loss: -0.021175764501094818
        total_loss: -0.020043572410941124
        vf_explained_var: 0.23881103098392487
        vf_loss: 13.278449058532715
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00036144000478088856
        entropy: 0.9719457030296326
        entropy_coeff: 0.0017600000137463212
        kl: 0.013743983581662178
        model: {}
        policy_loss: -0.029542241245508194
        total_loss: -0.028356444090604782
        vf_explained_var: 0.12872260808944702
        vf_loss: 15.220196723937988
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00036144000478088856
        entropy: 0.9647490382194519
        entropy_coeff: 0.0017600000137463212
        kl: 0.01504565216600895
        model: {}
        policy_loss: -0.03482738882303238
        total_loss: -0.03350713849067688
        vf_explained_var: 0.13335925340652466
        vf_loss: 15.136454582214355
    load_time_ms: 16079.598
    num_steps_sampled: 14496000
    num_steps_trained: 14496000
    sample_time_ms: 91640.724
    update_time_ms: 29.468
  iterations_since_restore: 131
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.689999999999998
    ram_util_percent: 14.355000000000002
  pid: 4061
  policy_reward_max:
    agent-0: 150.99999999999977
    agent-1: 150.99999999999977
    agent-2: 150.99999999999977
    agent-3: 150.99999999999977
    agent-4: 150.99999999999977
    agent-5: 150.99999999999977
  policy_reward_mean:
    agent-0: 108.2683333333337
    agent-1: 108.2683333333337
    agent-2: 108.2683333333337
    agent-3: 108.2683333333337
    agent-4: 108.2683333333337
    agent-5: 108.2683333333337
  policy_reward_min:
    agent-0: 41.999999999999986
    agent-1: 41.999999999999986
    agent-2: 41.999999999999986
    agent-3: 41.999999999999986
    agent-4: 41.999999999999986
    agent-5: 41.999999999999986
  sampler_perf:
    mean_env_wait_ms: 23.592218156602545
    mean_inference_ms: 12.312975224060013
    mean_processing_ms: 50.869918798506276
  time_since_restore: 17275.267929553986
  time_this_iter_s: 125.99322009086609
  time_total_s: 20486.331615686417
  timestamp: 1637034819
  timesteps_since_restore: 12576000
  timesteps_this_iter: 96000
  timesteps_total: 14496000
  training_iteration: 151
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 26.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    151 |          20486.3 | 14496000 |   649.61 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 53
    apples_agent-0_mean: 4.75
    apples_agent-0_min: 0
    apples_agent-1_max: 73
    apples_agent-1_mean: 22.69
    apples_agent-1_min: 0
    apples_agent-2_max: 89
    apples_agent-2_mean: 9.85
    apples_agent-2_min: 0
    apples_agent-3_max: 171
    apples_agent-3_mean: 74.16
    apples_agent-3_min: 28
    apples_agent-4_max: 83
    apples_agent-4_mean: 1.82
    apples_agent-4_min: 0
    apples_agent-5_max: 160
    apples_agent-5_mean: 79.8
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 509
    cleaning_beam_agent-0_mean: 321.85
    cleaning_beam_agent-0_min: 156
    cleaning_beam_agent-1_max: 373
    cleaning_beam_agent-1_mean: 211.37
    cleaning_beam_agent-1_min: 120
    cleaning_beam_agent-2_max: 395
    cleaning_beam_agent-2_mean: 219.19
    cleaning_beam_agent-2_min: 80
    cleaning_beam_agent-3_max: 182
    cleaning_beam_agent-3_mean: 39.94
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 531
    cleaning_beam_agent-4_mean: 371.52
    cleaning_beam_agent-4_min: 159
    cleaning_beam_agent-5_max: 858
    cleaning_beam_agent-5_mean: 126.05
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-55-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 906.9999999999675
  episode_reward_mean: 672.0599999999935
  episode_reward_min: 207.9999999999972
  episodes_this_iter: 96
  episodes_total: 14592
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20165.581
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0003554495924618095
        entropy: 1.1249299049377441
        entropy_coeff: 0.0017600000137463212
        kl: 0.012533784843981266
        model: {}
        policy_loss: -0.030804608017206192
        total_loss: -0.029863329604268074
        vf_explained_var: 0.05834697186946869
        vf_loss: 16.677791595458984
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0003554495924618095
        entropy: 1.1185569763183594
        entropy_coeff: 0.0017600000137463212
        kl: 0.014432288706302643
        model: {}
        policy_loss: -0.03374907746911049
        total_loss: -0.03254186362028122
        vf_explained_var: 0.021894186735153198
        vf_loss: 17.326467514038086
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.0003554495924618095
        entropy: 1.1360974311828613
        entropy_coeff: 0.0017600000137463212
        kl: 0.012478777207434177
        model: {}
        policy_loss: -0.03110463172197342
        total_loss: -0.029657039791345596
        vf_explained_var: 0.11077329516410828
        vf_loss: 15.753047943115234
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0003554495924618095
        entropy: 0.5926960110664368
        entropy_coeff: 0.0017600000137463212
        kl: 0.00900499988347292
        model: {}
        policy_loss: -0.0214533694088459
        total_loss: -0.02025563083589077
        vf_explained_var: 0.24341650307178497
        vf_loss: 13.40385913848877
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0003554495924618095
        entropy: 0.9602914452552795
        entropy_coeff: 0.0017600000137463212
        kl: 0.012186329811811447
        model: {}
        policy_loss: -0.0300153661519289
        total_loss: -0.02893299236893654
        vf_explained_var: 0.12289723753929138
        vf_loss: 15.538555145263672
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0003554495924618095
        entropy: 0.9657533764839172
        entropy_coeff: 0.0017600000137463212
        kl: 0.014397491700947285
        model: {}
        policy_loss: -0.03435926511883736
        total_loss: -0.03310658782720566
        vf_explained_var: 0.14567109942436218
        vf_loss: 15.126482963562012
    load_time_ms: 16094.45
    num_steps_sampled: 14592000
    num_steps_trained: 14592000
    sample_time_ms: 91834.374
    update_time_ms: 29.335
  iterations_since_restore: 132
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.61657458563536
    ram_util_percent: 14.355248618784534
  pid: 4061
  policy_reward_max:
    agent-0: 151.16666666666652
    agent-1: 151.16666666666652
    agent-2: 151.16666666666652
    agent-3: 151.16666666666652
    agent-4: 151.16666666666652
    agent-5: 151.16666666666652
  policy_reward_mean:
    agent-0: 112.01000000000033
    agent-1: 112.01000000000033
    agent-2: 112.01000000000033
    agent-3: 112.01000000000033
    agent-4: 112.01000000000033
    agent-5: 112.01000000000033
  policy_reward_min:
    agent-0: 34.66666666666669
    agent-1: 34.66666666666669
    agent-2: 34.66666666666669
    agent-3: 34.66666666666669
    agent-4: 34.66666666666669
    agent-5: 34.66666666666669
  sampler_perf:
    mean_env_wait_ms: 23.597679595983287
    mean_inference_ms: 12.313650244518412
    mean_processing_ms: 50.872745377530485
  time_since_restore: 17402.5061109066
  time_this_iter_s: 127.23818135261536
  time_total_s: 20613.569797039032
  timestamp: 1637034946
  timesteps_since_restore: 12672000
  timesteps_this_iter: 96000
  timesteps_total: 14592000
  training_iteration: 152
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 26.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    152 |          20613.6 | 14592000 |   672.06 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 46
    apples_agent-0_mean: 3.83
    apples_agent-0_min: 0
    apples_agent-1_max: 65
    apples_agent-1_mean: 20.0
    apples_agent-1_min: 0
    apples_agent-2_max: 120
    apples_agent-2_mean: 9.46
    apples_agent-2_min: 0
    apples_agent-3_max: 157
    apples_agent-3_mean: 73.81
    apples_agent-3_min: 26
    apples_agent-4_max: 66
    apples_agent-4_mean: 2.98
    apples_agent-4_min: 0
    apples_agent-5_max: 165
    apples_agent-5_mean: 85.03
    apples_agent-5_min: 5
    cleaning_beam_agent-0_max: 443
    cleaning_beam_agent-0_mean: 317.56
    cleaning_beam_agent-0_min: 194
    cleaning_beam_agent-1_max: 387
    cleaning_beam_agent-1_mean: 220.83
    cleaning_beam_agent-1_min: 125
    cleaning_beam_agent-2_max: 406
    cleaning_beam_agent-2_mean: 221.88
    cleaning_beam_agent-2_min: 77
    cleaning_beam_agent-3_max: 182
    cleaning_beam_agent-3_mean: 46.8
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 499
    cleaning_beam_agent-4_mean: 367.11
    cleaning_beam_agent-4_min: 111
    cleaning_beam_agent-5_max: 556
    cleaning_beam_agent-5_mean: 116.47
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-57-50
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 878.9999999999833
  episode_reward_mean: 662.6699999999945
  episode_reward_min: 344.00000000000006
  episodes_this_iter: 96
  episodes_total: 14688
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20142.823
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00034945920924656093
        entropy: 1.1239337921142578
        entropy_coeff: 0.0017600000137463212
        kl: 0.011758329346776009
        model: {}
        policy_loss: -0.030453475192189217
        total_loss: -0.02978292480111122
        vf_explained_var: 0.06959305703639984
        vf_loss: 14.728370666503906
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00034945920924656093
        entropy: 1.1104987859725952
        entropy_coeff: 0.0017600000137463212
        kl: 0.013856817036867142
        model: {}
        policy_loss: -0.033825524151325226
        total_loss: -0.03280344232916832
        vf_explained_var: -0.004585638642311096
        vf_loss: 15.908783912658691
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.00034945920924656093
        entropy: 1.132331371307373
        entropy_coeff: 0.0017600000137463212
        kl: 0.012270702980458736
        model: {}
        policy_loss: -0.030738767236471176
        total_loss: -0.02941201999783516
        vf_explained_var: 0.06570596992969513
        vf_loss: 14.790483474731445
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00034945920924656093
        entropy: 0.606346607208252
        entropy_coeff: 0.0017600000137463212
        kl: 0.008846882730722427
        model: {}
        policy_loss: -0.021395647898316383
        total_loss: -0.020352574065327644
        vf_explained_var: 0.2249092161655426
        vf_loss: 12.255568504333496
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00034945920924656093
        entropy: 0.9750287532806396
        entropy_coeff: 0.0017600000137463212
        kl: 0.012358780950307846
        model: {}
        policy_loss: -0.029477866366505623
        total_loss: -0.028499916195869446
        vf_explained_var: 0.07815772294998169
        vf_loss: 14.581215858459473
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00034945920924656093
        entropy: 0.9565266370773315
        entropy_coeff: 0.0017600000137463212
        kl: 0.014727875590324402
        model: {}
        policy_loss: -0.03529675677418709
        total_loss: -0.0340627059340477
        vf_explained_var: 0.08725239336490631
        vf_loss: 14.447446823120117
    load_time_ms: 16047.896
    num_steps_sampled: 14688000
    num_steps_trained: 14688000
    sample_time_ms: 91974.667
    update_time_ms: 29.571
  iterations_since_restore: 133
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.836158192090394
    ram_util_percent: 14.220338983050848
  pid: 4061
  policy_reward_max:
    agent-0: 146.49999999999994
    agent-1: 146.49999999999994
    agent-2: 146.49999999999994
    agent-3: 146.49999999999994
    agent-4: 146.49999999999994
    agent-5: 146.49999999999994
  policy_reward_mean:
    agent-0: 110.44500000000038
    agent-1: 110.44500000000038
    agent-2: 110.44500000000038
    agent-3: 110.44500000000038
    agent-4: 110.44500000000038
    agent-5: 110.44500000000038
  policy_reward_min:
    agent-0: 57.33333333333321
    agent-1: 57.33333333333321
    agent-2: 57.33333333333321
    agent-3: 57.33333333333321
    agent-4: 57.33333333333321
    agent-5: 57.33333333333321
  sampler_perf:
    mean_env_wait_ms: 23.60141879422593
    mean_inference_ms: 12.31385194456076
    mean_processing_ms: 50.87261905400123
  time_since_restore: 17526.412175416946
  time_this_iter_s: 123.90606451034546
  time_total_s: 20737.475861549377
  timestamp: 1637035070
  timesteps_since_restore: 12768000
  timesteps_this_iter: 96000
  timesteps_total: 14688000
  training_iteration: 153
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 26.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    153 |          20737.5 | 14688000 |   662.67 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 42
    apples_agent-0_mean: 4.18
    apples_agent-0_min: 0
    apples_agent-1_max: 54
    apples_agent-1_mean: 18.38
    apples_agent-1_min: 0
    apples_agent-2_max: 167
    apples_agent-2_mean: 9.64
    apples_agent-2_min: 0
    apples_agent-3_max: 139
    apples_agent-3_mean: 75.91
    apples_agent-3_min: 28
    apples_agent-4_max: 105
    apples_agent-4_mean: 1.86
    apples_agent-4_min: 0
    apples_agent-5_max: 234
    apples_agent-5_mean: 89.26
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 501
    cleaning_beam_agent-0_mean: 324.94
    cleaning_beam_agent-0_min: 197
    cleaning_beam_agent-1_max: 404
    cleaning_beam_agent-1_mean: 224.62
    cleaning_beam_agent-1_min: 126
    cleaning_beam_agent-2_max: 406
    cleaning_beam_agent-2_mean: 228.73
    cleaning_beam_agent-2_min: 97
    cleaning_beam_agent-3_max: 107
    cleaning_beam_agent-3_mean: 41.78
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 484
    cleaning_beam_agent-4_mean: 355.92
    cleaning_beam_agent-4_min: 165
    cleaning_beam_agent-5_max: 844
    cleaning_beam_agent-5_mean: 110.65
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-59-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 933.9999999999822
  episode_reward_mean: 706.7199999999898
  episode_reward_min: 184.99999999999903
  episodes_this_iter: 96
  episodes_total: 14784
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20135.529
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0003434687969274819
        entropy: 1.1097973585128784
        entropy_coeff: 0.0017600000137463212
        kl: 0.01164123322814703
        model: {}
        policy_loss: -0.02949540689587593
        total_loss: -0.02866130881011486
        vf_explained_var: 0.06474952399730682
        vf_loss: 16.232194900512695
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0003434687969274819
        entropy: 1.1173746585845947
        entropy_coeff: 0.0017600000137463212
        kl: 0.013768170028924942
        model: {}
        policy_loss: -0.032402731478214264
        total_loss: -0.031306423246860504
        vf_explained_var: 0.028959795832633972
        vf_loss: 16.860713958740234
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.0003434687969274819
        entropy: 1.1300764083862305
        entropy_coeff: 0.0017600000137463212
        kl: 0.011757247149944305
        model: {}
        policy_loss: -0.03044048696756363
        total_loss: -0.029025042429566383
        vf_explained_var: 0.05471299588680267
        vf_loss: 16.40795135498047
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0003434687969274819
        entropy: 0.5608861446380615
        entropy_coeff: 0.0017600000137463212
        kl: 0.00897508766502142
        model: {}
        policy_loss: -0.019655849784612656
        total_loss: -0.01842397451400757
        vf_explained_var: 0.23794439435005188
        vf_loss: 13.21523666381836
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0003434687969274819
        entropy: 0.9771555066108704
        entropy_coeff: 0.0017600000137463212
        kl: 0.012724741362035275
        model: {}
        policy_loss: -0.0317380428314209
        total_loss: -0.030632559210062027
        vf_explained_var: 0.10568925738334656
        vf_loss: 15.528077125549316
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0003434687969274819
        entropy: 0.9501573443412781
        entropy_coeff: 0.0017600000137463212
        kl: 0.013210196979343891
        model: {}
        policy_loss: -0.03323270380496979
        total_loss: -0.0320393368601799
        vf_explained_var: 0.11016052961349487
        vf_loss: 15.446242332458496
    load_time_ms: 16015.576
    num_steps_sampled: 14784000
    num_steps_trained: 14784000
    sample_time_ms: 92125.788
    update_time_ms: 29.519
  iterations_since_restore: 134
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.81460674157303
    ram_util_percent: 14.360674157303372
  pid: 4061
  policy_reward_max:
    agent-0: 155.66666666666617
    agent-1: 155.66666666666617
    agent-2: 155.66666666666617
    agent-3: 155.66666666666617
    agent-4: 155.66666666666617
    agent-5: 155.66666666666617
  policy_reward_mean:
    agent-0: 117.78666666666707
    agent-1: 117.78666666666707
    agent-2: 117.78666666666707
    agent-3: 117.78666666666707
    agent-4: 117.78666666666707
    agent-5: 117.78666666666707
  policy_reward_min:
    agent-0: 30.833333333333393
    agent-1: 30.833333333333393
    agent-2: 30.833333333333393
    agent-3: 30.833333333333393
    agent-4: 30.833333333333393
    agent-5: 30.833333333333393
  sampler_perf:
    mean_env_wait_ms: 23.605387425746347
    mean_inference_ms: 12.314266506722868
    mean_processing_ms: 50.875282442383465
  time_since_restore: 17651.058144569397
  time_this_iter_s: 124.64596915245056
  time_total_s: 20862.121830701828
  timestamp: 1637035195
  timesteps_since_restore: 12864000
  timesteps_this_iter: 96000
  timesteps_total: 14784000
  training_iteration: 154
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 26.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    154 |          20862.1 | 14784000 |   706.72 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 74
    apples_agent-0_mean: 5.65
    apples_agent-0_min: 0
    apples_agent-1_max: 112
    apples_agent-1_mean: 20.18
    apples_agent-1_min: 0
    apples_agent-2_max: 74
    apples_agent-2_mean: 9.39
    apples_agent-2_min: 0
    apples_agent-3_max: 136
    apples_agent-3_mean: 72.63
    apples_agent-3_min: 35
    apples_agent-4_max: 47
    apples_agent-4_mean: 2.54
    apples_agent-4_min: 0
    apples_agent-5_max: 239
    apples_agent-5_mean: 83.77
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 501
    cleaning_beam_agent-0_mean: 317.17
    cleaning_beam_agent-0_min: 168
    cleaning_beam_agent-1_max: 325
    cleaning_beam_agent-1_mean: 215.92
    cleaning_beam_agent-1_min: 107
    cleaning_beam_agent-2_max: 397
    cleaning_beam_agent-2_mean: 232.27
    cleaning_beam_agent-2_min: 90
    cleaning_beam_agent-3_max: 120
    cleaning_beam_agent-3_mean: 40.93
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 455
    cleaning_beam_agent-4_mean: 340.35
    cleaning_beam_agent-4_min: 152
    cleaning_beam_agent-5_max: 676
    cleaning_beam_agent-5_mean: 128.99
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-01-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 871.9999999999632
  episode_reward_mean: 670.0399999999931
  episode_reward_min: 237.9999999999956
  episodes_this_iter: 96
  episodes_total: 14880
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20150.555
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0003374784137122333
        entropy: 1.1035351753234863
        entropy_coeff: 0.0017600000137463212
        kl: 0.011185851879417896
        model: {}
        policy_loss: -0.028953511267900467
        total_loss: -0.028270762413740158
        vf_explained_var: 0.06535612046718597
        vf_loss: 15.063851356506348
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0003374784137122333
        entropy: 1.1232597827911377
        entropy_coeff: 0.0017600000137463212
        kl: 0.01413737889379263
        model: {}
        policy_loss: -0.034007132053375244
        total_loss: -0.033019162714481354
        vf_explained_var: 0.03645998239517212
        vf_loss: 15.51169490814209
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.0003374784137122333
        entropy: 1.1267459392547607
        entropy_coeff: 0.0017600000137463212
        kl: 0.011673672124743462
        model: {}
        policy_loss: -0.030440419912338257
        total_loss: -0.029192868620157242
        vf_explained_var: 0.08159296214580536
        vf_loss: 14.795693397521973
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0003374784137122333
        entropy: 0.6041158437728882
        entropy_coeff: 0.0017600000137463212
        kl: 0.008899450302124023
        model: {}
        policy_loss: -0.021299388259649277
        total_loss: -0.020235467702150345
        vf_explained_var: 0.23073162138462067
        vf_loss: 12.37219524383545
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0003374784137122333
        entropy: 0.9746200442314148
        entropy_coeff: 0.0017600000137463212
        kl: 0.012552523985505104
        model: {}
        policy_loss: -0.031082328408956528
        total_loss: -0.030090749263763428
        vf_explained_var: 0.09839284420013428
        vf_loss: 14.51657772064209
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0003374784137122333
        entropy: 0.9537568688392639
        entropy_coeff: 0.0017600000137463212
        kl: 0.013595419935882092
        model: {}
        policy_loss: -0.03130502998828888
        total_loss: -0.03021201118826866
        vf_explained_var: 0.1231672614812851
        vf_loss: 14.120868682861328
    load_time_ms: 15925.269
    num_steps_sampled: 14880000
    num_steps_trained: 14880000
    sample_time_ms: 92267.394
    update_time_ms: 29.521
  iterations_since_restore: 135
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.835028248587573
    ram_util_percent: 14.361581920903955
  pid: 4061
  policy_reward_max:
    agent-0: 145.3333333333336
    agent-1: 145.3333333333336
    agent-2: 145.3333333333336
    agent-3: 145.3333333333336
    agent-4: 145.3333333333336
    agent-5: 145.3333333333336
  policy_reward_mean:
    agent-0: 111.67333333333376
    agent-1: 111.67333333333376
    agent-2: 111.67333333333376
    agent-3: 111.67333333333376
    agent-4: 111.67333333333376
    agent-5: 111.67333333333376
  policy_reward_min:
    agent-0: 39.66666666666668
    agent-1: 39.66666666666668
    agent-2: 39.66666666666668
    agent-3: 39.66666666666668
    agent-4: 39.66666666666668
    agent-5: 39.66666666666668
  sampler_perf:
    mean_env_wait_ms: 23.608613634347197
    mean_inference_ms: 12.3142315542633
    mean_processing_ms: 50.87684075921561
  time_since_restore: 17775.22344326973
  time_this_iter_s: 124.16529870033264
  time_total_s: 20986.28712940216
  timestamp: 1637035319
  timesteps_since_restore: 12960000
  timesteps_this_iter: 96000
  timesteps_total: 14880000
  training_iteration: 155
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 26.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    155 |          20986.3 | 14880000 |   670.04 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 53
    apples_agent-0_mean: 4.93
    apples_agent-0_min: 0
    apples_agent-1_max: 101
    apples_agent-1_mean: 22.32
    apples_agent-1_min: 0
    apples_agent-2_max: 74
    apples_agent-2_mean: 10.31
    apples_agent-2_min: 0
    apples_agent-3_max: 165
    apples_agent-3_mean: 77.07
    apples_agent-3_min: 21
    apples_agent-4_max: 58
    apples_agent-4_mean: 0.93
    apples_agent-4_min: 0
    apples_agent-5_max: 288
    apples_agent-5_mean: 83.66
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 463
    cleaning_beam_agent-0_mean: 310.88
    cleaning_beam_agent-0_min: 175
    cleaning_beam_agent-1_max: 422
    cleaning_beam_agent-1_mean: 216.58
    cleaning_beam_agent-1_min: 126
    cleaning_beam_agent-2_max: 451
    cleaning_beam_agent-2_mean: 230.28
    cleaning_beam_agent-2_min: 90
    cleaning_beam_agent-3_max: 128
    cleaning_beam_agent-3_mean: 42.78
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 490
    cleaning_beam_agent-4_mean: 358.4
    cleaning_beam_agent-4_min: 157
    cleaning_beam_agent-5_max: 691
    cleaning_beam_agent-5_mean: 126.21
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-04-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 934.9999999999728
  episode_reward_mean: 665.1599999999929
  episode_reward_min: 149.0000000000002
  episodes_this_iter: 96
  episodes_total: 14976
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20170.032
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00033148800139315426
        entropy: 1.1144939661026
        entropy_coeff: 0.0017600000137463212
        kl: 0.01196648646146059
        model: {}
        policy_loss: -0.0306437686085701
        total_loss: -0.029845453798770905
        vf_explained_var: 0.08108252286911011
        vf_loss: 15.631781578063965
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00033148800139315426
        entropy: 1.1119312047958374
        entropy_coeff: 0.0017600000137463212
        kl: 0.013293786905705929
        model: {}
        policy_loss: -0.03286721557378769
        total_loss: -0.03180651366710663
        vf_explained_var: 0.008012652397155762
        vf_loss: 16.883201599121094
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.00033148800139315426
        entropy: 1.1163427829742432
        entropy_coeff: 0.0017600000137463212
        kl: 0.011968184262514114
        model: {}
        policy_loss: -0.02937396429479122
        total_loss: -0.028029918670654297
        vf_explained_var: 0.11091926693916321
        vf_loss: 15.135826110839844
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00033148800139315426
        entropy: 0.5918084383010864
        entropy_coeff: 0.0017600000137463212
        kl: 0.008539823815226555
        model: {}
        policy_loss: -0.019961785525083542
        total_loss: -0.018855683505535126
        vf_explained_var: 0.2394525110721588
        vf_loss: 12.937012672424316
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00033148800139315426
        entropy: 0.9836438298225403
        entropy_coeff: 0.0017600000137463212
        kl: 0.013701437041163445
        model: {}
        policy_loss: -0.030866703018546104
        total_loss: -0.029664266854524612
        vf_explained_var: 0.08081047236919403
        vf_loss: 15.635066032409668
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00033148800139315426
        entropy: 0.9379959106445312
        entropy_coeff: 0.0017600000137463212
        kl: 0.013006888329982758
        model: {}
        policy_loss: -0.03180568665266037
        total_loss: -0.030682766810059547
        vf_explained_var: 0.1339971125125885
        vf_loss: 14.731009483337402
    load_time_ms: 15781.683
    num_steps_sampled: 14976000
    num_steps_trained: 14976000
    sample_time_ms: 92131.177
    update_time_ms: 29.112
  iterations_since_restore: 136
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.627528089887644
    ram_util_percent: 14.369662921348315
  pid: 4061
  policy_reward_max:
    agent-0: 155.83333333333303
    agent-1: 155.83333333333303
    agent-2: 155.83333333333303
    agent-3: 155.83333333333303
    agent-4: 155.83333333333303
    agent-5: 155.83333333333303
  policy_reward_mean:
    agent-0: 110.8600000000004
    agent-1: 110.8600000000004
    agent-2: 110.8600000000004
    agent-3: 110.8600000000004
    agent-4: 110.8600000000004
    agent-5: 110.8600000000004
  policy_reward_min:
    agent-0: 24.83333333333337
    agent-1: 24.83333333333337
    agent-2: 24.83333333333337
    agent-3: 24.83333333333337
    agent-4: 24.83333333333337
    agent-5: 24.83333333333337
  sampler_perf:
    mean_env_wait_ms: 23.610729742561226
    mean_inference_ms: 12.31374654589077
    mean_processing_ms: 50.87425965729583
  time_since_restore: 17899.748759508133
  time_this_iter_s: 124.52531623840332
  time_total_s: 21110.812445640564
  timestamp: 1637035444
  timesteps_since_restore: 13056000
  timesteps_this_iter: 96000
  timesteps_total: 14976000
  training_iteration: 156
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 26.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    156 |          21110.8 | 14976000 |   665.16 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 46
    apples_agent-0_mean: 7.2
    apples_agent-0_min: 0
    apples_agent-1_max: 75
    apples_agent-1_mean: 21.29
    apples_agent-1_min: 0
    apples_agent-2_max: 93
    apples_agent-2_mean: 9.6
    apples_agent-2_min: 0
    apples_agent-3_max: 159
    apples_agent-3_mean: 78.65
    apples_agent-3_min: 31
    apples_agent-4_max: 78
    apples_agent-4_mean: 2.02
    apples_agent-4_min: 0
    apples_agent-5_max: 288
    apples_agent-5_mean: 82.16
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 459
    cleaning_beam_agent-0_mean: 316.47
    cleaning_beam_agent-0_min: 158
    cleaning_beam_agent-1_max: 300
    cleaning_beam_agent-1_mean: 204.7
    cleaning_beam_agent-1_min: 93
    cleaning_beam_agent-2_max: 379
    cleaning_beam_agent-2_mean: 225.52
    cleaning_beam_agent-2_min: 59
    cleaning_beam_agent-3_max: 148
    cleaning_beam_agent-3_mean: 46.9
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 483
    cleaning_beam_agent-4_mean: 356.9
    cleaning_beam_agent-4_min: 198
    cleaning_beam_agent-5_max: 822
    cleaning_beam_agent-5_mean: 135.64
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-06-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 875.9999999999717
  episode_reward_mean: 658.8499999999946
  episode_reward_min: 296.0000000000007
  episodes_this_iter: 96
  episodes_total: 15072
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20139.871
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0003254975890740752
        entropy: 1.1066291332244873
        entropy_coeff: 0.0017600000137463212
        kl: 0.011623186990618706
        model: {}
        policy_loss: -0.029653042554855347
        total_loss: -0.02899831160902977
        vf_explained_var: 0.08637361228466034
        vf_loss: 14.40079116821289
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0003254975890740752
        entropy: 1.1200971603393555
        entropy_coeff: 0.0017600000137463212
        kl: 0.014356154948472977
        model: {}
        policy_loss: -0.032818712294101715
        total_loss: -0.03183005005121231
        vf_explained_var: 0.03345286846160889
        vf_loss: 15.244185447692871
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.0003254975890740752
        entropy: 1.1158983707427979
        entropy_coeff: 0.0017600000137463212
        kl: 0.012198857963085175
        model: {}
        policy_loss: -0.030754130333662033
        total_loss: -0.029421675950288773
        vf_explained_var: 0.07043270766735077
        vf_loss: 14.666056632995605
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0003254975890740752
        entropy: 0.6077214479446411
        entropy_coeff: 0.0017600000137463212
        kl: 0.008731416426599026
        model: {}
        policy_loss: -0.021287139505147934
        total_loss: -0.020264577120542526
        vf_explained_var: 0.22661319375038147
        vf_loss: 12.190104484558105
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0003254975890740752
        entropy: 0.9847922325134277
        entropy_coeff: 0.0017600000137463212
        kl: 0.012368999421596527
        model: {}
        policy_loss: -0.029763054102659225
        total_loss: -0.028755703940987587
        vf_explained_var: 0.04679687321186066
        vf_loss: 15.036837577819824
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0003254975890740752
        entropy: 0.9822534918785095
        entropy_coeff: 0.0017600000137463212
        kl: 0.014057870954275131
        model: {}
        policy_loss: -0.03398749604821205
        total_loss: -0.03297252207994461
        vf_explained_var: 0.15075433254241943
        vf_loss: 13.379555702209473
    load_time_ms: 15389.047
    num_steps_sampled: 15072000
    num_steps_trained: 15072000
    sample_time_ms: 92050.728
    update_time_ms: 22.935
  iterations_since_restore: 137
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.675842696629214
    ram_util_percent: 14.354494382022473
  pid: 4061
  policy_reward_max:
    agent-0: 146.00000000000037
    agent-1: 146.00000000000037
    agent-2: 146.00000000000037
    agent-3: 146.00000000000037
    agent-4: 146.00000000000037
    agent-5: 146.00000000000037
  policy_reward_mean:
    agent-0: 109.80833333333374
    agent-1: 109.80833333333374
    agent-2: 109.80833333333374
    agent-3: 109.80833333333374
    agent-4: 109.80833333333374
    agent-5: 109.80833333333374
  policy_reward_min:
    agent-0: 49.33333333333323
    agent-1: 49.33333333333323
    agent-2: 49.33333333333323
    agent-3: 49.33333333333323
    agent-4: 49.33333333333323
    agent-5: 49.33333333333323
  sampler_perf:
    mean_env_wait_ms: 23.61445768103795
    mean_inference_ms: 12.314212989154358
    mean_processing_ms: 50.87563410286773
  time_since_restore: 18024.887063503265
  time_this_iter_s: 125.13830399513245
  time_total_s: 21235.950749635696
  timestamp: 1637035569
  timesteps_since_restore: 13152000
  timesteps_this_iter: 96000
  timesteps_total: 15072000
  training_iteration: 157
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 26.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    157 |            21236 | 15072000 |   658.85 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 69
    apples_agent-0_mean: 5.73
    apples_agent-0_min: 0
    apples_agent-1_max: 115
    apples_agent-1_mean: 19.16
    apples_agent-1_min: 0
    apples_agent-2_max: 101
    apples_agent-2_mean: 11.34
    apples_agent-2_min: 0
    apples_agent-3_max: 175
    apples_agent-3_mean: 75.69
    apples_agent-3_min: 29
    apples_agent-4_max: 77
    apples_agent-4_mean: 3.54
    apples_agent-4_min: 0
    apples_agent-5_max: 166
    apples_agent-5_mean: 83.02
    apples_agent-5_min: 6
    cleaning_beam_agent-0_max: 440
    cleaning_beam_agent-0_mean: 318.07
    cleaning_beam_agent-0_min: 195
    cleaning_beam_agent-1_max: 320
    cleaning_beam_agent-1_mean: 207.05
    cleaning_beam_agent-1_min: 99
    cleaning_beam_agent-2_max: 438
    cleaning_beam_agent-2_mean: 215.12
    cleaning_beam_agent-2_min: 53
    cleaning_beam_agent-3_max: 146
    cleaning_beam_agent-3_mean: 44.59
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 477
    cleaning_beam_agent-4_mean: 341.85
    cleaning_beam_agent-4_min: 135
    cleaning_beam_agent-5_max: 570
    cleaning_beam_agent-5_mean: 138.72
    cleaning_beam_agent-5_min: 17
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-08-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 899.99999999998
  episode_reward_mean: 651.6699999999951
  episode_reward_min: 257.99999999999596
  episodes_this_iter: 96
  episodes_total: 15168
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20148.777
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00031950720585882664
        entropy: 1.1182109117507935
        entropy_coeff: 0.0017600000137463212
        kl: 0.010758497752249241
        model: {}
        policy_loss: -0.028116323053836823
        total_loss: -0.02751898393034935
        vf_explained_var: 0.06042276322841644
        vf_loss: 14.89542293548584
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00031950720585882664
        entropy: 1.117258071899414
        entropy_coeff: 0.0017600000137463212
        kl: 0.014790383167564869
        model: {}
        policy_loss: -0.033900756388902664
        total_loss: -0.03276783972978592
        vf_explained_var: -0.021855264902114868
        vf_loss: 16.2025089263916
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.00031950720585882664
        entropy: 1.1155588626861572
        entropy_coeff: 0.0017600000137463212
        kl: 0.011503683403134346
        model: {}
        policy_loss: -0.0296095609664917
        total_loss: -0.028357377275824547
        vf_explained_var: 0.05966489017009735
        vf_loss: 14.900110244750977
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00031950720585882664
        entropy: 0.5978891849517822
        entropy_coeff: 0.0017600000137463212
        kl: 0.009283374063670635
        model: {}
        policy_loss: -0.02033887431025505
        total_loss: -0.019191380590200424
        vf_explained_var: 0.1975105106830597
        vf_loss: 12.714430809020996
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00031950720585882664
        entropy: 0.98659348487854
        entropy_coeff: 0.0017600000137463212
        kl: 0.012453620322048664
        model: {}
        policy_loss: -0.029665255919098854
        total_loss: -0.028703544288873672
        vf_explained_var: 0.0833420604467392
        vf_loss: 14.52757453918457
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00031950720585882664
        entropy: 0.979613184928894
        entropy_coeff: 0.0017600000137463212
        kl: 0.013717030175030231
        model: {}
        policy_loss: -0.03382553160190582
        total_loss: -0.03273235261440277
        vf_explained_var: 0.08762329816818237
        vf_loss: 14.455869674682617
    load_time_ms: 14842.672
    num_steps_sampled: 15168000
    num_steps_trained: 15168000
    sample_time_ms: 91122.098
    update_time_ms: 22.795
  iterations_since_restore: 138
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.605524861878454
    ram_util_percent: 14.277348066298341
  pid: 4061
  policy_reward_max:
    agent-0: 149.99999999999991
    agent-1: 149.99999999999991
    agent-2: 149.99999999999991
    agent-3: 149.99999999999991
    agent-4: 149.99999999999991
    agent-5: 149.99999999999991
  policy_reward_mean:
    agent-0: 108.61166666666706
    agent-1: 108.61166666666706
    agent-2: 108.61166666666706
    agent-3: 108.61166666666706
    agent-4: 108.61166666666706
    agent-5: 108.61166666666706
  policy_reward_min:
    agent-0: 42.99999999999995
    agent-1: 42.99999999999995
    agent-2: 42.99999999999995
    agent-3: 42.99999999999995
    agent-4: 42.99999999999995
    agent-5: 42.99999999999995
  sampler_perf:
    mean_env_wait_ms: 23.617233414266302
    mean_inference_ms: 12.314624815332268
    mean_processing_ms: 50.87925242767491
  time_since_restore: 18151.38667821884
  time_this_iter_s: 126.49961471557617
  time_total_s: 21362.450364351273
  timestamp: 1637035696
  timesteps_since_restore: 13248000
  timesteps_this_iter: 96000
  timesteps_total: 15168000
  training_iteration: 158
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 26.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    158 |          21362.5 | 15168000 |   651.67 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 45
    apples_agent-0_mean: 4.27
    apples_agent-0_min: 0
    apples_agent-1_max: 94
    apples_agent-1_mean: 20.31
    apples_agent-1_min: 0
    apples_agent-2_max: 97
    apples_agent-2_mean: 10.3
    apples_agent-2_min: 0
    apples_agent-3_max: 134
    apples_agent-3_mean: 75.66
    apples_agent-3_min: 26
    apples_agent-4_max: 115
    apples_agent-4_mean: 1.62
    apples_agent-4_min: 0
    apples_agent-5_max: 155
    apples_agent-5_mean: 86.89
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 456
    cleaning_beam_agent-0_mean: 333.61
    cleaning_beam_agent-0_min: 176
    cleaning_beam_agent-1_max: 366
    cleaning_beam_agent-1_mean: 213.57
    cleaning_beam_agent-1_min: 105
    cleaning_beam_agent-2_max: 455
    cleaning_beam_agent-2_mean: 213.31
    cleaning_beam_agent-2_min: 113
    cleaning_beam_agent-3_max: 124
    cleaning_beam_agent-3_mean: 44.86
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 447
    cleaning_beam_agent-4_mean: 348.53
    cleaning_beam_agent-4_min: 214
    cleaning_beam_agent-5_max: 843
    cleaning_beam_agent-5_mean: 123.9
    cleaning_beam_agent-5_min: 19
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-10-30
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 872.9999999999642
  episode_reward_mean: 663.7299999999949
  episode_reward_min: 326.00000000000455
  episodes_this_iter: 96
  episodes_total: 15264
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20167.047
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0003135167935397476
        entropy: 1.0878220796585083
        entropy_coeff: 0.0017600000137463212
        kl: 0.011721346527338028
        model: {}
        policy_loss: -0.02918018028140068
        total_loss: -0.02849084511399269
        vf_explained_var: 0.10137294232845306
        vf_loss: 14.317638397216797
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0003135167935397476
        entropy: 1.1073583364486694
        entropy_coeff: 0.0017600000137463212
        kl: 0.013364839367568493
        model: {}
        policy_loss: -0.03260611742734909
        total_loss: -0.03162183240056038
        vf_explained_var: -0.0024154633283615112
        vf_loss: 15.96753215789795
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.0003135167935397476
        entropy: 1.1099791526794434
        entropy_coeff: 0.0017600000137463212
        kl: 0.011534322053194046
        model: {}
        policy_loss: -0.029731150716543198
        total_loss: -0.02852487377822399
        vf_explained_var: 0.10258640348911285
        vf_loss: 14.296943664550781
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0003135167935397476
        entropy: 0.6125421524047852
        entropy_coeff: 0.0017600000137463212
        kl: 0.009505541063845158
        model: {}
        policy_loss: -0.02196931652724743
        total_loss: -0.020846813917160034
        vf_explained_var: 0.21553605794906616
        vf_loss: 12.500198364257812
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0003135167935397476
        entropy: 0.974877655506134
        entropy_coeff: 0.0017600000137463212
        kl: 0.01184239611029625
        model: {}
        policy_loss: -0.028558071702718735
        total_loss: -0.027573319151997566
        vf_explained_var: 0.048418447375297546
        vf_loss: 15.162922859191895
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0003135167935397476
        entropy: 0.9584337472915649
        entropy_coeff: 0.0017600000137463212
        kl: 0.01280103251338005
        model: {}
        policy_loss: -0.031472098082304
        total_loss: -0.030464602634310722
        vf_explained_var: 0.11295188963413239
        vf_loss: 14.14236068725586
    load_time_ms: 14196.855
    num_steps_sampled: 15264000
    num_steps_trained: 15264000
    sample_time_ms: 91696.277
    update_time_ms: 22.762
  iterations_since_restore: 139
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.889528795811515
    ram_util_percent: 14.37643979057592
  pid: 4061
  policy_reward_max:
    agent-0: 145.49999999999946
    agent-1: 145.49999999999946
    agent-2: 145.49999999999946
    agent-3: 145.49999999999946
    agent-4: 145.49999999999946
    agent-5: 145.49999999999946
  policy_reward_mean:
    agent-0: 110.62166666666704
    agent-1: 110.62166666666704
    agent-2: 110.62166666666704
    agent-3: 110.62166666666704
    agent-4: 110.62166666666704
    agent-5: 110.62166666666704
  policy_reward_min:
    agent-0: 54.33333333333313
    agent-1: 54.33333333333313
    agent-2: 54.33333333333313
    agent-3: 54.33333333333313
    agent-4: 54.33333333333313
    agent-5: 54.33333333333313
  sampler_perf:
    mean_env_wait_ms: 23.619385675632124
    mean_inference_ms: 12.315084935950225
    mean_processing_ms: 50.8910133519602
  time_since_restore: 18285.59554362297
  time_this_iter_s: 134.20886540412903
  time_total_s: 21496.6592297554
  timestamp: 1637035830
  timesteps_since_restore: 13344000
  timesteps_this_iter: 96000
  timesteps_total: 15264000
  training_iteration: 159
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 26.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    159 |          21496.7 | 15264000 |   663.73 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 43
    apples_agent-0_mean: 3.9
    apples_agent-0_min: 0
    apples_agent-1_max: 120
    apples_agent-1_mean: 23.15
    apples_agent-1_min: 0
    apples_agent-2_max: 81
    apples_agent-2_mean: 9.45
    apples_agent-2_min: 0
    apples_agent-3_max: 151
    apples_agent-3_mean: 74.72
    apples_agent-3_min: 20
    apples_agent-4_max: 43
    apples_agent-4_mean: 0.82
    apples_agent-4_min: 0
    apples_agent-5_max: 213
    apples_agent-5_mean: 81.51
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 506
    cleaning_beam_agent-0_mean: 331.94
    cleaning_beam_agent-0_min: 201
    cleaning_beam_agent-1_max: 299
    cleaning_beam_agent-1_mean: 195.55
    cleaning_beam_agent-1_min: 81
    cleaning_beam_agent-2_max: 369
    cleaning_beam_agent-2_mean: 210.77
    cleaning_beam_agent-2_min: 88
    cleaning_beam_agent-3_max: 136
    cleaning_beam_agent-3_mean: 46.79
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 512
    cleaning_beam_agent-4_mean: 356.3
    cleaning_beam_agent-4_min: 229
    cleaning_beam_agent-5_max: 721
    cleaning_beam_agent-5_mean: 115.63
    cleaning_beam_agent-5_min: 20
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-12-54
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 901.9999999999714
  episode_reward_mean: 672.9999999999924
  episode_reward_min: 210.99999999999721
  episodes_this_iter: 96
  episodes_total: 15360
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20161.571
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000307526410324499
        entropy: 1.0888856649398804
        entropy_coeff: 0.0017600000137463212
        kl: 0.010673596523702145
        model: {}
        policy_loss: -0.027183787897229195
        total_loss: -0.02656400389969349
        vf_explained_var: 0.08023630082607269
        vf_loss: 14.68862247467041
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000307526410324499
        entropy: 1.1043453216552734
        entropy_coeff: 0.0017600000137463212
        kl: 0.013238462619483471
        model: {}
        policy_loss: -0.031966838985681534
        total_loss: -0.030987288802862167
        vf_explained_var: -0.0012721121311187744
        vf_loss: 15.99349594116211
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.000307526410324499
        entropy: 1.1269770860671997
        entropy_coeff: 0.0017600000137463212
        kl: 0.011895138770341873
        model: {}
        policy_loss: -0.030521921813488007
        total_loss: -0.029252417385578156
        vf_explained_var: 0.08067519962787628
        vf_loss: 14.687097549438477
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000307526410324499
        entropy: 0.5836210250854492
        entropy_coeff: 0.0017600000137463212
        kl: 0.007947525940835476
        model: {}
        policy_loss: -0.01945630833506584
        total_loss: -0.018407125025987625
        vf_explained_var: 0.19750598073005676
        vf_loss: 12.81602954864502
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000307526410324499
        entropy: 0.9753189086914062
        entropy_coeff: 0.0017600000137463212
        kl: 0.01226599421352148
        model: {}
        policy_loss: -0.029350124299526215
        total_loss: -0.0283430814743042
        vf_explained_var: 0.0631546676158905
        vf_loss: 14.970003128051758
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000307526410324499
        entropy: 0.964454710483551
        entropy_coeff: 0.0017600000137463212
        kl: 0.012924488633871078
        model: {}
        policy_loss: -0.03266305476427078
        total_loss: -0.0316307470202446
        vf_explained_var: 0.10052399337291718
        vf_loss: 14.373050689697266
    load_time_ms: 15935.061
    num_steps_sampled: 15360000
    num_steps_trained: 15360000
    sample_time_ms: 91753.93
    update_time_ms: 23.222
  iterations_since_restore: 140
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.186764705882354
    ram_util_percent: 14.633333333333336
  pid: 4061
  policy_reward_max:
    agent-0: 150.33333333333343
    agent-1: 150.33333333333343
    agent-2: 150.33333333333343
    agent-3: 150.33333333333343
    agent-4: 150.33333333333343
    agent-5: 150.33333333333343
  policy_reward_mean:
    agent-0: 112.16666666666706
    agent-1: 112.16666666666706
    agent-2: 112.16666666666706
    agent-3: 112.16666666666706
    agent-4: 112.16666666666706
    agent-5: 112.16666666666706
  policy_reward_min:
    agent-0: 35.1666666666667
    agent-1: 35.1666666666667
    agent-2: 35.1666666666667
    agent-3: 35.1666666666667
    agent-4: 35.1666666666667
    agent-5: 35.1666666666667
  sampler_perf:
    mean_env_wait_ms: 23.621683393006528
    mean_inference_ms: 12.31556720810325
    mean_processing_ms: 50.89324697766198
  time_since_restore: 18428.969879627228
  time_this_iter_s: 143.3743360042572
  time_total_s: 21640.03356575966
  timestamp: 1637035974
  timesteps_since_restore: 13440000
  timesteps_this_iter: 96000
  timesteps_total: 15360000
  training_iteration: 160
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 26.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    160 |            21640 | 15360000 |      673 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 38
    apples_agent-0_mean: 4.73
    apples_agent-0_min: 0
    apples_agent-1_max: 89
    apples_agent-1_mean: 21.82
    apples_agent-1_min: 0
    apples_agent-2_max: 60
    apples_agent-2_mean: 12.45
    apples_agent-2_min: 0
    apples_agent-3_max: 128
    apples_agent-3_mean: 70.8
    apples_agent-3_min: 30
    apples_agent-4_max: 63
    apples_agent-4_mean: 1.53
    apples_agent-4_min: 0
    apples_agent-5_max: 205
    apples_agent-5_mean: 82.86
    apples_agent-5_min: 7
    cleaning_beam_agent-0_max: 482
    cleaning_beam_agent-0_mean: 331.76
    cleaning_beam_agent-0_min: 141
    cleaning_beam_agent-1_max: 344
    cleaning_beam_agent-1_mean: 200.59
    cleaning_beam_agent-1_min: 95
    cleaning_beam_agent-2_max: 366
    cleaning_beam_agent-2_mean: 203.85
    cleaning_beam_agent-2_min: 56
    cleaning_beam_agent-3_max: 144
    cleaning_beam_agent-3_mean: 46.37
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 483
    cleaning_beam_agent-4_mean: 340.25
    cleaning_beam_agent-4_min: 201
    cleaning_beam_agent-5_max: 709
    cleaning_beam_agent-5_mean: 119.13
    cleaning_beam_agent-5_min: 14
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-14-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 891.9999999999848
  episode_reward_mean: 656.2699999999943
  episode_reward_min: 256.9999999999959
  episodes_this_iter: 96
  episodes_total: 15456
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20149.421
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00030153599800541997
        entropy: 1.0846890211105347
        entropy_coeff: 0.0017600000137463212
        kl: 0.011421587318181992
        model: {}
        policy_loss: -0.02758721448481083
        total_loss: -0.02681744657456875
        vf_explained_var: 0.09912292659282684
        vf_loss: 15.366630554199219
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00030153599800541997
        entropy: 1.1081488132476807
        entropy_coeff: 0.0017600000137463212
        kl: 0.012749288231134415
        model: {}
        policy_loss: -0.03314049541950226
        total_loss: -0.032139748334884644
        vf_explained_var: 0.018701300024986267
        vf_loss: 16.761571884155273
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.00030153599800541997
        entropy: 1.11611008644104
        entropy_coeff: 0.0017600000137463212
        kl: 0.01149515900760889
        model: {}
        policy_loss: -0.028553584590554237
        total_loss: -0.027212880551815033
        vf_explained_var: 0.0734662115573883
        vf_loss: 15.807841300964355
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00030153599800541997
        entropy: 0.6009261608123779
        entropy_coeff: 0.0017600000137463212
        kl: 0.008045955561101437
        model: {}
        policy_loss: -0.01940404251217842
        total_loss: -0.018371231853961945
        vf_explained_var: 0.24486683309078217
        vf_loss: 12.858428001403809
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00030153599800541997
        entropy: 0.9881198406219482
        entropy_coeff: 0.0017600000137463212
        kl: 0.011433128267526627
        model: {}
        policy_loss: -0.028483770787715912
        total_loss: -0.02749214693903923
        vf_explained_var: 0.06903709471225739
        vf_loss: 15.874055862426758
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00030153599800541997
        entropy: 0.9719454050064087
        entropy_coeff: 0.0017600000137463212
        kl: 0.013485675677657127
        model: {}
        policy_loss: -0.03315170854330063
        total_loss: -0.03203752264380455
        vf_explained_var: 0.1345236450433731
        vf_loss: 14.762433052062988
    load_time_ms: 15892.824
    num_steps_sampled: 15456000
    num_steps_trained: 15456000
    sample_time_ms: 91572.652
    update_time_ms: 25.91
  iterations_since_restore: 141
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.757865168539325
    ram_util_percent: 14.351685393258428
  pid: 4061
  policy_reward_max:
    agent-0: 148.66666666666688
    agent-1: 148.66666666666688
    agent-2: 148.66666666666688
    agent-3: 148.66666666666688
    agent-4: 148.66666666666688
    agent-5: 148.66666666666688
  policy_reward_mean:
    agent-0: 109.3783333333337
    agent-1: 109.3783333333337
    agent-2: 109.3783333333337
    agent-3: 109.3783333333337
    agent-4: 109.3783333333337
    agent-5: 109.3783333333337
  policy_reward_min:
    agent-0: 42.8333333333333
    agent-1: 42.8333333333333
    agent-2: 42.8333333333333
    agent-3: 42.8333333333333
    agent-4: 42.8333333333333
    agent-5: 42.8333333333333
  sampler_perf:
    mean_env_wait_ms: 23.623641022968403
    mean_inference_ms: 12.316023848052234
    mean_processing_ms: 50.89514820463073
  time_since_restore: 18552.649277210236
  time_this_iter_s: 123.67939758300781
  time_total_s: 21763.712963342667
  timestamp: 1637036098
  timesteps_since_restore: 13536000
  timesteps_this_iter: 96000
  timesteps_total: 15456000
  training_iteration: 161
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    161 |          21763.7 | 15456000 |   656.27 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 43
    apples_agent-0_mean: 4.27
    apples_agent-0_min: 0
    apples_agent-1_max: 117
    apples_agent-1_mean: 21.19
    apples_agent-1_min: 0
    apples_agent-2_max: 137
    apples_agent-2_mean: 13.34
    apples_agent-2_min: 0
    apples_agent-3_max: 160
    apples_agent-3_mean: 69.88
    apples_agent-3_min: 26
    apples_agent-4_max: 89
    apples_agent-4_mean: 3.43
    apples_agent-4_min: 0
    apples_agent-5_max: 180
    apples_agent-5_mean: 87.72
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 456
    cleaning_beam_agent-0_mean: 343.74
    cleaning_beam_agent-0_min: 220
    cleaning_beam_agent-1_max: 327
    cleaning_beam_agent-1_mean: 194.0
    cleaning_beam_agent-1_min: 114
    cleaning_beam_agent-2_max: 370
    cleaning_beam_agent-2_mean: 225.63
    cleaning_beam_agent-2_min: 55
    cleaning_beam_agent-3_max: 121
    cleaning_beam_agent-3_mean: 43.63
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 466
    cleaning_beam_agent-4_mean: 343.44
    cleaning_beam_agent-4_min: 139
    cleaning_beam_agent-5_max: 621
    cleaning_beam_agent-5_mean: 105.15
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 3
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-17-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 911.9999999999814
  episode_reward_mean: 683.7699999999909
  episode_reward_min: 257.9999999999969
  episodes_this_iter: 96
  episodes_total: 15552
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20155.684
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00029554558568634093
        entropy: 1.0793859958648682
        entropy_coeff: 0.0017600000137463212
        kl: 0.011422943323850632
        model: {}
        policy_loss: -0.026709940284490585
        total_loss: -0.025788310915231705
        vf_explained_var: 0.060598403215408325
        vf_loss: 16.79055404663086
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00029554558568634093
        entropy: 1.1053571701049805
        entropy_coeff: 0.0017600000137463212
        kl: 0.01316223293542862
        model: {}
        policy_loss: -0.032053131610155106
        total_loss: -0.030896760523319244
        vf_explained_var: 0.0020536035299301147
        vf_loss: 17.855772018432617
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.00029554558568634093
        entropy: 1.1289715766906738
        entropy_coeff: 0.0017600000137463212
        kl: 0.0109245665371418
        model: {}
        policy_loss: -0.029857132583856583
        total_loss: -0.028549714013934135
        vf_explained_var: 0.07443243265151978
        vf_loss: 16.5572452545166
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00029554558568634093
        entropy: 0.5832656621932983
        entropy_coeff: 0.0017600000137463212
        kl: 0.008080986328423023
        model: {}
        policy_loss: -0.01956012472510338
        total_loss: -0.018427573144435883
        vf_explained_var: 0.24465876817703247
        vf_loss: 13.509986877441406
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00029554558568634093
        entropy: 0.9963170289993286
        entropy_coeff: 0.0017600000137463212
        kl: 0.011554298922419548
        model: {}
        policy_loss: -0.028775542974472046
        total_loss: -0.027806133031845093
        vf_explained_var: 0.12346498668193817
        vf_loss: 15.674985885620117
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00029554558568634093
        entropy: 0.9638835191726685
        entropy_coeff: 0.0017600000137463212
        kl: 0.013110082596540451
        model: {}
        policy_loss: -0.03183305263519287
        total_loss: -0.030743345618247986
        vf_explained_var: 0.17466911673545837
        vf_loss: 14.751375198364258
    load_time_ms: 16055.814
    num_steps_sampled: 15552000
    num_steps_trained: 15552000
    sample_time_ms: 91432.802
    update_time_ms: 25.606
  iterations_since_restore: 142
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.455494505494507
    ram_util_percent: 14.329670329670332
  pid: 4061
  policy_reward_max:
    agent-0: 151.99999999999997
    agent-1: 151.99999999999997
    agent-2: 151.99999999999997
    agent-3: 151.99999999999997
    agent-4: 151.99999999999997
    agent-5: 151.99999999999997
  policy_reward_mean:
    agent-0: 113.96166666666706
    agent-1: 113.96166666666706
    agent-2: 113.96166666666706
    agent-3: 113.96166666666706
    agent-4: 113.96166666666706
    agent-5: 113.96166666666706
  policy_reward_min:
    agent-0: 42.99999999999995
    agent-1: 42.99999999999995
    agent-2: 42.99999999999995
    agent-3: 42.99999999999995
    agent-4: 42.99999999999995
    agent-5: 42.99999999999995
  sampler_perf:
    mean_env_wait_ms: 23.625527367445795
    mean_inference_ms: 12.315999942547268
    mean_processing_ms: 50.8963082266475
  time_since_restore: 18680.170173168182
  time_this_iter_s: 127.52089595794678
  time_total_s: 21891.233859300613
  timestamp: 1637036226
  timesteps_since_restore: 13632000
  timesteps_this_iter: 96000
  timesteps_total: 15552000
  training_iteration: 162
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 26.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    162 |          21891.2 | 15552000 |   683.77 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 52
    apples_agent-0_mean: 4.71
    apples_agent-0_min: 0
    apples_agent-1_max: 77
    apples_agent-1_mean: 21.51
    apples_agent-1_min: 0
    apples_agent-2_max: 66
    apples_agent-2_mean: 8.24
    apples_agent-2_min: 0
    apples_agent-3_max: 121
    apples_agent-3_mean: 70.69
    apples_agent-3_min: 26
    apples_agent-4_max: 78
    apples_agent-4_mean: 2.28
    apples_agent-4_min: 0
    apples_agent-5_max: 169
    apples_agent-5_mean: 84.19
    apples_agent-5_min: 6
    cleaning_beam_agent-0_max: 478
    cleaning_beam_agent-0_mean: 332.87
    cleaning_beam_agent-0_min: 220
    cleaning_beam_agent-1_max: 327
    cleaning_beam_agent-1_mean: 195.83
    cleaning_beam_agent-1_min: 98
    cleaning_beam_agent-2_max: 404
    cleaning_beam_agent-2_mean: 220.69
    cleaning_beam_agent-2_min: 75
    cleaning_beam_agent-3_max: 170
    cleaning_beam_agent-3_mean: 45.63
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 480
    cleaning_beam_agent-4_mean: 343.22
    cleaning_beam_agent-4_min: 198
    cleaning_beam_agent-5_max: 619
    cleaning_beam_agent-5_mean: 103.73
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.05
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-19-11
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 907.9999999999797
  episode_reward_mean: 685.5199999999911
  episode_reward_min: 169.9999999999988
  episodes_this_iter: 96
  episodes_total: 15648
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20156.924
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00028955520247109234
        entropy: 1.087385892868042
        entropy_coeff: 0.0017600000137463212
        kl: 0.01063685305416584
        model: {}
        policy_loss: -0.026694027706980705
        total_loss: -0.025962114334106445
        vf_explained_var: 0.07445968687534332
        vf_loss: 15.820283889770508
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00028955520247109234
        entropy: 1.1075031757354736
        entropy_coeff: 0.0017600000137463212
        kl: 0.012669501826167107
        model: {}
        policy_loss: -0.03145235776901245
        total_loss: -0.03037729486823082
        vf_explained_var: -0.026326239109039307
        vf_loss: 17.57318115234375
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.00028955520247109234
        entropy: 1.1186487674713135
        entropy_coeff: 0.0017600000137463212
        kl: 0.011340177617967129
        model: {}
        policy_loss: -0.029615085572004318
        total_loss: -0.02830844558775425
        vf_explained_var: 0.07845067977905273
        vf_loss: 15.74435043334961
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00028955520247109234
        entropy: 0.5621559023857117
        entropy_coeff: 0.0017600000137463212
        kl: 0.0075467973947525024
        model: {}
        policy_loss: -0.018694084137678146
        total_loss: -0.01766420155763626
        vf_explained_var: 0.2598235607147217
        vf_loss: 12.645987510681152
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00028955520247109234
        entropy: 0.9847797155380249
        entropy_coeff: 0.0017600000137463212
        kl: 0.01164002250880003
        model: {}
        policy_loss: -0.029014432802796364
        total_loss: -0.028030114248394966
        vf_explained_var: 0.09077014029026031
        vf_loss: 15.535301208496094
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00028955520247109234
        entropy: 0.9719716310501099
        entropy_coeff: 0.0017600000137463212
        kl: 0.013383959420025349
        model: {}
        policy_loss: -0.032848816365003586
        total_loss: -0.031704287976026535
        vf_explained_var: 0.11195504665374756
        vf_loss: 15.16794490814209
    load_time_ms: 16054.794
    num_steps_sampled: 15648000
    num_steps_trained: 15648000
    sample_time_ms: 91533.727
    update_time_ms: 25.897
  iterations_since_restore: 143
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.8191011235955
    ram_util_percent: 14.351123595505621
  pid: 4061
  policy_reward_max:
    agent-0: 151.33333333333334
    agent-1: 151.33333333333334
    agent-2: 151.33333333333334
    agent-3: 151.33333333333334
    agent-4: 151.33333333333334
    agent-5: 151.33333333333334
  policy_reward_mean:
    agent-0: 114.25333333333376
    agent-1: 114.25333333333376
    agent-2: 114.25333333333376
    agent-3: 114.25333333333376
    agent-4: 114.25333333333376
    agent-5: 114.25333333333376
  policy_reward_min:
    agent-0: 28.333333333333393
    agent-1: 28.333333333333393
    agent-2: 28.333333333333393
    agent-3: 28.333333333333393
    agent-4: 28.333333333333393
    agent-5: 28.333333333333393
  sampler_perf:
    mean_env_wait_ms: 23.62761367350772
    mean_inference_ms: 12.316190164042169
    mean_processing_ms: 50.89998023120707
  time_since_restore: 18805.08376264572
  time_this_iter_s: 124.91358947753906
  time_total_s: 22016.147448778152
  timestamp: 1637036351
  timesteps_since_restore: 13728000
  timesteps_this_iter: 96000
  timesteps_total: 15648000
  training_iteration: 163
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    163 |          22016.1 | 15648000 |   685.52 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 48
    apples_agent-0_mean: 3.29
    apples_agent-0_min: 0
    apples_agent-1_max: 102
    apples_agent-1_mean: 25.46
    apples_agent-1_min: 0
    apples_agent-2_max: 84
    apples_agent-2_mean: 11.69
    apples_agent-2_min: 0
    apples_agent-3_max: 114
    apples_agent-3_mean: 68.79
    apples_agent-3_min: 26
    apples_agent-4_max: 79
    apples_agent-4_mean: 1.69
    apples_agent-4_min: 0
    apples_agent-5_max: 206
    apples_agent-5_mean: 84.0
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 447
    cleaning_beam_agent-0_mean: 346.75
    cleaning_beam_agent-0_min: 222
    cleaning_beam_agent-1_max: 301
    cleaning_beam_agent-1_mean: 187.16
    cleaning_beam_agent-1_min: 94
    cleaning_beam_agent-2_max: 341
    cleaning_beam_agent-2_mean: 212.59
    cleaning_beam_agent-2_min: 75
    cleaning_beam_agent-3_max: 189
    cleaning_beam_agent-3_mean: 44.71
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 483
    cleaning_beam_agent-4_mean: 339.84
    cleaning_beam_agent-4_min: 139
    cleaning_beam_agent-5_max: 694
    cleaning_beam_agent-5_mean: 130.11
    cleaning_beam_agent-5_min: 18
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-21-17
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 941.9999999999775
  episode_reward_mean: 678.909999999992
  episode_reward_min: 222.9999999999979
  episodes_this_iter: 96
  episodes_total: 15744
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20198.688
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002835647901520133
        entropy: 1.0823636054992676
        entropy_coeff: 0.0017600000137463212
        kl: 0.010488123632967472
        model: {}
        policy_loss: -0.026209762319922447
        total_loss: -0.025479741394519806
        vf_explained_var: 0.039045706391334534
        vf_loss: 15.861693382263184
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002835647901520133
        entropy: 1.1122143268585205
        entropy_coeff: 0.0017600000137463212
        kl: 0.012638367712497711
        model: {}
        policy_loss: -0.03164111450314522
        total_loss: -0.030690357089042664
        vf_explained_var: 0.0057180821895599365
        vf_loss: 16.444189071655273
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.0002835647901520133
        entropy: 1.1284959316253662
        entropy_coeff: 0.0017600000137463212
        kl: 0.01060219295322895
        model: {}
        policy_loss: -0.027565551921725273
        total_loss: -0.026444314047694206
        vf_explained_var: 0.08154003322124481
        vf_loss: 15.170611381530762
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002835647901520133
        entropy: 0.5742511749267578
        entropy_coeff: 0.0017600000137463212
        kl: 0.007632070686668158
        model: {}
        policy_loss: -0.018555287271738052
        total_loss: -0.017517440021038055
        vf_explained_var: 0.22246894240379333
        vf_loss: 12.853191375732422
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002835647901520133
        entropy: 0.9852161407470703
        entropy_coeff: 0.0017600000137463212
        kl: 0.011455930769443512
        model: {}
        policy_loss: -0.028392747044563293
        total_loss: -0.027457352727651596
        vf_explained_var: 0.07700958847999573
        vf_loss: 15.23783016204834
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002835647901520133
        entropy: 0.9632349014282227
        entropy_coeff: 0.0017600000137463212
        kl: 0.011841640807688236
        model: {}
        policy_loss: -0.030143078416585922
        total_loss: -0.029161151498556137
        vf_explained_var: 0.09659443795681
        vf_loss: 14.930560111999512
    load_time_ms: 15962.169
    num_steps_sampled: 15744000
    num_steps_trained: 15744000
    sample_time_ms: 91602.053
    update_time_ms: 26.943
  iterations_since_restore: 144
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.627374301675978
    ram_util_percent: 14.186033519553074
  pid: 4061
  policy_reward_max:
    agent-0: 156.99999999999972
    agent-1: 156.99999999999972
    agent-2: 156.99999999999972
    agent-3: 156.99999999999972
    agent-4: 156.99999999999972
    agent-5: 156.99999999999972
  policy_reward_mean:
    agent-0: 113.15166666666708
    agent-1: 113.15166666666708
    agent-2: 113.15166666666708
    agent-3: 113.15166666666708
    agent-4: 113.15166666666708
    agent-5: 113.15166666666708
  policy_reward_min:
    agent-0: 37.166666666666664
    agent-1: 37.166666666666664
    agent-2: 37.166666666666664
    agent-3: 37.166666666666664
    agent-4: 37.166666666666664
    agent-5: 37.166666666666664
  sampler_perf:
    mean_env_wait_ms: 23.630767147135312
    mean_inference_ms: 12.316931280440292
    mean_processing_ms: 50.902010711115345
  time_since_restore: 18929.913950443268
  time_this_iter_s: 124.83018779754639
  time_total_s: 22140.9776365757
  timestamp: 1637036477
  timesteps_since_restore: 13824000
  timesteps_this_iter: 96000
  timesteps_total: 15744000
  training_iteration: 164
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 19.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    164 |            22141 | 15744000 |   678.91 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 57
    apples_agent-0_mean: 3.78
    apples_agent-0_min: 0
    apples_agent-1_max: 93
    apples_agent-1_mean: 24.68
    apples_agent-1_min: 0
    apples_agent-2_max: 71
    apples_agent-2_mean: 8.82
    apples_agent-2_min: 0
    apples_agent-3_max: 124
    apples_agent-3_mean: 68.7
    apples_agent-3_min: 38
    apples_agent-4_max: 35
    apples_agent-4_mean: 0.83
    apples_agent-4_min: 0
    apples_agent-5_max: 168
    apples_agent-5_mean: 86.98
    apples_agent-5_min: 3
    cleaning_beam_agent-0_max: 486
    cleaning_beam_agent-0_mean: 343.56
    cleaning_beam_agent-0_min: 195
    cleaning_beam_agent-1_max: 295
    cleaning_beam_agent-1_mean: 189.64
    cleaning_beam_agent-1_min: 100
    cleaning_beam_agent-2_max: 402
    cleaning_beam_agent-2_mean: 218.5
    cleaning_beam_agent-2_min: 73
    cleaning_beam_agent-3_max: 161
    cleaning_beam_agent-3_mean: 40.63
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 468
    cleaning_beam_agent-4_mean: 344.32
    cleaning_beam_agent-4_min: 248
    cleaning_beam_agent-5_max: 672
    cleaning_beam_agent-5_mean: 111.64
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-23-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 954.9999999999869
  episode_reward_mean: 699.8999999999925
  episode_reward_min: 232.99999999999739
  episodes_this_iter: 96
  episodes_total: 15840
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20181.687
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002775744069367647
        entropy: 1.0771634578704834
        entropy_coeff: 0.0017600000137463212
        kl: 0.010297644883394241
        model: {}
        policy_loss: -0.02695152349770069
        total_loss: -0.026252873241901398
        vf_explained_var: 0.06084887683391571
        vf_loss: 15.646883010864258
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002775744069367647
        entropy: 1.0993796586990356
        entropy_coeff: 0.0017600000137463212
        kl: 0.01186428777873516
        model: {}
        policy_loss: -0.030830953270196915
        total_loss: -0.02992156334221363
        vf_explained_var: 0.006410658359527588
        vf_loss: 16.578655242919922
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.0002775744069367647
        entropy: 1.1213737726211548
        entropy_coeff: 0.0017600000137463212
        kl: 0.010525190271437168
        model: {}
        policy_loss: -0.027819780632853508
        total_loss: -0.026649251580238342
        vf_explained_var: 0.06092914938926697
        vf_loss: 15.65369987487793
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002775744069367647
        entropy: 0.5570440292358398
        entropy_coeff: 0.0017600000137463212
        kl: 0.007097011432051659
        model: {}
        policy_loss: -0.018021343275904655
        total_loss: -0.017015624791383743
        vf_explained_var: 0.23418349027633667
        vf_loss: 12.76414680480957
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002775744069367647
        entropy: 0.9779390692710876
        entropy_coeff: 0.0017600000137463212
        kl: 0.01174426544457674
        model: {}
        policy_loss: -0.02888343669474125
        total_loss: -0.02792479284107685
        vf_explained_var: 0.096303790807724
        vf_loss: 15.053924560546875
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002775744069367647
        entropy: 0.9592502117156982
        entropy_coeff: 0.0017600000137463212
        kl: 0.012587219476699829
        model: {}
        policy_loss: -0.030678432434797287
        total_loss: -0.029601994901895523
        vf_explained_var: 0.09625260531902313
        vf_loss: 15.059995651245117
    load_time_ms: 15965.835
    num_steps_sampled: 15840000
    num_steps_trained: 15840000
    sample_time_ms: 91514.039
    update_time_ms: 26.134
  iterations_since_restore: 145
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.353714285714286
    ram_util_percent: 9.667428571428573
  pid: 4061
  policy_reward_max:
    agent-0: 159.16666666666654
    agent-1: 159.16666666666654
    agent-2: 159.16666666666654
    agent-3: 159.16666666666654
    agent-4: 159.16666666666654
    agent-5: 159.16666666666654
  policy_reward_mean:
    agent-0: 116.6500000000004
    agent-1: 116.6500000000004
    agent-2: 116.6500000000004
    agent-3: 116.6500000000004
    agent-4: 116.6500000000004
    agent-5: 116.6500000000004
  policy_reward_min:
    agent-0: 38.83333333333332
    agent-1: 38.83333333333332
    agent-2: 38.83333333333332
    agent-3: 38.83333333333332
    agent-4: 38.83333333333332
    agent-5: 38.83333333333332
  sampler_perf:
    mean_env_wait_ms: 23.62989010635824
    mean_inference_ms: 12.315946013341017
    mean_processing_ms: 50.8971991516205
  time_since_restore: 19053.05601167679
  time_this_iter_s: 123.14206123352051
  time_total_s: 22264.11969780922
  timestamp: 1637036600
  timesteps_since_restore: 13920000
  timesteps_this_iter: 96000
  timesteps_total: 15840000
  training_iteration: 165
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    165 |          22264.1 | 15840000 |    699.9 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 52
    apples_agent-0_mean: 4.96
    apples_agent-0_min: 0
    apples_agent-1_max: 96
    apples_agent-1_mean: 23.32
    apples_agent-1_min: 0
    apples_agent-2_max: 83
    apples_agent-2_mean: 6.36
    apples_agent-2_min: 0
    apples_agent-3_max: 118
    apples_agent-3_mean: 68.47
    apples_agent-3_min: 18
    apples_agent-4_max: 109
    apples_agent-4_mean: 4.85
    apples_agent-4_min: 0
    apples_agent-5_max: 171
    apples_agent-5_mean: 77.81
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 477
    cleaning_beam_agent-0_mean: 344.36
    cleaning_beam_agent-0_min: 216
    cleaning_beam_agent-1_max: 306
    cleaning_beam_agent-1_mean: 196.13
    cleaning_beam_agent-1_min: 93
    cleaning_beam_agent-2_max: 461
    cleaning_beam_agent-2_mean: 235.01
    cleaning_beam_agent-2_min: 101
    cleaning_beam_agent-3_max: 122
    cleaning_beam_agent-3_mean: 41.47
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 460
    cleaning_beam_agent-4_mean: 338.09
    cleaning_beam_agent-4_min: 141
    cleaning_beam_agent-5_max: 793
    cleaning_beam_agent-5_mean: 152.54
    cleaning_beam_agent-5_min: 19
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-25-25
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 886.9999999999901
  episode_reward_mean: 671.7999999999917
  episode_reward_min: 174.99999999999878
  episodes_this_iter: 96
  episodes_total: 15936
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20156.317
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002715839946176857
        entropy: 1.0684391260147095
        entropy_coeff: 0.0017600000137463212
        kl: 0.009788673371076584
        model: {}
        policy_loss: -0.025705980136990547
        total_loss: -0.024989251047372818
        vf_explained_var: 0.05800442397594452
        vf_loss: 16.18312644958496
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002715839946176857
        entropy: 1.1152141094207764
        entropy_coeff: 0.0017600000137463212
        kl: 0.012000094167888165
        model: {}
        policy_loss: -0.03006773442029953
        total_loss: -0.029095355421304703
        vf_explained_var: -0.010354474186897278
        vf_loss: 17.351428985595703
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.0002715839946176857
        entropy: 1.1131494045257568
        entropy_coeff: 0.0017600000137463212
        kl: 0.010958418250083923
        model: {}
        policy_loss: -0.027783896774053574
        total_loss: -0.026517610996961594
        vf_explained_var: 0.07833504676818848
        vf_loss: 15.816686630249023
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002715839946176857
        entropy: 0.5535221099853516
        entropy_coeff: 0.0017600000137463212
        kl: 0.007338720373809338
        model: {}
        policy_loss: -0.018110446631908417
        total_loss: -0.017059694975614548
        vf_explained_var: 0.2484348863363266
        vf_loss: 12.910785675048828
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002715839946176857
        entropy: 0.9919551014900208
        entropy_coeff: 0.0017600000137463212
        kl: 0.011768410913646221
        model: {}
        policy_loss: -0.028619347140192986
        total_loss: -0.027696680277585983
        vf_explained_var: 0.13116349279880524
        vf_loss: 14.916681289672852
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002715839946176857
        entropy: 0.9626421928405762
        entropy_coeff: 0.0017600000137463212
        kl: 0.011960883624851704
        model: {}
        policy_loss: -0.03059353306889534
        total_loss: -0.029580581933259964
        vf_explained_var: 0.12004120647907257
        vf_loss: 15.111143112182617
    load_time_ms: 16080.596
    num_steps_sampled: 15936000
    num_steps_trained: 15936000
    sample_time_ms: 91427.829
    update_time_ms: 26.091
  iterations_since_restore: 146
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.191011235955056
    ram_util_percent: 9.600000000000001
  pid: 4061
  policy_reward_max:
    agent-0: 147.8333333333332
    agent-1: 147.8333333333332
    agent-2: 147.8333333333332
    agent-3: 147.8333333333332
    agent-4: 147.8333333333332
    agent-5: 147.8333333333332
  policy_reward_mean:
    agent-0: 111.9666666666671
    agent-1: 111.9666666666671
    agent-2: 111.9666666666671
    agent-3: 111.9666666666671
    agent-4: 111.9666666666671
    agent-5: 111.9666666666671
  policy_reward_min:
    agent-0: 29.16666666666672
    agent-1: 29.16666666666672
    agent-2: 29.16666666666672
    agent-3: 29.16666666666672
    agent-4: 29.16666666666672
    agent-5: 29.16666666666672
  sampler_perf:
    mean_env_wait_ms: 23.63034352386228
    mean_inference_ms: 12.314923371140868
    mean_processing_ms: 50.892869163670284
  time_since_restore: 19177.62901854515
  time_this_iter_s: 124.57300686836243
  time_total_s: 22388.69270467758
  timestamp: 1637036725
  timesteps_since_restore: 14016000
  timesteps_this_iter: 96000
  timesteps_total: 15936000
  training_iteration: 166
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    166 |          22388.7 | 15936000 |    671.8 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 49
    apples_agent-0_mean: 5.95
    apples_agent-0_min: 0
    apples_agent-1_max: 119
    apples_agent-1_mean: 23.05
    apples_agent-1_min: 0
    apples_agent-2_max: 69
    apples_agent-2_mean: 10.94
    apples_agent-2_min: 0
    apples_agent-3_max: 140
    apples_agent-3_mean: 70.1
    apples_agent-3_min: 40
    apples_agent-4_max: 37
    apples_agent-4_mean: 0.69
    apples_agent-4_min: 0
    apples_agent-5_max: 171
    apples_agent-5_mean: 82.88
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 480
    cleaning_beam_agent-0_mean: 343.77
    cleaning_beam_agent-0_min: 200
    cleaning_beam_agent-1_max: 313
    cleaning_beam_agent-1_mean: 196.78
    cleaning_beam_agent-1_min: 102
    cleaning_beam_agent-2_max: 461
    cleaning_beam_agent-2_mean: 222.15
    cleaning_beam_agent-2_min: 66
    cleaning_beam_agent-3_max: 84
    cleaning_beam_agent-3_mean: 36.91
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 525
    cleaning_beam_agent-4_mean: 341.52
    cleaning_beam_agent-4_min: 235
    cleaning_beam_agent-5_max: 651
    cleaning_beam_agent-5_mean: 124.44
    cleaning_beam_agent-5_min: 23
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 3
    fire_beam_agent-1_mean: 0.05
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-27-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 956.9999999999726
  episode_reward_mean: 690.999999999991
  episode_reward_min: 267.99999999999557
  episodes_this_iter: 96
  episodes_total: 16032
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20170.406
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002655936114024371
        entropy: 1.0795972347259521
        entropy_coeff: 0.0017600000137463212
        kl: 0.009963784366846085
        model: {}
        policy_loss: -0.025003153830766678
        total_loss: -0.02436847984790802
        vf_explained_var: 0.11013945937156677
        vf_loss: 15.383846282958984
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002655936114024371
        entropy: 1.126596212387085
        entropy_coeff: 0.0017600000137463212
        kl: 0.012081535533070564
        model: {}
        policy_loss: -0.03022502176463604
        total_loss: -0.029342764988541603
        vf_explained_var: 0.04271706938743591
        vf_loss: 16.56915855407715
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.0002655936114024371
        entropy: 1.1164308786392212
        entropy_coeff: 0.0017600000137463212
        kl: 0.010166347026824951
        model: {}
        policy_loss: -0.027477264404296875
        total_loss: -0.02632051706314087
        vf_explained_var: 0.07637752592563629
        vf_loss: 15.96714973449707
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002655936114024371
        entropy: 0.5480263233184814
        entropy_coeff: 0.0017600000137463212
        kl: 0.007240848150104284
        model: {}
        policy_loss: -0.017832359299063683
        total_loss: -0.016815975308418274
        vf_explained_var: 0.2728305757045746
        vf_loss: 12.568264961242676
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002655936114024371
        entropy: 0.9688411951065063
        entropy_coeff: 0.0017600000137463212
        kl: 0.011363811790943146
        model: {}
        policy_loss: -0.028589095920324326
        total_loss: -0.027593575417995453
        vf_explained_var: 0.09480500221252441
        vf_loss: 15.643013954162598
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002655936114024371
        entropy: 0.9592430591583252
        entropy_coeff: 0.0017600000137463212
        kl: 0.01181774865835905
        model: {}
        policy_loss: -0.03010878525674343
        total_loss: -0.02916262298822403
        vf_explained_var: 0.159913569688797
        vf_loss: 14.526583671569824
    load_time_ms: 15979.226
    num_steps_sampled: 16032000
    num_steps_trained: 16032000
    sample_time_ms: 91159.934
    update_time_ms: 26.544
  iterations_since_restore: 147
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.419653179190753
    ram_util_percent: 9.666473988439307
  pid: 4061
  policy_reward_max:
    agent-0: 159.50000000000003
    agent-1: 159.50000000000003
    agent-2: 159.50000000000003
    agent-3: 159.50000000000003
    agent-4: 159.50000000000003
    agent-5: 159.50000000000003
  policy_reward_mean:
    agent-0: 115.16666666666708
    agent-1: 115.16666666666708
    agent-2: 115.16666666666708
    agent-3: 115.16666666666708
    agent-4: 115.16666666666708
    agent-5: 115.16666666666708
  policy_reward_min:
    agent-0: 44.66666666666655
    agent-1: 44.66666666666655
    agent-2: 44.66666666666655
    agent-3: 44.66666666666655
    agent-4: 44.66666666666655
    agent-5: 44.66666666666655
  sampler_perf:
    mean_env_wait_ms: 23.629923617453475
    mean_inference_ms: 12.31380379059105
    mean_processing_ms: 50.88745459675703
  time_since_restore: 19299.246146917343
  time_this_iter_s: 121.61712837219238
  time_total_s: 22510.309833049774
  timestamp: 1637036846
  timesteps_since_restore: 14112000
  timesteps_this_iter: 96000
  timesteps_total: 16032000
  training_iteration: 167
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    167 |          22510.3 | 16032000 |      691 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 51
    apples_agent-0_mean: 4.78
    apples_agent-0_min: 0
    apples_agent-1_max: 90
    apples_agent-1_mean: 21.08
    apples_agent-1_min: 0
    apples_agent-2_max: 92
    apples_agent-2_mean: 9.61
    apples_agent-2_min: 0
    apples_agent-3_max: 135
    apples_agent-3_mean: 68.87
    apples_agent-3_min: 27
    apples_agent-4_max: 145
    apples_agent-4_mean: 2.6
    apples_agent-4_min: 0
    apples_agent-5_max: 157
    apples_agent-5_mean: 83.54
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 480
    cleaning_beam_agent-0_mean: 342.45
    cleaning_beam_agent-0_min: 225
    cleaning_beam_agent-1_max: 360
    cleaning_beam_agent-1_mean: 196.65
    cleaning_beam_agent-1_min: 120
    cleaning_beam_agent-2_max: 404
    cleaning_beam_agent-2_mean: 224.24
    cleaning_beam_agent-2_min: 46
    cleaning_beam_agent-3_max: 91
    cleaning_beam_agent-3_mean: 35.68
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 452
    cleaning_beam_agent-4_mean: 349.0
    cleaning_beam_agent-4_min: 103
    cleaning_beam_agent-5_max: 810
    cleaning_beam_agent-5_mean: 129.48
    cleaning_beam_agent-5_min: 15
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-29-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 978.9999999999899
  episode_reward_mean: 704.3399999999909
  episode_reward_min: 400.00000000000335
  episodes_this_iter: 96
  episodes_total: 16128
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20177.807
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00025960319908335805
        entropy: 1.084177017211914
        entropy_coeff: 0.0017600000137463212
        kl: 0.009688954800367355
        model: {}
        policy_loss: -0.02530740574002266
        total_loss: -0.024776022881269455
        vf_explained_var: 0.07027412950992584
        vf_loss: 14.706405639648438
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00025960319908335805
        entropy: 1.1200705766677856
        entropy_coeff: 0.0017600000137463212
        kl: 0.012275678105652332
        model: {}
        policy_loss: -0.030491903424263
        total_loss: -0.029601512476801872
        vf_explained_var: -0.030663669109344482
        vf_loss: 16.341506958007812
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.00025960319908335805
        entropy: 1.129155158996582
        entropy_coeff: 0.0017600000137463212
        kl: 0.01072525791823864
        model: {}
        policy_loss: -0.026793532073497772
        total_loss: -0.025695763528347015
        vf_explained_var: 0.06639513373374939
        vf_loss: 14.762943267822266
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00025960319908335805
        entropy: 0.5163862705230713
        entropy_coeff: 0.0017600000137463212
        kl: 0.006942571606487036
        model: {}
        policy_loss: -0.016278592869639397
        total_loss: -0.01524773333221674
        vf_explained_var: 0.21282246708869934
        vf_loss: 12.454401016235352
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00025960319908335805
        entropy: 0.9687821865081787
        entropy_coeff: 0.0017600000137463212
        kl: 0.011425326578319073
        model: {}
        policy_loss: -0.028224658221006393
        total_loss: -0.027345655485987663
        vf_explained_var: 0.08899474143981934
        vf_loss: 14.415271759033203
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00025960319908335805
        entropy: 0.9479107856750488
        entropy_coeff: 0.0017600000137463212
        kl: 0.011311128735542297
        model: {}
        policy_loss: -0.029136549681425095
        total_loss: -0.028274957090616226
        vf_explained_var: 0.11622832715511322
        vf_loss: 13.988029479980469
    load_time_ms: 16072.642
    num_steps_sampled: 16128000
    num_steps_trained: 16128000
    sample_time_ms: 90930.8
    update_time_ms: 26.665
  iterations_since_restore: 148
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.831284916201119
    ram_util_percent: 9.664804469273744
  pid: 4061
  policy_reward_max:
    agent-0: 163.1666666666666
    agent-1: 163.1666666666666
    agent-2: 163.1666666666666
    agent-3: 163.1666666666666
    agent-4: 163.1666666666666
    agent-5: 163.1666666666666
  policy_reward_mean:
    agent-0: 117.39000000000041
    agent-1: 117.39000000000041
    agent-2: 117.39000000000041
    agent-3: 117.39000000000041
    agent-4: 117.39000000000041
    agent-5: 117.39000000000041
  policy_reward_min:
    agent-0: 66.66666666666643
    agent-1: 66.66666666666643
    agent-2: 66.66666666666643
    agent-3: 66.66666666666643
    agent-4: 66.66666666666643
    agent-5: 66.66666666666643
  sampler_perf:
    mean_env_wait_ms: 23.630829351747334
    mean_inference_ms: 12.312895033858442
    mean_processing_ms: 50.883787831654935
  time_since_restore: 19424.479291677475
  time_this_iter_s: 125.23314476013184
  time_total_s: 22635.542977809906
  timestamp: 1637036972
  timesteps_since_restore: 14208000
  timesteps_this_iter: 96000
  timesteps_total: 16128000
  training_iteration: 168
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    168 |          22635.5 | 16128000 |   704.34 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 41
    apples_agent-0_mean: 3.8
    apples_agent-0_min: 0
    apples_agent-1_max: 114
    apples_agent-1_mean: 21.58
    apples_agent-1_min: 0
    apples_agent-2_max: 58
    apples_agent-2_mean: 10.54
    apples_agent-2_min: 0
    apples_agent-3_max: 126
    apples_agent-3_mean: 67.87
    apples_agent-3_min: 27
    apples_agent-4_max: 45
    apples_agent-4_mean: 1.75
    apples_agent-4_min: 0
    apples_agent-5_max: 209
    apples_agent-5_mean: 87.37
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 466
    cleaning_beam_agent-0_mean: 341.38
    cleaning_beam_agent-0_min: 234
    cleaning_beam_agent-1_max: 344
    cleaning_beam_agent-1_mean: 188.9
    cleaning_beam_agent-1_min: 100
    cleaning_beam_agent-2_max: 358
    cleaning_beam_agent-2_mean: 212.91
    cleaning_beam_agent-2_min: 88
    cleaning_beam_agent-3_max: 138
    cleaning_beam_agent-3_mean: 37.87
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 551
    cleaning_beam_agent-4_mean: 344.25
    cleaning_beam_agent-4_min: 157
    cleaning_beam_agent-5_max: 723
    cleaning_beam_agent-5_mean: 97.46
    cleaning_beam_agent-5_min: 14
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-31-39
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 899.9999999999694
  episode_reward_mean: 693.4099999999921
  episode_reward_min: 321.00000000000256
  episodes_this_iter: 96
  episodes_total: 16224
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20170.249
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000253612786764279
        entropy: 1.095198154449463
        entropy_coeff: 0.0017600000137463212
        kl: 0.010114526376128197
        model: {}
        policy_loss: -0.025228817015886307
        total_loss: -0.02443208545446396
        vf_explained_var: 0.049277231097221375
        vf_loss: 17.1282958984375
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000253612786764279
        entropy: 1.120186686515808
        entropy_coeff: 0.0017600000137463212
        kl: 0.01168896909803152
        model: {}
        policy_loss: -0.03018021583557129
        total_loss: -0.029230531305074692
        vf_explained_var: 0.02749420702457428
        vf_loss: 17.523155212402344
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.000253612786764279
        entropy: 1.1053119897842407
        entropy_coeff: 0.0017600000137463212
        kl: 0.010341786779463291
        model: {}
        policy_loss: -0.02712508663535118
        total_loss: -0.02593347430229187
        vf_explained_var: 0.11956143379211426
        vf_loss: 15.85689926147461
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000253612786764279
        entropy: 0.5591468811035156
        entropy_coeff: 0.0017600000137463212
        kl: 0.007594067603349686
        model: {}
        policy_loss: -0.018227949738502502
        total_loss: -0.01714448258280754
        vf_explained_var: 0.27328869700431824
        vf_loss: 13.08159065246582
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000253612786764279
        entropy: 0.9741402268409729
        entropy_coeff: 0.0017600000137463212
        kl: 0.011320475488901138
        model: {}
        policy_loss: -0.027599338442087173
        total_loss: -0.0265467781573534
        vf_explained_var: 0.09197984635829926
        vf_loss: 16.349958419799805
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000253612786764279
        entropy: 0.989183783531189
        entropy_coeff: 0.0017600000137463212
        kl: 0.012102972716093063
        model: {}
        policy_loss: -0.03036661073565483
        total_loss: -0.02930077165365219
        vf_explained_var: 0.11418639123439789
        vf_loss: 15.96503734588623
    load_time_ms: 16150.113
    num_steps_sampled: 16224000
    num_steps_trained: 16224000
    sample_time_ms: 90158.658
    update_time_ms: 27.038
  iterations_since_restore: 149
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 16.080662983425412
    ram_util_percent: 9.799447513812156
  pid: 4061
  policy_reward_max:
    agent-0: 150.00000000000009
    agent-1: 150.00000000000009
    agent-2: 150.00000000000009
    agent-3: 150.00000000000009
    agent-4: 150.00000000000009
    agent-5: 150.00000000000009
  policy_reward_mean:
    agent-0: 115.5683333333337
    agent-1: 115.5683333333337
    agent-2: 115.5683333333337
    agent-3: 115.5683333333337
    agent-4: 115.5683333333337
    agent-5: 115.5683333333337
  policy_reward_min:
    agent-0: 53.499999999999815
    agent-1: 53.499999999999815
    agent-2: 53.499999999999815
    agent-3: 53.499999999999815
    agent-4: 53.499999999999815
    agent-5: 53.499999999999815
  sampler_perf:
    mean_env_wait_ms: 23.631427694822282
    mean_inference_ms: 12.312971660174151
    mean_processing_ms: 50.88400776833291
  time_since_restore: 19551.659909009933
  time_this_iter_s: 127.1806173324585
  time_total_s: 22762.723595142365
  timestamp: 1637037099
  timesteps_since_restore: 14304000
  timesteps_this_iter: 96000
  timesteps_total: 16224000
  training_iteration: 169
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    169 |          22762.7 | 16224000 |   693.41 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 39
    apples_agent-0_mean: 3.71
    apples_agent-0_min: 0
    apples_agent-1_max: 129
    apples_agent-1_mean: 25.83
    apples_agent-1_min: 0
    apples_agent-2_max: 230
    apples_agent-2_mean: 15.52
    apples_agent-2_min: 0
    apples_agent-3_max: 176
    apples_agent-3_mean: 71.64
    apples_agent-3_min: 32
    apples_agent-4_max: 60
    apples_agent-4_mean: 2.3
    apples_agent-4_min: 0
    apples_agent-5_max: 143
    apples_agent-5_mean: 86.38
    apples_agent-5_min: 7
    cleaning_beam_agent-0_max: 484
    cleaning_beam_agent-0_mean: 345.93
    cleaning_beam_agent-0_min: 237
    cleaning_beam_agent-1_max: 309
    cleaning_beam_agent-1_mean: 186.02
    cleaning_beam_agent-1_min: 90
    cleaning_beam_agent-2_max: 409
    cleaning_beam_agent-2_mean: 203.04
    cleaning_beam_agent-2_min: 68
    cleaning_beam_agent-3_max: 143
    cleaning_beam_agent-3_mean: 37.15
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 473
    cleaning_beam_agent-4_mean: 351.36
    cleaning_beam_agent-4_min: 240
    cleaning_beam_agent-5_max: 720
    cleaning_beam_agent-5_mean: 99.75
    cleaning_beam_agent-5_min: 15
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-33-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 929.9999999999818
  episode_reward_mean: 693.609999999991
  episode_reward_min: 254.9999999999952
  episodes_this_iter: 96
  episodes_total: 16320
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20170.551
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002476224035490304
        entropy: 1.0863182544708252
        entropy_coeff: 0.0017600000137463212
        kl: 0.009779087267816067
        model: {}
        policy_loss: -0.024459579959511757
        total_loss: -0.023786470293998718
        vf_explained_var: 0.0684289038181305
        vf_loss: 16.071239471435547
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002476224035490304
        entropy: 1.1092643737792969
        entropy_coeff: 0.0017600000137463212
        kl: 0.011346273124217987
        model: {}
        policy_loss: -0.029026206582784653
        total_loss: -0.02813684567809105
        vf_explained_var: 0.011900380253791809
        vf_loss: 17.07037925720215
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.0002476224035490304
        entropy: 1.1189664602279663
        entropy_coeff: 0.0017600000137463212
        kl: 0.010819848626852036
        model: {}
        policy_loss: -0.025968587026000023
        total_loss: -0.02465539053082466
        vf_explained_var: 0.03772228956222534
        vf_loss: 16.59598731994629
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002476224035490304
        entropy: 0.5545986890792847
        entropy_coeff: 0.0017600000137463212
        kl: 0.006905457004904747
        model: {}
        policy_loss: -0.017480449751019478
        total_loss: -0.01638791896402836
        vf_explained_var: 0.20061993598937988
        vf_loss: 13.780818939208984
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002476224035490304
        entropy: 0.966937243938446
        entropy_coeff: 0.0017600000137463212
        kl: 0.012889967299997807
        model: {}
        policy_loss: -0.023961931467056274
        total_loss: -0.02280920371413231
        vf_explained_var: 0.09234786033630371
        vf_loss: 15.655397415161133
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002476224035490304
        entropy: 0.9672286510467529
        entropy_coeff: 0.0017600000137463212
        kl: 0.010907615534961224
        model: {}
        policy_loss: -0.028593741357326508
        total_loss: -0.027665283530950546
        vf_explained_var: 0.10790620744228363
        vf_loss: 15.400215148925781
    load_time_ms: 14459.829
    num_steps_sampled: 16320000
    num_steps_trained: 16320000
    sample_time_ms: 89900.021
    update_time_ms: 26.146
  iterations_since_restore: 150
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.27683615819209
    ram_util_percent: 9.60677966101695
  pid: 4061
  policy_reward_max:
    agent-0: 155.0000000000003
    agent-1: 155.0000000000003
    agent-2: 155.0000000000003
    agent-3: 155.0000000000003
    agent-4: 155.0000000000003
    agent-5: 155.0000000000003
  policy_reward_mean:
    agent-0: 115.60166666666706
    agent-1: 115.60166666666706
    agent-2: 115.60166666666706
    agent-3: 115.60166666666706
    agent-4: 115.60166666666706
    agent-5: 115.60166666666706
  policy_reward_min:
    agent-0: 42.49999999999993
    agent-1: 42.49999999999993
    agent-2: 42.49999999999993
    agent-3: 42.49999999999993
    agent-4: 42.49999999999993
    agent-5: 42.49999999999993
  sampler_perf:
    mean_env_wait_ms: 23.632067708057164
    mean_inference_ms: 12.31244984682284
    mean_processing_ms: 50.88223229592222
  time_since_restore: 19675.55159139633
  time_this_iter_s: 123.89168238639832
  time_total_s: 22886.615277528763
  timestamp: 1637037223
  timesteps_since_restore: 14400000
  timesteps_this_iter: 96000
  timesteps_total: 16320000
  training_iteration: 170
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    170 |          22886.6 | 16320000 |   693.61 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 54
    apples_agent-0_mean: 3.42
    apples_agent-0_min: 0
    apples_agent-1_max: 73
    apples_agent-1_mean: 22.66
    apples_agent-1_min: 0
    apples_agent-2_max: 100
    apples_agent-2_mean: 11.07
    apples_agent-2_min: 0
    apples_agent-3_max: 140
    apples_agent-3_mean: 71.01
    apples_agent-3_min: 11
    apples_agent-4_max: 77
    apples_agent-4_mean: 1.71
    apples_agent-4_min: 0
    apples_agent-5_max: 172
    apples_agent-5_mean: 89.95
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 488
    cleaning_beam_agent-0_mean: 343.18
    cleaning_beam_agent-0_min: 222
    cleaning_beam_agent-1_max: 313
    cleaning_beam_agent-1_mean: 187.45
    cleaning_beam_agent-1_min: 79
    cleaning_beam_agent-2_max: 366
    cleaning_beam_agent-2_mean: 207.36
    cleaning_beam_agent-2_min: 46
    cleaning_beam_agent-3_max: 126
    cleaning_beam_agent-3_mean: 34.35
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 481
    cleaning_beam_agent-4_mean: 342.61
    cleaning_beam_agent-4_min: 98
    cleaning_beam_agent-5_max: 577
    cleaning_beam_agent-5_mean: 91.07
    cleaning_beam_agent-5_min: 13
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-35-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 935.999999999984
  episode_reward_mean: 706.999999999989
  episode_reward_min: 199.99999999999721
  episodes_this_iter: 96
  episodes_total: 16416
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20187.227
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002416320057818666
        entropy: 1.0749781131744385
        entropy_coeff: 0.0017600000137463212
        kl: 0.00939195416867733
        model: {}
        policy_loss: -0.0240926593542099
        total_loss: -0.02331719547510147
        vf_explained_var: 0.03847256302833557
        vf_loss: 17.2822322845459
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002416320057818666
        entropy: 1.1350609064102173
        entropy_coeff: 0.0017600000137463212
        kl: 0.011962327174842358
        model: {}
        policy_loss: -0.029879514127969742
        total_loss: -0.028872855007648468
        vf_explained_var: -0.004120662808418274
        vf_loss: 18.081350326538086
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.0002416320057818666
        entropy: 1.1112953424453735
        entropy_coeff: 0.0017600000137463212
        kl: 0.010198518633842468
        model: {}
        policy_loss: -0.026031482964754105
        total_loss: -0.02475295588374138
        vf_explained_var: 0.05137781798839569
        vf_loss: 17.046287536621094
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002416320057818666
        entropy: 0.5291153192520142
        entropy_coeff: 0.0017600000137463212
        kl: 0.006918597035109997
        model: {}
        policy_loss: -0.017950881272554398
        total_loss: -0.01680830307304859
        vf_explained_var: 0.2298276424407959
        vf_loss: 13.819622993469238
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002416320057818666
        entropy: 0.9669710993766785
        entropy_coeff: 0.0017600000137463212
        kl: 0.011695023626089096
        model: {}
        policy_loss: -0.02678433619439602
        total_loss: -0.02573116309940815
        vf_explained_var: 0.11691854894161224
        vf_loss: 15.855443954467773
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002416320057818666
        entropy: 0.9727127552032471
        entropy_coeff: 0.0017600000137463212
        kl: 0.012014161795377731
        model: {}
        policy_loss: -0.03006037510931492
        total_loss: -0.02908066101372242
        vf_explained_var: 0.16987594962120056
        vf_loss: 14.902721405029297
    load_time_ms: 14492.7
    num_steps_sampled: 16416000
    num_steps_trained: 16416000
    sample_time_ms: 89701.179
    update_time_ms: 21.491
  iterations_since_restore: 151
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.386781609195403
    ram_util_percent: 9.597126436781611
  pid: 4061
  policy_reward_max:
    agent-0: 156.00000000000026
    agent-1: 156.00000000000026
    agent-2: 156.00000000000026
    agent-3: 156.00000000000026
    agent-4: 156.00000000000026
    agent-5: 156.00000000000026
  policy_reward_mean:
    agent-0: 117.83333333333373
    agent-1: 117.83333333333373
    agent-2: 117.83333333333373
    agent-3: 117.83333333333373
    agent-4: 117.83333333333373
    agent-5: 117.83333333333373
  policy_reward_min:
    agent-0: 33.33333333333339
    agent-1: 33.33333333333339
    agent-2: 33.33333333333339
    agent-3: 33.33333333333339
    agent-4: 33.33333333333339
    agent-5: 33.33333333333339
  sampler_perf:
    mean_env_wait_ms: 23.630700440108523
    mean_inference_ms: 12.311590867547142
    mean_processing_ms: 50.87784552194051
  time_since_restore: 19797.668275117874
  time_this_iter_s: 122.11668372154236
  time_total_s: 23008.731961250305
  timestamp: 1637037346
  timesteps_since_restore: 14496000
  timesteps_this_iter: 96000
  timesteps_total: 16416000
  training_iteration: 171
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    171 |          23008.7 | 16416000 |      707 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 30
    apples_agent-0_mean: 2.9
    apples_agent-0_min: 0
    apples_agent-1_max: 114
    apples_agent-1_mean: 30.66
    apples_agent-1_min: 0
    apples_agent-2_max: 72
    apples_agent-2_mean: 8.07
    apples_agent-2_min: 0
    apples_agent-3_max: 101
    apples_agent-3_mean: 65.26
    apples_agent-3_min: 27
    apples_agent-4_max: 89
    apples_agent-4_mean: 1.68
    apples_agent-4_min: 0
    apples_agent-5_max: 172
    apples_agent-5_mean: 82.33
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 455
    cleaning_beam_agent-0_mean: 350.63
    cleaning_beam_agent-0_min: 219
    cleaning_beam_agent-1_max: 321
    cleaning_beam_agent-1_mean: 183.89
    cleaning_beam_agent-1_min: 91
    cleaning_beam_agent-2_max: 397
    cleaning_beam_agent-2_mean: 216.27
    cleaning_beam_agent-2_min: 75
    cleaning_beam_agent-3_max: 96
    cleaning_beam_agent-3_mean: 33.04
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 468
    cleaning_beam_agent-4_mean: 346.3
    cleaning_beam_agent-4_min: 172
    cleaning_beam_agent-5_max: 793
    cleaning_beam_agent-5_mean: 139.03
    cleaning_beam_agent-5_min: 14
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-37-49
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 938.9999999999718
  episode_reward_mean: 703.6799999999901
  episode_reward_min: 162.0000000000003
  episodes_this_iter: 96
  episodes_total: 16512
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20185.012
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00023564159346278757
        entropy: 1.0862479209899902
        entropy_coeff: 0.0017600000137463212
        kl: 0.008913414552807808
        model: {}
        policy_loss: -0.023225871846079826
        total_loss: -0.022552400827407837
        vf_explained_var: 0.038528770208358765
        vf_loss: 16.939245223999023
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00023564159346278757
        entropy: 1.1097865104675293
        entropy_coeff: 0.0017600000137463212
        kl: 0.012038620188832283
        model: {}
        policy_loss: -0.029364751651883125
        total_loss: -0.02832522988319397
        vf_explained_var: -0.014738649129867554
        vf_loss: 17.888824462890625
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.00023564159346278757
        entropy: 1.127616286277771
        entropy_coeff: 0.0017600000137463212
        kl: 0.009596237912774086
        model: {}
        policy_loss: -0.025422418490052223
        total_loss: -0.024336297065019608
        vf_explained_var: 0.07386237382888794
        vf_loss: 16.31289291381836
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00023564159346278757
        entropy: 0.5237676501274109
        entropy_coeff: 0.0017600000137463212
        kl: 0.00696107791736722
        model: {}
        policy_loss: -0.016973596066236496
        total_loss: -0.01589091308414936
        vf_explained_var: 0.25747668743133545
        vf_loss: 13.08407974243164
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00023564159346278757
        entropy: 0.9647049903869629
        entropy_coeff: 0.0017600000137463212
        kl: 0.010224426165223122
        model: {}
        policy_loss: -0.02620898000895977
        total_loss: -0.025395464152097702
        vf_explained_var: 0.15541130304336548
        vf_loss: 14.889497756958008
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00023564159346278757
        entropy: 0.9422774314880371
        entropy_coeff: 0.0017600000137463212
        kl: 0.011292152106761932
        model: {}
        policy_loss: -0.02853425405919552
        total_loss: -0.027470998466014862
        vf_explained_var: 0.09633909165859222
        vf_loss: 15.924508094787598
    load_time_ms: 14327.79
    num_steps_sampled: 16512000
    num_steps_trained: 16512000
    sample_time_ms: 89459.069
    update_time_ms: 21.543
  iterations_since_restore: 152
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.400568181818182
    ram_util_percent: 9.671022727272726
  pid: 4061
  policy_reward_max:
    agent-0: 156.49999999999943
    agent-1: 156.49999999999943
    agent-2: 156.49999999999943
    agent-3: 156.49999999999943
    agent-4: 156.49999999999943
    agent-5: 156.49999999999943
  policy_reward_mean:
    agent-0: 117.2800000000004
    agent-1: 117.2800000000004
    agent-2: 117.2800000000004
    agent-3: 117.2800000000004
    agent-4: 117.2800000000004
    agent-5: 117.2800000000004
  policy_reward_min:
    agent-0: 27.00000000000004
    agent-1: 27.00000000000004
    agent-2: 27.00000000000004
    agent-3: 27.00000000000004
    agent-4: 27.00000000000004
    agent-5: 27.00000000000004
  sampler_perf:
    mean_env_wait_ms: 23.63127306444665
    mean_inference_ms: 12.310776117798332
    mean_processing_ms: 50.87435582894192
  time_since_restore: 19921.096029281616
  time_this_iter_s: 123.42775416374207
  time_total_s: 23132.159715414047
  timestamp: 1637037469
  timesteps_since_restore: 14592000
  timesteps_this_iter: 96000
  timesteps_total: 16512000
  training_iteration: 172
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    172 |          23132.2 | 16512000 |   703.68 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 37
    apples_agent-0_mean: 2.9
    apples_agent-0_min: 0
    apples_agent-1_max: 71
    apples_agent-1_mean: 25.1
    apples_agent-1_min: 0
    apples_agent-2_max: 243
    apples_agent-2_mean: 12.9
    apples_agent-2_min: 0
    apples_agent-3_max: 126
    apples_agent-3_mean: 65.03
    apples_agent-3_min: 25
    apples_agent-4_max: 70
    apples_agent-4_mean: 1.57
    apples_agent-4_min: 0
    apples_agent-5_max: 165
    apples_agent-5_mean: 78.25
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 449
    cleaning_beam_agent-0_mean: 343.65
    cleaning_beam_agent-0_min: 233
    cleaning_beam_agent-1_max: 321
    cleaning_beam_agent-1_mean: 186.33
    cleaning_beam_agent-1_min: 87
    cleaning_beam_agent-2_max: 397
    cleaning_beam_agent-2_mean: 197.25
    cleaning_beam_agent-2_min: 75
    cleaning_beam_agent-3_max: 108
    cleaning_beam_agent-3_mean: 39.45
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 476
    cleaning_beam_agent-4_mean: 342.77
    cleaning_beam_agent-4_min: 223
    cleaning_beam_agent-5_max: 802
    cleaning_beam_agent-5_mean: 158.22
    cleaning_beam_agent-5_min: 13
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-39-54
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 871.9999999999891
  episode_reward_mean: 691.3699999999903
  episode_reward_min: 351.00000000000455
  episodes_this_iter: 96
  episodes_total: 16608
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20187.469
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00022965119569562376
        entropy: 1.0859653949737549
        entropy_coeff: 0.0017600000137463212
        kl: 0.00915702898055315
        model: {}
        policy_loss: -0.023097340017557144
        total_loss: -0.022619839757680893
        vf_explained_var: 0.05576896667480469
        vf_loss: 14.730953216552734
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00022965119569562376
        entropy: 1.10390305519104
        entropy_coeff: 0.0017600000137463212
        kl: 0.010918490588665009
        model: {}
        policy_loss: -0.027859307825565338
        total_loss: -0.027115266770124435
        vf_explained_var: -0.022406205534934998
        vf_loss: 15.950546264648438
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.00022965119569562376
        entropy: 1.112146019935608
        entropy_coeff: 0.0017600000137463212
        kl: 0.009273827075958252
        model: {}
        policy_loss: -0.025059837847948074
        total_loss: -0.02415981888771057
        vf_explained_var: 0.06071953475475311
        vf_loss: 14.663211822509766
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00022965119569562376
        entropy: 0.5188535451889038
        entropy_coeff: 0.0017600000137463212
        kl: 0.0062469420954585075
        model: {}
        policy_loss: -0.01595219224691391
        total_loss: -0.014986826106905937
        vf_explained_var: 0.19696402549743652
        vf_loss: 12.538541793823242
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00022965119569562376
        entropy: 0.9593117237091064
        entropy_coeff: 0.0017600000137463212
        kl: 0.010515663772821426
        model: {}
        policy_loss: -0.025851108133792877
        total_loss: -0.025009624660015106
        vf_explained_var: 0.05352164804935455
        vf_loss: 14.78305435180664
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00022965119569562376
        entropy: 0.9381020665168762
        entropy_coeff: 0.0017600000137463212
        kl: 0.010881885886192322
        model: {}
        policy_loss: -0.027560582384467125
        total_loss: -0.026689516380429268
        vf_explained_var: 0.08124291896820068
        vf_loss: 14.339362144470215
    load_time_ms: 14346.647
    num_steps_sampled: 16608000
    num_steps_trained: 16608000
    sample_time_ms: 89432.084
    update_time_ms: 21.115
  iterations_since_restore: 153
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.10056179775281
    ram_util_percent: 13.00056179775281
  pid: 4061
  policy_reward_max:
    agent-0: 145.3333333333335
    agent-1: 145.3333333333335
    agent-2: 145.3333333333335
    agent-3: 145.3333333333335
    agent-4: 145.3333333333335
    agent-5: 145.3333333333335
  policy_reward_mean:
    agent-0: 115.22833333333381
    agent-1: 115.22833333333381
    agent-2: 115.22833333333381
    agent-3: 115.22833333333381
    agent-4: 115.22833333333381
    agent-5: 115.22833333333381
  policy_reward_min:
    agent-0: 58.49999999999978
    agent-1: 58.49999999999978
    agent-2: 58.49999999999978
    agent-3: 58.49999999999978
    agent-4: 58.49999999999978
    agent-5: 58.49999999999978
  sampler_perf:
    mean_env_wait_ms: 23.633762835366433
    mean_inference_ms: 12.310920327050514
    mean_processing_ms: 50.87395351748172
  time_since_restore: 20045.936073064804
  time_this_iter_s: 124.84004378318787
  time_total_s: 23256.999759197235
  timestamp: 1637037594
  timesteps_since_restore: 14688000
  timesteps_this_iter: 96000
  timesteps_total: 16608000
  training_iteration: 173
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    173 |            23257 | 16608000 |   691.37 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 31
    apples_agent-0_mean: 2.37
    apples_agent-0_min: 0
    apples_agent-1_max: 73
    apples_agent-1_mean: 23.85
    apples_agent-1_min: 0
    apples_agent-2_max: 227
    apples_agent-2_mean: 15.51
    apples_agent-2_min: 0
    apples_agent-3_max: 146
    apples_agent-3_mean: 68.86
    apples_agent-3_min: 35
    apples_agent-4_max: 59
    apples_agent-4_mean: 2.42
    apples_agent-4_min: 0
    apples_agent-5_max: 240
    apples_agent-5_mean: 84.21
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 477
    cleaning_beam_agent-0_mean: 350.79
    cleaning_beam_agent-0_min: 244
    cleaning_beam_agent-1_max: 367
    cleaning_beam_agent-1_mean: 195.17
    cleaning_beam_agent-1_min: 76
    cleaning_beam_agent-2_max: 348
    cleaning_beam_agent-2_mean: 205.81
    cleaning_beam_agent-2_min: 74
    cleaning_beam_agent-3_max: 156
    cleaning_beam_agent-3_mean: 37.18
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 448
    cleaning_beam_agent-4_mean: 341.46
    cleaning_beam_agent-4_min: 201
    cleaning_beam_agent-5_max: 766
    cleaning_beam_agent-5_mean: 148.7
    cleaning_beam_agent-5_min: 15
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-41-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 921.9999999999773
  episode_reward_mean: 713.2299999999896
  episode_reward_min: 242.9999999999968
  episodes_this_iter: 96
  episodes_total: 16704
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20168.108
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00022366079792845994
        entropy: 1.0789687633514404
        entropy_coeff: 0.0017600000137463212
        kl: 0.008814841508865356
        model: {}
        policy_loss: -0.022982904687523842
        total_loss: -0.022372111678123474
        vf_explained_var: 0.07193750143051147
        vf_loss: 16.28293800354004
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00022366079792845994
        entropy: 1.103100299835205
        entropy_coeff: 0.0017600000137463212
        kl: 0.01041178498417139
        model: {}
        policy_loss: -0.027278555557131767
        total_loss: -0.026380755007267
        vf_explained_var: -0.023847907781600952
        vf_loss: 17.980754852294922
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.00022366079792845994
        entropy: 1.1338632106781006
        entropy_coeff: 0.0017600000137463212
        kl: 0.009497981518507004
        model: {}
        policy_loss: -0.025387775152921677
        total_loss: -0.02437569573521614
        vf_explained_var: 0.09800402820110321
        vf_loss: 15.829835891723633
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00022366079792845994
        entropy: 0.5097694396972656
        entropy_coeff: 0.0017600000137463212
        kl: 0.006987837143242359
        model: {}
        policy_loss: -0.016364850103855133
        total_loss: -0.015183499082922935
        vf_explained_var: 0.21327580511569977
        vf_loss: 13.797642707824707
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00022366079792845994
        entropy: 0.963066041469574
        entropy_coeff: 0.0017600000137463212
        kl: 0.010523795150220394
        model: {}
        policy_loss: -0.02578631043434143
        total_loss: -0.024863123893737793
        vf_explained_var: 0.10798829793930054
        vf_loss: 15.658025741577148
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00022366079792845994
        entropy: 0.939902126789093
        entropy_coeff: 0.0017600000137463212
        kl: 0.009905502200126648
        model: {}
        policy_loss: -0.026230474933981895
        total_loss: -0.025339163839817047
        vf_explained_var: 0.11450280249118805
        vf_loss: 15.549886703491211
    load_time_ms: 14470.517
    num_steps_sampled: 16704000
    num_steps_trained: 16704000
    sample_time_ms: 89333.728
    update_time_ms: 20.621
  iterations_since_restore: 154
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.762359550561797
    ram_util_percent: 13.466853932584272
  pid: 4061
  policy_reward_max:
    agent-0: 153.66666666666688
    agent-1: 153.66666666666688
    agent-2: 153.66666666666688
    agent-3: 153.66666666666688
    agent-4: 153.66666666666688
    agent-5: 153.66666666666688
  policy_reward_mean:
    agent-0: 118.87166666666708
    agent-1: 118.87166666666708
    agent-2: 118.87166666666708
    agent-3: 118.87166666666708
    agent-4: 118.87166666666708
    agent-5: 118.87166666666708
  policy_reward_min:
    agent-0: 40.49999999999998
    agent-1: 40.49999999999998
    agent-2: 40.49999999999998
    agent-3: 40.49999999999998
    agent-4: 40.49999999999998
    agent-5: 40.49999999999998
  sampler_perf:
    mean_env_wait_ms: 23.635823324360928
    mean_inference_ms: 12.310892793751691
    mean_processing_ms: 50.87455432452722
  time_since_restore: 20170.814538002014
  time_this_iter_s: 124.87846493721008
  time_total_s: 23381.878224134445
  timestamp: 1637037719
  timesteps_since_restore: 14784000
  timesteps_this_iter: 96000
  timesteps_total: 16704000
  training_iteration: 174
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    174 |          23381.9 | 16704000 |   713.23 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 27
    apples_agent-0_mean: 2.18
    apples_agent-0_min: 0
    apples_agent-1_max: 73
    apples_agent-1_mean: 23.35
    apples_agent-1_min: 0
    apples_agent-2_max: 156
    apples_agent-2_mean: 15.65
    apples_agent-2_min: 0
    apples_agent-3_max: 171
    apples_agent-3_mean: 70.15
    apples_agent-3_min: 32
    apples_agent-4_max: 131
    apples_agent-4_mean: 1.76
    apples_agent-4_min: 0
    apples_agent-5_max: 160
    apples_agent-5_mean: 87.45
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 454
    cleaning_beam_agent-0_mean: 352.81
    cleaning_beam_agent-0_min: 134
    cleaning_beam_agent-1_max: 330
    cleaning_beam_agent-1_mean: 190.88
    cleaning_beam_agent-1_min: 96
    cleaning_beam_agent-2_max: 323
    cleaning_beam_agent-2_mean: 198.9
    cleaning_beam_agent-2_min: 66
    cleaning_beam_agent-3_max: 146
    cleaning_beam_agent-3_mean: 33.35
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 461
    cleaning_beam_agent-4_mean: 348.51
    cleaning_beam_agent-4_min: 173
    cleaning_beam_agent-5_max: 714
    cleaning_beam_agent-5_mean: 148.26
    cleaning_beam_agent-5_min: 13
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-44-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 958.9999999999641
  episode_reward_mean: 709.7199999999899
  episode_reward_min: 260.99999999999795
  episodes_this_iter: 96
  episodes_total: 16800
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20206.071
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021767040016129613
        entropy: 1.0891351699829102
        entropy_coeff: 0.0017600000137463212
        kl: 0.008480271324515343
        model: {}
        policy_loss: -0.022047074511647224
        total_loss: -0.021576404571533203
        vf_explained_var: 0.09077179431915283
        vf_loss: 15.39518928527832
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021767040016129613
        entropy: 1.1124677658081055
        entropy_coeff: 0.0017600000137463212
        kl: 0.010678522288799286
        model: {}
        policy_loss: -0.027607038617134094
        total_loss: -0.026829980313777924
        vf_explained_var: 0.015525519847869873
        vf_loss: 16.671470642089844
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.00021767040016129613
        entropy: 1.1380747556686401
        entropy_coeff: 0.0017600000137463212
        kl: 0.009646087884902954
        model: {}
        policy_loss: -0.024454770609736443
        total_loss: -0.02348703145980835
        vf_explained_var: 0.09977732598781586
        vf_loss: 15.238353729248047
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021767040016129613
        entropy: 0.5242577791213989
        entropy_coeff: 0.0017600000137463212
        kl: 0.006711319554597139
        model: {}
        policy_loss: -0.01650315709412098
        total_loss: -0.015396004542708397
        vf_explained_var: 0.1975783407688141
        vf_loss: 13.587113380432129
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021767040016129613
        entropy: 0.9560645222663879
        entropy_coeff: 0.0017600000137463212
        kl: 0.00998053327202797
        model: {}
        policy_loss: -0.026061469689011574
        total_loss: -0.02522774040699005
        vf_explained_var: 0.1027115136384964
        vf_loss: 15.183517456054688
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021767040016129613
        entropy: 0.9372806549072266
        entropy_coeff: 0.0017600000137463212
        kl: 0.009813947603106499
        model: {}
        policy_loss: -0.02692480757832527
        total_loss: -0.026061099022626877
        vf_explained_var: 0.09637580811977386
        vf_loss: 15.319279670715332
    load_time_ms: 14455.35
    num_steps_sampled: 16800000
    num_steps_trained: 16800000
    sample_time_ms: 89684.713
    update_time_ms: 20.897
  iterations_since_restore: 155
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.629834254143645
    ram_util_percent: 13.393922651933703
  pid: 4061
  policy_reward_max:
    agent-0: 159.8333333333336
    agent-1: 159.8333333333336
    agent-2: 159.8333333333336
    agent-3: 159.8333333333336
    agent-4: 159.8333333333336
    agent-5: 159.8333333333336
  policy_reward_mean:
    agent-0: 118.28666666666706
    agent-1: 118.28666666666706
    agent-2: 118.28666666666706
    agent-3: 118.28666666666706
    agent-4: 118.28666666666706
    agent-5: 118.28666666666706
  policy_reward_min:
    agent-0: 43.49999999999996
    agent-1: 43.49999999999996
    agent-2: 43.49999999999996
    agent-3: 43.49999999999996
    agent-4: 43.49999999999996
    agent-5: 43.49999999999996
  sampler_perf:
    mean_env_wait_ms: 23.63990461821761
    mean_inference_ms: 12.311585494124364
    mean_processing_ms: 50.877994624358315
  time_since_restore: 20297.700166463852
  time_this_iter_s: 126.88562846183777
  time_total_s: 23508.763852596283
  timestamp: 1637037846
  timesteps_since_restore: 14880000
  timesteps_this_iter: 96000
  timesteps_total: 16800000
  training_iteration: 175
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    175 |          23508.8 | 16800000 |   709.72 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 56
    apples_agent-0_mean: 3.29
    apples_agent-0_min: 0
    apples_agent-1_max: 70
    apples_agent-1_mean: 22.39
    apples_agent-1_min: 0
    apples_agent-2_max: 66
    apples_agent-2_mean: 13.65
    apples_agent-2_min: 0
    apples_agent-3_max: 138
    apples_agent-3_mean: 68.87
    apples_agent-3_min: 26
    apples_agent-4_max: 41
    apples_agent-4_mean: 0.55
    apples_agent-4_min: 0
    apples_agent-5_max: 165
    apples_agent-5_mean: 83.45
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 475
    cleaning_beam_agent-0_mean: 341.84
    cleaning_beam_agent-0_min: 213
    cleaning_beam_agent-1_max: 330
    cleaning_beam_agent-1_mean: 181.68
    cleaning_beam_agent-1_min: 73
    cleaning_beam_agent-2_max: 319
    cleaning_beam_agent-2_mean: 203.92
    cleaning_beam_agent-2_min: 55
    cleaning_beam_agent-3_max: 144
    cleaning_beam_agent-3_mean: 37.38
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 533
    cleaning_beam_agent-4_mean: 363.52
    cleaning_beam_agent-4_min: 225
    cleaning_beam_agent-5_max: 952
    cleaning_beam_agent-5_mean: 129.75
    cleaning_beam_agent-5_min: 16
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-46-13
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 981.9999999999744
  episode_reward_mean: 708.1699999999903
  episode_reward_min: 202.99999999999852
  episodes_this_iter: 96
  episodes_total: 16896
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20207.692
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021168000239413232
        entropy: 1.0956987142562866
        entropy_coeff: 0.0017600000137463212
        kl: 0.008370491676032543
        model: {}
        policy_loss: -0.022118665277957916
        total_loss: -0.02162861078977585
        vf_explained_var: 0.09150618314743042
        vf_loss: 15.814352035522461
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021168000239413232
        entropy: 1.1002330780029297
        entropy_coeff: 0.0017600000137463212
        kl: 0.010827801190316677
        model: {}
        policy_loss: -0.027492713183164597
        total_loss: -0.02660639025270939
        vf_explained_var: 0.0013771653175354004
        vf_loss: 17.39952850341797
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.00021168000239413232
        entropy: 1.1221411228179932
        entropy_coeff: 0.0017600000137463212
        kl: 0.009553931653499603
        model: {}
        policy_loss: -0.024277636781334877
        total_loss: -0.023260952904820442
        vf_explained_var: 0.10456867516040802
        vf_loss: 15.585576057434082
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021168000239413232
        entropy: 0.5257697105407715
        entropy_coeff: 0.0017600000137463212
        kl: 0.006652601063251495
        model: {}
        policy_loss: -0.016325581818819046
        total_loss: -0.015205416828393936
        vf_explained_var: 0.2056436687707901
        vf_loss: 13.802592277526855
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021168000239413232
        entropy: 0.9501351118087769
        entropy_coeff: 0.0017600000137463212
        kl: 0.009208309464156628
        model: {}
        policy_loss: -0.024495596066117287
        total_loss: -0.02360197901725769
        vf_explained_var: 0.05522066354751587
        vf_loss: 16.450212478637695
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021168000239413232
        entropy: 0.9351692795753479
        entropy_coeff: 0.0017600000137463212
        kl: 0.009910580702126026
        model: {}
        policy_loss: -0.026354867964982986
        total_loss: -0.02545841410756111
        vf_explained_var: 0.10865621268749237
        vf_loss: 15.512923240661621
    load_time_ms: 14391.196
    num_steps_sampled: 16896000
    num_steps_trained: 16896000
    sample_time_ms: 89911.143
    update_time_ms: 22.945
  iterations_since_restore: 156
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.61
    ram_util_percent: 13.465555555555557
  pid: 4061
  policy_reward_max:
    agent-0: 163.6666666666663
    agent-1: 163.6666666666663
    agent-2: 163.6666666666663
    agent-3: 163.6666666666663
    agent-4: 163.6666666666663
    agent-5: 163.6666666666663
  policy_reward_mean:
    agent-0: 118.02833333333372
    agent-1: 118.02833333333372
    agent-2: 118.02833333333372
    agent-3: 118.02833333333372
    agent-4: 118.02833333333372
    agent-5: 118.02833333333372
  policy_reward_min:
    agent-0: 33.833333333333385
    agent-1: 33.833333333333385
    agent-2: 33.833333333333385
    agent-3: 33.833333333333385
    agent-4: 33.833333333333385
    agent-5: 33.833333333333385
  sampler_perf:
    mean_env_wait_ms: 23.643611079223337
    mean_inference_ms: 12.311941616753133
    mean_processing_ms: 50.88065594022632
  time_since_restore: 20423.917815208435
  time_this_iter_s: 126.21764874458313
  time_total_s: 23634.981501340866
  timestamp: 1637037973
  timesteps_since_restore: 14976000
  timesteps_this_iter: 96000
  timesteps_total: 16896000
  training_iteration: 176
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    176 |            23635 | 16896000 |   708.17 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 56
    apples_agent-0_mean: 3.87
    apples_agent-0_min: 0
    apples_agent-1_max: 82
    apples_agent-1_mean: 26.22
    apples_agent-1_min: 0
    apples_agent-2_max: 99
    apples_agent-2_mean: 10.77
    apples_agent-2_min: 0
    apples_agent-3_max: 181
    apples_agent-3_mean: 71.02
    apples_agent-3_min: 32
    apples_agent-4_max: 59
    apples_agent-4_mean: 1.82
    apples_agent-4_min: 0
    apples_agent-5_max: 186
    apples_agent-5_mean: 83.73
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 471
    cleaning_beam_agent-0_mean: 346.95
    cleaning_beam_agent-0_min: 225
    cleaning_beam_agent-1_max: 390
    cleaning_beam_agent-1_mean: 193.32
    cleaning_beam_agent-1_min: 79
    cleaning_beam_agent-2_max: 501
    cleaning_beam_agent-2_mean: 212.7
    cleaning_beam_agent-2_min: 55
    cleaning_beam_agent-3_max: 143
    cleaning_beam_agent-3_mean: 37.81
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 523
    cleaning_beam_agent-4_mean: 362.14
    cleaning_beam_agent-4_min: 167
    cleaning_beam_agent-5_max: 766
    cleaning_beam_agent-5_mean: 133.17
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 3
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-48-17
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 963.9999999999889
  episode_reward_mean: 715.0199999999902
  episode_reward_min: 322.99999999999875
  episodes_this_iter: 96
  episodes_total: 16992
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20229.646
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002056896046269685
        entropy: 1.0894582271575928
        entropy_coeff: 0.0017600000137463212
        kl: 0.008051827549934387
        model: {}
        policy_loss: -0.02160307765007019
        total_loss: -0.021060597151517868
        vf_explained_var: 0.07295992970466614
        vf_loss: 16.547428131103516
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002056896046269685
        entropy: 1.107283353805542
        entropy_coeff: 0.0017600000137463212
        kl: 0.010941250249743462
        model: {}
        policy_loss: -0.027499396353960037
        total_loss: -0.02649841457605362
        vf_explained_var: -0.038272351026535034
        vf_loss: 18.55675506591797
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.0002056896046269685
        entropy: 1.1168882846832275
        entropy_coeff: 0.0017600000137463212
        kl: 0.009277925826609135
        model: {}
        policy_loss: -0.023875083774328232
        total_loss: -0.022758327424526215
        vf_explained_var: 0.05238986015319824
        vf_loss: 16.907886505126953
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002056896046269685
        entropy: 0.5154299139976501
        entropy_coeff: 0.0017600000137463212
        kl: 0.006203179247677326
        model: {}
        policy_loss: -0.015156958252191544
        total_loss: -0.014029084704816341
        vf_explained_var: 0.20665478706359863
        vf_loss: 14.147122383117676
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002056896046269685
        entropy: 0.9528525471687317
        entropy_coeff: 0.0017600000137463212
        kl: 0.009764177724719048
        model: {}
        policy_loss: -0.023854143917560577
        total_loss: -0.022984648123383522
        vf_explained_var: 0.119770348072052
        vf_loss: 15.700998306274414
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002056896046269685
        entropy: 0.9407089352607727
        entropy_coeff: 0.0017600000137463212
        kl: 0.009988807141780853
        model: {}
        policy_loss: -0.02566560171544552
        total_loss: -0.02472863905131817
        vf_explained_var: 0.10776089131832123
        vf_loss: 15.937260627746582
    load_time_ms: 14452.035
    num_steps_sampled: 16992000
    num_steps_trained: 16992000
    sample_time_ms: 90080.313
    update_time_ms: 22.434
  iterations_since_restore: 157
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.697175141242937
    ram_util_percent: 13.393785310734462
  pid: 4061
  policy_reward_max:
    agent-0: 160.6666666666669
    agent-1: 160.6666666666669
    agent-2: 160.6666666666669
    agent-3: 160.6666666666669
    agent-4: 160.6666666666669
    agent-5: 160.6666666666669
  policy_reward_mean:
    agent-0: 119.17000000000039
    agent-1: 119.17000000000039
    agent-2: 119.17000000000039
    agent-3: 119.17000000000039
    agent-4: 119.17000000000039
    agent-5: 119.17000000000039
  policy_reward_min:
    agent-0: 53.83333333333325
    agent-1: 53.83333333333325
    agent-2: 53.83333333333325
    agent-3: 53.83333333333325
    agent-4: 53.83333333333325
    agent-5: 53.83333333333325
  sampler_perf:
    mean_env_wait_ms: 23.64516422035022
    mean_inference_ms: 12.311837794947632
    mean_processing_ms: 50.88130364622869
  time_since_restore: 20548.027090072632
  time_this_iter_s: 124.10927486419678
  time_total_s: 23759.090776205063
  timestamp: 1637038097
  timesteps_since_restore: 15072000
  timesteps_this_iter: 96000
  timesteps_total: 16992000
  training_iteration: 177
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    177 |          23759.1 | 16992000 |   715.02 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 67
    apples_agent-0_mean: 2.93
    apples_agent-0_min: 0
    apples_agent-1_max: 152
    apples_agent-1_mean: 28.24
    apples_agent-1_min: 0
    apples_agent-2_max: 163
    apples_agent-2_mean: 15.02
    apples_agent-2_min: 0
    apples_agent-3_max: 110
    apples_agent-3_mean: 70.46
    apples_agent-3_min: 34
    apples_agent-4_max: 59
    apples_agent-4_mean: 1.68
    apples_agent-4_min: 0
    apples_agent-5_max: 189
    apples_agent-5_mean: 86.47
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 454
    cleaning_beam_agent-0_mean: 350.71
    cleaning_beam_agent-0_min: 219
    cleaning_beam_agent-1_max: 361
    cleaning_beam_agent-1_mean: 191.89
    cleaning_beam_agent-1_min: 92
    cleaning_beam_agent-2_max: 373
    cleaning_beam_agent-2_mean: 207.78
    cleaning_beam_agent-2_min: 44
    cleaning_beam_agent-3_max: 104
    cleaning_beam_agent-3_mean: 37.07
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 476
    cleaning_beam_agent-4_mean: 367.26
    cleaning_beam_agent-4_min: 190
    cleaning_beam_agent-5_max: 874
    cleaning_beam_agent-5_mean: 143.6
    cleaning_beam_agent-5_min: 14
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 3
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-50-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 889.9999999999656
  episode_reward_mean: 723.3999999999892
  episode_reward_min: 245.99999999999486
  episodes_this_iter: 96
  episodes_total: 17088
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20238.407
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001996992068598047
        entropy: 1.0870505571365356
        entropy_coeff: 0.0017600000137463212
        kl: 0.008677908219397068
        model: {}
        policy_loss: -0.02189505845308304
        total_loss: -0.021418752148747444
        vf_explained_var: 0.06272159516811371
        vf_loss: 15.2172269821167
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001996992068598047
        entropy: 1.1088894605636597
        entropy_coeff: 0.0017600000137463212
        kl: 0.009833022020757198
        model: {}
        policy_loss: -0.026367638260126114
        total_loss: -0.02560916543006897
        vf_explained_var: -0.060571372509002686
        vf_loss: 17.268173217773438
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.0001996992068598047
        entropy: 1.122130036354065
        entropy_coeff: 0.0017600000137463212
        kl: 0.009259235113859177
        model: {}
        policy_loss: -0.024317728355526924
        total_loss: -0.023383015766739845
        vf_explained_var: 0.06416493654251099
        vf_loss: 15.207757949829102
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001996992068598047
        entropy: 0.5084386467933655
        entropy_coeff: 0.0017600000137463212
        kl: 0.006143617909401655
        model: {}
        policy_loss: -0.014946999028325081
        total_loss: -0.01391994021832943
        vf_explained_var: 0.1935262531042099
        vf_loss: 13.07547664642334
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001996992068598047
        entropy: 0.9477779269218445
        entropy_coeff: 0.0017600000137463212
        kl: 0.009362657554447651
        model: {}
        policy_loss: -0.02537187747657299
        total_loss: -0.02458564005792141
        vf_explained_var: 0.06546822190284729
        vf_loss: 15.180638313293457
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001996992068598047
        entropy: 0.9263222217559814
        entropy_coeff: 0.0017600000137463212
        kl: 0.010309702716767788
        model: {}
        policy_loss: -0.02510327287018299
        total_loss: -0.024142105132341385
        vf_explained_var: 0.04002098739147186
        vf_loss: 15.60529613494873
    load_time_ms: 14425.362
    num_steps_sampled: 17088000
    num_steps_trained: 17088000
    sample_time_ms: 90134.144
    update_time_ms: 22.287
  iterations_since_restore: 158
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.693296089385477
    ram_util_percent: 13.469832402234639
  pid: 4061
  policy_reward_max:
    agent-0: 148.33333333333326
    agent-1: 148.33333333333326
    agent-2: 148.33333333333326
    agent-3: 148.33333333333326
    agent-4: 148.33333333333326
    agent-5: 148.33333333333326
  policy_reward_mean:
    agent-0: 120.56666666666706
    agent-1: 120.56666666666706
    agent-2: 120.56666666666706
    agent-3: 120.56666666666706
    agent-4: 120.56666666666706
    agent-5: 120.56666666666706
  policy_reward_min:
    agent-0: 40.99999999999997
    agent-1: 40.99999999999997
    agent-2: 40.99999999999997
    agent-3: 40.99999999999997
    agent-4: 40.99999999999997
    agent-5: 40.99999999999997
  sampler_perf:
    mean_env_wait_ms: 23.64821779353037
    mean_inference_ms: 12.311974579876132
    mean_processing_ms: 50.88068797317277
  time_since_restore: 20673.59663462639
  time_this_iter_s: 125.56954455375671
  time_total_s: 23884.66032075882
  timestamp: 1637038223
  timesteps_since_restore: 15168000
  timesteps_this_iter: 96000
  timesteps_total: 17088000
  training_iteration: 178
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    178 |          23884.7 | 17088000 |    723.4 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 33
    apples_agent-0_mean: 3.05
    apples_agent-0_min: 0
    apples_agent-1_max: 113
    apples_agent-1_mean: 25.21
    apples_agent-1_min: 0
    apples_agent-2_max: 125
    apples_agent-2_mean: 12.1
    apples_agent-2_min: 0
    apples_agent-3_max: 127
    apples_agent-3_mean: 71.59
    apples_agent-3_min: 25
    apples_agent-4_max: 83
    apples_agent-4_mean: 2.71
    apples_agent-4_min: 0
    apples_agent-5_max: 177
    apples_agent-5_mean: 86.72
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 470
    cleaning_beam_agent-0_mean: 352.06
    cleaning_beam_agent-0_min: 239
    cleaning_beam_agent-1_max: 315
    cleaning_beam_agent-1_mean: 181.65
    cleaning_beam_agent-1_min: 84
    cleaning_beam_agent-2_max: 405
    cleaning_beam_agent-2_mean: 203.29
    cleaning_beam_agent-2_min: 72
    cleaning_beam_agent-3_max: 114
    cleaning_beam_agent-3_mean: 34.52
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 508
    cleaning_beam_agent-4_mean: 363.82
    cleaning_beam_agent-4_min: 202
    cleaning_beam_agent-5_max: 701
    cleaning_beam_agent-5_mean: 114.14
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-52-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 961.9999999999677
  episode_reward_mean: 708.749999999989
  episode_reward_min: 246.99999999999565
  episodes_this_iter: 96
  episodes_total: 17184
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20226.193
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00019370879454072565
        entropy: 1.0936599969863892
        entropy_coeff: 0.0017600000137463212
        kl: 0.008244900964200497
        model: {}
        policy_loss: -0.02075892686843872
        total_loss: -0.020097726956009865
        vf_explained_var: 0.04409421980381012
        vf_loss: 17.615493774414062
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00019370879454072565
        entropy: 1.1082513332366943
        entropy_coeff: 0.0017600000137463212
        kl: 0.010310460813343525
        model: {}
        policy_loss: -0.025883620604872704
        total_loss: -0.02495363913476467
        vf_explained_var: -0.0043912529945373535
        vf_loss: 18.494590759277344
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.00019370879454072565
        entropy: 1.1409144401550293
        entropy_coeff: 0.0017600000137463212
        kl: 0.009181499481201172
        model: {}
        policy_loss: -0.023495471104979515
        total_loss: -0.022470977157354355
        vf_explained_var: 0.10079723596572876
        vf_loss: 16.55278968811035
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00019370879454072565
        entropy: 0.5292445421218872
        entropy_coeff: 0.0017600000137463212
        kl: 0.0066914064809679985
        model: {}
        policy_loss: -0.016829416155815125
        total_loss: -0.01570412702858448
        vf_explained_var: 0.2460918426513672
        vf_loss: 13.87619400024414
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00019370879454072565
        entropy: 0.9671489000320435
        entropy_coeff: 0.0017600000137463212
        kl: 0.009506065398454666
        model: {}
        policy_loss: -0.024796441197395325
        total_loss: -0.023963654413819313
        vf_explained_var: 0.13931579887866974
        vf_loss: 15.843611717224121
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00019370879454072565
        entropy: 0.9506143927574158
        entropy_coeff: 0.0017600000137463212
        kl: 0.009196681901812553
        model: {}
        policy_loss: -0.02530476450920105
        total_loss: -0.024471653625369072
        vf_explained_var: 0.13801544904708862
        vf_loss: 15.865269660949707
    load_time_ms: 14122.975
    num_steps_sampled: 17184000
    num_steps_trained: 17184000
    sample_time_ms: 90332.452
    update_time_ms: 22.211
  iterations_since_restore: 159
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.664245810055867
    ram_util_percent: 13.45251396648045
  pid: 4061
  policy_reward_max:
    agent-0: 160.33333333333326
    agent-1: 160.33333333333326
    agent-2: 160.33333333333326
    agent-3: 160.33333333333326
    agent-4: 160.33333333333326
    agent-5: 160.33333333333326
  policy_reward_mean:
    agent-0: 118.1250000000004
    agent-1: 118.1250000000004
    agent-2: 118.1250000000004
    agent-3: 118.1250000000004
    agent-4: 118.1250000000004
    agent-5: 118.1250000000004
  policy_reward_min:
    agent-0: 41.16666666666663
    agent-1: 41.16666666666663
    agent-2: 41.16666666666663
    agent-3: 41.16666666666663
    agent-4: 41.16666666666663
    agent-5: 41.16666666666663
  sampler_perf:
    mean_env_wait_ms: 23.65141147301193
    mean_inference_ms: 12.312642646972776
    mean_processing_ms: 50.88295547733138
  time_since_restore: 20799.620133399963
  time_this_iter_s: 126.02349877357483
  time_total_s: 24010.683819532394
  timestamp: 1637038349
  timesteps_since_restore: 15264000
  timesteps_this_iter: 96000
  timesteps_total: 17184000
  training_iteration: 179
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    179 |          24010.7 | 17184000 |   708.75 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 43
    apples_agent-0_mean: 2.89
    apples_agent-0_min: 0
    apples_agent-1_max: 107
    apples_agent-1_mean: 26.89
    apples_agent-1_min: 0
    apples_agent-2_max: 65
    apples_agent-2_mean: 13.07
    apples_agent-2_min: 0
    apples_agent-3_max: 162
    apples_agent-3_mean: 67.45
    apples_agent-3_min: 29
    apples_agent-4_max: 47
    apples_agent-4_mean: 1.38
    apples_agent-4_min: 0
    apples_agent-5_max: 168
    apples_agent-5_mean: 85.9
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 476
    cleaning_beam_agent-0_mean: 357.53
    cleaning_beam_agent-0_min: 239
    cleaning_beam_agent-1_max: 316
    cleaning_beam_agent-1_mean: 186.52
    cleaning_beam_agent-1_min: 88
    cleaning_beam_agent-2_max: 430
    cleaning_beam_agent-2_mean: 211.01
    cleaning_beam_agent-2_min: 71
    cleaning_beam_agent-3_max: 111
    cleaning_beam_agent-3_mean: 35.85
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 465
    cleaning_beam_agent-4_mean: 357.06
    cleaning_beam_agent-4_min: 144
    cleaning_beam_agent-5_max: 829
    cleaning_beam_agent-5_mean: 139.47
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-54-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 925.9999999999908
  episode_reward_mean: 718.0399999999889
  episode_reward_min: 246.99999999999565
  episodes_this_iter: 96
  episodes_total: 17280
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20235.319
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00018771839677356184
        entropy: 1.0817718505859375
        entropy_coeff: 0.0017600000137463212
        kl: 0.008091584779322147
        model: {}
        policy_loss: -0.021096091717481613
        total_loss: -0.020547151565551758
        vf_explained_var: 0.04601284861564636
        vf_loss: 16.436973571777344
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00018771839677356184
        entropy: 1.1059712171554565
        entropy_coeff: 0.0017600000137463212
        kl: 0.010011179372668266
        model: {}
        policy_loss: -0.02546815574169159
        total_loss: -0.02472723461687565
        vf_explained_var: 0.02222369611263275
        vf_loss: 16.863109588623047
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.00018771839677356184
        entropy: 1.1461787223815918
        entropy_coeff: 0.0017600000137463212
        kl: 0.008582083508372307
        model: {}
        policy_loss: -0.023765340447425842
        total_loss: -0.022970449179410934
        vf_explained_var: 0.1148720234632492
        vf_loss: 15.24855899810791
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00018771839677356184
        entropy: 0.516784131526947
        entropy_coeff: 0.0017600000137463212
        kl: 0.005581210367381573
        model: {}
        policy_loss: -0.015024682506918907
        total_loss: -0.014028605073690414
        vf_explained_var: 0.21740613877773285
        vf_loss: 13.474982261657715
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00018771839677356184
        entropy: 0.951497495174408
        entropy_coeff: 0.0017600000137463212
        kl: 0.009359504096210003
        model: {}
        policy_loss: -0.0242203027009964
        total_loss: -0.023402821272611618
        vf_explained_var: 0.09694461524486542
        vf_loss: 15.561683654785156
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00018771839677356184
        entropy: 0.9194431304931641
        entropy_coeff: 0.0017600000137463212
        kl: 0.009809251874685287
        model: {}
        policy_loss: -0.025140251964330673
        total_loss: -0.024241778999567032
        vf_explained_var: 0.11141285300254822
        vf_loss: 15.357666015625
    load_time_ms: 14163.305
    num_steps_sampled: 17280000
    num_steps_trained: 17280000
    sample_time_ms: 90679.958
    update_time_ms: 22.287
  iterations_since_restore: 160
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.654395604395607
    ram_util_percent: 13.459340659340661
  pid: 4061
  policy_reward_max:
    agent-0: 154.33333333333363
    agent-1: 154.33333333333363
    agent-2: 154.33333333333363
    agent-3: 154.33333333333363
    agent-4: 154.33333333333363
    agent-5: 154.33333333333363
  policy_reward_mean:
    agent-0: 119.67333333333376
    agent-1: 119.67333333333376
    agent-2: 119.67333333333376
    agent-3: 119.67333333333376
    agent-4: 119.67333333333376
    agent-5: 119.67333333333376
  policy_reward_min:
    agent-0: 41.16666666666663
    agent-1: 41.16666666666663
    agent-2: 41.16666666666663
    agent-3: 41.16666666666663
    agent-4: 41.16666666666663
    agent-5: 41.16666666666663
  sampler_perf:
    mean_env_wait_ms: 23.655859977256227
    mean_inference_ms: 12.313555853175323
    mean_processing_ms: 50.88701893625457
  time_since_restore: 20927.467647075653
  time_this_iter_s: 127.8475136756897
  time_total_s: 24138.531333208084
  timestamp: 1637038477
  timesteps_since_restore: 15360000
  timesteps_this_iter: 96000
  timesteps_total: 17280000
  training_iteration: 180
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 24.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    180 |          24138.5 | 17280000 |   718.04 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 39
    apples_agent-0_mean: 3.03
    apples_agent-0_min: 0
    apples_agent-1_max: 106
    apples_agent-1_mean: 25.93
    apples_agent-1_min: 0
    apples_agent-2_max: 123
    apples_agent-2_mean: 11.49
    apples_agent-2_min: 0
    apples_agent-3_max: 130
    apples_agent-3_mean: 65.7
    apples_agent-3_min: 22
    apples_agent-4_max: 51
    apples_agent-4_mean: 1.52
    apples_agent-4_min: 0
    apples_agent-5_max: 258
    apples_agent-5_mean: 90.28
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 459
    cleaning_beam_agent-0_mean: 359.98
    cleaning_beam_agent-0_min: 227
    cleaning_beam_agent-1_max: 314
    cleaning_beam_agent-1_mean: 189.66
    cleaning_beam_agent-1_min: 99
    cleaning_beam_agent-2_max: 368
    cleaning_beam_agent-2_mean: 210.85
    cleaning_beam_agent-2_min: 84
    cleaning_beam_agent-3_max: 160
    cleaning_beam_agent-3_mean: 39.43
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 479
    cleaning_beam_agent-4_mean: 352.83
    cleaning_beam_agent-4_min: 165
    cleaning_beam_agent-5_max: 753
    cleaning_beam_agent-5_mean: 124.64
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-56-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 914.9999999999914
  episode_reward_mean: 707.8899999999891
  episode_reward_min: 191.99999999999756
  episodes_this_iter: 96
  episodes_total: 17376
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20231.848
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00018172799900639802
        entropy: 1.0748945474624634
        entropy_coeff: 0.0017600000137463212
        kl: 0.008001490496098995
        model: {}
        policy_loss: -0.02071889117360115
        total_loss: -0.02008131518959999
        vf_explained_var: 0.07399684190750122
        vf_loss: 17.292430877685547
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00018172799900639802
        entropy: 1.1043790578842163
        entropy_coeff: 0.0017600000137463212
        kl: 0.009683143347501755
        model: {}
        policy_loss: -0.026041343808174133
        total_loss: -0.025171294808387756
        vf_explained_var: 0.012221008539199829
        vf_loss: 18.454425811767578
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.00018172799900639802
        entropy: 1.1338149309158325
        entropy_coeff: 0.0017600000137463212
        kl: 0.008878979831933975
        model: {}
        policy_loss: -0.023032940924167633
        total_loss: -0.02203521318733692
        vf_explained_var: 0.11095526814460754
        vf_loss: 16.613962173461914
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00018172799900639802
        entropy: 0.5141859650611877
        entropy_coeff: 0.0017600000137463212
        kl: 0.005857436917722225
        model: {}
        policy_loss: -0.0149666927754879
        total_loss: -0.013871160335838795
        vf_explained_var: 0.24283890426158905
        vf_loss: 14.147570610046387
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00018172799900639802
        entropy: 0.9664709568023682
        entropy_coeff: 0.0017600000137463212
        kl: 0.0093473419547081
        model: {}
        policy_loss: -0.022974621504545212
        total_loss: -0.022033479064702988
        vf_explained_var: 0.08545693755149841
        vf_loss: 17.07394790649414
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00018172799900639802
        entropy: 0.9222192168235779
        entropy_coeff: 0.0017600000137463212
        kl: 0.008981707505881786
        model: {}
        policy_loss: -0.025015437975525856
        total_loss: -0.024079326540231705
        vf_explained_var: 0.10987742245197296
        vf_loss: 16.610483169555664
    load_time_ms: 14152.281
    num_steps_sampled: 17376000
    num_steps_trained: 17376000
    sample_time_ms: 91021.532
    update_time_ms: 22.424
  iterations_since_restore: 161
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.6585635359116
    ram_util_percent: 13.367955801104971
  pid: 4061
  policy_reward_max:
    agent-0: 152.5000000000001
    agent-1: 152.5000000000001
    agent-2: 152.5000000000001
    agent-3: 152.5000000000001
    agent-4: 152.5000000000001
    agent-5: 152.5000000000001
  policy_reward_mean:
    agent-0: 117.98166666666707
    agent-1: 117.98166666666707
    agent-2: 117.98166666666707
    agent-3: 117.98166666666707
    agent-4: 117.98166666666707
    agent-5: 117.98166666666707
  policy_reward_min:
    agent-0: 32.00000000000008
    agent-1: 32.00000000000008
    agent-2: 32.00000000000008
    agent-3: 32.00000000000008
    agent-4: 32.00000000000008
    agent-5: 32.00000000000008
  sampler_perf:
    mean_env_wait_ms: 23.66094161678161
    mean_inference_ms: 12.314537023844133
    mean_processing_ms: 50.89211079776734
  time_since_restore: 21052.85707974434
  time_this_iter_s: 125.38943266868591
  time_total_s: 24263.92076587677
  timestamp: 1637038603
  timesteps_since_restore: 15456000
  timesteps_this_iter: 96000
  timesteps_total: 17376000
  training_iteration: 181
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    181 |          24263.9 | 17376000 |   707.89 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 50
    apples_agent-0_mean: 1.9
    apples_agent-0_min: 0
    apples_agent-1_max: 80
    apples_agent-1_mean: 24.9
    apples_agent-1_min: 0
    apples_agent-2_max: 64
    apples_agent-2_mean: 10.76
    apples_agent-2_min: 0
    apples_agent-3_max: 129
    apples_agent-3_mean: 64.26
    apples_agent-3_min: 17
    apples_agent-4_max: 51
    apples_agent-4_mean: 2.33
    apples_agent-4_min: 0
    apples_agent-5_max: 201
    apples_agent-5_mean: 90.66
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 494
    cleaning_beam_agent-0_mean: 360.49
    cleaning_beam_agent-0_min: 257
    cleaning_beam_agent-1_max: 311
    cleaning_beam_agent-1_mean: 190.27
    cleaning_beam_agent-1_min: 80
    cleaning_beam_agent-2_max: 344
    cleaning_beam_agent-2_mean: 209.01
    cleaning_beam_agent-2_min: 67
    cleaning_beam_agent-3_max: 125
    cleaning_beam_agent-3_mean: 38.87
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 499
    cleaning_beam_agent-4_mean: 367.79
    cleaning_beam_agent-4_min: 201
    cleaning_beam_agent-5_max: 817
    cleaning_beam_agent-5_mean: 119.36
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-58-48
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 966.9999999999736
  episode_reward_mean: 744.7599999999876
  episode_reward_min: 203.999999999998
  episodes_this_iter: 96
  episodes_total: 17472
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20212.853
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001757376012392342
        entropy: 1.0541045665740967
        entropy_coeff: 0.0017600000137463212
        kl: 0.008037731051445007
        model: {}
        policy_loss: -0.020716754719614983
        total_loss: -0.02016570419073105
        vf_explained_var: 0.0724371075630188
        vf_loss: 16.02503204345703
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001757376012392342
        entropy: 1.1109633445739746
        entropy_coeff: 0.0017600000137463212
        kl: 0.00924289133399725
        model: {}
        policy_loss: -0.024337517097592354
        total_loss: -0.023622771725058556
        vf_explained_var: -0.009210139513015747
        vf_loss: 17.457521438598633
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.0001757376012392342
        entropy: 1.126308560371399
        entropy_coeff: 0.0017600000137463212
        kl: 0.0082637257874012
        model: {}
        policy_loss: -0.022081738337874413
        total_loss: -0.02125299721956253
        vf_explained_var: 0.09222589433193207
        vf_loss: 15.71488094329834
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001757376012392342
        entropy: 0.5015993118286133
        entropy_coeff: 0.0017600000137463212
        kl: 0.0052450597286224365
        model: {}
        policy_loss: -0.014047885313630104
        total_loss: -0.013027043081820011
        vf_explained_var: 0.20006589591503143
        vf_loss: 13.791496276855469
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001757376012392342
        entropy: 0.9445905685424805
        entropy_coeff: 0.0017600000137463212
        kl: 0.008826985023915768
        model: {}
        policy_loss: -0.023327816277742386
        total_loss: -0.022527486085891724
        vf_explained_var: 0.083645299077034
        vf_loss: 15.801112174987793
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001757376012392342
        entropy: 0.9362704753875732
        entropy_coeff: 0.0017600000137463212
        kl: 0.009599754586815834
        model: {}
        policy_loss: -0.02516581118106842
        total_loss: -0.02434917911887169
        vf_explained_var: 0.12844634056091309
        vf_loss: 15.044904708862305
    load_time_ms: 14165.479
    num_steps_sampled: 17472000
    num_steps_trained: 17472000
    sample_time_ms: 91163.554
    update_time_ms: 23.099
  iterations_since_restore: 162
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 16.784180790960452
    ram_util_percent: 10.294350282485874
  pid: 4061
  policy_reward_max:
    agent-0: 161.16666666666623
    agent-1: 161.16666666666623
    agent-2: 161.16666666666623
    agent-3: 161.16666666666623
    agent-4: 161.16666666666623
    agent-5: 161.16666666666623
  policy_reward_mean:
    agent-0: 124.126666666667
    agent-1: 124.126666666667
    agent-2: 124.126666666667
    agent-3: 124.126666666667
    agent-4: 124.126666666667
    agent-5: 124.126666666667
  policy_reward_min:
    agent-0: 34.000000000000036
    agent-1: 34.000000000000036
    agent-2: 34.000000000000036
    agent-3: 34.000000000000036
    agent-4: 34.000000000000036
    agent-5: 34.000000000000036
  sampler_perf:
    mean_env_wait_ms: 23.66396625095518
    mean_inference_ms: 12.314731395110869
    mean_processing_ms: 50.892785612479685
  time_since_restore: 21177.640501976013
  time_this_iter_s: 124.7834222316742
  time_total_s: 24388.704188108444
  timestamp: 1637038728
  timesteps_since_restore: 15552000
  timesteps_this_iter: 96000
  timesteps_total: 17472000
  training_iteration: 182
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    182 |          24388.7 | 17472000 |   744.76 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 50
    apples_agent-0_mean: 4.34
    apples_agent-0_min: 0
    apples_agent-1_max: 106
    apples_agent-1_mean: 27.93
    apples_agent-1_min: 0
    apples_agent-2_max: 74
    apples_agent-2_mean: 14.4
    apples_agent-2_min: 0
    apples_agent-3_max: 127
    apples_agent-3_mean: 66.77
    apples_agent-3_min: 32
    apples_agent-4_max: 61
    apples_agent-4_mean: 0.86
    apples_agent-4_min: 0
    apples_agent-5_max: 207
    apples_agent-5_mean: 85.13
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 533
    cleaning_beam_agent-0_mean: 356.7
    cleaning_beam_agent-0_min: 238
    cleaning_beam_agent-1_max: 341
    cleaning_beam_agent-1_mean: 191.78
    cleaning_beam_agent-1_min: 85
    cleaning_beam_agent-2_max: 365
    cleaning_beam_agent-2_mean: 209.8
    cleaning_beam_agent-2_min: 91
    cleaning_beam_agent-3_max: 94
    cleaning_beam_agent-3_mean: 36.48
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 472
    cleaning_beam_agent-4_mean: 370.1
    cleaning_beam_agent-4_min: 182
    cleaning_beam_agent-5_max: 808
    cleaning_beam_agent-5_mean: 153.52
    cleaning_beam_agent-5_min: 13
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-00-56
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 957.9999999999721
  episode_reward_mean: 729.2199999999893
  episode_reward_min: 329.0000000000015
  episodes_this_iter: 96
  episodes_total: 17568
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20261.314
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001697472034720704
        entropy: 1.0725350379943848
        entropy_coeff: 0.0017600000137463212
        kl: 0.007433017715811729
        model: {}
        policy_loss: -0.020358672365546227
        total_loss: -0.019907396286725998
        vf_explained_var: 0.10030199587345123
        vf_loss: 15.956367492675781
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001697472034720704
        entropy: 1.101669430732727
        entropy_coeff: 0.0017600000137463212
        kl: 0.009200334548950195
        model: {}
        policy_loss: -0.024140339344739914
        total_loss: -0.023344462737441063
        vf_explained_var: -0.022367149591445923
        vf_loss: 18.147796630859375
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.0001697472034720704
        entropy: 1.1220104694366455
        entropy_coeff: 0.0017600000137463212
        kl: 0.008047635667026043
        model: {}
        policy_loss: -0.02269173413515091
        total_loss: -0.021768009290099144
        vf_explained_var: 0.046588823199272156
        vf_loss: 16.913164138793945
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001697472034720704
        entropy: 0.5081186294555664
        entropy_coeff: 0.0017600000137463212
        kl: 0.005740477237850428
        model: {}
        policy_loss: -0.013754736632108688
        total_loss: -0.012746868655085564
        vf_explained_var: 0.2512352466583252
        vf_loss: 13.281095504760742
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001697472034720704
        entropy: 0.9460213780403137
        entropy_coeff: 0.0017600000137463212
        kl: 0.008934656158089638
        model: {}
        policy_loss: -0.0221263375133276
        total_loss: -0.02120138332247734
        vf_explained_var: 0.04342085123062134
        vf_loss: 16.96487045288086
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001697472034720704
        entropy: 0.9159676432609558
        entropy_coeff: 0.0017600000137463212
        kl: 0.009139805100858212
        model: {}
        policy_loss: -0.023057352751493454
        total_loss: -0.022265983745455742
        vf_explained_var: 0.16027677059173584
        vf_loss: 14.894914627075195
    load_time_ms: 14556.397
    num_steps_sampled: 17568000
    num_steps_trained: 17568000
    sample_time_ms: 91020.915
    update_time_ms: 23.465
  iterations_since_restore: 163
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 16.743715846994533
    ram_util_percent: 9.798360655737707
  pid: 4061
  policy_reward_max:
    agent-0: 159.66666666666615
    agent-1: 159.66666666666615
    agent-2: 159.66666666666615
    agent-3: 159.66666666666615
    agent-4: 159.66666666666615
    agent-5: 159.66666666666615
  policy_reward_mean:
    agent-0: 121.53666666666705
    agent-1: 121.53666666666705
    agent-2: 121.53666666666705
    agent-3: 121.53666666666705
    agent-4: 121.53666666666705
    agent-5: 121.53666666666705
  policy_reward_min:
    agent-0: 54.833333333333265
    agent-1: 54.833333333333265
    agent-2: 54.833333333333265
    agent-3: 54.833333333333265
    agent-4: 54.833333333333265
    agent-5: 54.833333333333265
  sampler_perf:
    mean_env_wait_ms: 23.666095670641457
    mean_inference_ms: 12.314391376073466
    mean_processing_ms: 50.891465547713
  time_since_restore: 21305.467688322067
  time_this_iter_s: 127.82718634605408
  time_total_s: 24516.5313744545
  timestamp: 1637038856
  timesteps_since_restore: 15648000
  timesteps_this_iter: 96000
  timesteps_total: 17568000
  training_iteration: 183
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    183 |          24516.5 | 17568000 |   729.22 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 89
    apples_agent-0_mean: 4.45
    apples_agent-0_min: 0
    apples_agent-1_max: 88
    apples_agent-1_mean: 23.87
    apples_agent-1_min: 0
    apples_agent-2_max: 76
    apples_agent-2_mean: 11.94
    apples_agent-2_min: 0
    apples_agent-3_max: 98
    apples_agent-3_mean: 61.82
    apples_agent-3_min: 28
    apples_agent-4_max: 70
    apples_agent-4_mean: 2.05
    apples_agent-4_min: 0
    apples_agent-5_max: 217
    apples_agent-5_mean: 94.74
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 507
    cleaning_beam_agent-0_mean: 361.16
    cleaning_beam_agent-0_min: 260
    cleaning_beam_agent-1_max: 338
    cleaning_beam_agent-1_mean: 197.87
    cleaning_beam_agent-1_min: 91
    cleaning_beam_agent-2_max: 462
    cleaning_beam_agent-2_mean: 225.04
    cleaning_beam_agent-2_min: 96
    cleaning_beam_agent-3_max: 172
    cleaning_beam_agent-3_mean: 39.31
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 497
    cleaning_beam_agent-4_mean: 374.42
    cleaning_beam_agent-4_min: 135
    cleaning_beam_agent-5_max: 731
    cleaning_beam_agent-5_mean: 116.05
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-03-01
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 954.9999999999867
  episode_reward_mean: 736.5499999999877
  episode_reward_min: 198.99999999999767
  episodes_this_iter: 96
  episodes_total: 17664
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20266.792
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00016375680570490658
        entropy: 1.069176197052002
        entropy_coeff: 0.0017600000137463212
        kl: 0.008258191868662834
        model: {}
        policy_loss: -0.02076326310634613
        total_loss: -0.020052392035722733
        vf_explained_var: 0.07372631132602692
        vf_loss: 17.66802978515625
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00016375680570490658
        entropy: 1.1215379238128662
        entropy_coeff: 0.0017600000137463212
        kl: 0.010167675092816353
        model: {}
        policy_loss: -0.024149669334292412
        total_loss: -0.02323429472744465
        vf_explained_var: 0.018517881631851196
        vf_loss: 18.72506332397461
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.00016375680570490658
        entropy: 1.1307133436203003
        entropy_coeff: 0.0017600000137463212
        kl: 0.007673191837966442
        model: {}
        policy_loss: -0.02162470668554306
        total_loss: -0.020679688081145287
        vf_explained_var: 0.06375160813331604
        vf_loss: 17.840980529785156
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00016375680570490658
        entropy: 0.4910714626312256
        entropy_coeff: 0.0017600000137463212
        kl: 0.005436247680336237
        model: {}
        policy_loss: -0.013593403622508049
        total_loss: -0.012474551796913147
        vf_explained_var: 0.24434798955917358
        vf_loss: 14.39512825012207
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00016375680570490658
        entropy: 0.9442071914672852
        entropy_coeff: 0.0017600000137463212
        kl: 0.008797886781394482
        model: {}
        policy_loss: -0.022314656525850296
        total_loss: -0.021373292431235313
        vf_explained_var: 0.09568609297275543
        vf_loss: 17.233835220336914
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00016375680570490658
        entropy: 0.9343892335891724
        entropy_coeff: 0.0017600000137463212
        kl: 0.008928196504712105
        model: {}
        policy_loss: -0.023166868835687637
        total_loss: -0.02228555455803871
        vf_explained_var: 0.14269888401031494
        vf_loss: 16.33021354675293
    load_time_ms: 14606.758
    num_steps_sampled: 17664000
    num_steps_trained: 17664000
    sample_time_ms: 90918.01
    update_time_ms: 23.542
  iterations_since_restore: 164
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.255367231638417
    ram_util_percent: 9.599435028248589
  pid: 4061
  policy_reward_max:
    agent-0: 159.16666666666697
    agent-1: 159.16666666666697
    agent-2: 159.16666666666697
    agent-3: 159.16666666666697
    agent-4: 159.16666666666697
    agent-5: 159.16666666666697
  policy_reward_mean:
    agent-0: 122.75833333333365
    agent-1: 122.75833333333365
    agent-2: 122.75833333333365
    agent-3: 122.75833333333365
    agent-4: 122.75833333333365
    agent-5: 122.75833333333365
  policy_reward_min:
    agent-0: 33.166666666666735
    agent-1: 33.166666666666735
    agent-2: 33.166666666666735
    agent-3: 33.166666666666735
    agent-4: 33.166666666666735
    agent-5: 33.166666666666735
  sampler_perf:
    mean_env_wait_ms: 23.667153495182237
    mean_inference_ms: 12.31359652680656
    mean_processing_ms: 50.887778158884885
  time_since_restore: 21429.89085292816
  time_this_iter_s: 124.42316460609436
  time_total_s: 24640.954539060593
  timestamp: 1637038981
  timesteps_since_restore: 15744000
  timesteps_this_iter: 96000
  timesteps_total: 17664000
  training_iteration: 184
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    184 |            24641 | 17664000 |   736.55 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 51
    apples_agent-0_mean: 5.11
    apples_agent-0_min: 0
    apples_agent-1_max: 81
    apples_agent-1_mean: 25.07
    apples_agent-1_min: 0
    apples_agent-2_max: 70
    apples_agent-2_mean: 13.54
    apples_agent-2_min: 0
    apples_agent-3_max: 144
    apples_agent-3_mean: 68.12
    apples_agent-3_min: 15
    apples_agent-4_max: 45
    apples_agent-4_mean: 0.7
    apples_agent-4_min: 0
    apples_agent-5_max: 187
    apples_agent-5_mean: 98.32
    apples_agent-5_min: 21
    cleaning_beam_agent-0_max: 494
    cleaning_beam_agent-0_mean: 373.73
    cleaning_beam_agent-0_min: 261
    cleaning_beam_agent-1_max: 280
    cleaning_beam_agent-1_mean: 198.09
    cleaning_beam_agent-1_min: 106
    cleaning_beam_agent-2_max: 438
    cleaning_beam_agent-2_mean: 206.31
    cleaning_beam_agent-2_min: 91
    cleaning_beam_agent-3_max: 215
    cleaning_beam_agent-3_mean: 45.43
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 578
    cleaning_beam_agent-4_mean: 376.69
    cleaning_beam_agent-4_min: 188
    cleaning_beam_agent-5_max: 563
    cleaning_beam_agent-5_mean: 91.18
    cleaning_beam_agent-5_min: 14
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-05-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 942.9999999999558
  episode_reward_mean: 734.5699999999883
  episode_reward_min: 267.9999999999984
  episodes_this_iter: 96
  episodes_total: 17760
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20243.87
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00015776639338582754
        entropy: 1.072391152381897
        entropy_coeff: 0.0017600000137463212
        kl: 0.007445097900927067
        model: {}
        policy_loss: -0.01945863850414753
        total_loss: -0.018979642540216446
        vf_explained_var: 0.06499704718589783
        vf_loss: 16.21895980834961
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00015776639338582754
        entropy: 1.119664192199707
        entropy_coeff: 0.0017600000137463212
        kl: 0.009419498965144157
        model: {}
        policy_loss: -0.023560091853141785
        total_loss: -0.02288012020289898
        vf_explained_var: 0.014378443360328674
        vf_loss: 17.086341857910156
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.00015776639338582754
        entropy: 1.1407475471496582
        entropy_coeff: 0.0017600000137463212
        kl: 0.007911795750260353
        model: {}
        policy_loss: -0.0213465578854084
        total_loss: -0.020528005436062813
        vf_explained_var: 0.054772987961769104
        vf_loss: 16.394981384277344
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00015776639338582754
        entropy: 0.5061051249504089
        entropy_coeff: 0.0017600000137463212
        kl: 0.005094511434435844
        model: {}
        policy_loss: -0.013415060006082058
        total_loss: -0.012435050681233406
        vf_explained_var: 0.21390606462955475
        vf_loss: 13.61302375793457
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00015776639338582754
        entropy: 0.9470939636230469
        entropy_coeff: 0.0017600000137463212
        kl: 0.00788665097206831
        model: {}
        policy_loss: -0.021199190989136696
        total_loss: -0.020433494821190834
        vf_explained_var: 0.05092927813529968
        vf_loss: 16.439159393310547
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00015776639338582754
        entropy: 0.9470201730728149
        entropy_coeff: 0.0017600000137463212
        kl: 0.008836893364787102
        model: {}
        policy_loss: -0.023470882326364517
        total_loss: -0.02267473004758358
        vf_explained_var: 0.08963775634765625
        vf_loss: 15.792198181152344
    load_time_ms: 14661.301
    num_steps_sampled: 17760000
    num_steps_trained: 17760000
    sample_time_ms: 90467.528
    update_time_ms: 23.116
  iterations_since_restore: 165
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.356571428571426
    ram_util_percent: 9.670857142857146
  pid: 4061
  policy_reward_max:
    agent-0: 157.1666666666667
    agent-1: 157.1666666666667
    agent-2: 157.1666666666667
    agent-3: 157.1666666666667
    agent-4: 157.1666666666667
    agent-5: 157.1666666666667
  policy_reward_mean:
    agent-0: 122.4283333333337
    agent-1: 122.4283333333337
    agent-2: 122.4283333333337
    agent-3: 122.4283333333337
    agent-4: 122.4283333333337
    agent-5: 122.4283333333337
  policy_reward_min:
    agent-0: 44.666666666666586
    agent-1: 44.666666666666586
    agent-2: 44.666666666666586
    agent-3: 44.666666666666586
    agent-4: 44.666666666666586
    agent-5: 44.666666666666586
  sampler_perf:
    mean_env_wait_ms: 23.667411016937127
    mean_inference_ms: 12.31230530069776
    mean_processing_ms: 50.88292741256424
  time_since_restore: 21552.58326435089
  time_this_iter_s: 122.69241142272949
  time_total_s: 24763.646950483322
  timestamp: 1637039104
  timesteps_since_restore: 15840000
  timesteps_this_iter: 96000
  timesteps_total: 17760000
  training_iteration: 185
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    185 |          24763.6 | 17760000 |   734.57 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 27
    apples_agent-0_mean: 2.21
    apples_agent-0_min: 0
    apples_agent-1_max: 145
    apples_agent-1_mean: 25.07
    apples_agent-1_min: 0
    apples_agent-2_max: 87
    apples_agent-2_mean: 12.46
    apples_agent-2_min: 0
    apples_agent-3_max: 123
    apples_agent-3_mean: 65.02
    apples_agent-3_min: 34
    apples_agent-4_max: 53
    apples_agent-4_mean: 1.19
    apples_agent-4_min: 0
    apples_agent-5_max: 157
    apples_agent-5_mean: 96.74
    apples_agent-5_min: 19
    cleaning_beam_agent-0_max: 447
    cleaning_beam_agent-0_mean: 361.96
    cleaning_beam_agent-0_min: 247
    cleaning_beam_agent-1_max: 336
    cleaning_beam_agent-1_mean: 186.76
    cleaning_beam_agent-1_min: 65
    cleaning_beam_agent-2_max: 388
    cleaning_beam_agent-2_mean: 208.08
    cleaning_beam_agent-2_min: 76
    cleaning_beam_agent-3_max: 110
    cleaning_beam_agent-3_mean: 36.52
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 483
    cleaning_beam_agent-4_mean: 379.14
    cleaning_beam_agent-4_min: 178
    cleaning_beam_agent-5_max: 402
    cleaning_beam_agent-5_mean: 76.31
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-07-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 960.9999999999745
  episode_reward_mean: 742.509999999987
  episode_reward_min: 279.99999999999795
  episodes_this_iter: 96
  episodes_total: 17856
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20229.823
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00015177599561866373
        entropy: 1.0735535621643066
        entropy_coeff: 0.0017600000137463212
        kl: 0.006943678017705679
        model: {}
        policy_loss: -0.01927105151116848
        total_loss: -0.018769700080156326
        vf_explained_var: 0.09748807549476624
        vf_loss: 16.9643611907959
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00015177599561866373
        entropy: 1.1132055521011353
        entropy_coeff: 0.0017600000137463212
        kl: 0.00839306227862835
        model: {}
        policy_loss: -0.022428888827562332
        total_loss: -0.021676765754818916
        vf_explained_var: 0.005165368318557739
        vf_loss: 18.720539093017578
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.00015177599561866373
        entropy: 1.1238187551498413
        entropy_coeff: 0.0017600000137463212
        kl: 0.00814833864569664
        model: {}
        policy_loss: -0.021211184561252594
        total_loss: -0.02021213248372078
        vf_explained_var: 0.06655445694923401
        vf_loss: 17.54721450805664
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00015177599561866373
        entropy: 0.5076816082000732
        entropy_coeff: 0.0017600000137463212
        kl: 0.005021343939006329
        model: {}
        policy_loss: -0.013369171880185604
        total_loss: -0.012278211303055286
        vf_explained_var: 0.21146683394908905
        vf_loss: 14.823454856872559
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00015177599561866373
        entropy: 0.9516931772232056
        entropy_coeff: 0.0017600000137463212
        kl: 0.00786500796675682
        model: {}
        policy_loss: -0.02108381688594818
        total_loss: -0.020240213721990585
        vf_explained_var: 0.07841750979423523
        vf_loss: 17.320804595947266
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00015177599561866373
        entropy: 0.9446380138397217
        entropy_coeff: 0.0017600000137463212
        kl: 0.00877563375979662
        model: {}
        policy_loss: -0.022631622850894928
        total_loss: -0.021865729242563248
        vf_explained_var: 0.17486697435379028
        vf_loss: 15.508975982666016
    load_time_ms: 14702.531
    num_steps_sampled: 17856000
    num_steps_trained: 17856000
    sample_time_ms: 90184.95
    update_time_ms: 21.175
  iterations_since_restore: 166
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.431249999999999
    ram_util_percent: 9.600000000000001
  pid: 4061
  policy_reward_max:
    agent-0: 160.16666666666637
    agent-1: 160.16666666666637
    agent-2: 160.16666666666637
    agent-3: 160.16666666666637
    agent-4: 160.16666666666637
    agent-5: 160.16666666666637
  policy_reward_mean:
    agent-0: 123.751666666667
    agent-1: 123.751666666667
    agent-2: 123.751666666667
    agent-3: 123.751666666667
    agent-4: 123.751666666667
    agent-5: 123.751666666667
  policy_reward_min:
    agent-0: 46.66666666666672
    agent-1: 46.66666666666672
    agent-2: 46.66666666666672
    agent-3: 46.66666666666672
    agent-4: 46.66666666666672
    agent-5: 46.66666666666672
  sampler_perf:
    mean_env_wait_ms: 23.666882119092033
    mean_inference_ms: 12.311285704557655
    mean_processing_ms: 50.87939378190909
  time_since_restore: 21676.23426771164
  time_this_iter_s: 123.65100336074829
  time_total_s: 24887.29795384407
  timestamp: 1637039228
  timesteps_since_restore: 15936000
  timesteps_this_iter: 96000
  timesteps_total: 17856000
  training_iteration: 186
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    186 |          24887.3 | 17856000 |   742.51 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 76
    apples_agent-0_mean: 3.9
    apples_agent-0_min: 0
    apples_agent-1_max: 78
    apples_agent-1_mean: 21.11
    apples_agent-1_min: 0
    apples_agent-2_max: 116
    apples_agent-2_mean: 14.19
    apples_agent-2_min: 0
    apples_agent-3_max: 123
    apples_agent-3_mean: 63.73
    apples_agent-3_min: 27
    apples_agent-4_max: 42
    apples_agent-4_mean: 0.95
    apples_agent-4_min: 0
    apples_agent-5_max: 203
    apples_agent-5_mean: 98.47
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 519
    cleaning_beam_agent-0_mean: 365.3
    cleaning_beam_agent-0_min: 227
    cleaning_beam_agent-1_max: 359
    cleaning_beam_agent-1_mean: 196.18
    cleaning_beam_agent-1_min: 96
    cleaning_beam_agent-2_max: 388
    cleaning_beam_agent-2_mean: 212.28
    cleaning_beam_agent-2_min: 56
    cleaning_beam_agent-3_max: 101
    cleaning_beam_agent-3_mean: 38.61
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 474
    cleaning_beam_agent-4_mean: 367.81
    cleaning_beam_agent-4_min: 198
    cleaning_beam_agent-5_max: 694
    cleaning_beam_agent-5_mean: 102.68
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-09-11
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 920.9999999999809
  episode_reward_mean: 751.6399999999866
  episode_reward_min: 257.99999999999653
  episodes_this_iter: 96
  episodes_total: 17952
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20244.335
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00014578559785149992
        entropy: 1.061840295791626
        entropy_coeff: 0.0017600000137463212
        kl: 0.007433110848069191
        model: {}
        policy_loss: -0.01942199468612671
        total_loss: -0.01880958303809166
        vf_explained_var: 0.052545756101608276
        vf_loss: 17.37937355041504
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00014578559785149992
        entropy: 1.1285403966903687
        entropy_coeff: 0.0017600000137463212
        kl: 0.008535346016287804
        model: {}
        policy_loss: -0.02278311736881733
        total_loss: -0.022103343158960342
        vf_explained_var: 0.011310622096061707
        vf_loss: 18.124719619750977
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.00014578559785149992
        entropy: 1.1206793785095215
        entropy_coeff: 0.0017600000137463212
        kl: 0.007743095047771931
        model: {}
        policy_loss: -0.021432576701045036
        total_loss: -0.02047405019402504
        vf_explained_var: 0.03644879162311554
        vf_loss: 17.694595336914062
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00014578559785149992
        entropy: 0.4930230677127838
        entropy_coeff: 0.0017600000137463212
        kl: 0.005016576498746872
        model: {}
        policy_loss: -0.013284849002957344
        total_loss: -0.012238766998052597
        vf_explained_var: 0.22909429669380188
        vf_loss: 14.121440887451172
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00014578559785149992
        entropy: 0.956174910068512
        entropy_coeff: 0.0017600000137463212
        kl: 0.008109680376946926
        model: {}
        policy_loss: -0.02124079316854477
        total_loss: -0.02037821151316166
        vf_explained_var: 0.0532086044549942
        vf_loss: 17.344833374023438
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00014578559785149992
        entropy: 0.922541618347168
        entropy_coeff: 0.0017600000137463212
        kl: 0.007749222684651613
        model: {}
        policy_loss: -0.021351544186472893
        total_loss: -0.020586766302585602
        vf_explained_var: 0.12094096839427948
        vf_loss: 16.13525390625
    load_time_ms: 14663.065
    num_steps_sampled: 17952000
    num_steps_trained: 17952000
    sample_time_ms: 90084.801
    update_time_ms: 21.361
  iterations_since_restore: 167
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.267613636363638
    ram_util_percent: 9.656818181818181
  pid: 4061
  policy_reward_max:
    agent-0: 153.50000000000003
    agent-1: 153.50000000000003
    agent-2: 153.50000000000003
    agent-3: 153.50000000000003
    agent-4: 153.50000000000003
    agent-5: 153.50000000000003
  policy_reward_mean:
    agent-0: 125.27333333333368
    agent-1: 125.27333333333368
    agent-2: 125.27333333333368
    agent-3: 125.27333333333368
    agent-4: 125.27333333333368
    agent-5: 125.27333333333368
  policy_reward_min:
    agent-0: 43.0
    agent-1: 43.0
    agent-2: 43.0
    agent-3: 43.0
    agent-4: 43.0
    agent-5: 43.0
  sampler_perf:
    mean_env_wait_ms: 23.667157030391976
    mean_inference_ms: 12.310536488219451
    mean_processing_ms: 50.87476019327066
  time_since_restore: 21799.12086224556
  time_this_iter_s: 122.88659453392029
  time_total_s: 25010.18454837799
  timestamp: 1637039351
  timesteps_since_restore: 16032000
  timesteps_this_iter: 96000
  timesteps_total: 17952000
  training_iteration: 187
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    187 |          25010.2 | 17952000 |   751.64 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 45
    apples_agent-0_mean: 3.16
    apples_agent-0_min: 0
    apples_agent-1_max: 82
    apples_agent-1_mean: 22.7
    apples_agent-1_min: 0
    apples_agent-2_max: 69
    apples_agent-2_mean: 13.7
    apples_agent-2_min: 0
    apples_agent-3_max: 110
    apples_agent-3_mean: 66.34
    apples_agent-3_min: 29
    apples_agent-4_max: 23
    apples_agent-4_mean: 0.38
    apples_agent-4_min: 0
    apples_agent-5_max: 195
    apples_agent-5_mean: 94.68
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 472
    cleaning_beam_agent-0_mean: 366.92
    cleaning_beam_agent-0_min: 256
    cleaning_beam_agent-1_max: 352
    cleaning_beam_agent-1_mean: 188.55
    cleaning_beam_agent-1_min: 96
    cleaning_beam_agent-2_max: 420
    cleaning_beam_agent-2_mean: 220.2
    cleaning_beam_agent-2_min: 77
    cleaning_beam_agent-3_max: 103
    cleaning_beam_agent-3_mean: 34.74
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 495
    cleaning_beam_agent-4_mean: 374.97
    cleaning_beam_agent-4_min: 197
    cleaning_beam_agent-5_max: 734
    cleaning_beam_agent-5_mean: 114.29
    cleaning_beam_agent-5_min: 14
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-11-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 978.9999999999814
  episode_reward_mean: 743.0099999999902
  episode_reward_min: 168.99999999999892
  episodes_this_iter: 96
  episodes_total: 18048
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20219.364
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001397952000843361
        entropy: 1.0547229051589966
        entropy_coeff: 0.0017600000137463212
        kl: 0.007758785039186478
        model: {}
        policy_loss: -0.01971188187599182
        total_loss: -0.019073456525802612
        vf_explained_var: 0.09198103845119476
        vf_loss: 17.18856430053711
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001397952000843361
        entropy: 1.117305874824524
        entropy_coeff: 0.0017600000137463212
        kl: 0.008503098040819168
        model: {}
        policy_loss: -0.02201997861266136
        total_loss: -0.02126668021082878
        vf_explained_var: 0.013123080134391785
        vf_loss: 18.694509506225586
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.0001397952000843361
        entropy: 1.134822964668274
        entropy_coeff: 0.0017600000137463212
        kl: 0.007922105491161346
        model: {}
        policy_loss: -0.020450182259082794
        total_loss: -0.019444698467850685
        vf_explained_var: 0.040886834263801575
        vf_loss: 18.144561767578125
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001397952000843361
        entropy: 0.49198466539382935
        entropy_coeff: 0.0017600000137463212
        kl: 0.004872350953519344
        model: {}
        policy_loss: -0.013099540956318378
        total_loss: -0.011984316632151604
        vf_explained_var: 0.21059735119342804
        vf_loss: 14.938846588134766
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001397952000843361
        entropy: 0.9655439257621765
        entropy_coeff: 0.0017600000137463212
        kl: 0.008380791172385216
        model: {}
        policy_loss: -0.02100970596075058
        total_loss: -0.02015106752514839
        vf_explained_var: 0.09122394025325775
        vf_loss: 17.199174880981445
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001397952000843361
        entropy: 0.9171711802482605
        entropy_coeff: 0.0017600000137463212
        kl: 0.008049121126532555
        model: {}
        policy_loss: -0.021938791498541832
        total_loss: -0.021108876913785934
        vf_explained_var: 0.1349952667951584
        vf_loss: 16.392215728759766
    load_time_ms: 14683.673
    num_steps_sampled: 18048000
    num_steps_trained: 18048000
    sample_time_ms: 90096.697
    update_time_ms: 21.696
  iterations_since_restore: 168
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.89776536312849
    ram_util_percent: 9.592737430167599
  pid: 4061
  policy_reward_max:
    agent-0: 163.16666666666666
    agent-1: 163.16666666666666
    agent-2: 163.16666666666666
    agent-3: 163.16666666666666
    agent-4: 163.16666666666666
    agent-5: 163.16666666666666
  policy_reward_mean:
    agent-0: 123.83500000000035
    agent-1: 123.83500000000035
    agent-2: 123.83500000000035
    agent-3: 123.83500000000035
    agent-4: 123.83500000000035
    agent-5: 123.83500000000035
  policy_reward_min:
    agent-0: 28.166666666666725
    agent-1: 28.166666666666725
    agent-2: 28.166666666666725
    agent-3: 28.166666666666725
    agent-4: 28.166666666666725
    agent-5: 28.166666666666725
  sampler_perf:
    mean_env_wait_ms: 23.668048147912863
    mean_inference_ms: 12.309891362617003
    mean_processing_ms: 50.8723109470872
  time_since_restore: 21924.78016424179
  time_this_iter_s: 125.65930199623108
  time_total_s: 25135.84385037422
  timestamp: 1637039476
  timesteps_since_restore: 16128000
  timesteps_this_iter: 96000
  timesteps_total: 18048000
  training_iteration: 188
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    188 |          25135.8 | 18048000 |   743.01 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 65
    apples_agent-0_mean: 3.4
    apples_agent-0_min: 0
    apples_agent-1_max: 83
    apples_agent-1_mean: 26.68
    apples_agent-1_min: 1
    apples_agent-2_max: 88
    apples_agent-2_mean: 9.42
    apples_agent-2_min: 0
    apples_agent-3_max: 128
    apples_agent-3_mean: 65.03
    apples_agent-3_min: 35
    apples_agent-4_max: 31
    apples_agent-4_mean: 1.13
    apples_agent-4_min: 0
    apples_agent-5_max: 174
    apples_agent-5_mean: 91.43
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 493
    cleaning_beam_agent-0_mean: 367.13
    cleaning_beam_agent-0_min: 266
    cleaning_beam_agent-1_max: 268
    cleaning_beam_agent-1_mean: 179.35
    cleaning_beam_agent-1_min: 94
    cleaning_beam_agent-2_max: 359
    cleaning_beam_agent-2_mean: 221.08
    cleaning_beam_agent-2_min: 75
    cleaning_beam_agent-3_max: 88
    cleaning_beam_agent-3_mean: 30.82
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 530
    cleaning_beam_agent-4_mean: 374.46
    cleaning_beam_agent-4_min: 183
    cleaning_beam_agent-5_max: 766
    cleaning_beam_agent-5_mean: 130.61
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-13-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 978.9999999999766
  episode_reward_mean: 758.5099999999876
  episode_reward_min: 313.00000000000125
  episodes_this_iter: 96
  episodes_total: 18144
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20243.844
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001338048023171723
        entropy: 1.065185308456421
        entropy_coeff: 0.0017600000137463212
        kl: 0.006958186626434326
        model: {}
        policy_loss: -0.018563609570264816
        total_loss: -0.018047122284770012
        vf_explained_var: 0.05590906739234924
        vf_loss: 16.953937530517578
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001338048023171723
        entropy: 1.1305434703826904
        entropy_coeff: 0.0017600000137463212
        kl: 0.009006241336464882
        model: {}
        policy_loss: -0.021588362753391266
        total_loss: -0.020929649472236633
        vf_explained_var: 0.02708207070827484
        vf_loss: 17.478418350219727
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.0001338048023171723
        entropy: 1.1280055046081543
        entropy_coeff: 0.0017600000137463212
        kl: 0.006720828823745251
        model: {}
        policy_loss: -0.019455337896943092
        total_loss: -0.018803568556904793
        vf_explained_var: 0.09391836822032928
        vf_loss: 16.28931999206543
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0001338048023171723
        entropy: 0.4722856283187866
        entropy_coeff: 0.0017600000137463212
        kl: 0.004584736190736294
        model: {}
        policy_loss: -0.01137117575854063
        total_loss: -0.010533538646996021
        vf_explained_var: 0.19831757247447968
        vf_loss: 14.396224975585938
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001338048023171723
        entropy: 0.9626581072807312
        entropy_coeff: 0.0017600000137463212
        kl: 0.007684600539505482
        model: {}
        policy_loss: -0.020336125046014786
        total_loss: -0.019590599462389946
        vf_explained_var: 0.06915619969367981
        vf_loss: 16.71344566345215
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001338048023171723
        entropy: 0.905029296875
        entropy_coeff: 0.0017600000137463212
        kl: 0.007996865548193455
        model: {}
        policy_loss: -0.020861484110355377
        total_loss: -0.020041916519403458
        vf_explained_var: 0.1027391254901886
        vf_loss: 16.1273193359375
    load_time_ms: 14601.966
    num_steps_sampled: 18144000
    num_steps_trained: 18144000
    sample_time_ms: 89818.644
    update_time_ms: 21.735
  iterations_since_restore: 169
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.382857142857143
    ram_util_percent: 9.637714285714289
  pid: 4061
  policy_reward_max:
    agent-0: 163.16666666666632
    agent-1: 163.16666666666632
    agent-2: 163.16666666666632
    agent-3: 163.16666666666632
    agent-4: 163.16666666666632
    agent-5: 163.16666666666632
  policy_reward_mean:
    agent-0: 126.41833333333365
    agent-1: 126.41833333333365
    agent-2: 126.41833333333365
    agent-3: 126.41833333333365
    agent-4: 126.41833333333365
    agent-5: 126.41833333333365
  policy_reward_min:
    agent-0: 52.16666666666653
    agent-1: 52.16666666666653
    agent-2: 52.16666666666653
    agent-3: 52.16666666666653
    agent-4: 52.16666666666653
    agent-5: 52.16666666666653
  sampler_perf:
    mean_env_wait_ms: 23.66831944528302
    mean_inference_ms: 12.308978171485771
    mean_processing_ms: 50.86670014418339
  time_since_restore: 22047.52632522583
  time_this_iter_s: 122.7461609840393
  time_total_s: 25258.59001135826
  timestamp: 1637039599
  timesteps_since_restore: 16224000
  timesteps_this_iter: 96000
  timesteps_total: 18144000
  training_iteration: 189
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    189 |          25258.6 | 18144000 |   758.51 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 38
    apples_agent-0_mean: 2.72
    apples_agent-0_min: 0
    apples_agent-1_max: 75
    apples_agent-1_mean: 23.15
    apples_agent-1_min: 0
    apples_agent-2_max: 90
    apples_agent-2_mean: 12.35
    apples_agent-2_min: 0
    apples_agent-3_max: 112
    apples_agent-3_mean: 67.32
    apples_agent-3_min: 37
    apples_agent-4_max: 62
    apples_agent-4_mean: 1.43
    apples_agent-4_min: 0
    apples_agent-5_max: 189
    apples_agent-5_mean: 101.91
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 474
    cleaning_beam_agent-0_mean: 358.34
    cleaning_beam_agent-0_min: 251
    cleaning_beam_agent-1_max: 313
    cleaning_beam_agent-1_mean: 183.85
    cleaning_beam_agent-1_min: 95
    cleaning_beam_agent-2_max: 390
    cleaning_beam_agent-2_mean: 221.52
    cleaning_beam_agent-2_min: 99
    cleaning_beam_agent-3_max: 88
    cleaning_beam_agent-3_mean: 32.03
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 497
    cleaning_beam_agent-4_mean: 366.35
    cleaning_beam_agent-4_min: 169
    cleaning_beam_agent-5_max: 746
    cleaning_beam_agent-5_mean: 86.39
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-15-22
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 947.9999999999943
  episode_reward_mean: 767.2499999999877
  episode_reward_min: 310.99999999999807
  episodes_this_iter: 96
  episodes_total: 18240
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20220.769
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00012781440455000848
        entropy: 1.03791081905365
        entropy_coeff: 0.0017600000137463212
        kl: 0.006966630928218365
        model: {}
        policy_loss: -0.01808711513876915
        total_loss: -0.017556525766849518
        vf_explained_var: 0.0516703724861145
        vf_loss: 16.60649871826172
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00012781440455000848
        entropy: 1.1352322101593018
        entropy_coeff: 0.0017600000137463212
        kl: 0.0076962001621723175
        model: {}
        policy_loss: -0.02018553391098976
        total_loss: -0.019629718735814095
        vf_explained_var: -0.017288535833358765
        vf_loss: 17.842086791992188
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.00012781440455000848
        entropy: 1.1304261684417725
        entropy_coeff: 0.0017600000137463212
        kl: 0.006947298999875784
        model: {}
        policy_loss: -0.01926475577056408
        total_loss: -0.01855982467532158
        vf_explained_var: 0.05848833918571472
        vf_loss: 16.523893356323242
      agent-3:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.00012781440455000848
        entropy: 0.483574777841568
        entropy_coeff: 0.0017600000137463212
        kl: 0.004740951582789421
        model: {}
        policy_loss: -0.011386590078473091
        total_loss: -0.010694384574890137
        vf_explained_var: 0.18618012964725494
        vf_loss: 14.247706413269043
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00012781440455000848
        entropy: 0.966518759727478
        entropy_coeff: 0.0017600000137463212
        kl: 0.007725893519818783
        model: {}
        policy_loss: -0.019518522545695305
        total_loss: -0.018866408616304398
        vf_explained_var: 0.09683951735496521
        vf_loss: 15.805975914001465
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00012781440455000848
        entropy: 0.9323525428771973
        entropy_coeff: 0.0017600000137463212
        kl: 0.007559873629361391
        model: {}
        policy_loss: -0.020208416506648064
        total_loss: -0.01953308656811714
        vf_explained_var: 0.1088169515132904
        vf_loss: 15.602800369262695
    load_time_ms: 14417.49
    num_steps_sampled: 18240000
    num_steps_trained: 18240000
    sample_time_ms: 89440.959
    update_time_ms: 22.134
  iterations_since_restore: 170
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.493641618497113
    ram_util_percent: 9.571098265895957
  pid: 4061
  policy_reward_max:
    agent-0: 158.0000000000002
    agent-1: 158.0000000000002
    agent-2: 158.0000000000002
    agent-3: 158.0000000000002
    agent-4: 158.0000000000002
    agent-5: 158.0000000000002
  policy_reward_mean:
    agent-0: 127.87500000000038
    agent-1: 127.87500000000038
    agent-2: 127.87500000000038
    agent-3: 127.87500000000038
    agent-4: 127.87500000000038
    agent-5: 127.87500000000038
  policy_reward_min:
    agent-0: 51.8333333333332
    agent-1: 51.8333333333332
    agent-2: 51.8333333333332
    agent-3: 51.8333333333332
    agent-4: 51.8333333333332
    agent-5: 51.8333333333332
  sampler_perf:
    mean_env_wait_ms: 23.667865080258238
    mean_inference_ms: 12.3081820252036
    mean_processing_ms: 50.86187752213931
  time_since_restore: 22169.53627705574
  time_this_iter_s: 122.00995182991028
  time_total_s: 25380.59996318817
  timestamp: 1637039722
  timesteps_since_restore: 16320000
  timesteps_this_iter: 96000
  timesteps_total: 18240000
  training_iteration: 190
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    190 |          25380.6 | 18240000 |   767.25 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 39
    apples_agent-0_mean: 2.86
    apples_agent-0_min: 0
    apples_agent-1_max: 159
    apples_agent-1_mean: 24.45
    apples_agent-1_min: 0
    apples_agent-2_max: 70
    apples_agent-2_mean: 9.98
    apples_agent-2_min: 0
    apples_agent-3_max: 123
    apples_agent-3_mean: 67.68
    apples_agent-3_min: 15
    apples_agent-4_max: 51
    apples_agent-4_mean: 2.0
    apples_agent-4_min: 0
    apples_agent-5_max: 194
    apples_agent-5_mean: 100.07
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 539
    cleaning_beam_agent-0_mean: 366.08
    cleaning_beam_agent-0_min: 231
    cleaning_beam_agent-1_max: 299
    cleaning_beam_agent-1_mean: 197.45
    cleaning_beam_agent-1_min: 91
    cleaning_beam_agent-2_max: 392
    cleaning_beam_agent-2_mean: 231.51
    cleaning_beam_agent-2_min: 76
    cleaning_beam_agent-3_max: 256
    cleaning_beam_agent-3_mean: 31.62
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 469
    cleaning_beam_agent-4_mean: 361.89
    cleaning_beam_agent-4_min: 220
    cleaning_beam_agent-5_max: 776
    cleaning_beam_agent-5_mean: 113.24
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-17-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 995.999999999967
  episode_reward_mean: 772.9799999999856
  episode_reward_min: 356.00000000000443
  episodes_this_iter: 96
  episodes_total: 18336
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20237.541
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00012182399950688705
        entropy: 1.043190598487854
        entropy_coeff: 0.0017600000137463212
        kl: 0.006283793598413467
        model: {}
        policy_loss: -0.017596352845430374
        total_loss: -0.016983838751912117
        vf_explained_var: 0.01131691038608551
        vf_loss: 18.20148468017578
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00012182399950688705
        entropy: 1.1300194263458252
        entropy_coeff: 0.0017600000137463212
        kl: 0.0078111980110406876
        model: {}
        policy_loss: -0.02075311541557312
        total_loss: -0.020156115293502808
        vf_explained_var: 0.018782377243041992
        vf_loss: 18.04714012145996
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.00012182399950688705
        entropy: 1.1245776414871216
        entropy_coeff: 0.0017600000137463212
        kl: 0.006754431873559952
        model: {}
        policy_loss: -0.018928445875644684
        total_loss: -0.018161188811063766
        vf_explained_var: 0.05754369497299194
        vf_loss: 17.33346939086914
      agent-3:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 0.00012182399950688705
        entropy: 0.4648549556732178
        entropy_coeff: 0.0017600000137463212
        kl: 0.004908586852252483
        model: {}
        policy_loss: -0.011293590068817139
        total_loss: -0.01055931206792593
        vf_explained_var: 0.1891338974237442
        vf_loss: 14.91070556640625
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00012182399950688705
        entropy: 0.9640752673149109
        entropy_coeff: 0.0017600000137463212
        kl: 0.0068964227102696896
        model: {}
        policy_loss: -0.019154027104377747
        total_loss: -0.01847761869430542
        vf_explained_var: 0.08406016230583191
        vf_loss: 16.835371017456055
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00012182399950688705
        entropy: 0.9047186970710754
        entropy_coeff: 0.0017600000137463212
        kl: 0.007056981325149536
        model: {}
        policy_loss: -0.019223619252443314
        total_loss: -0.018445465713739395
        vf_explained_var: 0.09623458981513977
        vf_loss: 16.64762306213379
    load_time_ms: 14467.314
    num_steps_sampled: 18336000
    num_steps_trained: 18336000
    sample_time_ms: 89222.872
    update_time_ms: 22.13
  iterations_since_restore: 171
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.2864406779661
    ram_util_percent: 9.667231638418079
  pid: 4061
  policy_reward_max:
    agent-0: 165.9999999999998
    agent-1: 165.9999999999998
    agent-2: 165.9999999999998
    agent-3: 165.9999999999998
    agent-4: 165.9999999999998
    agent-5: 165.9999999999998
  policy_reward_mean:
    agent-0: 128.8300000000003
    agent-1: 128.8300000000003
    agent-2: 128.8300000000003
    agent-3: 128.8300000000003
    agent-4: 128.8300000000003
    agent-5: 128.8300000000003
  policy_reward_min:
    agent-0: 59.333333333333094
    agent-1: 59.333333333333094
    agent-2: 59.333333333333094
    agent-3: 59.333333333333094
    agent-4: 59.333333333333094
    agent-5: 59.333333333333094
  sampler_perf:
    mean_env_wait_ms: 23.66841496071563
    mean_inference_ms: 12.307363414033832
    mean_processing_ms: 50.85809897315837
  time_since_restore: 22293.46543431282
  time_this_iter_s: 123.92915725708008
  time_total_s: 25504.52912044525
  timestamp: 1637039846
  timesteps_since_restore: 16416000
  timesteps_this_iter: 96000
  timesteps_total: 18336000
  training_iteration: 191
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    191 |          25504.5 | 18336000 |   772.98 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 51
    apples_agent-0_mean: 3.05
    apples_agent-0_min: 0
    apples_agent-1_max: 119
    apples_agent-1_mean: 26.28
    apples_agent-1_min: 0
    apples_agent-2_max: 99
    apples_agent-2_mean: 13.73
    apples_agent-2_min: 0
    apples_agent-3_max: 118
    apples_agent-3_mean: 67.21
    apples_agent-3_min: 28
    apples_agent-4_max: 58
    apples_agent-4_mean: 1.54
    apples_agent-4_min: 0
    apples_agent-5_max: 273
    apples_agent-5_mean: 92.51
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 497
    cleaning_beam_agent-0_mean: 360.63
    cleaning_beam_agent-0_min: 246
    cleaning_beam_agent-1_max: 292
    cleaning_beam_agent-1_mean: 170.55
    cleaning_beam_agent-1_min: 62
    cleaning_beam_agent-2_max: 432
    cleaning_beam_agent-2_mean: 225.54
    cleaning_beam_agent-2_min: 57
    cleaning_beam_agent-3_max: 111
    cleaning_beam_agent-3_mean: 33.41
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 477
    cleaning_beam_agent-4_mean: 368.8
    cleaning_beam_agent-4_min: 223
    cleaning_beam_agent-5_max: 891
    cleaning_beam_agent-5_mean: 136.44
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-19-30
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 971.9999999999852
  episode_reward_mean: 744.1399999999882
  episode_reward_min: 275.9999999999981
  episodes_this_iter: 96
  episodes_total: 18432
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20255.866
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00011583360173972324
        entropy: 1.053273320198059
        entropy_coeff: 0.0017600000137463212
        kl: 0.007028391119092703
        model: {}
        policy_loss: -0.017953503876924515
        total_loss: -0.01744648441672325
        vf_explained_var: 0.08249643445014954
        vf_loss: 16.579452514648438
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00011583360173972324
        entropy: 1.1471986770629883
        entropy_coeff: 0.0017600000137463212
        kl: 0.007293287664651871
        model: {}
        policy_loss: -0.019640235230326653
        total_loss: -0.019124440848827362
        vf_explained_var: 0.0006653368473052979
        vf_loss: 18.05535125732422
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.00011583360173972324
        entropy: 1.1260559558868408
        entropy_coeff: 0.0017600000137463212
        kl: 0.006674311123788357
        model: {}
        policy_loss: -0.01850665546953678
        total_loss: -0.01779843680560589
        vf_explained_var: 0.0652986615896225
        vf_loss: 16.889259338378906
      agent-3:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 0.00011583360173972324
        entropy: 0.4953128695487976
        entropy_coeff: 0.0017600000137463212
        kl: 0.005593783222138882
        model: {}
        policy_loss: -0.011871181428432465
        total_loss: -0.011221326887607574
        vf_explained_var: 0.17729750275611877
        vf_loss: 14.866448402404785
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00011583360173972324
        entropy: 0.962927520275116
        entropy_coeff: 0.0017600000137463212
        kl: 0.006811324041336775
        model: {}
        policy_loss: -0.01839601621031761
        total_loss: -0.0177187267690897
        vf_explained_var: 0.0644894391298294
        vf_loss: 16.909107208251953
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00011583360173972324
        entropy: 0.9031491875648499
        entropy_coeff: 0.0017600000137463212
        kl: 0.0069739557802677155
        model: {}
        policy_loss: -0.019143100827932358
        total_loss: -0.01843510940670967
        vf_explained_var: 0.11384746432304382
        vf_loss: 16.001373291015625
    load_time_ms: 14454.467
    num_steps_sampled: 18432000
    num_steps_trained: 18432000
    sample_time_ms: 89144.346
    update_time_ms: 21.706
  iterations_since_restore: 172
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.445762711864408
    ram_util_percent: 9.602824858757064
  pid: 4061
  policy_reward_max:
    agent-0: 161.9999999999996
    agent-1: 161.9999999999996
    agent-2: 161.9999999999996
    agent-3: 161.9999999999996
    agent-4: 161.9999999999996
    agent-5: 161.9999999999996
  policy_reward_mean:
    agent-0: 124.02333333333367
    agent-1: 124.02333333333367
    agent-2: 124.02333333333367
    agent-3: 124.02333333333367
    agent-4: 124.02333333333367
    agent-5: 124.02333333333367
  policy_reward_min:
    agent-0: 45.999999999999936
    agent-1: 45.999999999999936
    agent-2: 45.999999999999936
    agent-3: 45.999999999999936
    agent-4: 45.999999999999936
    agent-5: 45.999999999999936
  sampler_perf:
    mean_env_wait_ms: 23.668176240941666
    mean_inference_ms: 12.306065932086929
    mean_processing_ms: 50.85348620522508
  time_since_restore: 22417.525674819946
  time_this_iter_s: 124.06024050712585
  time_total_s: 25628.589360952377
  timestamp: 1637039970
  timesteps_since_restore: 16512000
  timesteps_this_iter: 96000
  timesteps_total: 18432000
  training_iteration: 192
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    192 |          25628.6 | 18432000 |   744.14 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 30
    apples_agent-0_mean: 2.1
    apples_agent-0_min: 0
    apples_agent-1_max: 113
    apples_agent-1_mean: 25.9
    apples_agent-1_min: 0
    apples_agent-2_max: 84
    apples_agent-2_mean: 12.64
    apples_agent-2_min: 0
    apples_agent-3_max: 118
    apples_agent-3_mean: 69.3
    apples_agent-3_min: 31
    apples_agent-4_max: 135
    apples_agent-4_mean: 3.21
    apples_agent-4_min: 0
    apples_agent-5_max: 199
    apples_agent-5_mean: 88.44
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 475
    cleaning_beam_agent-0_mean: 351.85
    cleaning_beam_agent-0_min: 208
    cleaning_beam_agent-1_max: 328
    cleaning_beam_agent-1_mean: 186.9
    cleaning_beam_agent-1_min: 79
    cleaning_beam_agent-2_max: 441
    cleaning_beam_agent-2_mean: 225.52
    cleaning_beam_agent-2_min: 57
    cleaning_beam_agent-3_max: 88
    cleaning_beam_agent-3_mean: 29.2
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 493
    cleaning_beam_agent-4_mean: 364.86
    cleaning_beam_agent-4_min: 151
    cleaning_beam_agent-5_max: 763
    cleaning_beam_agent-5_mean: 147.69
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-21-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 946.9999999999752
  episode_reward_mean: 748.2899999999863
  episode_reward_min: 278.99999999999864
  episodes_this_iter: 96
  episodes_total: 18528
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20237.711
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00010984319669660181
        entropy: 1.0687177181243896
        entropy_coeff: 0.0017600000137463212
        kl: 0.006071618292480707
        model: {}
        policy_loss: -0.01614196226000786
        total_loss: -0.015754062682390213
        vf_explained_var: 0.06089502573013306
        vf_loss: 16.616851806640625
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00010984319669660181
        entropy: 1.149429440498352
        entropy_coeff: 0.0017600000137463212
        kl: 0.007525597233325243
        model: {}
        policy_loss: -0.019000228494405746
        total_loss: -0.018508145585656166
        vf_explained_var: 0.0025919973850250244
        vf_loss: 17.625205993652344
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.00010984319669660181
        entropy: 1.1238073110580444
        entropy_coeff: 0.0017600000137463212
        kl: 0.006255549378693104
        model: {}
        policy_loss: -0.017840996384620667
        total_loss: -0.017225969582796097
        vf_explained_var: 0.06353253126144409
        vf_loss: 16.545936584472656
      agent-3:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 0.00010984319669660181
        entropy: 0.4948523938655853
        entropy_coeff: 0.0017600000137463212
        kl: 0.005025440827012062
        model: {}
        policy_loss: -0.011016523465514183
        total_loss: -0.01039827149361372
        vf_explained_var: 0.17484377324581146
        vf_loss: 14.577828407287598
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00010984319669660181
        entropy: 0.9561954736709595
        entropy_coeff: 0.0017600000137463212
        kl: 0.0068457722663879395
        model: {}
        policy_loss: -0.01799631491303444
        total_loss: -0.01739286631345749
        vf_explained_var: 0.09309205412864685
        vf_loss: 16.01776123046875
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00010984319669660181
        entropy: 0.9010350108146667
        entropy_coeff: 0.0017600000137463212
        kl: 0.006335670128464699
        model: {}
        policy_loss: -0.018002592027187347
        total_loss: -0.017435595393180847
        vf_explained_var: 0.13964630663394928
        vf_loss: 15.19256591796875
    load_time_ms: 14166.669
    num_steps_sampled: 18528000
    num_steps_trained: 18528000
    sample_time_ms: 89237.393
    update_time_ms: 21.113
  iterations_since_restore: 173
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 16.105027932960894
    ram_util_percent: 9.668715083798883
  pid: 4061
  policy_reward_max:
    agent-0: 157.83333333333366
    agent-1: 157.83333333333366
    agent-2: 157.83333333333366
    agent-3: 157.83333333333366
    agent-4: 157.83333333333366
    agent-5: 157.83333333333366
  policy_reward_mean:
    agent-0: 124.71500000000034
    agent-1: 124.71500000000034
    agent-2: 124.71500000000034
    agent-3: 124.71500000000034
    agent-4: 124.71500000000034
    agent-5: 124.71500000000034
  policy_reward_min:
    agent-0: 46.49999999999988
    agent-1: 46.49999999999988
    agent-2: 46.49999999999988
    agent-3: 46.49999999999988
    agent-4: 46.49999999999988
    agent-5: 46.49999999999988
  sampler_perf:
    mean_env_wait_ms: 23.669607272177053
    mean_inference_ms: 12.306249144118553
    mean_processing_ms: 50.85392607932294
  time_since_restore: 22543.24886059761
  time_this_iter_s: 125.72318577766418
  time_total_s: 25754.31254673004
  timestamp: 1637040096
  timesteps_since_restore: 16608000
  timesteps_this_iter: 96000
  timesteps_total: 18528000
  training_iteration: 193
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    193 |          25754.3 | 18528000 |   748.29 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 31
    apples_agent-0_mean: 2.44
    apples_agent-0_min: 0
    apples_agent-1_max: 95
    apples_agent-1_mean: 24.97
    apples_agent-1_min: 0
    apples_agent-2_max: 72
    apples_agent-2_mean: 9.47
    apples_agent-2_min: 0
    apples_agent-3_max: 118
    apples_agent-3_mean: 66.08
    apples_agent-3_min: 32
    apples_agent-4_max: 53
    apples_agent-4_mean: 0.95
    apples_agent-4_min: 0
    apples_agent-5_max: 177
    apples_agent-5_mean: 91.24
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 457
    cleaning_beam_agent-0_mean: 353.31
    cleaning_beam_agent-0_min: 250
    cleaning_beam_agent-1_max: 311
    cleaning_beam_agent-1_mean: 174.25
    cleaning_beam_agent-1_min: 78
    cleaning_beam_agent-2_max: 493
    cleaning_beam_agent-2_mean: 233.03
    cleaning_beam_agent-2_min: 98
    cleaning_beam_agent-3_max: 128
    cleaning_beam_agent-3_mean: 26.73
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 517
    cleaning_beam_agent-4_mean: 367.13
    cleaning_beam_agent-4_min: 220
    cleaning_beam_agent-5_max: 726
    cleaning_beam_agent-5_mean: 117.49
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-23-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 948.9999999999602
  episode_reward_mean: 762.0599999999848
  episode_reward_min: 307.99999999999915
  episodes_this_iter: 96
  episodes_total: 18624
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20233.266
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000103852798929438
        entropy: 1.052802324295044
        entropy_coeff: 0.0017600000137463212
        kl: 0.005757544189691544
        model: {}
        policy_loss: -0.01605062372982502
        total_loss: -0.0155873978510499
        vf_explained_var: 0.02994804084300995
        vf_loss: 17.404014587402344
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000103852798929438
        entropy: 1.1494081020355225
        entropy_coeff: 0.0017600000137463212
        kl: 0.00716826319694519
        model: {}
        policy_loss: -0.018807735294103622
        total_loss: -0.01831105165183544
        vf_explained_var: -0.0031363070011138916
        vf_loss: 18.028156280517578
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 0.000103852798929438
        entropy: 1.1433742046356201
        entropy_coeff: 0.0017600000137463212
        kl: 0.006220159120857716
        model: {}
        policy_loss: -0.016822023317217827
        total_loss: -0.016158699989318848
        vf_explained_var: 0.02990533411502838
        vf_loss: 17.42637825012207
      agent-3:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 0.000103852798929438
        entropy: 0.4667688012123108
        entropy_coeff: 0.0017600000137463212
        kl: 0.004337362479418516
        model: {}
        policy_loss: -0.010249491780996323
        total_loss: -0.009569675661623478
        vf_explained_var: 0.1777970790863037
        vf_loss: 14.74219036102295
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000103852798929438
        entropy: 0.9644827842712402
        entropy_coeff: 0.0017600000137463212
        kl: 0.006758249364793301
        model: {}
        policy_loss: -0.017134331166744232
        total_loss: -0.01649099960923195
        vf_explained_var: 0.07124277949333191
        vf_loss: 16.64997673034668
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000103852798929438
        entropy: 0.9157581925392151
        entropy_coeff: 0.0017600000137463212
        kl: 0.006108429748564959
        model: {}
        policy_loss: -0.017488842830061913
        total_loss: -0.01688769832253456
        vf_explained_var: 0.10700516402721405
        vf_loss: 16.020387649536133
    load_time_ms: 14095.441
    num_steps_sampled: 18624000
    num_steps_trained: 18624000
    sample_time_ms: 89277.136
    update_time_ms: 21.388
  iterations_since_restore: 174
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.215254237288136
    ram_util_percent: 9.652542372881356
  pid: 4061
  policy_reward_max:
    agent-0: 158.16666666666666
    agent-1: 158.16666666666666
    agent-2: 158.16666666666666
    agent-3: 158.16666666666666
    agent-4: 158.16666666666666
    agent-5: 158.16666666666666
  policy_reward_mean:
    agent-0: 127.01000000000036
    agent-1: 127.01000000000036
    agent-2: 127.01000000000036
    agent-3: 127.01000000000036
    agent-4: 127.01000000000036
    agent-5: 127.01000000000036
  policy_reward_min:
    agent-0: 51.33333333333322
    agent-1: 51.33333333333322
    agent-2: 51.33333333333322
    agent-3: 51.33333333333322
    agent-4: 51.33333333333322
    agent-5: 51.33333333333322
  sampler_perf:
    mean_env_wait_ms: 23.670049900045097
    mean_inference_ms: 12.306036610104357
    mean_processing_ms: 50.85054321403054
  time_since_restore: 22667.296193361282
  time_this_iter_s: 124.04733276367188
  time_total_s: 25878.359879493713
  timestamp: 1637040220
  timesteps_since_restore: 16704000
  timesteps_this_iter: 96000
  timesteps_total: 18624000
  training_iteration: 194
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    194 |          25878.4 | 18624000 |   762.06 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 67
    apples_agent-0_mean: 2.41
    apples_agent-0_min: 0
    apples_agent-1_max: 68
    apples_agent-1_mean: 27.05
    apples_agent-1_min: 0
    apples_agent-2_max: 328
    apples_agent-2_mean: 15.08
    apples_agent-2_min: 0
    apples_agent-3_max: 309
    apples_agent-3_mean: 69.3
    apples_agent-3_min: 35
    apples_agent-4_max: 53
    apples_agent-4_mean: 0.56
    apples_agent-4_min: 0
    apples_agent-5_max: 179
    apples_agent-5_mean: 95.6
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 488
    cleaning_beam_agent-0_mean: 352.52
    cleaning_beam_agent-0_min: 177
    cleaning_beam_agent-1_max: 333
    cleaning_beam_agent-1_mean: 174.2
    cleaning_beam_agent-1_min: 78
    cleaning_beam_agent-2_max: 374
    cleaning_beam_agent-2_mean: 230.24
    cleaning_beam_agent-2_min: 28
    cleaning_beam_agent-3_max: 128
    cleaning_beam_agent-3_mean: 27.44
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 515
    cleaning_beam_agent-4_mean: 383.06
    cleaning_beam_agent-4_min: 297
    cleaning_beam_agent-5_max: 789
    cleaning_beam_agent-5_mean: 122.84
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-25-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1036.999999999984
  episode_reward_mean: 781.2399999999866
  episode_reward_min: 332.0000000000012
  episodes_this_iter: 96
  episodes_total: 18720
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20241.955
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 9.786240116227418e-05
        entropy: 1.058496117591858
        entropy_coeff: 0.0017600000137463212
        kl: 0.005800175480544567
        model: {}
        policy_loss: -0.015948276966810226
        total_loss: -0.01565569080412388
        vf_explained_var: 0.03845791518688202
        vf_loss: 15.755244255065918
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 9.786240116227418e-05
        entropy: 1.1402920484542847
        entropy_coeff: 0.0017600000137463212
        kl: 0.006708026863634586
        model: {}
        policy_loss: -0.017441701143980026
        total_loss: -0.017131011933088303
        vf_explained_var: 0.001146659255027771
        vf_loss: 16.468059539794922
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 9.786240116227418e-05
        entropy: 1.1391286849975586
        entropy_coeff: 0.0017600000137463212
        kl: 0.00567058939486742
        model: {}
        policy_loss: -0.01599803753197193
        total_loss: -0.01559364888817072
        vf_explained_var: 0.04904526472091675
        vf_loss: 15.586626052856445
      agent-3:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 9.786240116227418e-05
        entropy: 0.4591933488845825
        entropy_coeff: 0.0017600000137463212
        kl: 0.0041551957838237286
        model: {}
        policy_loss: -0.009320390410721302
        total_loss: -0.008681097999215126
        vf_explained_var: 0.1205972284078598
        vf_loss: 14.344908714294434
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 9.786240116227418e-05
        entropy: 0.958953320980072
        entropy_coeff: 0.0017600000137463212
        kl: 0.006497700698673725
        model: {}
        policy_loss: -0.016755355522036552
        total_loss: -0.01628975383937359
        vf_explained_var: 0.08173950016498566
        vf_loss: 15.035916328430176
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 9.786240116227418e-05
        entropy: 0.905337929725647
        entropy_coeff: 0.0017600000137463212
        kl: 0.00593189150094986
        model: {}
        policy_loss: -0.016322240233421326
        total_loss: -0.015787946060299873
        vf_explained_var: 0.06497319042682648
        vf_loss: 15.345020294189453
    load_time_ms: 14069.86
    num_steps_sampled: 18720000
    num_steps_trained: 18720000
    sample_time_ms: 89358.347
    update_time_ms: 21.686
  iterations_since_restore: 175
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.309659090909092
    ram_util_percent: 9.603409090909091
  pid: 4061
  policy_reward_max:
    agent-0: 172.8333333333334
    agent-1: 172.8333333333334
    agent-2: 172.8333333333334
    agent-3: 172.8333333333334
    agent-4: 172.8333333333334
    agent-5: 172.8333333333334
  policy_reward_mean:
    agent-0: 130.20666666666702
    agent-1: 130.20666666666702
    agent-2: 130.20666666666702
    agent-3: 130.20666666666702
    agent-4: 130.20666666666702
    agent-5: 130.20666666666702
  policy_reward_min:
    agent-0: 55.333333333333115
    agent-1: 55.333333333333115
    agent-2: 55.333333333333115
    agent-3: 55.333333333333115
    agent-4: 55.333333333333115
    agent-5: 55.333333333333115
  sampler_perf:
    mean_env_wait_ms: 23.670338770552235
    mean_inference_ms: 12.305129810165822
    mean_processing_ms: 50.84673580735538
  time_since_restore: 22790.626864910126
  time_this_iter_s: 123.33067154884338
  time_total_s: 26001.690551042557
  timestamp: 1637040343
  timesteps_since_restore: 16800000
  timesteps_this_iter: 96000
  timesteps_total: 18720000
  training_iteration: 195
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    195 |          26001.7 | 18720000 |   781.24 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 61
    apples_agent-0_mean: 3.84
    apples_agent-0_min: 0
    apples_agent-1_max: 88
    apples_agent-1_mean: 28.04
    apples_agent-1_min: 0
    apples_agent-2_max: 328
    apples_agent-2_mean: 14.31
    apples_agent-2_min: 0
    apples_agent-3_max: 309
    apples_agent-3_mean: 67.26
    apples_agent-3_min: 20
    apples_agent-4_max: 29
    apples_agent-4_mean: 0.34
    apples_agent-4_min: 0
    apples_agent-5_max: 236
    apples_agent-5_mean: 94.07
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 465
    cleaning_beam_agent-0_mean: 353.31
    cleaning_beam_agent-0_min: 230
    cleaning_beam_agent-1_max: 291
    cleaning_beam_agent-1_mean: 174.01
    cleaning_beam_agent-1_min: 80
    cleaning_beam_agent-2_max: 412
    cleaning_beam_agent-2_mean: 227.03
    cleaning_beam_agent-2_min: 28
    cleaning_beam_agent-3_max: 122
    cleaning_beam_agent-3_mean: 31.27
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 497
    cleaning_beam_agent-4_mean: 384.25
    cleaning_beam_agent-4_min: 300
    cleaning_beam_agent-5_max: 820
    cleaning_beam_agent-5_mean: 121.36
    cleaning_beam_agent-5_min: 16
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-27-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 969.9999999999851
  episode_reward_mean: 780.7099999999851
  episode_reward_min: 374.0000000000028
  episodes_this_iter: 96
  episodes_total: 18816
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20268.945
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 9.187200339511037e-05
        entropy: 1.056330919265747
        entropy_coeff: 0.0017600000137463212
        kl: 0.005721219815313816
        model: {}
        policy_loss: -0.014796951785683632
        total_loss: -0.0144163453951478
        vf_explained_var: 0.047599196434020996
        vf_loss: 16.6762752532959
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 9.187200339511037e-05
        entropy: 1.1437510251998901
        entropy_coeff: 0.0017600000137463212
        kl: 0.006063650827854872
        model: {}
        policy_loss: -0.0168007705360651
        total_loss: -0.01640312932431698
        vf_explained_var: -0.027860045433044434
        vf_loss: 18.042766571044922
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 9.187200339511037e-05
        entropy: 1.1114299297332764
        entropy_coeff: 0.0017600000137463212
        kl: 0.006097388919442892
        model: {}
        policy_loss: -0.016461601480841637
        total_loss: -0.01584186963737011
        vf_explained_var: 0.051186054944992065
        vf_loss: 16.612411499023438
      agent-3:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 9.187200339511037e-05
        entropy: 0.4664332866668701
        entropy_coeff: 0.0017600000137463212
        kl: 0.0040434603579342365
        model: {}
        policy_loss: -0.009458048269152641
        total_loss: -0.008794894441962242
        vf_explained_var: 0.1537637859582901
        vf_loss: 14.777613639831543
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 9.187200339511037e-05
        entropy: 0.9409011602401733
        entropy_coeff: 0.0017600000137463212
        kl: 0.006011585704982281
        model: {}
        policy_loss: -0.016079504042863846
        total_loss: -0.015472729690372944
        vf_explained_var: 0.04906079173088074
        vf_loss: 16.616025924682617
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 9.187200339511037e-05
        entropy: 0.9008455872535706
        entropy_coeff: 0.0017600000137463212
        kl: 0.006102321203798056
        model: {}
        policy_loss: -0.016256220638751984
        total_loss: -0.015715785324573517
        vf_explained_var: 0.13314636051654816
        vf_loss: 15.156896591186523
    load_time_ms: 13971.186
    num_steps_sampled: 18816000
    num_steps_trained: 18816000
    sample_time_ms: 89365.708
    update_time_ms: 21.848
  iterations_since_restore: 176
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.434857142857146
    ram_util_percent: 9.662285714285714
  pid: 4061
  policy_reward_max:
    agent-0: 161.66666666666688
    agent-1: 161.66666666666688
    agent-2: 161.66666666666688
    agent-3: 161.66666666666688
    agent-4: 161.66666666666688
    agent-5: 161.66666666666688
  policy_reward_mean:
    agent-0: 130.11833333333365
    agent-1: 130.11833333333365
    agent-2: 130.11833333333365
    agent-3: 130.11833333333365
    agent-4: 130.11833333333365
    agent-5: 130.11833333333365
  policy_reward_min:
    agent-0: 62.33333333333319
    agent-1: 62.33333333333319
    agent-2: 62.33333333333319
    agent-3: 62.33333333333319
    agent-4: 62.33333333333319
    agent-5: 62.33333333333319
  sampler_perf:
    mean_env_wait_ms: 23.67123087930242
    mean_inference_ms: 12.304229031521428
    mean_processing_ms: 50.843760333820185
  time_since_restore: 22913.6469540596
  time_this_iter_s: 123.0200891494751
  time_total_s: 26124.710640192032
  timestamp: 1637040467
  timesteps_since_restore: 16896000
  timesteps_this_iter: 96000
  timesteps_total: 18816000
  training_iteration: 196
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    196 |          26124.7 | 18816000 |   780.71 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 40
    apples_agent-0_mean: 2.18
    apples_agent-0_min: 0
    apples_agent-1_max: 102
    apples_agent-1_mean: 26.08
    apples_agent-1_min: 0
    apples_agent-2_max: 87
    apples_agent-2_mean: 13.12
    apples_agent-2_min: 0
    apples_agent-3_max: 303
    apples_agent-3_mean: 67.33
    apples_agent-3_min: 22
    apples_agent-4_max: 71
    apples_agent-4_mean: 2.17
    apples_agent-4_min: 0
    apples_agent-5_max: 172
    apples_agent-5_mean: 88.59
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 523
    cleaning_beam_agent-0_mean: 361.15
    cleaning_beam_agent-0_min: 172
    cleaning_beam_agent-1_max: 291
    cleaning_beam_agent-1_mean: 168.51
    cleaning_beam_agent-1_min: 82
    cleaning_beam_agent-2_max: 416
    cleaning_beam_agent-2_mean: 235.48
    cleaning_beam_agent-2_min: 88
    cleaning_beam_agent-3_max: 133
    cleaning_beam_agent-3_mean: 33.08
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 506
    cleaning_beam_agent-4_mean: 382.67
    cleaning_beam_agent-4_min: 280
    cleaning_beam_agent-5_max: 731
    cleaning_beam_agent-5_mean: 128.91
    cleaning_beam_agent-5_min: 14
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-29-49
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 977.9999999999783
  episode_reward_mean: 767.1199999999864
  episode_reward_min: 206.99999999999704
  episodes_this_iter: 96
  episodes_total: 18912
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20243.599
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 8.588159835198894e-05
        entropy: 1.0429060459136963
        entropy_coeff: 0.0017600000137463212
        kl: 0.00514930160716176
        model: {}
        policy_loss: -0.014310699887573719
        total_loss: -0.013697334565222263
        vf_explained_var: 0.03837628662586212
        vf_loss: 19.339492797851562
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 8.588159835198894e-05
        entropy: 1.162824273109436
        entropy_coeff: 0.0017600000137463212
        kl: 0.005959121976047754
        model: {}
        policy_loss: -0.015813859179615974
        total_loss: -0.015293050557374954
        vf_explained_var: 0.01965850591659546
        vf_loss: 19.714693069458008
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 8.588159835198894e-05
        entropy: 1.1234283447265625
        entropy_coeff: 0.0017600000137463212
        kl: 0.005852651782333851
        model: {}
        policy_loss: -0.015787702053785324
        total_loss: -0.015078596770763397
        vf_explained_var: 0.10189811885356903
        vf_loss: 18.084436416625977
      agent-3:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 8.588159835198894e-05
        entropy: 0.48478883504867554
        entropy_coeff: 0.0017600000137463212
        kl: 0.00417805090546608
        model: {}
        policy_loss: -0.010456728748977184
        total_loss: -0.009740353561937809
        vf_explained_var: 0.22133029997348785
        vf_loss: 15.663403511047363
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 8.588159835198894e-05
        entropy: 0.9559166431427002
        entropy_coeff: 0.0017600000137463212
        kl: 0.005799991078674793
        model: {}
        policy_loss: -0.015569265931844711
        total_loss: -0.01482163742184639
        vf_explained_var: 0.07978279888629913
        vf_loss: 18.500446319580078
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 8.588159835198894e-05
        entropy: 0.8992304801940918
        entropy_coeff: 0.0017600000137463212
        kl: 0.005897205322980881
        model: {}
        policy_loss: -0.015985218808054924
        total_loss: -0.015207826159894466
        vf_explained_var: 0.11940336227416992
        vf_loss: 17.703210830688477
    load_time_ms: 13967.868
    num_steps_sampled: 18912000
    num_steps_trained: 18912000
    sample_time_ms: 89354.383
    update_time_ms: 21.887
  iterations_since_restore: 177
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.739655172413793
    ram_util_percent: 9.575862068965519
  pid: 4061
  policy_reward_max:
    agent-0: 162.99999999999994
    agent-1: 162.99999999999994
    agent-2: 162.99999999999994
    agent-3: 162.99999999999994
    agent-4: 162.99999999999994
    agent-5: 162.99999999999994
  policy_reward_mean:
    agent-0: 127.85333333333364
    agent-1: 127.85333333333364
    agent-2: 127.85333333333364
    agent-3: 127.85333333333364
    agent-4: 127.85333333333364
    agent-5: 127.85333333333364
  policy_reward_min:
    agent-0: 34.50000000000005
    agent-1: 34.50000000000005
    agent-2: 34.50000000000005
    agent-3: 34.50000000000005
    agent-4: 34.50000000000005
    agent-5: 34.50000000000005
  sampler_perf:
    mean_env_wait_ms: 23.671840458989532
    mean_inference_ms: 12.303572466380087
    mean_processing_ms: 50.84014389599494
  time_since_restore: 23036.078632354736
  time_this_iter_s: 122.4316782951355
  time_total_s: 26247.142318487167
  timestamp: 1637040589
  timesteps_since_restore: 16992000
  timesteps_this_iter: 96000
  timesteps_total: 18912000
  training_iteration: 197
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    197 |          26247.1 | 18912000 |   767.12 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 33
    apples_agent-0_mean: 2.18
    apples_agent-0_min: 0
    apples_agent-1_max: 75
    apples_agent-1_mean: 27.5
    apples_agent-1_min: 1
    apples_agent-2_max: 109
    apples_agent-2_mean: 11.59
    apples_agent-2_min: 0
    apples_agent-3_max: 121
    apples_agent-3_mean: 69.56
    apples_agent-3_min: 23
    apples_agent-4_max: 68
    apples_agent-4_mean: 1.02
    apples_agent-4_min: 0
    apples_agent-5_max: 170
    apples_agent-5_mean: 98.45
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 488
    cleaning_beam_agent-0_mean: 357.38
    cleaning_beam_agent-0_min: 246
    cleaning_beam_agent-1_max: 272
    cleaning_beam_agent-1_mean: 165.17
    cleaning_beam_agent-1_min: 86
    cleaning_beam_agent-2_max: 409
    cleaning_beam_agent-2_mean: 234.62
    cleaning_beam_agent-2_min: 119
    cleaning_beam_agent-3_max: 103
    cleaning_beam_agent-3_mean: 30.34
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 492
    cleaning_beam_agent-4_mean: 381.42
    cleaning_beam_agent-4_min: 154
    cleaning_beam_agent-5_max: 987
    cleaning_beam_agent-5_mean: 126.71
    cleaning_beam_agent-5_min: 13
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-31-54
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 990.9999999999709
  episode_reward_mean: 800.2399999999835
  episode_reward_min: 203.9999999999977
  episodes_this_iter: 96
  episodes_total: 19008
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20250.45
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 7.989120058482513e-05
        entropy: 1.0480139255523682
        entropy_coeff: 0.0017600000137463212
        kl: 0.005003404803574085
        model: {}
        policy_loss: -0.013531379401683807
        total_loss: -0.0131835350766778
        vf_explained_var: 0.04098869860172272
        vf_loss: 16.92008399963379
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 7.989120058482513e-05
        entropy: 1.1660665273666382
        entropy_coeff: 0.0017600000137463212
        kl: 0.005713739898055792
        model: {}
        policy_loss: -0.015656137838959694
        total_loss: -0.015345845371484756
        vf_explained_var: -0.013461530208587646
        vf_loss: 17.9119815826416
      agent-2:
        cur_kl_coeff: 0.15000000596046448
        cur_lr: 7.989120058482513e-05
        entropy: 1.1342048645019531
        entropy_coeff: 0.0017600000137463212
        kl: 0.004942392464727163
        model: {}
        policy_loss: -0.014103102497756481
        total_loss: -0.013745304197072983
        vf_explained_var: 0.08761623501777649
        vf_loss: 16.126407623291016
      agent-3:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 7.989120058482513e-05
        entropy: 0.4712543487548828
        entropy_coeff: 0.0017600000137463212
        kl: 0.0040914136916399
        model: {}
        policy_loss: -0.009108912199735641
        total_loss: -0.008441297337412834
        vf_explained_var: 0.14982518553733826
        vf_loss: 14.95425033569336
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 7.989120058482513e-05
        entropy: 0.958939790725708
        entropy_coeff: 0.0017600000137463212
        kl: 0.005798633210361004
        model: {}
        policy_loss: -0.015064001083374023
        total_loss: -0.014566509053111076
        vf_explained_var: 0.09020711481571198
        vf_loss: 16.053630828857422
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 7.989120058482513e-05
        entropy: 0.8884353637695312
        entropy_coeff: 0.0017600000137463212
        kl: 0.005133854225277901
        model: {}
        policy_loss: -0.01527599710971117
        total_loss: -0.014767943881452084
        vf_explained_var: 0.11668838560581207
        vf_loss: 15.583160400390625
    load_time_ms: 13964.167
    num_steps_sampled: 19008000
    num_steps_trained: 19008000
    sample_time_ms: 89217.521
    update_time_ms: 21.534
  iterations_since_restore: 178
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 16.732584269662922
    ram_util_percent: 9.690449438202247
  pid: 4061
  policy_reward_max:
    agent-0: 165.1666666666666
    agent-1: 165.1666666666666
    agent-2: 165.1666666666666
    agent-3: 165.1666666666666
    agent-4: 165.1666666666666
    agent-5: 165.1666666666666
  policy_reward_mean:
    agent-0: 133.3733333333336
    agent-1: 133.3733333333336
    agent-2: 133.3733333333336
    agent-3: 133.3733333333336
    agent-4: 133.3733333333336
    agent-5: 133.3733333333336
  policy_reward_min:
    agent-0: 34.000000000000064
    agent-1: 34.000000000000064
    agent-2: 34.000000000000064
    agent-3: 34.000000000000064
    agent-4: 34.000000000000064
    agent-5: 34.000000000000064
  sampler_perf:
    mean_env_wait_ms: 23.673780041400825
    mean_inference_ms: 12.303433801095412
    mean_processing_ms: 50.83907386392613
  time_since_restore: 23160.402814626694
  time_this_iter_s: 124.3241822719574
  time_total_s: 26371.466500759125
  timestamp: 1637040714
  timesteps_since_restore: 17088000
  timesteps_this_iter: 96000
  timesteps_total: 19008000
  training_iteration: 198
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    198 |          26371.5 | 19008000 |   800.24 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 46
    apples_agent-0_mean: 2.42
    apples_agent-0_min: 0
    apples_agent-1_max: 93
    apples_agent-1_mean: 26.58
    apples_agent-1_min: 0
    apples_agent-2_max: 81
    apples_agent-2_mean: 12.91
    apples_agent-2_min: 0
    apples_agent-3_max: 120
    apples_agent-3_mean: 65.87
    apples_agent-3_min: 34
    apples_agent-4_max: 89
    apples_agent-4_mean: 1.28
    apples_agent-4_min: 0
    apples_agent-5_max: 172
    apples_agent-5_mean: 95.58
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 529
    cleaning_beam_agent-0_mean: 367.77
    cleaning_beam_agent-0_min: 228
    cleaning_beam_agent-1_max: 325
    cleaning_beam_agent-1_mean: 184.95
    cleaning_beam_agent-1_min: 95
    cleaning_beam_agent-2_max: 435
    cleaning_beam_agent-2_mean: 219.82
    cleaning_beam_agent-2_min: 92
    cleaning_beam_agent-3_max: 103
    cleaning_beam_agent-3_mean: 28.02
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 475
    cleaning_beam_agent-4_mean: 387.11
    cleaning_beam_agent-4_min: 253
    cleaning_beam_agent-5_max: 829
    cleaning_beam_agent-5_mean: 142.69
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-33-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 969.9999999999811
  episode_reward_mean: 786.9099999999853
  episode_reward_min: 432.00000000000944
  episodes_this_iter: 96
  episodes_total: 19104
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20232.479
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 7.390080281766132e-05
        entropy: 1.033887267112732
        entropy_coeff: 0.0017600000137463212
        kl: 0.004429342225193977
        model: {}
        policy_loss: -0.01232081837952137
        total_loss: -0.012008903548121452
        vf_explained_var: 0.05690599977970123
        vf_loss: 16.886245727539062
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 7.390080281766132e-05
        entropy: 1.14863920211792
        entropy_coeff: 0.0017600000137463212
        kl: 0.005275113508105278
        model: {}
        policy_loss: -0.014911577105522156
        total_loss: -0.014653157442808151
        vf_explained_var: 0.020839735865592957
        vf_loss: 17.525169372558594
      agent-2:
        cur_kl_coeff: 0.07500000298023224
        cur_lr: 7.390080281766132e-05
        entropy: 1.137115716934204
        entropy_coeff: 0.0017600000137463212
        kl: 0.00554061122238636
        model: {}
        policy_loss: -0.013855785131454468
        total_loss: -0.013756242580711842
        vf_explained_var: 0.05864092707633972
        vf_loss: 16.853229522705078
      agent-3:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 7.390080281766132e-05
        entropy: 0.4698954224586487
        entropy_coeff: 0.0017600000137463212
        kl: 0.003862106241285801
        model: {}
        policy_loss: -0.008714188821613789
        total_loss: -0.008060675114393234
        vf_explained_var: 0.17318353056907654
        vf_loss: 14.797782897949219
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 7.390080281766132e-05
        entropy: 0.9550721645355225
        entropy_coeff: 0.0017600000137463212
        kl: 0.005903263110667467
        model: {}
        policy_loss: -0.014823241159319878
        total_loss: -0.014226710423827171
        vf_explained_var: 0.05758422613143921
        vf_loss: 16.871309280395508
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 7.390080281766132e-05
        entropy: 0.8698620200157166
        entropy_coeff: 0.0017600000137463212
        kl: 0.005585499573498964
        model: {}
        policy_loss: -0.01434093527495861
        total_loss: -0.013690175488591194
        vf_explained_var: 0.0929311066865921
        vf_loss: 16.2316951751709
    load_time_ms: 14057.687
    num_steps_sampled: 19104000
    num_steps_trained: 19104000
    sample_time_ms: 89221.763
    update_time_ms: 21.599
  iterations_since_restore: 179
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.354545454545455
    ram_util_percent: 9.639772727272726
  pid: 4061
  policy_reward_max:
    agent-0: 161.66666666666674
    agent-1: 161.66666666666674
    agent-2: 161.66666666666674
    agent-3: 161.66666666666674
    agent-4: 161.66666666666674
    agent-5: 161.66666666666674
  policy_reward_mean:
    agent-0: 131.15166666666698
    agent-1: 131.15166666666698
    agent-2: 131.15166666666698
    agent-3: 131.15166666666698
    agent-4: 131.15166666666698
    agent-5: 131.15166666666698
  policy_reward_min:
    agent-0: 71.99999999999987
    agent-1: 71.99999999999987
    agent-2: 71.99999999999987
    agent-3: 71.99999999999987
    agent-4: 71.99999999999987
    agent-5: 71.99999999999987
  sampler_perf:
    mean_env_wait_ms: 23.675680225542582
    mean_inference_ms: 12.302886905578616
    mean_processing_ms: 50.83595525807986
  time_since_restore: 23283.867094516754
  time_this_iter_s: 123.46427989006042
  time_total_s: 26494.930780649185
  timestamp: 1637040837
  timesteps_since_restore: 17184000
  timesteps_this_iter: 96000
  timesteps_total: 19104000
  training_iteration: 199
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    199 |          26494.9 | 19104000 |   786.91 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 45
    apples_agent-0_mean: 3.56
    apples_agent-0_min: 0
    apples_agent-1_max: 126
    apples_agent-1_mean: 25.35
    apples_agent-1_min: 0
    apples_agent-2_max: 127
    apples_agent-2_mean: 14.79
    apples_agent-2_min: 0
    apples_agent-3_max: 112
    apples_agent-3_mean: 65.33
    apples_agent-3_min: 28
    apples_agent-4_max: 81
    apples_agent-4_mean: 1.89
    apples_agent-4_min: 0
    apples_agent-5_max: 179
    apples_agent-5_mean: 96.26
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 497
    cleaning_beam_agent-0_mean: 361.37
    cleaning_beam_agent-0_min: 241
    cleaning_beam_agent-1_max: 330
    cleaning_beam_agent-1_mean: 183.99
    cleaning_beam_agent-1_min: 72
    cleaning_beam_agent-2_max: 412
    cleaning_beam_agent-2_mean: 231.53
    cleaning_beam_agent-2_min: 79
    cleaning_beam_agent-3_max: 100
    cleaning_beam_agent-3_mean: 28.15
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 464
    cleaning_beam_agent-4_mean: 373.48
    cleaning_beam_agent-4_min: 184
    cleaning_beam_agent-5_max: 822
    cleaning_beam_agent-5_mean: 134.68
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-36-01
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 959.9999999999527
  episode_reward_mean: 773.9599999999853
  episode_reward_min: 324.0
  episodes_this_iter: 96
  episodes_total: 19200
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20218.237
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 6.791039777453989e-05
        entropy: 1.0314315557479858
        entropy_coeff: 0.0017600000137463212
        kl: 0.005017063580453396
        model: {}
        policy_loss: -0.01285075768828392
        total_loss: -0.012759281322360039
        vf_explained_var: 0.06926968693733215
        vf_loss: 16.559406280517578
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 6.791039777453989e-05
        entropy: 1.1539089679718018
        entropy_coeff: 0.0017600000137463212
        kl: 0.00537725817412138
        model: {}
        policy_loss: -0.014143137261271477
        total_loss: -0.01384470984339714
        vf_explained_var: -0.007312148809432983
        vf_loss: 17.915817260742188
      agent-2:
        cur_kl_coeff: 0.07500000298023224
        cur_lr: 6.791039777453989e-05
        entropy: 1.1229897737503052
        entropy_coeff: 0.0017600000137463212
        kl: 0.005373742431402206
        model: {}
        policy_loss: -0.01336072850972414
        total_loss: -0.013191454112529755
        vf_explained_var: 0.019848287105560303
        vf_loss: 17.427066802978516
      agent-3:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 6.791039777453989e-05
        entropy: 0.4812846779823303
        entropy_coeff: 0.0017600000137463212
        kl: 0.0032433231826871634
        model: {}
        policy_loss: -0.00845197681337595
        total_loss: -0.00772951589897275
        vf_explained_var: 0.11772358417510986
        vf_loss: 15.69208812713623
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 6.791039777453989e-05
        entropy: 0.9591946601867676
        entropy_coeff: 0.0017600000137463212
        kl: 0.0049728103913366795
        model: {}
        policy_loss: -0.014089610427618027
        total_loss: -0.013659140095114708
        vf_explained_var: 0.08867773413658142
        vf_loss: 16.21371841430664
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 6.791039777453989e-05
        entropy: 0.8852919340133667
        entropy_coeff: 0.0017600000137463212
        kl: 0.004837584216147661
        model: {}
        policy_loss: -0.013484599068760872
        total_loss: -0.012885362841188908
        vf_explained_var: 0.05823501944541931
        vf_loss: 16.735912322998047
    load_time_ms: 14233.21
    num_steps_sampled: 19200000
    num_steps_trained: 19200000
    sample_time_ms: 89214.688
    update_time_ms: 21.142
  iterations_since_restore: 180
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.454545454545455
    ram_util_percent: 9.67556818181818
  pid: 4061
  policy_reward_max:
    agent-0: 159.99999999999918
    agent-1: 159.99999999999918
    agent-2: 159.99999999999918
    agent-3: 159.99999999999918
    agent-4: 159.99999999999918
    agent-5: 159.99999999999918
  policy_reward_mean:
    agent-0: 128.99333333333365
    agent-1: 128.99333333333365
    agent-2: 128.99333333333365
    agent-3: 128.99333333333365
    agent-4: 128.99333333333365
    agent-5: 128.99333333333365
  policy_reward_min:
    agent-0: 53.99999999999985
    agent-1: 53.99999999999985
    agent-2: 53.99999999999985
    agent-3: 53.99999999999985
    agent-4: 53.99999999999985
    agent-5: 53.99999999999985
  sampler_perf:
    mean_env_wait_ms: 23.67608100181388
    mean_inference_ms: 12.302184793573458
    mean_processing_ms: 50.8323704152496
  time_since_restore: 23407.405915498734
  time_this_iter_s: 123.53882098197937
  time_total_s: 26618.469601631165
  timestamp: 1637040961
  timesteps_since_restore: 17280000
  timesteps_this_iter: 96000
  timesteps_total: 19200000
  training_iteration: 200
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    200 |          26618.5 | 19200000 |   773.96 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 86
    apples_agent-0_mean: 4.47
    apples_agent-0_min: 0
    apples_agent-1_max: 84
    apples_agent-1_mean: 26.37
    apples_agent-1_min: 0
    apples_agent-2_max: 115
    apples_agent-2_mean: 15.06
    apples_agent-2_min: 0
    apples_agent-3_max: 128
    apples_agent-3_mean: 62.72
    apples_agent-3_min: 26
    apples_agent-4_max: 74
    apples_agent-4_mean: 1.45
    apples_agent-4_min: 0
    apples_agent-5_max: 169
    apples_agent-5_mean: 94.91
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 490
    cleaning_beam_agent-0_mean: 360.19
    cleaning_beam_agent-0_min: 234
    cleaning_beam_agent-1_max: 330
    cleaning_beam_agent-1_mean: 188.54
    cleaning_beam_agent-1_min: 106
    cleaning_beam_agent-2_max: 455
    cleaning_beam_agent-2_mean: 229.07
    cleaning_beam_agent-2_min: 103
    cleaning_beam_agent-3_max: 126
    cleaning_beam_agent-3_mean: 27.71
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 453
    cleaning_beam_agent-4_mean: 361.72
    cleaning_beam_agent-4_min: 179
    cleaning_beam_agent-5_max: 822
    cleaning_beam_agent-5_mean: 135.12
    cleaning_beam_agent-5_min: 17
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-38-05
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 980.9999999999816
  episode_reward_mean: 788.7199999999834
  episode_reward_min: 234.99999999999656
  episodes_this_iter: 96
  episodes_total: 19296
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20216.37
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 6.192000000737607e-05
        entropy: 1.0216282606124878
        entropy_coeff: 0.0017600000137463212
        kl: 0.004923926666378975
        model: {}
        policy_loss: -0.01211796049028635
        total_loss: -0.011859435588121414
        vf_explained_var: 0.02199655771255493
        vf_loss: 18.10394859313965
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 6.192000000737607e-05
        entropy: 1.1550090312957764
        entropy_coeff: 0.0017600000137463212
        kl: 0.004938893020153046
        model: {}
        policy_loss: -0.01333429105579853
        total_loss: -0.013053006492555141
        vf_explained_var: 0.016660138964653015
        vf_loss: 18.202116012573242
      agent-2:
        cur_kl_coeff: 0.07500000298023224
        cur_lr: 6.192000000737607e-05
        entropy: 1.1376583576202393
        entropy_coeff: 0.0017600000137463212
        kl: 0.004863403737545013
        model: {}
        policy_loss: -0.012139864265918732
        total_loss: -0.01206243596971035
        vf_explained_var: 0.07776367664337158
        vf_loss: 17.14954376220703
      agent-3:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 6.192000000737607e-05
        entropy: 0.4704533815383911
        entropy_coeff: 0.0017600000137463212
        kl: 0.002977892756462097
        model: {}
        policy_loss: -0.008179057389497757
        total_loss: -0.007505543064326048
        vf_explained_var: 0.1869562268257141
        vf_loss: 15.013665199279785
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 6.192000000737607e-05
        entropy: 0.9682985544204712
        entropy_coeff: 0.0017600000137463212
        kl: 0.005572951398789883
        model: {}
        policy_loss: -0.013412410393357277
        total_loss: -0.013182101771235466
        vf_explained_var: 0.10436701774597168
        vf_loss: 16.558679580688477
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 6.192000000737607e-05
        entropy: 0.9023699164390564
        entropy_coeff: 0.0017600000137463212
        kl: 0.005616316571831703
        model: {}
        policy_loss: -0.013182307593524456
        total_loss: -0.012855480425059795
        vf_explained_var: 0.11628149449825287
        vf_loss: 16.341800689697266
    load_time_ms: 14264.517
    num_steps_sampled: 19296000
    num_steps_trained: 19296000
    sample_time_ms: 89177.356
    update_time_ms: 21.32
  iterations_since_restore: 181
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.069491525423727
    ram_util_percent: 9.601129943502825
  pid: 4061
  policy_reward_max:
    agent-0: 163.49999999999974
    agent-1: 163.49999999999974
    agent-2: 163.49999999999974
    agent-3: 163.49999999999974
    agent-4: 163.49999999999974
    agent-5: 163.49999999999974
  policy_reward_mean:
    agent-0: 131.45333333333366
    agent-1: 131.45333333333366
    agent-2: 131.45333333333366
    agent-3: 131.45333333333366
    agent-4: 131.45333333333366
    agent-5: 131.45333333333366
  policy_reward_min:
    agent-0: 39.166666666666714
    agent-1: 39.166666666666714
    agent-2: 39.166666666666714
    agent-3: 39.166666666666714
    agent-4: 39.166666666666714
    agent-5: 39.166666666666714
  sampler_perf:
    mean_env_wait_ms: 23.675865111558515
    mean_inference_ms: 12.30118248871836
    mean_processing_ms: 50.828020917223775
  time_since_restore: 23531.260141849518
  time_this_iter_s: 123.8542263507843
  time_total_s: 26742.32382798195
  timestamp: 1637041085
  timesteps_since_restore: 17376000
  timesteps_this_iter: 96000
  timesteps_total: 19296000
  training_iteration: 201
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    201 |          26742.3 | 19296000 |   788.72 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 27
    apples_agent-0_mean: 2.17
    apples_agent-0_min: 0
    apples_agent-1_max: 80
    apples_agent-1_mean: 24.96
    apples_agent-1_min: 0
    apples_agent-2_max: 121
    apples_agent-2_mean: 16.45
    apples_agent-2_min: 0
    apples_agent-3_max: 108
    apples_agent-3_mean: 65.85
    apples_agent-3_min: 27
    apples_agent-4_max: 39
    apples_agent-4_mean: 1.07
    apples_agent-4_min: 0
    apples_agent-5_max: 196
    apples_agent-5_mean: 94.58
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 464
    cleaning_beam_agent-0_mean: 356.23
    cleaning_beam_agent-0_min: 255
    cleaning_beam_agent-1_max: 359
    cleaning_beam_agent-1_mean: 189.61
    cleaning_beam_agent-1_min: 41
    cleaning_beam_agent-2_max: 408
    cleaning_beam_agent-2_mean: 216.12
    cleaning_beam_agent-2_min: 99
    cleaning_beam_agent-3_max: 91
    cleaning_beam_agent-3_mean: 27.4
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 493
    cleaning_beam_agent-4_mean: 361.05
    cleaning_beam_agent-4_min: 174
    cleaning_beam_agent-5_max: 817
    cleaning_beam_agent-5_mean: 122.57
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-40-10
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1001.9999999999795
  episode_reward_mean: 785.8099999999835
  episode_reward_min: 213.99999999999702
  episodes_this_iter: 96
  episodes_total: 19392
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20203.684
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 5.5929598602233455e-05
        entropy: 1.026594877243042
        entropy_coeff: 0.0017600000137463212
        kl: 0.004979363642632961
        model: {}
        policy_loss: -0.0114012211561203
        total_loss: -0.011306616477668285
        vf_explained_var: 0.013471513986587524
        vf_loss: 17.769268035888672
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 5.5929598602233455e-05
        entropy: 1.14906907081604
        entropy_coeff: 0.0017600000137463212
        kl: 0.005606817081570625
        model: {}
        policy_loss: -0.012317109853029251
        total_loss: -0.012324989773333073
        vf_explained_var: 0.03741838037967682
        vf_loss: 17.341413497924805
      agent-2:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 5.5929598602233455e-05
        entropy: 1.128232717514038
        entropy_coeff: 0.0017600000137463212
        kl: 0.005359348841011524
        model: {}
        policy_loss: -0.011831585317850113
        total_loss: -0.011860614642500877
        vf_explained_var: 0.03180430829524994
        vf_loss: 17.556854248046875
      agent-3:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 5.5929598602233455e-05
        entropy: 0.4647822380065918
        entropy_coeff: 0.0017600000137463212
        kl: 0.0030704420059919357
        model: {}
        policy_loss: -0.007288641761988401
        total_loss: -0.006599460728466511
        vf_explained_var: 0.16297222673892975
        vf_loss: 15.0712308883667
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 5.5929598602233455e-05
        entropy: 0.9575800895690918
        entropy_coeff: 0.0017600000137463212
        kl: 0.006038947030901909
        model: {}
        policy_loss: -0.012845179066061974
        total_loss: -0.012612775899469852
        vf_explained_var: 0.10266152024269104
        vf_loss: 16.157949447631836
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 5.5929598602233455e-05
        entropy: 0.899747908115387
        entropy_coeff: 0.0017600000137463212
        kl: 0.004713109228760004
        model: {}
        policy_loss: -0.012670214287936687
        total_loss: -0.012459722347557545
        vf_explained_var: 0.1349806934595108
        vf_loss: 15.583938598632812
    load_time_ms: 14428.48
    num_steps_sampled: 19392000
    num_steps_trained: 19392000
    sample_time_ms: 89054.176
    update_time_ms: 21.363
  iterations_since_restore: 182
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.803954802259888
    ram_util_percent: 9.621468926553673
  pid: 4061
  policy_reward_max:
    agent-0: 167.00000000000026
    agent-1: 167.00000000000026
    agent-2: 167.00000000000026
    agent-3: 167.00000000000026
    agent-4: 167.00000000000026
    agent-5: 167.00000000000026
  policy_reward_mean:
    agent-0: 130.96833333333373
    agent-1: 130.96833333333373
    agent-2: 130.96833333333373
    agent-3: 130.96833333333373
    agent-4: 130.96833333333373
    agent-5: 130.96833333333373
  policy_reward_min:
    agent-0: 35.666666666666686
    agent-1: 35.666666666666686
    agent-2: 35.666666666666686
    agent-3: 35.666666666666686
    agent-4: 35.666666666666686
    agent-5: 35.666666666666686
  sampler_perf:
    mean_env_wait_ms: 23.675052027981
    mean_inference_ms: 12.300013602506299
    mean_processing_ms: 50.82408880470227
  time_since_restore: 23655.59528708458
  time_this_iter_s: 124.33514523506165
  time_total_s: 26866.65897321701
  timestamp: 1637041210
  timesteps_since_restore: 17472000
  timesteps_this_iter: 96000
  timesteps_total: 19392000
  training_iteration: 202
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    202 |          26866.7 | 19392000 |   785.81 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 35
    apples_agent-0_mean: 2.49
    apples_agent-0_min: 0
    apples_agent-1_max: 106
    apples_agent-1_mean: 25.01
    apples_agent-1_min: 0
    apples_agent-2_max: 97
    apples_agent-2_mean: 13.52
    apples_agent-2_min: 0
    apples_agent-3_max: 165
    apples_agent-3_mean: 62.22
    apples_agent-3_min: 33
    apples_agent-4_max: 64
    apples_agent-4_mean: 2.57
    apples_agent-4_min: 0
    apples_agent-5_max: 153
    apples_agent-5_mean: 94.37
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 476
    cleaning_beam_agent-0_mean: 354.61
    cleaning_beam_agent-0_min: 225
    cleaning_beam_agent-1_max: 359
    cleaning_beam_agent-1_mean: 207.99
    cleaning_beam_agent-1_min: 110
    cleaning_beam_agent-2_max: 391
    cleaning_beam_agent-2_mean: 225.8
    cleaning_beam_agent-2_min: 64
    cleaning_beam_agent-3_max: 167
    cleaning_beam_agent-3_mean: 26.88
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 468
    cleaning_beam_agent-4_mean: 355.27
    cleaning_beam_agent-4_min: 195
    cleaning_beam_agent-5_max: 780
    cleaning_beam_agent-5_mean: 111.66
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-42-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1030.9999999999748
  episode_reward_mean: 789.2599999999829
  episode_reward_min: 284.9999999999991
  episodes_this_iter: 96
  episodes_total: 19488
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20184.635
    learner:
      agent-0:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 4.993920083506964e-05
        entropy: 1.0310174226760864
        entropy_coeff: 0.0017600000137463212
        kl: 0.004976098891347647
        model: {}
        policy_loss: -0.00980175007134676
        total_loss: -0.009648989886045456
        vf_explained_var: 0.00028930604457855225
        vf_loss: 19.051467895507812
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 4.993920083506964e-05
        entropy: 1.1508592367172241
        entropy_coeff: 0.0017600000137463212
        kl: 0.0061286380514502525
        model: {}
        policy_loss: -0.01246429979801178
        total_loss: -0.012330240570008755
        vf_explained_var: 0.02420222759246826
        vf_loss: 18.531383514404297
      agent-2:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 4.993920083506964e-05
        entropy: 1.1283655166625977
        entropy_coeff: 0.0017600000137463212
        kl: 0.004919044673442841
        model: {}
        policy_loss: -0.011058569885790348
        total_loss: -0.01106223650276661
        vf_explained_var: 0.05473487079143524
        vf_loss: 17.977951049804688
      agent-3:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 4.993920083506964e-05
        entropy: 0.4647001624107361
        entropy_coeff: 0.0017600000137463212
        kl: 0.002816834719851613
        model: {}
        policy_loss: -0.006661035120487213
        total_loss: -0.0059917098842561245
        vf_explained_var: 0.21701039373874664
        vf_loss: 14.87162971496582
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 4.993920083506964e-05
        entropy: 0.9576897621154785
        entropy_coeff: 0.0017600000137463212
        kl: 0.0047440482303500175
        model: {}
        policy_loss: -0.011045273393392563
        total_loss: -0.010822970420122147
        vf_explained_var: 0.12217791378498077
        vf_loss: 16.706317901611328
      agent-5:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 4.993920083506964e-05
        entropy: 0.8895353674888611
        entropy_coeff: 0.0017600000137463212
        kl: 0.005053022410720587
        model: {}
        policy_loss: -0.01191345602273941
        total_loss: -0.011611179448664188
        vf_explained_var: 0.08446244895458221
        vf_loss: 17.41536521911621
    load_time_ms: 14421.461
    num_steps_sampled: 19488000
    num_steps_trained: 19488000
    sample_time_ms: 88959.801
    update_time_ms: 21.945
  iterations_since_restore: 183
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.598876404494382
    ram_util_percent: 9.591011235955058
  pid: 4061
  policy_reward_max:
    agent-0: 171.83333333333331
    agent-1: 171.83333333333331
    agent-2: 171.83333333333331
    agent-3: 171.83333333333331
    agent-4: 171.83333333333331
    agent-5: 171.83333333333331
  policy_reward_mean:
    agent-0: 131.54333333333363
    agent-1: 131.54333333333363
    agent-2: 131.54333333333363
    agent-3: 131.54333333333363
    agent-4: 131.54333333333363
    agent-5: 131.54333333333363
  policy_reward_min:
    agent-0: 47.49999999999996
    agent-1: 47.49999999999996
    agent-2: 47.49999999999996
    agent-3: 47.49999999999996
    agent-4: 47.49999999999996
    agent-5: 47.49999999999996
  sampler_perf:
    mean_env_wait_ms: 23.676023365778615
    mean_inference_ms: 12.299237312597377
    mean_processing_ms: 50.822162629652695
  time_since_restore: 23780.089016914368
  time_this_iter_s: 124.49372982978821
  time_total_s: 26991.1527030468
  timestamp: 1637041334
  timesteps_since_restore: 17568000
  timesteps_this_iter: 96000
  timesteps_total: 19488000
  training_iteration: 203
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    203 |          26991.2 | 19488000 |   789.26 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 37
    apples_agent-0_mean: 2.22
    apples_agent-0_min: 0
    apples_agent-1_max: 78
    apples_agent-1_mean: 26.63
    apples_agent-1_min: 0
    apples_agent-2_max: 135
    apples_agent-2_mean: 16.6
    apples_agent-2_min: 0
    apples_agent-3_max: 165
    apples_agent-3_mean: 65.75
    apples_agent-3_min: 31
    apples_agent-4_max: 25
    apples_agent-4_mean: 0.25
    apples_agent-4_min: 0
    apples_agent-5_max: 206
    apples_agent-5_mean: 97.17
    apples_agent-5_min: 17
    cleaning_beam_agent-0_max: 526
    cleaning_beam_agent-0_mean: 373.66
    cleaning_beam_agent-0_min: 202
    cleaning_beam_agent-1_max: 336
    cleaning_beam_agent-1_mean: 200.79
    cleaning_beam_agent-1_min: 115
    cleaning_beam_agent-2_max: 407
    cleaning_beam_agent-2_mean: 223.13
    cleaning_beam_agent-2_min: 66
    cleaning_beam_agent-3_max: 55
    cleaning_beam_agent-3_mean: 20.55
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 445
    cleaning_beam_agent-4_mean: 361.14
    cleaning_beam_agent-4_min: 251
    cleaning_beam_agent-5_max: 507
    cleaning_beam_agent-5_mean: 121.9
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-44-18
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1030.9999999999748
  episode_reward_mean: 790.389999999985
  episode_reward_min: 502.0000000000106
  episodes_this_iter: 96
  episodes_total: 19584
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20182.436
    learner:
      agent-0:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 4.394879942992702e-05
        entropy: 1.0139240026474
        entropy_coeff: 0.0017600000137463212
        kl: 0.004690580070018768
        model: {}
        policy_loss: -0.009488516487181187
        total_loss: -0.009599230252206326
        vf_explained_var: 0.04129108786582947
        vf_loss: 16.444744110107422
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 4.394879942992702e-05
        entropy: 1.151043176651001
        entropy_coeff: 0.0017600000137463212
        kl: 0.0045851413160562515
        model: {}
        policy_loss: -0.011377016082406044
        total_loss: -0.011475393548607826
        vf_explained_var: 0.01149381697177887
        vf_loss: 16.982030868530273
      agent-2:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 4.394879942992702e-05
        entropy: 1.1330124139785767
        entropy_coeff: 0.0017600000137463212
        kl: 0.004100074525922537
        model: {}
        policy_loss: -0.009934959001839161
        total_loss: -0.010223927907645702
        vf_explained_var: 0.05637931823730469
        vf_loss: 16.282564163208008
      agent-3:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 4.394879942992702e-05
        entropy: 0.4566497802734375
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024812896735966206
        model: {}
        policy_loss: -0.006390368100255728
        total_loss: -0.0057846549898386
        vf_explained_var: 0.17837649583816528
        vf_loss: 14.094070434570312
      agent-4:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 4.394879942992702e-05
        entropy: 0.9555685520172119
        entropy_coeff: 0.0017600000137463212
        kl: 0.004532428923994303
        model: {}
        policy_loss: -0.010341580025851727
        total_loss: -0.010249363258481026
        vf_explained_var: 0.03198155760765076
        vf_loss: 16.607080459594727
      agent-5:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 4.394879942992702e-05
        entropy: 0.8975973129272461
        entropy_coeff: 0.0017600000137463212
        kl: 0.004105689469724894
        model: {}
        policy_loss: -0.010760056786239147
        total_loss: -0.010669611394405365
        vf_explained_var: 0.0889023095369339
        vf_loss: 15.675738334655762
    load_time_ms: 14375.946
    num_steps_sampled: 19584000
    num_steps_trained: 19584000
    sample_time_ms: 88960.117
    update_time_ms: 21.627
  iterations_since_restore: 184
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.371428571428574
    ram_util_percent: 9.573142857142859
  pid: 4061
  policy_reward_max:
    agent-0: 171.83333333333331
    agent-1: 171.83333333333331
    agent-2: 171.83333333333331
    agent-3: 171.83333333333331
    agent-4: 171.83333333333331
    agent-5: 171.83333333333331
  policy_reward_mean:
    agent-0: 131.731666666667
    agent-1: 131.731666666667
    agent-2: 131.731666666667
    agent-3: 131.731666666667
    agent-4: 131.731666666667
    agent-5: 131.731666666667
  policy_reward_min:
    agent-0: 83.66666666666654
    agent-1: 83.66666666666654
    agent-2: 83.66666666666654
    agent-3: 83.66666666666654
    agent-4: 83.66666666666654
    agent-5: 83.66666666666654
  sampler_perf:
    mean_env_wait_ms: 23.677221836788718
    mean_inference_ms: 12.299325481792055
    mean_processing_ms: 50.82124595710995
  time_since_restore: 23903.67917919159
  time_this_iter_s: 123.59016227722168
  time_total_s: 27114.74286532402
  timestamp: 1637041458
  timesteps_since_restore: 17664000
  timesteps_this_iter: 96000
  timesteps_total: 19584000
  training_iteration: 204
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    204 |          27114.7 | 19584000 |   790.39 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 28
    apples_agent-0_mean: 1.79
    apples_agent-0_min: 0
    apples_agent-1_max: 81
    apples_agent-1_mean: 26.51
    apples_agent-1_min: 0
    apples_agent-2_max: 126
    apples_agent-2_mean: 14.33
    apples_agent-2_min: 0
    apples_agent-3_max: 120
    apples_agent-3_mean: 64.97
    apples_agent-3_min: 38
    apples_agent-4_max: 102
    apples_agent-4_mean: 1.85
    apples_agent-4_min: 0
    apples_agent-5_max: 205
    apples_agent-5_mean: 97.72
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 524
    cleaning_beam_agent-0_mean: 397.98
    cleaning_beam_agent-0_min: 226
    cleaning_beam_agent-1_max: 340
    cleaning_beam_agent-1_mean: 206.35
    cleaning_beam_agent-1_min: 92
    cleaning_beam_agent-2_max: 478
    cleaning_beam_agent-2_mean: 225.36
    cleaning_beam_agent-2_min: 77
    cleaning_beam_agent-3_max: 69
    cleaning_beam_agent-3_mean: 22.1
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 445
    cleaning_beam_agent-4_mean: 352.89
    cleaning_beam_agent-4_min: 167
    cleaning_beam_agent-5_max: 701
    cleaning_beam_agent-5_mean: 143.33
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-46-22
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1027.999999999991
  episode_reward_mean: 801.9199999999844
  episode_reward_min: 385.00000000000824
  episodes_this_iter: 96
  episodes_total: 19680
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20166.001
    learner:
      agent-0:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 3.795840166276321e-05
        entropy: 0.9929208159446716
        entropy_coeff: 0.0017600000137463212
        kl: 0.003953363746404648
        model: {}
        policy_loss: -0.00821446068584919
        total_loss: -0.008226413279771805
        vf_explained_var: 0.039006754755973816
        vf_loss: 17.23232650756836
      agent-1:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 3.795840166276321e-05
        entropy: 1.147822618484497
        entropy_coeff: 0.0017600000137463212
        kl: 0.0042003728449344635
        model: {}
        policy_loss: -0.010413110256195068
        total_loss: -0.010563157498836517
        vf_explained_var: 0.016003981232643127
        vf_loss: 17.651121139526367
      agent-2:
        cur_kl_coeff: 0.00937500037252903
        cur_lr: 3.795840166276321e-05
        entropy: 1.1586859226226807
        entropy_coeff: 0.0017600000137463212
        kl: 0.004747738596051931
        model: {}
        policy_loss: -0.009545572102069855
        total_loss: -0.009853513911366463
        vf_explained_var: 0.06312213838100433
        vf_loss: 16.868391036987305
      agent-3:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 3.795840166276321e-05
        entropy: 0.44474273920059204
        entropy_coeff: 0.0017600000137463212
        kl: 0.002785894088447094
        model: {}
        policy_loss: -0.00593177042901516
        total_loss: -0.00523160956799984
        vf_explained_var: 0.17309071123600006
        vf_loss: 14.82902717590332
      agent-4:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 3.795840166276321e-05
        entropy: 0.9474629163742065
        entropy_coeff: 0.0017600000137463212
        kl: 0.0038083340041339397
        model: {}
        policy_loss: -0.009135525673627853
        total_loss: -0.00907512754201889
        vf_explained_var: 0.06531746685504913
        vf_loss: 16.803298950195312
      agent-5:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 3.795840166276321e-05
        entropy: 0.8722094893455505
        entropy_coeff: 0.0017600000137463212
        kl: 0.0036678577307611704
        model: {}
        policy_loss: -0.009862938895821571
        total_loss: -0.00971695501357317
        vf_explained_var: 0.09244787693023682
        vf_loss: 16.35224151611328
    load_time_ms: 14429.116
    num_steps_sampled: 19680000
    num_steps_trained: 19680000
    sample_time_ms: 88975.532
    update_time_ms: 21.566
  iterations_since_restore: 185
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.292655367231639
    ram_util_percent: 9.671751412429378
  pid: 4061
  policy_reward_max:
    agent-0: 171.33333333333286
    agent-1: 171.33333333333286
    agent-2: 171.33333333333286
    agent-3: 171.33333333333286
    agent-4: 171.33333333333286
    agent-5: 171.33333333333286
  policy_reward_mean:
    agent-0: 133.65333333333362
    agent-1: 133.65333333333362
    agent-2: 133.65333333333362
    agent-3: 133.65333333333362
    agent-4: 133.65333333333362
    agent-5: 133.65333333333362
  policy_reward_min:
    agent-0: 64.16666666666644
    agent-1: 64.16666666666644
    agent-2: 64.16666666666644
    agent-3: 64.16666666666644
    agent-4: 64.16666666666644
    agent-5: 64.16666666666644
  sampler_perf:
    mean_env_wait_ms: 23.678405559113163
    mean_inference_ms: 12.298882461450276
    mean_processing_ms: 50.818574482151874
  time_since_restore: 24027.533576250076
  time_this_iter_s: 123.85439705848694
  time_total_s: 27238.597262382507
  timestamp: 1637041582
  timesteps_since_restore: 17760000
  timesteps_this_iter: 96000
  timesteps_total: 19680000
  training_iteration: 205
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    205 |          27238.6 | 19680000 |   801.92 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 35
    apples_agent-0_mean: 1.99
    apples_agent-0_min: 0
    apples_agent-1_max: 107
    apples_agent-1_mean: 26.12
    apples_agent-1_min: 0
    apples_agent-2_max: 92
    apples_agent-2_mean: 16.62
    apples_agent-2_min: 0
    apples_agent-3_max: 102
    apples_agent-3_mean: 63.02
    apples_agent-3_min: 28
    apples_agent-4_max: 87
    apples_agent-4_mean: 1.27
    apples_agent-4_min: 0
    apples_agent-5_max: 180
    apples_agent-5_mean: 101.47
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 544
    cleaning_beam_agent-0_mean: 404.16
    cleaning_beam_agent-0_min: 221
    cleaning_beam_agent-1_max: 369
    cleaning_beam_agent-1_mean: 210.0
    cleaning_beam_agent-1_min: 53
    cleaning_beam_agent-2_max: 434
    cleaning_beam_agent-2_mean: 226.56
    cleaning_beam_agent-2_min: 107
    cleaning_beam_agent-3_max: 140
    cleaning_beam_agent-3_mean: 23.33
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 468
    cleaning_beam_agent-4_mean: 367.91
    cleaning_beam_agent-4_min: 221
    cleaning_beam_agent-5_max: 834
    cleaning_beam_agent-5_mean: 112.56
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-48-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 967.99999999997
  episode_reward_mean: 804.9399999999832
  episode_reward_min: 411.00000000000557
  episodes_this_iter: 96
  episodes_total: 19776
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20159.247
    learner:
      agent-0:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 3.196800025762059e-05
        entropy: 0.9902569651603699
        entropy_coeff: 0.0017600000137463212
        kl: 0.0034869674127548933
        model: {}
        policy_loss: -0.007579839788377285
        total_loss: -0.007561631966382265
        vf_explained_var: 0.00491410493850708
        vf_loss: 17.556123733520508
      agent-1:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 3.196800025762059e-05
        entropy: 1.1508984565734863
        entropy_coeff: 0.0017600000137463212
        kl: 0.0036983732134103775
        model: {}
        policy_loss: -0.008558889850974083
        total_loss: -0.00884050503373146
        vf_explained_var: 0.03693258762359619
        vf_loss: 16.977359771728516
      agent-2:
        cur_kl_coeff: 0.004687500186264515
        cur_lr: 3.196800025762059e-05
        entropy: 1.1435667276382446
        entropy_coeff: 0.0017600000137463212
        kl: 0.0037157502956688404
        model: {}
        policy_loss: -0.008462170138955116
        total_loss: -0.008743951097130775
        vf_explained_var: 0.03757280111312866
        vf_loss: 17.13478660583496
      agent-3:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 3.196800025762059e-05
        entropy: 0.45978108048439026
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019969420973211527
        model: {}
        policy_loss: -0.005021764896810055
        total_loss: -0.004340311512351036
        vf_explained_var: 0.1625111699104309
        vf_loss: 14.906641006469727
      agent-4:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 3.196800025762059e-05
        entropy: 0.9420011043548584
        entropy_coeff: 0.0017600000137463212
        kl: 0.004035705700516701
        model: {}
        policy_loss: -0.008476580493152142
        total_loss: -0.008471054024994373
        vf_explained_var: 0.07233233749866486
        vf_loss: 16.382286071777344
      agent-5:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 3.196800025762059e-05
        entropy: 0.8959425687789917
        entropy_coeff: 0.0017600000137463212
        kl: 0.003566537983715534
        model: {}
        policy_loss: -0.008789720013737679
        total_loss: -0.00873392354696989
        vf_explained_var: 0.08803863823413849
        vf_loss: 16.10363006591797
    load_time_ms: 14538.498
    num_steps_sampled: 19776000
    num_steps_trained: 19776000
    sample_time_ms: 88961.765
    update_time_ms: 22.165
  iterations_since_restore: 186
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.26723163841808
    ram_util_percent: 9.67457627118644
  pid: 4061
  policy_reward_max:
    agent-0: 161.33333333333297
    agent-1: 161.33333333333297
    agent-2: 161.33333333333297
    agent-3: 161.33333333333297
    agent-4: 161.33333333333297
    agent-5: 161.33333333333297
  policy_reward_mean:
    agent-0: 134.15666666666695
    agent-1: 134.15666666666695
    agent-2: 134.15666666666695
    agent-3: 134.15666666666695
    agent-4: 134.15666666666695
    agent-5: 134.15666666666695
  policy_reward_min:
    agent-0: 68.49999999999984
    agent-1: 68.49999999999984
    agent-2: 68.49999999999984
    agent-3: 68.49999999999984
    agent-4: 68.49999999999984
    agent-5: 68.49999999999984
  sampler_perf:
    mean_env_wait_ms: 23.678960491830793
    mean_inference_ms: 12.297960969744352
    mean_processing_ms: 50.81437725309946
  time_since_restore: 24151.522884130478
  time_this_iter_s: 123.98930788040161
  time_total_s: 27362.58657026291
  timestamp: 1637041706
  timesteps_since_restore: 17856000
  timesteps_this_iter: 96000
  timesteps_total: 19776000
  training_iteration: 206
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    206 |          27362.6 | 19776000 |   804.94 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 55
    apples_agent-0_mean: 2.16
    apples_agent-0_min: 0
    apples_agent-1_max: 86
    apples_agent-1_mean: 27.34
    apples_agent-1_min: 0
    apples_agent-2_max: 151
    apples_agent-2_mean: 16.57
    apples_agent-2_min: 0
    apples_agent-3_max: 134
    apples_agent-3_mean: 67.73
    apples_agent-3_min: 23
    apples_agent-4_max: 94
    apples_agent-4_mean: 1.91
    apples_agent-4_min: 0
    apples_agent-5_max: 176
    apples_agent-5_mean: 99.07
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 601
    cleaning_beam_agent-0_mean: 403.77
    cleaning_beam_agent-0_min: 257
    cleaning_beam_agent-1_max: 369
    cleaning_beam_agent-1_mean: 200.16
    cleaning_beam_agent-1_min: 90
    cleaning_beam_agent-2_max: 474
    cleaning_beam_agent-2_mean: 236.18
    cleaning_beam_agent-2_min: 70
    cleaning_beam_agent-3_max: 117
    cleaning_beam_agent-3_mean: 24.59
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 481
    cleaning_beam_agent-4_mean: 363.37
    cleaning_beam_agent-4_min: 219
    cleaning_beam_agent-5_max: 948
    cleaning_beam_agent-5_mean: 126.3
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-50-30
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 992.999999999987
  episode_reward_mean: 819.4299999999827
  episode_reward_min: 406.00000000000625
  episodes_this_iter: 96
  episodes_total: 19872
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20146.289
    learner:
      agent-0:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 2.597760067146737e-05
        entropy: 0.9852880239486694
        entropy_coeff: 0.0017600000137463212
        kl: 0.0036433308850973845
        model: {}
        policy_loss: -0.006437717936933041
        total_loss: -0.006409667897969484
        vf_explained_var: 0.014553263783454895
        vf_loss: 17.593090057373047
      agent-1:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 2.597760067146737e-05
        entropy: 1.1435749530792236
        entropy_coeff: 0.0017600000137463212
        kl: 0.0033576232381165028
        model: {}
        policy_loss: -0.00782732293009758
        total_loss: -0.008053501136600971
        vf_explained_var: 0.01664350926876068
        vf_loss: 17.655263900756836
      agent-2:
        cur_kl_coeff: 0.0023437500931322575
        cur_lr: 2.597760067146737e-05
        entropy: 1.126613736152649
        entropy_coeff: 0.0017600000137463212
        kl: 0.002753355773165822
        model: {}
        policy_loss: -0.00667688250541687
        total_loss: -0.006920702755451202
        vf_explained_var: 0.03756968677043915
        vf_loss: 17.325654983520508
      agent-3:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 2.597760067146737e-05
        entropy: 0.44912993907928467
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018014712259173393
        model: {}
        policy_loss: -0.00434900913387537
        total_loss: -0.0036164482589811087
        vf_explained_var: 0.14797353744506836
        vf_loss: 15.230293273925781
      agent-4:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 2.597760067146737e-05
        entropy: 0.9468163847923279
        entropy_coeff: 0.0017600000137463212
        kl: 0.0035035551991313696
        model: {}
        policy_loss: -0.007119328714907169
        total_loss: -0.0071075838059186935
        vf_explained_var: 0.06646515429019928
        vf_loss: 16.67192840576172
      agent-5:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 2.597760067146737e-05
        entropy: 0.8680453896522522
        entropy_coeff: 0.0017600000137463212
        kl: 0.0027312724851071835
        model: {}
        policy_loss: -0.0074709439650177956
        total_loss: -0.007346442900598049
        vf_explained_var: 0.079277902841568
        vf_loss: 16.437253952026367
    load_time_ms: 14624.643
    num_steps_sampled: 19872000
    num_steps_trained: 19872000
    sample_time_ms: 88991.663
    update_time_ms: 23.004
  iterations_since_restore: 187
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.893181818181818
    ram_util_percent: 9.469886363636363
  pid: 4061
  policy_reward_max:
    agent-0: 165.4999999999998
    agent-1: 165.4999999999998
    agent-2: 165.4999999999998
    agent-3: 165.4999999999998
    agent-4: 165.4999999999998
    agent-5: 165.4999999999998
  policy_reward_mean:
    agent-0: 136.57166666666689
    agent-1: 136.57166666666689
    agent-2: 136.57166666666689
    agent-3: 136.57166666666689
    agent-4: 136.57166666666689
    agent-5: 136.57166666666689
  policy_reward_min:
    agent-0: 67.66666666666647
    agent-1: 67.66666666666647
    agent-2: 67.66666666666647
    agent-3: 67.66666666666647
    agent-4: 67.66666666666647
    agent-5: 67.66666666666647
  sampler_perf:
    mean_env_wait_ms: 23.679873113595963
    mean_inference_ms: 12.29710445407886
    mean_processing_ms: 50.810624458873505
  time_since_restore: 24275.004487514496
  time_this_iter_s: 123.48160338401794
  time_total_s: 27486.068173646927
  timestamp: 1637041830
  timesteps_since_restore: 17952000
  timesteps_this_iter: 96000
  timesteps_total: 19872000
  training_iteration: 207
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    207 |          27486.1 | 19872000 |   819.43 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 33
    apples_agent-0_mean: 1.76
    apples_agent-0_min: 0
    apples_agent-1_max: 91
    apples_agent-1_mean: 27.34
    apples_agent-1_min: 0
    apples_agent-2_max: 62
    apples_agent-2_mean: 12.81
    apples_agent-2_min: 0
    apples_agent-3_max: 112
    apples_agent-3_mean: 64.81
    apples_agent-3_min: 31
    apples_agent-4_max: 51
    apples_agent-4_mean: 0.96
    apples_agent-4_min: 0
    apples_agent-5_max: 169
    apples_agent-5_mean: 106.92
    apples_agent-5_min: 1
    cleaning_beam_agent-0_max: 497
    cleaning_beam_agent-0_mean: 407.1
    cleaning_beam_agent-0_min: 278
    cleaning_beam_agent-1_max: 319
    cleaning_beam_agent-1_mean: 215.58
    cleaning_beam_agent-1_min: 96
    cleaning_beam_agent-2_max: 440
    cleaning_beam_agent-2_mean: 245.4
    cleaning_beam_agent-2_min: 115
    cleaning_beam_agent-3_max: 66
    cleaning_beam_agent-3_mean: 22.32
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 487
    cleaning_beam_agent-4_mean: 381.11
    cleaning_beam_agent-4_min: 281
    cleaning_beam_agent-5_max: 878
    cleaning_beam_agent-5_mean: 90.09
    cleaning_beam_agent-5_min: 13
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-52-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1052.9999999999866
  episode_reward_mean: 841.059999999982
  episode_reward_min: 401.0000000000114
  episodes_this_iter: 96
  episodes_total: 19968
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20165.068
    learner:
      agent-0:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 1.998719926632475e-05
        entropy: 0.97890704870224
        entropy_coeff: 0.0017600000137463212
        kl: 0.0025260422844439745
        model: {}
        policy_loss: -0.005531536415219307
        total_loss: -0.005361152347177267
        vf_explained_var: 0.011940628290176392
        vf_loss: 18.922748565673828
      agent-1:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 1.998719926632475e-05
        entropy: 1.16060471534729
        entropy_coeff: 0.0017600000137463212
        kl: 0.003189437324181199
        model: {}
        policy_loss: -0.006531622260808945
        total_loss: -0.006692918483167887
        vf_explained_var: 0.03806546330451965
        vf_loss: 18.714031219482422
      agent-2:
        cur_kl_coeff: 0.0011718750465661287
        cur_lr: 1.998719926632475e-05
        entropy: 1.145096778869629
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023877564817667007
        model: {}
        policy_loss: -0.005864140577614307
        total_loss: -0.006039588246494532
        vf_explained_var: 0.04514211416244507
        vf_loss: 18.37127113342285
      agent-3:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.998719926632475e-05
        entropy: 0.4416912794113159
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014851423911750317
        model: {}
        policy_loss: -0.0038244854658842087
        total_loss: -0.00300039304420352
        vf_explained_var: 0.1713615506887436
        vf_loss: 16.01465606689453
      agent-4:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 1.998719926632475e-05
        entropy: 0.9425576329231262
        entropy_coeff: 0.0017600000137463212
        kl: 0.0033427965827286243
        model: {}
        policy_loss: -0.006230175495147705
        total_loss: -0.006093266420066357
        vf_explained_var: 0.0690527856349945
        vf_loss: 17.905858993530273
      agent-5:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 1.998719926632475e-05
        entropy: 0.885866105556488
        entropy_coeff: 0.0017600000137463212
        kl: 0.002386931562796235
        model: {}
        policy_loss: -0.006215174216777086
        total_loss: -0.006100791972130537
        vf_explained_var: 0.12866006791591644
        vf_loss: 16.697811126708984
    load_time_ms: 14538.778
    num_steps_sampled: 19968000
    num_steps_trained: 19968000
    sample_time_ms: 89003.187
    update_time_ms: 23.131
  iterations_since_restore: 188
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.518079096045197
    ram_util_percent: 9.665536723163841
  pid: 4061
  policy_reward_max:
    agent-0: 175.49999999999932
    agent-1: 175.49999999999932
    agent-2: 175.49999999999932
    agent-3: 175.49999999999932
    agent-4: 175.49999999999932
    agent-5: 175.49999999999932
  policy_reward_mean:
    agent-0: 140.17666666666685
    agent-1: 140.17666666666685
    agent-2: 140.17666666666685
    agent-3: 140.17666666666685
    agent-4: 140.17666666666685
    agent-5: 140.17666666666685
  policy_reward_min:
    agent-0: 66.83333333333317
    agent-1: 66.83333333333317
    agent-2: 66.83333333333317
    agent-3: 66.83333333333317
    agent-4: 66.83333333333317
    agent-5: 66.83333333333317
  sampler_perf:
    mean_env_wait_ms: 23.681633684371572
    mean_inference_ms: 12.296404962058755
    mean_processing_ms: 50.807414939234114
  time_since_restore: 24398.795012950897
  time_this_iter_s: 123.79052543640137
  time_total_s: 27609.85869908333
  timestamp: 1637041954
  timesteps_since_restore: 18048000
  timesteps_this_iter: 96000
  timesteps_total: 19968000
  training_iteration: 208
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    208 |          27609.9 | 19968000 |   841.06 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 34
    apples_agent-0_mean: 2.26
    apples_agent-0_min: 0
    apples_agent-1_max: 77
    apples_agent-1_mean: 23.71
    apples_agent-1_min: 0
    apples_agent-2_max: 123
    apples_agent-2_mean: 13.6
    apples_agent-2_min: 0
    apples_agent-3_max: 125
    apples_agent-3_mean: 64.32
    apples_agent-3_min: 36
    apples_agent-4_max: 91
    apples_agent-4_mean: 2.16
    apples_agent-4_min: 0
    apples_agent-5_max: 221
    apples_agent-5_mean: 106.68
    apples_agent-5_min: 13
    cleaning_beam_agent-0_max: 530
    cleaning_beam_agent-0_mean: 402.06
    cleaning_beam_agent-0_min: 236
    cleaning_beam_agent-1_max: 360
    cleaning_beam_agent-1_mean: 218.48
    cleaning_beam_agent-1_min: 105
    cleaning_beam_agent-2_max: 432
    cleaning_beam_agent-2_mean: 246.8
    cleaning_beam_agent-2_min: 88
    cleaning_beam_agent-3_max: 115
    cleaning_beam_agent-3_mean: 22.97
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 487
    cleaning_beam_agent-4_mean: 386.59
    cleaning_beam_agent-4_min: 206
    cleaning_beam_agent-5_max: 720
    cleaning_beam_agent-5_mean: 100.56
    cleaning_beam_agent-5_min: 13
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-54-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1025.9999999999766
  episode_reward_mean: 848.4499999999824
  episode_reward_min: 399.00000000000904
  episodes_this_iter: 96
  episodes_total: 20064
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20194.852
    learner:
      agent-0:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.3996799680171534e-05
        entropy: 0.9994719624519348
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016273128567263484
        model: {}
        policy_loss: -0.00386892631649971
        total_loss: -0.003815131261944771
        vf_explained_var: 0.025914102792739868
        vf_loss: 18.12549591064453
      agent-1:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 1.3996799680171534e-05
        entropy: 1.1624408960342407
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018534243572503328
        model: {}
        policy_loss: -0.0049626948311924934
        total_loss: -0.005153356119990349
        vf_explained_var: 0.006651267409324646
        vf_loss: 18.52337074279785
      agent-2:
        cur_kl_coeff: 0.0005859375232830644
        cur_lr: 1.3996799680171534e-05
        entropy: 1.1299585103988647
        entropy_coeff: 0.0017600000137463212
        kl: 0.002255520783364773
        model: {}
        policy_loss: -0.004478750750422478
        total_loss: -0.004603497684001923
        vf_explained_var: 0.01008671522140503
        vf_loss: 18.626596450805664
      agent-3:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.3996799680171534e-05
        entropy: 0.41726407408714294
        entropy_coeff: 0.0017600000137463212
        kl: 0.001220337930135429
        model: {}
        policy_loss: -0.0027368238661438227
        total_loss: -0.0018978221341967583
        vf_explained_var: 0.16119904816150665
        vf_loss: 15.733888626098633
      agent-4:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 1.3996799680171534e-05
        entropy: 0.9423377513885498
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022805887274444103
        model: {}
        policy_loss: -0.004930766299366951
        total_loss: -0.004880849737673998
        vf_explained_var: 0.08942995965480804
        vf_loss: 17.066490173339844
      agent-5:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 1.3996799680171534e-05
        entropy: 0.8812434673309326
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014305836521089077
        model: {}
        policy_loss: -0.00455089658498764
        total_loss: -0.004433881491422653
        vf_explained_var: 0.09911143779754639
        vf_loss: 16.668851852416992
    load_time_ms: 14588.933
    num_steps_sampled: 20064000
    num_steps_trained: 20064000
    sample_time_ms: 88945.384
    update_time_ms: 23.399
  iterations_since_restore: 189
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.32897727272727
    ram_util_percent: 9.673863636363636
  pid: 4061
  policy_reward_max:
    agent-0: 170.9999999999999
    agent-1: 170.9999999999999
    agent-2: 170.9999999999999
    agent-3: 170.9999999999999
    agent-4: 170.9999999999999
    agent-5: 170.9999999999999
  policy_reward_mean:
    agent-0: 141.40833333333353
    agent-1: 141.40833333333353
    agent-2: 141.40833333333353
    agent-3: 141.40833333333353
    agent-4: 141.40833333333353
    agent-5: 141.40833333333353
  policy_reward_min:
    agent-0: 66.49999999999979
    agent-1: 66.49999999999979
    agent-2: 66.49999999999979
    agent-3: 66.49999999999979
    agent-4: 66.49999999999979
    agent-5: 66.49999999999979
  sampler_perf:
    mean_env_wait_ms: 23.683148637167008
    mean_inference_ms: 12.295542900610622
    mean_processing_ms: 50.80214674225859
  time_since_restore: 24522.46742272377
  time_this_iter_s: 123.67240977287292
  time_total_s: 27733.5311088562
  timestamp: 1637042078
  timesteps_since_restore: 18144000
  timesteps_this_iter: 96000
  timesteps_total: 20064000
  training_iteration: 209
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    209 |          27733.5 | 20064000 |   848.45 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 36
    apples_agent-0_mean: 1.77
    apples_agent-0_min: 0
    apples_agent-1_max: 66
    apples_agent-1_mean: 22.76
    apples_agent-1_min: 0
    apples_agent-2_max: 63
    apples_agent-2_mean: 10.46
    apples_agent-2_min: 0
    apples_agent-3_max: 113
    apples_agent-3_mean: 62.54
    apples_agent-3_min: 32
    apples_agent-4_max: 105
    apples_agent-4_mean: 3.24
    apples_agent-4_min: 0
    apples_agent-5_max: 191
    apples_agent-5_mean: 106.54
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 515
    cleaning_beam_agent-0_mean: 401.18
    cleaning_beam_agent-0_min: 250
    cleaning_beam_agent-1_max: 340
    cleaning_beam_agent-1_mean: 216.45
    cleaning_beam_agent-1_min: 115
    cleaning_beam_agent-2_max: 495
    cleaning_beam_agent-2_mean: 272.8
    cleaning_beam_agent-2_min: 117
    cleaning_beam_agent-3_max: 114
    cleaning_beam_agent-3_mean: 25.03
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 492
    cleaning_beam_agent-4_mean: 379.42
    cleaning_beam_agent-4_min: 178
    cleaning_beam_agent-5_max: 634
    cleaning_beam_agent-5_mean: 99.8
    cleaning_beam_agent-5_min: 13
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-56-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1025.9999999999766
  episode_reward_mean: 835.3399999999824
  episode_reward_min: 380.0000000000043
  episodes_this_iter: 96
  episodes_total: 20160
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20207.242
    learner:
      agent-0:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9965054988861084
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019291897770017385
        model: {}
        policy_loss: -0.003818508703261614
        total_loss: -0.0036864602006971836
        vf_explained_var: 0.042527586221694946
        vf_loss: 18.857145309448242
      agent-1:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1569607257843018
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017138307448476553
        model: {}
        policy_loss: -0.00438343919813633
        total_loss: -0.004476090427488089
        vf_explained_var: 0.014475807547569275
        vf_loss: 19.4226131439209
      agent-2:
        cur_kl_coeff: 0.0002929687616415322
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1221327781677246
        entropy_coeff: 0.0017600000137463212
        kl: 0.001286150305531919
        model: {}
        policy_loss: -0.0037417039275169373
        total_loss: -0.0037420494481921196
        vf_explained_var: -0.0013543665409088135
        vf_loss: 19.74227523803711
      agent-3:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4275095462799072
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010110505390912294
        model: {}
        policy_loss: -0.002560571301728487
        total_loss: -0.0016824959311634302
        vf_explained_var: 0.17288310825824738
        vf_loss: 16.304935455322266
      agent-4:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9466208815574646
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016866989899426699
        model: {}
        policy_loss: -0.004140521865338087
        total_loss: -0.004078712780028582
        vf_explained_var: 0.12225614488124847
        vf_loss: 17.27202796936035
      agent-5:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9123520851135254
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013712112559005618
        model: {}
        policy_loss: -0.0042961230501532555
        total_loss: -0.004150539170950651
        vf_explained_var: 0.1078626960515976
        vf_loss: 17.507863998413086
    load_time_ms: 14525.477
    num_steps_sampled: 20160000
    num_steps_trained: 20160000
    sample_time_ms: 89004.555
    update_time_ms: 23.472
  iterations_since_restore: 190
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.529378531073446
    ram_util_percent: 9.528813559322035
  pid: 4061
  policy_reward_max:
    agent-0: 170.9999999999999
    agent-1: 170.9999999999999
    agent-2: 170.9999999999999
    agent-3: 170.9999999999999
    agent-4: 170.9999999999999
    agent-5: 170.9999999999999
  policy_reward_mean:
    agent-0: 139.22333333333347
    agent-1: 139.22333333333347
    agent-2: 139.22333333333347
    agent-3: 139.22333333333347
    agent-4: 139.22333333333347
    agent-5: 139.22333333333347
  policy_reward_min:
    agent-0: 63.33333333333309
    agent-1: 63.33333333333309
    agent-2: 63.33333333333309
    agent-3: 63.33333333333309
    agent-4: 63.33333333333309
    agent-5: 63.33333333333309
  sampler_perf:
    mean_env_wait_ms: 23.685006976550035
    mean_inference_ms: 12.294874722712741
    mean_processing_ms: 50.79848918410928
  time_since_restore: 24646.13212800026
  time_this_iter_s: 123.66470527648926
  time_total_s: 27857.19581413269
  timestamp: 1637042202
  timesteps_since_restore: 18240000
  timesteps_this_iter: 96000
  timesteps_total: 20160000
  training_iteration: 210
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    210 |          27857.2 | 20160000 |   835.34 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 41
    apples_agent-0_mean: 1.5
    apples_agent-0_min: 0
    apples_agent-1_max: 110
    apples_agent-1_mean: 25.57
    apples_agent-1_min: 0
    apples_agent-2_max: 51
    apples_agent-2_mean: 10.34
    apples_agent-2_min: 0
    apples_agent-3_max: 123
    apples_agent-3_mean: 63.08
    apples_agent-3_min: 33
    apples_agent-4_max: 27
    apples_agent-4_mean: 0.3
    apples_agent-4_min: 0
    apples_agent-5_max: 168
    apples_agent-5_mean: 98.27
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 526
    cleaning_beam_agent-0_mean: 409.44
    cleaning_beam_agent-0_min: 290
    cleaning_beam_agent-1_max: 386
    cleaning_beam_agent-1_mean: 212.26
    cleaning_beam_agent-1_min: 110
    cleaning_beam_agent-2_max: 498
    cleaning_beam_agent-2_mean: 286.72
    cleaning_beam_agent-2_min: 102
    cleaning_beam_agent-3_max: 189
    cleaning_beam_agent-3_mean: 20.36
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 468
    cleaning_beam_agent-4_mean: 381.0
    cleaning_beam_agent-4_min: 289
    cleaning_beam_agent-5_max: 798
    cleaning_beam_agent-5_mean: 130.16
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 7
    fire_beam_agent-2_mean: 0.08
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-58-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1015.9999999999723
  episode_reward_mean: 848.7899999999802
  episode_reward_min: 270.99999999999244
  episodes_this_iter: 96
  episodes_total: 20256
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20167.684
    learner:
      agent-0:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9964909553527832
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018233624286949635
        model: {}
        policy_loss: -0.0035847153048962355
        total_loss: -0.0033943629823625088
        vf_explained_var: -0.0026717185974121094
        vf_loss: 19.440933227539062
      agent-1:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1482678651809692
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020267367362976074
        model: {}
        policy_loss: -0.004445136524736881
        total_loss: -0.004519707057625055
        vf_explained_var: 0.0012829750776290894
        vf_loss: 19.45589256286621
      agent-2:
        cur_kl_coeff: 0.0001464843808207661
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1238211393356323
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013083347585052252
        model: {}
        policy_loss: -0.0038184211589396
        total_loss: -0.00399854127317667
        vf_explained_var: 0.07154473662376404
        vf_loss: 17.97616958618164
      agent-3:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4052383005619049
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009149417746812105
        model: {}
        policy_loss: -0.0023158229887485504
        total_loss: -0.0012897374108433723
        vf_explained_var: 0.09940043091773987
        vf_loss: 17.393062591552734
      agent-4:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9432256817817688
        entropy_coeff: 0.0017600000137463212
        kl: 0.00245948345400393
        model: {}
        policy_loss: -0.004331165924668312
        total_loss: -0.004200121387839317
        vf_explained_var: 0.07782512903213501
        vf_loss: 17.906435012817383
      agent-5:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8701144456863403
        entropy_coeff: 0.0017600000137463212
        kl: 0.001234566094353795
        model: {}
        policy_loss: -0.004178137518465519
        total_loss: -0.003971860744059086
        vf_explained_var: 0.10992640256881714
        vf_loss: 17.3743896484375
    load_time_ms: 14550.669
    num_steps_sampled: 20256000
    num_steps_trained: 20256000
    sample_time_ms: 89123.562
    update_time_ms: 23.118
  iterations_since_restore: 191
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.309550561797755
    ram_util_percent: 9.669662921348314
  pid: 4061
  policy_reward_max:
    agent-0: 169.3333333333327
    agent-1: 169.3333333333327
    agent-2: 169.3333333333327
    agent-3: 169.3333333333327
    agent-4: 169.3333333333327
    agent-5: 169.3333333333327
  policy_reward_mean:
    agent-0: 141.46500000000017
    agent-1: 141.46500000000017
    agent-2: 141.46500000000017
    agent-3: 141.46500000000017
    agent-4: 141.46500000000017
    agent-5: 141.46500000000017
  policy_reward_min:
    agent-0: 45.16666666666644
    agent-1: 45.16666666666644
    agent-2: 45.16666666666644
    agent-3: 45.16666666666644
    agent-4: 45.16666666666644
    agent-5: 45.16666666666644
  sampler_perf:
    mean_env_wait_ms: 23.68789481235456
    mean_inference_ms: 12.294203321143602
    mean_processing_ms: 50.795430795871965
  time_since_restore: 24770.95211005211
  time_this_iter_s: 124.81998205184937
  time_total_s: 27982.01579618454
  timestamp: 1637042327
  timesteps_since_restore: 18336000
  timesteps_this_iter: 96000
  timesteps_total: 20256000
  training_iteration: 211
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    211 |            27982 | 20256000 |   848.79 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 39
    apples_agent-0_mean: 2.26
    apples_agent-0_min: 0
    apples_agent-1_max: 83
    apples_agent-1_mean: 25.93
    apples_agent-1_min: 0
    apples_agent-2_max: 92
    apples_agent-2_mean: 13.7
    apples_agent-2_min: 0
    apples_agent-3_max: 105
    apples_agent-3_mean: 63.96
    apples_agent-3_min: 35
    apples_agent-4_max: 58
    apples_agent-4_mean: 1.5
    apples_agent-4_min: 0
    apples_agent-5_max: 199
    apples_agent-5_mean: 99.68
    apples_agent-5_min: 1
    cleaning_beam_agent-0_max: 543
    cleaning_beam_agent-0_mean: 409.73
    cleaning_beam_agent-0_min: 211
    cleaning_beam_agent-1_max: 374
    cleaning_beam_agent-1_mean: 226.8
    cleaning_beam_agent-1_min: 111
    cleaning_beam_agent-2_max: 478
    cleaning_beam_agent-2_mean: 270.31
    cleaning_beam_agent-2_min: 93
    cleaning_beam_agent-3_max: 146
    cleaning_beam_agent-3_mean: 24.12
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 509
    cleaning_beam_agent-4_mean: 387.42
    cleaning_beam_agent-4_min: 205
    cleaning_beam_agent-5_max: 644
    cleaning_beam_agent-5_mean: 126.67
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 3
    fire_beam_agent-2_mean: 0.04
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-00-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1002.999999999984
  episode_reward_mean: 822.8199999999838
  episode_reward_min: 229.99999999999727
  episodes_this_iter: 96
  episodes_total: 20352
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20186.358
    learner:
      agent-0:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9893975257873535
        entropy_coeff: 0.0017600000137463212
        kl: 0.00262789917178452
        model: {}
        policy_loss: -0.003960530273616314
        total_loss: -0.003870540764182806
        vf_explained_var: 0.010768622159957886
        vf_loss: 18.312698364257812
      agent-1:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1389740705490112
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013480930356308818
        model: {}
        policy_loss: -0.003992809914052486
        total_loss: -0.004160781390964985
        vf_explained_var: 0.010006770491600037
        vf_loss: 18.36365509033203
      agent-2:
        cur_kl_coeff: 7.324219041038305e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 1.133415937423706
        entropy_coeff: 0.0017600000137463212
        kl: 0.00102808210067451
        model: {}
        policy_loss: -0.003910559229552746
        total_loss: -0.004076770041137934
        vf_explained_var: 0.014366894960403442
        vf_loss: 18.28529930114746
      agent-3:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43434375524520874
        entropy_coeff: 0.0017600000137463212
        kl: 0.001386384479701519
        model: {}
        policy_loss: -0.0027576179709285498
        total_loss: -0.0019235818181186914
        vf_explained_var: 0.1365927904844284
        vf_loss: 15.984807968139648
      agent-4:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9398802518844604
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019160935189574957
        model: {}
        policy_loss: -0.004417452495545149
        total_loss: -0.004350552801042795
        vf_explained_var: 0.07024899125099182
        vf_loss: 17.20905303955078
      agent-5:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8832226991653442
        entropy_coeff: 0.0017600000137463212
        kl: 0.001414629747159779
        model: {}
        policy_loss: -0.004114560782909393
        total_loss: -0.003902593394741416
        vf_explained_var: 0.0481177419424057
        vf_loss: 17.663043975830078
    load_time_ms: 15003.654
    num_steps_sampled: 20352000
    num_steps_trained: 20352000
    sample_time_ms: 89217.383
    update_time_ms: 22.698
  iterations_since_restore: 192
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 16.3345945945946
    ram_util_percent: 9.827567567567566
  pid: 4061
  policy_reward_max:
    agent-0: 167.16666666666626
    agent-1: 167.16666666666626
    agent-2: 167.16666666666626
    agent-3: 167.16666666666626
    agent-4: 167.16666666666626
    agent-5: 167.16666666666626
  policy_reward_mean:
    agent-0: 137.1366666666669
    agent-1: 137.1366666666669
    agent-2: 137.1366666666669
    agent-3: 137.1366666666669
    agent-4: 137.1366666666669
    agent-5: 137.1366666666669
  policy_reward_min:
    agent-0: 38.333333333333314
    agent-1: 38.333333333333314
    agent-2: 38.333333333333314
    agent-3: 38.333333333333314
    agent-4: 38.333333333333314
    agent-5: 38.333333333333314
  sampler_perf:
    mean_env_wait_ms: 23.69153833347834
    mean_inference_ms: 12.293726377757974
    mean_processing_ms: 50.79336548413686
  time_since_restore: 24901.01317691803
  time_this_iter_s: 130.06106686592102
  time_total_s: 28112.07686305046
  timestamp: 1637042457
  timesteps_since_restore: 18432000
  timesteps_this_iter: 96000
  timesteps_total: 20352000
  training_iteration: 212
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    212 |          28112.1 | 20352000 |   822.82 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 50
    apples_agent-0_mean: 3.21
    apples_agent-0_min: 0
    apples_agent-1_max: 83
    apples_agent-1_mean: 23.69
    apples_agent-1_min: 0
    apples_agent-2_max: 94
    apples_agent-2_mean: 12.61
    apples_agent-2_min: 0
    apples_agent-3_max: 102
    apples_agent-3_mean: 60.95
    apples_agent-3_min: 29
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 182
    apples_agent-5_mean: 109.46
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 574
    cleaning_beam_agent-0_mean: 431.8
    cleaning_beam_agent-0_min: 258
    cleaning_beam_agent-1_max: 395
    cleaning_beam_agent-1_mean: 220.13
    cleaning_beam_agent-1_min: 66
    cleaning_beam_agent-2_max: 500
    cleaning_beam_agent-2_mean: 285.76
    cleaning_beam_agent-2_min: 102
    cleaning_beam_agent-3_max: 114
    cleaning_beam_agent-3_mean: 23.03
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 478
    cleaning_beam_agent-4_mean: 393.8
    cleaning_beam_agent-4_min: 283
    cleaning_beam_agent-5_max: 887
    cleaning_beam_agent-5_mean: 98.78
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-03-02
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1025.9999999999757
  episode_reward_mean: 859.8399999999808
  episode_reward_min: 556.0000000000032
  episodes_this_iter: 96
  episodes_total: 20448
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20167.233
    learner:
      agent-0:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9678272008895874
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014453292824327946
        model: {}
        policy_loss: -0.0036430582404136658
        total_loss: -0.0035643046721816063
        vf_explained_var: 0.040849894285202026
        vf_loss: 17.821170806884766
      agent-1:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1485490798950195
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012365411967039108
        model: {}
        policy_loss: -0.003996833693236113
        total_loss: -0.004154277965426445
        vf_explained_var: 0.01013842225074768
        vf_loss: 18.63887596130371
      agent-2:
        cur_kl_coeff: 3.662109520519152e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1112034320831299
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015204658266156912
        model: {}
        policy_loss: -0.003918706905096769
        total_loss: -0.004102135542780161
        vf_explained_var: 0.04910552501678467
        vf_loss: 17.722387313842773
      agent-3:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3993397355079651
        entropy_coeff: 0.0017600000137463212
        kl: 0.000637971970718354
        model: {}
        policy_loss: -0.0021898425184190273
        total_loss: -0.0012522554025053978
        vf_explained_var: 0.11127108335494995
        vf_loss: 16.404224395751953
      agent-4:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9293391108512878
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022177291102707386
        model: {}
        policy_loss: -0.004345447290688753
        total_loss: -0.004244101233780384
        vf_explained_var: 0.05301181972026825
        vf_loss: 17.368749618530273
      agent-5:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8682423830032349
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018140493193641305
        model: {}
        policy_loss: -0.004044465720653534
        total_loss: -0.003933891654014587
        vf_explained_var: 0.10962167382240295
        vf_loss: 16.385967254638672
    load_time_ms: 15085.852
    num_steps_sampled: 20448000
    num_steps_trained: 20448000
    sample_time_ms: 89198.698
    update_time_ms: 23.065
  iterations_since_restore: 193
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.283240223463686
    ram_util_percent: 9.594972067039107
  pid: 4061
  policy_reward_max:
    agent-0: 171.00000000000003
    agent-1: 171.00000000000003
    agent-2: 171.00000000000003
    agent-3: 171.00000000000003
    agent-4: 171.00000000000003
    agent-5: 171.00000000000003
  policy_reward_mean:
    agent-0: 143.30666666666684
    agent-1: 143.30666666666684
    agent-2: 143.30666666666684
    agent-3: 143.30666666666684
    agent-4: 143.30666666666684
    agent-5: 143.30666666666684
  policy_reward_min:
    agent-0: 92.6666666666667
    agent-1: 92.6666666666667
    agent-2: 92.6666666666667
    agent-3: 92.6666666666667
    agent-4: 92.6666666666667
    agent-5: 92.6666666666667
  sampler_perf:
    mean_env_wait_ms: 23.695235850906514
    mean_inference_ms: 12.292908487072665
    mean_processing_ms: 50.79011881946585
  time_since_restore: 25025.932470321655
  time_this_iter_s: 124.91929340362549
  time_total_s: 28236.996156454086
  timestamp: 1637042582
  timesteps_since_restore: 18528000
  timesteps_this_iter: 96000
  timesteps_total: 20448000
  training_iteration: 213
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    213 |            28237 | 20448000 |   859.84 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 69
    apples_agent-0_mean: 3.13
    apples_agent-0_min: 0
    apples_agent-1_max: 83
    apples_agent-1_mean: 24.64
    apples_agent-1_min: 0
    apples_agent-2_max: 107
    apples_agent-2_mean: 12.11
    apples_agent-2_min: 0
    apples_agent-3_max: 129
    apples_agent-3_mean: 63.49
    apples_agent-3_min: 25
    apples_agent-4_max: 64
    apples_agent-4_mean: 2.16
    apples_agent-4_min: 0
    apples_agent-5_max: 169
    apples_agent-5_mean: 99.05
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 592
    cleaning_beam_agent-0_mean: 431.24
    cleaning_beam_agent-0_min: 239
    cleaning_beam_agent-1_max: 361
    cleaning_beam_agent-1_mean: 226.46
    cleaning_beam_agent-1_min: 118
    cleaning_beam_agent-2_max: 483
    cleaning_beam_agent-2_mean: 289.94
    cleaning_beam_agent-2_min: 112
    cleaning_beam_agent-3_max: 117
    cleaning_beam_agent-3_mean: 22.39
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 538
    cleaning_beam_agent-4_mean: 411.73
    cleaning_beam_agent-4_min: 169
    cleaning_beam_agent-5_max: 854
    cleaning_beam_agent-5_mean: 136.66
    cleaning_beam_agent-5_min: 14
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 3
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-05-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1032.9999999999939
  episode_reward_mean: 850.1899999999808
  episode_reward_min: 320.00000000000125
  episodes_this_iter: 96
  episodes_total: 20544
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20154.5
    learner:
      agent-0:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.983863115310669
        entropy_coeff: 0.0017600000137463212
        kl: 0.001511979615315795
        model: {}
        policy_loss: -0.00372760696336627
        total_loss: -0.0035562985576689243
        vf_explained_var: -0.017495855689048767
        vf_loss: 19.02901268005371
      agent-1:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1436254978179932
        entropy_coeff: 0.0017600000137463212
        kl: 0.001394658349454403
        model: {}
        policy_loss: -0.00414398405700922
        total_loss: -0.004287114366889
        vf_explained_var: 0.0026081353425979614
        vf_loss: 18.695859909057617
      agent-2:
        cur_kl_coeff: 1.831054760259576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 1.117606520652771
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013345403131097555
        model: {}
        policy_loss: -0.00390067370608449
        total_loss: -0.004051746800541878
        vf_explained_var: 0.03967784345149994
        vf_loss: 18.158946990966797
      agent-3:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3953879475593567
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009809660259634256
        model: {}
        policy_loss: -0.0024958411231637
        total_loss: -0.0015903908060863614
        vf_explained_var: 0.13919402658939362
        vf_loss: 16.013320922851562
      agent-4:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9231525659561157
        entropy_coeff: 0.0017600000137463212
        kl: 0.001617749105207622
        model: {}
        policy_loss: -0.003896764013916254
        total_loss: -0.0038150374311953783
        vf_explained_var: 0.09004327654838562
        vf_loss: 17.064428329467773
      agent-5:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8711274862289429
        entropy_coeff: 0.0017600000137463212
        kl: 0.00151799654122442
        model: {}
        policy_loss: -0.004042589105665684
        total_loss: -0.003841238562017679
        vf_explained_var: 0.07823057472705841
        vf_loss: 17.345027923583984
    load_time_ms: 15473.596
    num_steps_sampled: 20544000
    num_steps_trained: 20544000
    sample_time_ms: 89190.033
    update_time_ms: 23.259
  iterations_since_restore: 194
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.06243093922652
    ram_util_percent: 9.629281767955803
  pid: 4061
  policy_reward_max:
    agent-0: 172.16666666666654
    agent-1: 172.16666666666654
    agent-2: 172.16666666666654
    agent-3: 172.16666666666654
    agent-4: 172.16666666666654
    agent-5: 172.16666666666654
  policy_reward_mean:
    agent-0: 141.69833333333352
    agent-1: 141.69833333333352
    agent-2: 141.69833333333352
    agent-3: 141.69833333333352
    agent-4: 141.69833333333352
    agent-5: 141.69833333333352
  policy_reward_min:
    agent-0: 53.333333333333165
    agent-1: 53.333333333333165
    agent-2: 53.333333333333165
    agent-3: 53.333333333333165
    agent-4: 53.333333333333165
    agent-5: 53.333333333333165
  sampler_perf:
    mean_env_wait_ms: 23.6991042367257
    mean_inference_ms: 12.292056445259762
    mean_processing_ms: 50.78623888186111
  time_since_restore: 25153.192079544067
  time_this_iter_s: 127.25960922241211
  time_total_s: 28364.2557656765
  timestamp: 1637042709
  timesteps_since_restore: 18624000
  timesteps_this_iter: 96000
  timesteps_total: 20544000
  training_iteration: 214
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    214 |          28364.3 | 20544000 |   850.19 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 44
    apples_agent-0_mean: 1.81
    apples_agent-0_min: 0
    apples_agent-1_max: 98
    apples_agent-1_mean: 29.93
    apples_agent-1_min: 0
    apples_agent-2_max: 114
    apples_agent-2_mean: 12.33
    apples_agent-2_min: 0
    apples_agent-3_max: 102
    apples_agent-3_mean: 60.08
    apples_agent-3_min: 32
    apples_agent-4_max: 78
    apples_agent-4_mean: 2.36
    apples_agent-4_min: 0
    apples_agent-5_max: 200
    apples_agent-5_mean: 107.64
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 577
    cleaning_beam_agent-0_mean: 423.46
    cleaning_beam_agent-0_min: 305
    cleaning_beam_agent-1_max: 397
    cleaning_beam_agent-1_mean: 235.45
    cleaning_beam_agent-1_min: 111
    cleaning_beam_agent-2_max: 518
    cleaning_beam_agent-2_mean: 298.96
    cleaning_beam_agent-2_min: 124
    cleaning_beam_agent-3_max: 76
    cleaning_beam_agent-3_mean: 20.24
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 538
    cleaning_beam_agent-4_mean: 414.61
    cleaning_beam_agent-4_min: 299
    cleaning_beam_agent-5_max: 882
    cleaning_beam_agent-5_mean: 103.67
    cleaning_beam_agent-5_min: 13
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-07-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1055.9999999999977
  episode_reward_mean: 865.6199999999809
  episode_reward_min: 307.0000000000001
  episodes_this_iter: 96
  episodes_total: 20640
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20164.264
    learner:
      agent-0:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9799056649208069
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017745770746842027
        model: {}
        policy_loss: -0.003554678987711668
        total_loss: -0.0033564071636646986
        vf_explained_var: -0.01177138090133667
        vf_loss: 19.22907829284668
      agent-1:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 1.140056848526001
        entropy_coeff: 0.0017600000137463212
        kl: 0.00219691707752645
        model: {}
        policy_loss: -0.004226938355714083
        total_loss: -0.004282447509467602
        vf_explained_var: -0.01808801293373108
        vf_loss: 19.50941276550293
      agent-2:
        cur_kl_coeff: 9.15527380129788e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1137758493423462
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008701166370883584
        model: {}
        policy_loss: -0.003521444508805871
        total_loss: -0.003631334286183119
        vf_explained_var: 0.0270165354013443
        vf_loss: 18.503507614135742
      agent-3:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39443498849868774
        entropy_coeff: 0.0017600000137463212
        kl: 0.001181893632747233
        model: {}
        policy_loss: -0.0022891913540661335
        total_loss: -0.0013491427525877953
        vf_explained_var: 0.13782817125320435
        vf_loss: 16.34255027770996
      agent-4:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.919927716255188
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017917272634804249
        model: {}
        policy_loss: -0.003979125060141087
        total_loss: -0.003820801619440317
        vf_explained_var: 0.06581366062164307
        vf_loss: 17.773815155029297
      agent-5:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.875507116317749
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019387522479519248
        model: {}
        policy_loss: -0.004152500070631504
        total_loss: -0.003965733107179403
        vf_explained_var: 0.09220540523529053
        vf_loss: 17.276378631591797
    load_time_ms: 15540.409
    num_steps_sampled: 20640000
    num_steps_trained: 20640000
    sample_time_ms: 89188.214
    update_time_ms: 23.229
  iterations_since_restore: 195
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.251685393258429
    ram_util_percent: 9.690449438202247
  pid: 4061
  policy_reward_max:
    agent-0: 175.9999999999997
    agent-1: 175.9999999999997
    agent-2: 175.9999999999997
    agent-3: 175.9999999999997
    agent-4: 175.9999999999997
    agent-5: 175.9999999999997
  policy_reward_mean:
    agent-0: 144.27000000000012
    agent-1: 144.27000000000012
    agent-2: 144.27000000000012
    agent-3: 144.27000000000012
    agent-4: 144.27000000000012
    agent-5: 144.27000000000012
  policy_reward_min:
    agent-0: 51.166666666666536
    agent-1: 51.166666666666536
    agent-2: 51.166666666666536
    agent-3: 51.166666666666536
    agent-4: 51.166666666666536
    agent-5: 51.166666666666536
  sampler_perf:
    mean_env_wait_ms: 23.702424245143654
    mean_inference_ms: 12.291064093440092
    mean_processing_ms: 50.781773373812165
  time_since_restore: 25277.772645950317
  time_this_iter_s: 124.58056640625
  time_total_s: 28488.83633208275
  timestamp: 1637042834
  timesteps_since_restore: 18720000
  timesteps_this_iter: 96000
  timesteps_total: 20640000
  training_iteration: 215
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    215 |          28488.8 | 20640000 |   865.62 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 41
    apples_agent-0_mean: 2.19
    apples_agent-0_min: 0
    apples_agent-1_max: 90
    apples_agent-1_mean: 26.52
    apples_agent-1_min: 0
    apples_agent-2_max: 79
    apples_agent-2_mean: 13.89
    apples_agent-2_min: 0
    apples_agent-3_max: 114
    apples_agent-3_mean: 62.85
    apples_agent-3_min: 33
    apples_agent-4_max: 67
    apples_agent-4_mean: 1.35
    apples_agent-4_min: 0
    apples_agent-5_max: 200
    apples_agent-5_mean: 100.13
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 562
    cleaning_beam_agent-0_mean: 433.52
    cleaning_beam_agent-0_min: 288
    cleaning_beam_agent-1_max: 351
    cleaning_beam_agent-1_mean: 227.62
    cleaning_beam_agent-1_min: 118
    cleaning_beam_agent-2_max: 518
    cleaning_beam_agent-2_mean: 290.85
    cleaning_beam_agent-2_min: 129
    cleaning_beam_agent-3_max: 158
    cleaning_beam_agent-3_mean: 24.26
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 509
    cleaning_beam_agent-4_mean: 409.37
    cleaning_beam_agent-4_min: 205
    cleaning_beam_agent-5_max: 946
    cleaning_beam_agent-5_mean: 111.4
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-09-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1074.9999999999923
  episode_reward_mean: 857.1799999999827
  episode_reward_min: 398.00000000000847
  episodes_this_iter: 96
  episodes_total: 20736
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20147.316
    learner:
      agent-0:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9737709760665894
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022388258948922157
        model: {}
        policy_loss: -0.0038426555693149567
        total_loss: -0.003799848258495331
        vf_explained_var: 0.002275317907333374
        vf_loss: 17.566471099853516
      agent-1:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1466310024261475
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017930414760485291
        model: {}
        policy_loss: -0.004270500969141722
        total_loss: -0.004498904105275869
        vf_explained_var: -0.012971282005310059
        vf_loss: 17.896535873413086
      agent-2:
        cur_kl_coeff: 4.57763690064894e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1339797973632812
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016860950272530317
        model: {}
        policy_loss: -0.003954632207751274
        total_loss: -0.004157709423452616
        vf_explained_var: -0.004460781812667847
        vf_loss: 17.927196502685547
      agent-3:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39128410816192627
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011326870881021023
        model: {}
        policy_loss: -0.002393382368609309
        total_loss: -0.0014574893284589052
        vf_explained_var: 0.07534898817539215
        vf_loss: 16.245521545410156
      agent-4:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9233434200286865
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018838808173313737
        model: {}
        policy_loss: -0.00422719307243824
        total_loss: -0.004162205848842859
        vf_explained_var: 0.03925788402557373
        vf_loss: 16.900663375854492
      agent-5:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.869990348815918
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023072545882314444
        model: {}
        policy_loss: -0.004638336133211851
        total_loss: -0.004527151584625244
        vf_explained_var: 0.06487293541431427
        vf_loss: 16.4235782623291
    load_time_ms: 15539.554
    num_steps_sampled: 20736000
    num_steps_trained: 20736000
    sample_time_ms: 89288.818
    update_time_ms: 22.361
  iterations_since_restore: 196
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.142696629213484
    ram_util_percent: 9.65561797752809
  pid: 4061
  policy_reward_max:
    agent-0: 179.1666666666661
    agent-1: 179.1666666666661
    agent-2: 179.1666666666661
    agent-3: 179.1666666666661
    agent-4: 179.1666666666661
    agent-5: 179.1666666666661
  policy_reward_mean:
    agent-0: 142.86333333333351
    agent-1: 142.86333333333351
    agent-2: 142.86333333333351
    agent-3: 142.86333333333351
    agent-4: 142.86333333333351
    agent-5: 142.86333333333351
  policy_reward_min:
    agent-0: 66.33333333333317
    agent-1: 66.33333333333317
    agent-2: 66.33333333333317
    agent-3: 66.33333333333317
    agent-4: 66.33333333333317
    agent-5: 66.33333333333317
  sampler_perf:
    mean_env_wait_ms: 23.705696050327173
    mean_inference_ms: 12.290329603670965
    mean_processing_ms: 50.77795628580545
  time_since_restore: 25402.52135848999
  time_this_iter_s: 124.74871253967285
  time_total_s: 28613.58504462242
  timestamp: 1637042959
  timesteps_since_restore: 18816000
  timesteps_this_iter: 96000
  timesteps_total: 20736000
  training_iteration: 216
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    216 |          28613.6 | 20736000 |   857.18 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 42
    apples_agent-0_mean: 2.3
    apples_agent-0_min: 0
    apples_agent-1_max: 84
    apples_agent-1_mean: 25.18
    apples_agent-1_min: 0
    apples_agent-2_max: 87
    apples_agent-2_mean: 9.96
    apples_agent-2_min: 0
    apples_agent-3_max: 129
    apples_agent-3_mean: 63.3
    apples_agent-3_min: 29
    apples_agent-4_max: 39
    apples_agent-4_mean: 0.39
    apples_agent-4_min: 0
    apples_agent-5_max: 205
    apples_agent-5_mean: 109.18
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 536
    cleaning_beam_agent-0_mean: 412.6
    cleaning_beam_agent-0_min: 285
    cleaning_beam_agent-1_max: 390
    cleaning_beam_agent-1_mean: 235.76
    cleaning_beam_agent-1_min: 116
    cleaning_beam_agent-2_max: 508
    cleaning_beam_agent-2_mean: 313.21
    cleaning_beam_agent-2_min: 136
    cleaning_beam_agent-3_max: 82
    cleaning_beam_agent-3_mean: 19.86
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 548
    cleaning_beam_agent-4_mean: 421.0
    cleaning_beam_agent-4_min: 260
    cleaning_beam_agent-5_max: 829
    cleaning_beam_agent-5_mean: 93.41
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-11-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1026.9999999999761
  episode_reward_mean: 879.26999999998
  episode_reward_min: 288.00000000000017
  episodes_this_iter: 96
  episodes_total: 20832
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20156.058
    learner:
      agent-0:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9795945882797241
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018690424039959908
        model: {}
        policy_loss: -0.003319374518468976
        total_loss: -0.0031167331617325544
        vf_explained_var: 0.007703274488449097
        vf_loss: 19.267292022705078
      agent-1:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 1.135223388671875
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018370493780821562
        model: {}
        policy_loss: -0.004227340221405029
        total_loss: -0.004284943453967571
        vf_explained_var: 0.006088942289352417
        vf_loss: 19.403772354125977
      agent-2:
        cur_kl_coeff: 2.28881845032447e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1088780164718628
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011016583302989602
        model: {}
        policy_loss: -0.0034836800768971443
        total_loss: -0.003595084650442004
        vf_explained_var: 0.04911758005619049
        vf_loss: 18.402206420898438
      agent-3:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3770364820957184
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006489678053185344
        model: {}
        policy_loss: -0.002328694798052311
        total_loss: -0.0012875895481556654
        vf_explained_var: 0.11092488467693329
        vf_loss: 17.046886444091797
      agent-4:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9068825244903564
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012583171483129263
        model: {}
        policy_loss: -0.0038357526063919067
        total_loss: -0.0036102947778999805
        vf_explained_var: 0.06091712415218353
        vf_loss: 18.215702056884766
      agent-5:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8659902811050415
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019923667423427105
        model: {}
        policy_loss: -0.004273062571883202
        total_loss: -0.004086728673428297
        vf_explained_var: 0.110218346118927
        vf_loss: 17.104738235473633
    load_time_ms: 15646.485
    num_steps_sampled: 20832000
    num_steps_trained: 20832000
    sample_time_ms: 89334.394
    update_time_ms: 21.455
  iterations_since_restore: 197
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 16.23876404494382
    ram_util_percent: 9.538202247191013
  pid: 4061
  policy_reward_max:
    agent-0: 171.16666666666634
    agent-1: 171.16666666666634
    agent-2: 171.16666666666634
    agent-3: 171.16666666666634
    agent-4: 171.16666666666634
    agent-5: 171.16666666666634
  policy_reward_mean:
    agent-0: 146.54500000000004
    agent-1: 146.54500000000004
    agent-2: 146.54500000000004
    agent-3: 146.54500000000004
    agent-4: 146.54500000000004
    agent-5: 146.54500000000004
  policy_reward_min:
    agent-0: 47.99999999999991
    agent-1: 47.99999999999991
    agent-2: 47.99999999999991
    agent-3: 47.99999999999991
    agent-4: 47.99999999999991
    agent-5: 47.99999999999991
  sampler_perf:
    mean_env_wait_ms: 23.710192017975295
    mean_inference_ms: 12.29000869048888
    mean_processing_ms: 50.776156489696994
  time_since_restore: 25527.603198051453
  time_this_iter_s: 125.0818395614624
  time_total_s: 28738.666884183884
  timestamp: 1637043084
  timesteps_since_restore: 18912000
  timesteps_this_iter: 96000
  timesteps_total: 20832000
  training_iteration: 217
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    217 |          28738.7 | 20832000 |   879.27 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 36
    apples_agent-0_mean: 1.98
    apples_agent-0_min: 0
    apples_agent-1_max: 67
    apples_agent-1_mean: 23.61
    apples_agent-1_min: 0
    apples_agent-2_max: 189
    apples_agent-2_mean: 15.12
    apples_agent-2_min: 0
    apples_agent-3_max: 189
    apples_agent-3_mean: 63.27
    apples_agent-3_min: 25
    apples_agent-4_max: 79
    apples_agent-4_mean: 2.01
    apples_agent-4_min: 0
    apples_agent-5_max: 170
    apples_agent-5_mean: 108.48
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 515
    cleaning_beam_agent-0_mean: 417.33
    cleaning_beam_agent-0_min: 278
    cleaning_beam_agent-1_max: 369
    cleaning_beam_agent-1_mean: 230.67
    cleaning_beam_agent-1_min: 119
    cleaning_beam_agent-2_max: 465
    cleaning_beam_agent-2_mean: 296.39
    cleaning_beam_agent-2_min: 133
    cleaning_beam_agent-3_max: 174
    cleaning_beam_agent-3_mean: 26.63
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 514
    cleaning_beam_agent-4_mean: 409.0
    cleaning_beam_agent-4_min: 191
    cleaning_beam_agent-5_max: 531
    cleaning_beam_agent-5_mean: 76.62
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-13-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1034.9999999999845
  episode_reward_mean: 863.9599999999826
  episode_reward_min: 321.00000000000193
  episodes_this_iter: 96
  episodes_total: 20928
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20137.958
    learner:
      agent-0:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9656380414962769
        entropy_coeff: 0.0017600000137463212
        kl: 0.00191711843945086
        model: {}
        policy_loss: -0.0035961568355560303
        total_loss: -0.0033137346617877483
        vf_explained_var: -0.011309772729873657
        vf_loss: 19.819433212280273
      agent-1:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1390873193740845
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016306465258821845
        model: {}
        policy_loss: -0.004190213046967983
        total_loss: -0.004230027552694082
        vf_explained_var: 0.007469981908798218
        vf_loss: 19.64972686767578
      agent-2:
        cur_kl_coeff: 1.144409225162235e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1235086917877197
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016450888942927122
        model: {}
        policy_loss: -0.0036959885619580746
        total_loss: -0.0037021986208856106
        vf_explained_var: 0.011861428618431091
        vf_loss: 19.711631774902344
      agent-3:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4053439199924469
        entropy_coeff: 0.0017600000137463212
        kl: 0.000956746400333941
        model: {}
        policy_loss: -0.002380884252488613
        total_loss: -0.0014091050252318382
        vf_explained_var: 0.13946719467639923
        vf_loss: 16.85184669494629
      agent-4:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9200438261032104
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016633137129247189
        model: {}
        policy_loss: -0.004212478641420603
        total_loss: -0.004037370439618826
        vf_explained_var: 0.08899590373039246
        vf_loss: 17.943828582763672
      agent-5:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8769786953926086
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016833336558192968
        model: {}
        policy_loss: -0.004589182790368795
        total_loss: -0.0043987673707306385
        vf_explained_var: 0.1062251478433609
        vf_loss: 17.33896255493164
    load_time_ms: 15834.594
    num_steps_sampled: 20928000
    num_steps_trained: 20928000
    sample_time_ms: 89305.554
    update_time_ms: 21.202
  iterations_since_restore: 198
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.251685393258429
    ram_util_percent: 9.608426966292136
  pid: 4061
  policy_reward_max:
    agent-0: 172.49999999999952
    agent-1: 172.49999999999952
    agent-2: 172.49999999999952
    agent-3: 172.49999999999952
    agent-4: 172.49999999999952
    agent-5: 172.49999999999952
  policy_reward_mean:
    agent-0: 143.9933333333334
    agent-1: 143.9933333333334
    agent-2: 143.9933333333334
    agent-3: 143.9933333333334
    agent-4: 143.9933333333334
    agent-5: 143.9933333333334
  policy_reward_min:
    agent-0: 53.49999999999988
    agent-1: 53.49999999999988
    agent-2: 53.49999999999988
    agent-3: 53.49999999999988
    agent-4: 53.49999999999988
    agent-5: 53.49999999999988
  sampler_perf:
    mean_env_wait_ms: 23.71331162633061
    mean_inference_ms: 12.289367206958332
    mean_processing_ms: 50.773223086986725
  time_since_restore: 25652.805549383163
  time_this_iter_s: 125.20235133171082
  time_total_s: 28863.869235515594
  timestamp: 1637043209
  timesteps_since_restore: 19008000
  timesteps_this_iter: 96000
  timesteps_total: 20928000
  training_iteration: 218
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    218 |          28863.9 | 20928000 |   863.96 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 57
    apples_agent-0_mean: 3.05
    apples_agent-0_min: 0
    apples_agent-1_max: 109
    apples_agent-1_mean: 25.61
    apples_agent-1_min: 0
    apples_agent-2_max: 113
    apples_agent-2_mean: 9.1
    apples_agent-2_min: 0
    apples_agent-3_max: 157
    apples_agent-3_mean: 64.43
    apples_agent-3_min: 33
    apples_agent-4_max: 58
    apples_agent-4_mean: 0.74
    apples_agent-4_min: 0
    apples_agent-5_max: 185
    apples_agent-5_mean: 112.58
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 538
    cleaning_beam_agent-0_mean: 419.93
    cleaning_beam_agent-0_min: 294
    cleaning_beam_agent-1_max: 391
    cleaning_beam_agent-1_mean: 230.25
    cleaning_beam_agent-1_min: 98
    cleaning_beam_agent-2_max: 467
    cleaning_beam_agent-2_mean: 306.84
    cleaning_beam_agent-2_min: 108
    cleaning_beam_agent-3_max: 127
    cleaning_beam_agent-3_mean: 22.8
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 534
    cleaning_beam_agent-4_mean: 411.86
    cleaning_beam_agent-4_min: 291
    cleaning_beam_agent-5_max: 702
    cleaning_beam_agent-5_mean: 88.81
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-15-35
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1062.9999999999998
  episode_reward_mean: 876.9999999999808
  episode_reward_min: 651.9999999999916
  episodes_this_iter: 96
  episodes_total: 21024
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20113.8
    learner:
      agent-0:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9734125733375549
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023369549307972193
        model: {}
        policy_loss: -0.0036943210288882256
        total_loss: -0.0036381962709128857
        vf_explained_var: 0.040720149874687195
        vf_loss: 17.693313598632812
      agent-1:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1366547346115112
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013599108206108212
        model: {}
        policy_loss: -0.004023036919534206
        total_loss: -0.004151714500039816
        vf_explained_var: -0.004258960485458374
        vf_loss: 18.718347549438477
      agent-2:
        cur_kl_coeff: 5.722046125811175e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 1.128969430923462
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011532283388078213
        model: {}
        policy_loss: -0.003941655158996582
        total_loss: -0.0041381060145795345
        vf_explained_var: 0.022936850786209106
        vf_loss: 17.9053955078125
      agent-3:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3876676559448242
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009189650299958885
        model: {}
        policy_loss: -0.0022949115373194218
        total_loss: -0.0013588827569037676
        vf_explained_var: 0.11280925571918488
        vf_loss: 16.183229446411133
      agent-4:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9156848192214966
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017949535977095366
        model: {}
        policy_loss: -0.0040556942112743855
        total_loss: -0.003907914273440838
        vf_explained_var: 0.03625333309173584
        vf_loss: 17.593881607055664
      agent-5:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.856259822845459
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016269874759018421
        model: {}
        policy_loss: -0.0038267350755631924
        total_loss: -0.0036571254022419453
        vf_explained_var: 0.07650142908096313
        vf_loss: 16.766281127929688
    load_time_ms: 15708.384
    num_steps_sampled: 21024000
    num_steps_trained: 21024000
    sample_time_ms: 89586.126
    update_time_ms: 21.106
  iterations_since_restore: 199
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.194972067039107
    ram_util_percent: 9.63854748603352
  pid: 4061
  policy_reward_max:
    agent-0: 177.16666666666643
    agent-1: 177.16666666666643
    agent-2: 177.16666666666643
    agent-3: 177.16666666666643
    agent-4: 177.16666666666643
    agent-5: 177.16666666666643
  policy_reward_mean:
    agent-0: 146.16666666666674
    agent-1: 146.16666666666674
    agent-2: 146.16666666666674
    agent-3: 146.16666666666674
    agent-4: 146.16666666666674
    agent-5: 146.16666666666674
  policy_reward_min:
    agent-0: 108.66666666666704
    agent-1: 108.66666666666704
    agent-2: 108.66666666666704
    agent-3: 108.66666666666704
    agent-4: 108.66666666666704
    agent-5: 108.66666666666704
  sampler_perf:
    mean_env_wait_ms: 23.716932532616898
    mean_inference_ms: 12.288764742408093
    mean_processing_ms: 50.77044518517313
  time_since_restore: 25777.79473876953
  time_this_iter_s: 124.9891893863678
  time_total_s: 28988.858424901962
  timestamp: 1637043335
  timesteps_since_restore: 19104000
  timesteps_this_iter: 96000
  timesteps_total: 21024000
  training_iteration: 219
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    219 |          28988.9 | 21024000 |      877 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 36
    apples_agent-0_mean: 1.71
    apples_agent-0_min: 0
    apples_agent-1_max: 97
    apples_agent-1_mean: 30.81
    apples_agent-1_min: 0
    apples_agent-2_max: 132
    apples_agent-2_mean: 15.78
    apples_agent-2_min: 0
    apples_agent-3_max: 157
    apples_agent-3_mean: 64.21
    apples_agent-3_min: 21
    apples_agent-4_max: 67
    apples_agent-4_mean: 1.7
    apples_agent-4_min: 0
    apples_agent-5_max: 225
    apples_agent-5_mean: 111.02
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 556
    cleaning_beam_agent-0_mean: 411.97
    cleaning_beam_agent-0_min: 296
    cleaning_beam_agent-1_max: 399
    cleaning_beam_agent-1_mean: 235.53
    cleaning_beam_agent-1_min: 118
    cleaning_beam_agent-2_max: 478
    cleaning_beam_agent-2_mean: 294.01
    cleaning_beam_agent-2_min: 108
    cleaning_beam_agent-3_max: 191
    cleaning_beam_agent-3_mean: 23.53
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 528
    cleaning_beam_agent-4_mean: 407.63
    cleaning_beam_agent-4_min: 200
    cleaning_beam_agent-5_max: 744
    cleaning_beam_agent-5_mean: 103.82
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-17-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1025.9999999999793
  episode_reward_mean: 874.2399999999805
  episode_reward_min: 308.0000000000006
  episodes_this_iter: 96
  episodes_total: 21120
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20122.387
    learner:
      agent-0:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9690956473350525
        entropy_coeff: 0.0017600000137463212
        kl: 0.001205111388117075
        model: {}
        policy_loss: -0.003275381401181221
        total_loss: -0.0030696047469973564
        vf_explained_var: -0.008439123630523682
        vf_loss: 19.113872528076172
      agent-1:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1414631605148315
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012440577847883105
        model: {}
        policy_loss: -0.004006907809525728
        total_loss: -0.0040403809398412704
        vf_explained_var: -0.036878734827041626
        vf_loss: 19.75503158569336
      agent-2:
        cur_kl_coeff: 2.8610230629055877e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 1.130043387413025
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021963189356029034
        model: {}
        policy_loss: -0.004026573151350021
        total_loss: -0.004159287083894014
        vf_explained_var: 0.03282177448272705
        vf_loss: 18.561609268188477
      agent-3:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39843666553497314
        entropy_coeff: 0.0017600000137463212
        kl: 0.000906564702745527
        model: {}
        policy_loss: -0.002279639709740877
        total_loss: -0.0012885285541415215
        vf_explained_var: 0.10604675114154816
        vf_loss: 16.92359733581543
      agent-4:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9167342782020569
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015030289068818092
        model: {}
        policy_loss: -0.004005906172096729
        total_loss: -0.003944141790270805
        vf_explained_var: 0.11119326949119568
        vf_loss: 16.752164840698242
      agent-5:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8668214678764343
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015072086825966835
        model: {}
        policy_loss: -0.003967531491070986
        total_loss: -0.0037429218646138906
        vf_explained_var: 0.07431191205978394
        vf_loss: 17.502180099487305
    load_time_ms: 15627.056
    num_steps_sampled: 21120000
    num_steps_trained: 21120000
    sample_time_ms: 89620.091
    update_time_ms: 21.31
  iterations_since_restore: 200
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.446022727272727
    ram_util_percent: 9.579545454545455
  pid: 4061
  policy_reward_max:
    agent-0: 170.9999999999996
    agent-1: 170.9999999999996
    agent-2: 170.9999999999996
    agent-3: 170.9999999999996
    agent-4: 170.9999999999996
    agent-5: 170.9999999999996
  policy_reward_mean:
    agent-0: 145.70666666666676
    agent-1: 145.70666666666676
    agent-2: 145.70666666666676
    agent-3: 145.70666666666676
    agent-4: 145.70666666666676
    agent-5: 145.70666666666676
  policy_reward_min:
    agent-0: 51.33333333333317
    agent-1: 51.33333333333317
    agent-2: 51.33333333333317
    agent-3: 51.33333333333317
    agent-4: 51.33333333333317
    agent-5: 51.33333333333317
  sampler_perf:
    mean_env_wait_ms: 23.720606327359345
    mean_inference_ms: 12.288128228606078
    mean_processing_ms: 50.768298355982225
  time_since_restore: 25901.106947898865
  time_this_iter_s: 123.3122091293335
  time_total_s: 29112.170634031296
  timestamp: 1637043458
  timesteps_since_restore: 19200000
  timesteps_this_iter: 96000
  timesteps_total: 21120000
  training_iteration: 220
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    220 |          29112.2 | 21120000 |   874.24 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 51
    apples_agent-0_mean: 2.13
    apples_agent-0_min: 0
    apples_agent-1_max: 88
    apples_agent-1_mean: 28.16
    apples_agent-1_min: 0
    apples_agent-2_max: 99
    apples_agent-2_mean: 13.03
    apples_agent-2_min: 0
    apples_agent-3_max: 130
    apples_agent-3_mean: 59.98
    apples_agent-3_min: 31
    apples_agent-4_max: 29
    apples_agent-4_mean: 0.48
    apples_agent-4_min: 0
    apples_agent-5_max: 189
    apples_agent-5_mean: 109.07
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 575
    cleaning_beam_agent-0_mean: 411.13
    cleaning_beam_agent-0_min: 249
    cleaning_beam_agent-1_max: 347
    cleaning_beam_agent-1_mean: 234.89
    cleaning_beam_agent-1_min: 133
    cleaning_beam_agent-2_max: 496
    cleaning_beam_agent-2_mean: 288.14
    cleaning_beam_agent-2_min: 97
    cleaning_beam_agent-3_max: 84
    cleaning_beam_agent-3_mean: 23.6
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 519
    cleaning_beam_agent-4_mean: 406.72
    cleaning_beam_agent-4_min: 283
    cleaning_beam_agent-5_max: 877
    cleaning_beam_agent-5_mean: 93.42
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-19-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1083.9999999999905
  episode_reward_mean: 876.0599999999827
  episode_reward_min: 452.00000000000824
  episodes_this_iter: 96
  episodes_total: 21216
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20152.588
    learner:
      agent-0:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9710694551467896
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016264688456431031
        model: {}
        policy_loss: -0.0036598246078938246
        total_loss: -0.0034975730814039707
        vf_explained_var: -0.007174268364906311
        vf_loss: 18.71333885192871
      agent-1:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1436433792114258
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015563571359962225
        model: {}
        policy_loss: -0.004304052796214819
        total_loss: -0.00446705985814333
        vf_explained_var: 0.011177211999893188
        vf_loss: 18.498062133789062
      agent-2:
        cur_kl_coeff: 1.4305115314527939e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1249502897262573
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012744155246764421
        model: {}
        policy_loss: -0.0037521403282880783
        total_loss: -0.003892954671755433
        vf_explained_var: 0.016511663794517517
        vf_loss: 18.390941619873047
      agent-3:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38763177394866943
        entropy_coeff: 0.0017600000137463212
        kl: 0.00100128713529557
        model: {}
        policy_loss: -0.0023332827258855104
        total_loss: -0.0013996772468090057
        vf_explained_var: 0.12593737244606018
        vf_loss: 16.158409118652344
      agent-4:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9175109267234802
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019647444132715464
        model: {}
        policy_loss: -0.004157647490501404
        total_loss: -0.0040422119200229645
        vf_explained_var: 0.06424248218536377
        vf_loss: 17.302553176879883
      agent-5:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8520824313163757
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016478241886943579
        model: {}
        policy_loss: -0.003992775455117226
        total_loss: -0.003740520216524601
        vf_explained_var: 0.04620291292667389
        vf_loss: 17.519195556640625
    load_time_ms: 15720.827
    num_steps_sampled: 21216000
    num_steps_trained: 21216000
    sample_time_ms: 89715.072
    update_time_ms: 21.573
  iterations_since_restore: 201
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.575690607734806
    ram_util_percent: 9.597237569060775
  pid: 4061
  policy_reward_max:
    agent-0: 180.66666666666603
    agent-1: 180.66666666666603
    agent-2: 180.66666666666603
    agent-3: 180.66666666666603
    agent-4: 180.66666666666603
    agent-5: 180.66666666666603
  policy_reward_mean:
    agent-0: 146.0100000000001
    agent-1: 146.0100000000001
    agent-2: 146.0100000000001
    agent-3: 146.0100000000001
    agent-4: 146.0100000000001
    agent-5: 146.0100000000001
  policy_reward_min:
    agent-0: 75.33333333333317
    agent-1: 75.33333333333317
    agent-2: 75.33333333333317
    agent-3: 75.33333333333317
    agent-4: 75.33333333333317
    agent-5: 75.33333333333317
  sampler_perf:
    mean_env_wait_ms: 23.72431845568123
    mean_inference_ms: 12.287462393135433
    mean_processing_ms: 50.765553337601695
  time_since_restore: 26028.15099620819
  time_this_iter_s: 127.04404830932617
  time_total_s: 29239.214682340622
  timestamp: 1637043586
  timesteps_since_restore: 19296000
  timesteps_this_iter: 96000
  timesteps_total: 21216000
  training_iteration: 221
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    221 |          29239.2 | 21216000 |   876.06 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 52
    apples_agent-0_mean: 2.09
    apples_agent-0_min: 0
    apples_agent-1_max: 79
    apples_agent-1_mean: 27.96
    apples_agent-1_min: 0
    apples_agent-2_max: 99
    apples_agent-2_mean: 10.34
    apples_agent-2_min: 0
    apples_agent-3_max: 92
    apples_agent-3_mean: 56.69
    apples_agent-3_min: 34
    apples_agent-4_max: 74
    apples_agent-4_mean: 3.04
    apples_agent-4_min: 0
    apples_agent-5_max: 185
    apples_agent-5_mean: 106.7
    apples_agent-5_min: 8
    cleaning_beam_agent-0_max: 575
    cleaning_beam_agent-0_mean: 400.86
    cleaning_beam_agent-0_min: 294
    cleaning_beam_agent-1_max: 371
    cleaning_beam_agent-1_mean: 228.76
    cleaning_beam_agent-1_min: 136
    cleaning_beam_agent-2_max: 470
    cleaning_beam_agent-2_mean: 301.84
    cleaning_beam_agent-2_min: 139
    cleaning_beam_agent-3_max: 80
    cleaning_beam_agent-3_mean: 28.05
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 508
    cleaning_beam_agent-4_mean: 407.48
    cleaning_beam_agent-4_min: 225
    cleaning_beam_agent-5_max: 724
    cleaning_beam_agent-5_mean: 84.22
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-21-49
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1085.9999999999854
  episode_reward_mean: 875.0499999999807
  episode_reward_min: 295.99999999999966
  episodes_this_iter: 96
  episodes_total: 21312
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20122.818
    learner:
      agent-0:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9874669909477234
        entropy_coeff: 0.0017600000137463212
        kl: 0.002619902603328228
        model: {}
        policy_loss: -0.0036691189743578434
        total_loss: -0.003382827155292034
        vf_explained_var: 0.014553740620613098
        vf_loss: 20.242347717285156
      agent-1:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 1.126373529434204
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020131373312324286
        model: {}
        policy_loss: -0.004270337987691164
        total_loss: -0.0040909294039011
        vf_explained_var: -0.04450938105583191
        vf_loss: 21.61823272705078
      agent-2:
        cur_kl_coeff: 7.152557657263969e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 1.124815583229065
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014515521470457315
        model: {}
        policy_loss: -0.0035675973631441593
        total_loss: -0.0035637777764350176
        vf_explained_var: 0.029791101813316345
        vf_loss: 19.834959030151367
      agent-3:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39737117290496826
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009130824473686516
        model: {}
        policy_loss: -0.0023463969118893147
        total_loss: -0.0012718182988464832
        vf_explained_var: 0.13073337078094482
        vf_loss: 17.73952293395996
      agent-4:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9206717014312744
        entropy_coeff: 0.0017600000137463212
        kl: 0.00120061996858567
        model: {}
        policy_loss: -0.003688326571136713
        total_loss: -0.0034631446469575167
        vf_explained_var: 0.09656810760498047
        vf_loss: 18.45565414428711
      agent-5:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8508846759796143
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012968897353857756
        model: {}
        policy_loss: -0.003749965224415064
        total_loss: -0.003391899401322007
        vf_explained_var: 0.08843305706977844
        vf_loss: 18.556209564208984
    load_time_ms: 15018.344
    num_steps_sampled: 21312000
    num_steps_trained: 21312000
    sample_time_ms: 89735.104
    update_time_ms: 21.752
  iterations_since_restore: 202
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.97873563218391
    ram_util_percent: 9.639655172413795
  pid: 4061
  policy_reward_max:
    agent-0: 180.99999999999937
    agent-1: 180.99999999999937
    agent-2: 180.99999999999937
    agent-3: 180.99999999999937
    agent-4: 180.99999999999937
    agent-5: 180.99999999999937
  policy_reward_mean:
    agent-0: 145.8416666666667
    agent-1: 145.8416666666667
    agent-2: 145.8416666666667
    agent-3: 145.8416666666667
    agent-4: 145.8416666666667
    agent-5: 145.8416666666667
  policy_reward_min:
    agent-0: 49.333333333333165
    agent-1: 49.333333333333165
    agent-2: 49.333333333333165
    agent-3: 49.333333333333165
    agent-4: 49.333333333333165
    agent-5: 49.333333333333165
  sampler_perf:
    mean_env_wait_ms: 23.727726085192888
    mean_inference_ms: 12.286885040547363
    mean_processing_ms: 50.76360139430149
  time_since_restore: 26151.0477912426
  time_this_iter_s: 122.89679503440857
  time_total_s: 29362.11147737503
  timestamp: 1637043709
  timesteps_since_restore: 19392000
  timesteps_this_iter: 96000
  timesteps_total: 21312000
  training_iteration: 222
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    222 |          29362.1 | 21312000 |   875.05 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 1.74
    apples_agent-0_min: 0
    apples_agent-1_max: 72
    apples_agent-1_mean: 26.97
    apples_agent-1_min: 0
    apples_agent-2_max: 107
    apples_agent-2_mean: 14.55
    apples_agent-2_min: 0
    apples_agent-3_max: 110
    apples_agent-3_mean: 60.85
    apples_agent-3_min: 30
    apples_agent-4_max: 26
    apples_agent-4_mean: 0.46
    apples_agent-4_min: 0
    apples_agent-5_max: 178
    apples_agent-5_mean: 105.6
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 507
    cleaning_beam_agent-0_mean: 384.0
    cleaning_beam_agent-0_min: 225
    cleaning_beam_agent-1_max: 472
    cleaning_beam_agent-1_mean: 225.79
    cleaning_beam_agent-1_min: 114
    cleaning_beam_agent-2_max: 464
    cleaning_beam_agent-2_mean: 277.79
    cleaning_beam_agent-2_min: 91
    cleaning_beam_agent-3_max: 91
    cleaning_beam_agent-3_mean: 25.11
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 533
    cleaning_beam_agent-4_mean: 409.78
    cleaning_beam_agent-4_min: 318
    cleaning_beam_agent-5_max: 778
    cleaning_beam_agent-5_mean: 95.11
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-23-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1030.9999999999852
  episode_reward_mean: 879.3999999999813
  episode_reward_min: 633.9999999999884
  episodes_this_iter: 96
  episodes_total: 21408
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20142.626
    learner:
      agent-0:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9962701797485352
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014177273260429502
        model: {}
        policy_loss: -0.0035497834905982018
        total_loss: -0.0034690559841692448
        vf_explained_var: -0.00608515739440918
        vf_loss: 18.341638565063477
      agent-1:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1339877843856812
        entropy_coeff: 0.0017600000137463212
        kl: 0.001401222194544971
        model: {}
        policy_loss: -0.004067777190357447
        total_loss: -0.004270648583769798
        vf_explained_var: 0.016880661249160767
        vf_loss: 17.929479598999023
      agent-2:
        cur_kl_coeff: 3.5762788286319847e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 1.128385066986084
        entropy_coeff: 0.0017600000137463212
        kl: 0.00137388298753649
        model: {}
        policy_loss: -0.0034930906258523464
        total_loss: -0.0036293177399784327
        vf_explained_var: -0.015196919441223145
        vf_loss: 18.497310638427734
      agent-3:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3873005509376526
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011280867038294673
        model: {}
        policy_loss: -0.002183211036026478
        total_loss: -0.0012885861797258258
        vf_explained_var: 0.12032027542591095
        vf_loss: 15.762739181518555
      agent-4:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9061902761459351
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018269086722284555
        model: {}
        policy_loss: -0.003780410625040531
        total_loss: -0.003660033456981182
        vf_explained_var: 0.04640580713748932
        vf_loss: 17.1527099609375
      agent-5:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8408393263816833
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017803290393203497
        model: {}
        policy_loss: -0.00439249724149704
        total_loss: -0.004229161888360977
        vf_explained_var: 0.08463913202285767
        vf_loss: 16.43213653564453
    load_time_ms: 14954.099
    num_steps_sampled: 21408000
    num_steps_trained: 21408000
    sample_time_ms: 89694.459
    update_time_ms: 21.714
  iterations_since_restore: 203
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.257627118644066
    ram_util_percent: 9.530508474576273
  pid: 4061
  policy_reward_max:
    agent-0: 171.83333333333326
    agent-1: 171.83333333333326
    agent-2: 171.83333333333326
    agent-3: 171.83333333333326
    agent-4: 171.83333333333326
    agent-5: 171.83333333333326
  policy_reward_mean:
    agent-0: 146.56666666666683
    agent-1: 146.56666666666683
    agent-2: 146.56666666666683
    agent-3: 146.56666666666683
    agent-4: 146.56666666666683
    agent-5: 146.56666666666683
  policy_reward_min:
    agent-0: 105.66666666666696
    agent-1: 105.66666666666696
    agent-2: 105.66666666666696
    agent-3: 105.66666666666696
    agent-4: 105.66666666666696
    agent-5: 105.66666666666696
  sampler_perf:
    mean_env_wait_ms: 23.72991715212887
    mean_inference_ms: 12.28607974299613
    mean_processing_ms: 50.75991240005003
  time_since_restore: 26275.111652612686
  time_this_iter_s: 124.06386137008667
  time_total_s: 29486.175338745117
  timestamp: 1637043833
  timesteps_since_restore: 19488000
  timesteps_this_iter: 96000
  timesteps_total: 21408000
  training_iteration: 223
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    223 |          29486.2 | 21408000 |    879.4 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 21
    apples_agent-0_mean: 1.27
    apples_agent-0_min: 0
    apples_agent-1_max: 93
    apples_agent-1_mean: 23.73
    apples_agent-1_min: 0
    apples_agent-2_max: 67
    apples_agent-2_mean: 9.56
    apples_agent-2_min: 0
    apples_agent-3_max: 117
    apples_agent-3_mean: 62.1
    apples_agent-3_min: 28
    apples_agent-4_max: 65
    apples_agent-4_mean: 0.96
    apples_agent-4_min: 0
    apples_agent-5_max: 172
    apples_agent-5_mean: 110.87
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 554
    cleaning_beam_agent-0_mean: 386.55
    cleaning_beam_agent-0_min: 286
    cleaning_beam_agent-1_max: 472
    cleaning_beam_agent-1_mean: 238.09
    cleaning_beam_agent-1_min: 104
    cleaning_beam_agent-2_max: 532
    cleaning_beam_agent-2_mean: 294.56
    cleaning_beam_agent-2_min: 128
    cleaning_beam_agent-3_max: 142
    cleaning_beam_agent-3_mean: 27.04
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 571
    cleaning_beam_agent-4_mean: 427.95
    cleaning_beam_agent-4_min: 284
    cleaning_beam_agent-5_max: 772
    cleaning_beam_agent-5_mean: 72.73
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-25-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1082.9999999999916
  episode_reward_mean: 900.3399999999823
  episode_reward_min: 475.00000000001006
  episodes_this_iter: 96
  episodes_total: 21504
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20150.515
    learner:
      agent-0:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9875528216362
        entropy_coeff: 0.0017600000137463212
        kl: 0.002355727832764387
        model: {}
        policy_loss: -0.003552759066224098
        total_loss: -0.0034633767791092396
        vf_explained_var: -0.005136072635650635
        vf_loss: 18.27474594116211
      agent-1:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 1.136662483215332
        entropy_coeff: 0.0017600000137463212
        kl: 0.002161872573196888
        model: {}
        policy_loss: -0.0043838173151016235
        total_loss: -0.004528345540165901
        vf_explained_var: -0.0038581937551498413
        vf_loss: 18.559947967529297
      agent-2:
        cur_kl_coeff: 1.7881394143159923e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 1.131623387336731
        entropy_coeff: 0.0017600000137463212
        kl: 0.001738874358125031
        model: {}
        policy_loss: -0.003976873122155666
        total_loss: -0.004235714208334684
        vf_explained_var: 0.04503127932548523
        vf_loss: 17.328149795532227
      agent-3:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.369401216506958
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005888830637559295
        model: {}
        policy_loss: -0.0020804861560463905
        total_loss: -0.0011276639997959137
        vf_explained_var: 0.10485532879829407
        vf_loss: 16.029651641845703
      agent-4:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8971643447875977
        entropy_coeff: 0.0017600000137463212
        kl: 0.001835343544371426
        model: {}
        policy_loss: -0.004100923426449299
        total_loss: -0.004001338500529528
        vf_explained_var: 0.06940169632434845
        vf_loss: 16.785964965820312
      agent-5:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.81685870885849
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023414206225425005
        model: {}
        policy_loss: -0.004158665891736746
        total_loss: -0.003890221007168293
        vf_explained_var: 0.04340970516204834
        vf_loss: 17.061141967773438
    load_time_ms: 14735.046
    num_steps_sampled: 21504000
    num_steps_trained: 21504000
    sample_time_ms: 89696.476
    update_time_ms: 21.216
  iterations_since_restore: 204
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.134078212290502
    ram_util_percent: 9.603351955307264
  pid: 4061
  policy_reward_max:
    agent-0: 180.49999999999943
    agent-1: 180.49999999999943
    agent-2: 180.49999999999943
    agent-3: 180.49999999999943
    agent-4: 180.49999999999943
    agent-5: 180.49999999999943
  policy_reward_mean:
    agent-0: 150.05666666666673
    agent-1: 150.05666666666673
    agent-2: 150.05666666666673
    agent-3: 150.05666666666673
    agent-4: 150.05666666666673
    agent-5: 150.05666666666673
  policy_reward_min:
    agent-0: 79.1666666666668
    agent-1: 79.1666666666668
    agent-2: 79.1666666666668
    agent-3: 79.1666666666668
    agent-4: 79.1666666666668
    agent-5: 79.1666666666668
  sampler_perf:
    mean_env_wait_ms: 23.73238180480126
    mean_inference_ms: 12.28545874419744
    mean_processing_ms: 50.75726705776128
  time_since_restore: 26400.27006649971
  time_this_iter_s: 125.15841388702393
  time_total_s: 29611.33375263214
  timestamp: 1637043958
  timesteps_since_restore: 19584000
  timesteps_this_iter: 96000
  timesteps_total: 21504000
  training_iteration: 224
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    224 |          29611.3 | 21504000 |   900.34 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 25
    apples_agent-0_mean: 1.08
    apples_agent-0_min: 0
    apples_agent-1_max: 98
    apples_agent-1_mean: 29.03
    apples_agent-1_min: 0
    apples_agent-2_max: 68
    apples_agent-2_mean: 8.35
    apples_agent-2_min: 0
    apples_agent-3_max: 98
    apples_agent-3_mean: 60.35
    apples_agent-3_min: 27
    apples_agent-4_max: 92
    apples_agent-4_mean: 1.58
    apples_agent-4_min: 0
    apples_agent-5_max: 174
    apples_agent-5_mean: 100.98
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 539
    cleaning_beam_agent-0_mean: 384.77
    cleaning_beam_agent-0_min: 290
    cleaning_beam_agent-1_max: 372
    cleaning_beam_agent-1_mean: 216.73
    cleaning_beam_agent-1_min: 105
    cleaning_beam_agent-2_max: 461
    cleaning_beam_agent-2_mean: 305.06
    cleaning_beam_agent-2_min: 122
    cleaning_beam_agent-3_max: 140
    cleaning_beam_agent-3_mean: 28.05
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 545
    cleaning_beam_agent-4_mean: 427.23
    cleaning_beam_agent-4_min: 272
    cleaning_beam_agent-5_max: 771
    cleaning_beam_agent-5_mean: 89.94
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-28-02
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1042.999999999972
  episode_reward_mean: 890.9999999999809
  episode_reward_min: 440.0000000000086
  episodes_this_iter: 96
  episodes_total: 21600
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20156.302
    learner:
      agent-0:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9852378368377686
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016598417423665524
        model: {}
        policy_loss: -0.003387176664546132
        total_loss: -0.003184572560712695
        vf_explained_var: 0.007724165916442871
        vf_loss: 19.366241455078125
      agent-1:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1437046527862549
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019617697689682245
        model: {}
        policy_loss: -0.004178918898105621
        total_loss: -0.004246708005666733
        vf_explained_var: 0.015470653772354126
        vf_loss: 19.451305389404297
      agent-2:
        cur_kl_coeff: 8.940697071579962e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 1.122835636138916
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017831112490966916
        model: {}
        policy_loss: -0.0036706665996462107
        total_loss: -0.0037683837581425905
        vf_explained_var: 0.027265682816505432
        vf_loss: 18.784753799438477
      agent-3:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36991190910339355
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009849788621068
        model: {}
        policy_loss: -0.0024549250956624746
        total_loss: -0.0014285848010331392
        vf_explained_var: 0.11936616897583008
        vf_loss: 16.773866653442383
      agent-4:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9018112421035767
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016888707177713513
        model: {}
        policy_loss: -0.003977310843765736
        total_loss: -0.003747174981981516
        vf_explained_var: 0.06083895266056061
        vf_loss: 18.1732120513916
      agent-5:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.844342827796936
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014474143972620368
        model: {}
        policy_loss: -0.0038463138043880463
        total_loss: -0.00361004751175642
        vf_explained_var: 0.10669972002506256
        vf_loss: 17.22309684753418
    load_time_ms: 14488.194
    num_steps_sampled: 21600000
    num_steps_trained: 21600000
    sample_time_ms: 89856.179
    update_time_ms: 20.882
  iterations_since_restore: 205
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.400568181818182
    ram_util_percent: 9.648863636363636
  pid: 4061
  policy_reward_max:
    agent-0: 173.83333333333312
    agent-1: 173.83333333333312
    agent-2: 173.83333333333312
    agent-3: 173.83333333333312
    agent-4: 173.83333333333312
    agent-5: 173.83333333333312
  policy_reward_mean:
    agent-0: 148.50000000000009
    agent-1: 148.50000000000009
    agent-2: 148.50000000000009
    agent-3: 148.50000000000009
    agent-4: 148.50000000000009
    agent-5: 148.50000000000009
  policy_reward_min:
    agent-0: 73.33333333333337
    agent-1: 73.33333333333337
    agent-2: 73.33333333333337
    agent-3: 73.33333333333337
    agent-4: 73.33333333333337
    agent-5: 73.33333333333337
  sampler_perf:
    mean_env_wait_ms: 23.735963479619418
    mean_inference_ms: 12.28489357370361
    mean_processing_ms: 50.755068674675975
  time_since_restore: 26524.03358912468
  time_this_iter_s: 123.76352262496948
  time_total_s: 29735.09727525711
  timestamp: 1637044082
  timesteps_since_restore: 19680000
  timesteps_this_iter: 96000
  timesteps_total: 21600000
  training_iteration: 225
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    225 |          29735.1 | 21600000 |      891 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 50
    apples_agent-0_mean: 2.22
    apples_agent-0_min: 0
    apples_agent-1_max: 90
    apples_agent-1_mean: 31.17
    apples_agent-1_min: 0
    apples_agent-2_max: 76
    apples_agent-2_mean: 10.58
    apples_agent-2_min: 0
    apples_agent-3_max: 104
    apples_agent-3_mean: 62.38
    apples_agent-3_min: 30
    apples_agent-4_max: 40
    apples_agent-4_mean: 0.6
    apples_agent-4_min: 0
    apples_agent-5_max: 204
    apples_agent-5_mean: 109.62
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 509
    cleaning_beam_agent-0_mean: 389.68
    cleaning_beam_agent-0_min: 161
    cleaning_beam_agent-1_max: 342
    cleaning_beam_agent-1_mean: 211.16
    cleaning_beam_agent-1_min: 118
    cleaning_beam_agent-2_max: 535
    cleaning_beam_agent-2_mean: 302.83
    cleaning_beam_agent-2_min: 146
    cleaning_beam_agent-3_max: 99
    cleaning_beam_agent-3_mean: 24.79
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 538
    cleaning_beam_agent-4_mean: 419.74
    cleaning_beam_agent-4_min: 284
    cleaning_beam_agent-5_max: 548
    cleaning_beam_agent-5_mean: 64.4
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.04
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-30-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1060.999999999995
  episode_reward_mean: 888.9999999999824
  episode_reward_min: 418.0000000000024
  episodes_this_iter: 96
  episodes_total: 21696
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20159.076
    learner:
      agent-0:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9847608804702759
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011836753692477942
        model: {}
        policy_loss: -0.0033545363694429398
        total_loss: -0.0032203909941017628
        vf_explained_var: 0.011231347918510437
        vf_loss: 18.673236846923828
      agent-1:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1464653015136719
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012714886106550694
        model: {}
        policy_loss: -0.004025870468467474
        total_loss: -0.004099630750715733
        vf_explained_var: -0.012719184160232544
        vf_loss: 19.44020652770996
      agent-2:
        cur_kl_coeff: 4.470348535789981e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1314640045166016
        entropy_coeff: 0.0017600000137463212
        kl: 0.001753126154653728
        model: {}
        policy_loss: -0.003877196228131652
        total_loss: -0.003918320871889591
        vf_explained_var: -0.03710255026817322
        vf_loss: 19.50252342224121
      agent-3:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39364516735076904
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010825827484950423
        model: {}
        policy_loss: -0.0023927916772663593
        total_loss: -0.0014686437789350748
        vf_explained_var: 0.13384655117988586
        vf_loss: 16.169618606567383
      agent-4:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9246120452880859
        entropy_coeff: 0.0017600000137463212
        kl: 0.0025065322406589985
        model: {}
        policy_loss: -0.004206926561892033
        total_loss: -0.004075816832482815
        vf_explained_var: 0.06710819900035858
        vf_loss: 17.58426284790039
      agent-5:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8386003971099854
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015771596226841211
        model: {}
        policy_loss: -0.004078857600688934
        total_loss: -0.003897462273016572
        vf_explained_var: 0.10833561420440674
        vf_loss: 16.573314666748047
    load_time_ms: 14590.899
    num_steps_sampled: 21696000
    num_steps_trained: 21696000
    sample_time_ms: 89846.846
    update_time_ms: 20.865
  iterations_since_restore: 206
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.885000000000002
    ram_util_percent: 9.597222222222223
  pid: 4061
  policy_reward_max:
    agent-0: 176.83333333333326
    agent-1: 176.83333333333326
    agent-2: 176.83333333333326
    agent-3: 176.83333333333326
    agent-4: 176.83333333333326
    agent-5: 176.83333333333326
  policy_reward_mean:
    agent-0: 148.16666666666674
    agent-1: 148.16666666666674
    agent-2: 148.16666666666674
    agent-3: 148.16666666666674
    agent-4: 148.16666666666674
    agent-5: 148.16666666666674
  policy_reward_min:
    agent-0: 69.6666666666665
    agent-1: 69.6666666666665
    agent-2: 69.6666666666665
    agent-3: 69.6666666666665
    agent-4: 69.6666666666665
    agent-5: 69.6666666666665
  sampler_perf:
    mean_env_wait_ms: 23.738387097893124
    mean_inference_ms: 12.284186554964963
    mean_processing_ms: 50.752252186049425
  time_since_restore: 26649.758197784424
  time_this_iter_s: 125.72460865974426
  time_total_s: 29860.821883916855
  timestamp: 1637044208
  timesteps_since_restore: 19776000
  timesteps_this_iter: 96000
  timesteps_total: 21696000
  training_iteration: 226
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    226 |          29860.8 | 21696000 |      889 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 28
    apples_agent-0_mean: 1.91
    apples_agent-0_min: 0
    apples_agent-1_max: 81
    apples_agent-1_mean: 26.64
    apples_agent-1_min: 0
    apples_agent-2_max: 154
    apples_agent-2_mean: 15.13
    apples_agent-2_min: 0
    apples_agent-3_max: 96
    apples_agent-3_mean: 59.91
    apples_agent-3_min: 25
    apples_agent-4_max: 45
    apples_agent-4_mean: 1.12
    apples_agent-4_min: 0
    apples_agent-5_max: 217
    apples_agent-5_mean: 105.0
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 507
    cleaning_beam_agent-0_mean: 383.81
    cleaning_beam_agent-0_min: 210
    cleaning_beam_agent-1_max: 348
    cleaning_beam_agent-1_mean: 210.98
    cleaning_beam_agent-1_min: 94
    cleaning_beam_agent-2_max: 483
    cleaning_beam_agent-2_mean: 294.93
    cleaning_beam_agent-2_min: 92
    cleaning_beam_agent-3_max: 111
    cleaning_beam_agent-3_mean: 22.79
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 528
    cleaning_beam_agent-4_mean: 396.34
    cleaning_beam_agent-4_min: 216
    cleaning_beam_agent-5_max: 559
    cleaning_beam_agent-5_mean: 73.9
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-32-15
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1064.9999999999911
  episode_reward_mean: 873.9899999999807
  episode_reward_min: 223.9999999999976
  episodes_this_iter: 96
  episodes_total: 21792
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20170.993
    learner:
      agent-0:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9884466528892517
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016334080137312412
        model: {}
        policy_loss: -0.003707548603415489
        total_loss: -0.0034605315886437893
        vf_explained_var: -0.0018796175718307495
        vf_loss: 19.8668212890625
      agent-1:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1377021074295044
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014340386260300875
        model: {}
        policy_loss: -0.004076989367604256
        total_loss: -0.004051612690091133
        vf_explained_var: -0.011359944939613342
        vf_loss: 20.277341842651367
      agent-2:
        cur_kl_coeff: 2.2351742678949904e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1231335401535034
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015199425397440791
        model: {}
        policy_loss: -0.003693429520353675
        total_loss: -0.003795142052695155
        vf_explained_var: 0.054921895265579224
        vf_loss: 18.750030517578125
      agent-3:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3970738351345062
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007127248682081699
        model: {}
        policy_loss: -0.0026858183555305004
        total_loss: -0.001682666945271194
        vf_explained_var: 0.1389361172914505
        vf_loss: 17.020004272460938
      agent-4:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9272876977920532
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015352843329310417
        model: {}
        policy_loss: -0.0038914757315069437
        total_loss: -0.003784134751185775
        vf_explained_var: 0.12584026157855988
        vf_loss: 17.393634796142578
      agent-5:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8402319550514221
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013225356815382838
        model: {}
        policy_loss: -0.003908587619662285
        total_loss: -0.0035846922546625137
        vf_explained_var: 0.08677676320075989
        vf_loss: 18.02704620361328
    load_time_ms: 14561.108
    num_steps_sampled: 21792000
    num_steps_trained: 21792000
    sample_time_ms: 90008.257
    update_time_ms: 21.289
  iterations_since_restore: 207
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.967222222222219
    ram_util_percent: 9.62166666666667
  pid: 4061
  policy_reward_max:
    agent-0: 177.4999999999998
    agent-1: 177.4999999999998
    agent-2: 177.4999999999998
    agent-3: 177.4999999999998
    agent-4: 177.4999999999998
    agent-5: 177.4999999999998
  policy_reward_mean:
    agent-0: 145.6650000000001
    agent-1: 145.6650000000001
    agent-2: 145.6650000000001
    agent-3: 145.6650000000001
    agent-4: 145.6650000000001
    agent-5: 145.6650000000001
  policy_reward_min:
    agent-0: 37.33333333333334
    agent-1: 37.33333333333334
    agent-2: 37.33333333333334
    agent-3: 37.33333333333334
    agent-4: 37.33333333333334
    agent-5: 37.33333333333334
  sampler_perf:
    mean_env_wait_ms: 23.740746816507013
    mean_inference_ms: 12.284145929615462
    mean_processing_ms: 50.752197921106855
  time_since_restore: 26776.286516666412
  time_this_iter_s: 126.52831888198853
  time_total_s: 29987.350202798843
  timestamp: 1637044335
  timesteps_since_restore: 19872000
  timesteps_this_iter: 96000
  timesteps_total: 21792000
  training_iteration: 227
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    227 |          29987.4 | 21792000 |   873.99 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 56
    apples_agent-0_mean: 2.78
    apples_agent-0_min: 0
    apples_agent-1_max: 76
    apples_agent-1_mean: 25.05
    apples_agent-1_min: 0
    apples_agent-2_max: 255
    apples_agent-2_mean: 16.3
    apples_agent-2_min: 0
    apples_agent-3_max: 99
    apples_agent-3_mean: 60.67
    apples_agent-3_min: 29
    apples_agent-4_max: 62
    apples_agent-4_mean: 2.34
    apples_agent-4_min: 0
    apples_agent-5_max: 344
    apples_agent-5_mean: 105.9
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 577
    cleaning_beam_agent-0_mean: 395.54
    cleaning_beam_agent-0_min: 276
    cleaning_beam_agent-1_max: 325
    cleaning_beam_agent-1_mean: 217.02
    cleaning_beam_agent-1_min: 91
    cleaning_beam_agent-2_max: 533
    cleaning_beam_agent-2_mean: 287.56
    cleaning_beam_agent-2_min: 120
    cleaning_beam_agent-3_max: 111
    cleaning_beam_agent-3_mean: 24.67
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 499
    cleaning_beam_agent-4_mean: 395.59
    cleaning_beam_agent-4_min: 211
    cleaning_beam_agent-5_max: 822
    cleaning_beam_agent-5_mean: 91.17
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-34-18
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1085.9999999999889
  episode_reward_mean: 875.1599999999805
  episode_reward_min: 223.9999999999976
  episodes_this_iter: 96
  episodes_total: 21888
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20183.821
    learner:
      agent-0:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9877873063087463
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015730223385617137
        model: {}
        policy_loss: -0.003390499157831073
        total_loss: -0.0032353601418435574
        vf_explained_var: -0.01770995557308197
        vf_loss: 18.93643569946289
      agent-1:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1358590126037598
        entropy_coeff: 0.0017600000137463212
        kl: 0.001704563619568944
        model: {}
        policy_loss: -0.004217464942485094
        total_loss: -0.004260757472366095
        vf_explained_var: -0.04449322819709778
        vf_loss: 19.558208465576172
      agent-2:
        cur_kl_coeff: 1.1175871339474952e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1213663816452026
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015144140925258398
        model: {}
        policy_loss: -0.0035633938387036324
        total_loss: -0.0037278197705745697
        vf_explained_var: 0.02967086434364319
        vf_loss: 18.091814041137695
      agent-3:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38925811648368835
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008513524662703276
        model: {}
        policy_loss: -0.0022212592884898186
        total_loss: -0.00123619195073843
        vf_explained_var: 0.095662921667099
        vf_loss: 16.701627731323242
      agent-4:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9186782240867615
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014908042503520846
        model: {}
        policy_loss: -0.0038232551887631416
        total_loss: -0.003728068433701992
        vf_explained_var: 0.07822905480861664
        vf_loss: 17.120561599731445
      agent-5:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8428594470024109
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012466320767998695
        model: {}
        policy_loss: -0.0039596110582351685
        total_loss: -0.0036699087359011173
        vf_explained_var: 0.04566475749015808
        vf_loss: 17.731365203857422
    load_time_ms: 14330.113
    num_steps_sampled: 21888000
    num_steps_trained: 21888000
    sample_time_ms: 89986.775
    update_time_ms: 21.485
  iterations_since_restore: 208
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.426857142857145
    ram_util_percent: 9.647428571428572
  pid: 4061
  policy_reward_max:
    agent-0: 180.99999999999952
    agent-1: 180.99999999999952
    agent-2: 180.99999999999952
    agent-3: 180.99999999999952
    agent-4: 180.99999999999952
    agent-5: 180.99999999999952
  policy_reward_mean:
    agent-0: 145.86000000000007
    agent-1: 145.86000000000007
    agent-2: 145.86000000000007
    agent-3: 145.86000000000007
    agent-4: 145.86000000000007
    agent-5: 145.86000000000007
  policy_reward_min:
    agent-0: 37.33333333333334
    agent-1: 37.33333333333334
    agent-2: 37.33333333333334
    agent-3: 37.33333333333334
    agent-4: 37.33333333333334
    agent-5: 37.33333333333334
  sampler_perf:
    mean_env_wait_ms: 23.742637110746635
    mean_inference_ms: 12.283362135003683
    mean_processing_ms: 50.75095050224428
  time_since_restore: 26899.091380119324
  time_this_iter_s: 122.80486345291138
  time_total_s: 30110.155066251755
  timestamp: 1637044458
  timesteps_since_restore: 19968000
  timesteps_this_iter: 96000
  timesteps_total: 21888000
  training_iteration: 228
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    228 |          30110.2 | 21888000 |   875.16 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 39
    apples_agent-0_mean: 0.93
    apples_agent-0_min: 0
    apples_agent-1_max: 82
    apples_agent-1_mean: 26.01
    apples_agent-1_min: 0
    apples_agent-2_max: 79
    apples_agent-2_mean: 11.73
    apples_agent-2_min: 0
    apples_agent-3_max: 117
    apples_agent-3_mean: 57.99
    apples_agent-3_min: 23
    apples_agent-4_max: 114
    apples_agent-4_mean: 2.28
    apples_agent-4_min: 0
    apples_agent-5_max: 198
    apples_agent-5_mean: 110.39
    apples_agent-5_min: 24
    cleaning_beam_agent-0_max: 515
    cleaning_beam_agent-0_mean: 407.6
    cleaning_beam_agent-0_min: 196
    cleaning_beam_agent-1_max: 361
    cleaning_beam_agent-1_mean: 206.64
    cleaning_beam_agent-1_min: 98
    cleaning_beam_agent-2_max: 490
    cleaning_beam_agent-2_mean: 284.59
    cleaning_beam_agent-2_min: 101
    cleaning_beam_agent-3_max: 116
    cleaning_beam_agent-3_mean: 20.11
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 513
    cleaning_beam_agent-4_mean: 412.95
    cleaning_beam_agent-4_min: 170
    cleaning_beam_agent-5_max: 496
    cleaning_beam_agent-5_mean: 76.22
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-36-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1056.9999999999877
  episode_reward_mean: 894.6599999999821
  episode_reward_min: 421.00000000000847
  episodes_this_iter: 96
  episodes_total: 21984
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20206.045
    learner:
      agent-0:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9670165777206421
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015990398824214935
        model: {}
        policy_loss: -0.003389115445315838
        total_loss: -0.00327883567661047
        vf_explained_var: 0.021966755390167236
        vf_loss: 18.122257232666016
      agent-1:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1356914043426514
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014124347362667322
        model: {}
        policy_loss: -0.00400295527651906
        total_loss: -0.004095530137419701
        vf_explained_var: -0.008880198001861572
        vf_loss: 19.06243133544922
      agent-2:
        cur_kl_coeff: 5.587935669737476e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1497763395309448
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017898064106702805
        model: {}
        policy_loss: -0.0037949932739138603
        total_loss: -0.004041865933686495
        vf_explained_var: 0.054494455456733704
        vf_loss: 17.767333984375
      agent-3:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.369242399930954
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008719677571207285
        model: {}
        policy_loss: -0.002271442674100399
        total_loss: -0.0012687169946730137
        vf_explained_var: 0.09976130723953247
        vf_loss: 16.525903701782227
      agent-4:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9218428134918213
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018707404378801584
        model: {}
        policy_loss: -0.003935938701033592
        total_loss: -0.003848996013402939
        vf_explained_var: 0.08259893953800201
        vf_loss: 17.0938720703125
      agent-5:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8468825817108154
        entropy_coeff: 0.0017600000137463212
        kl: 0.001105557195842266
        model: {}
        policy_loss: -0.0038640627171844244
        total_loss: -0.0036054884549230337
        vf_explained_var: 0.05304831266403198
        vf_loss: 17.490882873535156
    load_time_ms: 14550.402
    num_steps_sampled: 21984000
    num_steps_trained: 21984000
    sample_time_ms: 89730.52
    update_time_ms: 21.393
  iterations_since_restore: 209
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.315168539325844
    ram_util_percent: 9.685955056179775
  pid: 4061
  policy_reward_max:
    agent-0: 176.16666666666654
    agent-1: 176.16666666666654
    agent-2: 176.16666666666654
    agent-3: 176.16666666666654
    agent-4: 176.16666666666654
    agent-5: 176.16666666666654
  policy_reward_mean:
    agent-0: 149.11
    agent-1: 149.11
    agent-2: 149.11
    agent-3: 149.11
    agent-4: 149.11
    agent-5: 149.11
  policy_reward_min:
    agent-0: 70.16666666666653
    agent-1: 70.16666666666653
    agent-2: 70.16666666666653
    agent-3: 70.16666666666653
    agent-4: 70.16666666666653
    agent-5: 70.16666666666653
  sampler_perf:
    mean_env_wait_ms: 23.744204652333497
    mean_inference_ms: 12.28246059808735
    mean_processing_ms: 50.74823369407376
  time_since_restore: 27023.925080776215
  time_this_iter_s: 124.83370065689087
  time_total_s: 30234.988766908646
  timestamp: 1637044583
  timesteps_since_restore: 20064000
  timesteps_this_iter: 96000
  timesteps_total: 21984000
  training_iteration: 229
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    229 |            30235 | 21984000 |   894.66 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 49
    apples_agent-0_mean: 1.83
    apples_agent-0_min: 0
    apples_agent-1_max: 87
    apples_agent-1_mean: 26.43
    apples_agent-1_min: 0
    apples_agent-2_max: 124
    apples_agent-2_mean: 12.98
    apples_agent-2_min: 0
    apples_agent-3_max: 101
    apples_agent-3_mean: 61.63
    apples_agent-3_min: 31
    apples_agent-4_max: 114
    apples_agent-4_mean: 2.78
    apples_agent-4_min: 0
    apples_agent-5_max: 189
    apples_agent-5_mean: 103.26
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 495
    cleaning_beam_agent-0_mean: 396.51
    cleaning_beam_agent-0_min: 248
    cleaning_beam_agent-1_max: 342
    cleaning_beam_agent-1_mean: 223.35
    cleaning_beam_agent-1_min: 110
    cleaning_beam_agent-2_max: 582
    cleaning_beam_agent-2_mean: 270.65
    cleaning_beam_agent-2_min: 96
    cleaning_beam_agent-3_max: 89
    cleaning_beam_agent-3_mean: 23.86
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 530
    cleaning_beam_agent-4_mean: 402.72
    cleaning_beam_agent-4_min: 244
    cleaning_beam_agent-5_max: 850
    cleaning_beam_agent-5_mean: 84.45
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 11
    fire_beam_agent-2_mean: 0.11
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-38-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1056.9999999999784
  episode_reward_mean: 880.4199999999821
  episode_reward_min: 310.00000000000006
  episodes_this_iter: 96
  episodes_total: 22080
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20195.506
    learner:
      agent-0:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9810354709625244
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016181865939870477
        model: {}
        policy_loss: -0.003622855991125107
        total_loss: -0.0034807645715773106
        vf_explained_var: -0.008843347430229187
        vf_loss: 18.687143325805664
      agent-1:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1285400390625
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014412157470360398
        model: {}
        policy_loss: -0.003887067548930645
        total_loss: -0.003963797353208065
        vf_explained_var: -0.02351340651512146
        vf_loss: 19.095014572143555
      agent-2:
        cur_kl_coeff: 2.793967834868738e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1521997451782227
        entropy_coeff: 0.0017600000137463212
        kl: 0.001937922090291977
        model: {}
        policy_loss: -0.0038220006972551346
        total_loss: -0.004036161117255688
        vf_explained_var: 0.02611374855041504
        vf_loss: 18.13711929321289
      agent-3:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3789302706718445
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009770351462066174
        model: {}
        policy_loss: -0.002189390128478408
        total_loss: -0.00120230158790946
        vf_explained_var: 0.10143116116523743
        vf_loss: 16.540050506591797
      agent-4:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.930598795413971
        entropy_coeff: 0.0017600000137463212
        kl: 0.00199555023573339
        model: {}
        policy_loss: -0.00420241616666317
        total_loss: -0.00414169579744339
        vf_explained_var: 0.08736799657344818
        vf_loss: 16.98574447631836
      agent-5:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8366008400917053
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018590422114357352
        model: {}
        policy_loss: -0.0040891822427511215
        total_loss: -0.0038384536746889353
        vf_explained_var: 0.0660993754863739
        vf_loss: 17.231456756591797
    load_time_ms: 14563.344
    num_steps_sampled: 22080000
    num_steps_trained: 22080000
    sample_time_ms: 89697.541
    update_time_ms: 21.202
  iterations_since_restore: 210
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.368571428571428
    ram_util_percent: 9.569714285714287
  pid: 4061
  policy_reward_max:
    agent-0: 176.16666666666555
    agent-1: 176.16666666666555
    agent-2: 176.16666666666555
    agent-3: 176.16666666666555
    agent-4: 176.16666666666555
    agent-5: 176.16666666666555
  policy_reward_mean:
    agent-0: 146.73666666666674
    agent-1: 146.73666666666674
    agent-2: 146.73666666666674
    agent-3: 146.73666666666674
    agent-4: 146.73666666666674
    agent-5: 146.73666666666674
  policy_reward_min:
    agent-0: 51.66666666666654
    agent-1: 51.66666666666654
    agent-2: 51.66666666666654
    agent-3: 51.66666666666654
    agent-4: 51.66666666666654
    agent-5: 51.66666666666654
  sampler_perf:
    mean_env_wait_ms: 23.746118538963323
    mean_inference_ms: 12.281905122180545
    mean_processing_ms: 50.74547084898528
  time_since_restore: 27146.898529291153
  time_this_iter_s: 122.97344851493835
  time_total_s: 30357.962215423584
  timestamp: 1637044706
  timesteps_since_restore: 20160000
  timesteps_this_iter: 96000
  timesteps_total: 22080000
  training_iteration: 230
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    230 |            30358 | 22080000 |   880.42 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 32
    apples_agent-0_mean: 1.97
    apples_agent-0_min: 0
    apples_agent-1_max: 115
    apples_agent-1_mean: 27.98
    apples_agent-1_min: 0
    apples_agent-2_max: 90
    apples_agent-2_mean: 10.77
    apples_agent-2_min: 0
    apples_agent-3_max: 136
    apples_agent-3_mean: 60.88
    apples_agent-3_min: 33
    apples_agent-4_max: 66
    apples_agent-4_mean: 2.73
    apples_agent-4_min: 0
    apples_agent-5_max: 280
    apples_agent-5_mean: 110.89
    apples_agent-5_min: 34
    cleaning_beam_agent-0_max: 517
    cleaning_beam_agent-0_mean: 405.89
    cleaning_beam_agent-0_min: 247
    cleaning_beam_agent-1_max: 316
    cleaning_beam_agent-1_mean: 224.18
    cleaning_beam_agent-1_min: 113
    cleaning_beam_agent-2_max: 496
    cleaning_beam_agent-2_mean: 285.82
    cleaning_beam_agent-2_min: 117
    cleaning_beam_agent-3_max: 160
    cleaning_beam_agent-3_mean: 23.37
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 555
    cleaning_beam_agent-4_mean: 419.91
    cleaning_beam_agent-4_min: 205
    cleaning_beam_agent-5_max: 438
    cleaning_beam_agent-5_mean: 68.15
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-40-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1052.9999999999748
  episode_reward_mean: 894.1499999999824
  episode_reward_min: 233.99999999999716
  episodes_this_iter: 96
  episodes_total: 22176
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20194.802
    learner:
      agent-0:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9678562879562378
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015418048715218902
        model: {}
        policy_loss: -0.00337317306548357
        total_loss: -0.003123226575553417
        vf_explained_var: 0.044817209243774414
        vf_loss: 19.53371810913086
      agent-1:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 1.147652506828308
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010840650647878647
        model: {}
        policy_loss: -0.0039044232107698917
        total_loss: -0.0038375798612833023
        vf_explained_var: -0.01770108938217163
        vf_loss: 20.867067337036133
      agent-2:
        cur_kl_coeff: 1.396983917434369e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1335246562957764
        entropy_coeff: 0.0017600000137463212
        kl: 0.001917341724038124
        model: {}
        policy_loss: -0.0037515955045819283
        total_loss: -0.0036898162215948105
        vf_explained_var: -0.01152867078781128
        vf_loss: 20.56785011291504
      agent-3:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3825101852416992
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006909951334819198
        model: {}
        policy_loss: -0.0020366888493299484
        total_loss: -0.000933393370360136
        vf_explained_var: 0.12251284718513489
        vf_loss: 17.765127182006836
      agent-4:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.930292010307312
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022170809097588062
        model: {}
        policy_loss: -0.004058223683387041
        total_loss: -0.003901425749063492
        vf_explained_var: 0.12321093678474426
        vf_loss: 17.941123962402344
      agent-5:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.826776385307312
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020719915628433228
        model: {}
        policy_loss: -0.004116017837077379
        total_loss: -0.0038146397564560175
        vf_explained_var: 0.12693092226982117
        vf_loss: 17.565013885498047
    load_time_ms: 15030.498
    num_steps_sampled: 22176000
    num_steps_trained: 22176000
    sample_time_ms: 89625.607
    update_time_ms: 21.479
  iterations_since_restore: 211
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.583870967741937
    ram_util_percent: 9.670967741935485
  pid: 4061
  policy_reward_max:
    agent-0: 175.49999999999977
    agent-1: 175.49999999999977
    agent-2: 175.49999999999977
    agent-3: 175.49999999999977
    agent-4: 175.49999999999977
    agent-5: 175.49999999999977
  policy_reward_mean:
    agent-0: 149.02500000000003
    agent-1: 149.02500000000003
    agent-2: 149.02500000000003
    agent-3: 149.02500000000003
    agent-4: 149.02500000000003
    agent-5: 149.02500000000003
  policy_reward_min:
    agent-0: 38.999999999999986
    agent-1: 38.999999999999986
    agent-2: 38.999999999999986
    agent-3: 38.999999999999986
    agent-4: 38.999999999999986
    agent-5: 38.999999999999986
  sampler_perf:
    mean_env_wait_ms: 23.749173032911667
    mean_inference_ms: 12.281267673591524
    mean_processing_ms: 50.74406790729179
  time_since_restore: 27277.88404583931
  time_this_iter_s: 130.98551654815674
  time_total_s: 30488.94773197174
  timestamp: 1637044837
  timesteps_since_restore: 20256000
  timesteps_this_iter: 96000
  timesteps_total: 22176000
  training_iteration: 231
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    231 |          30488.9 | 22176000 |   894.15 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 25
    apples_agent-0_mean: 2.15
    apples_agent-0_min: 0
    apples_agent-1_max: 91
    apples_agent-1_mean: 28.22
    apples_agent-1_min: 0
    apples_agent-2_max: 187
    apples_agent-2_mean: 14.03
    apples_agent-2_min: 0
    apples_agent-3_max: 136
    apples_agent-3_mean: 59.37
    apples_agent-3_min: 31
    apples_agent-4_max: 70
    apples_agent-4_mean: 1.61
    apples_agent-4_min: 0
    apples_agent-5_max: 195
    apples_agent-5_mean: 105.09
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 522
    cleaning_beam_agent-0_mean: 402.92
    cleaning_beam_agent-0_min: 282
    cleaning_beam_agent-1_max: 369
    cleaning_beam_agent-1_mean: 202.69
    cleaning_beam_agent-1_min: 121
    cleaning_beam_agent-2_max: 543
    cleaning_beam_agent-2_mean: 274.22
    cleaning_beam_agent-2_min: 117
    cleaning_beam_agent-3_max: 111
    cleaning_beam_agent-3_mean: 23.68
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 523
    cleaning_beam_agent-4_mean: 414.36
    cleaning_beam_agent-4_min: 208
    cleaning_beam_agent-5_max: 596
    cleaning_beam_agent-5_mean: 88.49
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-42-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1066.999999999982
  episode_reward_mean: 882.579999999983
  episode_reward_min: 317.99999999999903
  episodes_this_iter: 96
  episodes_total: 22272
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20204.527
    learner:
      agent-0:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9707108736038208
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014589255442842841
        model: {}
        policy_loss: -0.003358950838446617
        total_loss: -0.003092053346335888
        vf_explained_var: 0.03058101236820221
        vf_loss: 19.75348663330078
      agent-1:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1345306634902954
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017197596607729793
        model: {}
        policy_loss: -0.004112901631742716
        total_loss: -0.003969544544816017
        vf_explained_var: -0.04690808057785034
        vf_loss: 21.401321411132812
      agent-2:
        cur_kl_coeff: 6.984919587171845e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 1.131689190864563
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013359838631004095
        model: {}
        policy_loss: -0.0035361764021217823
        total_loss: -0.00350275170058012
        vf_explained_var: 0.006357133388519287
        vf_loss: 20.251972198486328
      agent-3:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38985636830329895
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006891743978485465
        model: {}
        policy_loss: -0.002364750951528549
        total_loss: -0.001316821319051087
        vf_explained_var: 0.14526107907295227
        vf_loss: 17.34075355529785
      agent-4:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9202277660369873
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017391052097082138
        model: {}
        policy_loss: -0.003661312162876129
        total_loss: -0.003433462232351303
        vf_explained_var: 0.09289909899234772
        vf_loss: 18.474510192871094
      agent-5:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8219192028045654
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008581756264902651
        model: {}
        policy_loss: -0.003768861759454012
        total_loss: -0.0034177578054368496
        vf_explained_var: 0.11483390629291534
        vf_loss: 17.97681999206543
    load_time_ms: 15101.582
    num_steps_sampled: 22272000
    num_steps_trained: 22272000
    sample_time_ms: 89543.305
    update_time_ms: 21.727
  iterations_since_restore: 212
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.454285714285714
    ram_util_percent: 9.654857142857145
  pid: 4061
  policy_reward_max:
    agent-0: 177.83333333333297
    agent-1: 177.83333333333297
    agent-2: 177.83333333333297
    agent-3: 177.83333333333297
    agent-4: 177.83333333333297
    agent-5: 177.83333333333297
  policy_reward_mean:
    agent-0: 147.09666666666678
    agent-1: 147.09666666666678
    agent-2: 147.09666666666678
    agent-3: 147.09666666666678
    agent-4: 147.09666666666678
    agent-5: 147.09666666666678
  policy_reward_min:
    agent-0: 52.9999999999999
    agent-1: 52.9999999999999
    agent-2: 52.9999999999999
    agent-3: 52.9999999999999
    agent-4: 52.9999999999999
    agent-5: 52.9999999999999
  sampler_perf:
    mean_env_wait_ms: 23.751528430409554
    mean_inference_ms: 12.2805627889037
    mean_processing_ms: 50.740994823400214
  time_since_restore: 27400.795214176178
  time_this_iter_s: 122.91116833686829
  time_total_s: 30611.85890030861
  timestamp: 1637044960
  timesteps_since_restore: 20352000
  timesteps_this_iter: 96000
  timesteps_total: 22272000
  training_iteration: 232
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    232 |          30611.9 | 22272000 |   882.58 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 58
    apples_agent-0_mean: 3.35
    apples_agent-0_min: 0
    apples_agent-1_max: 91
    apples_agent-1_mean: 27.17
    apples_agent-1_min: 0
    apples_agent-2_max: 69
    apples_agent-2_mean: 7.2
    apples_agent-2_min: 0
    apples_agent-3_max: 98
    apples_agent-3_mean: 58.24
    apples_agent-3_min: 20
    apples_agent-4_max: 28
    apples_agent-4_mean: 0.8
    apples_agent-4_min: 0
    apples_agent-5_max: 162
    apples_agent-5_mean: 102.18
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 514
    cleaning_beam_agent-0_mean: 400.25
    cleaning_beam_agent-0_min: 281
    cleaning_beam_agent-1_max: 347
    cleaning_beam_agent-1_mean: 204.42
    cleaning_beam_agent-1_min: 76
    cleaning_beam_agent-2_max: 497
    cleaning_beam_agent-2_mean: 292.37
    cleaning_beam_agent-2_min: 127
    cleaning_beam_agent-3_max: 84
    cleaning_beam_agent-3_mean: 20.41
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 556
    cleaning_beam_agent-4_mean: 432.74
    cleaning_beam_agent-4_min: 343
    cleaning_beam_agent-5_max: 731
    cleaning_beam_agent-5_mean: 90.14
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-44-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1072.9999999999864
  episode_reward_mean: 910.539999999982
  episode_reward_min: 532.0000000000044
  episodes_this_iter: 96
  episodes_total: 22368
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20212.899
    learner:
      agent-0:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9462960958480835
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009025176987051964
        model: {}
        policy_loss: -0.0032068388536572456
        total_loss: -0.0029297969304025173
        vf_explained_var: -0.035314977169036865
        vf_loss: 19.425214767456055
      agent-1:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1317031383514404
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014225177001208067
        model: {}
        policy_loss: -0.004194634035229683
        total_loss: -0.004258650820702314
        vf_explained_var: -0.016979828476905823
        vf_loss: 19.27780532836914
      agent-2:
        cur_kl_coeff: 3.4924597935859225e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1278650760650635
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014653651742264628
        model: {}
        policy_loss: -0.0038151685148477554
        total_loss: -0.003909995313733816
        vf_explained_var: -0.02308756113052368
        vf_loss: 18.902183532714844
      agent-3:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35916778445243835
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009988588280975819
        model: {}
        policy_loss: -0.0018718973733484745
        total_loss: -0.0008225394412875175
        vf_explained_var: 0.09182910621166229
        vf_loss: 16.814970016479492
      agent-4:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9089974761009216
        entropy_coeff: 0.0017600000137463212
        kl: 0.0029444023966789246
        model: {}
        policy_loss: -0.004050048533827066
        total_loss: -0.0038951504975557327
        vf_explained_var: 0.05465410649776459
        vf_loss: 17.547311782836914
      agent-5:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7901173830032349
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015516441781073809
        model: {}
        policy_loss: -0.0034800630528479815
        total_loss: -0.003114418825134635
        vf_explained_var: 0.051239609718322754
        vf_loss: 17.562515258789062
    load_time_ms: 14919.045
    num_steps_sampled: 22368000
    num_steps_trained: 22368000
    sample_time_ms: 89614.749
    update_time_ms: 21.966
  iterations_since_restore: 213
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.399428571428569
    ram_util_percent: 9.564571428571432
  pid: 4061
  policy_reward_max:
    agent-0: 178.8333333333332
    agent-1: 178.8333333333332
    agent-2: 178.8333333333332
    agent-3: 178.8333333333332
    agent-4: 178.8333333333332
    agent-5: 178.8333333333332
  policy_reward_mean:
    agent-0: 151.75666666666666
    agent-1: 151.75666666666666
    agent-2: 151.75666666666666
    agent-3: 151.75666666666666
    agent-4: 151.75666666666666
    agent-5: 151.75666666666666
  policy_reward_min:
    agent-0: 88.66666666666664
    agent-1: 88.66666666666664
    agent-2: 88.66666666666664
    agent-3: 88.66666666666664
    agent-4: 88.66666666666664
    agent-5: 88.66666666666664
  sampler_perf:
    mean_env_wait_ms: 23.75401323990899
    mean_inference_ms: 12.27975584510263
    mean_processing_ms: 50.73788672584037
  time_since_restore: 27523.849044561386
  time_this_iter_s: 123.05383038520813
  time_total_s: 30734.912730693817
  timestamp: 1637045083
  timesteps_since_restore: 20448000
  timesteps_this_iter: 96000
  timesteps_total: 22368000
  training_iteration: 233
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    233 |          30734.9 | 22368000 |   910.54 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 27
    apples_agent-0_mean: 1.51
    apples_agent-0_min: 0
    apples_agent-1_max: 104
    apples_agent-1_mean: 26.3
    apples_agent-1_min: 0
    apples_agent-2_max: 166
    apples_agent-2_mean: 13.87
    apples_agent-2_min: 0
    apples_agent-3_max: 103
    apples_agent-3_mean: 58.01
    apples_agent-3_min: 35
    apples_agent-4_max: 64
    apples_agent-4_mean: 2.13
    apples_agent-4_min: 0
    apples_agent-5_max: 165
    apples_agent-5_mean: 97.87
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 539
    cleaning_beam_agent-0_mean: 407.5
    cleaning_beam_agent-0_min: 258
    cleaning_beam_agent-1_max: 358
    cleaning_beam_agent-1_mean: 215.02
    cleaning_beam_agent-1_min: 106
    cleaning_beam_agent-2_max: 471
    cleaning_beam_agent-2_mean: 274.29
    cleaning_beam_agent-2_min: 110
    cleaning_beam_agent-3_max: 66
    cleaning_beam_agent-3_mean: 19.85
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 583
    cleaning_beam_agent-4_mean: 440.93
    cleaning_beam_agent-4_min: 321
    cleaning_beam_agent-5_max: 742
    cleaning_beam_agent-5_mean: 102.68
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-46-48
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1092.9999999999886
  episode_reward_mean: 892.4199999999828
  episode_reward_min: 317.99999999999955
  episodes_this_iter: 96
  episodes_total: 22464
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20231.411
    learner:
      agent-0:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9471681118011475
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017515912186354399
        model: {}
        policy_loss: -0.0035036697518080473
        total_loss: -0.0032260334119200706
        vf_explained_var: 0.019956722855567932
        vf_loss: 19.446491241455078
      agent-1:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1182522773742676
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011487668380141258
        model: {}
        policy_loss: -0.00408858573064208
        total_loss: -0.004063743632286787
        vf_explained_var: 0.0011635571718215942
        vf_loss: 19.92966079711914
      agent-2:
        cur_kl_coeff: 1.7462298967929613e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1538265943527222
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018674425082281232
        model: {}
        policy_loss: -0.0037717907689511776
        total_loss: -0.003822804894298315
        vf_explained_var: 0.0032014697790145874
        vf_loss: 19.797203063964844
      agent-3:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38337600231170654
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012627153191715479
        model: {}
        policy_loss: -0.0026367458049207926
        total_loss: -0.0016558263450860977
        vf_explained_var: 0.16456717252731323
        vf_loss: 16.55661392211914
      agent-4:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.919373631477356
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014218674041330814
        model: {}
        policy_loss: -0.003768663387745619
        total_loss: -0.003603483084589243
        vf_explained_var: 0.10013765096664429
        vf_loss: 17.832792282104492
      agent-5:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8115012645721436
        entropy_coeff: 0.0017600000137463212
        kl: 0.001542951911687851
        model: {}
        policy_loss: -0.003916807472705841
        total_loss: -0.003492106683552265
        vf_explained_var: 0.07106968760490417
        vf_loss: 18.529403686523438
    load_time_ms: 14945.853
    num_steps_sampled: 22464000
    num_steps_trained: 22464000
    sample_time_ms: 89492.732
    update_time_ms: 21.9
  iterations_since_restore: 214
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.337640449438204
    ram_util_percent: 9.614044943820225
  pid: 4061
  policy_reward_max:
    agent-0: 182.16666666666643
    agent-1: 182.16666666666643
    agent-2: 182.16666666666643
    agent-3: 182.16666666666643
    agent-4: 182.16666666666643
    agent-5: 182.16666666666643
  policy_reward_mean:
    agent-0: 148.73666666666668
    agent-1: 148.73666666666668
    agent-2: 148.73666666666668
    agent-3: 148.73666666666668
    agent-4: 148.73666666666668
    agent-5: 148.73666666666668
  policy_reward_min:
    agent-0: 52.999999999999865
    agent-1: 52.999999999999865
    agent-2: 52.999999999999865
    agent-3: 52.999999999999865
    agent-4: 52.999999999999865
    agent-5: 52.999999999999865
  sampler_perf:
    mean_env_wait_ms: 23.75666497608646
    mean_inference_ms: 12.278893256309157
    mean_processing_ms: 50.733743769064965
  time_since_restore: 27648.250574350357
  time_this_iter_s: 124.40152978897095
  time_total_s: 30859.314260482788
  timestamp: 1637045208
  timesteps_since_restore: 20544000
  timesteps_this_iter: 96000
  timesteps_total: 22464000
  training_iteration: 234
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    234 |          30859.3 | 22464000 |   892.42 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 0.79
    apples_agent-0_min: 0
    apples_agent-1_max: 96
    apples_agent-1_mean: 24.85
    apples_agent-1_min: 0
    apples_agent-2_max: 136
    apples_agent-2_mean: 15.45
    apples_agent-2_min: 0
    apples_agent-3_max: 105
    apples_agent-3_mean: 58.62
    apples_agent-3_min: 28
    apples_agent-4_max: 90
    apples_agent-4_mean: 2.65
    apples_agent-4_min: 0
    apples_agent-5_max: 241
    apples_agent-5_mean: 107.18
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 549
    cleaning_beam_agent-0_mean: 418.79
    cleaning_beam_agent-0_min: 302
    cleaning_beam_agent-1_max: 367
    cleaning_beam_agent-1_mean: 213.87
    cleaning_beam_agent-1_min: 104
    cleaning_beam_agent-2_max: 424
    cleaning_beam_agent-2_mean: 249.59
    cleaning_beam_agent-2_min: 116
    cleaning_beam_agent-3_max: 75
    cleaning_beam_agent-3_mean: 22.04
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 532
    cleaning_beam_agent-4_mean: 439.28
    cleaning_beam_agent-4_min: 212
    cleaning_beam_agent-5_max: 591
    cleaning_beam_agent-5_mean: 80.22
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 3
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-48-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1086.0
  episode_reward_mean: 897.9699999999833
  episode_reward_min: 294.9999999999967
  episodes_this_iter: 96
  episodes_total: 22560
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20219.918
    learner:
      agent-0:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.936050534248352
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018642217619344592
        model: {}
        policy_loss: -0.003315670182928443
        total_loss: -0.0028690528124570847
        vf_explained_var: -0.027430862188339233
        vf_loss: 20.94066047668457
      agent-1:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 1.134497880935669
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014615771360695362
        model: {}
        policy_loss: -0.0037366882897913456
        total_loss: -0.0036617862060666084
        vf_explained_var: -0.00780525803565979
        vf_loss: 20.71619415283203
      agent-2:
        cur_kl_coeff: 8.731149483964806e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1478397846221924
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010641339467838407
        model: {}
        policy_loss: -0.0037182981614023447
        total_loss: -0.0037332470528781414
        vf_explained_var: 0.022370561957359314
        vf_loss: 20.05249786376953
      agent-3:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3762180209159851
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007307045161724091
        model: {}
        policy_loss: -0.002409924054518342
        total_loss: -0.001335710403509438
        vf_explained_var: 0.14878802001476288
        vf_loss: 17.363574981689453
      agent-4:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9071574807167053
        entropy_coeff: 0.0017600000137463212
        kl: 0.001987319439649582
        model: {}
        policy_loss: -0.003899824805557728
        total_loss: -0.003696969710290432
        vf_explained_var: 0.11833678185939789
        vf_loss: 17.994516372680664
      agent-5:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8014075756072998
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008696127333678305
        model: {}
        policy_loss: -0.003341089468449354
        total_loss: -0.0029046733397990465
        vf_explained_var: 0.09265634417533875
        vf_loss: 18.46895980834961
    load_time_ms: 15076.63
    num_steps_sampled: 22560000
    num_steps_trained: 22560000
    sample_time_ms: 89315.337
    update_time_ms: 21.839
  iterations_since_restore: 215
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.41942857142857
    ram_util_percent: 9.66057142857143
  pid: 4061
  policy_reward_max:
    agent-0: 180.9999999999999
    agent-1: 180.9999999999999
    agent-2: 180.9999999999999
    agent-3: 180.9999999999999
    agent-4: 180.9999999999999
    agent-5: 180.9999999999999
  policy_reward_mean:
    agent-0: 149.66166666666675
    agent-1: 149.66166666666675
    agent-2: 149.66166666666675
    agent-3: 149.66166666666675
    agent-4: 149.66166666666675
    agent-5: 149.66166666666675
  policy_reward_min:
    agent-0: 49.16666666666658
    agent-1: 49.16666666666658
    agent-2: 49.16666666666658
    agent-3: 49.16666666666658
    agent-4: 49.16666666666658
    agent-5: 49.16666666666658
  sampler_perf:
    mean_env_wait_ms: 23.75862994229559
    mean_inference_ms: 12.277974594661893
    mean_processing_ms: 50.72974529760195
  time_since_restore: 27771.426869392395
  time_this_iter_s: 123.17629504203796
  time_total_s: 30982.490555524826
  timestamp: 1637045331
  timesteps_since_restore: 20640000
  timesteps_this_iter: 96000
  timesteps_total: 22560000
  training_iteration: 235
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    235 |          30982.5 | 22560000 |   897.97 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 0.85
    apples_agent-0_min: 0
    apples_agent-1_max: 118
    apples_agent-1_mean: 31.15
    apples_agent-1_min: 0
    apples_agent-2_max: 98
    apples_agent-2_mean: 12.15
    apples_agent-2_min: 0
    apples_agent-3_max: 114
    apples_agent-3_mean: 58.89
    apples_agent-3_min: 26
    apples_agent-4_max: 76
    apples_agent-4_mean: 1.61
    apples_agent-4_min: 0
    apples_agent-5_max: 217
    apples_agent-5_mean: 105.15
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 586
    cleaning_beam_agent-0_mean: 407.85
    cleaning_beam_agent-0_min: 290
    cleaning_beam_agent-1_max: 337
    cleaning_beam_agent-1_mean: 201.21
    cleaning_beam_agent-1_min: 75
    cleaning_beam_agent-2_max: 498
    cleaning_beam_agent-2_mean: 260.16
    cleaning_beam_agent-2_min: 127
    cleaning_beam_agent-3_max: 66
    cleaning_beam_agent-3_mean: 18.6
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 579
    cleaning_beam_agent-4_mean: 450.25
    cleaning_beam_agent-4_min: 270
    cleaning_beam_agent-5_max: 543
    cleaning_beam_agent-5_mean: 61.82
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-51-00
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1098.9999999999777
  episode_reward_mean: 910.8899999999812
  episode_reward_min: 356.00000000000193
  episodes_this_iter: 96
  episodes_total: 22656
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20236.206
    learner:
      agent-0:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9467587471008301
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016289155464619398
        model: {}
        policy_loss: -0.00344639434479177
        total_loss: -0.0031632939353585243
        vf_explained_var: -0.0019116401672363281
        vf_loss: 19.49393081665039
      agent-1:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 1.132135033607483
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019110654247924685
        model: {}
        policy_loss: -0.00435637217015028
        total_loss: -0.004327284637838602
        vf_explained_var: -0.027009427547454834
        vf_loss: 20.216459274291992
      agent-2:
        cur_kl_coeff: 4.365574741982403e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1546506881713867
        entropy_coeff: 0.0017600000137463212
        kl: 0.001487069413997233
        model: {}
        policy_loss: -0.00396764138713479
        total_loss: -0.0040859030559659
        vf_explained_var: 0.02301257848739624
        vf_loss: 19.139244079589844
      agent-3:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36263254284858704
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011151988292112947
        model: {}
        policy_loss: -0.002355689648538828
        total_loss: -0.001286056824028492
        vf_explained_var: 0.12541314959526062
        vf_loss: 17.078636169433594
      agent-4:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9166461229324341
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015278501668944955
        model: {}
        policy_loss: -0.0038321379106491804
        total_loss: -0.0036765928380191326
        vf_explained_var: 0.08779779076576233
        vf_loss: 17.68844985961914
      agent-5:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7974359393119812
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016687383176758885
        model: {}
        policy_loss: -0.003542110323905945
        total_loss: -0.0031905993819236755
        vf_explained_var: 0.08953268826007843
        vf_loss: 17.54993438720703
    load_time_ms: 15300.176
    num_steps_sampled: 22656000
    num_steps_trained: 22656000
    sample_time_ms: 89341.941
    update_time_ms: 22.089
  iterations_since_restore: 216
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.813114754098356
    ram_util_percent: 9.702185792349727
  pid: 4061
  policy_reward_max:
    agent-0: 183.16666666666617
    agent-1: 183.16666666666617
    agent-2: 183.16666666666617
    agent-3: 183.16666666666617
    agent-4: 183.16666666666617
    agent-5: 183.16666666666617
  policy_reward_mean:
    agent-0: 151.81500000000003
    agent-1: 151.81500000000003
    agent-2: 151.81500000000003
    agent-3: 151.81500000000003
    agent-4: 151.81500000000003
    agent-5: 151.81500000000003
  policy_reward_min:
    agent-0: 59.33333333333314
    agent-1: 59.33333333333314
    agent-2: 59.33333333333314
    agent-3: 59.33333333333314
    agent-4: 59.33333333333314
    agent-5: 59.33333333333314
  sampler_perf:
    mean_env_wait_ms: 23.76110859295284
    mean_inference_ms: 12.277628179015982
    mean_processing_ms: 50.72765485506325
  time_since_restore: 27899.81717824936
  time_this_iter_s: 128.3903088569641
  time_total_s: 31110.88086438179
  timestamp: 1637045460
  timesteps_since_restore: 20736000
  timesteps_this_iter: 96000
  timesteps_total: 22656000
  training_iteration: 236
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    236 |          31110.9 | 22656000 |   910.89 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 49
    apples_agent-0_mean: 3.13
    apples_agent-0_min: 0
    apples_agent-1_max: 100
    apples_agent-1_mean: 30.31
    apples_agent-1_min: 0
    apples_agent-2_max: 225
    apples_agent-2_mean: 14.44
    apples_agent-2_min: 0
    apples_agent-3_max: 108
    apples_agent-3_mean: 58.18
    apples_agent-3_min: 26
    apples_agent-4_max: 50
    apples_agent-4_mean: 0.63
    apples_agent-4_min: 0
    apples_agent-5_max: 204
    apples_agent-5_mean: 102.29
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 583
    cleaning_beam_agent-0_mean: 398.97
    cleaning_beam_agent-0_min: 249
    cleaning_beam_agent-1_max: 351
    cleaning_beam_agent-1_mean: 196.2
    cleaning_beam_agent-1_min: 113
    cleaning_beam_agent-2_max: 441
    cleaning_beam_agent-2_mean: 272.4
    cleaning_beam_agent-2_min: 92
    cleaning_beam_agent-3_max: 112
    cleaning_beam_agent-3_mean: 23.41
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 567
    cleaning_beam_agent-4_mean: 453.42
    cleaning_beam_agent-4_min: 324
    cleaning_beam_agent-5_max: 590
    cleaning_beam_agent-5_mean: 80.46
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-53-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1084.9999999999973
  episode_reward_mean: 905.2099999999824
  episode_reward_min: 493.00000000000836
  episodes_this_iter: 96
  episodes_total: 22752
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20217.959
    learner:
      agent-0:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9610503315925598
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015596281737089157
        model: {}
        policy_loss: -0.0030872472561895847
        total_loss: -0.0028405790217220783
        vf_explained_var: -0.01364116370677948
        vf_loss: 19.381141662597656
      agent-1:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1236793994903564
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016308703925460577
        model: {}
        policy_loss: -0.003995010629296303
        total_loss: -0.00403033010661602
        vf_explained_var: -0.014637380838394165
        vf_loss: 19.423532485961914
      agent-2:
        cur_kl_coeff: 2.1827873709912016e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 1.14182710647583
        entropy_coeff: 0.0017600000137463212
        kl: 0.000960623670835048
        model: {}
        policy_loss: -0.0036205295473337173
        total_loss: -0.0037297001108527184
        vf_explained_var: -0.0002805441617965698
        vf_loss: 19.004453659057617
      agent-3:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37853509187698364
        entropy_coeff: 0.0017600000137463212
        kl: 0.001010524109005928
        model: {}
        policy_loss: -0.002255635801702738
        total_loss: -0.0012799014803022146
        vf_explained_var: 0.1289932131767273
        vf_loss: 16.419580459594727
      agent-4:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9028517603874207
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015691922744736075
        model: {}
        policy_loss: -0.003755782963708043
        total_loss: -0.0035245816688984632
        vf_explained_var: 0.033322811126708984
        vf_loss: 18.202207565307617
      agent-5:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7974013090133667
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011016791686415672
        model: {}
        policy_loss: -0.00343212834559381
        total_loss: -0.0031319940462708473
        vf_explained_var: 0.09716084599494934
        vf_loss: 17.03561019897461
    load_time_ms: 15445.325
    num_steps_sampled: 22752000
    num_steps_trained: 22752000
    sample_time_ms: 89209.815
    update_time_ms: 21.597
  iterations_since_restore: 217
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.044198895027625
    ram_util_percent: 9.609392265193371
  pid: 4061
  policy_reward_max:
    agent-0: 180.83333333333323
    agent-1: 180.83333333333323
    agent-2: 180.83333333333323
    agent-3: 180.83333333333323
    agent-4: 180.83333333333323
    agent-5: 180.83333333333323
  policy_reward_mean:
    agent-0: 150.86833333333337
    agent-1: 150.86833333333337
    agent-2: 150.86833333333337
    agent-3: 150.86833333333337
    agent-4: 150.86833333333337
    agent-5: 150.86833333333337
  policy_reward_min:
    agent-0: 82.16666666666673
    agent-1: 82.16666666666673
    agent-2: 82.16666666666673
    agent-3: 82.16666666666673
    agent-4: 82.16666666666673
    agent-5: 82.16666666666673
  sampler_perf:
    mean_env_wait_ms: 23.763777800095525
    mean_inference_ms: 12.277153188073855
    mean_processing_ms: 50.72601465204175
  time_since_restore: 28026.280914068222
  time_this_iter_s: 126.46373581886292
  time_total_s: 31237.344600200653
  timestamp: 1637045586
  timesteps_since_restore: 20832000
  timesteps_this_iter: 96000
  timesteps_total: 22752000
  training_iteration: 237
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    237 |          31237.3 | 22752000 |   905.21 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 49
    apples_agent-0_mean: 2.01
    apples_agent-0_min: 0
    apples_agent-1_max: 75
    apples_agent-1_mean: 26.87
    apples_agent-1_min: 0
    apples_agent-2_max: 89
    apples_agent-2_mean: 10.84
    apples_agent-2_min: 0
    apples_agent-3_max: 98
    apples_agent-3_mean: 57.86
    apples_agent-3_min: 30
    apples_agent-4_max: 39
    apples_agent-4_mean: 0.81
    apples_agent-4_min: 0
    apples_agent-5_max: 176
    apples_agent-5_mean: 101.37
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 499
    cleaning_beam_agent-0_mean: 394.36
    cleaning_beam_agent-0_min: 208
    cleaning_beam_agent-1_max: 321
    cleaning_beam_agent-1_mean: 199.97
    cleaning_beam_agent-1_min: 91
    cleaning_beam_agent-2_max: 486
    cleaning_beam_agent-2_mean: 275.66
    cleaning_beam_agent-2_min: 67
    cleaning_beam_agent-3_max: 92
    cleaning_beam_agent-3_mean: 20.09
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 593
    cleaning_beam_agent-4_mean: 453.45
    cleaning_beam_agent-4_min: 326
    cleaning_beam_agent-5_max: 702
    cleaning_beam_agent-5_mean: 88.3
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-55-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1058.9999999999927
  episode_reward_mean: 915.6299999999817
  episode_reward_min: 397.0000000000077
  episodes_this_iter: 96
  episodes_total: 22848
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20221.845
    learner:
      agent-0:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9468954801559448
        entropy_coeff: 0.0017600000137463212
        kl: 0.001066790078766644
        model: {}
        policy_loss: -0.003365103155374527
        total_loss: -0.0029245507903397083
        vf_explained_var: -0.018431931734085083
        vf_loss: 21.07089614868164
      agent-1:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 1.139608383178711
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018711511511355639
        model: {}
        policy_loss: -0.003954183775931597
        total_loss: -0.0038793229032307863
        vf_explained_var: -0.001172393560409546
        vf_loss: 20.80569076538086
      agent-2:
        cur_kl_coeff: 1.0913936854956008e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 1.119364857673645
        entropy_coeff: 0.0017600000137463212
        kl: 0.001425381749868393
        model: {}
        policy_loss: -0.003975529223680496
        total_loss: -0.00391722284257412
        vf_explained_var: 0.00964386761188507
        vf_loss: 20.28386116027832
      agent-3:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3608696162700653
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009434120147489011
        model: {}
        policy_loss: -0.002311833668500185
        total_loss: -0.001162185799330473
        vf_explained_var: 0.13134163618087769
        vf_loss: 17.847789764404297
      agent-4:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9044193625450134
        entropy_coeff: 0.0017600000137463212
        kl: 0.00161396199837327
        model: {}
        policy_loss: -0.003919700626283884
        total_loss: -0.00362010532990098
        vf_explained_var: 0.08065086603164673
        vf_loss: 18.913738250732422
      agent-5:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7880034446716309
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014362407382577658
        model: {}
        policy_loss: -0.003951471298933029
        total_loss: -0.0035025449469685555
        vf_explained_var: 0.10602797567844391
        vf_loss: 18.358144760131836
    load_time_ms: 15664.312
    num_steps_sampled: 22848000
    num_steps_trained: 22848000
    sample_time_ms: 89258.155
    update_time_ms: 21.601
  iterations_since_restore: 218
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.191011235955056
    ram_util_percent: 9.598314606741575
  pid: 4061
  policy_reward_max:
    agent-0: 176.50000000000006
    agent-1: 176.50000000000006
    agent-2: 176.50000000000006
    agent-3: 176.50000000000006
    agent-4: 176.50000000000006
    agent-5: 176.50000000000006
  policy_reward_mean:
    agent-0: 152.60500000000002
    agent-1: 152.60500000000002
    agent-2: 152.60500000000002
    agent-3: 152.60500000000002
    agent-4: 152.60500000000002
    agent-5: 152.60500000000002
  policy_reward_min:
    agent-0: 66.16666666666639
    agent-1: 66.16666666666639
    agent-2: 66.16666666666639
    agent-3: 66.16666666666639
    agent-4: 66.16666666666639
    agent-5: 66.16666666666639
  sampler_perf:
    mean_env_wait_ms: 23.766774430124997
    mean_inference_ms: 12.276728328126895
    mean_processing_ms: 50.72426537097716
  time_since_restore: 28151.77392745018
  time_this_iter_s: 125.49301338195801
  time_total_s: 31362.83761358261
  timestamp: 1637045712
  timesteps_since_restore: 20928000
  timesteps_this_iter: 96000
  timesteps_total: 22848000
  training_iteration: 238
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    238 |          31362.8 | 22848000 |   915.63 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 37
    apples_agent-0_mean: 1.93
    apples_agent-0_min: 0
    apples_agent-1_max: 109
    apples_agent-1_mean: 30.79
    apples_agent-1_min: 0
    apples_agent-2_max: 87
    apples_agent-2_mean: 11.5
    apples_agent-2_min: 0
    apples_agent-3_max: 97
    apples_agent-3_mean: 58.9
    apples_agent-3_min: 30
    apples_agent-4_max: 84
    apples_agent-4_mean: 1.25
    apples_agent-4_min: 0
    apples_agent-5_max: 182
    apples_agent-5_mean: 106.49
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 536
    cleaning_beam_agent-0_mean: 395.53
    cleaning_beam_agent-0_min: 235
    cleaning_beam_agent-1_max: 408
    cleaning_beam_agent-1_mean: 208.39
    cleaning_beam_agent-1_min: 108
    cleaning_beam_agent-2_max: 426
    cleaning_beam_agent-2_mean: 266.24
    cleaning_beam_agent-2_min: 113
    cleaning_beam_agent-3_max: 69
    cleaning_beam_agent-3_mean: 21.43
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 569
    cleaning_beam_agent-4_mean: 461.51
    cleaning_beam_agent-4_min: 279
    cleaning_beam_agent-5_max: 615
    cleaning_beam_agent-5_mean: 76.59
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 17
    fire_beam_agent-5_mean: 0.17
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-57-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1089.9999999999889
  episode_reward_mean: 917.3299999999816
  episode_reward_min: 471.0000000000059
  episodes_this_iter: 96
  episodes_total: 22944
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20175.589
    learner:
      agent-0:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9444268941879272
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017158857081085443
        model: {}
        policy_loss: -0.0034765852615237236
        total_loss: -0.0031536705791950226
        vf_explained_var: -0.010952010750770569
        vf_loss: 19.851089477539062
      agent-1:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 1.130662441253662
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012919737491756678
        model: {}
        policy_loss: -0.00400160439312458
        total_loss: -0.004010714590549469
        vf_explained_var: -0.010393232107162476
        vf_loss: 19.808570861816406
      agent-2:
        cur_kl_coeff: 5.456968427478004e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1406707763671875
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016809434164315462
        model: {}
        policy_loss: -0.0038347123190760612
        total_loss: -0.0038735156413167715
        vf_explained_var: -0.013093307614326477
        vf_loss: 19.68777084350586
      agent-3:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36949002742767334
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014274702407419682
        model: {}
        policy_loss: -0.002055519027635455
        total_loss: -0.0010351825039833784
        vf_explained_var: 0.14192555844783783
        vf_loss: 16.706371307373047
      agent-4:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.889691948890686
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018917251145467162
        model: {}
        policy_loss: -0.0038356410805135965
        total_loss: -0.0035751494579017162
        vf_explained_var: 0.05755789577960968
        vf_loss: 18.263486862182617
      agent-5:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8022990226745605
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012955558486282825
        model: {}
        policy_loss: -0.0037924207281321287
        total_loss: -0.003386442083865404
        vf_explained_var: 0.06308837234973907
        vf_loss: 18.18026351928711
    load_time_ms: 15515.088
    num_steps_sampled: 22944000
    num_steps_trained: 22944000
    sample_time_ms: 89389.897
    update_time_ms: 21.701
  iterations_since_restore: 219
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.408474576271187
    ram_util_percent: 9.650282485875707
  pid: 4061
  policy_reward_max:
    agent-0: 181.66666666666686
    agent-1: 181.66666666666686
    agent-2: 181.66666666666686
    agent-3: 181.66666666666686
    agent-4: 181.66666666666686
    agent-5: 181.66666666666686
  policy_reward_mean:
    agent-0: 152.88833333333335
    agent-1: 152.88833333333335
    agent-2: 152.88833333333335
    agent-3: 152.88833333333335
    agent-4: 152.88833333333335
    agent-5: 152.88833333333335
  policy_reward_min:
    agent-0: 78.49999999999984
    agent-1: 78.49999999999984
    agent-2: 78.49999999999984
    agent-3: 78.49999999999984
    agent-4: 78.49999999999984
    agent-5: 78.49999999999984
  sampler_perf:
    mean_env_wait_ms: 23.76975966723971
    mean_inference_ms: 12.276280632000603
    mean_processing_ms: 50.72268598285021
  time_since_restore: 28275.973773241043
  time_this_iter_s: 124.19984579086304
  time_total_s: 31487.037459373474
  timestamp: 1637045836
  timesteps_since_restore: 21024000
  timesteps_this_iter: 96000
  timesteps_total: 22944000
  training_iteration: 239
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    239 |            31487 | 22944000 |   917.33 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 1.29
    apples_agent-0_min: 0
    apples_agent-1_max: 89
    apples_agent-1_mean: 27.57
    apples_agent-1_min: 0
    apples_agent-2_max: 262
    apples_agent-2_mean: 16.11
    apples_agent-2_min: 0
    apples_agent-3_max: 124
    apples_agent-3_mean: 56.17
    apples_agent-3_min: 18
    apples_agent-4_max: 59
    apples_agent-4_mean: 2.31
    apples_agent-4_min: 0
    apples_agent-5_max: 276
    apples_agent-5_mean: 103.68
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 529
    cleaning_beam_agent-0_mean: 415.55
    cleaning_beam_agent-0_min: 267
    cleaning_beam_agent-1_max: 324
    cleaning_beam_agent-1_mean: 216.89
    cleaning_beam_agent-1_min: 101
    cleaning_beam_agent-2_max: 456
    cleaning_beam_agent-2_mean: 267.05
    cleaning_beam_agent-2_min: 114
    cleaning_beam_agent-3_max: 141
    cleaning_beam_agent-3_mean: 19.96
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 561
    cleaning_beam_agent-4_mean: 464.58
    cleaning_beam_agent-4_min: 279
    cleaning_beam_agent-5_max: 662
    cleaning_beam_agent-5_mean: 62.93
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-59-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1068.9999999999982
  episode_reward_mean: 910.7699999999833
  episode_reward_min: 348.00000000000307
  episodes_this_iter: 96
  episodes_total: 23040
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20172.437
    learner:
      agent-0:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9467313289642334
        entropy_coeff: 0.0017600000137463212
        kl: 0.001577305025421083
        model: {}
        policy_loss: -0.003240281483158469
        total_loss: -0.0028326467145234346
        vf_explained_var: -0.02493014931678772
        vf_loss: 20.738821029663086
      agent-1:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1284632682800293
        entropy_coeff: 0.0017600000137463212
        kl: 0.00202577724121511
        model: {}
        policy_loss: -0.004208342172205448
        total_loss: -0.004115906078368425
        vf_explained_var: -0.033563703298568726
        vf_loss: 20.785324096679688
      agent-2:
        cur_kl_coeff: 2.728484213739002e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1478010416030884
        entropy_coeff: 0.0017600000137463212
        kl: 0.001408286509104073
        model: {}
        policy_loss: -0.003486356697976589
        total_loss: -0.0034801531583070755
        vf_explained_var: -0.004909157752990723
        vf_loss: 20.263320922851562
      agent-3:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3693344295024872
        entropy_coeff: 0.0017600000137463212
        kl: 0.001092973630875349
        model: {}
        policy_loss: -0.0023360522463917732
        total_loss: -0.00125348090659827
        vf_explained_var: 0.14882779121398926
        vf_loss: 17.326017379760742
      agent-4:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9121604561805725
        entropy_coeff: 0.0017600000137463212
        kl: 0.001321822521276772
        model: {}
        policy_loss: -0.0038579856045544147
        total_loss: -0.0036435252986848354
        vf_explained_var: 0.10038787126541138
        vf_loss: 18.19861602783203
      agent-5:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7927058935165405
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012095108395442367
        model: {}
        policy_loss: -0.0035684979520738125
        total_loss: -0.0031340434215962887
        vf_explained_var: 0.08446203172206879
        vf_loss: 18.296159744262695
    load_time_ms: 15616.212
    num_steps_sampled: 23040000
    num_steps_trained: 23040000
    sample_time_ms: 89367.968
    update_time_ms: 21.631
  iterations_since_restore: 220
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.33954802259887
    ram_util_percent: 9.664406779661018
  pid: 4061
  policy_reward_max:
    agent-0: 178.16666666666666
    agent-1: 178.16666666666666
    agent-2: 178.16666666666666
    agent-3: 178.16666666666666
    agent-4: 178.16666666666666
    agent-5: 178.16666666666666
  policy_reward_mean:
    agent-0: 151.79500000000002
    agent-1: 151.79500000000002
    agent-2: 151.79500000000002
    agent-3: 151.79500000000002
    agent-4: 151.79500000000002
    agent-5: 151.79500000000002
  policy_reward_min:
    agent-0: 57.99999999999988
    agent-1: 57.99999999999988
    agent-2: 57.99999999999988
    agent-3: 57.99999999999988
    agent-4: 57.99999999999988
    agent-5: 57.99999999999988
  sampler_perf:
    mean_env_wait_ms: 23.772344173226593
    mean_inference_ms: 12.27600403393393
    mean_processing_ms: 50.71992728948424
  time_since_restore: 28399.70140504837
  time_this_iter_s: 123.72763180732727
  time_total_s: 31610.7650911808
  timestamp: 1637045960
  timesteps_since_restore: 21120000
  timesteps_this_iter: 96000
  timesteps_total: 23040000
  training_iteration: 240
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    240 |          31610.8 | 23040000 |   910.77 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 37
    apples_agent-0_mean: 1.31
    apples_agent-0_min: 0
    apples_agent-1_max: 89
    apples_agent-1_mean: 28.27
    apples_agent-1_min: 0
    apples_agent-2_max: 165
    apples_agent-2_mean: 15.22
    apples_agent-2_min: 0
    apples_agent-3_max: 144
    apples_agent-3_mean: 57.61
    apples_agent-3_min: 18
    apples_agent-4_max: 50
    apples_agent-4_mean: 1.28
    apples_agent-4_min: 0
    apples_agent-5_max: 281
    apples_agent-5_mean: 107.02
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 535
    cleaning_beam_agent-0_mean: 416.76
    cleaning_beam_agent-0_min: 298
    cleaning_beam_agent-1_max: 374
    cleaning_beam_agent-1_mean: 219.66
    cleaning_beam_agent-1_min: 112
    cleaning_beam_agent-2_max: 477
    cleaning_beam_agent-2_mean: 267.11
    cleaning_beam_agent-2_min: 93
    cleaning_beam_agent-3_max: 102
    cleaning_beam_agent-3_mean: 20.54
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 593
    cleaning_beam_agent-4_mean: 459.35
    cleaning_beam_agent-4_min: 325
    cleaning_beam_agent-5_max: 630
    cleaning_beam_agent-5_mean: 58.29
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-01-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1083.9999999999784
  episode_reward_mean: 914.3399999999818
  episode_reward_min: 216.9999999999982
  episodes_this_iter: 96
  episodes_total: 23136
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20155.335
    learner:
      agent-0:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9511688947677612
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017964759608730674
        model: {}
        policy_loss: -0.0032319868914783
        total_loss: -0.002778411377221346
        vf_explained_var: -0.01687440276145935
        vf_loss: 21.276323318481445
      agent-1:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 1.114640235900879
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016541179502382874
        model: {}
        policy_loss: -0.004075611475855112
        total_loss: -0.00389576843008399
        vf_explained_var: -0.02229517698287964
        vf_loss: 21.416101455688477
      agent-2:
        cur_kl_coeff: 1.364242106869501e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 1.136899709701538
        entropy_coeff: 0.0017600000137463212
        kl: 0.001909116399474442
        model: {}
        policy_loss: -0.003982727415859699
        total_loss: -0.003945270553231239
        vf_explained_var: 0.018400371074676514
        vf_loss: 20.383983612060547
      agent-3:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37354612350463867
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007309705251827836
        model: {}
        policy_loss: -0.002186087891459465
        total_loss: -0.001054219901561737
        vf_explained_var: 0.14236487448215485
        vf_loss: 17.893152236938477
      agent-4:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.910571277141571
        entropy_coeff: 0.0017600000137463212
        kl: 0.001773569267243147
        model: {}
        policy_loss: -0.003970855847001076
        total_loss: -0.0036548790521919727
        vf_explained_var: 0.07951973378658295
        vf_loss: 19.185792922973633
      agent-5:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7738481760025024
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009629620471969247
        model: {}
        policy_loss: -0.003632680745795369
        total_loss: -0.003118707099929452
        vf_explained_var: 0.09657351672649384
        vf_loss: 18.75948715209961
    load_time_ms: 15142.16
    num_steps_sampled: 23136000
    num_steps_trained: 23136000
    sample_time_ms: 89612.886
    update_time_ms: 21.583
  iterations_since_restore: 221
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 16.816304347826087
    ram_util_percent: 9.696195652173914
  pid: 4061
  policy_reward_max:
    agent-0: 180.6666666666663
    agent-1: 180.6666666666663
    agent-2: 180.6666666666663
    agent-3: 180.6666666666663
    agent-4: 180.6666666666663
    agent-5: 180.6666666666663
  policy_reward_mean:
    agent-0: 152.38999999999996
    agent-1: 152.38999999999996
    agent-2: 152.38999999999996
    agent-3: 152.38999999999996
    agent-4: 152.38999999999996
    agent-5: 152.38999999999996
  policy_reward_min:
    agent-0: 36.1666666666667
    agent-1: 36.1666666666667
    agent-2: 36.1666666666667
    agent-3: 36.1666666666667
    agent-4: 36.1666666666667
    agent-5: 36.1666666666667
  sampler_perf:
    mean_env_wait_ms: 23.776370972168507
    mean_inference_ms: 12.276157551568245
    mean_processing_ms: 50.72251165887832
  time_since_restore: 28528.200174570084
  time_this_iter_s: 128.49876952171326
  time_total_s: 31739.263860702515
  timestamp: 1637046089
  timesteps_since_restore: 21216000
  timesteps_this_iter: 96000
  timesteps_total: 23136000
  training_iteration: 241
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    241 |          31739.3 | 23136000 |   914.34 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 37
    apples_agent-0_mean: 1.6
    apples_agent-0_min: 0
    apples_agent-1_max: 88
    apples_agent-1_mean: 29.13
    apples_agent-1_min: 0
    apples_agent-2_max: 137
    apples_agent-2_mean: 10.05
    apples_agent-2_min: 0
    apples_agent-3_max: 284
    apples_agent-3_mean: 60.78
    apples_agent-3_min: 28
    apples_agent-4_max: 71
    apples_agent-4_mean: 1.66
    apples_agent-4_min: 0
    apples_agent-5_max: 281
    apples_agent-5_mean: 108.68
    apples_agent-5_min: 17
    cleaning_beam_agent-0_max: 532
    cleaning_beam_agent-0_mean: 405.95
    cleaning_beam_agent-0_min: 267
    cleaning_beam_agent-1_max: 400
    cleaning_beam_agent-1_mean: 223.02
    cleaning_beam_agent-1_min: 80
    cleaning_beam_agent-2_max: 480
    cleaning_beam_agent-2_mean: 287.19
    cleaning_beam_agent-2_min: 107
    cleaning_beam_agent-3_max: 70
    cleaning_beam_agent-3_mean: 20.04
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 574
    cleaning_beam_agent-4_mean: 465.23
    cleaning_beam_agent-4_min: 256
    cleaning_beam_agent-5_max: 404
    cleaning_beam_agent-5_mean: 51.42
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-03-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1105.000000000001
  episode_reward_mean: 926.7499999999809
  episode_reward_min: 320.99999999999716
  episodes_this_iter: 96
  episodes_total: 23232
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20154.45
    learner:
      agent-0:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9575241804122925
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010154498741030693
        model: {}
        policy_loss: -0.0031938725151121616
        total_loss: -0.002711900509893894
        vf_explained_var: -0.030533984303474426
        vf_loss: 21.672142028808594
      agent-1:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1079108715057373
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016432018019258976
        model: {}
        policy_loss: -0.004147155210375786
        total_loss: -0.0038991530891507864
        vf_explained_var: -0.04764997959136963
        vf_loss: 21.979246139526367
      agent-2:
        cur_kl_coeff: 6.821210534347505e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1411153078079224
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014407208655029535
        model: {}
        policy_loss: -0.0036971308290958405
        total_loss: -0.0036712056025862694
        vf_explained_var: 0.01239842176437378
        vf_loss: 20.34286117553711
      agent-3:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3698956072330475
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007123628165572882
        model: {}
        policy_loss: -0.0022225857246667147
        total_loss: -0.001036899397149682
        vf_explained_var: 0.11596167087554932
        vf_loss: 18.36701202392578
      agent-4:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9183166027069092
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023432536981999874
        model: {}
        policy_loss: -0.004219521768391132
        total_loss: -0.003991083707660437
        vf_explained_var: 0.11870406568050385
        vf_loss: 18.446733474731445
      agent-5:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.781416118144989
        entropy_coeff: 0.0017600000137463212
        kl: 0.00159708340652287
        model: {}
        policy_loss: -0.0034748567268252373
        total_loss: -0.002939117606729269
        vf_explained_var: 0.07427603006362915
        vf_loss: 19.110301971435547
    load_time_ms: 15148.466
    num_steps_sampled: 23232000
    num_steps_trained: 23232000
    sample_time_ms: 89745.434
    update_time_ms: 21.3
  iterations_since_restore: 222
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.388135593220339
    ram_util_percent: 9.651977401129944
  pid: 4061
  policy_reward_max:
    agent-0: 184.1666666666662
    agent-1: 184.1666666666662
    agent-2: 184.1666666666662
    agent-3: 184.1666666666662
    agent-4: 184.1666666666662
    agent-5: 184.1666666666662
  policy_reward_mean:
    agent-0: 154.45833333333326
    agent-1: 154.45833333333326
    agent-2: 154.45833333333326
    agent-3: 154.45833333333326
    agent-4: 154.45833333333326
    agent-5: 154.45833333333326
  policy_reward_min:
    agent-0: 53.4999999999999
    agent-1: 53.4999999999999
    agent-2: 53.4999999999999
    agent-3: 53.4999999999999
    agent-4: 53.4999999999999
    agent-5: 53.4999999999999
  sampler_perf:
    mean_env_wait_ms: 23.778880295419622
    mean_inference_ms: 12.27567389233387
    mean_processing_ms: 50.71931583949082
  time_since_restore: 28652.483622789383
  time_this_iter_s: 124.28344821929932
  time_total_s: 31863.547308921814
  timestamp: 1637046214
  timesteps_since_restore: 21312000
  timesteps_this_iter: 96000
  timesteps_total: 23232000
  training_iteration: 242
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 16.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    242 |          31863.5 | 23232000 |   926.75 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 68
    apples_agent-0_mean: 2.33
    apples_agent-0_min: 0
    apples_agent-1_max: 120
    apples_agent-1_mean: 29.39
    apples_agent-1_min: 0
    apples_agent-2_max: 97
    apples_agent-2_mean: 12.68
    apples_agent-2_min: 0
    apples_agent-3_max: 150
    apples_agent-3_mean: 58.0
    apples_agent-3_min: 28
    apples_agent-4_max: 96
    apples_agent-4_mean: 0.96
    apples_agent-4_min: 0
    apples_agent-5_max: 184
    apples_agent-5_mean: 100.95
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 513
    cleaning_beam_agent-0_mean: 413.34
    cleaning_beam_agent-0_min: 231
    cleaning_beam_agent-1_max: 495
    cleaning_beam_agent-1_mean: 248.65
    cleaning_beam_agent-1_min: 101
    cleaning_beam_agent-2_max: 470
    cleaning_beam_agent-2_mean: 271.0
    cleaning_beam_agent-2_min: 126
    cleaning_beam_agent-3_max: 154
    cleaning_beam_agent-3_mean: 21.24
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 589
    cleaning_beam_agent-4_mean: 458.93
    cleaning_beam_agent-4_min: 336
    cleaning_beam_agent-5_max: 699
    cleaning_beam_agent-5_mean: 72.26
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-05-39
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1130.9999999999939
  episode_reward_mean: 915.4099999999808
  episode_reward_min: 467.0000000000014
  episodes_this_iter: 96
  episodes_total: 23328
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20158.475
    learner:
      agent-0:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.953627347946167
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010709952330216765
        model: {}
        policy_loss: -0.0031082588247954845
        total_loss: -0.002869226038455963
        vf_explained_var: -0.00748269259929657
        vf_loss: 19.17416000366211
      agent-1:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 1.127541422843933
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012823977740481496
        model: {}
        policy_loss: -0.003769045928493142
        total_loss: -0.003858851734548807
        vf_explained_var: -0.005307421088218689
        vf_loss: 18.94664192199707
      agent-2:
        cur_kl_coeff: 3.4106052671737525e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1439129114151
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017030274029821157
        model: {}
        policy_loss: -0.004048205446451902
        total_loss: -0.004210380837321281
        vf_explained_var: 0.014015361666679382
        vf_loss: 18.511079788208008
      agent-3:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37144702672958374
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007737744599580765
        model: {}
        policy_loss: -0.0020958215463906527
        total_loss: -0.0010832557454705238
        vf_explained_var: 0.11788588762283325
        vf_loss: 16.663127899169922
      agent-4:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9033467173576355
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015661370707675815
        model: {}
        policy_loss: -0.003874515648931265
        total_loss: -0.0036978069692850113
        vf_explained_var: 0.06130245327949524
        vf_loss: 17.665996551513672
      agent-5:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7798365354537964
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014199475990608335
        model: {}
        policy_loss: -0.003468880895525217
        total_loss: -0.0031119207851588726
        vf_explained_var: 0.07837900519371033
        vf_loss: 17.294708251953125
    load_time_ms: 15210.7
    num_steps_sampled: 23328000
    num_steps_trained: 23328000
    sample_time_ms: 89869.197
    update_time_ms: 20.888
  iterations_since_restore: 223
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.335955056179776
    ram_util_percent: 9.527528089887644
  pid: 4061
  policy_reward_max:
    agent-0: 188.49999999999952
    agent-1: 188.49999999999952
    agent-2: 188.49999999999952
    agent-3: 188.49999999999952
    agent-4: 188.49999999999952
    agent-5: 188.49999999999952
  policy_reward_mean:
    agent-0: 152.56833333333336
    agent-1: 152.56833333333336
    agent-2: 152.56833333333336
    agent-3: 152.56833333333336
    agent-4: 152.56833333333336
    agent-5: 152.56833333333336
  policy_reward_min:
    agent-0: 77.83333333333317
    agent-1: 77.83333333333317
    agent-2: 77.83333333333317
    agent-3: 77.83333333333317
    agent-4: 77.83333333333317
    agent-5: 77.83333333333317
  sampler_perf:
    mean_env_wait_ms: 23.782073191184736
    mean_inference_ms: 12.275050290128625
    mean_processing_ms: 50.71663301459003
  time_since_restore: 28777.421827077866
  time_this_iter_s: 124.93820428848267
  time_total_s: 31988.485513210297
  timestamp: 1637046339
  timesteps_since_restore: 21408000
  timesteps_this_iter: 96000
  timesteps_total: 23328000
  training_iteration: 243
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    243 |          31988.5 | 23328000 |   915.41 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 18
    apples_agent-0_mean: 0.9
    apples_agent-0_min: 0
    apples_agent-1_max: 105
    apples_agent-1_mean: 27.54
    apples_agent-1_min: 0
    apples_agent-2_max: 94
    apples_agent-2_mean: 12.99
    apples_agent-2_min: 0
    apples_agent-3_max: 93
    apples_agent-3_mean: 54.98
    apples_agent-3_min: 23
    apples_agent-4_max: 27
    apples_agent-4_mean: 0.64
    apples_agent-4_min: 0
    apples_agent-5_max: 160
    apples_agent-5_mean: 101.94
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 524
    cleaning_beam_agent-0_mean: 413.74
    cleaning_beam_agent-0_min: 300
    cleaning_beam_agent-1_max: 400
    cleaning_beam_agent-1_mean: 236.39
    cleaning_beam_agent-1_min: 100
    cleaning_beam_agent-2_max: 448
    cleaning_beam_agent-2_mean: 270.04
    cleaning_beam_agent-2_min: 124
    cleaning_beam_agent-3_max: 86
    cleaning_beam_agent-3_mean: 23.29
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 564
    cleaning_beam_agent-4_mean: 462.27
    cleaning_beam_agent-4_min: 331
    cleaning_beam_agent-5_max: 559
    cleaning_beam_agent-5_mean: 58.68
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-07-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1088.999999999992
  episode_reward_mean: 925.3199999999821
  episode_reward_min: 227.99999999999727
  episodes_this_iter: 96
  episodes_total: 23424
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20153.776
    learner:
      agent-0:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9723346829414368
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016211598413065076
        model: {}
        policy_loss: -0.003133766818791628
        total_loss: -0.0027989312075078487
        vf_explained_var: 0.01693148910999298
        vf_loss: 20.46138572692871
      agent-1:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1207430362701416
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019891848787665367
        model: {}
        policy_loss: -0.0044072880409657955
        total_loss: -0.0042808568105101585
        vf_explained_var: -0.008246690034866333
        vf_loss: 20.989416122436523
      agent-2:
        cur_kl_coeff: 1.7053026335868762e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1422635316848755
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014930607285350561
        model: {}
        policy_loss: -0.0036976865958422422
        total_loss: -0.003666478442028165
        vf_explained_var: 0.014209523797035217
        vf_loss: 20.415916442871094
      agent-3:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37992382049560547
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006188513361848891
        model: {}
        policy_loss: -0.0020802682265639305
        total_loss: -0.0009916783310472965
        vf_explained_var: 0.15154215693473816
        vf_loss: 17.57256317138672
      agent-4:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8961833119392395
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019362929742783308
        model: {}
        policy_loss: -0.004172372166067362
        total_loss: -0.003838347503915429
        vf_explained_var: 0.06919829547405243
        vf_loss: 19.113088607788086
      agent-5:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7762062549591064
        entropy_coeff: 0.0017600000137463212
        kl: 0.001223294297233224
        model: {}
        policy_loss: -0.0038307099603116512
        total_loss: -0.003320032265037298
        vf_explained_var: 0.08801209926605225
        vf_loss: 18.768003463745117
    load_time_ms: 15286.294
    num_steps_sampled: 23424000
    num_steps_trained: 23424000
    sample_time_ms: 89953.404
    update_time_ms: 21.233
  iterations_since_restore: 224
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.12611111111111
    ram_util_percent: 9.674444444444445
  pid: 4061
  policy_reward_max:
    agent-0: 181.49999999999974
    agent-1: 181.49999999999974
    agent-2: 181.49999999999974
    agent-3: 181.49999999999974
    agent-4: 181.49999999999974
    agent-5: 181.49999999999974
  policy_reward_mean:
    agent-0: 154.21999999999997
    agent-1: 154.21999999999997
    agent-2: 154.21999999999997
    agent-3: 154.21999999999997
    agent-4: 154.21999999999997
    agent-5: 154.21999999999997
  policy_reward_min:
    agent-0: 37.999999999999986
    agent-1: 37.999999999999986
    agent-2: 37.999999999999986
    agent-3: 37.999999999999986
    agent-4: 37.999999999999986
    agent-5: 37.999999999999986
  sampler_perf:
    mean_env_wait_ms: 23.78481466607754
    mean_inference_ms: 12.274441818368944
    mean_processing_ms: 50.71393015361781
  time_since_restore: 28903.37147974968
  time_this_iter_s: 125.94965267181396
  time_total_s: 32114.43516588211
  timestamp: 1637046465
  timesteps_since_restore: 21504000
  timesteps_this_iter: 96000
  timesteps_total: 23424000
  training_iteration: 244
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    244 |          32114.4 | 23424000 |   925.32 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 47
    apples_agent-0_mean: 2.46
    apples_agent-0_min: 0
    apples_agent-1_max: 96
    apples_agent-1_mean: 27.34
    apples_agent-1_min: 0
    apples_agent-2_max: 82
    apples_agent-2_mean: 11.96
    apples_agent-2_min: 0
    apples_agent-3_max: 220
    apples_agent-3_mean: 60.77
    apples_agent-3_min: 33
    apples_agent-4_max: 48
    apples_agent-4_mean: 0.95
    apples_agent-4_min: 0
    apples_agent-5_max: 280
    apples_agent-5_mean: 105.59
    apples_agent-5_min: 11
    cleaning_beam_agent-0_max: 521
    cleaning_beam_agent-0_mean: 399.77
    cleaning_beam_agent-0_min: 210
    cleaning_beam_agent-1_max: 411
    cleaning_beam_agent-1_mean: 236.79
    cleaning_beam_agent-1_min: 112
    cleaning_beam_agent-2_max: 478
    cleaning_beam_agent-2_mean: 264.56
    cleaning_beam_agent-2_min: 132
    cleaning_beam_agent-3_max: 103
    cleaning_beam_agent-3_mean: 21.22
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 566
    cleaning_beam_agent-4_mean: 460.89
    cleaning_beam_agent-4_min: 235
    cleaning_beam_agent-5_max: 440
    cleaning_beam_agent-5_mean: 63.38
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-09-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1135.9999999999977
  episode_reward_mean: 908.5699999999824
  episode_reward_min: 220.99999999999727
  episodes_this_iter: 96
  episodes_total: 23520
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20155.606
    learner:
      agent-0:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9702750444412231
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012686117552220821
        model: {}
        policy_loss: -0.0032607330940663815
        total_loss: -0.002820524387061596
        vf_explained_var: 0.014265090227127075
        vf_loss: 21.478910446166992
      agent-1:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1140506267547607
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023884710390120745
        model: {}
        policy_loss: -0.004076121840626001
        total_loss: -0.003783230436965823
        vf_explained_var: -0.038121894001960754
        vf_loss: 22.536218643188477
      agent-2:
        cur_kl_coeff: 8.526513167934381e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 1.146246314048767
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013698955299332738
        model: {}
        policy_loss: -0.003822538536041975
        total_loss: -0.003673881059512496
        vf_explained_var: 0.002580001950263977
        vf_loss: 21.66049575805664
      agent-3:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3844151496887207
        entropy_coeff: 0.0017600000137463212
        kl: 0.001001408789306879
        model: {}
        policy_loss: -0.002490042708814144
        total_loss: -0.0013390742242336273
        vf_explained_var: 0.15655000507831573
        vf_loss: 18.27539825439453
      agent-4:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9114609956741333
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014506764709949493
        model: {}
        policy_loss: -0.0038594179786741734
        total_loss: -0.0035109755117446184
        vf_explained_var: 0.09814387559890747
        vf_loss: 19.52614402770996
      agent-5:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.786248505115509
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013433718122541904
        model: {}
        policy_loss: -0.003651141654700041
        total_loss: -0.0030938212294131517
        vf_explained_var: 0.10305750370025635
        vf_loss: 19.411190032958984
    load_time_ms: 15529.584
    num_steps_sampled: 23520000
    num_steps_trained: 23520000
    sample_time_ms: 90165.155
    update_time_ms: 21.349
  iterations_since_restore: 225
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.929670329670328
    ram_util_percent: 9.693956043956044
  pid: 4061
  policy_reward_max:
    agent-0: 189.33333333333294
    agent-1: 189.33333333333294
    agent-2: 189.33333333333294
    agent-3: 189.33333333333294
    agent-4: 189.33333333333294
    agent-5: 189.33333333333294
  policy_reward_mean:
    agent-0: 151.42833333333326
    agent-1: 151.42833333333326
    agent-2: 151.42833333333326
    agent-3: 151.42833333333326
    agent-4: 151.42833333333326
    agent-5: 151.42833333333326
  policy_reward_min:
    agent-0: 36.83333333333334
    agent-1: 36.83333333333334
    agent-2: 36.83333333333334
    agent-3: 36.83333333333334
    agent-4: 36.83333333333334
    agent-5: 36.83333333333334
  sampler_perf:
    mean_env_wait_ms: 23.787591528954763
    mean_inference_ms: 12.274036574550191
    mean_processing_ms: 50.7126199793328
  time_since_restore: 29031.125036478043
  time_this_iter_s: 127.75355672836304
  time_total_s: 32242.188722610474
  timestamp: 1637046593
  timesteps_since_restore: 21600000
  timesteps_this_iter: 96000
  timesteps_total: 23520000
  training_iteration: 245
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    245 |          32242.2 | 23520000 |   908.57 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 45
    apples_agent-0_mean: 2.56
    apples_agent-0_min: 0
    apples_agent-1_max: 98
    apples_agent-1_mean: 25.18
    apples_agent-1_min: 0
    apples_agent-2_max: 613
    apples_agent-2_mean: 18.66
    apples_agent-2_min: 0
    apples_agent-3_max: 100
    apples_agent-3_mean: 58.31
    apples_agent-3_min: 32
    apples_agent-4_max: 84
    apples_agent-4_mean: 1.74
    apples_agent-4_min: 0
    apples_agent-5_max: 407
    apples_agent-5_mean: 104.95
    apples_agent-5_min: 17
    cleaning_beam_agent-0_max: 572
    cleaning_beam_agent-0_mean: 399.48
    cleaning_beam_agent-0_min: 207
    cleaning_beam_agent-1_max: 393
    cleaning_beam_agent-1_mean: 246.74
    cleaning_beam_agent-1_min: 117
    cleaning_beam_agent-2_max: 409
    cleaning_beam_agent-2_mean: 265.38
    cleaning_beam_agent-2_min: 54
    cleaning_beam_agent-3_max: 162
    cleaning_beam_agent-3_mean: 25.19
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 629
    cleaning_beam_agent-4_mean: 465.85
    cleaning_beam_agent-4_min: 279
    cleaning_beam_agent-5_max: 452
    cleaning_beam_agent-5_mean: 55.04
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-11-56
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1097.000000000002
  episode_reward_mean: 910.8099999999838
  episode_reward_min: 314.0000000000016
  episodes_this_iter: 96
  episodes_total: 23616
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20143.732
    learner:
      agent-0:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.970382809638977
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016078121261671185
        model: {}
        policy_loss: -0.003031850792467594
        total_loss: -0.0025121597573161125
        vf_explained_var: 0.01010635495185852
        vf_loss: 22.27566146850586
      agent-1:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1141448020935059
        entropy_coeff: 0.0017600000137463212
        kl: 0.001547822030261159
        model: {}
        policy_loss: -0.004061066545546055
        total_loss: -0.003769430797547102
        vf_explained_var: -0.00794827938079834
        vf_loss: 22.525299072265625
      agent-2:
        cur_kl_coeff: 4.2632565839671906e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1419405937194824
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015753328334540129
        model: {}
        policy_loss: -0.003753725904971361
        total_loss: -0.003567509353160858
        vf_explained_var: 0.019308671355247498
        vf_loss: 21.960290908813477
      agent-3:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3972591757774353
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007268610643222928
        model: {}
        policy_loss: -0.0024359519593417645
        total_loss: -0.001302544493228197
        vf_explained_var: 0.1805775910615921
        vf_loss: 18.325836181640625
      agent-4:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9037433862686157
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012569617247208953
        model: {}
        policy_loss: -0.003788987174630165
        total_loss: -0.0033389381133019924
        vf_explained_var: 0.0849992036819458
        vf_loss: 20.406404495239258
      agent-5:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7542074918746948
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013361208839341998
        model: {}
        policy_loss: -0.0038096774369478226
        total_loss: -0.0031963037326931953
        vf_explained_var: 0.12816517055034637
        vf_loss: 19.407819747924805
    load_time_ms: 15066.128
    num_steps_sampled: 23616000
    num_steps_trained: 23616000
    sample_time_ms: 90089.573
    update_time_ms: 21.304
  iterations_since_restore: 226
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.456000000000001
    ram_util_percent: 9.512571428571432
  pid: 4061
  policy_reward_max:
    agent-0: 182.833333333333
    agent-1: 182.833333333333
    agent-2: 182.833333333333
    agent-3: 182.833333333333
    agent-4: 182.833333333333
    agent-5: 182.833333333333
  policy_reward_mean:
    agent-0: 151.8016666666667
    agent-1: 151.8016666666667
    agent-2: 151.8016666666667
    agent-3: 151.8016666666667
    agent-4: 151.8016666666667
    agent-5: 151.8016666666667
  policy_reward_min:
    agent-0: 52.33333333333321
    agent-1: 52.33333333333321
    agent-2: 52.33333333333321
    agent-3: 52.33333333333321
    agent-4: 52.33333333333321
    agent-5: 52.33333333333321
  sampler_perf:
    mean_env_wait_ms: 23.790312340117335
    mean_inference_ms: 12.273623643832616
    mean_processing_ms: 50.70957610777338
  time_since_restore: 29154.00134921074
  time_this_iter_s: 122.87631273269653
  time_total_s: 32365.06503534317
  timestamp: 1637046716
  timesteps_since_restore: 21696000
  timesteps_this_iter: 96000
  timesteps_total: 23616000
  training_iteration: 246
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    246 |          32365.1 | 23616000 |   910.81 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 0.75
    apples_agent-0_min: 0
    apples_agent-1_max: 98
    apples_agent-1_mean: 25.35
    apples_agent-1_min: 0
    apples_agent-2_max: 150
    apples_agent-2_mean: 17.0
    apples_agent-2_min: 0
    apples_agent-3_max: 165
    apples_agent-3_mean: 62.76
    apples_agent-3_min: 33
    apples_agent-4_max: 33
    apples_agent-4_mean: 0.78
    apples_agent-4_min: 0
    apples_agent-5_max: 190
    apples_agent-5_mean: 106.37
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 524
    cleaning_beam_agent-0_mean: 403.58
    cleaning_beam_agent-0_min: 257
    cleaning_beam_agent-1_max: 427
    cleaning_beam_agent-1_mean: 250.91
    cleaning_beam_agent-1_min: 135
    cleaning_beam_agent-2_max: 431
    cleaning_beam_agent-2_mean: 267.04
    cleaning_beam_agent-2_min: 87
    cleaning_beam_agent-3_max: 139
    cleaning_beam_agent-3_mean: 20.27
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 583
    cleaning_beam_agent-4_mean: 466.7
    cleaning_beam_agent-4_min: 329
    cleaning_beam_agent-5_max: 790
    cleaning_beam_agent-5_mean: 57.23
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-14-01
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1097.000000000002
  episode_reward_mean: 955.0099999999817
  episode_reward_min: 408.0000000000105
  episodes_this_iter: 96
  episodes_total: 23712
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20158.39
    learner:
      agent-0:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9587445855140686
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013099582865834236
        model: {}
        policy_loss: -0.003090578829869628
        total_loss: -0.0027382178232073784
        vf_explained_var: -0.01948218047618866
        vf_loss: 20.397533416748047
      agent-1:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 1.118939995765686
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012432814110070467
        model: {}
        policy_loss: -0.003920798189938068
        total_loss: -0.0037386566400527954
        vf_explained_var: -0.06608262658119202
        vf_loss: 21.514801025390625
      agent-2:
        cur_kl_coeff: 2.1316282919835953e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1440033912658691
        entropy_coeff: 0.0017600000137463212
        kl: 0.001301075448282063
        model: {}
        policy_loss: -0.003589993342757225
        total_loss: -0.003628500970080495
        vf_explained_var: 0.021108895540237427
        vf_loss: 19.74938201904297
      agent-3:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3562513291835785
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006826838362030685
        model: {}
        policy_loss: -0.0020484300330281258
        total_loss: -0.0008637567516416311
        vf_explained_var: 0.08519876003265381
        vf_loss: 18.116714477539062
      agent-4:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9096992015838623
        entropy_coeff: 0.0017600000137463212
        kl: 0.00158202787861228
        model: {}
        policy_loss: -0.003651745617389679
        total_loss: -0.0034130262210965157
        vf_explained_var: 0.07274752855300903
        vf_loss: 18.39786720275879
      agent-5:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7394243478775024
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009534515556879342
        model: {}
        policy_loss: -0.0033619150053709745
        total_loss: -0.0028153453022241592
        vf_explained_var: 0.06668239831924438
        vf_loss: 18.47957992553711
    load_time_ms: 14964.182
    num_steps_sampled: 23712000
    num_steps_trained: 23712000
    sample_time_ms: 90083.498
    update_time_ms: 21.194
  iterations_since_restore: 227
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.248044692737428
    ram_util_percent: 9.675977653631286
  pid: 4061
  policy_reward_max:
    agent-0: 182.833333333333
    agent-1: 182.833333333333
    agent-2: 182.833333333333
    agent-3: 182.833333333333
    agent-4: 182.833333333333
    agent-5: 182.833333333333
  policy_reward_mean:
    agent-0: 159.16833333333324
    agent-1: 159.16833333333324
    agent-2: 159.16833333333324
    agent-3: 159.16833333333324
    agent-4: 159.16833333333324
    agent-5: 159.16833333333324
  policy_reward_min:
    agent-0: 67.99999999999983
    agent-1: 67.99999999999983
    agent-2: 67.99999999999983
    agent-3: 67.99999999999983
    agent-4: 67.99999999999983
    agent-5: 67.99999999999983
  sampler_perf:
    mean_env_wait_ms: 23.79351279534134
    mean_inference_ms: 12.272970355801759
    mean_processing_ms: 50.70718995608588
  time_since_restore: 29279.519308805466
  time_this_iter_s: 125.51795959472656
  time_total_s: 32490.582994937897
  timestamp: 1637046841
  timesteps_since_restore: 21792000
  timesteps_this_iter: 96000
  timesteps_total: 23712000
  training_iteration: 247
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    247 |          32490.6 | 23712000 |   955.01 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 23
    apples_agent-0_mean: 1.25
    apples_agent-0_min: 0
    apples_agent-1_max: 96
    apples_agent-1_mean: 27.08
    apples_agent-1_min: 0
    apples_agent-2_max: 90
    apples_agent-2_mean: 10.87
    apples_agent-2_min: 0
    apples_agent-3_max: 148
    apples_agent-3_mean: 58.57
    apples_agent-3_min: 31
    apples_agent-4_max: 19
    apples_agent-4_mean: 0.36
    apples_agent-4_min: 0
    apples_agent-5_max: 193
    apples_agent-5_mean: 106.1
    apples_agent-5_min: 48
    cleaning_beam_agent-0_max: 524
    cleaning_beam_agent-0_mean: 406.37
    cleaning_beam_agent-0_min: 251
    cleaning_beam_agent-1_max: 403
    cleaning_beam_agent-1_mean: 242.52
    cleaning_beam_agent-1_min: 87
    cleaning_beam_agent-2_max: 447
    cleaning_beam_agent-2_mean: 277.61
    cleaning_beam_agent-2_min: 76
    cleaning_beam_agent-3_max: 67
    cleaning_beam_agent-3_mean: 19.71
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 542
    cleaning_beam_agent-4_mean: 455.83
    cleaning_beam_agent-4_min: 357
    cleaning_beam_agent-5_max: 468
    cleaning_beam_agent-5_mean: 58.27
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-16-07
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1091.999999999989
  episode_reward_mean: 944.449999999981
  episode_reward_min: 600.9999999999964
  episodes_this_iter: 96
  episodes_total: 23808
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20142.945
    learner:
      agent-0:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9671514630317688
        entropy_coeff: 0.0017600000137463212
        kl: 0.001600779825821519
        model: {}
        policy_loss: -0.0032514813356101513
        total_loss: -0.002984924241900444
        vf_explained_var: -0.026670485734939575
        vf_loss: 19.687461853027344
      agent-1:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1117827892303467
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011564521118998528
        model: {}
        policy_loss: -0.003837190568447113
        total_loss: -0.0037422601599246264
        vf_explained_var: -0.06618618965148926
        vf_loss: 20.516708374023438
      agent-2:
        cur_kl_coeff: 1.0658141459917976e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1433993577957153
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013356695417314768
        model: {}
        policy_loss: -0.003256271593272686
        total_loss: -0.0033479775302112103
        vf_explained_var: 0.0017155110836029053
        vf_loss: 19.20676612854004
      agent-3:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36373066902160645
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010703185107558966
        model: {}
        policy_loss: -0.002235227497294545
        total_loss: -0.0011581159196794033
        vf_explained_var: 0.0898408442735672
        vf_loss: 17.172765731811523
      agent-4:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9076474905014038
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017118004616349936
        model: {}
        policy_loss: -0.0036211563274264336
        total_loss: -0.003382500261068344
        vf_explained_var: 0.03674599528312683
        vf_loss: 18.361156463623047
      agent-5:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7499100565910339
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011464082635939121
        model: {}
        policy_loss: -0.003494404722005129
        total_loss: -0.0029841901268810034
        vf_explained_var: 0.03101964294910431
        vf_loss: 18.30052947998047
    load_time_ms: 14987.729
    num_steps_sampled: 23808000
    num_steps_trained: 23808000
    sample_time_ms: 90058.016
    update_time_ms: 20.981
  iterations_since_restore: 228
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.215642458100556
    ram_util_percent: 9.66927374301676
  pid: 4061
  policy_reward_max:
    agent-0: 181.99999999999986
    agent-1: 181.99999999999986
    agent-2: 181.99999999999986
    agent-3: 181.99999999999986
    agent-4: 181.99999999999986
    agent-5: 181.99999999999986
  policy_reward_mean:
    agent-0: 157.40833333333325
    agent-1: 157.40833333333325
    agent-2: 157.40833333333325
    agent-3: 157.40833333333325
    agent-4: 157.40833333333325
    agent-5: 157.40833333333325
  policy_reward_min:
    agent-0: 100.16666666666697
    agent-1: 100.16666666666697
    agent-2: 100.16666666666697
    agent-3: 100.16666666666697
    agent-4: 100.16666666666697
    agent-5: 100.16666666666697
  sampler_perf:
    mean_env_wait_ms: 23.796178941302866
    mean_inference_ms: 12.272573536522554
    mean_processing_ms: 50.70479162877518
  time_since_restore: 29404.865165233612
  time_this_iter_s: 125.34585642814636
  time_total_s: 32615.928851366043
  timestamp: 1637046967
  timesteps_since_restore: 21888000
  timesteps_this_iter: 96000
  timesteps_total: 23808000
  training_iteration: 248
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    248 |          32615.9 | 23808000 |   944.45 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 40
    apples_agent-0_mean: 2.95
    apples_agent-0_min: 0
    apples_agent-1_max: 94
    apples_agent-1_mean: 24.5
    apples_agent-1_min: 0
    apples_agent-2_max: 80
    apples_agent-2_mean: 13.97
    apples_agent-2_min: 0
    apples_agent-3_max: 128
    apples_agent-3_mean: 59.83
    apples_agent-3_min: 35
    apples_agent-4_max: 65
    apples_agent-4_mean: 1.5
    apples_agent-4_min: 0
    apples_agent-5_max: 171
    apples_agent-5_mean: 101.99
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 523
    cleaning_beam_agent-0_mean: 392.65
    cleaning_beam_agent-0_min: 249
    cleaning_beam_agent-1_max: 411
    cleaning_beam_agent-1_mean: 254.32
    cleaning_beam_agent-1_min: 130
    cleaning_beam_agent-2_max: 494
    cleaning_beam_agent-2_mean: 280.4
    cleaning_beam_agent-2_min: 95
    cleaning_beam_agent-3_max: 74
    cleaning_beam_agent-3_mean: 19.38
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 575
    cleaning_beam_agent-4_mean: 452.46
    cleaning_beam_agent-4_min: 298
    cleaning_beam_agent-5_max: 502
    cleaning_beam_agent-5_mean: 47.89
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-18-13
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1088.9999999999877
  episode_reward_mean: 904.0399999999815
  episode_reward_min: 353.00000000000165
  episodes_this_iter: 96
  episodes_total: 23904
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20182.118
    learner:
      agent-0:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9694392681121826
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013923855731263757
        model: {}
        policy_loss: -0.0033526578918099403
        total_loss: -0.003018407616764307
        vf_explained_var: 0.028633594512939453
        vf_loss: 20.404626846313477
      agent-1:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1312122344970703
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015809799078851938
        model: {}
        policy_loss: -0.0040817419067025185
        total_loss: -0.00394134595990181
        vf_explained_var: -0.015303447842597961
        vf_loss: 21.313283920288086
      agent-2:
        cur_kl_coeff: 5.329070729958988e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1265742778778076
        entropy_coeff: 0.0017600000137463212
        kl: 0.00207591918297112
        model: {}
        policy_loss: -0.0038953011389821768
        total_loss: -0.003871239721775055
        vf_explained_var: 0.044841423630714417
        vf_loss: 20.068315505981445
      agent-3:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4047859013080597
        entropy_coeff: 0.0017600000137463212
        kl: 0.001443964894860983
        model: {}
        policy_loss: -0.002535189501941204
        total_loss: -0.001528504304587841
        vf_explained_var: 0.1803790032863617
        vf_loss: 17.191064834594727
      agent-4:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9113714694976807
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015801014378666878
        model: {}
        policy_loss: -0.0037350011989474297
        total_loss: -0.003401157446205616
        vf_explained_var: 0.0763729065656662
        vf_loss: 19.37857437133789
      agent-5:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7734767198562622
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013394355773925781
        model: {}
        policy_loss: -0.0037601767107844353
        total_loss: -0.0032814827281981707
        vf_explained_var: 0.12412899732589722
        vf_loss: 18.40013313293457
    load_time_ms: 15160.668
    num_steps_sampled: 23904000
    num_steps_trained: 23904000
    sample_time_ms: 90020.746
    update_time_ms: 21.216
  iterations_since_restore: 229
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.162011173184357
    ram_util_percent: 9.550837988826817
  pid: 4061
  policy_reward_max:
    agent-0: 181.49999999999946
    agent-1: 181.49999999999946
    agent-2: 181.49999999999946
    agent-3: 181.49999999999946
    agent-4: 181.49999999999946
    agent-5: 181.49999999999946
  policy_reward_mean:
    agent-0: 150.67333333333332
    agent-1: 150.67333333333332
    agent-2: 150.67333333333332
    agent-3: 150.67333333333332
    agent-4: 150.67333333333332
    agent-5: 150.67333333333332
  policy_reward_min:
    agent-0: 58.83333333333321
    agent-1: 58.83333333333321
    agent-2: 58.83333333333321
    agent-3: 58.83333333333321
    agent-4: 58.83333333333321
    agent-5: 58.83333333333321
  sampler_perf:
    mean_env_wait_ms: 23.798195614806627
    mean_inference_ms: 12.271915279760956
    mean_processing_ms: 50.701276552555534
  time_since_restore: 29530.803239822388
  time_this_iter_s: 125.93807458877563
  time_total_s: 32741.86692595482
  timestamp: 1637047093
  timesteps_since_restore: 21984000
  timesteps_this_iter: 96000
  timesteps_total: 23904000
  training_iteration: 249
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    249 |          32741.9 | 23904000 |   904.04 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 42
    apples_agent-0_mean: 2.76
    apples_agent-0_min: 0
    apples_agent-1_max: 111
    apples_agent-1_mean: 24.82
    apples_agent-1_min: 0
    apples_agent-2_max: 54
    apples_agent-2_mean: 9.31
    apples_agent-2_min: 0
    apples_agent-3_max: 122
    apples_agent-3_mean: 60.38
    apples_agent-3_min: 29
    apples_agent-4_max: 81
    apples_agent-4_mean: 2.9
    apples_agent-4_min: 0
    apples_agent-5_max: 168
    apples_agent-5_mean: 100.01
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 542
    cleaning_beam_agent-0_mean: 396.07
    cleaning_beam_agent-0_min: 206
    cleaning_beam_agent-1_max: 379
    cleaning_beam_agent-1_mean: 244.41
    cleaning_beam_agent-1_min: 130
    cleaning_beam_agent-2_max: 501
    cleaning_beam_agent-2_mean: 308.42
    cleaning_beam_agent-2_min: 141
    cleaning_beam_agent-3_max: 117
    cleaning_beam_agent-3_mean: 21.57
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 604
    cleaning_beam_agent-4_mean: 454.63
    cleaning_beam_agent-4_min: 326
    cleaning_beam_agent-5_max: 940
    cleaning_beam_agent-5_mean: 74.31
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-20-21
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1099.0000000000002
  episode_reward_mean: 924.4699999999839
  episode_reward_min: 439.00000000000955
  episodes_this_iter: 96
  episodes_total: 24000
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20185.075
    learner:
      agent-0:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9662015438079834
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015131173422560096
        model: {}
        policy_loss: -0.00337878055870533
        total_loss: -0.0028703389689326286
        vf_explained_var: 0.017114296555519104
        vf_loss: 22.089574813842773
      agent-1:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 1.122499942779541
        entropy_coeff: 0.0017600000137463212
        kl: 0.00137199810706079
        model: {}
        policy_loss: -0.0037342084106057882
        total_loss: -0.003440853441134095
        vf_explained_var: -0.015566319227218628
        vf_loss: 22.68956184387207
      agent-2:
        cur_kl_coeff: 2.664535364979494e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1211189031600952
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014095852384343743
        model: {}
        policy_loss: -0.0036507039330899715
        total_loss: -0.0034538498148322105
        vf_explained_var: 0.02327914535999298
        vf_loss: 21.700239181518555
      agent-3:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39339056611061096
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008854609914124012
        model: {}
        policy_loss: -0.002257930813357234
        total_loss: -0.0010306916665285826
        vf_explained_var: 0.13403914868831635
        vf_loss: 19.196086883544922
      agent-4:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9127053618431091
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019961921498179436
        model: {}
        policy_loss: -0.003787062130868435
        total_loss: -0.0033705169335007668
        vf_explained_var: 0.09426422417163849
        vf_loss: 20.229045867919922
      agent-5:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7317631244659424
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012974117416888475
        model: {}
        policy_loss: -0.003465726040303707
        total_loss: -0.0028505660593509674
        vf_explained_var: 0.14654120802879333
        vf_loss: 19.0306339263916
    load_time_ms: 15421.273
    num_steps_sampled: 24000000
    num_steps_trained: 24000000
    sample_time_ms: 90177.361
    update_time_ms: 21.304
  iterations_since_restore: 230
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.884615384615385
    ram_util_percent: 9.68131868131868
  pid: 4061
  policy_reward_max:
    agent-0: 183.16666666666634
    agent-1: 183.16666666666634
    agent-2: 183.16666666666634
    agent-3: 183.16666666666634
    agent-4: 183.16666666666634
    agent-5: 183.16666666666634
  policy_reward_mean:
    agent-0: 154.0783333333333
    agent-1: 154.0783333333333
    agent-2: 154.0783333333333
    agent-3: 154.0783333333333
    agent-4: 154.0783333333333
    agent-5: 154.0783333333333
  policy_reward_min:
    agent-0: 73.16666666666663
    agent-1: 73.16666666666663
    agent-2: 73.16666666666663
    agent-3: 73.16666666666663
    agent-4: 73.16666666666663
    agent-5: 73.16666666666663
  sampler_perf:
    mean_env_wait_ms: 23.80172353114091
    mean_inference_ms: 12.271780872212508
    mean_processing_ms: 50.69872609849137
  time_since_restore: 29658.726611852646
  time_this_iter_s: 127.92337203025818
  time_total_s: 32869.79029798508
  timestamp: 1637047221
  timesteps_since_restore: 22080000
  timesteps_this_iter: 96000
  timesteps_total: 24000000
  training_iteration: 250
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    250 |          32869.8 | 24000000 |   924.47 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 55
    apples_agent-0_mean: 1.87
    apples_agent-0_min: 0
    apples_agent-1_max: 77
    apples_agent-1_mean: 25.58
    apples_agent-1_min: 0
    apples_agent-2_max: 102
    apples_agent-2_mean: 11.28
    apples_agent-2_min: 0
    apples_agent-3_max: 188
    apples_agent-3_mean: 64.29
    apples_agent-3_min: 31
    apples_agent-4_max: 79
    apples_agent-4_mean: 1.34
    apples_agent-4_min: 0
    apples_agent-5_max: 187
    apples_agent-5_mean: 104.01
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 481
    cleaning_beam_agent-0_mean: 388.34
    cleaning_beam_agent-0_min: 222
    cleaning_beam_agent-1_max: 380
    cleaning_beam_agent-1_mean: 243.09
    cleaning_beam_agent-1_min: 134
    cleaning_beam_agent-2_max: 485
    cleaning_beam_agent-2_mean: 304.77
    cleaning_beam_agent-2_min: 158
    cleaning_beam_agent-3_max: 130
    cleaning_beam_agent-3_mean: 18.65
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 575
    cleaning_beam_agent-4_mean: 465.41
    cleaning_beam_agent-4_min: 280
    cleaning_beam_agent-5_max: 637
    cleaning_beam_agent-5_mean: 49.57
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-22-27
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1090.9999999999984
  episode_reward_mean: 943.0599999999827
  episode_reward_min: 444.00000000000824
  episodes_this_iter: 96
  episodes_total: 24096
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20215.584
    learner:
      agent-0:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9682438969612122
        entropy_coeff: 0.0017600000137463212
        kl: 0.001631423132494092
        model: {}
        policy_loss: -0.003105809446424246
        total_loss: -0.0028372276574373245
        vf_explained_var: -0.023588597774505615
        vf_loss: 19.726882934570312
      agent-1:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1223607063293457
        entropy_coeff: 0.0017600000137463212
        kl: 0.001467759138904512
        model: {}
        policy_loss: -0.003744213841855526
        total_loss: -0.0037255091592669487
        vf_explained_var: -0.03672128915786743
        vf_loss: 19.940574645996094
      agent-2:
        cur_kl_coeff: 1.332267682489747e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1348233222961426
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012624477967619896
        model: {}
        policy_loss: -0.003640672191977501
        total_loss: -0.0037448895163834095
        vf_explained_var: 0.007098212838172913
        vf_loss: 18.93072509765625
      agent-3:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37297236919403076
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010620546527206898
        model: {}
        policy_loss: -0.002240197965875268
        total_loss: -0.001146427122876048
        vf_explained_var: 0.07428810000419617
        vf_loss: 17.502050399780273
      agent-4:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8994354605674744
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020918697118759155
        model: {}
        policy_loss: -0.003739920910447836
        total_loss: -0.0034697605296969414
        vf_explained_var: 0.030326247215270996
        vf_loss: 18.53165054321289
      agent-5:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7464179992675781
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010430226102471352
        model: {}
        policy_loss: -0.003362232819199562
        total_loss: -0.0028695352375507355
        vf_explained_var: 0.0460590124130249
        vf_loss: 18.06395721435547
    load_time_ms: 15349.603
    num_steps_sampled: 24096000
    num_steps_trained: 24096000
    sample_time_ms: 89893.756
    update_time_ms: 21.221
  iterations_since_restore: 231
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.160893854748604
    ram_util_percent: 9.678212290502794
  pid: 4061
  policy_reward_max:
    agent-0: 181.83333333333277
    agent-1: 181.83333333333277
    agent-2: 181.83333333333277
    agent-3: 181.83333333333277
    agent-4: 181.83333333333277
    agent-5: 181.83333333333277
  policy_reward_mean:
    agent-0: 157.17666666666665
    agent-1: 157.17666666666665
    agent-2: 157.17666666666665
    agent-3: 157.17666666666665
    agent-4: 157.17666666666665
    agent-5: 157.17666666666665
  policy_reward_min:
    agent-0: 73.99999999999993
    agent-1: 73.99999999999993
    agent-2: 73.99999999999993
    agent-3: 73.99999999999993
    agent-4: 73.99999999999993
    agent-5: 73.99999999999993
  sampler_perf:
    mean_env_wait_ms: 23.80459139796153
    mean_inference_ms: 12.271248084183739
    mean_processing_ms: 50.696488152249046
  time_since_restore: 29784.03714108467
  time_this_iter_s: 125.31052923202515
  time_total_s: 32995.1008272171
  timestamp: 1637047347
  timesteps_since_restore: 22176000
  timesteps_this_iter: 96000
  timesteps_total: 24096000
  training_iteration: 251
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    251 |          32995.1 | 24096000 |   943.06 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 51
    apples_agent-0_mean: 2.94
    apples_agent-0_min: 0
    apples_agent-1_max: 90
    apples_agent-1_mean: 23.34
    apples_agent-1_min: 0
    apples_agent-2_max: 103
    apples_agent-2_mean: 14.72
    apples_agent-2_min: 0
    apples_agent-3_max: 108
    apples_agent-3_mean: 62.28
    apples_agent-3_min: 31
    apples_agent-4_max: 27
    apples_agent-4_mean: 0.87
    apples_agent-4_min: 0
    apples_agent-5_max: 179
    apples_agent-5_mean: 100.77
    apples_agent-5_min: 21
    cleaning_beam_agent-0_max: 584
    cleaning_beam_agent-0_mean: 391.86
    cleaning_beam_agent-0_min: 207
    cleaning_beam_agent-1_max: 387
    cleaning_beam_agent-1_mean: 246.3
    cleaning_beam_agent-1_min: 119
    cleaning_beam_agent-2_max: 489
    cleaning_beam_agent-2_mean: 292.12
    cleaning_beam_agent-2_min: 104
    cleaning_beam_agent-3_max: 113
    cleaning_beam_agent-3_mean: 19.35
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 569
    cleaning_beam_agent-4_mean: 469.69
    cleaning_beam_agent-4_min: 360
    cleaning_beam_agent-5_max: 631
    cleaning_beam_agent-5_mean: 78.81
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-24-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1092.9999999999968
  episode_reward_mean: 918.1999999999827
  episode_reward_min: 456.00000000001296
  episodes_this_iter: 96
  episodes_total: 24192
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20214.096
    learner:
      agent-0:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9414402842521667
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013852353440597653
        model: {}
        policy_loss: -0.003401972819119692
        total_loss: -0.00304656894877553
        vf_explained_var: 0.021761178970336914
        vf_loss: 20.12339973449707
      agent-1:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1310096979141235
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011619110591709614
        model: {}
        policy_loss: -0.003893566317856312
        total_loss: -0.003809379879385233
        vf_explained_var: -0.014932781457901001
        vf_loss: 20.74762535095215
      agent-2:
        cur_kl_coeff: 6.661338412448735e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 1.138102650642395
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016357738059014082
        model: {}
        policy_loss: -0.004128879401832819
        total_loss: -0.004131550434976816
        vf_explained_var: 0.02045208215713501
        vf_loss: 20.003904342651367
      agent-3:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3875623643398285
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005151837249286473
        model: {}
        policy_loss: -0.002053371164947748
        total_loss: -0.0009061327436938882
        vf_explained_var: 0.10253781080245972
        vf_loss: 18.293474197387695
      agent-4:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8939371109008789
        entropy_coeff: 0.0017600000137463212
        kl: 0.00207387818954885
        model: {}
        policy_loss: -0.0041060070507228374
        total_loss: -0.003768285270780325
        vf_explained_var: 0.06254011392593384
        vf_loss: 19.11055564880371
      agent-5:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7563542127609253
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011137766996398568
        model: {}
        policy_loss: -0.0036059804260730743
        total_loss: -0.0030264011584222317
        vf_explained_var: 0.0663267970085144
        vf_loss: 19.10763168334961
    load_time_ms: 15483.046
    num_steps_sampled: 24192000
    num_steps_trained: 24192000
    sample_time_ms: 89890.231
    update_time_ms: 21.199
  iterations_since_restore: 232
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.162011173184357
    ram_util_percent: 9.668156424581005
  pid: 4061
  policy_reward_max:
    agent-0: 182.16666666666674
    agent-1: 182.16666666666674
    agent-2: 182.16666666666674
    agent-3: 182.16666666666674
    agent-4: 182.16666666666674
    agent-5: 182.16666666666674
  policy_reward_mean:
    agent-0: 153.03333333333333
    agent-1: 153.03333333333333
    agent-2: 153.03333333333333
    agent-3: 153.03333333333333
    agent-4: 153.03333333333333
    agent-5: 153.03333333333333
  policy_reward_min:
    agent-0: 76.00000000000004
    agent-1: 76.00000000000004
    agent-2: 76.00000000000004
    agent-3: 76.00000000000004
    agent-4: 76.00000000000004
    agent-5: 76.00000000000004
  sampler_perf:
    mean_env_wait_ms: 23.807302456949788
    mean_inference_ms: 12.270523010257389
    mean_processing_ms: 50.69425281594269
  time_since_restore: 29909.55572628975
  time_this_iter_s: 125.51858520507812
  time_total_s: 33120.61941242218
  timestamp: 1637047472
  timesteps_since_restore: 22272000
  timesteps_this_iter: 96000
  timesteps_total: 24192000
  training_iteration: 252
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 17.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    252 |          33120.6 | 24192000 |    918.2 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 25
    apples_agent-0_mean: 1.66
    apples_agent-0_min: 0
    apples_agent-1_max: 85
    apples_agent-1_mean: 28.77
    apples_agent-1_min: 0
    apples_agent-2_max: 140
    apples_agent-2_mean: 11.93
    apples_agent-2_min: 0
    apples_agent-3_max: 168
    apples_agent-3_mean: 61.39
    apples_agent-3_min: 23
    apples_agent-4_max: 41
    apples_agent-4_mean: 1.78
    apples_agent-4_min: 0
    apples_agent-5_max: 162
    apples_agent-5_mean: 102.46
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 473
    cleaning_beam_agent-0_mean: 384.45
    cleaning_beam_agent-0_min: 272
    cleaning_beam_agent-1_max: 360
    cleaning_beam_agent-1_mean: 233.29
    cleaning_beam_agent-1_min: 49
    cleaning_beam_agent-2_max: 557
    cleaning_beam_agent-2_mean: 293.45
    cleaning_beam_agent-2_min: 108
    cleaning_beam_agent-3_max: 125
    cleaning_beam_agent-3_mean: 16.95
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 598
    cleaning_beam_agent-4_mean: 480.79
    cleaning_beam_agent-4_min: 362
    cleaning_beam_agent-5_max: 594
    cleaning_beam_agent-5_mean: 68.32
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-26-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1084.999999999988
  episode_reward_mean: 933.5199999999817
  episode_reward_min: 217.99999999999756
  episodes_this_iter: 96
  episodes_total: 24288
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20216.576
    learner:
      agent-0:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9512389898300171
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021166380029171705
        model: {}
        policy_loss: -0.0033880751579999924
        total_loss: -0.0029655219987034798
        vf_explained_var: -0.025569528341293335
        vf_loss: 20.967329025268555
      agent-1:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1272958517074585
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024240207858383656
        model: {}
        policy_loss: -0.004263083916157484
        total_loss: -0.004147791303694248
        vf_explained_var: -0.02420850098133087
        vf_loss: 20.993328094482422
      agent-2:
        cur_kl_coeff: 3.3306692062243676e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1351616382598877
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015362170524895191
        model: {}
        policy_loss: -0.003515853313729167
        total_loss: -0.003488462418317795
        vf_explained_var: 0.003415510058403015
        vf_loss: 20.25274658203125
      agent-3:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38094329833984375
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010743137681856751
        model: {}
        policy_loss: -0.0024019419215619564
        total_loss: -0.0012260149233043194
        vf_explained_var: 0.08614273369312286
        vf_loss: 18.463865280151367
      agent-4:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8867784738540649
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015926067717373371
        model: {}
        policy_loss: -0.003689431119710207
        total_loss: -0.0033000661060214043
        vf_explained_var: 0.039435938000679016
        vf_loss: 19.500959396362305
      agent-5:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7360518574714661
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009620758355595171
        model: {}
        policy_loss: -0.0033752317540347576
        total_loss: -0.0028342632576823235
        vf_explained_var: 0.09734629094600677
        vf_loss: 18.364192962646484
    load_time_ms: 15668.963
    num_steps_sampled: 24288000
    num_steps_trained: 24288000
    sample_time_ms: 89721.422
    update_time_ms: 20.969
  iterations_since_restore: 233
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.205617977528092
    ram_util_percent: 9.610112359550564
  pid: 4061
  policy_reward_max:
    agent-0: 180.83333333333297
    agent-1: 180.83333333333297
    agent-2: 180.83333333333297
    agent-3: 180.83333333333297
    agent-4: 180.83333333333297
    agent-5: 180.83333333333297
  policy_reward_mean:
    agent-0: 155.58666666666656
    agent-1: 155.58666666666656
    agent-2: 155.58666666666656
    agent-3: 155.58666666666656
    agent-4: 155.58666666666656
    agent-5: 155.58666666666656
  policy_reward_min:
    agent-0: 36.33333333333333
    agent-1: 36.33333333333333
    agent-2: 36.33333333333333
    agent-3: 36.33333333333333
    agent-4: 36.33333333333333
    agent-5: 36.33333333333333
  sampler_perf:
    mean_env_wait_ms: 23.810048714261555
    mean_inference_ms: 12.269936661178757
    mean_processing_ms: 50.69052858512896
  time_since_restore: 30034.712257623672
  time_this_iter_s: 125.15653133392334
  time_total_s: 33245.7759437561
  timestamp: 1637047598
  timesteps_since_restore: 22368000
  timesteps_this_iter: 96000
  timesteps_total: 24288000
  training_iteration: 253
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    253 |          33245.8 | 24288000 |   933.52 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 59
    apples_agent-0_mean: 1.72
    apples_agent-0_min: 0
    apples_agent-1_max: 87
    apples_agent-1_mean: 24.87
    apples_agent-1_min: 0
    apples_agent-2_max: 93
    apples_agent-2_mean: 11.33
    apples_agent-2_min: 0
    apples_agent-3_max: 109
    apples_agent-3_mean: 57.8
    apples_agent-3_min: 27
    apples_agent-4_max: 69
    apples_agent-4_mean: 2.6
    apples_agent-4_min: 0
    apples_agent-5_max: 160
    apples_agent-5_mean: 102.72
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 493
    cleaning_beam_agent-0_mean: 367.85
    cleaning_beam_agent-0_min: 180
    cleaning_beam_agent-1_max: 379
    cleaning_beam_agent-1_mean: 234.25
    cleaning_beam_agent-1_min: 87
    cleaning_beam_agent-2_max: 505
    cleaning_beam_agent-2_mean: 296.96
    cleaning_beam_agent-2_min: 111
    cleaning_beam_agent-3_max: 103
    cleaning_beam_agent-3_mean: 18.79
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 623
    cleaning_beam_agent-4_mean: 490.83
    cleaning_beam_agent-4_min: 259
    cleaning_beam_agent-5_max: 455
    cleaning_beam_agent-5_mean: 51.95
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-28-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1092.9999999999995
  episode_reward_mean: 926.6299999999834
  episode_reward_min: 383.0000000000029
  episodes_this_iter: 96
  episodes_total: 24384
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20211.205
    learner:
      agent-0:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9544696807861328
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011264365166425705
        model: {}
        policy_loss: -0.002907136455178261
        total_loss: -0.0024616774171590805
        vf_explained_var: -0.0124741792678833
        vf_loss: 21.253263473510742
      agent-1:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1218870878219604
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011192235397174954
        model: {}
        policy_loss: -0.004084281623363495
        total_loss: -0.003886827500537038
        vf_explained_var: -0.03326290845870972
        vf_loss: 21.719755172729492
      agent-2:
        cur_kl_coeff: 1.6653346031121838e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1435409784317017
        entropy_coeff: 0.0017600000137463212
        kl: 0.00123319320846349
        model: {}
        policy_loss: -0.0033292165026068687
        total_loss: -0.003258001757785678
        vf_explained_var: 0.002349287271499634
        vf_loss: 20.83844566345215
      agent-3:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.380548894405365
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013265409506857395
        model: {}
        policy_loss: -0.002546863164752722
        total_loss: -0.0014115567319095135
        vf_explained_var: 0.13128088414669037
        vf_loss: 18.05072784423828
      agent-4:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8803364038467407
        entropy_coeff: 0.0017600000137463212
        kl: 0.001751606585457921
        model: {}
        policy_loss: -0.0034722338896244764
        total_loss: -0.0031209802255034447
        vf_explained_var: 0.08893978595733643
        vf_loss: 19.006454467773438
      agent-5:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7507703304290771
        entropy_coeff: 0.0017600000137463212
        kl: 0.001327774254605174
        model: {}
        policy_loss: -0.0034492546692490578
        total_loss: -0.00289283599704504
        vf_explained_var: 0.09862269461154938
        vf_loss: 18.777751922607422
    load_time_ms: 15542.841
    num_steps_sampled: 24384000
    num_steps_trained: 24384000
    sample_time_ms: 89671.337
    update_time_ms: 21.431
  iterations_since_restore: 234
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.307344632768364
    ram_util_percent: 9.67005649717514
  pid: 4061
  policy_reward_max:
    agent-0: 182.16666666666654
    agent-1: 182.16666666666654
    agent-2: 182.16666666666654
    agent-3: 182.16666666666654
    agent-4: 182.16666666666654
    agent-5: 182.16666666666654
  policy_reward_mean:
    agent-0: 154.4383333333333
    agent-1: 154.4383333333333
    agent-2: 154.4383333333333
    agent-3: 154.4383333333333
    agent-4: 154.4383333333333
    agent-5: 154.4383333333333
  policy_reward_min:
    agent-0: 63.83333333333313
    agent-1: 63.83333333333313
    agent-2: 63.83333333333313
    agent-3: 63.83333333333313
    agent-4: 63.83333333333313
    agent-5: 63.83333333333313
  sampler_perf:
    mean_env_wait_ms: 23.81221809995817
    mean_inference_ms: 12.269348575570259
    mean_processing_ms: 50.68772369548943
  time_since_restore: 30158.826486587524
  time_this_iter_s: 124.11422896385193
  time_total_s: 33369.890172719955
  timestamp: 1637047722
  timesteps_since_restore: 22464000
  timesteps_this_iter: 96000
  timesteps_total: 24384000
  training_iteration: 254
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    254 |          33369.9 | 24384000 |   926.63 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 33
    apples_agent-0_mean: 2.49
    apples_agent-0_min: 0
    apples_agent-1_max: 90
    apples_agent-1_mean: 24.16
    apples_agent-1_min: 0
    apples_agent-2_max: 111
    apples_agent-2_mean: 14.93
    apples_agent-2_min: 0
    apples_agent-3_max: 104
    apples_agent-3_mean: 58.04
    apples_agent-3_min: 26
    apples_agent-4_max: 31
    apples_agent-4_mean: 0.61
    apples_agent-4_min: 0
    apples_agent-5_max: 200
    apples_agent-5_mean: 94.96
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 475
    cleaning_beam_agent-0_mean: 361.35
    cleaning_beam_agent-0_min: 259
    cleaning_beam_agent-1_max: 388
    cleaning_beam_agent-1_mean: 244.58
    cleaning_beam_agent-1_min: 90
    cleaning_beam_agent-2_max: 498
    cleaning_beam_agent-2_mean: 272.78
    cleaning_beam_agent-2_min: 105
    cleaning_beam_agent-3_max: 85
    cleaning_beam_agent-3_mean: 15.18
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 591
    cleaning_beam_agent-4_mean: 485.9
    cleaning_beam_agent-4_min: 347
    cleaning_beam_agent-5_max: 675
    cleaning_beam_agent-5_mean: 80.4
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-30-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1103.9999999999786
  episode_reward_mean: 915.4599999999816
  episode_reward_min: 570.0000000000048
  episodes_this_iter: 96
  episodes_total: 24480
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20227.058
    learner:
      agent-0:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.974541187286377
        entropy_coeff: 0.0017600000137463212
        kl: 0.002137881936505437
        model: {}
        policy_loss: -0.0034920373000204563
        total_loss: -0.0032927365973591805
        vf_explained_var: -0.021937400102615356
        vf_loss: 19.144954681396484
      agent-1:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1326956748962402
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017071733018383384
        model: {}
        policy_loss: -0.0039129238575696945
        total_loss: -0.0039469958283007145
        vf_explained_var: -0.0459522008895874
        vf_loss: 19.5947322845459
      agent-2:
        cur_kl_coeff: 8.326673015560919e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1484707593917847
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019498364999890327
        model: {}
        policy_loss: -0.00360308145172894
        total_loss: -0.0038175731897354126
        vf_explained_var: 0.035643160343170166
        vf_loss: 18.06816864013672
      agent-3:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3773503005504608
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007687522447668016
        model: {}
        policy_loss: -0.002214828971773386
        total_loss: -0.0011811694130301476
        vf_explained_var: 0.09117165207862854
        vf_loss: 16.97797393798828
      agent-4:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8875135779380798
        entropy_coeff: 0.0017600000137463212
        kl: 0.002221281873062253
        model: {}
        policy_loss: -0.003790035843849182
        total_loss: -0.0035615521483123302
        vf_explained_var: 0.04091627895832062
        vf_loss: 17.905071258544922
      agent-5:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7382248044013977
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013656559167429805
        model: {}
        policy_loss: -0.0034426474012434483
        total_loss: -0.0030001099221408367
        vf_explained_var: 0.07010039687156677
        vf_loss: 17.418106079101562
    load_time_ms: 15191.622
    num_steps_sampled: 24480000
    num_steps_trained: 24480000
    sample_time_ms: 89577.557
    update_time_ms: 21.682
  iterations_since_restore: 235
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 17.10625
    ram_util_percent: 9.648295454545455
  pid: 4061
  policy_reward_max:
    agent-0: 183.99999999999966
    agent-1: 183.99999999999966
    agent-2: 183.99999999999966
    agent-3: 183.99999999999966
    agent-4: 183.99999999999966
    agent-5: 183.99999999999966
  policy_reward_mean:
    agent-0: 152.57666666666668
    agent-1: 152.57666666666668
    agent-2: 152.57666666666668
    agent-3: 152.57666666666668
    agent-4: 152.57666666666668
    agent-5: 152.57666666666668
  policy_reward_min:
    agent-0: 95.0000000000002
    agent-1: 95.0000000000002
    agent-2: 95.0000000000002
    agent-3: 95.0000000000002
    agent-4: 95.0000000000002
    agent-5: 95.0000000000002
  sampler_perf:
    mean_env_wait_ms: 23.815101588306483
    mean_inference_ms: 12.268945537424264
    mean_processing_ms: 50.685803230550704
  time_since_restore: 30282.289977550507
  time_this_iter_s: 123.46349096298218
  time_total_s: 33493.35366368294
  timestamp: 1637047845
  timesteps_since_restore: 22560000
  timesteps_this_iter: 96000
  timesteps_total: 24480000
  training_iteration: 255
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 17.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    255 |          33493.4 | 24480000 |   915.46 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 68
    apples_agent-0_mean: 2.7
    apples_agent-0_min: 0
    apples_agent-1_max: 77
    apples_agent-1_mean: 20.4
    apples_agent-1_min: 0
    apples_agent-2_max: 131
    apples_agent-2_mean: 14.13
    apples_agent-2_min: 0
    apples_agent-3_max: 163
    apples_agent-3_mean: 59.71
    apples_agent-3_min: 30
    apples_agent-4_max: 48
    apples_agent-4_mean: 0.82
    apples_agent-4_min: 0
    apples_agent-5_max: 235
    apples_agent-5_mean: 103.18
    apples_agent-5_min: 22
    cleaning_beam_agent-0_max: 491
    cleaning_beam_agent-0_mean: 375.95
    cleaning_beam_agent-0_min: 197
    cleaning_beam_agent-1_max: 400
    cleaning_beam_agent-1_mean: 248.46
    cleaning_beam_agent-1_min: 99
    cleaning_beam_agent-2_max: 487
    cleaning_beam_agent-2_mean: 279.92
    cleaning_beam_agent-2_min: 127
    cleaning_beam_agent-3_max: 48
    cleaning_beam_agent-3_mean: 13.54
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 592
    cleaning_beam_agent-4_mean: 488.26
    cleaning_beam_agent-4_min: 381
    cleaning_beam_agent-5_max: 542
    cleaning_beam_agent-5_mean: 62.24
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-32-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1127.0000000000055
  episode_reward_mean: 939.8699999999792
  episode_reward_min: 320.0000000000016
  episodes_this_iter: 96
  episodes_total: 24576
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20224.681
    learner:
      agent-0:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9703817963600159
        entropy_coeff: 0.0017600000137463212
        kl: 0.002137411618605256
        model: {}
        policy_loss: -0.003514643060043454
        total_loss: -0.003098314395174384
        vf_explained_var: -0.04342535138130188
        vf_loss: 21.242023468017578
      agent-1:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1414151191711426
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019494035514071584
        model: {}
        policy_loss: -0.003957331646233797
        total_loss: -0.003910749219357967
        vf_explained_var: -0.010074377059936523
        vf_loss: 20.554729461669922
      agent-2:
        cur_kl_coeff: 4.1633365077804595e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1567184925079346
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014161732979118824
        model: {}
        policy_loss: -0.0037971532437950373
        total_loss: -0.0038431999273598194
        vf_explained_var: 0.024199381470680237
        vf_loss: 19.89778709411621
      agent-3:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36714163422584534
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008203657926060259
        model: {}
        policy_loss: -0.0022851135581731796
        total_loss: -0.001142050139605999
        vf_explained_var: 0.11184848845005035
        vf_loss: 17.892314910888672
      agent-4:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.884747326374054
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015963204205036163
        model: {}
        policy_loss: -0.003829202614724636
        total_loss: -0.003467850387096405
        vf_explained_var: 0.055541858077049255
        vf_loss: 19.18507957458496
      agent-5:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7303925156593323
        entropy_coeff: 0.0017600000137463212
        kl: 0.000779667985625565
        model: {}
        policy_loss: -0.003518470097333193
        total_loss: -0.0029801088385283947
        vf_explained_var: 0.10328121483325958
        vf_loss: 18.238513946533203
    load_time_ms: 15377.119
    num_steps_sampled: 24576000
    num_steps_trained: 24576000
    sample_time_ms: 89621.731
    update_time_ms: 21.564
  iterations_since_restore: 236
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.35195530726257
    ram_util_percent: 9.705027932960894
  pid: 4061
  policy_reward_max:
    agent-0: 187.83333333333263
    agent-1: 187.83333333333263
    agent-2: 187.83333333333263
    agent-3: 187.83333333333263
    agent-4: 187.83333333333263
    agent-5: 187.83333333333263
  policy_reward_mean:
    agent-0: 156.64499999999995
    agent-1: 156.64499999999995
    agent-2: 156.64499999999995
    agent-3: 156.64499999999995
    agent-4: 156.64499999999995
    agent-5: 156.64499999999995
  policy_reward_min:
    agent-0: 53.33333333333315
    agent-1: 53.33333333333315
    agent-2: 53.33333333333315
    agent-3: 53.33333333333315
    agent-4: 53.33333333333315
    agent-5: 53.33333333333315
  sampler_perf:
    mean_env_wait_ms: 23.81777890556118
    mean_inference_ms: 12.268326557400885
    mean_processing_ms: 50.68302422045024
  time_since_restore: 30407.43623495102
  time_this_iter_s: 125.1462574005127
  time_total_s: 33618.49992108345
  timestamp: 1637047971
  timesteps_since_restore: 22656000
  timesteps_this_iter: 96000
  timesteps_total: 24576000
  training_iteration: 256
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    256 |          33618.5 | 24576000 |   939.87 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 41
    apples_agent-0_mean: 2.46
    apples_agent-0_min: 0
    apples_agent-1_max: 87
    apples_agent-1_mean: 26.14
    apples_agent-1_min: 0
    apples_agent-2_max: 110
    apples_agent-2_mean: 15.08
    apples_agent-2_min: 0
    apples_agent-3_max: 127
    apples_agent-3_mean: 62.28
    apples_agent-3_min: 35
    apples_agent-4_max: 40
    apples_agent-4_mean: 1.39
    apples_agent-4_min: 0
    apples_agent-5_max: 173
    apples_agent-5_mean: 100.51
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 523
    cleaning_beam_agent-0_mean: 366.48
    cleaning_beam_agent-0_min: 253
    cleaning_beam_agent-1_max: 373
    cleaning_beam_agent-1_mean: 238.95
    cleaning_beam_agent-1_min: 93
    cleaning_beam_agent-2_max: 476
    cleaning_beam_agent-2_mean: 288.05
    cleaning_beam_agent-2_min: 123
    cleaning_beam_agent-3_max: 91
    cleaning_beam_agent-3_mean: 16.61
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 571
    cleaning_beam_agent-4_mean: 484.36
    cleaning_beam_agent-4_min: 267
    cleaning_beam_agent-5_max: 697
    cleaning_beam_agent-5_mean: 70.8
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-34-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1101.0000000000039
  episode_reward_mean: 923.5199999999821
  episode_reward_min: 445.00000000000944
  episodes_this_iter: 96
  episodes_total: 24672
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20207.91
    learner:
      agent-0:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9734140634536743
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016272130887955427
        model: {}
        policy_loss: -0.0032475460320711136
        total_loss: -0.0029501013923436403
        vf_explained_var: -0.00039827823638916016
        vf_loss: 20.106515884399414
      agent-1:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1283878087997437
        entropy_coeff: 0.0017600000137463212
        kl: 0.001679025823250413
        model: {}
        policy_loss: -0.003802807303145528
        total_loss: -0.0037421088200062513
        vf_explained_var: -0.020119696855545044
        vf_loss: 20.46664047241211
      agent-2:
        cur_kl_coeff: 2.0816682538902298e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1509112119674683
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012564040953293443
        model: {}
        policy_loss: -0.003600185038521886
        total_loss: -0.003659723559394479
        vf_explained_var: 0.024180203676223755
        vf_loss: 19.660667419433594
      agent-3:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3742802143096924
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008234341512434185
        model: {}
        policy_loss: -0.0022656982764601707
        total_loss: -0.0011712375562638044
        vf_explained_var: 0.12255831062793732
        vf_loss: 17.53192901611328
      agent-4:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9004406929016113
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015847082249820232
        model: {}
        policy_loss: -0.00401284359395504
        total_loss: -0.0037032896652817726
        vf_explained_var: 0.05390903353691101
        vf_loss: 18.94327735900879
      agent-5:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.726851761341095
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010650450130924582
        model: {}
        policy_loss: -0.0035756882280111313
        total_loss: -0.003063515294343233
        vf_explained_var: 0.10832276940345764
        vf_loss: 17.914325714111328
    load_time_ms: 15213.479
    num_steps_sampled: 24672000
    num_steps_trained: 24672000
    sample_time_ms: 89628.474
    update_time_ms: 21.657
  iterations_since_restore: 237
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.323863636363637
    ram_util_percent: 9.653977272727273
  pid: 4061
  policy_reward_max:
    agent-0: 183.49999999999991
    agent-1: 183.49999999999991
    agent-2: 183.49999999999991
    agent-3: 183.49999999999991
    agent-4: 183.49999999999991
    agent-5: 183.49999999999991
  policy_reward_mean:
    agent-0: 153.92
    agent-1: 153.92
    agent-2: 153.92
    agent-3: 153.92
    agent-4: 153.92
    agent-5: 153.92
  policy_reward_min:
    agent-0: 74.16666666666677
    agent-1: 74.16666666666677
    agent-2: 74.16666666666677
    agent-3: 74.16666666666677
    agent-4: 74.16666666666677
    agent-5: 74.16666666666677
  sampler_perf:
    mean_env_wait_ms: 23.819953595214084
    mean_inference_ms: 12.267844834054602
    mean_processing_ms: 50.679701525859215
  time_since_restore: 30531.238270759583
  time_this_iter_s: 123.80203580856323
  time_total_s: 33742.30195689201
  timestamp: 1637048095
  timesteps_since_restore: 22752000
  timesteps_this_iter: 96000
  timesteps_total: 24672000
  training_iteration: 257
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    257 |          33742.3 | 24672000 |   923.52 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 46
    apples_agent-0_mean: 1.91
    apples_agent-0_min: 0
    apples_agent-1_max: 105
    apples_agent-1_mean: 29.2
    apples_agent-1_min: 0
    apples_agent-2_max: 56
    apples_agent-2_mean: 10.46
    apples_agent-2_min: 0
    apples_agent-3_max: 112
    apples_agent-3_mean: 62.11
    apples_agent-3_min: 28
    apples_agent-4_max: 76
    apples_agent-4_mean: 1.3
    apples_agent-4_min: 0
    apples_agent-5_max: 192
    apples_agent-5_mean: 94.88
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 489
    cleaning_beam_agent-0_mean: 364.22
    cleaning_beam_agent-0_min: 241
    cleaning_beam_agent-1_max: 391
    cleaning_beam_agent-1_mean: 235.61
    cleaning_beam_agent-1_min: 95
    cleaning_beam_agent-2_max: 464
    cleaning_beam_agent-2_mean: 281.72
    cleaning_beam_agent-2_min: 109
    cleaning_beam_agent-3_max: 83
    cleaning_beam_agent-3_mean: 19.35
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 576
    cleaning_beam_agent-4_mean: 468.75
    cleaning_beam_agent-4_min: 292
    cleaning_beam_agent-5_max: 728
    cleaning_beam_agent-5_mean: 72.06
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-37-00
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1132.9999999999875
  episode_reward_mean: 902.1499999999818
  episode_reward_min: 421.00000000000875
  episodes_this_iter: 96
  episodes_total: 24768
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20215.359
    learner:
      agent-0:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9692674279212952
        entropy_coeff: 0.0017600000137463212
        kl: 0.001072435174137354
        model: {}
        policy_loss: -0.0032047582790255547
        total_loss: -0.0027227774262428284
        vf_explained_var: -0.017254918813705444
        vf_loss: 21.878881454467773
      agent-1:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1244282722473145
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020227222703397274
        model: {}
        policy_loss: -0.004208903759717941
        total_loss: -0.004024911671876907
        vf_explained_var: -0.005421683192253113
        vf_loss: 21.62984275817871
      agent-2:
        cur_kl_coeff: 1.0408341269451149e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1646173000335693
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013931626453995705
        model: {}
        policy_loss: -0.003826539497822523
        total_loss: -0.003814470488578081
        vf_explained_var: 0.041188791394233704
        vf_loss: 20.61795425415039
      agent-3:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39991670846939087
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008397923083975911
        model: {}
        policy_loss: -0.0022874739952385426
        total_loss: -0.0011821943335235119
        vf_explained_var: 0.15807344019412994
        vf_loss: 18.091333389282227
      agent-4:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8996282815933228
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015576074365526438
        model: {}
        policy_loss: -0.0038374890573322773
        total_loss: -0.0034303083084523678
        vf_explained_var: 0.07378548383712769
        vf_loss: 19.905254364013672
      agent-5:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.732239842414856
        entropy_coeff: 0.0017600000137463212
        kl: 0.001284975209273398
        model: {}
        policy_loss: -0.0036400784738361835
        total_loss: -0.0029877168126404285
        vf_explained_var: 0.09757566452026367
        vf_loss: 19.411043167114258
    load_time_ms: 15082.32
    num_steps_sampled: 24768000
    num_steps_trained: 24768000
    sample_time_ms: 89679.754
    update_time_ms: 22.011
  iterations_since_restore: 238
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.267415730337081
    ram_util_percent: 9.653370786516854
  pid: 4061
  policy_reward_max:
    agent-0: 188.833333333333
    agent-1: 188.833333333333
    agent-2: 188.833333333333
    agent-3: 188.833333333333
    agent-4: 188.833333333333
    agent-5: 188.833333333333
  policy_reward_mean:
    agent-0: 150.35833333333338
    agent-1: 150.35833333333338
    agent-2: 150.35833333333338
    agent-3: 150.35833333333338
    agent-4: 150.35833333333338
    agent-5: 150.35833333333338
  policy_reward_min:
    agent-0: 70.16666666666666
    agent-1: 70.16666666666666
    agent-2: 70.16666666666666
    agent-3: 70.16666666666666
    agent-4: 70.16666666666666
    agent-5: 70.16666666666666
  sampler_perf:
    mean_env_wait_ms: 23.822678079765293
    mean_inference_ms: 12.267416817173286
    mean_processing_ms: 50.678262368666495
  time_since_restore: 30655.931770324707
  time_this_iter_s: 124.69349956512451
  time_total_s: 33866.99545645714
  timestamp: 1637048220
  timesteps_since_restore: 22848000
  timesteps_this_iter: 96000
  timesteps_total: 24768000
  training_iteration: 258
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    258 |            33867 | 24768000 |   902.15 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 55
    apples_agent-0_mean: 1.55
    apples_agent-0_min: 0
    apples_agent-1_max: 90
    apples_agent-1_mean: 28.93
    apples_agent-1_min: 0
    apples_agent-2_max: 156
    apples_agent-2_mean: 12.91
    apples_agent-2_min: 0
    apples_agent-3_max: 112
    apples_agent-3_mean: 58.76
    apples_agent-3_min: 35
    apples_agent-4_max: 79
    apples_agent-4_mean: 2.36
    apples_agent-4_min: 0
    apples_agent-5_max: 226
    apples_agent-5_mean: 105.21
    apples_agent-5_min: 1
    cleaning_beam_agent-0_max: 514
    cleaning_beam_agent-0_mean: 384.08
    cleaning_beam_agent-0_min: 202
    cleaning_beam_agent-1_max: 375
    cleaning_beam_agent-1_mean: 237.45
    cleaning_beam_agent-1_min: 118
    cleaning_beam_agent-2_max: 444
    cleaning_beam_agent-2_mean: 264.02
    cleaning_beam_agent-2_min: 80
    cleaning_beam_agent-3_max: 82
    cleaning_beam_agent-3_mean: 16.88
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 579
    cleaning_beam_agent-4_mean: 469.36
    cleaning_beam_agent-4_min: 319
    cleaning_beam_agent-5_max: 458
    cleaning_beam_agent-5_mean: 56.74
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-39-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1141.0000000000214
  episode_reward_mean: 929.5699999999824
  episode_reward_min: 320.0000000000005
  episodes_this_iter: 96
  episodes_total: 24864
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20183.013
    learner:
      agent-0:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9439531564712524
        entropy_coeff: 0.0017600000137463212
        kl: 0.001451835734769702
        model: {}
        policy_loss: -0.0033429069444537163
        total_loss: -0.0029244699981063604
        vf_explained_var: 0.010058104991912842
        vf_loss: 20.79793357849121
      agent-1:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1289340257644653
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015845306916162372
        model: {}
        policy_loss: -0.0039988053031265736
        total_loss: -0.003879901487380266
        vf_explained_var: 0.0036389678716659546
        vf_loss: 21.05828094482422
      agent-2:
        cur_kl_coeff: 5.204170634725574e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1656774282455444
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017728261882439256
        model: {}
        policy_loss: -0.0035750996321439743
        total_loss: -0.0035132290795445442
        vf_explained_var: 0.00227530300617218
        vf_loss: 21.13463020324707
      agent-3:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3911297619342804
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007715910905972123
        model: {}
        policy_loss: -0.002253544982522726
        total_loss: -0.0010949182324111462
        vf_explained_var: 0.12513725459575653
        vf_loss: 18.470134735107422
      agent-4:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9022619724273682
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016471389681100845
        model: {}
        policy_loss: -0.003602199722081423
        total_loss: -0.0032564764842391014
        vf_explained_var: 0.08464574813842773
        vf_loss: 19.33704948425293
      agent-5:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7204582691192627
        entropy_coeff: 0.0017600000137463212
        kl: 0.001410878961905837
        model: {}
        policy_loss: -0.0033891249913722277
        total_loss: -0.0028161252848803997
        vf_explained_var: 0.12553448975086212
        vf_loss: 18.410083770751953
    load_time_ms: 15049.297
    num_steps_sampled: 24864000
    num_steps_trained: 24864000
    sample_time_ms: 89730.589
    update_time_ms: 21.773
  iterations_since_restore: 239
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.481564245810056
    ram_util_percent: 9.606145251396649
  pid: 4061
  policy_reward_max:
    agent-0: 190.16666666666626
    agent-1: 190.16666666666626
    agent-2: 190.16666666666626
    agent-3: 190.16666666666626
    agent-4: 190.16666666666626
    agent-5: 190.16666666666626
  policy_reward_mean:
    agent-0: 154.92833333333326
    agent-1: 154.92833333333326
    agent-2: 154.92833333333326
    agent-3: 154.92833333333326
    agent-4: 154.92833333333326
    agent-5: 154.92833333333326
  policy_reward_min:
    agent-0: 53.33333333333326
    agent-1: 53.33333333333326
    agent-2: 53.33333333333326
    agent-3: 53.33333333333326
    agent-4: 53.33333333333326
    agent-5: 53.33333333333326
  sampler_perf:
    mean_env_wait_ms: 23.82475349263062
    mean_inference_ms: 12.267023380131215
    mean_processing_ms: 50.67652086180269
  time_since_restore: 30781.74720788002
  time_this_iter_s: 125.81543755531311
  time_total_s: 33992.81089401245
  timestamp: 1637048346
  timesteps_since_restore: 22944000
  timesteps_this_iter: 96000
  timesteps_total: 24864000
  training_iteration: 259
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    259 |          33992.8 | 24864000 |   929.57 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 40
    apples_agent-0_mean: 1.57
    apples_agent-0_min: 0
    apples_agent-1_max: 112
    apples_agent-1_mean: 27.62
    apples_agent-1_min: 0
    apples_agent-2_max: 201
    apples_agent-2_mean: 15.42
    apples_agent-2_min: 0
    apples_agent-3_max: 104
    apples_agent-3_mean: 61.4
    apples_agent-3_min: 22
    apples_agent-4_max: 64
    apples_agent-4_mean: 1.4
    apples_agent-4_min: 0
    apples_agent-5_max: 213
    apples_agent-5_mean: 102.51
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 493
    cleaning_beam_agent-0_mean: 382.14
    cleaning_beam_agent-0_min: 253
    cleaning_beam_agent-1_max: 376
    cleaning_beam_agent-1_mean: 245.18
    cleaning_beam_agent-1_min: 96
    cleaning_beam_agent-2_max: 440
    cleaning_beam_agent-2_mean: 262.97
    cleaning_beam_agent-2_min: 102
    cleaning_beam_agent-3_max: 56
    cleaning_beam_agent-3_mean: 17.79
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 587
    cleaning_beam_agent-4_mean: 476.0
    cleaning_beam_agent-4_min: 244
    cleaning_beam_agent-5_max: 363
    cleaning_beam_agent-5_mean: 58.74
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-41-11
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1113.0000000000086
  episode_reward_mean: 935.1399999999829
  episode_reward_min: 414.00000000000597
  episodes_this_iter: 96
  episodes_total: 24960
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20191.165
    learner:
      agent-0:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9482909440994263
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016076184110715985
        model: {}
        policy_loss: -0.0031783506274223328
        total_loss: -0.0027430271729826927
        vf_explained_var: 0.03036448359489441
        vf_loss: 21.043188095092773
      agent-1:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1188485622406006
        entropy_coeff: 0.0017600000137463212
        kl: 0.001836502691730857
        model: {}
        policy_loss: -0.003843105398118496
        total_loss: -0.0036401022225618362
        vf_explained_var: 0.003382086753845215
        vf_loss: 21.721786499023438
      agent-2:
        cur_kl_coeff: 2.602085317362787e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 1.16949462890625
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013643770944327116
        model: {}
        policy_loss: -0.003468011040240526
        total_loss: -0.0034188427962362766
        vf_explained_var: 0.0381772518157959
        vf_loss: 21.074779510498047
      agent-3:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38773977756500244
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011277343146502972
        model: {}
        policy_loss: -0.002642134204506874
        total_loss: -0.0014989646151661873
        vf_explained_var: 0.16257108747959137
        vf_loss: 18.255935668945312
      agent-4:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8883560299873352
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013025454245507717
        model: {}
        policy_loss: -0.003283819416537881
        total_loss: -0.0028701433911919594
        vf_explained_var: 0.0868694931268692
        vf_loss: 19.771848678588867
      agent-5:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7303409576416016
        entropy_coeff: 0.0017600000137463212
        kl: 0.000967774074524641
        model: {}
        policy_loss: -0.003698303597047925
        total_loss: -0.0030669686384499073
        vf_explained_var: 0.11625081300735474
        vf_loss: 19.167362213134766
    load_time_ms: 14928.834
    num_steps_sampled: 24960000
    num_steps_trained: 24960000
    sample_time_ms: 89625.652
    update_time_ms: 21.872
  iterations_since_restore: 240
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.85977653631285
    ram_util_percent: 9.684357541899441
  pid: 4061
  policy_reward_max:
    agent-0: 185.4999999999997
    agent-1: 185.4999999999997
    agent-2: 185.4999999999997
    agent-3: 185.4999999999997
    agent-4: 185.4999999999997
    agent-5: 185.4999999999997
  policy_reward_mean:
    agent-0: 155.85666666666654
    agent-1: 155.85666666666654
    agent-2: 155.85666666666654
    agent-3: 155.85666666666654
    agent-4: 155.85666666666654
    agent-5: 155.85666666666654
  policy_reward_min:
    agent-0: 69.00000000000003
    agent-1: 69.00000000000003
    agent-2: 69.00000000000003
    agent-3: 69.00000000000003
    agent-4: 69.00000000000003
    agent-5: 69.00000000000003
  sampler_perf:
    mean_env_wait_ms: 23.82700228443304
    mean_inference_ms: 12.266863869059382
    mean_processing_ms: 50.67417969823363
  time_since_restore: 30907.496728897095
  time_this_iter_s: 125.74952101707458
  time_total_s: 34118.560415029526
  timestamp: 1637048471
  timesteps_since_restore: 23040000
  timesteps_this_iter: 96000
  timesteps_total: 24960000
  training_iteration: 260
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    260 |          34118.6 | 24960000 |   935.14 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 55
    apples_agent-0_mean: 1.85
    apples_agent-0_min: 0
    apples_agent-1_max: 98
    apples_agent-1_mean: 27.86
    apples_agent-1_min: 0
    apples_agent-2_max: 129
    apples_agent-2_mean: 10.3
    apples_agent-2_min: 0
    apples_agent-3_max: 112
    apples_agent-3_mean: 58.96
    apples_agent-3_min: 18
    apples_agent-4_max: 77
    apples_agent-4_mean: 3.35
    apples_agent-4_min: 0
    apples_agent-5_max: 250
    apples_agent-5_mean: 98.71
    apples_agent-5_min: 29
    cleaning_beam_agent-0_max: 490
    cleaning_beam_agent-0_mean: 390.21
    cleaning_beam_agent-0_min: 230
    cleaning_beam_agent-1_max: 370
    cleaning_beam_agent-1_mean: 244.94
    cleaning_beam_agent-1_min: 139
    cleaning_beam_agent-2_max: 513
    cleaning_beam_agent-2_mean: 281.01
    cleaning_beam_agent-2_min: 122
    cleaning_beam_agent-3_max: 101
    cleaning_beam_agent-3_mean: 20.03
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 604
    cleaning_beam_agent-4_mean: 473.25
    cleaning_beam_agent-4_min: 315
    cleaning_beam_agent-5_max: 467
    cleaning_beam_agent-5_mean: 66.16
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-43-17
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1104.9999999999836
  episode_reward_mean: 919.2799999999839
  episode_reward_min: 276.99999999999875
  episodes_this_iter: 96
  episodes_total: 25056
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20175.446
    learner:
      agent-0:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9502757787704468
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022294761147350073
        model: {}
        policy_loss: -0.003244037739932537
        total_loss: -0.002682668622583151
        vf_explained_var: 0.015836790204048157
        vf_loss: 22.33854103088379
      agent-1:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1239396333694458
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013060246128588915
        model: {}
        policy_loss: -0.003907803446054459
        total_loss: -0.0035499301739037037
        vf_explained_var: -0.026381224393844604
        vf_loss: 23.360055923461914
      agent-2:
        cur_kl_coeff: 1.3010426586813936e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1629672050476074
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015054477844387293
        model: {}
        policy_loss: -0.0035431268624961376
        total_loss: -0.0033415162470191717
        vf_explained_var: 0.00913134217262268
        vf_loss: 22.48427963256836
      agent-3:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3892522156238556
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012455697869881988
        model: {}
        policy_loss: -0.002742948941886425
        total_loss: -0.0015370291657745838
        vf_explained_var: 0.16621316969394684
        vf_loss: 18.910036087036133
      agent-4:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8930743932723999
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017090325709432364
        model: {}
        policy_loss: -0.0037292111665010452
        total_loss: -0.0033175861462950706
        vf_explained_var: 0.12473899126052856
        vf_loss: 19.83438491821289
      agent-5:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7159659266471863
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008906848961487412
        model: {}
        policy_loss: -0.0033293263986706734
        total_loss: -0.002591535449028015
        vf_explained_var: 0.11954687535762787
        vf_loss: 19.97895050048828
    load_time_ms: 14934.495
    num_steps_sampled: 25056000
    num_steps_trained: 25056000
    sample_time_ms: 89575.675
    update_time_ms: 22.248
  iterations_since_restore: 241
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.188202247191011
    ram_util_percent: 9.603370786516855
  pid: 4061
  policy_reward_max:
    agent-0: 184.16666666666612
    agent-1: 184.16666666666612
    agent-2: 184.16666666666612
    agent-3: 184.16666666666612
    agent-4: 184.16666666666612
    agent-5: 184.16666666666612
  policy_reward_mean:
    agent-0: 153.21333333333325
    agent-1: 153.21333333333325
    agent-2: 153.21333333333325
    agent-3: 153.21333333333325
    agent-4: 153.21333333333325
    agent-5: 153.21333333333325
  policy_reward_min:
    agent-0: 46.16666666666663
    agent-1: 46.16666666666663
    agent-2: 46.16666666666663
    agent-3: 46.16666666666663
    agent-4: 46.16666666666663
    agent-5: 46.16666666666663
  sampler_perf:
    mean_env_wait_ms: 23.82897201301125
    mean_inference_ms: 12.266414419873621
    mean_processing_ms: 50.67029801815761
  time_since_restore: 31032.16377544403
  time_this_iter_s: 124.66704654693604
  time_total_s: 34243.22746157646
  timestamp: 1637048597
  timesteps_since_restore: 23136000
  timesteps_this_iter: 96000
  timesteps_total: 25056000
  training_iteration: 261
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    261 |          34243.2 | 25056000 |   919.28 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 43
    apples_agent-0_mean: 1.55
    apples_agent-0_min: 0
    apples_agent-1_max: 79
    apples_agent-1_mean: 25.77
    apples_agent-1_min: 0
    apples_agent-2_max: 132
    apples_agent-2_mean: 12.72
    apples_agent-2_min: 0
    apples_agent-3_max: 150
    apples_agent-3_mean: 62.98
    apples_agent-3_min: 27
    apples_agent-4_max: 35
    apples_agent-4_mean: 1.11
    apples_agent-4_min: 0
    apples_agent-5_max: 169
    apples_agent-5_mean: 96.35
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 498
    cleaning_beam_agent-0_mean: 390.92
    cleaning_beam_agent-0_min: 231
    cleaning_beam_agent-1_max: 370
    cleaning_beam_agent-1_mean: 234.7
    cleaning_beam_agent-1_min: 107
    cleaning_beam_agent-2_max: 460
    cleaning_beam_agent-2_mean: 281.62
    cleaning_beam_agent-2_min: 99
    cleaning_beam_agent-3_max: 105
    cleaning_beam_agent-3_mean: 16.44
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 604
    cleaning_beam_agent-4_mean: 468.19
    cleaning_beam_agent-4_min: 305
    cleaning_beam_agent-5_max: 668
    cleaning_beam_agent-5_mean: 66.57
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-45-22
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1099.0000000000084
  episode_reward_mean: 943.2299999999824
  episode_reward_min: 128.00000000000134
  episodes_this_iter: 96
  episodes_total: 25152
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20178.355
    learner:
      agent-0:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9327631592750549
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019098808988928795
        model: {}
        policy_loss: -0.0032424391247332096
        total_loss: -0.002758320886641741
        vf_explained_var: 0.001529306173324585
        vf_loss: 21.257795333862305
      agent-1:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1372624635696411
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014672534307464957
        model: {}
        policy_loss: -0.004024282563477755
        total_loss: -0.003807486966252327
        vf_explained_var: -0.0324760377407074
        vf_loss: 22.183757781982422
      agent-2:
        cur_kl_coeff: 6.505213293406968e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1610143184661865
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009181206114590168
        model: {}
        policy_loss: -0.0032646674662828445
        total_loss: -0.0032560392282903194
        vf_explained_var: 0.03978559374809265
        vf_loss: 20.520118713378906
      agent-3:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37401869893074036
        entropy_coeff: 0.0017600000137463212
        kl: 0.001165943220257759
        model: {}
        policy_loss: -0.002590198302641511
        total_loss: -0.0013707755133509636
        vf_explained_var: 0.11355698108673096
        vf_loss: 18.776954650878906
      agent-4:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8917987942695618
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017918195808306336
        model: {}
        policy_loss: -0.0035905581898987293
        total_loss: -0.00318976235575974
        vf_explained_var: 0.0751524567604065
        vf_loss: 19.703622817993164
      agent-5:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7086498737335205
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011381604708731174
        model: {}
        policy_loss: -0.003310984931886196
        total_loss: -0.0026885769329965115
        vf_explained_var: 0.11764340102672577
        vf_loss: 18.69632339477539
    load_time_ms: 14957.057
    num_steps_sampled: 25152000
    num_steps_trained: 25152000
    sample_time_ms: 89571.506
    update_time_ms: 22.846
  iterations_since_restore: 242
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.134078212290502
    ram_util_percent: 9.60558659217877
  pid: 4061
  policy_reward_max:
    agent-0: 183.16666666666615
    agent-1: 183.16666666666615
    agent-2: 183.16666666666615
    agent-3: 183.16666666666615
    agent-4: 183.16666666666615
    agent-5: 183.16666666666615
  policy_reward_mean:
    agent-0: 157.20499999999996
    agent-1: 157.20499999999996
    agent-2: 157.20499999999996
    agent-3: 157.20499999999996
    agent-4: 157.20499999999996
    agent-5: 157.20499999999996
  policy_reward_min:
    agent-0: 21.33333333333335
    agent-1: 21.33333333333335
    agent-2: 21.33333333333335
    agent-3: 21.33333333333335
    agent-4: 21.33333333333335
    agent-5: 21.33333333333335
  sampler_perf:
    mean_env_wait_ms: 23.8314295306858
    mean_inference_ms: 12.266100695404507
    mean_processing_ms: 50.66868919719336
  time_since_restore: 31157.89684510231
  time_this_iter_s: 125.73306965827942
  time_total_s: 34368.96053123474
  timestamp: 1637048722
  timesteps_since_restore: 23232000
  timesteps_this_iter: 96000
  timesteps_total: 25152000
  training_iteration: 262
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    262 |            34369 | 25152000 |   943.23 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 66
    apples_agent-0_mean: 1.78
    apples_agent-0_min: 0
    apples_agent-1_max: 94
    apples_agent-1_mean: 27.52
    apples_agent-1_min: 0
    apples_agent-2_max: 84
    apples_agent-2_mean: 12.95
    apples_agent-2_min: 0
    apples_agent-3_max: 104
    apples_agent-3_mean: 56.37
    apples_agent-3_min: 18
    apples_agent-4_max: 37
    apples_agent-4_mean: 1.13
    apples_agent-4_min: 0
    apples_agent-5_max: 176
    apples_agent-5_mean: 105.03
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 524
    cleaning_beam_agent-0_mean: 396.19
    cleaning_beam_agent-0_min: 220
    cleaning_beam_agent-1_max: 382
    cleaning_beam_agent-1_mean: 239.13
    cleaning_beam_agent-1_min: 127
    cleaning_beam_agent-2_max: 523
    cleaning_beam_agent-2_mean: 273.08
    cleaning_beam_agent-2_min: 113
    cleaning_beam_agent-3_max: 73
    cleaning_beam_agent-3_mean: 18.81
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 598
    cleaning_beam_agent-4_mean: 463.62
    cleaning_beam_agent-4_min: 355
    cleaning_beam_agent-5_max: 486
    cleaning_beam_agent-5_mean: 52.07
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-47-28
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1163.9999999999977
  episode_reward_mean: 934.569999999984
  episode_reward_min: 323.0000000000046
  episodes_this_iter: 96
  episodes_total: 25248
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20181.157
    learner:
      agent-0:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9442664384841919
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016483337385579944
        model: {}
        policy_loss: -0.0032816666644066572
        total_loss: -0.002838150132447481
        vf_explained_var: -0.0012570023536682129
        vf_loss: 21.054244995117188
      agent-1:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1267657279968262
        entropy_coeff: 0.0017600000137463212
        kl: 0.001372425234876573
        model: {}
        policy_loss: -0.004004800692200661
        total_loss: -0.003920407500118017
        vf_explained_var: 0.01788368821144104
        vf_loss: 20.675003051757812
      agent-2:
        cur_kl_coeff: 3.252606646703484e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1686166524887085
        entropy_coeff: 0.0017600000137463212
        kl: 0.001990526681765914
        model: {}
        policy_loss: -0.003962692804634571
        total_loss: -0.004007050767540932
        vf_explained_var: 0.043320730328559875
        vf_loss: 20.12407112121582
      agent-3:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39836812019348145
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012254116591066122
        model: {}
        policy_loss: -0.0024347323924303055
        total_loss: -0.0013434411957859993
        vf_explained_var: 0.14387570321559906
        vf_loss: 17.924198150634766
      agent-4:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.894687294960022
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012692115269601345
        model: {}
        policy_loss: -0.0032907379791140556
        total_loss: -0.0029636924155056477
        vf_explained_var: 0.09375736117362976
        vf_loss: 19.0169734954834
      agent-5:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7256299257278442
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014204795006662607
        model: {}
        policy_loss: -0.003633064217865467
        total_loss: -0.0030685148667544127
        vf_explained_var: 0.11867976188659668
        vf_loss: 18.41659164428711
    load_time_ms: 14891.592
    num_steps_sampled: 25248000
    num_steps_trained: 25248000
    sample_time_ms: 89620.623
    update_time_ms: 22.971
  iterations_since_restore: 243
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.211173184357543
    ram_util_percent: 9.684916201117318
  pid: 4061
  policy_reward_max:
    agent-0: 194.00000000000017
    agent-1: 194.00000000000017
    agent-2: 194.00000000000017
    agent-3: 194.00000000000017
    agent-4: 194.00000000000017
    agent-5: 194.00000000000017
  policy_reward_mean:
    agent-0: 155.76166666666663
    agent-1: 155.76166666666663
    agent-2: 155.76166666666663
    agent-3: 155.76166666666663
    agent-4: 155.76166666666663
    agent-5: 155.76166666666663
  policy_reward_min:
    agent-0: 53.83333333333317
    agent-1: 53.83333333333317
    agent-2: 53.83333333333317
    agent-3: 53.83333333333317
    agent-4: 53.83333333333317
    agent-5: 53.83333333333317
  sampler_perf:
    mean_env_wait_ms: 23.833590209072717
    mean_inference_ms: 12.26578876170704
    mean_processing_ms: 50.66677427581372
  time_since_restore: 31282.905217647552
  time_this_iter_s: 125.00837254524231
  time_total_s: 34493.96890377998
  timestamp: 1637048848
  timesteps_since_restore: 23328000
  timesteps_this_iter: 96000
  timesteps_total: 25248000
  training_iteration: 263
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    263 |            34494 | 25248000 |   934.57 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 66
    apples_agent-0_mean: 1.73
    apples_agent-0_min: 0
    apples_agent-1_max: 92
    apples_agent-1_mean: 26.01
    apples_agent-1_min: 0
    apples_agent-2_max: 136
    apples_agent-2_mean: 11.75
    apples_agent-2_min: 0
    apples_agent-3_max: 171
    apples_agent-3_mean: 57.54
    apples_agent-3_min: 31
    apples_agent-4_max: 43
    apples_agent-4_mean: 1.73
    apples_agent-4_min: 0
    apples_agent-5_max: 253
    apples_agent-5_mean: 100.76
    apples_agent-5_min: 32
    cleaning_beam_agent-0_max: 482
    cleaning_beam_agent-0_mean: 394.52
    cleaning_beam_agent-0_min: 236
    cleaning_beam_agent-1_max: 355
    cleaning_beam_agent-1_mean: 233.74
    cleaning_beam_agent-1_min: 102
    cleaning_beam_agent-2_max: 491
    cleaning_beam_agent-2_mean: 267.07
    cleaning_beam_agent-2_min: 106
    cleaning_beam_agent-3_max: 80
    cleaning_beam_agent-3_mean: 16.78
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 584
    cleaning_beam_agent-4_mean: 466.54
    cleaning_beam_agent-4_min: 255
    cleaning_beam_agent-5_max: 392
    cleaning_beam_agent-5_mean: 56.05
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-49-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1155.9999999999955
  episode_reward_mean: 919.2799999999836
  episode_reward_min: 354.00000000000136
  episodes_this_iter: 96
  episodes_total: 25344
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20176.238
    learner:
      agent-0:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9435038566589355
        entropy_coeff: 0.0017600000137463212
        kl: 0.0032365296501666307
        model: {}
        policy_loss: -0.0038161566480994225
        total_loss: -0.003191603347659111
        vf_explained_var: -0.000830039381980896
        vf_loss: 22.851226806640625
      agent-1:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1350198984146118
        entropy_coeff: 0.0017600000137463212
        kl: 0.001457918668165803
        model: {}
        policy_loss: -0.0042097484692931175
        total_loss: -0.003820837941020727
        vf_explained_var: -0.041980236768722534
        vf_loss: 23.865427017211914
      agent-2:
        cur_kl_coeff: 1.626303323351742e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1784934997558594
        entropy_coeff: 0.0017600000137463212
        kl: 0.001677277497947216
        model: {}
        policy_loss: -0.003928981721401215
        total_loss: -0.003820924088358879
        vf_explained_var: 0.046238794922828674
        vf_loss: 21.822093963623047
      agent-3:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40064722299575806
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009202115470543504
        model: {}
        policy_loss: -0.002580861793830991
        total_loss: -0.0013746251352131367
        vf_explained_var: 0.16219764947891235
        vf_loss: 19.113750457763672
      agent-4:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8854280710220337
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018862677970901132
        model: {}
        policy_loss: -0.0039295777678489685
        total_loss: -0.0033903466537594795
        vf_explained_var: 0.08062496781349182
        vf_loss: 20.975830078125
      agent-5:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.733798086643219
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010178167140111327
        model: {}
        policy_loss: -0.003624596633017063
        total_loss: -0.00292500713840127
        vf_explained_var: 0.12651529908180237
        vf_loss: 19.910743713378906
    load_time_ms: 15040.101
    num_steps_sampled: 25344000
    num_steps_trained: 25344000
    sample_time_ms: 89651.81
    update_time_ms: 22.345
  iterations_since_restore: 244
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.586592178770951
    ram_util_percent: 9.686033519553074
  pid: 4061
  policy_reward_max:
    agent-0: 192.6666666666665
    agent-1: 192.6666666666665
    agent-2: 192.6666666666665
    agent-3: 192.6666666666665
    agent-4: 192.6666666666665
    agent-5: 192.6666666666665
  policy_reward_mean:
    agent-0: 153.2133333333333
    agent-1: 153.2133333333333
    agent-2: 153.2133333333333
    agent-3: 153.2133333333333
    agent-4: 153.2133333333333
    agent-5: 153.2133333333333
  policy_reward_min:
    agent-0: 58.99999999999984
    agent-1: 58.99999999999984
    agent-2: 58.99999999999984
    agent-3: 58.99999999999984
    agent-4: 58.99999999999984
    agent-5: 58.99999999999984
  sampler_perf:
    mean_env_wait_ms: 23.83537623748063
    mean_inference_ms: 12.265436730912013
    mean_processing_ms: 50.66420111045535
  time_since_restore: 31408.812790870667
  time_this_iter_s: 125.90757322311401
  time_total_s: 34619.8764770031
  timestamp: 1637048974
  timesteps_since_restore: 23424000
  timesteps_this_iter: 96000
  timesteps_total: 25344000
  training_iteration: 264
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    264 |          34619.9 | 25344000 |   919.28 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 1.07
    apples_agent-0_min: 0
    apples_agent-1_max: 87
    apples_agent-1_mean: 26.41
    apples_agent-1_min: 0
    apples_agent-2_max: 279
    apples_agent-2_mean: 16.8
    apples_agent-2_min: 0
    apples_agent-3_max: 123
    apples_agent-3_mean: 59.2
    apples_agent-3_min: 35
    apples_agent-4_max: 41
    apples_agent-4_mean: 0.8
    apples_agent-4_min: 0
    apples_agent-5_max: 240
    apples_agent-5_mean: 99.17
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 610
    cleaning_beam_agent-0_mean: 412.57
    cleaning_beam_agent-0_min: 307
    cleaning_beam_agent-1_max: 377
    cleaning_beam_agent-1_mean: 239.68
    cleaning_beam_agent-1_min: 102
    cleaning_beam_agent-2_max: 504
    cleaning_beam_agent-2_mean: 274.83
    cleaning_beam_agent-2_min: 101
    cleaning_beam_agent-3_max: 57
    cleaning_beam_agent-3_mean: 15.76
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 575
    cleaning_beam_agent-4_mean: 473.96
    cleaning_beam_agent-4_min: 303
    cleaning_beam_agent-5_max: 699
    cleaning_beam_agent-5_mean: 73.59
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-51-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1129.9999999999968
  episode_reward_mean: 928.0699999999827
  episode_reward_min: 439.0000000000088
  episodes_this_iter: 96
  episodes_total: 25440
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20148.012
    learner:
      agent-0:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9441205263137817
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015252602752298117
        model: {}
        policy_loss: -0.003666034433990717
        total_loss: -0.003503561019897461
        vf_explained_var: 0.024217113852500916
        vf_loss: 18.241249084472656
      agent-1:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1345579624176025
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013234184589236975
        model: {}
        policy_loss: -0.003573159221559763
        total_loss: -0.0036585628986358643
        vf_explained_var: -0.02398011088371277
        vf_loss: 19.11417007446289
      agent-2:
        cur_kl_coeff: 8.13151661675871e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 1.152445673942566
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016040564514696598
        model: {}
        policy_loss: -0.0038502439856529236
        total_loss: -0.0040312595665454865
        vf_explained_var: 0.014976471662521362
        vf_loss: 18.472877502441406
      agent-3:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37593406438827515
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009653542074374855
        model: {}
        policy_loss: -0.002191151026636362
        total_loss: -0.0011387853883206844
        vf_explained_var: 0.06857934594154358
        vf_loss: 17.140117645263672
      agent-4:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8745442628860474
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012912641977891326
        model: {}
        policy_loss: -0.0036207744851708412
        total_loss: -0.003353075124323368
        vf_explained_var: 0.025348082184791565
        vf_loss: 18.06899070739746
      agent-5:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.722932755947113
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008355059544555843
        model: {}
        policy_loss: -0.0031517045572400093
        total_loss: -0.0027058571577072144
        vf_explained_var: 0.07414707541465759
        vf_loss: 17.182086944580078
    load_time_ms: 15290.931
    num_steps_sampled: 25440000
    num_steps_trained: 25440000
    sample_time_ms: 89686.654
    update_time_ms: 22.712
  iterations_since_restore: 245
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.704444444444446
    ram_util_percent: 9.606666666666667
  pid: 4061
  policy_reward_max:
    agent-0: 188.33333333333275
    agent-1: 188.33333333333275
    agent-2: 188.33333333333275
    agent-3: 188.33333333333275
    agent-4: 188.33333333333275
    agent-5: 188.33333333333275
  policy_reward_mean:
    agent-0: 154.6783333333333
    agent-1: 154.6783333333333
    agent-2: 154.6783333333333
    agent-3: 154.6783333333333
    agent-4: 154.6783333333333
    agent-5: 154.6783333333333
  policy_reward_min:
    agent-0: 73.16666666666667
    agent-1: 73.16666666666667
    agent-2: 73.16666666666667
    agent-3: 73.16666666666667
    agent-4: 73.16666666666667
    agent-5: 73.16666666666667
  sampler_perf:
    mean_env_wait_ms: 23.837900556381538
    mean_inference_ms: 12.264975039746512
    mean_processing_ms: 50.66195492385851
  time_since_restore: 31534.8604388237
  time_this_iter_s: 126.04764795303345
  time_total_s: 34745.92412495613
  timestamp: 1637049100
  timesteps_since_restore: 23520000
  timesteps_this_iter: 96000
  timesteps_total: 25440000
  training_iteration: 265
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    265 |          34745.9 | 25440000 |   928.07 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 31
    apples_agent-0_mean: 1.64
    apples_agent-0_min: 0
    apples_agent-1_max: 86
    apples_agent-1_mean: 23.52
    apples_agent-1_min: 0
    apples_agent-2_max: 87
    apples_agent-2_mean: 9.83
    apples_agent-2_min: 0
    apples_agent-3_max: 148
    apples_agent-3_mean: 60.65
    apples_agent-3_min: 34
    apples_agent-4_max: 33
    apples_agent-4_mean: 0.54
    apples_agent-4_min: 0
    apples_agent-5_max: 178
    apples_agent-5_mean: 102.63
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 580
    cleaning_beam_agent-0_mean: 421.01
    cleaning_beam_agent-0_min: 291
    cleaning_beam_agent-1_max: 345
    cleaning_beam_agent-1_mean: 244.85
    cleaning_beam_agent-1_min: 136
    cleaning_beam_agent-2_max: 429
    cleaning_beam_agent-2_mean: 267.07
    cleaning_beam_agent-2_min: 129
    cleaning_beam_agent-3_max: 73
    cleaning_beam_agent-3_mean: 14.22
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 643
    cleaning_beam_agent-4_mean: 479.84
    cleaning_beam_agent-4_min: 383
    cleaning_beam_agent-5_max: 538
    cleaning_beam_agent-5_mean: 56.29
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-53-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1127.9999999999864
  episode_reward_mean: 951.8599999999838
  episode_reward_min: 596.0000000000048
  episodes_this_iter: 96
  episodes_total: 25536
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20130.163
    learner:
      agent-0:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9315300583839417
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014925121795386076
        model: {}
        policy_loss: -0.0032736307475715876
        total_loss: -0.00295920274220407
        vf_explained_var: 0.011905238032341003
        vf_loss: 19.539207458496094
      agent-1:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1352320909500122
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012862224830314517
        model: {}
        policy_loss: -0.003893580986186862
        total_loss: -0.003870120272040367
        vf_explained_var: -0.026138663291931152
        vf_loss: 20.214710235595703
      agent-2:
        cur_kl_coeff: 4.065758308379355e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 1.16796875
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014137239195406437
        model: {}
        policy_loss: -0.0033962447196245193
        total_loss: -0.0035488144494593143
        vf_explained_var: 0.028012841939926147
        vf_loss: 19.030555725097656
      agent-3:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36981549859046936
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008204675395973027
        model: {}
        policy_loss: -0.0019879015162587166
        total_loss: -0.000887600239366293
        vf_explained_var: 0.09197337925434113
        vf_loss: 17.511768341064453
      agent-4:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8820633888244629
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016455219592899084
        model: {}
        policy_loss: -0.003473286982625723
        total_loss: -0.0031487992964684963
        vf_explained_var: 0.04294228553771973
        vf_loss: 18.769168853759766
      agent-5:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7151391506195068
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014836082700639963
        model: {}
        policy_loss: -0.0032791709527373314
        total_loss: -0.002719008829444647
        vf_explained_var: 0.06400442123413086
        vf_loss: 18.188058853149414
    load_time_ms: 15351.898
    num_steps_sampled: 25536000
    num_steps_trained: 25536000
    sample_time_ms: 89767.997
    update_time_ms: 22.71
  iterations_since_restore: 246
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.17388888888889
    ram_util_percent: 9.603333333333335
  pid: 4061
  policy_reward_max:
    agent-0: 187.99999999999986
    agent-1: 187.99999999999986
    agent-2: 187.99999999999986
    agent-3: 187.99999999999986
    agent-4: 187.99999999999986
    agent-5: 187.99999999999986
  policy_reward_mean:
    agent-0: 158.64333333333326
    agent-1: 158.64333333333326
    agent-2: 158.64333333333326
    agent-3: 158.64333333333326
    agent-4: 158.64333333333326
    agent-5: 158.64333333333326
  policy_reward_min:
    agent-0: 99.33333333333378
    agent-1: 99.33333333333378
    agent-2: 99.33333333333378
    agent-3: 99.33333333333378
    agent-4: 99.33333333333378
    agent-5: 99.33333333333378
  sampler_perf:
    mean_env_wait_ms: 23.84076932000534
    mean_inference_ms: 12.264456028021591
    mean_processing_ms: 50.66096402691099
  time_since_restore: 31661.257078886032
  time_this_iter_s: 126.39664006233215
  time_total_s: 34872.32076501846
  timestamp: 1637049226
  timesteps_since_restore: 23616000
  timesteps_this_iter: 96000
  timesteps_total: 25536000
  training_iteration: 266
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    266 |          34872.3 | 25536000 |   951.86 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 42
    apples_agent-0_mean: 2.9
    apples_agent-0_min: 0
    apples_agent-1_max: 94
    apples_agent-1_mean: 24.99
    apples_agent-1_min: 0
    apples_agent-2_max: 76
    apples_agent-2_mean: 14.34
    apples_agent-2_min: 0
    apples_agent-3_max: 95
    apples_agent-3_mean: 58.21
    apples_agent-3_min: 32
    apples_agent-4_max: 48
    apples_agent-4_mean: 1.33
    apples_agent-4_min: 0
    apples_agent-5_max: 364
    apples_agent-5_mean: 96.83
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 507
    cleaning_beam_agent-0_mean: 413.81
    cleaning_beam_agent-0_min: 256
    cleaning_beam_agent-1_max: 383
    cleaning_beam_agent-1_mean: 251.46
    cleaning_beam_agent-1_min: 129
    cleaning_beam_agent-2_max: 447
    cleaning_beam_agent-2_mean: 272.44
    cleaning_beam_agent-2_min: 131
    cleaning_beam_agent-3_max: 50
    cleaning_beam_agent-3_mean: 15.88
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 600
    cleaning_beam_agent-4_mean: 470.65
    cleaning_beam_agent-4_min: 285
    cleaning_beam_agent-5_max: 561
    cleaning_beam_agent-5_mean: 68.14
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-55-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1092.9999999999977
  episode_reward_mean: 921.6799999999852
  episode_reward_min: 283.99999999999875
  episodes_this_iter: 96
  episodes_total: 25632
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20165.517
    learner:
      agent-0:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9266150593757629
        entropy_coeff: 0.0017600000137463212
        kl: 0.002099450910463929
        model: {}
        policy_loss: -0.0031378990970551968
        total_loss: -0.002671989845111966
        vf_explained_var: 0.019013524055480957
        vf_loss: 20.967529296875
      agent-1:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 1.137513518333435
        entropy_coeff: 0.0017600000137463212
        kl: 0.001736901351250708
        model: {}
        policy_loss: -0.003934845328330994
        total_loss: -0.0037314631044864655
        vf_explained_var: -0.03579774498939514
        vf_loss: 22.054039001464844
      agent-2:
        cur_kl_coeff: 2.0328791541896775e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 1.159630298614502
        entropy_coeff: 0.0017600000137463212
        kl: 0.001756148412823677
        model: {}
        policy_loss: -0.0038468539714813232
        total_loss: -0.003822740400210023
        vf_explained_var: 0.030553355813026428
        vf_loss: 20.650604248046875
      agent-3:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4116845428943634
        entropy_coeff: 0.0017600000137463212
        kl: 0.001025412348099053
        model: {}
        policy_loss: -0.0023921807296574116
        total_loss: -0.0012537939473986626
        vf_explained_var: 0.1250978261232376
        vf_loss: 18.629541397094727
      agent-4:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8909828066825867
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017346550012007356
        model: {}
        policy_loss: -0.003754128934815526
        total_loss: -0.003366058925166726
        vf_explained_var: 0.08186905086040497
        vf_loss: 19.562026977539062
      agent-5:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7102394700050354
        entropy_coeff: 0.0017600000137463212
        kl: 0.001459836377762258
        model: {}
        policy_loss: -0.003275864291936159
        total_loss: -0.0026169815100729465
        vf_explained_var: 0.10354858636856079
        vf_loss: 19.089059829711914
    load_time_ms: 15520.98
    num_steps_sampled: 25632000
    num_steps_trained: 25632000
    sample_time_ms: 89813.732
    update_time_ms: 22.815
  iterations_since_restore: 247
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.12265193370166
    ram_util_percent: 9.678453038674034
  pid: 4061
  policy_reward_max:
    agent-0: 182.1666666666667
    agent-1: 182.1666666666667
    agent-2: 182.1666666666667
    agent-3: 182.1666666666667
    agent-4: 182.1666666666667
    agent-5: 182.1666666666667
  policy_reward_mean:
    agent-0: 153.6133333333333
    agent-1: 153.6133333333333
    agent-2: 153.6133333333333
    agent-3: 153.6133333333333
    agent-4: 153.6133333333333
    agent-5: 153.6133333333333
  policy_reward_min:
    agent-0: 47.333333333333236
    agent-1: 47.333333333333236
    agent-2: 47.333333333333236
    agent-3: 47.333333333333236
    agent-4: 47.333333333333236
    agent-5: 47.333333333333236
  sampler_perf:
    mean_env_wait_ms: 23.84292156996354
    mean_inference_ms: 12.263736111278753
    mean_processing_ms: 50.658389157354144
  time_since_restore: 31787.559022188187
  time_this_iter_s: 126.30194330215454
  time_total_s: 34998.62270832062
  timestamp: 1637049353
  timesteps_since_restore: 23712000
  timesteps_this_iter: 96000
  timesteps_total: 25632000
  training_iteration: 267
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 17.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    267 |          34998.6 | 25632000 |   921.68 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 35
    apples_agent-0_mean: 1.66
    apples_agent-0_min: 0
    apples_agent-1_max: 104
    apples_agent-1_mean: 26.4
    apples_agent-1_min: 0
    apples_agent-2_max: 75
    apples_agent-2_mean: 8.35
    apples_agent-2_min: 0
    apples_agent-3_max: 105
    apples_agent-3_mean: 57.01
    apples_agent-3_min: 25
    apples_agent-4_max: 69
    apples_agent-4_mean: 2.19
    apples_agent-4_min: 0
    apples_agent-5_max: 177
    apples_agent-5_mean: 99.7
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 539
    cleaning_beam_agent-0_mean: 429.49
    cleaning_beam_agent-0_min: 321
    cleaning_beam_agent-1_max: 390
    cleaning_beam_agent-1_mean: 240.71
    cleaning_beam_agent-1_min: 115
    cleaning_beam_agent-2_max: 464
    cleaning_beam_agent-2_mean: 286.31
    cleaning_beam_agent-2_min: 105
    cleaning_beam_agent-3_max: 54
    cleaning_beam_agent-3_mean: 14.78
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 594
    cleaning_beam_agent-4_mean: 472.14
    cleaning_beam_agent-4_min: 297
    cleaning_beam_agent-5_max: 798
    cleaning_beam_agent-5_mean: 79.85
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-57-56
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1092.9999999999827
  episode_reward_mean: 946.0399999999806
  episode_reward_min: 279.99999999999585
  episodes_this_iter: 96
  episodes_total: 25728
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20158.108
    learner:
      agent-0:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9206154942512512
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013131803134456277
        model: {}
        policy_loss: -0.0031525420490652323
        total_loss: -0.0027220468036830425
        vf_explained_var: 0.03959186375141144
        vf_loss: 20.507797241210938
      agent-1:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1309401988983154
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016018166206777096
        model: {}
        policy_loss: -0.003598769661039114
        total_loss: -0.003427480813115835
        vf_explained_var: -0.014370471239089966
        vf_loss: 21.617456436157227
      agent-2:
        cur_kl_coeff: 1.0164395770948388e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 1.157422423362732
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013458302710205317
        model: {}
        policy_loss: -0.003473510965704918
        total_loss: -0.0034236498177051544
        vf_explained_var: 0.014801770448684692
        vf_loss: 20.869211196899414
      agent-3:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3621535897254944
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007101193186827004
        model: {}
        policy_loss: -0.0021482682786881924
        total_loss: -0.0009172381833195686
        vf_explained_var: 0.11142952740192413
        vf_loss: 18.684219360351562
      agent-4:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8862393498420715
        entropy_coeff: 0.0017600000137463212
        kl: 0.001858277479186654
        model: {}
        policy_loss: -0.0038632522337138653
        total_loss: -0.0034310058690607548
        vf_explained_var: 0.06848295032978058
        vf_loss: 19.920255661010742
      agent-5:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7148979902267456
        entropy_coeff: 0.0017600000137463212
        kl: 0.001139339292421937
        model: {}
        policy_loss: -0.003311839187517762
        total_loss: -0.002554525388404727
        vf_explained_var: 0.0509372353553772
        vf_loss: 20.15534782409668
    load_time_ms: 15497.81
    num_steps_sampled: 25728000
    num_steps_trained: 25728000
    sample_time_ms: 89690.04
    update_time_ms: 22.603
  iterations_since_restore: 248
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.378857142857145
    ram_util_percent: 9.59314285714286
  pid: 4061
  policy_reward_max:
    agent-0: 182.16666666666643
    agent-1: 182.16666666666643
    agent-2: 182.16666666666643
    agent-3: 182.16666666666643
    agent-4: 182.16666666666643
    agent-5: 182.16666666666643
  policy_reward_mean:
    agent-0: 157.67333333333332
    agent-1: 157.67333333333332
    agent-2: 157.67333333333332
    agent-3: 157.67333333333332
    agent-4: 157.67333333333332
    agent-5: 157.67333333333332
  policy_reward_min:
    agent-0: 46.66666666666658
    agent-1: 46.66666666666658
    agent-2: 46.66666666666658
    agent-3: 46.66666666666658
    agent-4: 46.66666666666658
    agent-5: 46.66666666666658
  sampler_perf:
    mean_env_wait_ms: 23.845644127897113
    mean_inference_ms: 12.263273059021815
    mean_processing_ms: 50.655301069636316
  time_since_restore: 31910.69194793701
  time_this_iter_s: 123.13292574882507
  time_total_s: 35121.75563406944
  timestamp: 1637049476
  timesteps_since_restore: 23808000
  timesteps_this_iter: 96000
  timesteps_total: 25728000
  training_iteration: 268
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    268 |          35121.8 | 25728000 |   946.04 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 53
    apples_agent-0_mean: 3.13
    apples_agent-0_min: 0
    apples_agent-1_max: 82
    apples_agent-1_mean: 21.34
    apples_agent-1_min: 0
    apples_agent-2_max: 183
    apples_agent-2_mean: 14.83
    apples_agent-2_min: 0
    apples_agent-3_max: 99
    apples_agent-3_mean: 54.2
    apples_agent-3_min: 26
    apples_agent-4_max: 49
    apples_agent-4_mean: 1.13
    apples_agent-4_min: 0
    apples_agent-5_max: 207
    apples_agent-5_mean: 99.27
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 554
    cleaning_beam_agent-0_mean: 413.05
    cleaning_beam_agent-0_min: 267
    cleaning_beam_agent-1_max: 378
    cleaning_beam_agent-1_mean: 236.55
    cleaning_beam_agent-1_min: 102
    cleaning_beam_agent-2_max: 461
    cleaning_beam_agent-2_mean: 263.7
    cleaning_beam_agent-2_min: 104
    cleaning_beam_agent-3_max: 54
    cleaning_beam_agent-3_mean: 15.39
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 604
    cleaning_beam_agent-4_mean: 475.82
    cleaning_beam_agent-4_min: 378
    cleaning_beam_agent-5_max: 735
    cleaning_beam_agent-5_mean: 76.09
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-00-02
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1127.999999999993
  episode_reward_mean: 933.4399999999835
  episode_reward_min: 482.0000000000166
  episodes_this_iter: 96
  episodes_total: 25824
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20214.222
    learner:
      agent-0:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9504690170288086
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020896834321320057
        model: {}
        policy_loss: -0.0033056028187274933
        total_loss: -0.002886318601667881
        vf_explained_var: -0.0020179152488708496
        vf_loss: 20.92108154296875
      agent-1:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1448171138763428
        entropy_coeff: 0.0017600000137463212
        kl: 0.00156278139911592
        model: {}
        policy_loss: -0.003753294236958027
        total_loss: -0.003575114533305168
        vf_explained_var: -0.05313333868980408
        vf_loss: 21.930561065673828
      agent-2:
        cur_kl_coeff: 5.082197885474194e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1583209037780762
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012565999059006572
        model: {}
        policy_loss: -0.003357919864356518
        total_loss: -0.003339267335832119
        vf_explained_var: 0.014011472463607788
        vf_loss: 20.573001861572266
      agent-3:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38319528102874756
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009948628721758723
        model: {}
        policy_loss: -0.0021110856905579567
        total_loss: -0.0009123240015469491
        vf_explained_var: 0.09338968992233276
        vf_loss: 18.731855392456055
      agent-4:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8846903443336487
        entropy_coeff: 0.0017600000137463212
        kl: 0.001769653637893498
        model: {}
        policy_loss: -0.0037581203505396843
        total_loss: -0.0033108768984675407
        vf_explained_var: 0.034586742520332336
        vf_loss: 20.042966842651367
      agent-5:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7140809297561646
        entropy_coeff: 0.0017600000137463212
        kl: 0.001352836610749364
        model: {}
        policy_loss: -0.0035510417073965073
        total_loss: -0.0029294639825820923
        vf_explained_var: 0.09341253340244293
        vf_loss: 18.783573150634766
    load_time_ms: 15442.22
    num_steps_sampled: 25824000
    num_steps_trained: 25824000
    sample_time_ms: 89649.744
    update_time_ms: 22.506
  iterations_since_restore: 249
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.894972067039106
    ram_util_percent: 9.658100558659218
  pid: 4061
  policy_reward_max:
    agent-0: 188.0000000000003
    agent-1: 188.0000000000003
    agent-2: 188.0000000000003
    agent-3: 188.0000000000003
    agent-4: 188.0000000000003
    agent-5: 188.0000000000003
  policy_reward_mean:
    agent-0: 155.57333333333335
    agent-1: 155.57333333333335
    agent-2: 155.57333333333335
    agent-3: 155.57333333333335
    agent-4: 155.57333333333335
    agent-5: 155.57333333333335
  policy_reward_min:
    agent-0: 80.33333333333351
    agent-1: 80.33333333333351
    agent-2: 80.33333333333351
    agent-3: 80.33333333333351
    agent-4: 80.33333333333351
    agent-5: 80.33333333333351
  sampler_perf:
    mean_env_wait_ms: 23.847990737729823
    mean_inference_ms: 12.262870290240453
    mean_processing_ms: 50.65303221231384
  time_since_restore: 32036.15844488144
  time_this_iter_s: 125.46649694442749
  time_total_s: 35247.22213101387
  timestamp: 1637049602
  timesteps_since_restore: 23904000
  timesteps_this_iter: 96000
  timesteps_total: 25824000
  training_iteration: 269
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    269 |          35247.2 | 25824000 |   933.44 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 51
    apples_agent-0_mean: 2.71
    apples_agent-0_min: 0
    apples_agent-1_max: 91
    apples_agent-1_mean: 27.16
    apples_agent-1_min: 0
    apples_agent-2_max: 88
    apples_agent-2_mean: 10.65
    apples_agent-2_min: 0
    apples_agent-3_max: 119
    apples_agent-3_mean: 61.13
    apples_agent-3_min: 30
    apples_agent-4_max: 80
    apples_agent-4_mean: 1.05
    apples_agent-4_min: 0
    apples_agent-5_max: 164
    apples_agent-5_mean: 102.03
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 546
    cleaning_beam_agent-0_mean: 416.9
    cleaning_beam_agent-0_min: 268
    cleaning_beam_agent-1_max: 398
    cleaning_beam_agent-1_mean: 235.28
    cleaning_beam_agent-1_min: 109
    cleaning_beam_agent-2_max: 455
    cleaning_beam_agent-2_mean: 266.43
    cleaning_beam_agent-2_min: 93
    cleaning_beam_agent-3_max: 56
    cleaning_beam_agent-3_mean: 15.89
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 574
    cleaning_beam_agent-4_mean: 470.23
    cleaning_beam_agent-4_min: 290
    cleaning_beam_agent-5_max: 443
    cleaning_beam_agent-5_mean: 56.81
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-02-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1126.999999999997
  episode_reward_mean: 934.2799999999846
  episode_reward_min: 500.0000000000136
  episodes_this_iter: 96
  episodes_total: 25920
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20199.593
    learner:
      agent-0:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.957315981388092
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012123340275138617
        model: {}
        policy_loss: -0.003275881055742502
        total_loss: -0.0029669629875570536
        vf_explained_var: 0.012667745351791382
        vf_loss: 19.937904357910156
      agent-1:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1383552551269531
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014375071041285992
        model: {}
        policy_loss: -0.0038979346863925457
        total_loss: -0.0038727745413780212
        vf_explained_var: -0.015053480863571167
        vf_loss: 20.286685943603516
      agent-2:
        cur_kl_coeff: 2.541098942737097e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1739085912704468
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018938292050734162
        model: {}
        policy_loss: -0.003790478687733412
        total_loss: -0.003958253189921379
        vf_explained_var: 0.05936838686466217
        vf_loss: 18.983041763305664
      agent-3:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40181419253349304
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007129282457754016
        model: {}
        policy_loss: -0.0021746617276221514
        total_loss: -0.0011049811728298664
        vf_explained_var: 0.10742351412773132
        vf_loss: 17.768688201904297
      agent-4:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8812913298606873
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014936038060113788
        model: {}
        policy_loss: -0.003787588095292449
        total_loss: -0.0034867236390709877
        vf_explained_var: 0.07278852164745331
        vf_loss: 18.519397735595703
      agent-5:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7191973924636841
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011068928288295865
        model: {}
        policy_loss: -0.003304708283394575
        total_loss: -0.0027235758025199175
        vf_explained_var: 0.0727577805519104
        vf_loss: 18.469181060791016
    load_time_ms: 15445.752
    num_steps_sampled: 25920000
    num_steps_trained: 25920000
    sample_time_ms: 89768.425
    update_time_ms: 22.268
  iterations_since_restore: 250
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 16.166666666666668
    ram_util_percent: 9.702222222222224
  pid: 4061
  policy_reward_max:
    agent-0: 187.833333333333
    agent-1: 187.833333333333
    agent-2: 187.833333333333
    agent-3: 187.833333333333
    agent-4: 187.833333333333
    agent-5: 187.833333333333
  policy_reward_mean:
    agent-0: 155.7133333333333
    agent-1: 155.7133333333333
    agent-2: 155.7133333333333
    agent-3: 155.7133333333333
    agent-4: 155.7133333333333
    agent-5: 155.7133333333333
  policy_reward_min:
    agent-0: 83.33333333333353
    agent-1: 83.33333333333353
    agent-2: 83.33333333333353
    agent-3: 83.33333333333353
    agent-4: 83.33333333333353
    agent-5: 83.33333333333353
  sampler_perf:
    mean_env_wait_ms: 23.85062545385129
    mean_inference_ms: 12.262926696759473
    mean_processing_ms: 50.6521641004022
  time_since_restore: 32162.95282816887
  time_this_iter_s: 126.79438328742981
  time_total_s: 35374.0165143013
  timestamp: 1637049729
  timesteps_since_restore: 24000000
  timesteps_this_iter: 96000
  timesteps_total: 25920000
  training_iteration: 270
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    270 |            35374 | 25920000 |   934.28 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 34
    apples_agent-0_mean: 0.97
    apples_agent-0_min: 0
    apples_agent-1_max: 96
    apples_agent-1_mean: 24.48
    apples_agent-1_min: 0
    apples_agent-2_max: 109
    apples_agent-2_mean: 13.48
    apples_agent-2_min: 0
    apples_agent-3_max: 156
    apples_agent-3_mean: 58.98
    apples_agent-3_min: 30
    apples_agent-4_max: 55
    apples_agent-4_mean: 0.55
    apples_agent-4_min: 0
    apples_agent-5_max: 213
    apples_agent-5_mean: 100.53
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 536
    cleaning_beam_agent-0_mean: 414.01
    cleaning_beam_agent-0_min: 288
    cleaning_beam_agent-1_max: 386
    cleaning_beam_agent-1_mean: 240.7
    cleaning_beam_agent-1_min: 82
    cleaning_beam_agent-2_max: 485
    cleaning_beam_agent-2_mean: 269.66
    cleaning_beam_agent-2_min: 92
    cleaning_beam_agent-3_max: 53
    cleaning_beam_agent-3_mean: 13.79
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 593
    cleaning_beam_agent-4_mean: 479.19
    cleaning_beam_agent-4_min: 365
    cleaning_beam_agent-5_max: 718
    cleaning_beam_agent-5_mean: 81.9
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-04-13
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1093.9999999999875
  episode_reward_mean: 951.1399999999827
  episode_reward_min: 558.0000000000042
  episodes_this_iter: 96
  episodes_total: 26016
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20187.149
    learner:
      agent-0:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9509676694869995
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009934530826285481
        model: {}
        policy_loss: -0.0030984205659478903
        total_loss: -0.0027490099892020226
        vf_explained_var: -0.014189407229423523
        vf_loss: 20.231098175048828
      agent-1:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 1.141263484954834
        entropy_coeff: 0.0017600000137463212
        kl: 0.001112943165935576
        model: {}
        policy_loss: -0.0036594592966139317
        total_loss: -0.0036673080176115036
        vf_explained_var: -0.011279970407485962
        vf_loss: 20.007766723632812
      agent-2:
        cur_kl_coeff: 1.2705494713685484e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1646666526794434
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018076197011396289
        model: {}
        policy_loss: -0.0034295942168682814
        total_loss: -0.003610367653891444
        vf_explained_var: 0.05925546586513519
        vf_loss: 18.690393447875977
      agent-3:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36905595660209656
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008144887397065759
        model: {}
        policy_loss: -0.002113690599799156
        total_loss: -0.0009485427290201187
        vf_explained_var: 0.07510511577129364
        vf_loss: 18.146907806396484
      agent-4:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.881971001625061
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016909168334677815
        model: {}
        policy_loss: -0.0035997594241052866
        total_loss: -0.0032620008569210768
        vf_explained_var: 0.03987926244735718
        vf_loss: 18.900270462036133
      agent-5:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7076201438903809
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010564771946519613
        model: {}
        policy_loss: -0.0034367148764431477
        total_loss: -0.0028286734595894814
        vf_explained_var: 0.06548483669757843
        vf_loss: 18.534563064575195
    load_time_ms: 15277.969
    num_steps_sampled: 26016000
    num_steps_trained: 26016000
    sample_time_ms: 89854.297
    update_time_ms: 21.712
  iterations_since_restore: 251
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.200000000000001
    ram_util_percent: 9.578531073446328
  pid: 4061
  policy_reward_max:
    agent-0: 182.33333333333283
    agent-1: 182.33333333333283
    agent-2: 182.33333333333283
    agent-3: 182.33333333333283
    agent-4: 182.33333333333283
    agent-5: 182.33333333333283
  policy_reward_mean:
    agent-0: 158.5233333333333
    agent-1: 158.5233333333333
    agent-2: 158.5233333333333
    agent-3: 158.5233333333333
    agent-4: 158.5233333333333
    agent-5: 158.5233333333333
  policy_reward_min:
    agent-0: 93.00000000000009
    agent-1: 93.00000000000009
    agent-2: 93.00000000000009
    agent-3: 93.00000000000009
    agent-4: 93.00000000000009
    agent-5: 93.00000000000009
  sampler_perf:
    mean_env_wait_ms: 23.852977611347995
    mean_inference_ms: 12.262592478664073
    mean_processing_ms: 50.64928455151919
  time_since_restore: 32286.652683734894
  time_this_iter_s: 123.69985556602478
  time_total_s: 35497.716369867325
  timestamp: 1637049853
  timesteps_since_restore: 24096000
  timesteps_this_iter: 96000
  timesteps_total: 26016000
  training_iteration: 271
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 17.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    271 |          35497.7 | 26016000 |   951.14 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 31
    apples_agent-0_mean: 1.24
    apples_agent-0_min: 0
    apples_agent-1_max: 97
    apples_agent-1_mean: 20.95
    apples_agent-1_min: 0
    apples_agent-2_max: 87
    apples_agent-2_mean: 14.43
    apples_agent-2_min: 0
    apples_agent-3_max: 166
    apples_agent-3_mean: 59.13
    apples_agent-3_min: 29
    apples_agent-4_max: 66
    apples_agent-4_mean: 1.68
    apples_agent-4_min: 0
    apples_agent-5_max: 192
    apples_agent-5_mean: 101.24
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 518
    cleaning_beam_agent-0_mean: 414.36
    cleaning_beam_agent-0_min: 292
    cleaning_beam_agent-1_max: 357
    cleaning_beam_agent-1_mean: 237.76
    cleaning_beam_agent-1_min: 119
    cleaning_beam_agent-2_max: 393
    cleaning_beam_agent-2_mean: 268.25
    cleaning_beam_agent-2_min: 122
    cleaning_beam_agent-3_max: 71
    cleaning_beam_agent-3_mean: 14.38
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 626
    cleaning_beam_agent-4_mean: 469.94
    cleaning_beam_agent-4_min: 220
    cleaning_beam_agent-5_max: 782
    cleaning_beam_agent-5_mean: 90.63
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-06-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1102.999999999982
  episode_reward_mean: 942.3599999999818
  episode_reward_min: 555.0000000000043
  episodes_this_iter: 96
  episodes_total: 26112
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20193.08
    learner:
      agent-0:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.947880744934082
        entropy_coeff: 0.0017600000137463212
        kl: 0.001709673786535859
        model: {}
        policy_loss: -0.003642483614385128
        total_loss: -0.0032553153578191996
        vf_explained_var: -0.01928393542766571
        vf_loss: 20.554391860961914
      agent-1:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1352120637893677
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009403715375810862
        model: {}
        policy_loss: -0.003734355326741934
        total_loss: -0.0036681503988802433
        vf_explained_var: -0.0255090594291687
        vf_loss: 20.641773223876953
      agent-2:
        cur_kl_coeff: 6.352747356842742e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1606431007385254
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012368359602987766
        model: {}
        policy_loss: -0.0032347231172025204
        total_loss: -0.0032944343984127045
        vf_explained_var: 0.0157843679189682
        vf_loss: 19.830263137817383
      agent-3:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37535643577575684
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010098766069859266
        model: {}
        policy_loss: -0.0020702946931123734
        total_loss: -0.000917116180062294
        vf_explained_var: 0.09273549914360046
        vf_loss: 18.138071060180664
      agent-4:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8917291164398193
        entropy_coeff: 0.0017600000137463212
        kl: 0.002405899576842785
        model: {}
        policy_loss: -0.004176887683570385
        total_loss: -0.0038861390203237534
        vf_explained_var: 0.07557877898216248
        vf_loss: 18.60188865661621
      agent-5:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6989789009094238
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009276495547965169
        model: {}
        policy_loss: -0.00355338747613132
        total_loss: -0.00293256645090878
        vf_explained_var: 0.08021548390388489
        vf_loss: 18.510250091552734
    load_time_ms: 15040.772
    num_steps_sampled: 26112000
    num_steps_trained: 26112000
    sample_time_ms: 89804.6
    update_time_ms: 21.486
  iterations_since_restore: 252
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.437714285714286
    ram_util_percent: 9.583428571428573
  pid: 4061
  policy_reward_max:
    agent-0: 183.8333333333331
    agent-1: 183.8333333333331
    agent-2: 183.8333333333331
    agent-3: 183.8333333333331
    agent-4: 183.8333333333331
    agent-5: 183.8333333333331
  policy_reward_mean:
    agent-0: 157.05999999999997
    agent-1: 157.05999999999997
    agent-2: 157.05999999999997
    agent-3: 157.05999999999997
    agent-4: 157.05999999999997
    agent-5: 157.05999999999997
  policy_reward_min:
    agent-0: 92.50000000000011
    agent-1: 92.50000000000011
    agent-2: 92.50000000000011
    agent-3: 92.50000000000011
    agent-4: 92.50000000000011
    agent-5: 92.50000000000011
  sampler_perf:
    mean_env_wait_ms: 23.85547128308961
    mean_inference_ms: 12.262036566182013
    mean_processing_ms: 50.6474684453619
  time_since_restore: 32409.63654446602
  time_this_iter_s: 122.98386073112488
  time_total_s: 35620.70023059845
  timestamp: 1637049976
  timesteps_since_restore: 24192000
  timesteps_this_iter: 96000
  timesteps_total: 26112000
  training_iteration: 272
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    272 |          35620.7 | 26112000 |   942.36 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 45
    apples_agent-0_mean: 1.86
    apples_agent-0_min: 0
    apples_agent-1_max: 104
    apples_agent-1_mean: 24.35
    apples_agent-1_min: 0
    apples_agent-2_max: 165
    apples_agent-2_mean: 15.78
    apples_agent-2_min: 0
    apples_agent-3_max: 130
    apples_agent-3_mean: 60.69
    apples_agent-3_min: 17
    apples_agent-4_max: 44
    apples_agent-4_mean: 0.6
    apples_agent-4_min: 0
    apples_agent-5_max: 231
    apples_agent-5_mean: 103.57
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 521
    cleaning_beam_agent-0_mean: 404.46
    cleaning_beam_agent-0_min: 284
    cleaning_beam_agent-1_max: 368
    cleaning_beam_agent-1_mean: 238.56
    cleaning_beam_agent-1_min: 132
    cleaning_beam_agent-2_max: 506
    cleaning_beam_agent-2_mean: 289.23
    cleaning_beam_agent-2_min: 69
    cleaning_beam_agent-3_max: 91
    cleaning_beam_agent-3_mean: 16.63
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 626
    cleaning_beam_agent-4_mean: 464.43
    cleaning_beam_agent-4_min: 358
    cleaning_beam_agent-5_max: 571
    cleaning_beam_agent-5_mean: 82.27
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-08-22
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1127.0000000000073
  episode_reward_mean: 936.0199999999854
  episode_reward_min: 380.00000000000955
  episodes_this_iter: 96
  episodes_total: 26208
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20190.443
    learner:
      agent-0:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9586847424507141
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018435600213706493
        model: {}
        policy_loss: -0.003459902945905924
        total_loss: -0.00303276302292943
        vf_explained_var: 0.01511828601360321
        vf_loss: 21.144241333007812
      agent-1:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1342687606811523
        entropy_coeff: 0.0017600000137463212
        kl: 0.001381834619678557
        model: {}
        policy_loss: -0.0036290420684963465
        total_loss: -0.003496744204312563
        vf_explained_var: 0.008521810173988342
        vf_loss: 21.28612518310547
      agent-2:
        cur_kl_coeff: 3.176373678421371e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 1.156790018081665
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013221389381214976
        model: {}
        policy_loss: -0.0038746953941881657
        total_loss: -0.003883941099047661
        vf_explained_var: 0.05382910370826721
        vf_loss: 20.267078399658203
      agent-3:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3897988796234131
        entropy_coeff: 0.0017600000137463212
        kl: 0.001372666796669364
        model: {}
        policy_loss: -0.002431095577776432
        total_loss: -0.0012566111981868744
        vf_explained_var: 0.12874332070350647
        vf_loss: 18.60529327392578
      agent-4:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8801295161247253
        entropy_coeff: 0.0017600000137463212
        kl: 0.001804537489078939
        model: {}
        policy_loss: -0.003962389659136534
        total_loss: -0.0034537548199295998
        vf_explained_var: 0.037648797035217285
        vf_loss: 20.576587677001953
      agent-5:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6901513338088989
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009141172049567103
        model: {}
        policy_loss: -0.0033925375901162624
        total_loss: -0.0027222302742302418
        vf_explained_var: 0.12035015225410461
        vf_loss: 18.84971809387207
    load_time_ms: 15142.864
    num_steps_sampled: 26208000
    num_steps_trained: 26208000
    sample_time_ms: 89831.647
    update_time_ms: 21.586
  iterations_since_restore: 253
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.140000000000002
    ram_util_percent: 9.621666666666668
  pid: 4061
  policy_reward_max:
    agent-0: 187.83333333333354
    agent-1: 187.83333333333354
    agent-2: 187.83333333333354
    agent-3: 187.83333333333354
    agent-4: 187.83333333333354
    agent-5: 187.83333333333354
  policy_reward_mean:
    agent-0: 156.00333333333327
    agent-1: 156.00333333333327
    agent-2: 156.00333333333327
    agent-3: 156.00333333333327
    agent-4: 156.00333333333327
    agent-5: 156.00333333333327
  policy_reward_min:
    agent-0: 63.33333333333299
    agent-1: 63.33333333333299
    agent-2: 63.33333333333299
    agent-3: 63.33333333333299
    agent-4: 63.33333333333299
    agent-5: 63.33333333333299
  sampler_perf:
    mean_env_wait_ms: 23.857549345217226
    mean_inference_ms: 12.261411712118232
    mean_processing_ms: 50.64513290622374
  time_since_restore: 32535.948813676834
  time_this_iter_s: 126.31226921081543
  time_total_s: 35747.012499809265
  timestamp: 1637050102
  timesteps_since_restore: 24288000
  timesteps_this_iter: 96000
  timesteps_total: 26208000
  training_iteration: 273
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    273 |            35747 | 26208000 |   936.02 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 30
    apples_agent-0_mean: 1.71
    apples_agent-0_min: 0
    apples_agent-1_max: 100
    apples_agent-1_mean: 25.51
    apples_agent-1_min: 0
    apples_agent-2_max: 123
    apples_agent-2_mean: 12.53
    apples_agent-2_min: 0
    apples_agent-3_max: 112
    apples_agent-3_mean: 59.67
    apples_agent-3_min: 28
    apples_agent-4_max: 24
    apples_agent-4_mean: 0.24
    apples_agent-4_min: 0
    apples_agent-5_max: 240
    apples_agent-5_mean: 101.87
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 539
    cleaning_beam_agent-0_mean: 424.53
    cleaning_beam_agent-0_min: 315
    cleaning_beam_agent-1_max: 386
    cleaning_beam_agent-1_mean: 235.63
    cleaning_beam_agent-1_min: 112
    cleaning_beam_agent-2_max: 466
    cleaning_beam_agent-2_mean: 281.06
    cleaning_beam_agent-2_min: 125
    cleaning_beam_agent-3_max: 55
    cleaning_beam_agent-3_mean: 12.77
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 578
    cleaning_beam_agent-4_mean: 460.26
    cleaning_beam_agent-4_min: 374
    cleaning_beam_agent-5_max: 605
    cleaning_beam_agent-5_mean: 70.24
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-10-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1109.9999999999916
  episode_reward_mean: 959.9799999999836
  episode_reward_min: 666.9999999999976
  episodes_this_iter: 96
  episodes_total: 26304
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20183.851
    learner:
      agent-0:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9513106942176819
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010833266424015164
        model: {}
        policy_loss: -0.0031100441701710224
        total_loss: -0.0028690313920378685
        vf_explained_var: 0.012658357620239258
        vf_loss: 19.153209686279297
      agent-1:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 1.145032525062561
        entropy_coeff: 0.0017600000137463212
        kl: 0.001372623024508357
        model: {}
        policy_loss: -0.0038367705419659615
        total_loss: -0.003830317873507738
        vf_explained_var: -0.04014056921005249
        vf_loss: 20.217071533203125
      agent-2:
        cur_kl_coeff: 1.5881868392106856e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1569744348526
        entropy_coeff: 0.0017600000137463212
        kl: 0.001948165474459529
        model: {}
        policy_loss: -0.003631201572716236
        total_loss: -0.003752005286514759
        vf_explained_var: 0.0186222642660141
        vf_loss: 19.15472412109375
      agent-3:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37026023864746094
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007928241975605488
        model: {}
        policy_loss: -0.0019259103573858738
        total_loss: -0.0007756404811516404
        vf_explained_var: 0.061736613512039185
        vf_loss: 18.019289016723633
      agent-4:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8862370252609253
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015104913618415594
        model: {}
        policy_loss: -0.0038643116131424904
        total_loss: -0.0035689170472323895
        vf_explained_var: 0.043301284313201904
        vf_loss: 18.551700592041016
      agent-5:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6878221035003662
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007348288781940937
        model: {}
        policy_loss: -0.0030788430012762547
        total_loss: -0.002500888891518116
        vf_explained_var: 0.07395583391189575
        vf_loss: 17.88521957397461
    load_time_ms: 15116.53
    num_steps_sampled: 26304000
    num_steps_trained: 26304000
    sample_time_ms: 89886.429
    update_time_ms: 21.473
  iterations_since_restore: 254
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.423333333333332
    ram_util_percent: 9.675
  pid: 4061
  policy_reward_max:
    agent-0: 184.99999999999972
    agent-1: 184.99999999999972
    agent-2: 184.99999999999972
    agent-3: 184.99999999999972
    agent-4: 184.99999999999972
    agent-5: 184.99999999999972
  policy_reward_mean:
    agent-0: 159.9966666666666
    agent-1: 159.9966666666666
    agent-2: 159.9966666666666
    agent-3: 159.9966666666666
    agent-4: 159.9966666666666
    agent-5: 159.9966666666666
  policy_reward_min:
    agent-0: 111.16666666666714
    agent-1: 111.16666666666714
    agent-2: 111.16666666666714
    agent-3: 111.16666666666714
    agent-4: 111.16666666666714
    agent-5: 111.16666666666714
  sampler_perf:
    mean_env_wait_ms: 23.85998618535107
    mean_inference_ms: 12.260976280001154
    mean_processing_ms: 50.64329469237224
  time_since_restore: 32662.03720474243
  time_this_iter_s: 126.08839106559753
  time_total_s: 35873.10089087486
  timestamp: 1637050229
  timesteps_since_restore: 24384000
  timesteps_this_iter: 96000
  timesteps_total: 26304000
  training_iteration: 274
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 16.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    274 |          35873.1 | 26304000 |   959.98 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 34
    apples_agent-0_mean: 1.28
    apples_agent-0_min: 0
    apples_agent-1_max: 88
    apples_agent-1_mean: 25.53
    apples_agent-1_min: 0
    apples_agent-2_max: 88
    apples_agent-2_mean: 10.66
    apples_agent-2_min: 0
    apples_agent-3_max: 119
    apples_agent-3_mean: 58.68
    apples_agent-3_min: 29
    apples_agent-4_max: 79
    apples_agent-4_mean: 1.81
    apples_agent-4_min: 0
    apples_agent-5_max: 163
    apples_agent-5_mean: 102.5
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 539
    cleaning_beam_agent-0_mean: 420.95
    cleaning_beam_agent-0_min: 292
    cleaning_beam_agent-1_max: 454
    cleaning_beam_agent-1_mean: 237.85
    cleaning_beam_agent-1_min: 130
    cleaning_beam_agent-2_max: 507
    cleaning_beam_agent-2_mean: 286.9
    cleaning_beam_agent-2_min: 130
    cleaning_beam_agent-3_max: 73
    cleaning_beam_agent-3_mean: 14.02
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 606
    cleaning_beam_agent-4_mean: 454.6
    cleaning_beam_agent-4_min: 287
    cleaning_beam_agent-5_max: 633
    cleaning_beam_agent-5_mean: 61.3
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-12-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1083.999999999994
  episode_reward_mean: 938.8199999999835
  episode_reward_min: 580.0
  episodes_this_iter: 96
  episodes_total: 26400
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20213.598
    learner:
      agent-0:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9550145268440247
        entropy_coeff: 0.0017600000137463212
        kl: 0.00220031407661736
        model: {}
        policy_loss: -0.0033939999993890524
        total_loss: -0.003048144979402423
        vf_explained_var: 0.0014873594045639038
        vf_loss: 20.26679229736328
      agent-1:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 1.132629156112671
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013221332337707281
        model: {}
        policy_loss: -0.0035664828028529882
        total_loss: -0.0034856905695050955
        vf_explained_var: -0.022006630897521973
        vf_loss: 20.742210388183594
      agent-2:
        cur_kl_coeff: 7.940934196053428e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1606475114822388
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014790287241339684
        model: {}
        policy_loss: -0.0032673594541847706
        total_loss: -0.003333712462335825
        vf_explained_var: 0.028474509716033936
        vf_loss: 19.763883590698242
      agent-3:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39296549558639526
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011444597039371729
        model: {}
        policy_loss: -0.002607673406600952
        total_loss: -0.0015057201962918043
        vf_explained_var: 0.11494177579879761
        vf_loss: 17.935710906982422
      agent-4:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8936782479286194
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018106520874425769
        model: {}
        policy_loss: -0.00373852695338428
        total_loss: -0.003428164403885603
        vf_explained_var: 0.0722808986902237
        vf_loss: 18.83234405517578
      agent-5:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7060016393661499
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007882211357355118
        model: {}
        policy_loss: -0.0033078333362936974
        total_loss: -0.0027375323697924614
        vf_explained_var: 0.10618095099925995
        vf_loss: 18.128643035888672
    load_time_ms: 14870.942
    num_steps_sampled: 26400000
    num_steps_trained: 26400000
    sample_time_ms: 89815.68
    update_time_ms: 21.23
  iterations_since_restore: 255
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.713714285714289
    ram_util_percent: 9.584571428571431
  pid: 4061
  policy_reward_max:
    agent-0: 180.66666666666657
    agent-1: 180.66666666666657
    agent-2: 180.66666666666657
    agent-3: 180.66666666666657
    agent-4: 180.66666666666657
    agent-5: 180.66666666666657
  policy_reward_mean:
    agent-0: 156.47
    agent-1: 156.47
    agent-2: 156.47
    agent-3: 156.47
    agent-4: 156.47
    agent-5: 156.47
  policy_reward_min:
    agent-0: 96.66666666666664
    agent-1: 96.66666666666664
    agent-2: 96.66666666666664
    agent-3: 96.66666666666664
    agent-4: 96.66666666666664
    agent-5: 96.66666666666664
  sampler_perf:
    mean_env_wait_ms: 23.86272365193038
    mean_inference_ms: 12.260679625231289
    mean_processing_ms: 50.64184085241294
  time_since_restore: 32785.23340678215
  time_this_iter_s: 123.19620203971863
  time_total_s: 35996.29709291458
  timestamp: 1637050352
  timesteps_since_restore: 24480000
  timesteps_this_iter: 96000
  timesteps_total: 26400000
  training_iteration: 275
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    275 |          35996.3 | 26400000 |   938.82 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 52
    apples_agent-0_mean: 1.85
    apples_agent-0_min: 0
    apples_agent-1_max: 88
    apples_agent-1_mean: 23.51
    apples_agent-1_min: 0
    apples_agent-2_max: 92
    apples_agent-2_mean: 9.95
    apples_agent-2_min: 0
    apples_agent-3_max: 107
    apples_agent-3_mean: 56.56
    apples_agent-3_min: 22
    apples_agent-4_max: 81
    apples_agent-4_mean: 3.26
    apples_agent-4_min: 0
    apples_agent-5_max: 163
    apples_agent-5_mean: 101.19
    apples_agent-5_min: 2
    cleaning_beam_agent-0_max: 507
    cleaning_beam_agent-0_mean: 412.04
    cleaning_beam_agent-0_min: 233
    cleaning_beam_agent-1_max: 401
    cleaning_beam_agent-1_mean: 242.95
    cleaning_beam_agent-1_min: 138
    cleaning_beam_agent-2_max: 456
    cleaning_beam_agent-2_mean: 293.57
    cleaning_beam_agent-2_min: 126
    cleaning_beam_agent-3_max: 78
    cleaning_beam_agent-3_mean: 14.47
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 592
    cleaning_beam_agent-4_mean: 456.24
    cleaning_beam_agent-4_min: 287
    cleaning_beam_agent-5_max: 639
    cleaning_beam_agent-5_mean: 54.94
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-14-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1111.0000000000023
  episode_reward_mean: 927.7199999999851
  episode_reward_min: 236.9999999999975
  episodes_this_iter: 96
  episodes_total: 26496
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20238.429
    learner:
      agent-0:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9575111865997314
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016320026479661465
        model: {}
        policy_loss: -0.0033412063494324684
        total_loss: -0.0028685759752988815
        vf_explained_var: 0.02987496554851532
        vf_loss: 21.57850456237793
      agent-1:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 1.137199878692627
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015717132482677698
        model: {}
        policy_loss: -0.004052840173244476
        total_loss: -0.00379212386906147
        vf_explained_var: -0.02389061450958252
        vf_loss: 22.62187957763672
      agent-2:
        cur_kl_coeff: 3.970467098026714e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 1.152948021888733
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012426057364791632
        model: {}
        policy_loss: -0.0034974864684045315
        total_loss: -0.00333704287186265
        vf_explained_var: 0.014114350080490112
        vf_loss: 21.896331787109375
      agent-3:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3898637294769287
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007822327897883952
        model: {}
        policy_loss: -0.0024755154736340046
        total_loss: -0.0012070827651768923
        vf_explained_var: 0.11482679843902588
        vf_loss: 19.545940399169922
      agent-4:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8876596093177795
        entropy_coeff: 0.0017600000137463212
        kl: 0.001540199387818575
        model: {}
        policy_loss: -0.003666447475552559
        total_loss: -0.003228718414902687
        vf_explained_var: 0.09459218382835388
        vf_loss: 20.000083923339844
      agent-5:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7186384201049805
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011054226197302341
        model: {}
        policy_loss: -0.003466040827333927
        total_loss: -0.0028012448456138372
        vf_explained_var: 0.12645983695983887
        vf_loss: 19.295984268188477
    load_time_ms: 14867.739
    num_steps_sampled: 26496000
    num_steps_trained: 26496000
    sample_time_ms: 89727.935
    update_time_ms: 21.063
  iterations_since_restore: 256
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.176666666666668
    ram_util_percent: 9.681666666666667
  pid: 4061
  policy_reward_max:
    agent-0: 185.16666666666646
    agent-1: 185.16666666666646
    agent-2: 185.16666666666646
    agent-3: 185.16666666666646
    agent-4: 185.16666666666646
    agent-5: 185.16666666666646
  policy_reward_mean:
    agent-0: 154.6199999999999
    agent-1: 154.6199999999999
    agent-2: 154.6199999999999
    agent-3: 154.6199999999999
    agent-4: 154.6199999999999
    agent-5: 154.6199999999999
  policy_reward_min:
    agent-0: 39.50000000000001
    agent-1: 39.50000000000001
    agent-2: 39.50000000000001
    agent-3: 39.50000000000001
    agent-4: 39.50000000000001
    agent-5: 39.50000000000001
  sampler_perf:
    mean_env_wait_ms: 23.864292491865623
    mean_inference_ms: 12.260236364971972
    mean_processing_ms: 50.63932966816966
  time_since_restore: 32910.91762185097
  time_this_iter_s: 125.68421506881714
  time_total_s: 36121.9813079834
  timestamp: 1637050478
  timesteps_since_restore: 24576000
  timesteps_this_iter: 96000
  timesteps_total: 26496000
  training_iteration: 276
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 16.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    276 |            36122 | 26496000 |   927.72 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 35
    apples_agent-0_mean: 1.66
    apples_agent-0_min: 0
    apples_agent-1_max: 94
    apples_agent-1_mean: 26.66
    apples_agent-1_min: 0
    apples_agent-2_max: 118
    apples_agent-2_mean: 14.42
    apples_agent-2_min: 0
    apples_agent-3_max: 110
    apples_agent-3_mean: 54.6
    apples_agent-3_min: 19
    apples_agent-4_max: 39
    apples_agent-4_mean: 2.07
    apples_agent-4_min: 0
    apples_agent-5_max: 167
    apples_agent-5_mean: 102.05
    apples_agent-5_min: 7
    cleaning_beam_agent-0_max: 527
    cleaning_beam_agent-0_mean: 410.77
    cleaning_beam_agent-0_min: 290
    cleaning_beam_agent-1_max: 366
    cleaning_beam_agent-1_mean: 224.95
    cleaning_beam_agent-1_min: 111
    cleaning_beam_agent-2_max: 524
    cleaning_beam_agent-2_mean: 272.32
    cleaning_beam_agent-2_min: 97
    cleaning_beam_agent-3_max: 59
    cleaning_beam_agent-3_mean: 16.53
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 550
    cleaning_beam_agent-4_mean: 456.29
    cleaning_beam_agent-4_min: 286
    cleaning_beam_agent-5_max: 592
    cleaning_beam_agent-5_mean: 57.57
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.05
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-16-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1104.9999999999816
  episode_reward_mean: 919.0199999999843
  episode_reward_min: 120.00000000000068
  episodes_this_iter: 96
  episodes_total: 26592
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20219.097
    learner:
      agent-0:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.94693922996521
        entropy_coeff: 0.0017600000137463212
        kl: 0.002704153535887599
        model: {}
        policy_loss: -0.0038382946513593197
        total_loss: -0.003129554446786642
        vf_explained_var: 0.03221103549003601
        vf_loss: 23.75351905822754
      agent-1:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1469417810440063
        entropy_coeff: 0.0017600000137463212
        kl: 0.001915609696879983
        model: {}
        policy_loss: -0.004032324068248272
        total_loss: -0.0035627037286758423
        vf_explained_var: -0.013764813542366028
        vf_loss: 24.882427215576172
      agent-2:
        cur_kl_coeff: 1.985233549013357e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1728899478912354
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013552187010645866
        model: {}
        policy_loss: -0.003477781545370817
        total_loss: -0.003161396598443389
        vf_explained_var: 0.02937278151512146
        vf_loss: 23.80669403076172
      agent-3:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4113020896911621
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014204818289726973
        model: {}
        policy_loss: -0.002939382568001747
        total_loss: -0.001575724221765995
        vf_explained_var: 0.14916642010211945
        vf_loss: 20.875484466552734
      agent-4:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8911259174346924
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022970191203057766
        model: {}
        policy_loss: -0.004174711182713509
        total_loss: -0.003618163987994194
        vf_explained_var: 0.13348254561424255
        vf_loss: 21.249298095703125
      agent-5:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7085788249969482
        entropy_coeff: 0.0017600000137463212
        kl: 0.001300490228459239
        model: {}
        policy_loss: -0.0037630144506692886
        total_loss: -0.0028469348326325417
        vf_explained_var: 0.11809161305427551
        vf_loss: 21.631784439086914
    load_time_ms: 14930.59
    num_steps_sampled: 26592000
    num_steps_trained: 26592000
    sample_time_ms: 89736.73
    update_time_ms: 21.018
  iterations_since_restore: 257
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.152777777777779
    ram_util_percent: 9.611111111111112
  pid: 4061
  policy_reward_max:
    agent-0: 184.1666666666663
    agent-1: 184.1666666666663
    agent-2: 184.1666666666663
    agent-3: 184.1666666666663
    agent-4: 184.1666666666663
    agent-5: 184.1666666666663
  policy_reward_mean:
    agent-0: 153.1699999999999
    agent-1: 153.1699999999999
    agent-2: 153.1699999999999
    agent-3: 153.1699999999999
    agent-4: 153.1699999999999
    agent-5: 153.1699999999999
  policy_reward_min:
    agent-0: 20.000000000000018
    agent-1: 20.000000000000018
    agent-2: 20.000000000000018
    agent-3: 20.000000000000018
    agent-4: 20.000000000000018
    agent-5: 20.000000000000018
  sampler_perf:
    mean_env_wait_ms: 23.865767208883753
    mean_inference_ms: 12.259899756575287
    mean_processing_ms: 50.637884637602816
  time_since_restore: 33037.79512524605
  time_this_iter_s: 126.87750339508057
  time_total_s: 36248.85881137848
  timestamp: 1637050605
  timesteps_since_restore: 24672000
  timesteps_this_iter: 96000
  timesteps_total: 26592000
  training_iteration: 277
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    277 |          36248.9 | 26592000 |   919.02 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 43
    apples_agent-0_mean: 2.1
    apples_agent-0_min: 0
    apples_agent-1_max: 99
    apples_agent-1_mean: 24.66
    apples_agent-1_min: 0
    apples_agent-2_max: 148
    apples_agent-2_mean: 12.59
    apples_agent-2_min: 0
    apples_agent-3_max: 113
    apples_agent-3_mean: 59.1
    apples_agent-3_min: 31
    apples_agent-4_max: 63
    apples_agent-4_mean: 1.56
    apples_agent-4_min: 0
    apples_agent-5_max: 239
    apples_agent-5_mean: 107.17
    apples_agent-5_min: 7
    cleaning_beam_agent-0_max: 532
    cleaning_beam_agent-0_mean: 428.36
    cleaning_beam_agent-0_min: 323
    cleaning_beam_agent-1_max: 463
    cleaning_beam_agent-1_mean: 251.95
    cleaning_beam_agent-1_min: 98
    cleaning_beam_agent-2_max: 497
    cleaning_beam_agent-2_mean: 293.59
    cleaning_beam_agent-2_min: 136
    cleaning_beam_agent-3_max: 64
    cleaning_beam_agent-3_mean: 12.9
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 560
    cleaning_beam_agent-4_mean: 467.43
    cleaning_beam_agent-4_min: 323
    cleaning_beam_agent-5_max: 467
    cleaning_beam_agent-5_mean: 52.87
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-18-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1135.9999999999964
  episode_reward_mean: 958.7699999999855
  episode_reward_min: 506.0000000000128
  episodes_this_iter: 96
  episodes_total: 26688
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20226.622
    learner:
      agent-0:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9217025637626648
        entropy_coeff: 0.0017600000137463212
        kl: 0.001862485776655376
        model: {}
        policy_loss: -0.003209260292351246
        total_loss: -0.002762187272310257
        vf_explained_var: 0.03842160105705261
        vf_loss: 20.692703247070312
      agent-1:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 1.134756326675415
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018114123959094286
        model: {}
        policy_loss: -0.003950563725084066
        total_loss: -0.0037767922040075064
        vf_explained_var: -0.018959343433380127
        vf_loss: 21.709409713745117
      agent-2:
        cur_kl_coeff: 9.926167745066785e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1572396755218506
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016103095840662718
        model: {}
        policy_loss: -0.0037933683488518
        total_loss: -0.003746188711374998
        vf_explained_var: 0.020192325115203857
        vf_loss: 20.83922004699707
      agent-3:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3746234178543091
        entropy_coeff: 0.0017600000137463212
        kl: 0.000890321796759963
        model: {}
        policy_loss: -0.002187996404245496
        total_loss: -0.0010066532995551825
        vf_explained_var: 0.12915053963661194
        vf_loss: 18.406803131103516
      agent-4:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.887035608291626
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011601527221500874
        model: {}
        policy_loss: -0.0036478033289313316
        total_loss: -0.0031805678736418486
        vf_explained_var: 0.045935094356536865
        vf_loss: 20.284191131591797
      agent-5:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6794179677963257
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008406631532125175
        model: {}
        policy_loss: -0.0030862013809382915
        total_loss: -0.0023443507961928844
        vf_explained_var: 0.085834801197052
        vf_loss: 19.376256942749023
    load_time_ms: 15085.998
    num_steps_sampled: 26688000
    num_steps_trained: 26688000
    sample_time_ms: 89865.678
    update_time_ms: 21.042
  iterations_since_restore: 258
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.196111111111112
    ram_util_percent: 9.609444444444446
  pid: 4061
  policy_reward_max:
    agent-0: 189.33333333333297
    agent-1: 189.33333333333297
    agent-2: 189.33333333333297
    agent-3: 189.33333333333297
    agent-4: 189.33333333333297
    agent-5: 189.33333333333297
  policy_reward_mean:
    agent-0: 159.7949999999999
    agent-1: 159.7949999999999
    agent-2: 159.7949999999999
    agent-3: 159.7949999999999
    agent-4: 159.7949999999999
    agent-5: 159.7949999999999
  policy_reward_min:
    agent-0: 84.33333333333347
    agent-1: 84.33333333333347
    agent-2: 84.33333333333347
    agent-3: 84.33333333333347
    agent-4: 84.33333333333347
    agent-5: 84.33333333333347
  sampler_perf:
    mean_env_wait_ms: 23.868787248846644
    mean_inference_ms: 12.25976937868464
    mean_processing_ms: 50.63695553247554
  time_since_restore: 33163.75371336937
  time_this_iter_s: 125.95858812332153
  time_total_s: 36374.8173995018
  timestamp: 1637050731
  timesteps_since_restore: 24768000
  timesteps_this_iter: 96000
  timesteps_total: 26688000
  training_iteration: 278
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    278 |          36374.8 | 26688000 |   958.77 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 33
    apples_agent-0_mean: 1.41
    apples_agent-0_min: 0
    apples_agent-1_max: 99
    apples_agent-1_mean: 19.26
    apples_agent-1_min: 0
    apples_agent-2_max: 103
    apples_agent-2_mean: 15.08
    apples_agent-2_min: 0
    apples_agent-3_max: 96
    apples_agent-3_mean: 57.06
    apples_agent-3_min: 23
    apples_agent-4_max: 77
    apples_agent-4_mean: 2.59
    apples_agent-4_min: 0
    apples_agent-5_max: 184
    apples_agent-5_mean: 104.77
    apples_agent-5_min: 2
    cleaning_beam_agent-0_max: 534
    cleaning_beam_agent-0_mean: 423.37
    cleaning_beam_agent-0_min: 298
    cleaning_beam_agent-1_max: 489
    cleaning_beam_agent-1_mean: 264.04
    cleaning_beam_agent-1_min: 136
    cleaning_beam_agent-2_max: 449
    cleaning_beam_agent-2_mean: 281.68
    cleaning_beam_agent-2_min: 111
    cleaning_beam_agent-3_max: 52
    cleaning_beam_agent-3_mean: 14.27
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 596
    cleaning_beam_agent-4_mean: 482.91
    cleaning_beam_agent-4_min: 336
    cleaning_beam_agent-5_max: 462
    cleaning_beam_agent-5_mean: 40.69
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-20-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1101.99999999999
  episode_reward_mean: 957.3099999999854
  episode_reward_min: 392.0000000000075
  episodes_this_iter: 96
  episodes_total: 26784
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20182.838
    learner:
      agent-0:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9365417957305908
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010637163650244474
        model: {}
        policy_loss: -0.0032217889092862606
        total_loss: -0.0026809279806911945
        vf_explained_var: 0.02014806866645813
        vf_loss: 21.89181137084961
      agent-1:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 1.128415822982788
        entropy_coeff: 0.0017600000137463212
        kl: 0.001955256797373295
        model: {}
        policy_loss: -0.0038143699057400227
        total_loss: -0.003588642692193389
        vf_explained_var: -0.0022581815719604492
        vf_loss: 22.11741065979004
      agent-2:
        cur_kl_coeff: 4.963083872533392e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1625779867172241
        entropy_coeff: 0.0017600000137463212
        kl: 0.001016877475194633
        model: {}
        policy_loss: -0.0032525649294257164
        total_loss: -0.003146335482597351
        vf_explained_var: 0.03293520212173462
        vf_loss: 21.52362823486328
      agent-3:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3851845860481262
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011963907163590193
        model: {}
        policy_loss: -0.002403419464826584
        total_loss: -0.0011717472225427628
        vf_explained_var: 0.13477405905723572
        vf_loss: 19.09598159790039
      agent-4:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8803133964538574
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014337110333144665
        model: {}
        policy_loss: -0.0037966594099998474
        total_loss: -0.003291298635303974
        vf_explained_var: 0.06856343150138855
        vf_loss: 20.547122955322266
      agent-5:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6919598579406738
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007732587982900441
        model: {}
        policy_loss: -0.0030975136905908585
        total_loss: -0.0023591129574924707
        vf_explained_var: 0.10743746161460876
        vf_loss: 19.562484741210938
    load_time_ms: 15182.429
    num_steps_sampled: 26784000
    num_steps_trained: 26784000
    sample_time_ms: 89893.483
    update_time_ms: 21.229
  iterations_since_restore: 259
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.577777777777778
    ram_util_percent: 9.60777777777778
  pid: 4061
  policy_reward_max:
    agent-0: 183.66666666666652
    agent-1: 183.66666666666652
    agent-2: 183.66666666666652
    agent-3: 183.66666666666652
    agent-4: 183.66666666666652
    agent-5: 183.66666666666652
  policy_reward_mean:
    agent-0: 159.55166666666656
    agent-1: 159.55166666666656
    agent-2: 159.55166666666656
    agent-3: 159.55166666666656
    agent-4: 159.55166666666656
    agent-5: 159.55166666666656
  policy_reward_min:
    agent-0: 65.3333333333331
    agent-1: 65.3333333333331
    agent-2: 65.3333333333331
    agent-3: 65.3333333333331
    agent-4: 65.3333333333331
    agent-5: 65.3333333333331
  sampler_perf:
    mean_env_wait_ms: 23.8717697490151
    mean_inference_ms: 12.259484672624405
    mean_processing_ms: 50.63617296454966
  time_since_restore: 33289.969720840454
  time_this_iter_s: 126.2160074710846
  time_total_s: 36501.033406972885
  timestamp: 1637050857
  timesteps_since_restore: 24864000
  timesteps_this_iter: 96000
  timesteps_total: 26784000
  training_iteration: 279
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    279 |            36501 | 26784000 |   957.31 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 48
    apples_agent-0_mean: 1.87
    apples_agent-0_min: 0
    apples_agent-1_max: 105
    apples_agent-1_mean: 22.3
    apples_agent-1_min: 0
    apples_agent-2_max: 219
    apples_agent-2_mean: 13.49
    apples_agent-2_min: 0
    apples_agent-3_max: 113
    apples_agent-3_mean: 57.17
    apples_agent-3_min: 21
    apples_agent-4_max: 53
    apples_agent-4_mean: 0.77
    apples_agent-4_min: 0
    apples_agent-5_max: 347
    apples_agent-5_mean: 104.15
    apples_agent-5_min: 42
    cleaning_beam_agent-0_max: 517
    cleaning_beam_agent-0_mean: 420.07
    cleaning_beam_agent-0_min: 233
    cleaning_beam_agent-1_max: 392
    cleaning_beam_agent-1_mean: 253.1
    cleaning_beam_agent-1_min: 109
    cleaning_beam_agent-2_max: 512
    cleaning_beam_agent-2_mean: 303.74
    cleaning_beam_agent-2_min: 139
    cleaning_beam_agent-3_max: 65
    cleaning_beam_agent-3_mean: 14.49
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 611
    cleaning_beam_agent-4_mean: 489.05
    cleaning_beam_agent-4_min: 299
    cleaning_beam_agent-5_max: 407
    cleaning_beam_agent-5_mean: 53.79
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-23-01
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1106.999999999984
  episode_reward_mean: 955.0399999999831
  episode_reward_min: 302.99999999999983
  episodes_this_iter: 96
  episodes_total: 26880
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20198.104
    learner:
      agent-0:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9425609111785889
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016704233130440116
        model: {}
        policy_loss: -0.0033798767253756523
        total_loss: -0.002983422949910164
        vf_explained_var: -0.0035659223794937134
        vf_loss: 20.553564071655273
      agent-1:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1450783014297485
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013680392876267433
        model: {}
        policy_loss: -0.0037988186813890934
        total_loss: -0.00369445001706481
        vf_explained_var: -0.050338953733444214
        vf_loss: 21.19706153869629
      agent-2:
        cur_kl_coeff: 2.481541936266696e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 1.150597095489502
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014350770507007837
        model: {}
        policy_loss: -0.0036336155608296394
        total_loss: -0.0036928632762283087
        vf_explained_var: 0.02197265625
        vf_loss: 19.658031463623047
      agent-3:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3595527708530426
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012025496689602733
        model: {}
        policy_loss: -0.002197147347033024
        total_loss: -0.0010502571240067482
        vf_explained_var: 0.1117231696844101
        vf_loss: 17.797046661376953
      agent-4:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8637293577194214
        entropy_coeff: 0.0017600000137463212
        kl: 0.001968853175640106
        model: {}
        policy_loss: -0.003833032213151455
        total_loss: -0.003457562765106559
        vf_explained_var: 0.054265037178993225
        vf_loss: 18.956321716308594
      agent-5:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6901034116744995
        entropy_coeff: 0.0017600000137463212
        kl: 0.001148824580013752
        model: {}
        policy_loss: -0.003261284437030554
        total_loss: -0.0026246728375554085
        vf_explained_var: 0.07730460166931152
        vf_loss: 18.511919021606445
    load_time_ms: 15024.127
    num_steps_sampled: 26880000
    num_steps_trained: 26880000
    sample_time_ms: 89733.069
    update_time_ms: 21.42
  iterations_since_restore: 260
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.540677966101693
    ram_util_percent: 9.664971751412429
  pid: 4061
  policy_reward_max:
    agent-0: 184.49999999999983
    agent-1: 184.49999999999983
    agent-2: 184.49999999999983
    agent-3: 184.49999999999983
    agent-4: 184.49999999999983
    agent-5: 184.49999999999983
  policy_reward_mean:
    agent-0: 159.17333333333326
    agent-1: 159.17333333333326
    agent-2: 159.17333333333326
    agent-3: 159.17333333333326
    agent-4: 159.17333333333326
    agent-5: 159.17333333333326
  policy_reward_min:
    agent-0: 50.49999999999988
    agent-1: 50.49999999999988
    agent-2: 50.49999999999988
    agent-3: 50.49999999999988
    agent-4: 50.49999999999988
    agent-5: 50.49999999999988
  sampler_perf:
    mean_env_wait_ms: 23.874707608918875
    mean_inference_ms: 12.259032818022568
    mean_processing_ms: 50.63440922714309
  time_since_restore: 33413.745865345
  time_this_iter_s: 123.77614450454712
  time_total_s: 36624.80955147743
  timestamp: 1637050981
  timesteps_since_restore: 24960000
  timesteps_this_iter: 96000
  timesteps_total: 26880000
  training_iteration: 280
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    280 |          36624.8 | 26880000 |   955.04 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 32
    apples_agent-0_mean: 1.08
    apples_agent-0_min: 0
    apples_agent-1_max: 106
    apples_agent-1_mean: 26.98
    apples_agent-1_min: 0
    apples_agent-2_max: 236
    apples_agent-2_mean: 19.87
    apples_agent-2_min: 0
    apples_agent-3_max: 214
    apples_agent-3_mean: 60.52
    apples_agent-3_min: 22
    apples_agent-4_max: 54
    apples_agent-4_mean: 1.57
    apples_agent-4_min: 0
    apples_agent-5_max: 259
    apples_agent-5_mean: 102.78
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 519
    cleaning_beam_agent-0_mean: 404.86
    cleaning_beam_agent-0_min: 290
    cleaning_beam_agent-1_max: 437
    cleaning_beam_agent-1_mean: 252.66
    cleaning_beam_agent-1_min: 121
    cleaning_beam_agent-2_max: 437
    cleaning_beam_agent-2_mean: 287.22
    cleaning_beam_agent-2_min: 97
    cleaning_beam_agent-3_max: 50
    cleaning_beam_agent-3_mean: 12.01
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 606
    cleaning_beam_agent-4_mean: 484.84
    cleaning_beam_agent-4_min: 267
    cleaning_beam_agent-5_max: 631
    cleaning_beam_agent-5_mean: 64.97
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-25-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1104.9999999999834
  episode_reward_mean: 940.3999999999846
  episode_reward_min: 359.00000000000455
  episodes_this_iter: 96
  episodes_total: 26976
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20201.351
    learner:
      agent-0:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9461003541946411
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014397945487871766
        model: {}
        policy_loss: -0.003091334830969572
        total_loss: -0.0027324992697685957
        vf_explained_var: 0.03678475320339203
        vf_loss: 20.23971939086914
      agent-1:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 1.135704517364502
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018755056662485003
        model: {}
        policy_loss: -0.0043914783746004105
        total_loss: -0.004278106149286032
        vf_explained_var: -0.013385593891143799
        vf_loss: 21.12209701538086
      agent-2:
        cur_kl_coeff: 1.240770968133348e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1647157669067383
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011552318464964628
        model: {}
        policy_loss: -0.00356415007263422
        total_loss: -0.003575277980417013
        vf_explained_var: 0.025690019130706787
        vf_loss: 20.38770294189453
      agent-3:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36364805698394775
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012090547243133187
        model: {}
        policy_loss: -0.002309749834239483
        total_loss: -0.0011463190894573927
        vf_explained_var: 0.13236837089061737
        vf_loss: 18.0345458984375
      agent-4:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8729245662689209
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015928003704175353
        model: {}
        policy_loss: -0.003778374521061778
        total_loss: -0.0033341036178171635
        vf_explained_var: 0.04701143503189087
        vf_loss: 19.806203842163086
      agent-5:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7015981674194336
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015329805901274085
        model: {}
        policy_loss: -0.0037611108273267746
        total_loss: -0.003095627762377262
        vf_explained_var: 0.08625374734401703
        vf_loss: 19.002952575683594
    load_time_ms: 15254.517
    num_steps_sampled: 26976000
    num_steps_trained: 26976000
    sample_time_ms: 89780.271
    update_time_ms: 21.544
  iterations_since_restore: 261
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.170165745856355
    ram_util_percent: 9.680662983425416
  pid: 4061
  policy_reward_max:
    agent-0: 184.16666666666634
    agent-1: 184.16666666666634
    agent-2: 184.16666666666634
    agent-3: 184.16666666666634
    agent-4: 184.16666666666634
    agent-5: 184.16666666666634
  policy_reward_mean:
    agent-0: 156.73333333333323
    agent-1: 156.73333333333323
    agent-2: 156.73333333333323
    agent-3: 156.73333333333323
    agent-4: 156.73333333333323
    agent-5: 156.73333333333323
  policy_reward_min:
    agent-0: 59.83333333333315
    agent-1: 59.83333333333315
    agent-2: 59.83333333333315
    agent-3: 59.83333333333315
    agent-4: 59.83333333333315
    agent-5: 59.83333333333315
  sampler_perf:
    mean_env_wait_ms: 23.876958046035092
    mean_inference_ms: 12.258429668045487
    mean_processing_ms: 50.63181843620564
  time_since_restore: 33540.27181959152
  time_this_iter_s: 126.525954246521
  time_total_s: 36751.33550572395
  timestamp: 1637051108
  timesteps_since_restore: 25056000
  timesteps_this_iter: 96000
  timesteps_total: 26976000
  training_iteration: 281
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    281 |          36751.3 | 26976000 |    940.4 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 27
    apples_agent-0_mean: 0.96
    apples_agent-0_min: 0
    apples_agent-1_max: 108
    apples_agent-1_mean: 26.22
    apples_agent-1_min: 0
    apples_agent-2_max: 110
    apples_agent-2_mean: 13.08
    apples_agent-2_min: 0
    apples_agent-3_max: 123
    apples_agent-3_mean: 56.53
    apples_agent-3_min: 28
    apples_agent-4_max: 43
    apples_agent-4_mean: 0.56
    apples_agent-4_min: 0
    apples_agent-5_max: 171
    apples_agent-5_mean: 102.11
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 493
    cleaning_beam_agent-0_mean: 391.59
    cleaning_beam_agent-0_min: 259
    cleaning_beam_agent-1_max: 401
    cleaning_beam_agent-1_mean: 250.29
    cleaning_beam_agent-1_min: 136
    cleaning_beam_agent-2_max: 624
    cleaning_beam_agent-2_mean: 303.08
    cleaning_beam_agent-2_min: 123
    cleaning_beam_agent-3_max: 54
    cleaning_beam_agent-3_mean: 14.67
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 616
    cleaning_beam_agent-4_mean: 499.21
    cleaning_beam_agent-4_min: 253
    cleaning_beam_agent-5_max: 931
    cleaning_beam_agent-5_mean: 58.43
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-27-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1126.9999999999807
  episode_reward_mean: 947.8299999999832
  episode_reward_min: 587.000000000002
  episodes_this_iter: 96
  episodes_total: 27072
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20193.027
    learner:
      agent-0:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9374212026596069
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010909800184890628
        model: {}
        policy_loss: -0.003043210133910179
        total_loss: -0.0027128830552101135
        vf_explained_var: -0.005996093153953552
        vf_loss: 19.801898956298828
      agent-1:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1301143169403076
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011264894856140018
        model: {}
        policy_loss: -0.0035539008677005768
        total_loss: -0.0035370681434869766
        vf_explained_var: -0.03992331027984619
        vf_loss: 20.058334350585938
      agent-2:
        cur_kl_coeff: 6.20385484066674e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1501998901367188
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014263717457652092
        model: {}
        policy_loss: -0.0035845409147441387
        total_loss: -0.0036522606387734413
        vf_explained_var: -0.01110200583934784
        vf_loss: 19.566299438476562
      agent-3:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3672003746032715
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008519715629518032
        model: {}
        policy_loss: -0.0021477453410625458
        total_loss: -0.0010138312354683876
        vf_explained_var: 0.07214963436126709
        vf_loss: 17.80188751220703
      agent-4:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8679201602935791
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016539986245334148
        model: {}
        policy_loss: -0.003635127330198884
        total_loss: -0.003310529049485922
        vf_explained_var: 0.03624217212200165
        vf_loss: 18.521385192871094
      agent-5:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6953707337379456
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013727998593822122
        model: {}
        policy_loss: -0.003000529482960701
        total_loss: -0.002412091940641403
        vf_explained_var: 0.05593816936016083
        vf_loss: 18.12286949157715
    load_time_ms: 15270.715
    num_steps_sampled: 27072000
    num_steps_trained: 27072000
    sample_time_ms: 89833.156
    update_time_ms: 21.254
  iterations_since_restore: 262
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.410795454545456
    ram_util_percent: 9.58125
  pid: 4061
  policy_reward_max:
    agent-0: 187.83333333333266
    agent-1: 187.83333333333266
    agent-2: 187.83333333333266
    agent-3: 187.83333333333266
    agent-4: 187.83333333333266
    agent-5: 187.83333333333266
  policy_reward_mean:
    agent-0: 157.97166666666664
    agent-1: 157.97166666666664
    agent-2: 157.97166666666664
    agent-3: 157.97166666666664
    agent-4: 157.97166666666664
    agent-5: 157.97166666666664
  policy_reward_min:
    agent-0: 97.83333333333394
    agent-1: 97.83333333333394
    agent-2: 97.83333333333394
    agent-3: 97.83333333333394
    agent-4: 97.83333333333394
    agent-5: 97.83333333333394
  sampler_perf:
    mean_env_wait_ms: 23.879414454044213
    mean_inference_ms: 12.258194609387486
    mean_processing_ms: 50.6293008110896
  time_since_restore: 33663.89517450333
  time_this_iter_s: 123.6233549118042
  time_total_s: 36874.95886063576
  timestamp: 1637051232
  timesteps_since_restore: 25152000
  timesteps_this_iter: 96000
  timesteps_total: 27072000
  training_iteration: 282
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    282 |            36875 | 27072000 |   947.83 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 44
    apples_agent-0_mean: 1.25
    apples_agent-0_min: 0
    apples_agent-1_max: 84
    apples_agent-1_mean: 26.58
    apples_agent-1_min: 0
    apples_agent-2_max: 102
    apples_agent-2_mean: 16.77
    apples_agent-2_min: 0
    apples_agent-3_max: 144
    apples_agent-3_mean: 58.65
    apples_agent-3_min: 28
    apples_agent-4_max: 77
    apples_agent-4_mean: 1.31
    apples_agent-4_min: 0
    apples_agent-5_max: 187
    apples_agent-5_mean: 102.81
    apples_agent-5_min: 31
    cleaning_beam_agent-0_max: 527
    cleaning_beam_agent-0_mean: 400.47
    cleaning_beam_agent-0_min: 287
    cleaning_beam_agent-1_max: 383
    cleaning_beam_agent-1_mean: 234.63
    cleaning_beam_agent-1_min: 118
    cleaning_beam_agent-2_max: 521
    cleaning_beam_agent-2_mean: 294.86
    cleaning_beam_agent-2_min: 127
    cleaning_beam_agent-3_max: 99
    cleaning_beam_agent-3_mean: 13.47
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 583
    cleaning_beam_agent-4_mean: 493.05
    cleaning_beam_agent-4_min: 315
    cleaning_beam_agent-5_max: 515
    cleaning_beam_agent-5_mean: 45.77
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-29-18
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1130.9999999999961
  episode_reward_mean: 960.2399999999848
  episode_reward_min: 485.00000000001285
  episodes_this_iter: 96
  episodes_total: 27168
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20173.424
    learner:
      agent-0:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9362159967422485
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015196010936051607
        model: {}
        policy_loss: -0.0031019486486911774
        total_loss: -0.0028141913935542107
        vf_explained_var: 0.022467732429504395
        vf_loss: 19.354969024658203
      agent-1:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1339246034622192
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016798573778942227
        model: {}
        policy_loss: -0.00392392510548234
        total_loss: -0.003946284763514996
        vf_explained_var: -0.013643994927406311
        vf_loss: 19.733478546142578
      agent-2:
        cur_kl_coeff: 3.10192742033337e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1500861644744873
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017593377269804478
        model: {}
        policy_loss: -0.0038867811672389507
        total_loss: -0.003985261078923941
        vf_explained_var: 0.015914037823677063
        vf_loss: 19.256694793701172
      agent-3:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36537206172943115
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011516398517414927
        model: {}
        policy_loss: -0.0022859256714582443
        total_loss: -0.0011799406493082643
        vf_explained_var: 0.097688227891922
        vf_loss: 17.490406036376953
      agent-4:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8816302418708801
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020044436678290367
        model: {}
        policy_loss: -0.0037857270799577236
        total_loss: -0.0034884256310760975
        vf_explained_var: 0.043974146246910095
        vf_loss: 18.489700317382812
      agent-5:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7028189301490784
        entropy_coeff: 0.0017600000137463212
        kl: 0.001256760791875422
        model: {}
        policy_loss: -0.003315976122394204
        total_loss: -0.002771258819848299
        vf_explained_var: 0.07897794246673584
        vf_loss: 17.816783905029297
    load_time_ms: 15223.717
    num_steps_sampled: 27168000
    num_steps_trained: 27168000
    sample_time_ms: 89832.635
    update_time_ms: 20.791
  iterations_since_restore: 263
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.168715083798881
    ram_util_percent: 9.606703910614526
  pid: 4061
  policy_reward_max:
    agent-0: 188.49999999999957
    agent-1: 188.49999999999957
    agent-2: 188.49999999999957
    agent-3: 188.49999999999957
    agent-4: 188.49999999999957
    agent-5: 188.49999999999957
  policy_reward_mean:
    agent-0: 160.03999999999988
    agent-1: 160.03999999999988
    agent-2: 160.03999999999988
    agent-3: 160.03999999999988
    agent-4: 160.03999999999988
    agent-5: 160.03999999999988
  policy_reward_min:
    agent-0: 80.83333333333356
    agent-1: 80.83333333333356
    agent-2: 80.83333333333356
    agent-3: 80.83333333333356
    agent-4: 80.83333333333356
    agent-5: 80.83333333333356
  sampler_perf:
    mean_env_wait_ms: 23.881332800647737
    mean_inference_ms: 12.257807826454048
    mean_processing_ms: 50.62658328148073
  time_since_restore: 33789.48278212547
  time_this_iter_s: 125.5876076221466
  time_total_s: 37000.546468257904
  timestamp: 1637051358
  timesteps_since_restore: 25248000
  timesteps_this_iter: 96000
  timesteps_total: 27168000
  training_iteration: 283
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    283 |          37000.5 | 27168000 |   960.24 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 22
    apples_agent-0_mean: 0.77
    apples_agent-0_min: 0
    apples_agent-1_max: 88
    apples_agent-1_mean: 25.55
    apples_agent-1_min: 0
    apples_agent-2_max: 104
    apples_agent-2_mean: 13.75
    apples_agent-2_min: 0
    apples_agent-3_max: 101
    apples_agent-3_mean: 56.08
    apples_agent-3_min: 22
    apples_agent-4_max: 40
    apples_agent-4_mean: 0.5
    apples_agent-4_min: 0
    apples_agent-5_max: 226
    apples_agent-5_mean: 106.39
    apples_agent-5_min: 27
    cleaning_beam_agent-0_max: 486
    cleaning_beam_agent-0_mean: 391.73
    cleaning_beam_agent-0_min: 291
    cleaning_beam_agent-1_max: 419
    cleaning_beam_agent-1_mean: 251.63
    cleaning_beam_agent-1_min: 104
    cleaning_beam_agent-2_max: 479
    cleaning_beam_agent-2_mean: 309.55
    cleaning_beam_agent-2_min: 161
    cleaning_beam_agent-3_max: 39
    cleaning_beam_agent-3_mean: 11.97
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 614
    cleaning_beam_agent-4_mean: 481.18
    cleaning_beam_agent-4_min: 366
    cleaning_beam_agent-5_max: 344
    cleaning_beam_agent-5_mean: 39.06
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-31-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1130.9999999999961
  episode_reward_mean: 991.2299999999843
  episode_reward_min: 614.0000000000023
  episodes_this_iter: 96
  episodes_total: 27264
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20176.907
    learner:
      agent-0:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9359312057495117
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021712365560233593
        model: {}
        policy_loss: -0.0035537462681531906
        total_loss: -0.0031837173737585545
        vf_explained_var: 0.00220334529876709
        vf_loss: 20.17268943786621
      agent-1:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1509783267974854
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014943121932446957
        model: {}
        policy_loss: -0.003946661949157715
        total_loss: -0.0039129797369241714
        vf_explained_var: -0.049641430377960205
        vf_loss: 20.594053268432617
      agent-2:
        cur_kl_coeff: 1.550963710166685e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 1.147324562072754
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015603932552039623
        model: {}
        policy_loss: -0.0036613154225051403
        total_loss: -0.0037662959657609463
        vf_explained_var: 0.013474836945533752
        vf_loss: 19.143123626708984
      agent-3:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3391989767551422
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008821708615869284
        model: {}
        policy_loss: -0.0019891150295734406
        total_loss: -0.0008088564500212669
        vf_explained_var: 0.07473698258399963
        vf_loss: 17.772480010986328
      agent-4:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8690529465675354
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019513203296810389
        model: {}
        policy_loss: -0.003859945572912693
        total_loss: -0.0035189753398299217
        vf_explained_var: 0.044132545590400696
        vf_loss: 18.705028533935547
      agent-5:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6560099720954895
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010233682114630938
        model: {}
        policy_loss: -0.0028941070195287466
        total_loss: -0.002242772374302149
        vf_explained_var: 0.06466960906982422
        vf_loss: 18.059101104736328
    load_time_ms: 15211.571
    num_steps_sampled: 27264000
    num_steps_trained: 27264000
    sample_time_ms: 90023.902
    update_time_ms: 21.301
  iterations_since_restore: 264
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 16.87802197802198
    ram_util_percent: 9.756043956043959
  pid: 4061
  policy_reward_max:
    agent-0: 188.49999999999957
    agent-1: 188.49999999999957
    agent-2: 188.49999999999957
    agent-3: 188.49999999999957
    agent-4: 188.49999999999957
    agent-5: 188.49999999999957
  policy_reward_mean:
    agent-0: 165.20499999999984
    agent-1: 165.20499999999984
    agent-2: 165.20499999999984
    agent-3: 165.20499999999984
    agent-4: 165.20499999999984
    agent-5: 165.20499999999984
  policy_reward_min:
    agent-0: 102.33333333333358
    agent-1: 102.33333333333358
    agent-2: 102.33333333333358
    agent-3: 102.33333333333358
    agent-4: 102.33333333333358
    agent-5: 102.33333333333358
  sampler_perf:
    mean_env_wait_ms: 23.885130119067952
    mean_inference_ms: 12.258242110799824
    mean_processing_ms: 50.62779935634323
  time_since_restore: 33917.41326332092
  time_this_iter_s: 127.93048119544983
  time_total_s: 37128.476949453354
  timestamp: 1637051486
  timesteps_since_restore: 25344000
  timesteps_this_iter: 96000
  timesteps_total: 27264000
  training_iteration: 284
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    284 |          37128.5 | 27264000 |   991.23 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 50
    apples_agent-0_mean: 2.15
    apples_agent-0_min: 0
    apples_agent-1_max: 76
    apples_agent-1_mean: 20.41
    apples_agent-1_min: 0
    apples_agent-2_max: 97
    apples_agent-2_mean: 13.81
    apples_agent-2_min: 0
    apples_agent-3_max: 120
    apples_agent-3_mean: 57.98
    apples_agent-3_min: 31
    apples_agent-4_max: 45
    apples_agent-4_mean: 0.92
    apples_agent-4_min: 0
    apples_agent-5_max: 178
    apples_agent-5_mean: 99.34
    apples_agent-5_min: 15
    cleaning_beam_agent-0_max: 486
    cleaning_beam_agent-0_mean: 401.38
    cleaning_beam_agent-0_min: 256
    cleaning_beam_agent-1_max: 434
    cleaning_beam_agent-1_mean: 256.53
    cleaning_beam_agent-1_min: 117
    cleaning_beam_agent-2_max: 514
    cleaning_beam_agent-2_mean: 303.38
    cleaning_beam_agent-2_min: 140
    cleaning_beam_agent-3_max: 49
    cleaning_beam_agent-3_mean: 15.25
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 564
    cleaning_beam_agent-4_mean: 480.19
    cleaning_beam_agent-4_min: 288
    cleaning_beam_agent-5_max: 459
    cleaning_beam_agent-5_mean: 47.76
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-33-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1129.9999999999939
  episode_reward_mean: 972.6099999999849
  episode_reward_min: 248.9999999999973
  episodes_this_iter: 96
  episodes_total: 27360
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20169.591
    learner:
      agent-0:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9349522590637207
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017229540972039104
        model: {}
        policy_loss: -0.0030952254310250282
        total_loss: -0.002684931270778179
        vf_explained_var: 0.0288131982088089
        vf_loss: 20.558135986328125
      agent-1:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1394884586334229
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015662134392187
        model: {}
        policy_loss: -0.0035917782224714756
        total_loss: -0.003505728906020522
        vf_explained_var: -0.004180103540420532
        vf_loss: 20.915502548217773
      agent-2:
        cur_kl_coeff: 7.754818550833426e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1471307277679443
        entropy_coeff: 0.0017600000137463212
        kl: 0.001638126326724887
        model: {}
        policy_loss: -0.003609870094805956
        total_loss: -0.0035408928524702787
        vf_explained_var: 0.002537444233894348
        vf_loss: 20.879268646240234
      agent-3:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36184462904930115
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006660827784799039
        model: {}
        policy_loss: -0.0021885232999920845
        total_loss: -0.0009010331705212593
        vf_explained_var: 0.0745011568069458
        vf_loss: 19.243371963500977
      agent-4:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8574726581573486
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016897008754312992
        model: {}
        policy_loss: -0.0035082167014479637
        total_loss: -0.003104429692029953
        vf_explained_var: 0.08206687867641449
        vf_loss: 19.129375457763672
      agent-5:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6752427816390991
        entropy_coeff: 0.0017600000137463212
        kl: 0.001133061945438385
        model: {}
        policy_loss: -0.003262745449319482
        total_loss: -0.0025394342374056578
        vf_explained_var: 0.07464632391929626
        vf_loss: 19.117372512817383
    load_time_ms: 15212.252
    num_steps_sampled: 27360000
    num_steps_trained: 27360000
    sample_time_ms: 90035.485
    update_time_ms: 21.751
  iterations_since_restore: 265
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.34772727272727
    ram_util_percent: 9.582954545454545
  pid: 4061
  policy_reward_max:
    agent-0: 188.33333333333297
    agent-1: 188.33333333333297
    agent-2: 188.33333333333297
    agent-3: 188.33333333333297
    agent-4: 188.33333333333297
    agent-5: 188.33333333333297
  policy_reward_mean:
    agent-0: 162.10166666666657
    agent-1: 162.10166666666657
    agent-2: 162.10166666666657
    agent-3: 162.10166666666657
    agent-4: 162.10166666666657
    agent-5: 162.10166666666657
  policy_reward_min:
    agent-0: 41.499999999999986
    agent-1: 41.499999999999986
    agent-2: 41.499999999999986
    agent-3: 41.499999999999986
    agent-4: 41.499999999999986
    agent-5: 41.499999999999986
  sampler_perf:
    mean_env_wait_ms: 23.887444837681414
    mean_inference_ms: 12.257831378413796
    mean_processing_ms: 50.62581160145512
  time_since_restore: 34040.65285563469
  time_this_iter_s: 123.23959231376648
  time_total_s: 37251.71654176712
  timestamp: 1637051609
  timesteps_since_restore: 25440000
  timesteps_this_iter: 96000
  timesteps_total: 27360000
  training_iteration: 285
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    285 |          37251.7 | 27360000 |   972.61 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 28
    apples_agent-0_mean: 1.47
    apples_agent-0_min: 0
    apples_agent-1_max: 103
    apples_agent-1_mean: 25.48
    apples_agent-1_min: 0
    apples_agent-2_max: 101
    apples_agent-2_mean: 11.87
    apples_agent-2_min: 0
    apples_agent-3_max: 118
    apples_agent-3_mean: 60.28
    apples_agent-3_min: 31
    apples_agent-4_max: 60
    apples_agent-4_mean: 1.73
    apples_agent-4_min: 0
    apples_agent-5_max: 243
    apples_agent-5_mean: 99.41
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 530
    cleaning_beam_agent-0_mean: 417.02
    cleaning_beam_agent-0_min: 327
    cleaning_beam_agent-1_max: 360
    cleaning_beam_agent-1_mean: 245.32
    cleaning_beam_agent-1_min: 107
    cleaning_beam_agent-2_max: 513
    cleaning_beam_agent-2_mean: 319.46
    cleaning_beam_agent-2_min: 136
    cleaning_beam_agent-3_max: 65
    cleaning_beam_agent-3_mean: 13.24
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 610
    cleaning_beam_agent-4_mean: 478.67
    cleaning_beam_agent-4_min: 316
    cleaning_beam_agent-5_max: 784
    cleaning_beam_agent-5_mean: 62.89
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-35-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1103.999999999986
  episode_reward_mean: 958.4599999999856
  episode_reward_min: 404.0000000000064
  episodes_this_iter: 96
  episodes_total: 27456
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20175.731
    learner:
      agent-0:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9217305183410645
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024849448818713427
        model: {}
        policy_loss: -0.003319562878459692
        total_loss: -0.0028514389414340258
        vf_explained_var: 0.03932489454746246
        vf_loss: 20.903688430786133
      agent-1:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1492516994476318
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012989290989935398
        model: {}
        policy_loss: -0.003841891884803772
        total_loss: -0.0036420421674847603
        vf_explained_var: -0.022447407245635986
        vf_loss: 22.22529411315918
      agent-2:
        cur_kl_coeff: 3.877409275416713e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1376451253890991
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012155065778642893
        model: {}
        policy_loss: -0.0034604938700795174
        total_loss: -0.003276846371591091
        vf_explained_var: -0.009962722659111023
        vf_loss: 21.858999252319336
      agent-3:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3654513359069824
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010392916155979037
        model: {}
        policy_loss: -0.002025824971497059
        total_loss: -0.0007320111617445946
        vf_explained_var: 0.10459393262863159
        vf_loss: 19.370105743408203
      agent-4:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8752930164337158
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022447158116847277
        model: {}
        policy_loss: -0.003732343204319477
        total_loss: -0.0032545551657676697
        vf_explained_var: 0.07048729062080383
        vf_loss: 20.18303108215332
      agent-5:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6412750482559204
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014453113544732332
        model: {}
        policy_loss: -0.0031709084287285805
        total_loss: -0.002372284419834614
        vf_explained_var: 0.10749390721321106
        vf_loss: 19.27266502380371
    load_time_ms: 15228.698
    num_steps_sampled: 27456000
    num_steps_trained: 27456000
    sample_time_ms: 90100.509
    update_time_ms: 21.923
  iterations_since_restore: 266
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.089502762430937
    ram_util_percent: 9.612707182320444
  pid: 4061
  policy_reward_max:
    agent-0: 183.99999999999974
    agent-1: 183.99999999999974
    agent-2: 183.99999999999974
    agent-3: 183.99999999999974
    agent-4: 183.99999999999974
    agent-5: 183.99999999999974
  policy_reward_mean:
    agent-0: 159.74333333333323
    agent-1: 159.74333333333323
    agent-2: 159.74333333333323
    agent-3: 159.74333333333323
    agent-4: 159.74333333333323
    agent-5: 159.74333333333323
  policy_reward_min:
    agent-0: 67.33333333333312
    agent-1: 67.33333333333312
    agent-2: 67.33333333333312
    agent-3: 67.33333333333312
    agent-4: 67.33333333333312
    agent-5: 67.33333333333312
  sampler_perf:
    mean_env_wait_ms: 23.889995714192906
    mean_inference_ms: 12.25733406567989
    mean_processing_ms: 50.6238442801736
  time_since_restore: 34167.22265434265
  time_this_iter_s: 126.56979870796204
  time_total_s: 37378.28634047508
  timestamp: 1637051736
  timesteps_since_restore: 25536000
  timesteps_this_iter: 96000
  timesteps_total: 27456000
  training_iteration: 286
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    286 |          37378.3 | 27456000 |   958.46 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 0.51
    apples_agent-0_min: 0
    apples_agent-1_max: 90
    apples_agent-1_mean: 24.35
    apples_agent-1_min: 0
    apples_agent-2_max: 94
    apples_agent-2_mean: 10.47
    apples_agent-2_min: 0
    apples_agent-3_max: 121
    apples_agent-3_mean: 57.57
    apples_agent-3_min: 28
    apples_agent-4_max: 65
    apples_agent-4_mean: 1.85
    apples_agent-4_min: 0
    apples_agent-5_max: 169
    apples_agent-5_mean: 99.43
    apples_agent-5_min: 25
    cleaning_beam_agent-0_max: 523
    cleaning_beam_agent-0_mean: 432.24
    cleaning_beam_agent-0_min: 302
    cleaning_beam_agent-1_max: 450
    cleaning_beam_agent-1_mean: 242.25
    cleaning_beam_agent-1_min: 108
    cleaning_beam_agent-2_max: 518
    cleaning_beam_agent-2_mean: 320.16
    cleaning_beam_agent-2_min: 148
    cleaning_beam_agent-3_max: 46
    cleaning_beam_agent-3_mean: 12.63
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 564
    cleaning_beam_agent-4_mean: 477.63
    cleaning_beam_agent-4_min: 307
    cleaning_beam_agent-5_max: 400
    cleaning_beam_agent-5_mean: 44.94
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-37-41
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1134.999999999991
  episode_reward_mean: 979.2399999999874
  episode_reward_min: 338.00000000000176
  episodes_this_iter: 96
  episodes_total: 27552
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20179.021
    learner:
      agent-0:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.913127601146698
        entropy_coeff: 0.0017600000137463212
        kl: 0.002115905750542879
        model: {}
        policy_loss: -0.0032516401261091232
        total_loss: -0.002472848631441593
        vf_explained_var: -0.016009896993637085
        vf_loss: 23.858966827392578
      agent-1:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1433660984039307
        entropy_coeff: 0.0017600000137463212
        kl: 0.002016003243625164
        model: {}
        policy_loss: -0.0036789108999073505
        total_loss: -0.003285977290943265
        vf_explained_var: -0.021993368864059448
        vf_loss: 24.052587509155273
      agent-2:
        cur_kl_coeff: 1.9387046377083564e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1430292129516602
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014782409416511655
        model: {}
        policy_loss: -0.0034738313406705856
        total_loss: -0.0031761908903717995
        vf_explained_var: 0.011765316128730774
        vf_loss: 23.093719482421875
      agent-3:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3559209108352661
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013124137185513973
        model: {}
        policy_loss: -0.002545112743973732
        total_loss: -0.001161663793027401
        vf_explained_var: 0.13578273355960846
        vf_loss: 20.098735809326172
      agent-4:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8801049590110779
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017400780925527215
        model: {}
        policy_loss: -0.0039281584322452545
        total_loss: -0.0033488976769149303
        vf_explained_var: 0.09119988977909088
        vf_loss: 21.282440185546875
      agent-5:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6561036705970764
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012897223932668567
        model: {}
        policy_loss: -0.0032129106111824512
        total_loss: -0.002343876287341118
        vf_explained_var: 0.12734833359718323
        vf_loss: 20.23776626586914
    load_time_ms: 15136.456
    num_steps_sampled: 27552000
    num_steps_trained: 27552000
    sample_time_ms: 90015.105
    update_time_ms: 22.062
  iterations_since_restore: 267
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.318539325842696
    ram_util_percent: 9.680898876404493
  pid: 4061
  policy_reward_max:
    agent-0: 189.1666666666669
    agent-1: 189.1666666666669
    agent-2: 189.1666666666669
    agent-3: 189.1666666666669
    agent-4: 189.1666666666669
    agent-5: 189.1666666666669
  policy_reward_mean:
    agent-0: 163.2066666666665
    agent-1: 163.2066666666665
    agent-2: 163.2066666666665
    agent-3: 163.2066666666665
    agent-4: 163.2066666666665
    agent-5: 163.2066666666665
  policy_reward_min:
    agent-0: 56.33333333333323
    agent-1: 56.33333333333323
    agent-2: 56.33333333333323
    agent-3: 56.33333333333323
    agent-4: 56.33333333333323
    agent-5: 56.33333333333323
  sampler_perf:
    mean_env_wait_ms: 23.892340689082722
    mean_inference_ms: 12.256883827431368
    mean_processing_ms: 50.62104361748753
  time_since_restore: 34292.29645586014
  time_this_iter_s: 125.07380151748657
  time_total_s: 37503.36014199257
  timestamp: 1637051861
  timesteps_since_restore: 25632000
  timesteps_this_iter: 96000
  timesteps_total: 27552000
  training_iteration: 287
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    287 |          37503.4 | 27552000 |   979.24 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 36
    apples_agent-0_mean: 2.86
    apples_agent-0_min: 0
    apples_agent-1_max: 80
    apples_agent-1_mean: 20.87
    apples_agent-1_min: 0
    apples_agent-2_max: 165
    apples_agent-2_mean: 13.13
    apples_agent-2_min: 0
    apples_agent-3_max: 102
    apples_agent-3_mean: 54.61
    apples_agent-3_min: 25
    apples_agent-4_max: 64
    apples_agent-4_mean: 1.56
    apples_agent-4_min: 0
    apples_agent-5_max: 211
    apples_agent-5_mean: 99.52
    apples_agent-5_min: 42
    cleaning_beam_agent-0_max: 531
    cleaning_beam_agent-0_mean: 428.76
    cleaning_beam_agent-0_min: 277
    cleaning_beam_agent-1_max: 421
    cleaning_beam_agent-1_mean: 251.05
    cleaning_beam_agent-1_min: 64
    cleaning_beam_agent-2_max: 538
    cleaning_beam_agent-2_mean: 311.06
    cleaning_beam_agent-2_min: 105
    cleaning_beam_agent-3_max: 64
    cleaning_beam_agent-3_mean: 13.42
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 606
    cleaning_beam_agent-4_mean: 480.98
    cleaning_beam_agent-4_min: 322
    cleaning_beam_agent-5_max: 240
    cleaning_beam_agent-5_mean: 35.22
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-39-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1154.99999999999
  episode_reward_mean: 959.3099999999873
  episode_reward_min: 433.0000000000132
  episodes_this_iter: 96
  episodes_total: 27648
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20183.766
    learner:
      agent-0:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9130080938339233
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019582086242735386
        model: {}
        policy_loss: -0.0031701908446848392
        total_loss: -0.0023598733823746443
        vf_explained_var: 0.013873696327209473
        vf_loss: 24.172096252441406
      agent-1:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 1.143731713294983
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012577574234455824
        model: {}
        policy_loss: -0.0038268226198852062
        total_loss: -0.003375015454366803
        vf_explained_var: -0.0073094964027404785
        vf_loss: 24.647741317749023
      agent-2:
        cur_kl_coeff: 9.693523188541782e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1513351202011108
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013186074793338776
        model: {}
        policy_loss: -0.0033048419281840324
        total_loss: -0.0030056871473789215
        vf_explained_var: 0.04943521320819855
        vf_loss: 23.25504493713379
      agent-3:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3852517306804657
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009169682743959129
        model: {}
        policy_loss: -0.002312085824087262
        total_loss: -0.0009237788617610931
        vf_explained_var: 0.15418438613414764
        vf_loss: 20.66349983215332
      agent-4:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8671859502792358
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015921624144539237
        model: {}
        policy_loss: -0.0035720858722925186
        total_loss: -0.002820835914462805
        vf_explained_var: 0.06883515417575836
        vf_loss: 22.774986267089844
      agent-5:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6619532108306885
        entropy_coeff: 0.0017600000137463212
        kl: 0.001164037617854774
        model: {}
        policy_loss: -0.003355087712407112
        total_loss: -0.002441462129354477
        vf_explained_var: 0.14780284464359283
        vf_loss: 20.786630630493164
    load_time_ms: 14917.997
    num_steps_sampled: 27648000
    num_steps_trained: 27648000
    sample_time_ms: 90015.248
    update_time_ms: 22.357
  iterations_since_restore: 268
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.953107344632768
    ram_util_percent: 9.578531073446328
  pid: 4061
  policy_reward_max:
    agent-0: 192.49999999999943
    agent-1: 192.49999999999943
    agent-2: 192.49999999999943
    agent-3: 192.49999999999943
    agent-4: 192.49999999999943
    agent-5: 192.49999999999943
  policy_reward_mean:
    agent-0: 159.88499999999988
    agent-1: 159.88499999999988
    agent-2: 159.88499999999988
    agent-3: 159.88499999999988
    agent-4: 159.88499999999988
    agent-5: 159.88499999999988
  policy_reward_min:
    agent-0: 72.16666666666656
    agent-1: 72.16666666666656
    agent-2: 72.16666666666656
    agent-3: 72.16666666666656
    agent-4: 72.16666666666656
    agent-5: 72.16666666666656
  sampler_perf:
    mean_env_wait_ms: 23.895392493365918
    mean_inference_ms: 12.256830190360807
    mean_processing_ms: 50.62015464636658
  time_since_restore: 34416.163774728775
  time_this_iter_s: 123.86731886863708
  time_total_s: 37627.227460861206
  timestamp: 1637051985
  timesteps_since_restore: 25728000
  timesteps_this_iter: 96000
  timesteps_total: 27648000
  training_iteration: 288
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    288 |          37627.2 | 27648000 |   959.31 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 33
    apples_agent-0_mean: 1.62
    apples_agent-0_min: 0
    apples_agent-1_max: 89
    apples_agent-1_mean: 20.43
    apples_agent-1_min: 0
    apples_agent-2_max: 193
    apples_agent-2_mean: 14.89
    apples_agent-2_min: 0
    apples_agent-3_max: 239
    apples_agent-3_mean: 58.39
    apples_agent-3_min: 29
    apples_agent-4_max: 56
    apples_agent-4_mean: 1.36
    apples_agent-4_min: 0
    apples_agent-5_max: 249
    apples_agent-5_mean: 99.82
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 493
    cleaning_beam_agent-0_mean: 419.03
    cleaning_beam_agent-0_min: 286
    cleaning_beam_agent-1_max: 403
    cleaning_beam_agent-1_mean: 239.79
    cleaning_beam_agent-1_min: 124
    cleaning_beam_agent-2_max: 556
    cleaning_beam_agent-2_mean: 322.37
    cleaning_beam_agent-2_min: 113
    cleaning_beam_agent-3_max: 74
    cleaning_beam_agent-3_mean: 13.81
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 572
    cleaning_beam_agent-4_mean: 479.67
    cleaning_beam_agent-4_min: 335
    cleaning_beam_agent-5_max: 889
    cleaning_beam_agent-5_mean: 76.88
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-41-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1181.0000000000132
  episode_reward_mean: 955.4799999999844
  episode_reward_min: 444.00000000000813
  episodes_this_iter: 96
  episodes_total: 27744
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20180.852
    learner:
      agent-0:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9055430889129639
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012971870601177216
        model: {}
        policy_loss: -0.003181794425472617
        total_loss: -0.002630789764225483
        vf_explained_var: 0.04737769067287445
        vf_loss: 21.447587966918945
      agent-1:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1583878993988037
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021379319950938225
        model: {}
        policy_loss: -0.003961528651416302
        total_loss: -0.003730433527380228
        vf_explained_var: -0.01007804274559021
        vf_loss: 22.69858169555664
      agent-2:
        cur_kl_coeff: 4.846761594270891e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1351176500320435
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014964722795411944
        model: {}
        policy_loss: -0.0037055895663797855
        total_loss: -0.0034802970476448536
        vf_explained_var: 0.007972002029418945
        vf_loss: 22.23102569580078
      agent-3:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3695112466812134
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008186391787603498
        model: {}
        policy_loss: -0.0022934451699256897
        total_loss: -0.0010579968802630901
        vf_explained_var: 0.15927724540233612
        vf_loss: 18.857906341552734
      agent-4:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8626630306243896
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018503989558666945
        model: {}
        policy_loss: -0.003655362641438842
        total_loss: -0.003048168495297432
        vf_explained_var: 0.05270083248615265
        vf_loss: 21.254823684692383
      agent-5:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6407912969589233
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008140363497659564
        model: {}
        policy_loss: -0.0028315363451838493
        total_loss: -0.0019364242907613516
        vf_explained_var: 0.09723372757434845
        vf_loss: 20.229068756103516
    load_time_ms: 14835.041
    num_steps_sampled: 27744000
    num_steps_trained: 27744000
    sample_time_ms: 90021.684
    update_time_ms: 22.204
  iterations_since_restore: 269
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.334831460674154
    ram_util_percent: 9.66741573033708
  pid: 4061
  policy_reward_max:
    agent-0: 196.83333333333272
    agent-1: 196.83333333333272
    agent-2: 196.83333333333272
    agent-3: 196.83333333333272
    agent-4: 196.83333333333272
    agent-5: 196.83333333333272
  policy_reward_mean:
    agent-0: 159.24666666666656
    agent-1: 159.24666666666656
    agent-2: 159.24666666666656
    agent-3: 159.24666666666656
    agent-4: 159.24666666666656
    agent-5: 159.24666666666656
  policy_reward_min:
    agent-0: 74.00000000000001
    agent-1: 74.00000000000001
    agent-2: 74.00000000000001
    agent-3: 74.00000000000001
    agent-4: 74.00000000000001
    agent-5: 74.00000000000001
  sampler_perf:
    mean_env_wait_ms: 23.89856562679675
    mean_inference_ms: 12.256449660610746
    mean_processing_ms: 50.61855084697312
  time_since_restore: 34541.58608222008
  time_this_iter_s: 125.42230749130249
  time_total_s: 37752.64976835251
  timestamp: 1637052111
  timesteps_since_restore: 25824000
  timesteps_this_iter: 96000
  timesteps_total: 27744000
  training_iteration: 289
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    289 |          37752.6 | 27744000 |   955.48 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 29
    apples_agent-0_mean: 2.38
    apples_agent-0_min: 0
    apples_agent-1_max: 78
    apples_agent-1_mean: 21.66
    apples_agent-1_min: 0
    apples_agent-2_max: 105
    apples_agent-2_mean: 12.19
    apples_agent-2_min: 0
    apples_agent-3_max: 136
    apples_agent-3_mean: 59.81
    apples_agent-3_min: 29
    apples_agent-4_max: 46
    apples_agent-4_mean: 1.09
    apples_agent-4_min: 0
    apples_agent-5_max: 160
    apples_agent-5_mean: 103.83
    apples_agent-5_min: 21
    cleaning_beam_agent-0_max: 541
    cleaning_beam_agent-0_mean: 426.44
    cleaning_beam_agent-0_min: 286
    cleaning_beam_agent-1_max: 382
    cleaning_beam_agent-1_mean: 242.64
    cleaning_beam_agent-1_min: 98
    cleaning_beam_agent-2_max: 546
    cleaning_beam_agent-2_mean: 322.07
    cleaning_beam_agent-2_min: 168
    cleaning_beam_agent-3_max: 62
    cleaning_beam_agent-3_mean: 15.17
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 579
    cleaning_beam_agent-4_mean: 487.54
    cleaning_beam_agent-4_min: 343
    cleaning_beam_agent-5_max: 483
    cleaning_beam_agent-5_mean: 48.1
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-43-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1157.0000000000098
  episode_reward_mean: 972.7899999999854
  episode_reward_min: 329.0000000000032
  episodes_this_iter: 96
  episodes_total: 27840
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20186.149
    learner:
      agent-0:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9132137298583984
        entropy_coeff: 0.0017600000137463212
        kl: 0.000963184516876936
        model: {}
        policy_loss: -0.002979271113872528
        total_loss: -0.0023153740912675858
        vf_explained_var: 0.010506883263587952
        vf_loss: 22.711498260498047
      agent-1:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1524734497070312
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016272468492388725
        model: {}
        policy_loss: -0.003971228841692209
        total_loss: -0.0036838329397141933
        vf_explained_var: -0.010730475187301636
        vf_loss: 23.157489776611328
      agent-2:
        cur_kl_coeff: 2.4233807971354455e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1391445398330688
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014517842791974545
        model: {}
        policy_loss: -0.0034652643371373415
        total_loss: -0.003188679227605462
        vf_explained_var: -0.0025573670864105225
        vf_loss: 22.81477165222168
      agent-3:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37153875827789307
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009223815286532044
        model: {}
        policy_loss: -0.002193251857534051
        total_loss: -0.0009046564809978008
        vf_explained_var: 0.14807888865470886
        vf_loss: 19.42500877380371
      agent-4:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8673675060272217
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016052366700023413
        model: {}
        policy_loss: -0.003699076594784856
        total_loss: -0.0030735281761735678
        vf_explained_var: 0.05811972916126251
        vf_loss: 21.52115821838379
      agent-5:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6456859111785889
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008294160943478346
        model: {}
        policy_loss: -0.003087348770350218
        total_loss: -0.002191681880503893
        vf_explained_var: 0.10931234061717987
        vf_loss: 20.320716857910156
    load_time_ms: 15008.294
    num_steps_sampled: 27840000
    num_steps_trained: 27840000
    sample_time_ms: 90120.963
    update_time_ms: 22.314
  iterations_since_restore: 270
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.128729281767956
    ram_util_percent: 9.686187845303868
  pid: 4061
  policy_reward_max:
    agent-0: 192.8333333333331
    agent-1: 192.8333333333331
    agent-2: 192.8333333333331
    agent-3: 192.8333333333331
    agent-4: 192.8333333333331
    agent-5: 192.8333333333331
  policy_reward_mean:
    agent-0: 162.13166666666655
    agent-1: 162.13166666666655
    agent-2: 162.13166666666655
    agent-3: 162.13166666666655
    agent-4: 162.13166666666655
    agent-5: 162.13166666666655
  policy_reward_min:
    agent-0: 54.83333333333314
    agent-1: 54.83333333333314
    agent-2: 54.83333333333314
    agent-3: 54.83333333333314
    agent-4: 54.83333333333314
    agent-5: 54.83333333333314
  sampler_perf:
    mean_env_wait_ms: 23.901308103852458
    mean_inference_ms: 12.256083210404933
    mean_processing_ms: 50.61644676366227
  time_since_restore: 34668.15675544739
  time_this_iter_s: 126.57067322731018
  time_total_s: 37879.22044157982
  timestamp: 1637052238
  timesteps_since_restore: 25920000
  timesteps_this_iter: 96000
  timesteps_total: 27840000
  training_iteration: 290
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    290 |          37879.2 | 27840000 |   972.79 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 29
    apples_agent-0_mean: 2.14
    apples_agent-0_min: 0
    apples_agent-1_max: 67
    apples_agent-1_mean: 20.41
    apples_agent-1_min: 0
    apples_agent-2_max: 86
    apples_agent-2_mean: 9.75
    apples_agent-2_min: 0
    apples_agent-3_max: 360
    apples_agent-3_mean: 58.81
    apples_agent-3_min: 26
    apples_agent-4_max: 50
    apples_agent-4_mean: 0.75
    apples_agent-4_min: 0
    apples_agent-5_max: 396
    apples_agent-5_mean: 103.92
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 512
    cleaning_beam_agent-0_mean: 431.71
    cleaning_beam_agent-0_min: 229
    cleaning_beam_agent-1_max: 364
    cleaning_beam_agent-1_mean: 236.07
    cleaning_beam_agent-1_min: 116
    cleaning_beam_agent-2_max: 513
    cleaning_beam_agent-2_mean: 326.29
    cleaning_beam_agent-2_min: 162
    cleaning_beam_agent-3_max: 61
    cleaning_beam_agent-3_mean: 13.45
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 624
    cleaning_beam_agent-4_mean: 482.43
    cleaning_beam_agent-4_min: 370
    cleaning_beam_agent-5_max: 692
    cleaning_beam_agent-5_mean: 59.7
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-46-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1141.9999999999934
  episode_reward_mean: 965.6499999999837
  episode_reward_min: 450.0000000000135
  episodes_this_iter: 96
  episodes_total: 27936
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20201.426
    learner:
      agent-0:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9064237475395203
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020586922764778137
        model: {}
        policy_loss: -0.0032512573525309563
        total_loss: -0.002732081338763237
        vf_explained_var: 0.009534642100334167
        vf_loss: 21.14482879638672
      agent-1:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1577625274658203
        entropy_coeff: 0.0017600000137463212
        kl: 0.001945686643011868
        model: {}
        policy_loss: -0.004027745220810175
        total_loss: -0.003828400280326605
        vf_explained_var: -0.04763171076774597
        vf_loss: 22.370052337646484
      agent-2:
        cur_kl_coeff: 1.2116903985677227e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 1.131590485572815
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012877502012997866
        model: {}
        policy_loss: -0.0033864639699459076
        total_loss: -0.00327687943354249
        vf_explained_var: 0.012662962079048157
        vf_loss: 21.0118408203125
      agent-3:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35047855973243713
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005326771643012762
        model: {}
        policy_loss: -0.0019337644334882498
        total_loss: -0.0006452968809753656
        vf_explained_var: 0.10847508907318115
        vf_loss: 19.053112030029297
      agent-4:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8581160306930542
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015410056803375483
        model: {}
        policy_loss: -0.0037922500632703304
        total_loss: -0.0032058311626315117
        vf_explained_var: 0.017058774828910828
        vf_loss: 20.967050552368164
      agent-5:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.653934121131897
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012162227649241686
        model: {}
        policy_loss: -0.00302042905241251
        total_loss: -0.0022156843915581703
        vf_explained_var: 0.08143603801727295
        vf_loss: 19.55669593811035
    load_time_ms: 14944.97
    num_steps_sampled: 27936000
    num_steps_trained: 27936000
    sample_time_ms: 90159.39
    update_time_ms: 22.376
  iterations_since_restore: 271
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.167777777777777
    ram_util_percent: 9.601666666666668
  pid: 4061
  policy_reward_max:
    agent-0: 190.33333333333314
    agent-1: 190.33333333333314
    agent-2: 190.33333333333314
    agent-3: 190.33333333333314
    agent-4: 190.33333333333314
    agent-5: 190.33333333333314
  policy_reward_mean:
    agent-0: 160.94166666666658
    agent-1: 160.94166666666658
    agent-2: 160.94166666666658
    agent-3: 160.94166666666658
    agent-4: 160.94166666666658
    agent-5: 160.94166666666658
  policy_reward_min:
    agent-0: 75.0
    agent-1: 75.0
    agent-2: 75.0
    agent-3: 75.0
    agent-4: 75.0
    agent-5: 75.0
  sampler_perf:
    mean_env_wait_ms: 23.904531346602184
    mean_inference_ms: 12.255908984768096
    mean_processing_ms: 50.6145827948749
  time_since_restore: 34794.58111715317
  time_this_iter_s: 126.42436170578003
  time_total_s: 38005.6448032856
  timestamp: 1637052364
  timesteps_since_restore: 26016000
  timesteps_this_iter: 96000
  timesteps_total: 27936000
  training_iteration: 291
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    291 |          38005.6 | 27936000 |   965.65 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 25
    apples_agent-0_mean: 1.84
    apples_agent-0_min: 0
    apples_agent-1_max: 88
    apples_agent-1_mean: 25.05
    apples_agent-1_min: 0
    apples_agent-2_max: 98
    apples_agent-2_mean: 9.64
    apples_agent-2_min: 0
    apples_agent-3_max: 159
    apples_agent-3_mean: 58.4
    apples_agent-3_min: 28
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 243
    apples_agent-5_mean: 103.49
    apples_agent-5_min: 32
    cleaning_beam_agent-0_max: 552
    cleaning_beam_agent-0_mean: 419.88
    cleaning_beam_agent-0_min: 225
    cleaning_beam_agent-1_max: 390
    cleaning_beam_agent-1_mean: 226.92
    cleaning_beam_agent-1_min: 73
    cleaning_beam_agent-2_max: 546
    cleaning_beam_agent-2_mean: 336.57
    cleaning_beam_agent-2_min: 116
    cleaning_beam_agent-3_max: 58
    cleaning_beam_agent-3_mean: 15.6
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 583
    cleaning_beam_agent-4_mean: 484.32
    cleaning_beam_agent-4_min: 372
    cleaning_beam_agent-5_max: 590
    cleaning_beam_agent-5_mean: 49.2
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-48-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1145.000000000005
  episode_reward_mean: 973.4799999999856
  episode_reward_min: 400.00000000000404
  episodes_this_iter: 96
  episodes_total: 28032
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20197.853
    learner:
      agent-0:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.916287899017334
        entropy_coeff: 0.0017600000137463212
        kl: 0.001161511754617095
        model: {}
        policy_loss: -0.0031616142950952053
        total_loss: -0.0027540819719433784
        vf_explained_var: 0.034999921917915344
        vf_loss: 20.20195770263672
      agent-1:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1611049175262451
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016953074373304844
        model: {}
        policy_loss: -0.0038823869545012712
        total_loss: -0.0038517166394740343
        vf_explained_var: 0.0051206499338150024
        vf_loss: 20.742130279541016
      agent-2:
        cur_kl_coeff: 6.058451992838614e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 1.127206802368164
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019158563809469342
        model: {}
        policy_loss: -0.0035356306470930576
        total_loss: -0.003474300727248192
        vf_explained_var: 0.012647077441215515
        vf_loss: 20.452125549316406
      agent-3:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35607481002807617
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006615995080210268
        model: {}
        policy_loss: -0.002038507955148816
        total_loss: -0.0007891978602856398
        vf_explained_var: 0.09795655310153961
        vf_loss: 18.760013580322266
      agent-4:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.864824116230011
        entropy_coeff: 0.0017600000137463212
        kl: 0.001833567745052278
        model: {}
        policy_loss: -0.003761792089790106
        total_loss: -0.0033058756962418556
        vf_explained_var: 0.04836948215961456
        vf_loss: 19.780067443847656
      agent-5:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6588383913040161
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009817839600145817
        model: {}
        policy_loss: -0.002939875703305006
        total_loss: -0.0021788824815303087
        vf_explained_var: 0.07464507222175598
        vf_loss: 19.20547103881836
    load_time_ms: 14938.713
    num_steps_sampled: 28032000
    num_steps_trained: 28032000
    sample_time_ms: 90136.675
    update_time_ms: 22.668
  iterations_since_restore: 272
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.314204545454546
    ram_util_percent: 9.651704545454544
  pid: 4061
  policy_reward_max:
    agent-0: 190.83333333333286
    agent-1: 190.83333333333286
    agent-2: 190.83333333333286
    agent-3: 190.83333333333286
    agent-4: 190.83333333333286
    agent-5: 190.83333333333286
  policy_reward_mean:
    agent-0: 162.24666666666658
    agent-1: 162.24666666666658
    agent-2: 162.24666666666658
    agent-3: 162.24666666666658
    agent-4: 162.24666666666658
    agent-5: 162.24666666666658
  policy_reward_min:
    agent-0: 66.66666666666652
    agent-1: 66.66666666666652
    agent-2: 66.66666666666652
    agent-3: 66.66666666666652
    agent-4: 66.66666666666652
    agent-5: 66.66666666666652
  sampler_perf:
    mean_env_wait_ms: 23.9071881818707
    mean_inference_ms: 12.255550159727347
    mean_processing_ms: 50.61232557936927
  time_since_restore: 34917.81979703903
  time_this_iter_s: 123.23867988586426
  time_total_s: 38128.88348317146
  timestamp: 1637052488
  timesteps_since_restore: 26112000
  timesteps_this_iter: 96000
  timesteps_total: 28032000
  training_iteration: 292
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    292 |          38128.9 | 28032000 |   973.48 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 36
    apples_agent-0_mean: 1.37
    apples_agent-0_min: 0
    apples_agent-1_max: 86
    apples_agent-1_mean: 23.03
    apples_agent-1_min: 0
    apples_agent-2_max: 117
    apples_agent-2_mean: 12.38
    apples_agent-2_min: 0
    apples_agent-3_max: 110
    apples_agent-3_mean: 57.68
    apples_agent-3_min: 28
    apples_agent-4_max: 58
    apples_agent-4_mean: 0.87
    apples_agent-4_min: 0
    apples_agent-5_max: 167
    apples_agent-5_mean: 106.21
    apples_agent-5_min: 35
    cleaning_beam_agent-0_max: 500
    cleaning_beam_agent-0_mean: 416.5
    cleaning_beam_agent-0_min: 225
    cleaning_beam_agent-1_max: 329
    cleaning_beam_agent-1_mean: 216.81
    cleaning_beam_agent-1_min: 120
    cleaning_beam_agent-2_max: 587
    cleaning_beam_agent-2_mean: 342.74
    cleaning_beam_agent-2_min: 127
    cleaning_beam_agent-3_max: 59
    cleaning_beam_agent-3_mean: 13.23
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 597
    cleaning_beam_agent-4_mean: 472.61
    cleaning_beam_agent-4_min: 352
    cleaning_beam_agent-5_max: 221
    cleaning_beam_agent-5_mean: 33.87
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-50-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1142.0000000000089
  episode_reward_mean: 981.2099999999856
  episode_reward_min: 400.00000000000404
  episodes_this_iter: 96
  episodes_total: 28128
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20212.51
    learner:
      agent-0:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9093551635742188
        entropy_coeff: 0.0017600000137463212
        kl: 0.001234607887454331
        model: {}
        policy_loss: -0.002982927020639181
        total_loss: -0.0025018868036568165
        vf_explained_var: 0.028661251068115234
        vf_loss: 20.81505584716797
      agent-1:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1565675735473633
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011046453146263957
        model: {}
        policy_loss: -0.003668143879622221
        total_loss: -0.003592128399759531
        vf_explained_var: 0.01043035089969635
        vf_loss: 21.115737915039062
      agent-2:
        cur_kl_coeff: 3.029225996419307e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 1.134852409362793
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011659752344712615
        model: {}
        policy_loss: -0.0032932788599282503
        total_loss: -0.0032302995678037405
        vf_explained_var: 0.028255939483642578
        vf_loss: 20.60321807861328
      agent-3:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3613043427467346
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009886869229376316
        model: {}
        policy_loss: -0.0021496457047760487
        total_loss: -0.000899104867130518
        vf_explained_var: 0.11019189655780792
        vf_loss: 18.864328384399414
      agent-4:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8727936744689941
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015008461195975542
        model: {}
        policy_loss: -0.0038017004262655973
        total_loss: -0.0033187794033437967
        vf_explained_var: 0.0554775595664978
        vf_loss: 20.19033432006836
      agent-5:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6553301811218262
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006850367062725127
        model: {}
        policy_loss: -0.002782144583761692
        total_loss: -0.002001834101974964
        vf_explained_var: 0.08335694670677185
        vf_loss: 19.33692741394043
    load_time_ms: 14790.662
    num_steps_sampled: 28128000
    num_steps_trained: 28128000
    sample_time_ms: 90145.342
    update_time_ms: 22.643
  iterations_since_restore: 273
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.816292134831462
    ram_util_percent: 9.635393258426966
  pid: 4061
  policy_reward_max:
    agent-0: 190.33333333333312
    agent-1: 190.33333333333312
    agent-2: 190.33333333333312
    agent-3: 190.33333333333312
    agent-4: 190.33333333333312
    agent-5: 190.33333333333312
  policy_reward_mean:
    agent-0: 163.53499999999985
    agent-1: 163.53499999999985
    agent-2: 163.53499999999985
    agent-3: 163.53499999999985
    agent-4: 163.53499999999985
    agent-5: 163.53499999999985
  policy_reward_min:
    agent-0: 66.66666666666652
    agent-1: 66.66666666666652
    agent-2: 66.66666666666652
    agent-3: 66.66666666666652
    agent-4: 66.66666666666652
    agent-5: 66.66666666666652
  sampler_perf:
    mean_env_wait_ms: 23.90922763967114
    mean_inference_ms: 12.255224888335853
    mean_processing_ms: 50.61040129726609
  time_since_restore: 35042.16543984413
  time_this_iter_s: 124.34564280509949
  time_total_s: 38253.22912597656
  timestamp: 1637052612
  timesteps_since_restore: 26208000
  timesteps_this_iter: 96000
  timesteps_total: 28128000
  training_iteration: 293
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    293 |          38253.2 | 28128000 |   981.21 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 37
    apples_agent-0_mean: 2.44
    apples_agent-0_min: 0
    apples_agent-1_max: 83
    apples_agent-1_mean: 24.85
    apples_agent-1_min: 0
    apples_agent-2_max: 117
    apples_agent-2_mean: 17.29
    apples_agent-2_min: 0
    apples_agent-3_max: 93
    apples_agent-3_mean: 55.59
    apples_agent-3_min: 29
    apples_agent-4_max: 82
    apples_agent-4_mean: 2.52
    apples_agent-4_min: 0
    apples_agent-5_max: 203
    apples_agent-5_mean: 103.58
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 519
    cleaning_beam_agent-0_mean: 422.49
    cleaning_beam_agent-0_min: 257
    cleaning_beam_agent-1_max: 365
    cleaning_beam_agent-1_mean: 212.74
    cleaning_beam_agent-1_min: 125
    cleaning_beam_agent-2_max: 575
    cleaning_beam_agent-2_mean: 333.35
    cleaning_beam_agent-2_min: 139
    cleaning_beam_agent-3_max: 58
    cleaning_beam_agent-3_mean: 13.91
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 552
    cleaning_beam_agent-4_mean: 471.15
    cleaning_beam_agent-4_min: 313
    cleaning_beam_agent-5_max: 629
    cleaning_beam_agent-5_mean: 60.31
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-52-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1117.9999999999984
  episode_reward_mean: 948.7899999999846
  episode_reward_min: 354.00000000000483
  episodes_this_iter: 96
  episodes_total: 28224
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20206.092
    learner:
      agent-0:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9128527641296387
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018029666971415281
        model: {}
        policy_loss: -0.003401016816496849
        total_loss: -0.002758380491286516
        vf_explained_var: 0.012134954333305359
        vf_loss: 22.492572784423828
      agent-1:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1662311553955078
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016408233204856515
        model: {}
        policy_loss: -0.004112923517823219
        total_loss: -0.003841751953586936
        vf_explained_var: -0.02059262990951538
        vf_loss: 23.237350463867188
      agent-2:
        cur_kl_coeff: 1.5146129982096534e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1350088119506836
        entropy_coeff: 0.0017600000137463212
        kl: 0.001865447498857975
        model: {}
        policy_loss: -0.003596492111682892
        total_loss: -0.003375040367245674
        vf_explained_var: 0.025300979614257812
        vf_loss: 22.1906795501709
      agent-3:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3824523687362671
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008831185987219214
        model: {}
        policy_loss: -0.0022709695622324944
        total_loss: -0.0010502440854907036
        vf_explained_var: 0.1700974404811859
        vf_loss: 18.938444137573242
      agent-4:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8872198462486267
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015616423916071653
        model: {}
        policy_loss: -0.004076811484992504
        total_loss: -0.0035268717911094427
        vf_explained_var: 0.07406249642372131
        vf_loss: 21.11444854736328
      agent-5:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6929808259010315
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012351087061688304
        model: {}
        policy_loss: -0.003476305864751339
        total_loss: -0.0026962682604789734
        vf_explained_var: 0.12237964570522308
        vf_loss: 19.996837615966797
    load_time_ms: 15159.231
    num_steps_sampled: 28224000
    num_steps_trained: 28224000
    sample_time_ms: 90033.211
    update_time_ms: 22.264
  iterations_since_restore: 274
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 14.886021505376343
    ram_util_percent: 9.708064516129033
  pid: 4061
  policy_reward_max:
    agent-0: 186.33333333333312
    agent-1: 186.33333333333312
    agent-2: 186.33333333333312
    agent-3: 186.33333333333312
    agent-4: 186.33333333333312
    agent-5: 186.33333333333312
  policy_reward_mean:
    agent-0: 158.13166666666652
    agent-1: 158.13166666666652
    agent-2: 158.13166666666652
    agent-3: 158.13166666666652
    agent-4: 158.13166666666652
    agent-5: 158.13166666666652
  policy_reward_min:
    agent-0: 58.99999999999976
    agent-1: 58.99999999999976
    agent-2: 58.99999999999976
    agent-3: 58.99999999999976
    agent-4: 58.99999999999976
    agent-5: 58.99999999999976
  sampler_perf:
    mean_env_wait_ms: 23.91188045497931
    mean_inference_ms: 12.254957915213486
    mean_processing_ms: 50.609756733054425
  time_since_restore: 35172.58879852295
  time_this_iter_s: 130.42335867881775
  time_total_s: 38383.65248465538
  timestamp: 1637052743
  timesteps_since_restore: 26304000
  timesteps_this_iter: 96000
  timesteps_total: 28224000
  training_iteration: 294
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    294 |          38383.7 | 28224000 |   948.79 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 0.87
    apples_agent-0_min: 0
    apples_agent-1_max: 88
    apples_agent-1_mean: 22.2
    apples_agent-1_min: 0
    apples_agent-2_max: 103
    apples_agent-2_mean: 14.32
    apples_agent-2_min: 0
    apples_agent-3_max: 86
    apples_agent-3_mean: 56.12
    apples_agent-3_min: 29
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 222
    apples_agent-5_mean: 107.12
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 520
    cleaning_beam_agent-0_mean: 419.26
    cleaning_beam_agent-0_min: 273
    cleaning_beam_agent-1_max: 362
    cleaning_beam_agent-1_mean: 215.1
    cleaning_beam_agent-1_min: 73
    cleaning_beam_agent-2_max: 559
    cleaning_beam_agent-2_mean: 342.84
    cleaning_beam_agent-2_min: 161
    cleaning_beam_agent-3_max: 78
    cleaning_beam_agent-3_mean: 11.48
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 593
    cleaning_beam_agent-4_mean: 480.6
    cleaning_beam_agent-4_min: 353
    cleaning_beam_agent-5_max: 787
    cleaning_beam_agent-5_mean: 60.52
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-54-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1175.9999999999927
  episode_reward_mean: 987.5799999999856
  episode_reward_min: 480.0000000000131
  episodes_this_iter: 96
  episodes_total: 28320
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20201.625
    learner:
      agent-0:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8990172743797302
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023808423429727554
        model: {}
        policy_loss: -0.0031269285827875137
        total_loss: -0.0026460736989974976
        vf_explained_var: 0.013933002948760986
        vf_loss: 20.631263732910156
      agent-1:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 1.161610722541809
        entropy_coeff: 0.0017600000137463212
        kl: 0.001849306165240705
        model: {}
        policy_loss: -0.0037229456938803196
        total_loss: -0.0036300618667155504
        vf_explained_var: -0.013388678431510925
        vf_loss: 21.373180389404297
      agent-2:
        cur_kl_coeff: 7.573064991048267e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 1.126876711845398
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015541079919785261
        model: {}
        policy_loss: -0.0034195741172879934
        total_loss: -0.003387538716197014
        vf_explained_var: 0.026044711470603943
        vf_loss: 20.153411865234375
      agent-3:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.34200742840766907
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005184012115933001
        model: {}
        policy_loss: -0.0018422073917463422
        total_loss: -0.0005815047770738602
        vf_explained_var: 0.10738597810268402
        vf_loss: 18.626392364501953
      agent-4:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.872757077217102
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018189233960583806
        model: {}
        policy_loss: -0.003774758893996477
        total_loss: -0.003359015565365553
        vf_explained_var: 0.06233467161655426
        vf_loss: 19.517955780029297
      agent-5:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6631339192390442
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013453653082251549
        model: {}
        policy_loss: -0.003338475478813052
        total_loss: -0.0025784852914512157
        vf_explained_var: 0.07837177813053131
        vf_loss: 19.271076202392578
    load_time_ms: 15417.674
    num_steps_sampled: 28320000
    num_steps_trained: 28320000
    sample_time_ms: 90073.171
    update_time_ms: 21.349
  iterations_since_restore: 275
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.15111111111111
    ram_util_percent: 9.678333333333335
  pid: 4061
  policy_reward_max:
    agent-0: 195.99999999999974
    agent-1: 195.99999999999974
    agent-2: 195.99999999999974
    agent-3: 195.99999999999974
    agent-4: 195.99999999999974
    agent-5: 195.99999999999974
  policy_reward_mean:
    agent-0: 164.59666666666652
    agent-1: 164.59666666666652
    agent-2: 164.59666666666652
    agent-3: 164.59666666666652
    agent-4: 164.59666666666652
    agent-5: 164.59666666666652
  policy_reward_min:
    agent-0: 80.00000000000011
    agent-1: 80.00000000000011
    agent-2: 80.00000000000011
    agent-3: 80.00000000000011
    agent-4: 80.00000000000011
    agent-5: 80.00000000000011
  sampler_perf:
    mean_env_wait_ms: 23.91425275882711
    mean_inference_ms: 12.254522521053703
    mean_processing_ms: 50.60743011487309
  time_since_restore: 35298.748540878296
  time_this_iter_s: 126.15974235534668
  time_total_s: 38509.81222701073
  timestamp: 1637052869
  timesteps_since_restore: 26400000
  timesteps_this_iter: 96000
  timesteps_total: 28320000
  training_iteration: 295
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    295 |          38509.8 | 28320000 |   987.58 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 0.49
    apples_agent-0_min: 0
    apples_agent-1_max: 91
    apples_agent-1_mean: 24.91
    apples_agent-1_min: 0
    apples_agent-2_max: 76
    apples_agent-2_mean: 11.55
    apples_agent-2_min: 0
    apples_agent-3_max: 111
    apples_agent-3_mean: 56.0
    apples_agent-3_min: 29
    apples_agent-4_max: 69
    apples_agent-4_mean: 2.15
    apples_agent-4_min: 0
    apples_agent-5_max: 182
    apples_agent-5_mean: 105.56
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 524
    cleaning_beam_agent-0_mean: 437.14
    cleaning_beam_agent-0_min: 344
    cleaning_beam_agent-1_max: 321
    cleaning_beam_agent-1_mean: 210.48
    cleaning_beam_agent-1_min: 94
    cleaning_beam_agent-2_max: 552
    cleaning_beam_agent-2_mean: 340.47
    cleaning_beam_agent-2_min: 166
    cleaning_beam_agent-3_max: 44
    cleaning_beam_agent-3_mean: 12.49
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 560
    cleaning_beam_agent-4_mean: 458.29
    cleaning_beam_agent-4_min: 238
    cleaning_beam_agent-5_max: 752
    cleaning_beam_agent-5_mean: 48.73
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-56-35
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1132.0000000000102
  episode_reward_mean: 990.5699999999847
  episode_reward_min: 294.00000000000034
  episodes_this_iter: 96
  episodes_total: 28416
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20188.646
    learner:
      agent-0:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8868936896324158
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012644710950553417
        model: {}
        policy_loss: -0.003015968482941389
        total_loss: -0.0023957942612469196
        vf_explained_var: 0.029341459274291992
        vf_loss: 21.81107521057129
      agent-1:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1664042472839355
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011300903279334307
        model: {}
        policy_loss: -0.0036536483094096184
        total_loss: -0.0033933925442397594
        vf_explained_var: -0.022281408309936523
        vf_loss: 23.131250381469727
      agent-2:
        cur_kl_coeff: 3.7865324955241336e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1420073509216309
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012557198060676455
        model: {}
        policy_loss: -0.003477400867268443
        total_loss: -0.00336634274572134
        vf_explained_var: 0.045450359582901
        vf_loss: 21.209909439086914
      agent-3:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3401024639606476
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009420530986972153
        model: {}
        policy_loss: -0.002138306386768818
        total_loss: -0.0008349447743967175
        vf_explained_var: 0.141801118850708
        vf_loss: 19.019428253173828
      agent-4:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8812673091888428
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015269755385816097
        model: {}
        policy_loss: -0.003979267552495003
        total_loss: -0.003467692993581295
        vf_explained_var: 0.08511331677436829
        vf_loss: 20.62606430053711
      agent-5:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6687188148498535
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011042458936572075
        model: {}
        policy_loss: -0.003430435201153159
        total_loss: -0.00262311939150095
        vf_explained_var: 0.11242355406284332
        vf_loss: 19.842592239379883
    load_time_ms: 15369.239
    num_steps_sampled: 28416000
    num_steps_trained: 28416000
    sample_time_ms: 90082.226
    update_time_ms: 21.622
  iterations_since_restore: 276
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.279329608938548
    ram_util_percent: 9.60558659217877
  pid: 4061
  policy_reward_max:
    agent-0: 188.666666666666
    agent-1: 188.666666666666
    agent-2: 188.666666666666
    agent-3: 188.666666666666
    agent-4: 188.666666666666
    agent-5: 188.666666666666
  policy_reward_mean:
    agent-0: 165.09499999999983
    agent-1: 165.09499999999983
    agent-2: 165.09499999999983
    agent-3: 165.09499999999983
    agent-4: 165.09499999999983
    agent-5: 165.09499999999983
  policy_reward_min:
    agent-0: 48.999999999999865
    agent-1: 48.999999999999865
    agent-2: 48.999999999999865
    agent-3: 48.999999999999865
    agent-4: 48.999999999999865
    agent-5: 48.999999999999865
  sampler_perf:
    mean_env_wait_ms: 23.916692598831542
    mean_inference_ms: 12.254401949364576
    mean_processing_ms: 50.606583083549374
  time_since_restore: 35424.80479621887
  time_this_iter_s: 126.05625534057617
  time_total_s: 38635.8684823513
  timestamp: 1637052995
  timesteps_since_restore: 26496000
  timesteps_this_iter: 96000
  timesteps_total: 28416000
  training_iteration: 296
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    296 |          38635.9 | 28416000 |   990.57 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 0.87
    apples_agent-0_min: 0
    apples_agent-1_max: 91
    apples_agent-1_mean: 20.93
    apples_agent-1_min: 0
    apples_agent-2_max: 138
    apples_agent-2_mean: 11.79
    apples_agent-2_min: 0
    apples_agent-3_max: 164
    apples_agent-3_mean: 54.5
    apples_agent-3_min: 24
    apples_agent-4_max: 49
    apples_agent-4_mean: 1.63
    apples_agent-4_min: 0
    apples_agent-5_max: 195
    apples_agent-5_mean: 98.5
    apples_agent-5_min: 24
    cleaning_beam_agent-0_max: 525
    cleaning_beam_agent-0_mean: 416.68
    cleaning_beam_agent-0_min: 262
    cleaning_beam_agent-1_max: 309
    cleaning_beam_agent-1_mean: 210.17
    cleaning_beam_agent-1_min: 106
    cleaning_beam_agent-2_max: 528
    cleaning_beam_agent-2_mean: 331.62
    cleaning_beam_agent-2_min: 180
    cleaning_beam_agent-3_max: 70
    cleaning_beam_agent-3_mean: 13.72
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 574
    cleaning_beam_agent-4_mean: 466.09
    cleaning_beam_agent-4_min: 294
    cleaning_beam_agent-5_max: 493
    cleaning_beam_agent-5_mean: 66.65
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-58-41
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1170.9999999999948
  episode_reward_mean: 955.3699999999874
  episode_reward_min: 384.0000000000079
  episodes_this_iter: 96
  episodes_total: 28512
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20187.156
    learner:
      agent-0:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9049293398857117
        entropy_coeff: 0.0017600000137463212
        kl: 0.001384284463711083
        model: {}
        policy_loss: -0.003400013782083988
        total_loss: -0.0028232738841325045
        vf_explained_var: 0.0331977903842926
        vf_loss: 21.69413948059082
      agent-1:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 1.165490984916687
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013110938016325235
        model: {}
        policy_loss: -0.003648735349997878
        total_loss: -0.003426565555855632
        vf_explained_var: -0.014012336730957031
        vf_loss: 22.734333038330078
      agent-2:
        cur_kl_coeff: 1.8932662477620668e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1333640813827515
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014325403608381748
        model: {}
        policy_loss: -0.0036317736376076937
        total_loss: -0.003466095309704542
        vf_explained_var: 0.03501078486442566
        vf_loss: 21.603960037231445
      agent-3:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3577077090740204
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010761497542262077
        model: {}
        policy_loss: -0.0021836680825799704
        total_loss: -0.0009210021235048771
        vf_explained_var: 0.15479888021945953
        vf_loss: 18.92231559753418
      agent-4:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8868999481201172
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014054961502552032
        model: {}
        policy_loss: -0.00387903256341815
        total_loss: -0.0034663735423237085
        vf_explained_var: 0.1193205863237381
        vf_loss: 19.736003875732422
      agent-5:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6751242876052856
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014349285047501326
        model: {}
        policy_loss: -0.0037342850118875504
        total_loss: -0.002882958622649312
        vf_explained_var: 0.09037958085536957
        vf_loss: 20.395484924316406
    load_time_ms: 15375.692
    num_steps_sampled: 28512000
    num_steps_trained: 28512000
    sample_time_ms: 90150.653
    update_time_ms: 21.387
  iterations_since_restore: 277
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.190555555555557
    ram_util_percent: 9.675
  pid: 4061
  policy_reward_max:
    agent-0: 195.1666666666668
    agent-1: 195.1666666666668
    agent-2: 195.1666666666668
    agent-3: 195.1666666666668
    agent-4: 195.1666666666668
    agent-5: 195.1666666666668
  policy_reward_mean:
    agent-0: 159.2283333333333
    agent-1: 159.2283333333333
    agent-2: 159.2283333333333
    agent-3: 159.2283333333333
    agent-4: 159.2283333333333
    agent-5: 159.2283333333333
  policy_reward_min:
    agent-0: 63.99999999999979
    agent-1: 63.99999999999979
    agent-2: 63.99999999999979
    agent-3: 63.99999999999979
    agent-4: 63.99999999999979
    agent-5: 63.99999999999979
  sampler_perf:
    mean_env_wait_ms: 23.91877374875138
    mean_inference_ms: 12.25408005016566
    mean_processing_ms: 50.605004816756406
  time_since_restore: 35550.609546899796
  time_this_iter_s: 125.80475068092346
  time_total_s: 38761.67323303223
  timestamp: 1637053121
  timesteps_since_restore: 26592000
  timesteps_this_iter: 96000
  timesteps_total: 28512000
  training_iteration: 297
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 17.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    297 |          38761.7 | 28512000 |   955.37 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 0.91
    apples_agent-0_min: 0
    apples_agent-1_max: 87
    apples_agent-1_mean: 25.09
    apples_agent-1_min: 0
    apples_agent-2_max: 229
    apples_agent-2_mean: 15.51
    apples_agent-2_min: 0
    apples_agent-3_max: 117
    apples_agent-3_mean: 54.48
    apples_agent-3_min: 23
    apples_agent-4_max: 84
    apples_agent-4_mean: 2.21
    apples_agent-4_min: 0
    apples_agent-5_max: 195
    apples_agent-5_mean: 101.59
    apples_agent-5_min: 24
    cleaning_beam_agent-0_max: 573
    cleaning_beam_agent-0_mean: 422.17
    cleaning_beam_agent-0_min: 312
    cleaning_beam_agent-1_max: 358
    cleaning_beam_agent-1_mean: 206.51
    cleaning_beam_agent-1_min: 85
    cleaning_beam_agent-2_max: 503
    cleaning_beam_agent-2_mean: 331.47
    cleaning_beam_agent-2_min: 127
    cleaning_beam_agent-3_max: 47
    cleaning_beam_agent-3_mean: 11.58
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 588
    cleaning_beam_agent-4_mean: 462.61
    cleaning_beam_agent-4_min: 281
    cleaning_beam_agent-5_max: 361
    cleaning_beam_agent-5_mean: 54.15
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-00-48
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1153.0000000000018
  episode_reward_mean: 978.5099999999858
  episode_reward_min: 512.0000000000147
  episodes_this_iter: 96
  episodes_total: 28608
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20151.2
    learner:
      agent-0:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9022690653800964
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014177242992445827
        model: {}
        policy_loss: -0.0034285634756088257
        total_loss: -0.0028519490733742714
        vf_explained_var: 0.002423509955406189
        vf_loss: 21.64609146118164
      agent-1:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 1.15762197971344
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014195272233337164
        model: {}
        policy_loss: -0.003591695800423622
        total_loss: -0.0033859126269817352
        vf_explained_var: -0.029049068689346313
        vf_loss: 22.431964874267578
      agent-2:
        cur_kl_coeff: 9.466331238810334e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1402510404586792
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015362859703600407
        model: {}
        policy_loss: -0.0034123293589800596
        total_loss: -0.003304378129541874
        vf_explained_var: 0.016619935631752014
        vf_loss: 21.147930145263672
      agent-3:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3519030511379242
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008383294916711748
        model: {}
        policy_loss: -0.002096140291541815
        total_loss: -0.0007904694066382945
        vf_explained_var: 0.1025211364030838
        vf_loss: 19.250194549560547
      agent-4:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8908302783966064
        entropy_coeff: 0.0017600000137463212
        kl: 0.002435087226331234
        model: {}
        policy_loss: -0.004037484060972929
        total_loss: -0.0035964148119091988
        vf_explained_var: 0.07039657235145569
        vf_loss: 20.08928680419922
      agent-5:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6594815254211426
        entropy_coeff: 0.0017600000137463212
        kl: 0.001104059279896319
        model: {}
        policy_loss: -0.003305282676592469
        total_loss: -0.002584561239928007
        vf_explained_var: 0.12841643393039703
        vf_loss: 18.814098358154297
    load_time_ms: 15693.544
    num_steps_sampled: 28608000
    num_steps_trained: 28608000
    sample_time_ms: 90123.724
    update_time_ms: 20.832
  iterations_since_restore: 278
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 16.84
    ram_util_percent: 9.751666666666665
  pid: 4061
  policy_reward_max:
    agent-0: 192.16666666666586
    agent-1: 192.16666666666586
    agent-2: 192.16666666666586
    agent-3: 192.16666666666586
    agent-4: 192.16666666666586
    agent-5: 192.16666666666586
  policy_reward_mean:
    agent-0: 163.08499999999984
    agent-1: 163.08499999999984
    agent-2: 163.08499999999984
    agent-3: 163.08499999999984
    agent-4: 163.08499999999984
    agent-5: 163.08499999999984
  policy_reward_min:
    agent-0: 85.33333333333351
    agent-1: 85.33333333333351
    agent-2: 85.33333333333351
    agent-3: 85.33333333333351
    agent-4: 85.33333333333351
    agent-5: 85.33333333333351
  sampler_perf:
    mean_env_wait_ms: 23.921185254782532
    mean_inference_ms: 12.253955264918206
    mean_processing_ms: 50.6041103227444
  time_since_restore: 35677.01504445076
  time_this_iter_s: 126.40549755096436
  time_total_s: 38888.07873058319
  timestamp: 1637053248
  timesteps_since_restore: 26688000
  timesteps_this_iter: 96000
  timesteps_total: 28608000
  training_iteration: 298
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    298 |          38888.1 | 28608000 |   978.51 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 29
    apples_agent-0_mean: 1.48
    apples_agent-0_min: 0
    apples_agent-1_max: 81
    apples_agent-1_mean: 21.99
    apples_agent-1_min: 0
    apples_agent-2_max: 172
    apples_agent-2_mean: 13.94
    apples_agent-2_min: 0
    apples_agent-3_max: 141
    apples_agent-3_mean: 57.14
    apples_agent-3_min: 26
    apples_agent-4_max: 39
    apples_agent-4_mean: 0.77
    apples_agent-4_min: 0
    apples_agent-5_max: 205
    apples_agent-5_mean: 101.69
    apples_agent-5_min: 14
    cleaning_beam_agent-0_max: 500
    cleaning_beam_agent-0_mean: 415.45
    cleaning_beam_agent-0_min: 279
    cleaning_beam_agent-1_max: 361
    cleaning_beam_agent-1_mean: 217.05
    cleaning_beam_agent-1_min: 96
    cleaning_beam_agent-2_max: 488
    cleaning_beam_agent-2_mean: 328.93
    cleaning_beam_agent-2_min: 173
    cleaning_beam_agent-3_max: 70
    cleaning_beam_agent-3_mean: 12.42
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 599
    cleaning_beam_agent-4_mean: 461.28
    cleaning_beam_agent-4_min: 260
    cleaning_beam_agent-5_max: 424
    cleaning_beam_agent-5_mean: 65.58
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-02-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1153.0000000000018
  episode_reward_mean: 969.6299999999828
  episode_reward_min: 219.99999999999795
  episodes_this_iter: 96
  episodes_total: 28704
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20151.144
    learner:
      agent-0:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9044736623764038
        entropy_coeff: 0.0017600000137463212
        kl: 0.002173918066546321
        model: {}
        policy_loss: -0.003536797361448407
        total_loss: -0.0029684705659747124
        vf_explained_var: 0.0017452090978622437
        vf_loss: 21.60198974609375
      agent-1:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1526005268096924
        entropy_coeff: 0.0017600000137463212
        kl: 0.001624869997613132
        model: {}
        policy_loss: -0.0037288465537130833
        total_loss: -0.003579915501177311
        vf_explained_var: -0.010715886950492859
        vf_loss: 21.77505874633789
      agent-2:
        cur_kl_coeff: 4.733165619405167e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1435909271240234
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013918307377025485
        model: {}
        policy_loss: -0.0034757060930132866
        total_loss: -0.0034089935943484306
        vf_explained_var: 0.032267361879348755
        vf_loss: 20.794265747070312
      agent-3:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3572254180908203
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006736541981808841
        model: {}
        policy_loss: -0.002166586462408304
        total_loss: -0.0009168251417577267
        vf_explained_var: 0.125057190656662
        vf_loss: 18.784791946411133
      agent-4:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8874905109405518
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016896832967177033
        model: {}
        policy_loss: -0.0040028891526162624
        total_loss: -0.0036164072807878256
        vf_explained_var: 0.09396103024482727
        vf_loss: 19.484661102294922
      agent-5:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6730321049690247
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009596889140084386
        model: {}
        policy_loss: -0.0029816252645105124
        total_loss: -0.002219534246250987
        vf_explained_var: 0.09794259071350098
        vf_loss: 19.466259002685547
    load_time_ms: 15685.284
    num_steps_sampled: 28704000
    num_steps_trained: 28704000
    sample_time_ms: 90110.271
    update_time_ms: 21.533
  iterations_since_restore: 279
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.414606741573035
    ram_util_percent: 9.630337078651685
  pid: 4061
  policy_reward_max:
    agent-0: 192.16666666666586
    agent-1: 192.16666666666586
    agent-2: 192.16666666666586
    agent-3: 192.16666666666586
    agent-4: 192.16666666666586
    agent-5: 192.16666666666586
  policy_reward_mean:
    agent-0: 161.6049999999999
    agent-1: 161.6049999999999
    agent-2: 161.6049999999999
    agent-3: 161.6049999999999
    agent-4: 161.6049999999999
    agent-5: 161.6049999999999
  policy_reward_min:
    agent-0: 36.6666666666667
    agent-1: 36.6666666666667
    agent-2: 36.6666666666667
    agent-3: 36.6666666666667
    agent-4: 36.6666666666667
    agent-5: 36.6666666666667
  sampler_perf:
    mean_env_wait_ms: 23.923268795732437
    mean_inference_ms: 12.253733632108137
    mean_processing_ms: 50.60159427499469
  time_since_restore: 35802.22483539581
  time_this_iter_s: 125.2097909450531
  time_total_s: 39013.288521528244
  timestamp: 1637053373
  timesteps_since_restore: 26784000
  timesteps_this_iter: 96000
  timesteps_total: 28704000
  training_iteration: 299
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    299 |          39013.3 | 28704000 |   969.63 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 1.34
    apples_agent-0_min: 0
    apples_agent-1_max: 107
    apples_agent-1_mean: 21.9
    apples_agent-1_min: 0
    apples_agent-2_max: 284
    apples_agent-2_mean: 15.18
    apples_agent-2_min: 0
    apples_agent-3_max: 84
    apples_agent-3_mean: 55.57
    apples_agent-3_min: 27
    apples_agent-4_max: 55
    apples_agent-4_mean: 1.08
    apples_agent-4_min: 0
    apples_agent-5_max: 169
    apples_agent-5_mean: 100.39
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 522
    cleaning_beam_agent-0_mean: 418.53
    cleaning_beam_agent-0_min: 288
    cleaning_beam_agent-1_max: 360
    cleaning_beam_agent-1_mean: 214.31
    cleaning_beam_agent-1_min: 73
    cleaning_beam_agent-2_max: 485
    cleaning_beam_agent-2_mean: 325.65
    cleaning_beam_agent-2_min: 162
    cleaning_beam_agent-3_max: 37
    cleaning_beam_agent-3_mean: 11.45
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 568
    cleaning_beam_agent-4_mean: 462.2
    cleaning_beam_agent-4_min: 256
    cleaning_beam_agent-5_max: 694
    cleaning_beam_agent-5_mean: 55.15
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-04-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1165.999999999979
  episode_reward_mean: 984.8499999999844
  episode_reward_min: 487.0000000000083
  episodes_this_iter: 96
  episodes_total: 28800
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20131.964
    learner:
      agent-0:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8964876532554626
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017115256050601602
        model: {}
        policy_loss: -0.0033221649937331676
        total_loss: -0.0028198822401463985
        vf_explained_var: 0.046933338046073914
        vf_loss: 20.801025390625
      agent-1:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1545279026031494
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017657086718827486
        model: {}
        policy_loss: -0.003981869667768478
        total_loss: -0.003760169493034482
        vf_explained_var: -0.03455808758735657
        vf_loss: 22.53668785095215
      agent-2:
        cur_kl_coeff: 2.3665828097025835e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 1.139390230178833
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011115154484286904
        model: {}
        policy_loss: -0.0031899651512503624
        total_loss: -0.0031531965360045433
        vf_explained_var: 0.058693528175354004
        vf_loss: 20.420969009399414
      agent-3:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3416175842285156
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012669869465753436
        model: {}
        policy_loss: -0.0020918836817145348
        total_loss: -0.0006844969466328621
        vf_explained_var: 0.06894126534461975
        vf_loss: 20.086349487304688
      agent-4:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8917545080184937
        entropy_coeff: 0.0017600000137463212
        kl: 0.002275150502100587
        model: {}
        policy_loss: -0.0038568461313843727
        total_loss: -0.003415003651753068
        vf_explained_var: 0.0743061900138855
        vf_loss: 20.11330795288086
      agent-5:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6356737017631531
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007614133064635098
        model: {}
        policy_loss: -0.0031445478089153767
        total_loss: -0.002284030895680189
        vf_explained_var: 0.08329744637012482
        vf_loss: 19.79304313659668
    load_time_ms: 15585.921
    num_steps_sampled: 28800000
    num_steps_trained: 28800000
    sample_time_ms: 90043.267
    update_time_ms: 21.31
  iterations_since_restore: 280
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.207303370786518
    ram_util_percent: 9.599438202247192
  pid: 4061
  policy_reward_max:
    agent-0: 194.33333333333337
    agent-1: 194.33333333333337
    agent-2: 194.33333333333337
    agent-3: 194.33333333333337
    agent-4: 194.33333333333337
    agent-5: 194.33333333333337
  policy_reward_mean:
    agent-0: 164.1416666666666
    agent-1: 164.1416666666666
    agent-2: 164.1416666666666
    agent-3: 164.1416666666666
    agent-4: 164.1416666666666
    agent-5: 164.1416666666666
  policy_reward_min:
    agent-0: 81.16666666666666
    agent-1: 81.16666666666666
    agent-2: 81.16666666666666
    agent-3: 81.16666666666666
    agent-4: 81.16666666666666
    agent-5: 81.16666666666666
  sampler_perf:
    mean_env_wait_ms: 23.925197228389735
    mean_inference_ms: 12.253488456935939
    mean_processing_ms: 50.59954959214964
  time_since_restore: 35926.937237262726
  time_this_iter_s: 124.71240186691284
  time_total_s: 39138.00092339516
  timestamp: 1637053498
  timesteps_since_restore: 26880000
  timesteps_this_iter: 96000
  timesteps_total: 28800000
  training_iteration: 300
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    300 |            39138 | 28800000 |   984.85 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 22
    apples_agent-0_mean: 1.41
    apples_agent-0_min: 0
    apples_agent-1_max: 118
    apples_agent-1_mean: 22.55
    apples_agent-1_min: 0
    apples_agent-2_max: 284
    apples_agent-2_mean: 15.45
    apples_agent-2_min: 0
    apples_agent-3_max: 89
    apples_agent-3_mean: 53.52
    apples_agent-3_min: 29
    apples_agent-4_max: 22
    apples_agent-4_mean: 0.71
    apples_agent-4_min: 0
    apples_agent-5_max: 299
    apples_agent-5_mean: 109.76
    apples_agent-5_min: 32
    cleaning_beam_agent-0_max: 547
    cleaning_beam_agent-0_mean: 429.88
    cleaning_beam_agent-0_min: 328
    cleaning_beam_agent-1_max: 348
    cleaning_beam_agent-1_mean: 215.52
    cleaning_beam_agent-1_min: 100
    cleaning_beam_agent-2_max: 476
    cleaning_beam_agent-2_mean: 318.72
    cleaning_beam_agent-2_min: 159
    cleaning_beam_agent-3_max: 42
    cleaning_beam_agent-3_mean: 11.27
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 574
    cleaning_beam_agent-4_mean: 451.97
    cleaning_beam_agent-4_min: 364
    cleaning_beam_agent-5_max: 346
    cleaning_beam_agent-5_mean: 42.36
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-07-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1127.9999999999945
  episode_reward_mean: 985.2199999999859
  episode_reward_min: 391.0000000000115
  episodes_this_iter: 96
  episodes_total: 28896
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20112.967
    learner:
      agent-0:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.897780179977417
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013766351621598005
        model: {}
        policy_loss: -0.003135513048619032
        total_loss: -0.0025621256791055202
        vf_explained_var: 0.015944361686706543
        vf_loss: 21.53481674194336
      agent-1:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1546024084091187
        entropy_coeff: 0.0017600000137463212
        kl: 0.001921235118061304
        model: {}
        policy_loss: -0.003710684832185507
        total_loss: -0.0035703247413039207
        vf_explained_var: -0.0029873251914978027
        vf_loss: 21.7246036529541
      agent-2:
        cur_kl_coeff: 1.1832914048512917e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1423394680023193
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012258890783414245
        model: {}
        policy_loss: -0.0032961606048047543
        total_loss: -0.003257689531892538
        vf_explained_var: 0.05044950544834137
        vf_loss: 20.4898624420166
      agent-3:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3380647599697113
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010009013349190354
        model: {}
        policy_loss: -0.002459958428516984
        total_loss: -0.0011240141466259956
        vf_explained_var: 0.10071700811386108
        vf_loss: 19.309371948242188
      agent-4:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8860963582992554
        entropy_coeff: 0.0017600000137463212
        kl: 0.001916999462991953
        model: {}
        policy_loss: -0.003831086214631796
        total_loss: -0.00339439301751554
        vf_explained_var: 0.07629019021987915
        vf_loss: 19.962244033813477
      agent-5:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6681967973709106
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011444775154814124
        model: {}
        policy_loss: -0.003189055249094963
        total_loss: -0.002377210184931755
        vf_explained_var: 0.07365761697292328
        vf_loss: 19.878690719604492
    load_time_ms: 15581.69
    num_steps_sampled: 28896000
    num_steps_trained: 28896000
    sample_time_ms: 89854.171
    update_time_ms: 21.2
  iterations_since_restore: 281
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.33876404494382
    ram_util_percent: 9.67191011235955
  pid: 4061
  policy_reward_max:
    agent-0: 187.99999999999986
    agent-1: 187.99999999999986
    agent-2: 187.99999999999986
    agent-3: 187.99999999999986
    agent-4: 187.99999999999986
    agent-5: 187.99999999999986
  policy_reward_mean:
    agent-0: 164.20333333333326
    agent-1: 164.20333333333326
    agent-2: 164.20333333333326
    agent-3: 164.20333333333326
    agent-4: 164.20333333333326
    agent-5: 164.20333333333326
  policy_reward_min:
    agent-0: 65.16666666666637
    agent-1: 65.16666666666637
    agent-2: 65.16666666666637
    agent-3: 65.16666666666637
    agent-4: 65.16666666666637
    agent-5: 65.16666666666637
  sampler_perf:
    mean_env_wait_ms: 23.926470205500287
    mean_inference_ms: 12.25300989706903
    mean_processing_ms: 50.59687286427873
  time_since_restore: 36051.230081796646
  time_this_iter_s: 124.29284453392029
  time_total_s: 39262.29376792908
  timestamp: 1637053623
  timesteps_since_restore: 26976000
  timesteps_this_iter: 96000
  timesteps_total: 28896000
  training_iteration: 301
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    301 |          39262.3 | 28896000 |   985.22 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 14
    apples_agent-0_mean: 0.55
    apples_agent-0_min: 0
    apples_agent-1_max: 77
    apples_agent-1_mean: 22.72
    apples_agent-1_min: 0
    apples_agent-2_max: 128
    apples_agent-2_mean: 14.38
    apples_agent-2_min: 0
    apples_agent-3_max: 191
    apples_agent-3_mean: 56.93
    apples_agent-3_min: 32
    apples_agent-4_max: 164
    apples_agent-4_mean: 1.73
    apples_agent-4_min: 0
    apples_agent-5_max: 176
    apples_agent-5_mean: 105.55
    apples_agent-5_min: 9
    cleaning_beam_agent-0_max: 559
    cleaning_beam_agent-0_mean: 418.34
    cleaning_beam_agent-0_min: 310
    cleaning_beam_agent-1_max: 340
    cleaning_beam_agent-1_mean: 223.0
    cleaning_beam_agent-1_min: 135
    cleaning_beam_agent-2_max: 465
    cleaning_beam_agent-2_mean: 320.2
    cleaning_beam_agent-2_min: 103
    cleaning_beam_agent-3_max: 60
    cleaning_beam_agent-3_mean: 14.05
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 562
    cleaning_beam_agent-4_mean: 453.82
    cleaning_beam_agent-4_min: 208
    cleaning_beam_agent-5_max: 586
    cleaning_beam_agent-5_mean: 55.8
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-09-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1160.9999999999848
  episode_reward_mean: 988.4699999999859
  episode_reward_min: 509.0000000000087
  episodes_this_iter: 96
  episodes_total: 28992
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20119.315
    learner:
      agent-0:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8946950435638428
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014338407199829817
        model: {}
        policy_loss: -0.003407633863389492
        total_loss: -0.0029110852628946304
        vf_explained_var: -0.009932741522789001
        vf_loss: 20.712148666381836
      agent-1:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1537134647369385
        entropy_coeff: 0.0017600000137463212
        kl: 0.001264532096683979
        model: {}
        policy_loss: -0.003860755590721965
        total_loss: -0.0038081088569015265
        vf_explained_var: -0.02415144443511963
        vf_loss: 20.831819534301758
      agent-2:
        cur_kl_coeff: 5.916457024256459e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1404615640640259
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019259890541434288
        model: {}
        policy_loss: -0.0036900253035128117
        total_loss: -0.0036887708120048046
        vf_explained_var: 0.01570470631122589
        vf_loss: 20.08464813232422
      agent-3:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35006654262542725
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007702013244852424
        model: {}
        policy_loss: -0.002092729788273573
        total_loss: -0.0008081789128482342
        vf_explained_var: 0.05944223701953888
        vf_loss: 19.006694793701172
      agent-4:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8866367340087891
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019084445666521788
        model: {}
        policy_loss: -0.003701527137309313
        total_loss: -0.003351891413331032
        vf_explained_var: 0.05669905245304108
        vf_loss: 19.10116195678711
      agent-5:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.648148238658905
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009631257853470743
        model: {}
        policy_loss: -0.0029661133885383606
        total_loss: -0.002246732823550701
        vf_explained_var: 0.07962919771671295
        vf_loss: 18.601242065429688
    load_time_ms: 15767.284
    num_steps_sampled: 28992000
    num_steps_trained: 28992000
    sample_time_ms: 89899.723
    update_time_ms: 21.137
  iterations_since_restore: 282
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.316201117318437
    ram_util_percent: 9.670391061452515
  pid: 4061
  policy_reward_max:
    agent-0: 193.50000000000028
    agent-1: 193.50000000000028
    agent-2: 193.50000000000028
    agent-3: 193.50000000000028
    agent-4: 193.50000000000028
    agent-5: 193.50000000000028
  policy_reward_mean:
    agent-0: 164.7449999999999
    agent-1: 164.7449999999999
    agent-2: 164.7449999999999
    agent-3: 164.7449999999999
    agent-4: 164.7449999999999
    agent-5: 164.7449999999999
  policy_reward_min:
    agent-0: 84.83333333333326
    agent-1: 84.83333333333326
    agent-2: 84.83333333333326
    agent-3: 84.83333333333326
    agent-4: 84.83333333333326
    agent-5: 84.83333333333326
  sampler_perf:
    mean_env_wait_ms: 23.928766478057902
    mean_inference_ms: 12.252571784911936
    mean_processing_ms: 50.59563306289965
  time_since_restore: 36176.82930588722
  time_this_iter_s: 125.59922409057617
  time_total_s: 39387.89299201965
  timestamp: 1637053749
  timesteps_since_restore: 27072000
  timesteps_this_iter: 96000
  timesteps_total: 28992000
  training_iteration: 302
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    302 |          39387.9 | 28992000 |   988.47 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 22
    apples_agent-0_mean: 1.13
    apples_agent-0_min: 0
    apples_agent-1_max: 101
    apples_agent-1_mean: 23.2
    apples_agent-1_min: 0
    apples_agent-2_max: 107
    apples_agent-2_mean: 15.53
    apples_agent-2_min: 0
    apples_agent-3_max: 101
    apples_agent-3_mean: 56.45
    apples_agent-3_min: 29
    apples_agent-4_max: 49
    apples_agent-4_mean: 0.89
    apples_agent-4_min: 0
    apples_agent-5_max: 177
    apples_agent-5_mean: 103.49
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 532
    cleaning_beam_agent-0_mean: 415.45
    cleaning_beam_agent-0_min: 305
    cleaning_beam_agent-1_max: 392
    cleaning_beam_agent-1_mean: 225.75
    cleaning_beam_agent-1_min: 94
    cleaning_beam_agent-2_max: 515
    cleaning_beam_agent-2_mean: 320.06
    cleaning_beam_agent-2_min: 133
    cleaning_beam_agent-3_max: 67
    cleaning_beam_agent-3_mean: 13.29
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 609
    cleaning_beam_agent-4_mean: 462.12
    cleaning_beam_agent-4_min: 238
    cleaning_beam_agent-5_max: 519
    cleaning_beam_agent-5_mean: 40.18
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-11-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1159.000000000004
  episode_reward_mean: 988.5999999999866
  episode_reward_min: 383.0000000000063
  episodes_this_iter: 96
  episodes_total: 29088
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20118.232
    learner:
      agent-0:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8908601999282837
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016534002497792244
        model: {}
        policy_loss: -0.003387651639059186
        total_loss: -0.0028033670969307423
        vf_explained_var: 0.019818171858787537
        vf_loss: 21.521934509277344
      agent-1:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1640539169311523
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019191683968529105
        model: {}
        policy_loss: -0.003804719541221857
        total_loss: -0.0036495309323072433
        vf_explained_var: -0.00774739682674408
        vf_loss: 22.039213180541992
      agent-2:
        cur_kl_coeff: 2.9582285121282294e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 1.143075704574585
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011090032057836652
        model: {}
        policy_loss: -0.002987910993397236
        total_loss: -0.0027387903537601233
        vf_explained_var: -0.03291282057762146
        vf_loss: 22.60936164855957
      agent-3:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.34769657254219055
        entropy_coeff: 0.0017600000137463212
        kl: 0.001151102827861905
        model: {}
        policy_loss: -0.0023717558942735195
        total_loss: -0.0010408563539385796
        vf_explained_var: 0.11067593097686768
        vf_loss: 19.42844009399414
      agent-4:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8843278884887695
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020919416565448046
        model: {}
        policy_loss: -0.003847796004265547
        total_loss: -0.0034084953367710114
        vf_explained_var: 0.08689697086811066
        vf_loss: 19.957197189331055
      agent-5:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6738311648368835
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013200307730585337
        model: {}
        policy_loss: -0.0032329538371413946
        total_loss: -0.002475044224411249
        vf_explained_var: 0.10630643367767334
        vf_loss: 19.438528060913086
    load_time_ms: 15803.316
    num_steps_sampled: 29088000
    num_steps_trained: 29088000
    sample_time_ms: 89888.864
    update_time_ms: 21.436
  iterations_since_restore: 283
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.416853932584269
    ram_util_percent: 9.575280898876406
  pid: 4061
  policy_reward_max:
    agent-0: 193.16666666666626
    agent-1: 193.16666666666626
    agent-2: 193.16666666666626
    agent-3: 193.16666666666626
    agent-4: 193.16666666666626
    agent-5: 193.16666666666626
  policy_reward_mean:
    agent-0: 164.76666666666654
    agent-1: 164.76666666666654
    agent-2: 164.76666666666654
    agent-3: 164.76666666666654
    agent-4: 164.76666666666654
    agent-5: 164.76666666666654
  policy_reward_min:
    agent-0: 63.833333333333215
    agent-1: 63.833333333333215
    agent-2: 63.833333333333215
    agent-3: 63.833333333333215
    agent-4: 63.833333333333215
    agent-5: 63.833333333333215
  sampler_perf:
    mean_env_wait_ms: 23.930302783717522
    mean_inference_ms: 12.252302444462813
    mean_processing_ms: 50.593994667318526
  time_since_restore: 36301.41748046875
  time_this_iter_s: 124.58817458152771
  time_total_s: 39512.48116660118
  timestamp: 1637053874
  timesteps_since_restore: 27168000
  timesteps_this_iter: 96000
  timesteps_total: 29088000
  training_iteration: 303
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    303 |          39512.5 | 29088000 |    988.6 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 23
    apples_agent-0_mean: 1.33
    apples_agent-0_min: 0
    apples_agent-1_max: 97
    apples_agent-1_mean: 22.29
    apples_agent-1_min: 0
    apples_agent-2_max: 97
    apples_agent-2_mean: 12.2
    apples_agent-2_min: 0
    apples_agent-3_max: 101
    apples_agent-3_mean: 55.41
    apples_agent-3_min: 30
    apples_agent-4_max: 52
    apples_agent-4_mean: 0.93
    apples_agent-4_min: 0
    apples_agent-5_max: 188
    apples_agent-5_mean: 106.63
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 486
    cleaning_beam_agent-0_mean: 410.46
    cleaning_beam_agent-0_min: 302
    cleaning_beam_agent-1_max: 348
    cleaning_beam_agent-1_mean: 216.63
    cleaning_beam_agent-1_min: 120
    cleaning_beam_agent-2_max: 521
    cleaning_beam_agent-2_mean: 338.7
    cleaning_beam_agent-2_min: 176
    cleaning_beam_agent-3_max: 60
    cleaning_beam_agent-3_mean: 14.5
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 573
    cleaning_beam_agent-4_mean: 466.16
    cleaning_beam_agent-4_min: 237
    cleaning_beam_agent-5_max: 969
    cleaning_beam_agent-5_mean: 49.62
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-13-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1160.9999999999927
  episode_reward_mean: 970.2199999999849
  episode_reward_min: 266.9999999999957
  episodes_this_iter: 96
  episodes_total: 29184
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20118.708
    learner:
      agent-0:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.885517954826355
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019292482174932957
        model: {}
        policy_loss: -0.003305196762084961
        total_loss: -0.002690996741876006
        vf_explained_var: 0.024271413683891296
        vf_loss: 21.727128982543945
      agent-1:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1595087051391602
        entropy_coeff: 0.0017600000137463212
        kl: 0.002105094725266099
        model: {}
        policy_loss: -0.004158796742558479
        total_loss: -0.003944315481930971
        vf_explained_var: -0.018690288066864014
        vf_loss: 22.552181243896484
      agent-2:
        cur_kl_coeff: 1.4791142560641147e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1385293006896973
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009890589863061905
        model: {}
        policy_loss: -0.0029884048271924257
        total_loss: -0.0028151562437415123
        vf_explained_var: 0.01180817186832428
        vf_loss: 21.770626068115234
      agent-3:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3594736158847809
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011706686345860362
        model: {}
        policy_loss: -0.002393841277807951
        total_loss: -0.0010784855112433434
        vf_explained_var: 0.11836071312427521
        vf_loss: 19.480270385742188
      agent-4:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8772329092025757
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017071521142497659
        model: {}
        policy_loss: -0.003920867573469877
        total_loss: -0.0034012622199952602
        vf_explained_var: 0.06599612534046173
        vf_loss: 20.63534164428711
      agent-5:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6557714939117432
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008739411132410169
        model: {}
        policy_loss: -0.002967940177768469
        total_loss: -0.0021806934382766485
        vf_explained_var: 0.11958253383636475
        vf_loss: 19.414031982421875
    load_time_ms: 15323.491
    num_steps_sampled: 29184000
    num_steps_trained: 29184000
    sample_time_ms: 89827.497
    update_time_ms: 21.359
  iterations_since_restore: 284
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.220224719101122
    ram_util_percent: 9.670224719101125
  pid: 4061
  policy_reward_max:
    agent-0: 193.49999999999966
    agent-1: 193.49999999999966
    agent-2: 193.49999999999966
    agent-3: 193.49999999999966
    agent-4: 193.49999999999966
    agent-5: 193.49999999999966
  policy_reward_mean:
    agent-0: 161.70333333333326
    agent-1: 161.70333333333326
    agent-2: 161.70333333333326
    agent-3: 161.70333333333326
    agent-4: 161.70333333333326
    agent-5: 161.70333333333326
  policy_reward_min:
    agent-0: 44.49999999999995
    agent-1: 44.49999999999995
    agent-2: 44.49999999999995
    agent-3: 44.49999999999995
    agent-4: 44.49999999999995
    agent-5: 44.49999999999995
  sampler_perf:
    mean_env_wait_ms: 23.932197947187223
    mean_inference_ms: 12.251885826964445
    mean_processing_ms: 50.59211032212754
  time_since_restore: 36426.44387817383
  time_this_iter_s: 125.02639770507812
  time_total_s: 39637.50756430626
  timestamp: 1637053999
  timesteps_since_restore: 27264000
  timesteps_this_iter: 96000
  timesteps_total: 29184000
  training_iteration: 304
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    304 |          39637.5 | 29184000 |   970.22 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 44
    apples_agent-0_mean: 2.0
    apples_agent-0_min: 0
    apples_agent-1_max: 103
    apples_agent-1_mean: 25.41
    apples_agent-1_min: 0
    apples_agent-2_max: 72
    apples_agent-2_mean: 11.1
    apples_agent-2_min: 0
    apples_agent-3_max: 89
    apples_agent-3_mean: 54.26
    apples_agent-3_min: 30
    apples_agent-4_max: 67
    apples_agent-4_mean: 0.8
    apples_agent-4_min: 0
    apples_agent-5_max: 182
    apples_agent-5_mean: 103.17
    apples_agent-5_min: 11
    cleaning_beam_agent-0_max: 537
    cleaning_beam_agent-0_mean: 418.34
    cleaning_beam_agent-0_min: 301
    cleaning_beam_agent-1_max: 358
    cleaning_beam_agent-1_mean: 221.87
    cleaning_beam_agent-1_min: 81
    cleaning_beam_agent-2_max: 489
    cleaning_beam_agent-2_mean: 334.56
    cleaning_beam_agent-2_min: 179
    cleaning_beam_agent-3_max: 67
    cleaning_beam_agent-3_mean: 14.07
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 575
    cleaning_beam_agent-4_mean: 469.01
    cleaning_beam_agent-4_min: 317
    cleaning_beam_agent-5_max: 524
    cleaning_beam_agent-5_mean: 54.69
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-15-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1173.9999999999873
  episode_reward_mean: 994.4399999999861
  episode_reward_min: 441.00000000000716
  episodes_this_iter: 96
  episodes_total: 29280
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20131.031
    learner:
      agent-0:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8931295871734619
        entropy_coeff: 0.0017600000137463212
        kl: 0.001559284282848239
        model: {}
        policy_loss: -0.0032772740814834833
        total_loss: -0.0027950298972427845
        vf_explained_var: 0.009245619177818298
        vf_loss: 20.541534423828125
      agent-1:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1501305103302002
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010087622795253992
        model: {}
        policy_loss: -0.0034406199119985104
        total_loss: -0.0033068135380744934
        vf_explained_var: -0.043987929821014404
        vf_loss: 21.580379486083984
      agent-2:
        cur_kl_coeff: 7.395571280320573e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 1.139695644378662
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014646981144323945
        model: {}
        policy_loss: -0.003226254601031542
        total_loss: -0.003220388200134039
        vf_explained_var: 0.012619689106941223
        vf_loss: 20.117311477661133
      agent-3:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.33326098322868347
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009113333071582019
        model: {}
        policy_loss: -0.0018965238705277443
        total_loss: -0.0006684707477688789
        vf_explained_var: 0.11306288838386536
        vf_loss: 18.145938873291016
      agent-4:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8721587657928467
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019790567457675934
        model: {}
        policy_loss: -0.004242613911628723
        total_loss: -0.003824058920145035
        vf_explained_var: 0.044531792402267456
        vf_loss: 19.535545349121094
      agent-5:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6804162859916687
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012822005664929748
        model: {}
        policy_loss: -0.0032501991372555494
        total_loss: -0.00249861110933125
        vf_explained_var: 0.05108438432216644
        vf_loss: 19.49121856689453
    load_time_ms: 15126.674
    num_steps_sampled: 29280000
    num_steps_trained: 29280000
    sample_time_ms: 89844.342
    update_time_ms: 21.252
  iterations_since_restore: 285
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.305649717514127
    ram_util_percent: 9.589265536723165
  pid: 4061
  policy_reward_max:
    agent-0: 195.66666666666646
    agent-1: 195.66666666666646
    agent-2: 195.66666666666646
    agent-3: 195.66666666666646
    agent-4: 195.66666666666646
    agent-5: 195.66666666666646
  policy_reward_mean:
    agent-0: 165.7399999999999
    agent-1: 165.7399999999999
    agent-2: 165.7399999999999
    agent-3: 165.7399999999999
    agent-4: 165.7399999999999
    agent-5: 165.7399999999999
  policy_reward_min:
    agent-0: 73.50000000000006
    agent-1: 73.50000000000006
    agent-2: 73.50000000000006
    agent-3: 73.50000000000006
    agent-4: 73.50000000000006
    agent-5: 73.50000000000006
  sampler_perf:
    mean_env_wait_ms: 23.934938775039303
    mean_inference_ms: 12.251641254899555
    mean_processing_ms: 50.59109474392182
  time_since_restore: 36550.9277279377
  time_this_iter_s: 124.48384976387024
  time_total_s: 39761.99141407013
  timestamp: 1637054123
  timesteps_since_restore: 27360000
  timesteps_this_iter: 96000
  timesteps_total: 29280000
  training_iteration: 305
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    305 |            39762 | 29280000 |   994.44 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 1.35
    apples_agent-0_min: 0
    apples_agent-1_max: 62
    apples_agent-1_mean: 21.92
    apples_agent-1_min: 0
    apples_agent-2_max: 152
    apples_agent-2_mean: 15.76
    apples_agent-2_min: 0
    apples_agent-3_max: 91
    apples_agent-3_mean: 55.63
    apples_agent-3_min: 31
    apples_agent-4_max: 44
    apples_agent-4_mean: 0.69
    apples_agent-4_min: 0
    apples_agent-5_max: 205
    apples_agent-5_mean: 102.06
    apples_agent-5_min: 11
    cleaning_beam_agent-0_max: 520
    cleaning_beam_agent-0_mean: 424.35
    cleaning_beam_agent-0_min: 310
    cleaning_beam_agent-1_max: 312
    cleaning_beam_agent-1_mean: 214.34
    cleaning_beam_agent-1_min: 118
    cleaning_beam_agent-2_max: 552
    cleaning_beam_agent-2_mean: 331.29
    cleaning_beam_agent-2_min: 118
    cleaning_beam_agent-3_max: 48
    cleaning_beam_agent-3_mean: 12.7
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 608
    cleaning_beam_agent-4_mean: 472.68
    cleaning_beam_agent-4_min: 339
    cleaning_beam_agent-5_max: 524
    cleaning_beam_agent-5_mean: 53.93
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-17-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1123.0000000000002
  episode_reward_mean: 978.7999999999861
  episode_reward_min: 345.0000000000037
  episodes_this_iter: 96
  episodes_total: 29376
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20130.452
    learner:
      agent-0:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8990404009819031
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014225650811567903
        model: {}
        policy_loss: -0.0033031064085662365
        total_loss: -0.002713629510253668
        vf_explained_var: -0.0057291388511657715
        vf_loss: 21.717891693115234
      agent-1:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 1.165953516960144
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013914420269429684
        model: {}
        policy_loss: -0.0037787510082125664
        total_loss: -0.0036156978458166122
        vf_explained_var: -0.028651267290115356
        vf_loss: 22.151321411132812
      agent-2:
        cur_kl_coeff: 3.697785640160287e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1469141244888306
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015844990266487002
        model: {}
        policy_loss: -0.003216212848201394
        total_loss: -0.0031628720462322235
        vf_explained_var: 0.030114516615867615
        vf_loss: 20.719093322753906
      agent-3:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3445911407470703
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010840592440217733
        model: {}
        policy_loss: -0.0022976140026003122
        total_loss: -0.0010151719907298684
        vf_explained_var: 0.11452558636665344
        vf_loss: 18.88926124572754
      agent-4:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.870283305644989
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015948580112308264
        model: {}
        policy_loss: -0.0038890938740223646
        total_loss: -0.0034129538107663393
        vf_explained_var: 0.060991838574409485
        vf_loss: 20.078369140625
      agent-5:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6895895004272461
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007601129473187029
        model: {}
        policy_loss: -0.002987888641655445
        total_loss: -0.0022705160081386566
        vf_explained_var: 0.09685327112674713
        vf_loss: 19.31051254272461
    load_time_ms: 14916.272
    num_steps_sampled: 29376000
    num_steps_trained: 29376000
    sample_time_ms: 89710.989
    update_time_ms: 20.983
  iterations_since_restore: 286
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.409714285714285
    ram_util_percent: 9.649714285714287
  pid: 4061
  policy_reward_max:
    agent-0: 187.16666666666674
    agent-1: 187.16666666666674
    agent-2: 187.16666666666674
    agent-3: 187.16666666666674
    agent-4: 187.16666666666674
    agent-5: 187.16666666666674
  policy_reward_mean:
    agent-0: 163.13333333333318
    agent-1: 163.13333333333318
    agent-2: 163.13333333333318
    agent-3: 163.13333333333318
    agent-4: 163.13333333333318
    agent-5: 163.13333333333318
  policy_reward_min:
    agent-0: 57.49999999999977
    agent-1: 57.49999999999977
    agent-2: 57.49999999999977
    agent-3: 57.49999999999977
    agent-4: 57.49999999999977
    agent-5: 57.49999999999977
  sampler_perf:
    mean_env_wait_ms: 23.936814494644135
    mean_inference_ms: 12.251187522595337
    mean_processing_ms: 50.58909921066704
  time_since_restore: 36673.55887174606
  time_this_iter_s: 122.63114380836487
  time_total_s: 39884.622557878494
  timestamp: 1637054246
  timesteps_since_restore: 27456000
  timesteps_this_iter: 96000
  timesteps_total: 29376000
  training_iteration: 306
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    306 |          39884.6 | 29376000 |    978.8 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 1.02
    apples_agent-0_min: 0
    apples_agent-1_max: 84
    apples_agent-1_mean: 23.15
    apples_agent-1_min: 0
    apples_agent-2_max: 186
    apples_agent-2_mean: 14.86
    apples_agent-2_min: 0
    apples_agent-3_max: 111
    apples_agent-3_mean: 57.12
    apples_agent-3_min: 31
    apples_agent-4_max: 82
    apples_agent-4_mean: 1.47
    apples_agent-4_min: 0
    apples_agent-5_max: 172
    apples_agent-5_mean: 108.48
    apples_agent-5_min: 26
    cleaning_beam_agent-0_max: 567
    cleaning_beam_agent-0_mean: 415.83
    cleaning_beam_agent-0_min: 284
    cleaning_beam_agent-1_max: 331
    cleaning_beam_agent-1_mean: 221.9
    cleaning_beam_agent-1_min: 114
    cleaning_beam_agent-2_max: 576
    cleaning_beam_agent-2_mean: 341.6
    cleaning_beam_agent-2_min: 137
    cleaning_beam_agent-3_max: 72
    cleaning_beam_agent-3_mean: 12.71
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 607
    cleaning_beam_agent-4_mean: 474.99
    cleaning_beam_agent-4_min: 288
    cleaning_beam_agent-5_max: 501
    cleaning_beam_agent-5_mean: 45.2
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 5
    fire_beam_agent-4_mean: 0.06
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-19-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1156.9999999999814
  episode_reward_mean: 992.3199999999848
  episode_reward_min: 576.0000000000042
  episodes_this_iter: 96
  episodes_total: 29472
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20130.521
    learner:
      agent-0:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8977556228637695
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012565694050863385
        model: {}
        policy_loss: -0.0030182332266122103
        total_loss: -0.002455252455547452
        vf_explained_var: 0.00841638445854187
        vf_loss: 21.430328369140625
      agent-1:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1482455730438232
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014756277669221163
        model: {}
        policy_loss: -0.0038438518531620502
        total_loss: -0.0037315613590180874
        vf_explained_var: 0.0088481605052948
        vf_loss: 21.332046508789062
      agent-2:
        cur_kl_coeff: 1.8488928200801433e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1382019519805908
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011449477169662714
        model: {}
        policy_loss: -0.003071070183068514
        total_loss: -0.003010938409715891
        vf_explained_var: 0.03340280055999756
        vf_loss: 20.63365936279297
      agent-3:
        cur_kl_coeff: 6.018531165893214e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.34268343448638916
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010453591821715236
        model: {}
        policy_loss: -0.0021261346992105246
        total_loss: -0.0007741262670606375
        vf_explained_var: 0.08441378176212311
        vf_loss: 19.551319122314453
      agent-4:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8653563857078552
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013285024324432015
        model: {}
        policy_loss: -0.0035235867835581303
        total_loss: -0.0029815901070833206
        vf_explained_var: 0.03849083185195923
        vf_loss: 20.650238037109375
      agent-5:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6692665219306946
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014061391120776534
        model: {}
        policy_loss: -0.0034167892299592495
        total_loss: -0.002616466023027897
        vf_explained_var: 0.07583853602409363
        vf_loss: 19.78233528137207
    load_time_ms: 14851.148
    num_steps_sampled: 29472000
    num_steps_trained: 29472000
    sample_time_ms: 89731.526
    update_time_ms: 21.225
  iterations_since_restore: 287
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.5731843575419
    ram_util_percent: 9.598882681564247
  pid: 4061
  policy_reward_max:
    agent-0: 192.8333333333332
    agent-1: 192.8333333333332
    agent-2: 192.8333333333332
    agent-3: 192.8333333333332
    agent-4: 192.8333333333332
    agent-5: 192.8333333333332
  policy_reward_mean:
    agent-0: 165.3866666666666
    agent-1: 165.3866666666666
    agent-2: 165.3866666666666
    agent-3: 165.3866666666666
    agent-4: 165.3866666666666
    agent-5: 165.3866666666666
  policy_reward_min:
    agent-0: 95.99999999999999
    agent-1: 95.99999999999999
    agent-2: 95.99999999999999
    agent-3: 95.99999999999999
    agent-4: 95.99999999999999
    agent-5: 95.99999999999999
  sampler_perf:
    mean_env_wait_ms: 23.939153476947528
    mean_inference_ms: 12.250935744355857
    mean_processing_ms: 50.58757315445638
  time_since_restore: 36798.9347884655
  time_this_iter_s: 125.37591671943665
  time_total_s: 40009.99847459793
  timestamp: 1637054372
  timesteps_since_restore: 27552000
  timesteps_this_iter: 96000
  timesteps_total: 29472000
  training_iteration: 307
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    307 |            40010 | 29472000 |   992.32 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 28
    apples_agent-0_mean: 1.06
    apples_agent-0_min: 0
    apples_agent-1_max: 127
    apples_agent-1_mean: 26.68
    apples_agent-1_min: 0
    apples_agent-2_max: 148
    apples_agent-2_mean: 17.57
    apples_agent-2_min: 0
    apples_agent-3_max: 177
    apples_agent-3_mean: 59.71
    apples_agent-3_min: 31
    apples_agent-4_max: 82
    apples_agent-4_mean: 1.55
    apples_agent-4_min: 0
    apples_agent-5_max: 170
    apples_agent-5_mean: 107.02
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 506
    cleaning_beam_agent-0_mean: 417.69
    cleaning_beam_agent-0_min: 284
    cleaning_beam_agent-1_max: 357
    cleaning_beam_agent-1_mean: 209.12
    cleaning_beam_agent-1_min: 93
    cleaning_beam_agent-2_max: 576
    cleaning_beam_agent-2_mean: 328.69
    cleaning_beam_agent-2_min: 123
    cleaning_beam_agent-3_max: 64
    cleaning_beam_agent-3_mean: 13.16
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 601
    cleaning_beam_agent-4_mean: 487.9
    cleaning_beam_agent-4_min: 297
    cleaning_beam_agent-5_max: 624
    cleaning_beam_agent-5_mean: 42.37
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-21-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1185.9999999999827
  episode_reward_mean: 992.9399999999858
  episode_reward_min: 578.0000000000041
  episodes_this_iter: 96
  episodes_total: 29568
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20142.962
    learner:
      agent-0:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8923825621604919
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015786616131663322
        model: {}
        policy_loss: -0.003114815801382065
        total_loss: -0.0026554963551461697
        vf_explained_var: 0.023313716053962708
        vf_loss: 20.29911231994629
      agent-1:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 1.150412917137146
        entropy_coeff: 0.0017600000137463212
        kl: 0.001259105047211051
        model: {}
        policy_loss: -0.0035856026224792004
        total_loss: -0.003523819148540497
        vf_explained_var: 0.0007415562868118286
        vf_loss: 20.865108489990234
      agent-2:
        cur_kl_coeff: 9.244464100400717e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 1.129995346069336
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018922321032732725
        model: {}
        policy_loss: -0.0036195493303239346
        total_loss: -0.0035795848816633224
        vf_explained_var: 0.014448225498199463
        vf_loss: 20.287559509277344
      agent-3:
        cur_kl_coeff: 3.009265582946607e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35825008153915405
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011293392162770033
        model: {}
        policy_loss: -0.002165716141462326
        total_loss: -0.0009285034611821175
        vf_explained_var: 0.0912540853023529
        vf_loss: 18.67731285095215
      agent-4:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8527138233184814
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015045125037431717
        model: {}
        policy_loss: -0.003658166155219078
        total_loss: -0.003202628344297409
        vf_explained_var: 0.052729398012161255
        vf_loss: 19.563125610351562
      agent-5:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6652054190635681
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010384400375187397
        model: {}
        policy_loss: -0.0034646608401089907
        total_loss: -0.0027653572615236044
        vf_explained_var: 0.08976136147975922
        vf_loss: 18.70066261291504
    load_time_ms: 14633.415
    num_steps_sampled: 29568000
    num_steps_trained: 29568000
    sample_time_ms: 89658.832
    update_time_ms: 21.758
  iterations_since_restore: 288
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.38125
    ram_util_percent: 9.66647727272727
  pid: 4061
  policy_reward_max:
    agent-0: 197.6666666666667
    agent-1: 197.6666666666667
    agent-2: 197.6666666666667
    agent-3: 197.6666666666667
    agent-4: 197.6666666666667
    agent-5: 197.6666666666667
  policy_reward_mean:
    agent-0: 165.48999999999987
    agent-1: 165.48999999999987
    agent-2: 165.48999999999987
    agent-3: 165.48999999999987
    agent-4: 165.48999999999987
    agent-5: 165.48999999999987
  policy_reward_min:
    agent-0: 96.3333333333337
    agent-1: 96.3333333333337
    agent-2: 96.3333333333337
    agent-3: 96.3333333333337
    agent-4: 96.3333333333337
    agent-5: 96.3333333333337
  sampler_perf:
    mean_env_wait_ms: 23.94092907903117
    mean_inference_ms: 12.250634094370865
    mean_processing_ms: 50.58483188148324
  time_since_restore: 36922.58996462822
  time_this_iter_s: 123.65517616271973
  time_total_s: 40133.65365076065
  timestamp: 1637054496
  timesteps_since_restore: 27648000
  timesteps_this_iter: 96000
  timesteps_total: 29568000
  training_iteration: 308
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    308 |          40133.7 | 29568000 |   992.94 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 31
    apples_agent-0_mean: 1.28
    apples_agent-0_min: 0
    apples_agent-1_max: 141
    apples_agent-1_mean: 27.11
    apples_agent-1_min: 0
    apples_agent-2_max: 141
    apples_agent-2_mean: 14.99
    apples_agent-2_min: 0
    apples_agent-3_max: 125
    apples_agent-3_mean: 57.87
    apples_agent-3_min: 32
    apples_agent-4_max: 44
    apples_agent-4_mean: 0.73
    apples_agent-4_min: 0
    apples_agent-5_max: 156
    apples_agent-5_mean: 104.92
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 510
    cleaning_beam_agent-0_mean: 409.9
    cleaning_beam_agent-0_min: 265
    cleaning_beam_agent-1_max: 335
    cleaning_beam_agent-1_mean: 206.74
    cleaning_beam_agent-1_min: 104
    cleaning_beam_agent-2_max: 517
    cleaning_beam_agent-2_mean: 346.69
    cleaning_beam_agent-2_min: 113
    cleaning_beam_agent-3_max: 51
    cleaning_beam_agent-3_mean: 12.82
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 598
    cleaning_beam_agent-4_mean: 478.57
    cleaning_beam_agent-4_min: 346
    cleaning_beam_agent-5_max: 760
    cleaning_beam_agent-5_mean: 42.58
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-23-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1169.0000000000068
  episode_reward_mean: 994.1599999999856
  episode_reward_min: 424.00000000001035
  episodes_this_iter: 96
  episodes_total: 29664
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20129.788
    learner:
      agent-0:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8903490304946899
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018318762304261327
        model: {}
        policy_loss: -0.003252434078603983
        total_loss: -0.0026430031284689903
        vf_explained_var: -0.019217774271965027
        vf_loss: 21.764476776123047
      agent-1:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1497929096221924
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017426712438464165
        model: {}
        policy_loss: -0.0038769198581576347
        total_loss: -0.0037363118026405573
        vf_explained_var: -0.02028624713420868
        vf_loss: 21.6424617767334
      agent-2:
        cur_kl_coeff: 4.622232050200358e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 1.132306694984436
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015575701836496592
        model: {}
        policy_loss: -0.003338603535667062
        total_loss: -0.003358270274475217
        vf_explained_var: 0.05502060055732727
        vf_loss: 19.73194122314453
      agent-3:
        cur_kl_coeff: 1.5046327914733034e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.34393978118896484
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009180193301290274
        model: {}
        policy_loss: -0.0021099280565977097
        total_loss: -0.0007777911378070712
        vf_explained_var: 0.07289832830429077
        vf_loss: 19.374710083007812
      agent-4:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8623131513595581
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012108783703297377
        model: {}
        policy_loss: -0.003859264310449362
        total_loss: -0.003337112721055746
        vf_explained_var: 0.026199370622634888
        vf_loss: 20.398212432861328
      agent-5:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6607899069786072
        entropy_coeff: 0.0017600000137463212
        kl: 0.001273157773539424
        model: {}
        policy_loss: -0.0028844084590673447
        total_loss: -0.002145894803106785
        vf_explained_var: 0.09500733017921448
        vf_loss: 19.014997482299805
    load_time_ms: 14655.506
    num_steps_sampled: 29664000
    num_steps_trained: 29664000
    sample_time_ms: 89593.428
    update_time_ms: 20.98
  iterations_since_restore: 289
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.15056179775281
    ram_util_percent: 9.605056179775282
  pid: 4061
  policy_reward_max:
    agent-0: 194.83333333333275
    agent-1: 194.83333333333275
    agent-2: 194.83333333333275
    agent-3: 194.83333333333275
    agent-4: 194.83333333333275
    agent-5: 194.83333333333275
  policy_reward_mean:
    agent-0: 165.69333333333321
    agent-1: 165.69333333333321
    agent-2: 165.69333333333321
    agent-3: 165.69333333333321
    agent-4: 165.69333333333321
    agent-5: 165.69333333333321
  policy_reward_min:
    agent-0: 70.6666666666664
    agent-1: 70.6666666666664
    agent-2: 70.6666666666664
    agent-3: 70.6666666666664
    agent-4: 70.6666666666664
    agent-5: 70.6666666666664
  sampler_perf:
    mean_env_wait_ms: 23.942653604197307
    mean_inference_ms: 12.250373186747554
    mean_processing_ms: 50.58226483155669
  time_since_restore: 37047.22740459442
  time_this_iter_s: 124.63743996620178
  time_total_s: 40258.29109072685
  timestamp: 1637054620
  timesteps_since_restore: 27744000
  timesteps_this_iter: 96000
  timesteps_total: 29664000
  training_iteration: 309
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 17.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    309 |          40258.3 | 29664000 |   994.16 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 1.76
    apples_agent-0_min: 0
    apples_agent-1_max: 78
    apples_agent-1_mean: 28.93
    apples_agent-1_min: 0
    apples_agent-2_max: 124
    apples_agent-2_mean: 16.45
    apples_agent-2_min: 0
    apples_agent-3_max: 102
    apples_agent-3_mean: 55.6
    apples_agent-3_min: 19
    apples_agent-4_max: 46
    apples_agent-4_mean: 0.95
    apples_agent-4_min: 0
    apples_agent-5_max: 185
    apples_agent-5_mean: 101.13
    apples_agent-5_min: 8
    cleaning_beam_agent-0_max: 520
    cleaning_beam_agent-0_mean: 404.86
    cleaning_beam_agent-0_min: 227
    cleaning_beam_agent-1_max: 372
    cleaning_beam_agent-1_mean: 214.44
    cleaning_beam_agent-1_min: 98
    cleaning_beam_agent-2_max: 589
    cleaning_beam_agent-2_mean: 352.22
    cleaning_beam_agent-2_min: 137
    cleaning_beam_agent-3_max: 67
    cleaning_beam_agent-3_mean: 12.31
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 564
    cleaning_beam_agent-4_mean: 467.69
    cleaning_beam_agent-4_min: 335
    cleaning_beam_agent-5_max: 520
    cleaning_beam_agent-5_mean: 39.63
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-25-44
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1139.999999999999
  episode_reward_mean: 996.2299999999871
  episode_reward_min: 368.0000000000027
  episodes_this_iter: 96
  episodes_total: 29760
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20146.742
    learner:
      agent-0:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9046518206596375
        entropy_coeff: 0.0017600000137463212
        kl: 0.001790903857909143
        model: {}
        policy_loss: -0.002938438905403018
        total_loss: -0.0023881690576672554
        vf_explained_var: 0.020666420459747314
        vf_loss: 21.424564361572266
      agent-1:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1529308557510376
        entropy_coeff: 0.0017600000137463212
        kl: 0.002524792682379484
        model: {}
        policy_loss: -0.004208347760140896
        total_loss: -0.004005257040262222
        vf_explained_var: -0.022654950618743896
        vf_loss: 22.322484970092773
      agent-2:
        cur_kl_coeff: 2.311116025100179e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1243809461593628
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014841653173789382
        model: {}
        policy_loss: -0.003416336141526699
        total_loss: -0.0033023389987647533
        vf_explained_var: 0.031216546893119812
        vf_loss: 20.929119110107422
      agent-3:
        cur_kl_coeff: 7.523163957366517e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3552166819572449
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010021950583904982
        model: {}
        policy_loss: -0.002178068505600095
        total_loss: -0.000865056412294507
        vf_explained_var: 0.10728280246257782
        vf_loss: 19.381921768188477
      agent-4:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8684852123260498
        entropy_coeff: 0.0017600000137463212
        kl: 0.001384455244988203
        model: {}
        policy_loss: -0.00348503771238029
        total_loss: -0.0029322816990315914
        vf_explained_var: 0.03985753655433655
        vf_loss: 20.812911987304688
      agent-5:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.650303304195404
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010442663915455341
        model: {}
        policy_loss: -0.0031381528824567795
        total_loss: -0.0023957788944244385
        vf_explained_var: 0.12805034220218658
        vf_loss: 18.869110107421875
    load_time_ms: 14521.186
    num_steps_sampled: 29760000
    num_steps_trained: 29760000
    sample_time_ms: 89573.778
    update_time_ms: 20.939
  iterations_since_restore: 290
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.314772727272727
    ram_util_percent: 9.59375
  pid: 4061
  policy_reward_max:
    agent-0: 190.00000000000003
    agent-1: 190.00000000000003
    agent-2: 190.00000000000003
    agent-3: 190.00000000000003
    agent-4: 190.00000000000003
    agent-5: 190.00000000000003
  policy_reward_mean:
    agent-0: 166.0383333333332
    agent-1: 166.0383333333332
    agent-2: 166.0383333333332
    agent-3: 166.0383333333332
    agent-4: 166.0383333333332
    agent-5: 166.0383333333332
  policy_reward_min:
    agent-0: 61.333333333333144
    agent-1: 61.333333333333144
    agent-2: 61.333333333333144
    agent-3: 61.333333333333144
    agent-4: 61.333333333333144
    agent-5: 61.333333333333144
  sampler_perf:
    mean_env_wait_ms: 23.944214862144808
    mean_inference_ms: 12.25010056943886
    mean_processing_ms: 50.58064988981356
  time_since_restore: 37170.57220363617
  time_this_iter_s: 123.34479904174805
  time_total_s: 40381.6358897686
  timestamp: 1637054744
  timesteps_since_restore: 27840000
  timesteps_this_iter: 96000
  timesteps_total: 29760000
  training_iteration: 310
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    310 |          40381.6 | 29760000 |   996.23 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 50
    apples_agent-0_mean: 1.67
    apples_agent-0_min: 0
    apples_agent-1_max: 76
    apples_agent-1_mean: 29.09
    apples_agent-1_min: 0
    apples_agent-2_max: 147
    apples_agent-2_mean: 14.44
    apples_agent-2_min: 0
    apples_agent-3_max: 101
    apples_agent-3_mean: 58.55
    apples_agent-3_min: 32
    apples_agent-4_max: 12
    apples_agent-4_mean: 0.12
    apples_agent-4_min: 0
    apples_agent-5_max: 217
    apples_agent-5_mean: 102.27
    apples_agent-5_min: 11
    cleaning_beam_agent-0_max: 500
    cleaning_beam_agent-0_mean: 409.68
    cleaning_beam_agent-0_min: 232
    cleaning_beam_agent-1_max: 320
    cleaning_beam_agent-1_mean: 202.61
    cleaning_beam_agent-1_min: 93
    cleaning_beam_agent-2_max: 526
    cleaning_beam_agent-2_mean: 345.08
    cleaning_beam_agent-2_min: 169
    cleaning_beam_agent-3_max: 71
    cleaning_beam_agent-3_mean: 13.35
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 600
    cleaning_beam_agent-4_mean: 471.74
    cleaning_beam_agent-4_min: 343
    cleaning_beam_agent-5_max: 341
    cleaning_beam_agent-5_mean: 43.22
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-27-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1190.0000000000093
  episode_reward_mean: 997.7499999999853
  episode_reward_min: 740.9999999999914
  episodes_this_iter: 96
  episodes_total: 29856
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20170.899
    learner:
      agent-0:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8912203907966614
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014386533293873072
        model: {}
        policy_loss: -0.002938356250524521
        total_loss: -0.0025319699198007584
        vf_explained_var: -0.011700361967086792
        vf_loss: 19.749364852905273
      agent-1:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1487159729003906
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014785972889512777
        model: {}
        policy_loss: -0.003657606430351734
        total_loss: -0.0036724694073200226
        vf_explained_var: -0.025317609310150146
        vf_loss: 20.068801879882812
      agent-2:
        cur_kl_coeff: 1.1555580125500896e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1318390369415283
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012767501175403595
        model: {}
        policy_loss: -0.0031177885830402374
        total_loss: -0.0032178813125938177
        vf_explained_var: 0.015980198979377747
        vf_loss: 18.919445037841797
      agent-3:
        cur_kl_coeff: 3.7615819786832586e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3430398404598236
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007486192625947297
        model: {}
        policy_loss: -0.0020227842032909393
        total_loss: -0.0008081925334408879
        vf_explained_var: 0.05281704664230347
        vf_loss: 18.18341827392578
      agent-4:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8621190786361694
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012552780099213123
        model: {}
        policy_loss: -0.003299928270280361
        total_loss: -0.002893260680139065
        vf_explained_var: 0.0023468583822250366
        vf_loss: 19.23996925354004
      agent-5:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6526023149490356
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012949449010193348
        model: {}
        policy_loss: -0.0035095103085041046
        total_loss: -0.0028142062947154045
        vf_explained_var: 0.041632577776908875
        vf_loss: 18.438844680786133
    load_time_ms: 14677.603
    num_steps_sampled: 29856000
    num_steps_trained: 29856000
    sample_time_ms: 89699.536
    update_time_ms: 20.853
  iterations_since_restore: 291
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.10054945054945
    ram_util_percent: 9.676923076923076
  pid: 4061
  policy_reward_max:
    agent-0: 198.33333333333323
    agent-1: 198.33333333333323
    agent-2: 198.33333333333323
    agent-3: 198.33333333333323
    agent-4: 198.33333333333323
    agent-5: 198.33333333333323
  policy_reward_mean:
    agent-0: 166.2916666666665
    agent-1: 166.2916666666665
    agent-2: 166.2916666666665
    agent-3: 166.2916666666665
    agent-4: 166.2916666666665
    agent-5: 166.2916666666665
  policy_reward_min:
    agent-0: 123.50000000000058
    agent-1: 123.50000000000058
    agent-2: 123.50000000000058
    agent-3: 123.50000000000058
    agent-4: 123.50000000000058
    agent-5: 123.50000000000058
  sampler_perf:
    mean_env_wait_ms: 23.946366041467382
    mean_inference_ms: 12.249839029352446
    mean_processing_ms: 50.57949502752174
  time_since_restore: 37297.925265312195
  time_this_iter_s: 127.35306167602539
  time_total_s: 40508.988951444626
  timestamp: 1637054871
  timesteps_since_restore: 27936000
  timesteps_this_iter: 96000
  timesteps_total: 29856000
  training_iteration: 311
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    311 |            40509 | 29856000 |   997.75 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 35
    apples_agent-0_mean: 2.07
    apples_agent-0_min: 0
    apples_agent-1_max: 92
    apples_agent-1_mean: 30.14
    apples_agent-1_min: 0
    apples_agent-2_max: 81
    apples_agent-2_mean: 13.01
    apples_agent-2_min: 0
    apples_agent-3_max: 143
    apples_agent-3_mean: 57.91
    apples_agent-3_min: 31
    apples_agent-4_max: 54
    apples_agent-4_mean: 0.54
    apples_agent-4_min: 0
    apples_agent-5_max: 145
    apples_agent-5_mean: 101.12
    apples_agent-5_min: 11
    cleaning_beam_agent-0_max: 504
    cleaning_beam_agent-0_mean: 405.45
    cleaning_beam_agent-0_min: 263
    cleaning_beam_agent-1_max: 317
    cleaning_beam_agent-1_mean: 202.25
    cleaning_beam_agent-1_min: 90
    cleaning_beam_agent-2_max: 537
    cleaning_beam_agent-2_mean: 346.26
    cleaning_beam_agent-2_min: 149
    cleaning_beam_agent-3_max: 85
    cleaning_beam_agent-3_mean: 14.47
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 607
    cleaning_beam_agent-4_mean: 468.55
    cleaning_beam_agent-4_min: 352
    cleaning_beam_agent-5_max: 664
    cleaning_beam_agent-5_mean: 42.26
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-30-02
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1178.999999999992
  episode_reward_mean: 996.1799999999874
  episode_reward_min: 413.0000000000091
  episodes_this_iter: 96
  episodes_total: 29952
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20185.644
    learner:
      agent-0:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8886986970901489
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019744685851037502
        model: {}
        policy_loss: -0.0032896839547902346
        total_loss: -0.0027204358484596014
        vf_explained_var: 0.020961806178092957
        vf_loss: 21.33358383178711
      agent-1:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 1.14524245262146
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014215735718607903
        model: {}
        policy_loss: -0.004012748599052429
        total_loss: -0.0037910332903265953
        vf_explained_var: -0.02438455820083618
        vf_loss: 22.373416900634766
      agent-2:
        cur_kl_coeff: 5.777790062750448e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1383273601531982
        entropy_coeff: 0.0017600000137463212
        kl: 0.001171397976577282
        model: {}
        policy_loss: -0.00301102502271533
        total_loss: -0.002830532379448414
        vf_explained_var: -0.012560635805130005
        vf_loss: 21.83953094482422
      agent-3:
        cur_kl_coeff: 1.8807909893416293e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3582790791988373
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008523825090378523
        model: {}
        policy_loss: -0.0021449949126690626
        total_loss: -0.0008002105168998241
        vf_explained_var: 0.08596575260162354
        vf_loss: 19.753570556640625
      agent-4:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8648403286933899
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016054707812145352
        model: {}
        policy_loss: -0.003932759165763855
        total_loss: -0.003397054970264435
        vf_explained_var: 0.05023577809333801
        vf_loss: 20.578203201293945
      agent-5:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6582704782485962
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009042243473231792
        model: {}
        policy_loss: -0.003170217387378216
        total_loss: -0.002360178157687187
        vf_explained_var: 0.0885477066040039
        vf_loss: 19.685956954956055
    load_time_ms: 15059.442
    num_steps_sampled: 29952000
    num_steps_trained: 29952000
    sample_time_ms: 89749.879
    update_time_ms: 20.694
  iterations_since_restore: 292
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.845945945945946
    ram_util_percent: 9.622702702702703
  pid: 4061
  policy_reward_max:
    agent-0: 196.4999999999999
    agent-1: 196.4999999999999
    agent-2: 196.4999999999999
    agent-3: 196.4999999999999
    agent-4: 196.4999999999999
    agent-5: 196.4999999999999
  policy_reward_mean:
    agent-0: 166.02999999999986
    agent-1: 166.02999999999986
    agent-2: 166.02999999999986
    agent-3: 166.02999999999986
    agent-4: 166.02999999999986
    agent-5: 166.02999999999986
  policy_reward_min:
    agent-0: 68.83333333333313
    agent-1: 68.83333333333313
    agent-2: 68.83333333333313
    agent-3: 68.83333333333313
    agent-4: 68.83333333333313
    agent-5: 68.83333333333313
  sampler_perf:
    mean_env_wait_ms: 23.948706532559285
    mean_inference_ms: 12.249660281292604
    mean_processing_ms: 50.57925388808283
  time_since_restore: 37428.006769657135
  time_this_iter_s: 130.08150434494019
  time_total_s: 40639.070455789566
  timestamp: 1637055002
  timesteps_since_restore: 28032000
  timesteps_this_iter: 96000
  timesteps_total: 29952000
  training_iteration: 312
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    312 |          40639.1 | 29952000 |   996.18 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 48
    apples_agent-0_mean: 1.5
    apples_agent-0_min: 0
    apples_agent-1_max: 87
    apples_agent-1_mean: 31.97
    apples_agent-1_min: 0
    apples_agent-2_max: 148
    apples_agent-2_mean: 10.42
    apples_agent-2_min: 0
    apples_agent-3_max: 161
    apples_agent-3_mean: 57.97
    apples_agent-3_min: 27
    apples_agent-4_max: 66
    apples_agent-4_mean: 1.33
    apples_agent-4_min: 0
    apples_agent-5_max: 173
    apples_agent-5_mean: 103.96
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 499
    cleaning_beam_agent-0_mean: 411.8
    cleaning_beam_agent-0_min: 314
    cleaning_beam_agent-1_max: 354
    cleaning_beam_agent-1_mean: 189.22
    cleaning_beam_agent-1_min: 94
    cleaning_beam_agent-2_max: 566
    cleaning_beam_agent-2_mean: 358.59
    cleaning_beam_agent-2_min: 187
    cleaning_beam_agent-3_max: 48
    cleaning_beam_agent-3_mean: 12.81
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 590
    cleaning_beam_agent-4_mean: 450.99
    cleaning_beam_agent-4_min: 337
    cleaning_beam_agent-5_max: 585
    cleaning_beam_agent-5_mean: 50.14
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-32-07
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1149.0000000000055
  episode_reward_mean: 1002.1899999999863
  episode_reward_min: 514.0000000000139
  episodes_this_iter: 96
  episodes_total: 30048
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20165.781
    learner:
      agent-0:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9000769853591919
        entropy_coeff: 0.0017600000137463212
        kl: 0.0025046528317034245
        model: {}
        policy_loss: -0.0034094429574906826
        total_loss: -0.002976084128022194
        vf_explained_var: 0.007297873497009277
        vf_loss: 20.174936294555664
      agent-1:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 1.143014907836914
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013325733598321676
        model: {}
        policy_loss: -0.0038888021372258663
        total_loss: -0.003826441243290901
        vf_explained_var: -0.009528905153274536
        vf_loss: 20.740642547607422
      agent-2:
        cur_kl_coeff: 2.888895031375224e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1399976015090942
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020105657167732716
        model: {}
        policy_loss: -0.0032100663520395756
        total_loss: -0.003220418468117714
        vf_explained_var: 0.01098664104938507
        vf_loss: 19.96042823791504
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.33838096261024475
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007259527919813991
        model: {}
        policy_loss: -0.0016746618784964085
        total_loss: -0.0003657829947769642
        vf_explained_var: 0.06047552824020386
        vf_loss: 19.044292449951172
      agent-4:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8843235969543457
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019350270740687847
        model: {}
        policy_loss: -0.004106343258172274
        total_loss: -0.0036819912493228912
        vf_explained_var: 0.03125666081905365
        vf_loss: 19.807592391967773
      agent-5:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6352909207344055
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007864270592108369
        model: {}
        policy_loss: -0.0030175638385117054
        total_loss: -0.0022405977360904217
        vf_explained_var: 0.06316429376602173
        vf_loss: 18.95078468322754
    load_time_ms: 15042.701
    num_steps_sampled: 30048000
    num_steps_trained: 30048000
    sample_time_ms: 89812.677
    update_time_ms: 20.604
  iterations_since_restore: 293
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 16.18370786516854
    ram_util_percent: 9.750561797752809
  pid: 4061
  policy_reward_max:
    agent-0: 191.49999999999986
    agent-1: 191.49999999999986
    agent-2: 191.49999999999986
    agent-3: 191.49999999999986
    agent-4: 191.49999999999986
    agent-5: 191.49999999999986
  policy_reward_mean:
    agent-0: 167.0316666666665
    agent-1: 167.0316666666665
    agent-2: 167.0316666666665
    agent-3: 167.0316666666665
    agent-4: 167.0316666666665
    agent-5: 167.0316666666665
  policy_reward_min:
    agent-0: 85.66666666666674
    agent-1: 85.66666666666674
    agent-2: 85.66666666666674
    agent-3: 85.66666666666674
    agent-4: 85.66666666666674
    agent-5: 85.66666666666674
  sampler_perf:
    mean_env_wait_ms: 23.950977922269587
    mean_inference_ms: 12.249721556769252
    mean_processing_ms: 50.57850096947897
  time_since_restore: 37552.85473585129
  time_this_iter_s: 124.84796619415283
  time_total_s: 40763.91842198372
  timestamp: 1637055127
  timesteps_since_restore: 28128000
  timesteps_this_iter: 96000
  timesteps_total: 30048000
  training_iteration: 313
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    313 |          40763.9 | 30048000 |  1002.19 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 30
    apples_agent-0_mean: 0.95
    apples_agent-0_min: 0
    apples_agent-1_max: 90
    apples_agent-1_mean: 26.74
    apples_agent-1_min: 0
    apples_agent-2_max: 75
    apples_agent-2_mean: 9.32
    apples_agent-2_min: 0
    apples_agent-3_max: 116
    apples_agent-3_mean: 58.07
    apples_agent-3_min: 23
    apples_agent-4_max: 31
    apples_agent-4_mean: 0.58
    apples_agent-4_min: 0
    apples_agent-5_max: 160
    apples_agent-5_mean: 99.18
    apples_agent-5_min: 11
    cleaning_beam_agent-0_max: 510
    cleaning_beam_agent-0_mean: 426.38
    cleaning_beam_agent-0_min: 307
    cleaning_beam_agent-1_max: 371
    cleaning_beam_agent-1_mean: 212.02
    cleaning_beam_agent-1_min: 72
    cleaning_beam_agent-2_max: 515
    cleaning_beam_agent-2_mean: 350.26
    cleaning_beam_agent-2_min: 132
    cleaning_beam_agent-3_max: 63
    cleaning_beam_agent-3_mean: 12.19
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 564
    cleaning_beam_agent-4_mean: 456.04
    cleaning_beam_agent-4_min: 353
    cleaning_beam_agent-5_max: 472
    cleaning_beam_agent-5_mean: 48.7
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-34-10
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1167.999999999999
  episode_reward_mean: 1010.8199999999862
  episode_reward_min: 420.000000000004
  episodes_this_iter: 96
  episodes_total: 30144
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20162.014
    learner:
      agent-0:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8765576481819153
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021113192196935415
        model: {}
        policy_loss: -0.003257026895880699
        total_loss: -0.002788471756502986
        vf_explained_var: 0.023259758949279785
        vf_loss: 20.11300277709961
      agent-1:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 1.157828688621521
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020165611058473587
        model: {}
        policy_loss: -0.00392522057518363
        total_loss: -0.0038573117926716805
        vf_explained_var: -0.006554111838340759
        vf_loss: 21.056842803955078
      agent-2:
        cur_kl_coeff: 1.444447515687612e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1334657669067383
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014792653964832425
        model: {}
        policy_loss: -0.003264026017859578
        total_loss: -0.003299689618870616
        vf_explained_var: 0.050116509199142456
        vf_loss: 19.59236717224121
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3489261269569397
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009199343621730804
        model: {}
        policy_loss: -0.002110837958753109
        total_loss: -0.0008121617138385773
        vf_explained_var: 0.06744766235351562
        vf_loss: 19.12786102294922
      agent-4:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8748250007629395
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011129194172099233
        model: {}
        policy_loss: -0.0033711623400449753
        total_loss: -0.0029215351678431034
        vf_explained_var: 0.04103103280067444
        vf_loss: 19.893199920654297
      agent-5:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6287182569503784
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010649098549038172
        model: {}
        policy_loss: -0.0031152793671935797
        total_loss: -0.0023038992658257484
        vf_explained_var: 0.06378732621669769
        vf_loss: 19.179302215576172
    load_time_ms: 14939.754
    num_steps_sampled: 30144000
    num_steps_trained: 30144000
    sample_time_ms: 89766.391
    update_time_ms: 20.702
  iterations_since_restore: 294
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.30909090909091
    ram_util_percent: 9.595454545454546
  pid: 4061
  policy_reward_max:
    agent-0: 194.66666666666606
    agent-1: 194.66666666666606
    agent-2: 194.66666666666606
    agent-3: 194.66666666666606
    agent-4: 194.66666666666606
    agent-5: 194.66666666666606
  policy_reward_mean:
    agent-0: 168.46999999999983
    agent-1: 168.46999999999983
    agent-2: 168.46999999999983
    agent-3: 168.46999999999983
    agent-4: 168.46999999999983
    agent-5: 168.46999999999983
  policy_reward_min:
    agent-0: 69.99999999999993
    agent-1: 69.99999999999993
    agent-2: 69.99999999999993
    agent-3: 69.99999999999993
    agent-4: 69.99999999999993
    agent-5: 69.99999999999993
  sampler_perf:
    mean_env_wait_ms: 23.952913436148837
    mean_inference_ms: 12.249417533295416
    mean_processing_ms: 50.57677307661487
  time_since_restore: 37676.347789525986
  time_this_iter_s: 123.49305367469788
  time_total_s: 40887.41147565842
  timestamp: 1637055250
  timesteps_since_restore: 28224000
  timesteps_this_iter: 96000
  timesteps_total: 30144000
  training_iteration: 314
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    314 |          40887.4 | 30144000 |  1010.82 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 36
    apples_agent-0_mean: 1.81
    apples_agent-0_min: 0
    apples_agent-1_max: 85
    apples_agent-1_mean: 26.92
    apples_agent-1_min: 0
    apples_agent-2_max: 202
    apples_agent-2_mean: 17.16
    apples_agent-2_min: 0
    apples_agent-3_max: 148
    apples_agent-3_mean: 57.69
    apples_agent-3_min: 27
    apples_agent-4_max: 73
    apples_agent-4_mean: 2.42
    apples_agent-4_min: 0
    apples_agent-5_max: 215
    apples_agent-5_mean: 100.42
    apples_agent-5_min: 40
    cleaning_beam_agent-0_max: 530
    cleaning_beam_agent-0_mean: 424.63
    cleaning_beam_agent-0_min: 198
    cleaning_beam_agent-1_max: 335
    cleaning_beam_agent-1_mean: 212.29
    cleaning_beam_agent-1_min: 104
    cleaning_beam_agent-2_max: 473
    cleaning_beam_agent-2_mean: 325.86
    cleaning_beam_agent-2_min: 130
    cleaning_beam_agent-3_max: 71
    cleaning_beam_agent-3_mean: 14.25
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 612
    cleaning_beam_agent-4_mean: 454.76
    cleaning_beam_agent-4_min: 236
    cleaning_beam_agent-5_max: 316
    cleaning_beam_agent-5_mean: 41.47
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-36-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1176.000000000006
  episode_reward_mean: 972.669999999988
  episode_reward_min: 393.0000000000036
  episodes_this_iter: 96
  episodes_total: 30240
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20165.622
    learner:
      agent-0:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.88570237159729
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014056237414479256
        model: {}
        policy_loss: -0.0031461454927921295
        total_loss: -0.0025628446601331234
        vf_explained_var: 0.026180371642112732
        vf_loss: 21.421377182006836
      agent-1:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1561328172683716
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013901437632739544
        model: {}
        policy_loss: -0.0038066692650318146
        total_loss: -0.0036021433770656586
        vf_explained_var: -0.01844426989555359
        vf_loss: 22.393234252929688
      agent-2:
        cur_kl_coeff: 7.22223757843806e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 1.144681692123413
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013241153210401535
        model: {}
        policy_loss: -0.0031571064610034227
        total_loss: -0.003046989208087325
        vf_explained_var: 0.032875433564186096
        vf_loss: 21.247581481933594
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3816671371459961
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008243477204814553
        model: {}
        policy_loss: -0.0022323792800307274
        total_loss: -0.0009214701130986214
        vf_explained_var: 0.09816262125968933
        vf_loss: 19.826446533203125
      agent-4:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8808118104934692
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014534397050738335
        model: {}
        policy_loss: -0.003662710776552558
        total_loss: -0.0031807641498744488
        vf_explained_var: 0.07526727020740509
        vf_loss: 20.321725845336914
      agent-5:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.659911036491394
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015129853272810578
        model: {}
        policy_loss: -0.0032003987580537796
        total_loss: -0.0023499056696891785
        vf_explained_var: 0.08553674817085266
        vf_loss: 20.11933708190918
    load_time_ms: 15127.076
    num_steps_sampled: 30240000
    num_steps_trained: 30240000
    sample_time_ms: 89719.196
    update_time_ms: 20.823
  iterations_since_restore: 295
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.19888888888889
    ram_util_percent: 9.617222222222225
  pid: 4061
  policy_reward_max:
    agent-0: 196.0000000000001
    agent-1: 196.0000000000001
    agent-2: 196.0000000000001
    agent-3: 196.0000000000001
    agent-4: 196.0000000000001
    agent-5: 196.0000000000001
  policy_reward_mean:
    agent-0: 162.11166666666657
    agent-1: 162.11166666666657
    agent-2: 162.11166666666657
    agent-3: 162.11166666666657
    agent-4: 162.11166666666657
    agent-5: 162.11166666666657
  policy_reward_min:
    agent-0: 65.49999999999991
    agent-1: 65.49999999999991
    agent-2: 65.49999999999991
    agent-3: 65.49999999999991
    agent-4: 65.49999999999991
    agent-5: 65.49999999999991
  sampler_perf:
    mean_env_wait_ms: 23.954589488905707
    mean_inference_ms: 12.24910596417744
    mean_processing_ms: 50.575035594681296
  time_since_restore: 37802.263641119
  time_this_iter_s: 125.91585159301758
  time_total_s: 41013.327327251434
  timestamp: 1637055376
  timesteps_since_restore: 28320000
  timesteps_this_iter: 96000
  timesteps_total: 30240000
  training_iteration: 315
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    315 |          41013.3 | 30240000 |   972.67 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 0.91
    apples_agent-0_min: 0
    apples_agent-1_max: 90
    apples_agent-1_mean: 24.42
    apples_agent-1_min: 0
    apples_agent-2_max: 107
    apples_agent-2_mean: 13.2
    apples_agent-2_min: 0
    apples_agent-3_max: 113
    apples_agent-3_mean: 57.5
    apples_agent-3_min: 30
    apples_agent-4_max: 57
    apples_agent-4_mean: 0.76
    apples_agent-4_min: 0
    apples_agent-5_max: 187
    apples_agent-5_mean: 100.07
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 544
    cleaning_beam_agent-0_mean: 425.37
    cleaning_beam_agent-0_min: 286
    cleaning_beam_agent-1_max: 368
    cleaning_beam_agent-1_mean: 220.82
    cleaning_beam_agent-1_min: 105
    cleaning_beam_agent-2_max: 462
    cleaning_beam_agent-2_mean: 334.5
    cleaning_beam_agent-2_min: 127
    cleaning_beam_agent-3_max: 51
    cleaning_beam_agent-3_mean: 12.58
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 588
    cleaning_beam_agent-4_mean: 451.59
    cleaning_beam_agent-4_min: 269
    cleaning_beam_agent-5_max: 640
    cleaning_beam_agent-5_mean: 48.68
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-38-21
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1150.999999999997
  episode_reward_mean: 1007.3099999999863
  episode_reward_min: 542.0000000000027
  episodes_this_iter: 96
  episodes_total: 30336
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20154.209
    learner:
      agent-0:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8963680267333984
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012150800321251154
        model: {}
        policy_loss: -0.0029614223167300224
        total_loss: -0.0026019783690571785
        vf_explained_var: 0.020495668053627014
        vf_loss: 19.37051010131836
      agent-1:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1615241765975952
        entropy_coeff: 0.0017600000137463212
        kl: 0.00157016736920923
        model: {}
        policy_loss: -0.003989639226347208
        total_loss: -0.004047113936394453
        vf_explained_var: 0.0026384443044662476
        vf_loss: 19.868053436279297
      agent-2:
        cur_kl_coeff: 3.61111878921903e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1420917510986328
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012759388191625476
        model: {}
        policy_loss: -0.0027502162847667933
        total_loss: -0.002793441526591778
        vf_explained_var: 0.00902925431728363
        vf_loss: 19.668575286865234
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.34823286533355713
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007383277988992631
        model: {}
        policy_loss: -0.002163840923458338
        total_loss: -0.0009256608318537474
        vf_explained_var: 0.062257811427116394
        vf_loss: 18.510692596435547
      agent-4:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8864760994911194
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018339919624850154
        model: {}
        policy_loss: -0.004021031782031059
        total_loss: -0.0037463048938661814
        vf_explained_var: 0.08021032810211182
        vf_loss: 18.34930419921875
      agent-5:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6011074781417847
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009503338951617479
        model: {}
        policy_loss: -0.003213287331163883
        total_loss: -0.0023984545841813087
        vf_explained_var: 0.049001872539520264
        vf_loss: 18.727815628051758
    load_time_ms: 15318.892
    num_steps_sampled: 30336000
    num_steps_trained: 30336000
    sample_time_ms: 89762.985
    update_time_ms: 21.106
  iterations_since_restore: 296
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.172471910112359
    ram_util_percent: 9.676404494382023
  pid: 4061
  policy_reward_max:
    agent-0: 191.83333333333306
    agent-1: 191.83333333333306
    agent-2: 191.83333333333306
    agent-3: 191.83333333333306
    agent-4: 191.83333333333306
    agent-5: 191.83333333333306
  policy_reward_mean:
    agent-0: 167.88499999999982
    agent-1: 167.88499999999982
    agent-2: 167.88499999999982
    agent-3: 167.88499999999982
    agent-4: 167.88499999999982
    agent-5: 167.88499999999982
  policy_reward_min:
    agent-0: 90.33333333333354
    agent-1: 90.33333333333354
    agent-2: 90.33333333333354
    agent-3: 90.33333333333354
    agent-4: 90.33333333333354
    agent-5: 90.33333333333354
  sampler_perf:
    mean_env_wait_ms: 23.955972089396145
    mean_inference_ms: 12.24866394154085
    mean_processing_ms: 50.572705580239386
  time_since_restore: 37927.14054083824
  time_this_iter_s: 124.87689971923828
  time_total_s: 41138.20422697067
  timestamp: 1637055501
  timesteps_since_restore: 28416000
  timesteps_this_iter: 96000
  timesteps_total: 30336000
  training_iteration: 316
  trial_id: '00000'
  [2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes

== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    316 |          41138.2 | 30336000 |  1007.31 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 1.48
    apples_agent-0_min: 0
    apples_agent-1_max: 100
    apples_agent-1_mean: 29.8
    apples_agent-1_min: 0
    apples_agent-2_max: 82
    apples_agent-2_mean: 15.25
    apples_agent-2_min: 0
    apples_agent-3_max: 201
    apples_agent-3_mean: 56.07
    apples_agent-3_min: 21
    apples_agent-4_max: 57
    apples_agent-4_mean: 0.84
    apples_agent-4_min: 0
    apples_agent-5_max: 165
    apples_agent-5_mean: 99.37
    apples_agent-5_min: 36
    cleaning_beam_agent-0_max: 504
    cleaning_beam_agent-0_mean: 418.24
    cleaning_beam_agent-0_min: 319
    cleaning_beam_agent-1_max: 331
    cleaning_beam_agent-1_mean: 211.69
    cleaning_beam_agent-1_min: 101
    cleaning_beam_agent-2_max: 470
    cleaning_beam_agent-2_mean: 317.47
    cleaning_beam_agent-2_min: 161
    cleaning_beam_agent-3_max: 58
    cleaning_beam_agent-3_mean: 12.9
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 575
    cleaning_beam_agent-4_mean: 449.43
    cleaning_beam_agent-4_min: 321
    cleaning_beam_agent-5_max: 275
    cleaning_beam_agent-5_mean: 46.16
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-40-28
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1183.0000000000032
  episode_reward_mean: 989.6699999999847
  episode_reward_min: 462.0000000000027
  episodes_this_iter: 96
  episodes_total: 30432
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20140.328
    learner:
      agent-0:
        cur_kl_coeff: 6.018531165893214e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8910974860191345
        entropy_coeff: 0.0017600000137463212
        kl: 0.001395730534568429
        model: {}
        policy_loss: -0.0030670335981994867
        total_loss: -0.002421024488285184
        vf_explained_var: 0.03280961513519287
        vf_loss: 22.143415451049805
      agent-1:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1475179195404053
        entropy_coeff: 0.0017600000137463212
        kl: 0.000980409444309771
        model: {}
        policy_loss: -0.003641114104539156
        total_loss: -0.0033134412951767445
        vf_explained_var: -0.023064732551574707
        vf_loss: 23.473037719726562
      agent-2:
        cur_kl_coeff: 1.805559394609515e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1646860837936401
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010392043041065335
        model: {}
        policy_loss: -0.003109073033556342
        total_loss: -0.00297937192954123
        vf_explained_var: 0.04891633987426758
        vf_loss: 21.795438766479492
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36819377541542053
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007943392847664654
        model: {}
        policy_loss: -0.0024201474152505398
        total_loss: -0.001045913202688098
        vf_explained_var: 0.11676082015037537
        vf_loss: 20.222576141357422
      agent-4:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8922544717788696
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019633721094578505
        model: {}
        policy_loss: -0.003754772711545229
        total_loss: -0.0031365645118057728
        vf_explained_var: 0.043826475739479065
        vf_loss: 21.88574981689453
      agent-5:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6350657939910889
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008967575849965215
        model: {}
        policy_loss: -0.0031553530134260654
        total_loss: -0.002238245913758874
        vf_explained_var: 0.11030276119709015
        vf_loss: 20.34822654724121
    load_time_ms: 15479.323
    num_steps_sampled: 30432000
    num_steps_trained: 30432000
    sample_time_ms: 89765.02
    update_time_ms: 20.94
  iterations_since_restore: 297
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 16.028176795580112
    ram_util_percent: 9.688950276243096
  pid: 4061
  policy_reward_max:
    agent-0: 197.16666666666652
    agent-1: 197.16666666666652
    agent-2: 197.16666666666652
    agent-3: 197.16666666666652
    agent-4: 197.16666666666652
    agent-5: 197.16666666666652
  policy_reward_mean:
    agent-0: 164.9449999999998
    agent-1: 164.9449999999998
    agent-2: 164.9449999999998
    agent-3: 164.9449999999998
    agent-4: 164.9449999999998
    agent-5: 164.9449999999998
  policy_reward_min:
    agent-0: 77.00000000000009
    agent-1: 77.00000000000009
    agent-2: 77.00000000000009
    agent-3: 77.00000000000009
    agent-4: 77.00000000000009
    agent-5: 77.00000000000009
  sampler_perf:
    mean_env_wait_ms: 23.957457902693914
    mean_inference_ms: 12.248441787430895
    mean_processing_ms: 50.571980024356705
  time_since_restore: 38053.982929229736
  time_this_iter_s: 126.84238839149475
  time_total_s: 41265.04661536217
  timestamp: 1637055628
  timesteps_since_restore: 28512000
  timesteps_this_iter: 96000
  timesteps_total: 30432000
  training_iteration: 317
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    317 |            41265 | 30432000 |   989.67 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 21
    apples_agent-0_mean: 1.69
    apples_agent-0_min: 0
    apples_agent-1_max: 76
    apples_agent-1_mean: 26.36
    apples_agent-1_min: 0
    apples_agent-2_max: 134
    apples_agent-2_mean: 18.99
    apples_agent-2_min: 0
    apples_agent-3_max: 94
    apples_agent-3_mean: 56.78
    apples_agent-3_min: 32
    apples_agent-4_max: 54
    apples_agent-4_mean: 0.67
    apples_agent-4_min: 0
    apples_agent-5_max: 175
    apples_agent-5_mean: 96.4
    apples_agent-5_min: 1
    cleaning_beam_agent-0_max: 507
    cleaning_beam_agent-0_mean: 418.42
    cleaning_beam_agent-0_min: 314
    cleaning_beam_agent-1_max: 384
    cleaning_beam_agent-1_mean: 222.39
    cleaning_beam_agent-1_min: 97
    cleaning_beam_agent-2_max: 472
    cleaning_beam_agent-2_mean: 318.29
    cleaning_beam_agent-2_min: 146
    cleaning_beam_agent-3_max: 57
    cleaning_beam_agent-3_mean: 13.66
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 566
    cleaning_beam_agent-4_mean: 446.67
    cleaning_beam_agent-4_min: 294
    cleaning_beam_agent-5_max: 709
    cleaning_beam_agent-5_mean: 39.48
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-42-35
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1162.999999999992
  episode_reward_mean: 991.4299999999862
  episode_reward_min: 616.0000000000013
  episodes_this_iter: 96
  episodes_total: 30528
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20152.662
    learner:
      agent-0:
        cur_kl_coeff: 3.009265582946607e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8950495719909668
        entropy_coeff: 0.0017600000137463212
        kl: 0.001253176829777658
        model: {}
        policy_loss: -0.0028966902755200863
        total_loss: -0.002417266368865967
        vf_explained_var: 0.0545104444026947
        vf_loss: 20.54708480834961
      agent-1:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1574890613555908
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017426918493583798
        model: {}
        policy_loss: -0.004074877128005028
        total_loss: -0.0039064232259988785
        vf_explained_var: -0.015980154275894165
        vf_loss: 22.056344985961914
      agent-2:
        cur_kl_coeff: 9.027796973047575e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1522613763809204
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012715912889689207
        model: {}
        policy_loss: -0.003165711648762226
        total_loss: -0.0030867066234350204
        vf_explained_var: 0.03542330861091614
        vf_loss: 21.069833755493164
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3798208236694336
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013216561637818813
        model: {}
        policy_loss: -0.002351622097194195
        total_loss: -0.0010632449993863702
        vf_explained_var: 0.09855569899082184
        vf_loss: 19.56864356994629
      agent-4:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8955998420715332
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015892528463155031
        model: {}
        policy_loss: -0.0038884913083165884
        total_loss: -0.0033897843677550554
        vf_explained_var: 0.04412391781806946
        vf_loss: 20.749649047851562
      agent-5:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6207040548324585
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009916588896885514
        model: {}
        policy_loss: -0.003047631122171879
        total_loss: -0.002191815059632063
        vf_explained_var: 0.10253021121025085
        vf_loss: 19.48257827758789
    load_time_ms: 15744.718
    num_steps_sampled: 30528000
    num_steps_trained: 30528000
    sample_time_ms: 89817.417
    update_time_ms: 20.774
  iterations_since_restore: 298
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.100552486187844
    ram_util_percent: 9.632596685082875
  pid: 4061
  policy_reward_max:
    agent-0: 193.83333333333314
    agent-1: 193.83333333333314
    agent-2: 193.83333333333314
    agent-3: 193.83333333333314
    agent-4: 193.83333333333314
    agent-5: 193.83333333333314
  policy_reward_mean:
    agent-0: 165.2383333333332
    agent-1: 165.2383333333332
    agent-2: 165.2383333333332
    agent-3: 165.2383333333332
    agent-4: 165.2383333333332
    agent-5: 165.2383333333332
  policy_reward_min:
    agent-0: 102.66666666666698
    agent-1: 102.66666666666698
    agent-2: 102.66666666666698
    agent-3: 102.66666666666698
    agent-4: 102.66666666666698
    agent-5: 102.66666666666698
  sampler_perf:
    mean_env_wait_ms: 23.95885601629885
    mean_inference_ms: 12.248203119113484
    mean_processing_ms: 50.57080508610145
  time_since_restore: 38180.90490984917
  time_this_iter_s: 126.92198061943054
  time_total_s: 41391.9685959816
  timestamp: 1637055755
  timesteps_since_restore: 28608000
  timesteps_this_iter: 96000
  timesteps_total: 30528000
  training_iteration: 318
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    318 |            41392 | 30528000 |   991.43 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 31
    apples_agent-0_mean: 1.08
    apples_agent-0_min: 0
    apples_agent-1_max: 90
    apples_agent-1_mean: 24.8
    apples_agent-1_min: 0
    apples_agent-2_max: 130
    apples_agent-2_mean: 12.43
    apples_agent-2_min: 0
    apples_agent-3_max: 120
    apples_agent-3_mean: 59.42
    apples_agent-3_min: 33
    apples_agent-4_max: 23
    apples_agent-4_mean: 0.27
    apples_agent-4_min: 0
    apples_agent-5_max: 156
    apples_agent-5_mean: 102.04
    apples_agent-5_min: 40
    cleaning_beam_agent-0_max: 538
    cleaning_beam_agent-0_mean: 422.95
    cleaning_beam_agent-0_min: 303
    cleaning_beam_agent-1_max: 353
    cleaning_beam_agent-1_mean: 215.17
    cleaning_beam_agent-1_min: 81
    cleaning_beam_agent-2_max: 522
    cleaning_beam_agent-2_mean: 342.27
    cleaning_beam_agent-2_min: 187
    cleaning_beam_agent-3_max: 48
    cleaning_beam_agent-3_mean: 12.23
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 570
    cleaning_beam_agent-4_mean: 441.89
    cleaning_beam_agent-4_min: 340
    cleaning_beam_agent-5_max: 272
    cleaning_beam_agent-5_mean: 26.8
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-44-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1154.9999999999925
  episode_reward_mean: 1023.9399999999873
  episode_reward_min: 688.9999999999972
  episodes_this_iter: 96
  episodes_total: 30624
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20187.802
    learner:
      agent-0:
        cur_kl_coeff: 1.5046327914733034e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.895533561706543
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015805340372025967
        model: {}
        policy_loss: -0.0030139898881316185
        total_loss: -0.002614104188978672
        vf_explained_var: 0.026041820645332336
        vf_loss: 19.760292053222656
      agent-1:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 1.16293466091156
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018269092543050647
        model: {}
        policy_loss: -0.0039377701468765736
        total_loss: -0.003902755444869399
        vf_explained_var: -0.009983986616134644
        vf_loss: 20.817758560180664
      agent-2:
        cur_kl_coeff: 4.5138984865237875e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1493892669677734
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014206836931407452
        model: {}
        policy_loss: -0.0032694812398403883
        total_loss: -0.0032712717074900866
        vf_explained_var: 0.00890776515007019
        vf_loss: 20.21137237548828
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35372740030288696
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008297418244183064
        model: {}
        policy_loss: -0.0019583972170948982
        total_loss: -0.0006673360476270318
        vf_explained_var: 0.06649073958396912
        vf_loss: 19.136211395263672
      agent-4:
        cur_kl_coeff: 6.018531165893214e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8910638689994812
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017345742089673877
        model: {}
        policy_loss: -0.003878689603880048
        total_loss: -0.0034797023981809616
        vf_explained_var: 0.03905734419822693
        vf_loss: 19.672569274902344
      agent-5:
        cur_kl_coeff: 6.018531165893214e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5877814292907715
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005943421274423599
        model: {}
        policy_loss: -0.0027190041728317738
        total_loss: -0.0018573955167084932
        vf_explained_var: 0.060988277196884155
        vf_loss: 18.961071014404297
    load_time_ms: 15771.674
    num_steps_sampled: 30624000
    num_steps_trained: 30624000
    sample_time_ms: 89930.015
    update_time_ms: 20.706
  iterations_since_restore: 299
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.146111111111113
    ram_util_percent: 9.676111111111112
  pid: 4061
  policy_reward_max:
    agent-0: 192.49999999999991
    agent-1: 192.49999999999991
    agent-2: 192.49999999999991
    agent-3: 192.49999999999991
    agent-4: 192.49999999999991
    agent-5: 192.49999999999991
  policy_reward_mean:
    agent-0: 170.65666666666652
    agent-1: 170.65666666666652
    agent-2: 170.65666666666652
    agent-3: 170.65666666666652
    agent-4: 170.65666666666652
    agent-5: 170.65666666666652
  policy_reward_min:
    agent-0: 114.8333333333336
    agent-1: 114.8333333333336
    agent-2: 114.8333333333336
    agent-3: 114.8333333333336
    agent-4: 114.8333333333336
    agent-5: 114.8333333333336
  sampler_perf:
    mean_env_wait_ms: 23.960344025158598
    mean_inference_ms: 12.247908648440964
    mean_processing_ms: 50.56972394123164
  time_since_restore: 38307.27428340912
  time_this_iter_s: 126.36937355995178
  time_total_s: 41518.33796954155
  timestamp: 1637055882
  timesteps_since_restore: 28704000
  timesteps_this_iter: 96000
  timesteps_total: 30624000
  training_iteration: 319
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    319 |          41518.3 | 30624000 |  1023.94 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 34
    apples_agent-0_mean: 1.99
    apples_agent-0_min: 0
    apples_agent-1_max: 83
    apples_agent-1_mean: 22.68
    apples_agent-1_min: 0
    apples_agent-2_max: 165
    apples_agent-2_mean: 17.97
    apples_agent-2_min: 0
    apples_agent-3_max: 86
    apples_agent-3_mean: 54.02
    apples_agent-3_min: 23
    apples_agent-4_max: 65
    apples_agent-4_mean: 1.18
    apples_agent-4_min: 0
    apples_agent-5_max: 221
    apples_agent-5_mean: 99.49
    apples_agent-5_min: 32
    cleaning_beam_agent-0_max: 523
    cleaning_beam_agent-0_mean: 432.07
    cleaning_beam_agent-0_min: 315
    cleaning_beam_agent-1_max: 429
    cleaning_beam_agent-1_mean: 235.71
    cleaning_beam_agent-1_min: 114
    cleaning_beam_agent-2_max: 526
    cleaning_beam_agent-2_mean: 334.49
    cleaning_beam_agent-2_min: 151
    cleaning_beam_agent-3_max: 154
    cleaning_beam_agent-3_mean: 14.03
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 546
    cleaning_beam_agent-4_mean: 444.18
    cleaning_beam_agent-4_min: 263
    cleaning_beam_agent-5_max: 404
    cleaning_beam_agent-5_mean: 46.19
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-46-49
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1164.9999999999895
  episode_reward_mean: 1003.4999999999849
  episode_reward_min: 427.00000000000546
  episodes_this_iter: 96
  episodes_total: 30720
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20201.41
    learner:
      agent-0:
        cur_kl_coeff: 7.523163957366517e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8939142227172852
        entropy_coeff: 0.0017600000137463212
        kl: 0.001148260897025466
        model: {}
        policy_loss: -0.002872490556910634
        total_loss: -0.0022595273330807686
        vf_explained_var: 0.014937609434127808
        vf_loss: 21.862503051757812
      agent-1:
        cur_kl_coeff: 6.018531165893214e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 1.149025797843933
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024122565519064665
        model: {}
        policy_loss: -0.004195569083094597
        total_loss: -0.00397454435005784
        vf_explained_var: -0.007819384336471558
        vf_loss: 22.43309783935547
      agent-2:
        cur_kl_coeff: 2.2569492432618937e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1581403017044067
        entropy_coeff: 0.0017600000137463212
        kl: 0.002029594499617815
        model: {}
        policy_loss: -0.003398088039830327
        total_loss: -0.0032321028411388397
        vf_explained_var: 0.008779212832450867
        vf_loss: 22.043094635009766
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.361244261264801
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012189660919830203
        model: {}
        policy_loss: -0.0021060495637357235
        total_loss: -0.0007913804147392511
        vf_explained_var: 0.12024249136447906
        vf_loss: 19.504552841186523
      agent-4:
        cur_kl_coeff: 3.009265582946607e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8858934640884399
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018756975186988711
        model: {}
        policy_loss: -0.00391332758590579
        total_loss: -0.003304488491266966
        vf_explained_var: 0.02392686903476715
        vf_loss: 21.680150985717773
      agent-5:
        cur_kl_coeff: 3.009265582946607e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6085610389709473
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010213261703029275
        model: {}
        policy_loss: -0.0029903333634138107
        total_loss: -0.0020335374865680933
        vf_explained_var: 0.08668199181556702
        vf_loss: 20.27863121032715
    load_time_ms: 15991.175
    num_steps_sampled: 30720000
    num_steps_trained: 30720000
    sample_time_ms: 90016.434
    update_time_ms: 20.825
  iterations_since_restore: 300
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 18.871111111111112
    ram_util_percent: 10.938888888888894
  pid: 4061
  policy_reward_max:
    agent-0: 194.16666666666617
    agent-1: 194.16666666666617
    agent-2: 194.16666666666617
    agent-3: 194.16666666666617
    agent-4: 194.16666666666617
    agent-5: 194.16666666666617
  policy_reward_mean:
    agent-0: 167.24999999999983
    agent-1: 167.24999999999983
    agent-2: 167.24999999999983
    agent-3: 167.24999999999983
    agent-4: 167.24999999999983
    agent-5: 167.24999999999983
  policy_reward_min:
    agent-0: 71.16666666666683
    agent-1: 71.16666666666683
    agent-2: 71.16666666666683
    agent-3: 71.16666666666683
    agent-4: 71.16666666666683
    agent-5: 71.16666666666683
  sampler_perf:
    mean_env_wait_ms: 23.962693584038046
    mean_inference_ms: 12.247654090602428
    mean_processing_ms: 50.570053773031894
  time_since_restore: 38433.8084321022
  time_this_iter_s: 126.53414869308472
  time_total_s: 41644.872118234634
  timestamp: 1637056009
  timesteps_since_restore: 28800000
  timesteps_this_iter: 96000
  timesteps_total: 30720000
  training_iteration: 320
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    320 |          41644.9 | 30720000 |   1003.5 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 30
    apples_agent-0_mean: 0.79
    apples_agent-0_min: 0
    apples_agent-1_max: 113
    apples_agent-1_mean: 28.74
    apples_agent-1_min: 0
    apples_agent-2_max: 117
    apples_agent-2_mean: 11.58
    apples_agent-2_min: 0
    apples_agent-3_max: 136
    apples_agent-3_mean: 59.76
    apples_agent-3_min: 30
    apples_agent-4_max: 110
    apples_agent-4_mean: 2.72
    apples_agent-4_min: 0
    apples_agent-5_max: 235
    apples_agent-5_mean: 99.23
    apples_agent-5_min: 6
    cleaning_beam_agent-0_max: 565
    cleaning_beam_agent-0_mean: 446.67
    cleaning_beam_agent-0_min: 339
    cleaning_beam_agent-1_max: 403
    cleaning_beam_agent-1_mean: 228.86
    cleaning_beam_agent-1_min: 122
    cleaning_beam_agent-2_max: 510
    cleaning_beam_agent-2_mean: 335.81
    cleaning_beam_agent-2_min: 151
    cleaning_beam_agent-3_max: 49
    cleaning_beam_agent-3_mean: 12.2
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 546
    cleaning_beam_agent-4_mean: 443.17
    cleaning_beam_agent-4_min: 261
    cleaning_beam_agent-5_max: 484
    cleaning_beam_agent-5_mean: 47.48
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-49-28
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1160.9999999999995
  episode_reward_mean: 998.1699999999877
  episode_reward_min: 312.0000000000003
  episodes_this_iter: 96
  episodes_total: 30816
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20205.955
    learner:
      agent-0:
        cur_kl_coeff: 3.7615819786832586e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8798191547393799
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014432687312364578
        model: {}
        policy_loss: -0.0031227683648467064
        total_loss: -0.002483286429196596
        vf_explained_var: 0.01459340751171112
        vf_loss: 21.879634857177734
      agent-1:
        cur_kl_coeff: 3.009265582946607e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 1.166121006011963
        entropy_coeff: 0.0017600000137463212
        kl: 0.001679857261478901
        model: {}
        policy_loss: -0.0038908785209059715
        total_loss: -0.0036931228823959827
        vf_explained_var: -0.009522229433059692
        vf_loss: 22.501296997070312
      agent-2:
        cur_kl_coeff: 1.1284746216309469e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 1.139369010925293
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011261386098340154
        model: {}
        policy_loss: -0.0030653905123472214
        total_loss: -0.00287040276452899
        vf_explained_var: 0.009660288691520691
        vf_loss: 22.00278091430664
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36268579959869385
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008602335583418608
        model: {}
        policy_loss: -0.0021158852614462376
        total_loss: -0.0007988382130861282
        vf_explained_var: 0.11968997120857239
        vf_loss: 19.553756713867188
      agent-4:
        cur_kl_coeff: 1.5046327914733034e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8965457677841187
        entropy_coeff: 0.0017600000137463212
        kl: 0.00131850759498775
        model: {}
        policy_loss: -0.003687390359118581
        total_loss: -0.003264101454988122
        vf_explained_var: 0.10434982180595398
        vf_loss: 20.01211166381836
      agent-5:
        cur_kl_coeff: 1.5046327914733034e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6357014775276184
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007558679790236056
        model: {}
        policy_loss: -0.002875742968171835
        total_loss: -0.001992016565054655
        vf_explained_var: 0.09998258948326111
        vf_loss: 20.025630950927734
    load_time_ms: 18195.192
    num_steps_sampled: 30816000
    num_steps_trained: 30816000
    sample_time_ms: 90902.28
    update_time_ms: 20.701
  iterations_since_restore: 301
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.476651982378858
    ram_util_percent: 17.259030837004406
  pid: 4061
  policy_reward_max:
    agent-0: 193.4999999999998
    agent-1: 193.4999999999998
    agent-2: 193.4999999999998
    agent-3: 193.4999999999998
    agent-4: 193.4999999999998
    agent-5: 193.4999999999998
  policy_reward_mean:
    agent-0: 166.36166666666654
    agent-1: 166.36166666666654
    agent-2: 166.36166666666654
    agent-3: 166.36166666666654
    agent-4: 166.36166666666654
    agent-5: 166.36166666666654
  policy_reward_min:
    agent-0: 51.99999999999982
    agent-1: 51.99999999999982
    agent-2: 51.99999999999982
    agent-3: 51.99999999999982
    agent-4: 51.99999999999982
    agent-5: 51.99999999999982
  sampler_perf:
    mean_env_wait_ms: 23.970218307292363
    mean_inference_ms: 12.250190350540162
    mean_processing_ms: 50.580273398808515
  time_since_restore: 38592.12750792503
  time_this_iter_s: 158.3190758228302
  time_total_s: 41803.191194057465
  timestamp: 1637056168
  timesteps_since_restore: 28896000
  timesteps_this_iter: 96000
  timesteps_total: 30816000
  training_iteration: 321
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 31.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    321 |          41803.2 | 30816000 |   998.17 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 38
    apples_agent-0_mean: 1.32
    apples_agent-0_min: 0
    apples_agent-1_max: 138
    apples_agent-1_mean: 25.91
    apples_agent-1_min: 0
    apples_agent-2_max: 115
    apples_agent-2_mean: 16.1
    apples_agent-2_min: 0
    apples_agent-3_max: 116
    apples_agent-3_mean: 60.18
    apples_agent-3_min: 22
    apples_agent-4_max: 73
    apples_agent-4_mean: 1.69
    apples_agent-4_min: 0
    apples_agent-5_max: 199
    apples_agent-5_mean: 97.54
    apples_agent-5_min: 50
    cleaning_beam_agent-0_max: 541
    cleaning_beam_agent-0_mean: 447.67
    cleaning_beam_agent-0_min: 347
    cleaning_beam_agent-1_max: 387
    cleaning_beam_agent-1_mean: 238.58
    cleaning_beam_agent-1_min: 114
    cleaning_beam_agent-2_max: 513
    cleaning_beam_agent-2_mean: 318.41
    cleaning_beam_agent-2_min: 116
    cleaning_beam_agent-3_max: 59
    cleaning_beam_agent-3_mean: 12.24
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 586
    cleaning_beam_agent-4_mean: 452.77
    cleaning_beam_agent-4_min: 315
    cleaning_beam_agent-5_max: 221
    cleaning_beam_agent-5_mean: 38.12
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-52-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1190.000000000002
  episode_reward_mean: 1003.4199999999867
  episode_reward_min: 482.00000000000614
  episodes_this_iter: 96
  episodes_total: 30912
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20217.098
    learner:
      agent-0:
        cur_kl_coeff: 1.8807909893416293e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8800133466720581
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013726352481171489
        model: {}
        policy_loss: -0.0029300865717232227
        total_loss: -0.0023638925049453974
        vf_explained_var: 0.033424049615859985
        vf_loss: 21.15019989013672
      agent-1:
        cur_kl_coeff: 1.5046327914733034e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1670563220977783
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019186974968761206
        model: {}
        policy_loss: -0.00397268682718277
        total_loss: -0.0038067728746682405
        vf_explained_var: -0.01076275110244751
        vf_loss: 22.199344635009766
      agent-2:
        cur_kl_coeff: 5.642373108154734e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1587698459625244
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012831782223656774
        model: {}
        policy_loss: -0.003297087736427784
        total_loss: -0.0031694024801254272
        vf_explained_var: 0.017346039414405823
        vf_loss: 21.671215057373047
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37086403369903564
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012834729859605432
        model: {}
        policy_loss: -0.002359837293624878
        total_loss: -0.000986201222985983
        vf_explained_var: 0.07484148442745209
        vf_loss: 20.26360321044922
      agent-4:
        cur_kl_coeff: 7.523163957366517e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8910397291183472
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018281356897205114
        model: {}
        policy_loss: -0.0036750133149325848
        total_loss: -0.003229547291994095
        vf_explained_var: 0.08552312850952148
        vf_loss: 20.136964797973633
      agent-5:
        cur_kl_coeff: 7.523163957366517e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6096318960189819
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009232648299075663
        model: {}
        policy_loss: -0.002780342474579811
        total_loss: -0.0018285205587744713
        vf_explained_var: 0.07581914961338043
        vf_loss: 20.24772071838379
    load_time_ms: 20429.083
    num_steps_sampled: 30912000
    num_steps_trained: 30912000
    sample_time_ms: 92013.064
    update_time_ms: 20.498
  iterations_since_restore: 302
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.008154506437766
    ram_util_percent: 17.33090128755365
  pid: 4061
  policy_reward_max:
    agent-0: 198.33333333333312
    agent-1: 198.33333333333312
    agent-2: 198.33333333333312
    agent-3: 198.33333333333312
    agent-4: 198.33333333333312
    agent-5: 198.33333333333312
  policy_reward_mean:
    agent-0: 167.23666666666654
    agent-1: 167.23666666666654
    agent-2: 167.23666666666654
    agent-3: 167.23666666666654
    agent-4: 167.23666666666654
    agent-5: 167.23666666666654
  policy_reward_min:
    agent-0: 80.33333333333334
    agent-1: 80.33333333333334
    agent-2: 80.33333333333334
    agent-3: 80.33333333333334
    agent-4: 80.33333333333334
    agent-5: 80.33333333333334
  sampler_perf:
    mean_env_wait_ms: 23.97847913509203
    mean_inference_ms: 12.252827800602615
    mean_processing_ms: 50.59167076057667
  time_since_restore: 38755.78606438637
  time_this_iter_s: 163.65855646133423
  time_total_s: 41966.8497505188
  timestamp: 1637056332
  timesteps_since_restore: 28992000
  timesteps_this_iter: 96000
  timesteps_total: 30912000
  training_iteration: 322
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 31.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    322 |          41966.8 | 30912000 |  1003.42 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 18
    apples_agent-0_mean: 0.79
    apples_agent-0_min: 0
    apples_agent-1_max: 91
    apples_agent-1_mean: 25.24
    apples_agent-1_min: 0
    apples_agent-2_max: 95
    apples_agent-2_mean: 16.54
    apples_agent-2_min: 0
    apples_agent-3_max: 122
    apples_agent-3_mean: 59.35
    apples_agent-3_min: 25
    apples_agent-4_max: 74
    apples_agent-4_mean: 0.96
    apples_agent-4_min: 0
    apples_agent-5_max: 137
    apples_agent-5_mean: 94.58
    apples_agent-5_min: 24
    cleaning_beam_agent-0_max: 532
    cleaning_beam_agent-0_mean: 449.33
    cleaning_beam_agent-0_min: 337
    cleaning_beam_agent-1_max: 434
    cleaning_beam_agent-1_mean: 230.33
    cleaning_beam_agent-1_min: 46
    cleaning_beam_agent-2_max: 508
    cleaning_beam_agent-2_mean: 340.4
    cleaning_beam_agent-2_min: 190
    cleaning_beam_agent-3_max: 45
    cleaning_beam_agent-3_mean: 13.21
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 559
    cleaning_beam_agent-4_mean: 442.98
    cleaning_beam_agent-4_min: 281
    cleaning_beam_agent-5_max: 541
    cleaning_beam_agent-5_mean: 52.04
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-54-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1177.00000000002
  episode_reward_mean: 1014.6899999999887
  episode_reward_min: 453.0000000000014
  episodes_this_iter: 96
  episodes_total: 31008
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20269.855
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8729667663574219
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017174434615299106
        model: {}
        policy_loss: -0.0033029927872121334
        total_loss: -0.002784071024507284
        vf_explained_var: 0.03909316658973694
        vf_loss: 20.553422927856445
      agent-1:
        cur_kl_coeff: 7.523163957366517e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1608020067214966
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013489990960806608
        model: {}
        policy_loss: -0.0036434996873140335
        total_loss: -0.003464409848675132
        vf_explained_var: -0.03398612141609192
        vf_loss: 22.22101402282715
      agent-2:
        cur_kl_coeff: 2.821186554077367e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1415520906448364
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011653716210275888
        model: {}
        policy_loss: -0.0028430884703993797
        total_loss: -0.0028632269240915775
        vf_explained_var: 0.07249405980110168
        vf_loss: 19.88992691040039
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3557279706001282
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007814553682692349
        model: {}
        policy_loss: -0.0023189964704215527
        total_loss: -0.0010049823904410005
        vf_explained_var: 0.0921967625617981
        vf_loss: 19.40094566345215
      agent-4:
        cur_kl_coeff: 3.7615819786832586e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8922647833824158
        entropy_coeff: 0.0017600000137463212
        kl: 0.002131631365045905
        model: {}
        policy_loss: -0.0036652935668826103
        total_loss: -0.003243417479097843
        vf_explained_var: 0.0776500254869461
        vf_loss: 19.922582626342773
      agent-5:
        cur_kl_coeff: 3.7615819786832586e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5994938611984253
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007150254677981138
        model: {}
        policy_loss: -0.0029163130093365908
        total_loss: -0.001969708828255534
        vf_explained_var: 0.06585873663425446
        vf_loss: 20.017139434814453
    load_time_ms: 21901.832
    num_steps_sampled: 31008000
    num_steps_trained: 31008000
    sample_time_ms: 93111.267
    update_time_ms: 21.283
  iterations_since_restore: 303
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.98240740740741
    ram_util_percent: 17.352777777777774
  pid: 4061
  policy_reward_max:
    agent-0: 196.16666666666615
    agent-1: 196.16666666666615
    agent-2: 196.16666666666615
    agent-3: 196.16666666666615
    agent-4: 196.16666666666615
    agent-5: 196.16666666666615
  policy_reward_mean:
    agent-0: 169.1149999999999
    agent-1: 169.1149999999999
    agent-2: 169.1149999999999
    agent-3: 169.1149999999999
    agent-4: 169.1149999999999
    agent-5: 169.1149999999999
  policy_reward_min:
    agent-0: 75.50000000000003
    agent-1: 75.50000000000003
    agent-2: 75.50000000000003
    agent-3: 75.50000000000003
    agent-4: 75.50000000000003
    agent-5: 75.50000000000003
  sampler_perf:
    mean_env_wait_ms: 23.987243571524058
    mean_inference_ms: 12.255568230532065
    mean_processing_ms: 50.604508259652675
  time_since_restore: 38906.909901857376
  time_this_iter_s: 151.1238374710083
  time_total_s: 42117.97358798981
  timestamp: 1637056483
  timesteps_since_restore: 29088000
  timesteps_this_iter: 96000
  timesteps_total: 31008000
  training_iteration: 323
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 31.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    323 |            42118 | 31008000 |  1014.69 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 0.65
    apples_agent-0_min: 0
    apples_agent-1_max: 82
    apples_agent-1_mean: 23.72
    apples_agent-1_min: 0
    apples_agent-2_max: 249
    apples_agent-2_mean: 16.92
    apples_agent-2_min: 0
    apples_agent-3_max: 101
    apples_agent-3_mean: 56.55
    apples_agent-3_min: 30
    apples_agent-4_max: 52
    apples_agent-4_mean: 0.77
    apples_agent-4_min: 0
    apples_agent-5_max: 200
    apples_agent-5_mean: 98.31
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 571
    cleaning_beam_agent-0_mean: 461.4
    cleaning_beam_agent-0_min: 319
    cleaning_beam_agent-1_max: 369
    cleaning_beam_agent-1_mean: 221.41
    cleaning_beam_agent-1_min: 94
    cleaning_beam_agent-2_max: 592
    cleaning_beam_agent-2_mean: 345.03
    cleaning_beam_agent-2_min: 147
    cleaning_beam_agent-3_max: 52
    cleaning_beam_agent-3_mean: 11.58
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 569
    cleaning_beam_agent-4_mean: 438.88
    cleaning_beam_agent-4_min: 324
    cleaning_beam_agent-5_max: 529
    cleaning_beam_agent-5_mean: 45.74
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-57-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1173.9999999999934
  episode_reward_mean: 1014.5599999999873
  episode_reward_min: 605.0000000000074
  episodes_this_iter: 96
  episodes_total: 31104
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20320.544
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8618732690811157
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022923913784325123
        model: {}
        policy_loss: -0.003273146692663431
        total_loss: -0.002696987707167864
        vf_explained_var: -0.015630751848220825
        vf_loss: 20.9305477142334
      agent-1:
        cur_kl_coeff: 3.7615819786832586e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1728521585464478
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015095911221578717
        model: {}
        policy_loss: -0.0038168849423527718
        total_loss: -0.003812531940639019
        vf_explained_var: 0.0005066990852355957
        vf_loss: 20.685754776000977
      agent-2:
        cur_kl_coeff: 1.4105932770386836e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 1.138838291168213
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012601213529706001
        model: {}
        policy_loss: -0.003185632638633251
        total_loss: -0.0032076523639261723
        vf_explained_var: 0.042255014181137085
        vf_loss: 19.823394775390625
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.34720954298973083
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013125053374096751
        model: {}
        policy_loss: -0.0023671411909163
        total_loss: -0.0010802606120705605
        vf_explained_var: 0.0793452113866806
        vf_loss: 18.979686737060547
      agent-4:
        cur_kl_coeff: 1.8807909893416293e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8898044228553772
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015015719691291451
        model: {}
        policy_loss: -0.0035347058437764645
        total_loss: -0.0031074571888893843
        vf_explained_var: 0.039984241127967834
        vf_loss: 19.933046340942383
      agent-5:
        cur_kl_coeff: 1.8807909893416293e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5856218338012695
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015018293634057045
        model: {}
        policy_loss: -0.003039610106498003
        total_loss: -0.002166944555938244
        vf_explained_var: 0.07468681037425995
        vf_loss: 19.033599853515625
    load_time_ms: 24697.735
    num_steps_sampled: 31104000
    num_steps_trained: 31104000
    sample_time_ms: 94053.585
    update_time_ms: 21.265
  iterations_since_restore: 304
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.33695652173913
    ram_util_percent: 17.36695652173913
  pid: 4061
  policy_reward_max:
    agent-0: 195.66666666666595
    agent-1: 195.66666666666595
    agent-2: 195.66666666666595
    agent-3: 195.66666666666595
    agent-4: 195.66666666666595
    agent-5: 195.66666666666595
  policy_reward_mean:
    agent-0: 169.09333333333322
    agent-1: 169.09333333333322
    agent-2: 169.09333333333322
    agent-3: 169.09333333333322
    agent-4: 169.09333333333322
    agent-5: 169.09333333333322
  policy_reward_min:
    agent-0: 100.8333333333339
    agent-1: 100.8333333333339
    agent-2: 100.8333333333339
    agent-3: 100.8333333333339
    agent-4: 100.8333333333339
    agent-5: 100.8333333333339
  sampler_perf:
    mean_env_wait_ms: 23.994709642880977
    mean_inference_ms: 12.257980468468258
    mean_processing_ms: 50.615414286580624
  time_since_restore: 39068.363792181015
  time_this_iter_s: 161.45389032363892
  time_total_s: 42279.427478313446
  timestamp: 1637056644
  timesteps_since_restore: 29184000
  timesteps_this_iter: 96000
  timesteps_total: 31104000
  training_iteration: 324
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 31.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    324 |          42279.4 | 31104000 |  1014.56 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 40
    apples_agent-0_mean: 1.18
    apples_agent-0_min: 0
    apples_agent-1_max: 105
    apples_agent-1_mean: 21.32
    apples_agent-1_min: 0
    apples_agent-2_max: 83
    apples_agent-2_mean: 12.24
    apples_agent-2_min: 0
    apples_agent-3_max: 90
    apples_agent-3_mean: 55.18
    apples_agent-3_min: 33
    apples_agent-4_max: 69
    apples_agent-4_mean: 1.77
    apples_agent-4_min: 0
    apples_agent-5_max: 206
    apples_agent-5_mean: 97.72
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 580
    cleaning_beam_agent-0_mean: 467.34
    cleaning_beam_agent-0_min: 335
    cleaning_beam_agent-1_max: 402
    cleaning_beam_agent-1_mean: 229.19
    cleaning_beam_agent-1_min: 103
    cleaning_beam_agent-2_max: 560
    cleaning_beam_agent-2_mean: 348.64
    cleaning_beam_agent-2_min: 181
    cleaning_beam_agent-3_max: 46
    cleaning_beam_agent-3_mean: 12.51
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 524
    cleaning_beam_agent-4_mean: 424.67
    cleaning_beam_agent-4_min: 261
    cleaning_beam_agent-5_max: 818
    cleaning_beam_agent-5_mean: 57.96
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-59-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1148.9999999999993
  episode_reward_mean: 1002.7799999999852
  episode_reward_min: 514.0000000000134
  episodes_this_iter: 96
  episodes_total: 31200
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20337.033
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8681992292404175
        entropy_coeff: 0.0017600000137463212
        kl: 0.001700322492979467
        model: {}
        policy_loss: -0.0031940953340381384
        total_loss: -0.0026352261193096638
        vf_explained_var: 0.01684868335723877
        vf_loss: 20.868959426879883
      agent-1:
        cur_kl_coeff: 1.8807909893416293e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1519275903701782
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013344439212232828
        model: {}
        policy_loss: -0.0036626080982387066
        total_loss: -0.0035152710042893887
        vf_explained_var: -0.024251431226730347
        vf_loss: 21.747283935546875
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1445133686065674
        entropy_coeff: 0.0017600000137463212
        kl: 0.001816937467083335
        model: {}
        policy_loss: -0.0034154464956372976
        total_loss: -0.0033309808932244778
        vf_explained_var: 0.011998817324638367
        vf_loss: 20.988054275512695
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3465324938297272
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009501096792519093
        model: {}
        policy_loss: -0.0020142849534749985
        total_loss: -0.0006543085910379887
        vf_explained_var: 0.0733424574136734
        vf_loss: 19.698741912841797
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9016489386558533
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016471429262310266
        model: {}
        policy_loss: -0.003957660868763924
        total_loss: -0.003499820129945874
        vf_explained_var: 0.0418575257062912
        vf_loss: 20.44743537902832
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6180514097213745
        entropy_coeff: 0.0017600000137463212
        kl: 0.001365605858154595
        model: {}
        policy_loss: -0.003147735958918929
        total_loss: -0.0022640610113739967
        vf_explained_var: 0.07267604768276215
        vf_loss: 19.714462280273438
    load_time_ms: 26279.716
    num_steps_sampled: 31200000
    num_steps_trained: 31200000
    sample_time_ms: 95236.589
    update_time_ms: 21.148
  iterations_since_restore: 305
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.902283105022832
    ram_util_percent: 15.693150684931508
  pid: 4061
  policy_reward_max:
    agent-0: 191.4999999999999
    agent-1: 191.4999999999999
    agent-2: 191.4999999999999
    agent-3: 191.4999999999999
    agent-4: 191.4999999999999
    agent-5: 191.4999999999999
  policy_reward_mean:
    agent-0: 167.12999999999977
    agent-1: 167.12999999999977
    agent-2: 167.12999999999977
    agent-3: 167.12999999999977
    agent-4: 167.12999999999977
    agent-5: 167.12999999999977
  policy_reward_min:
    agent-0: 85.66666666666683
    agent-1: 85.66666666666683
    agent-2: 85.66666666666683
    agent-3: 85.66666666666683
    agent-4: 85.66666666666683
    agent-5: 85.66666666666683
  sampler_perf:
    mean_env_wait_ms: 24.00292138494357
    mean_inference_ms: 12.260549011981187
    mean_processing_ms: 50.62664904702488
  time_since_restore: 39222.123759031296
  time_this_iter_s: 153.75996685028076
  time_total_s: 42433.18744516373
  timestamp: 1637056798
  timesteps_since_restore: 29280000
  timesteps_this_iter: 96000
  timesteps_total: 31200000
  training_iteration: 325
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    325 |          42433.2 | 31200000 |  1002.78 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 38
    apples_agent-0_mean: 1.01
    apples_agent-0_min: 0
    apples_agent-1_max: 78
    apples_agent-1_mean: 21.21
    apples_agent-1_min: 0
    apples_agent-2_max: 114
    apples_agent-2_mean: 12.69
    apples_agent-2_min: 0
    apples_agent-3_max: 136
    apples_agent-3_mean: 55.32
    apples_agent-3_min: 27
    apples_agent-4_max: 74
    apples_agent-4_mean: 1.89
    apples_agent-4_min: 0
    apples_agent-5_max: 271
    apples_agent-5_mean: 96.7
    apples_agent-5_min: 36
    cleaning_beam_agent-0_max: 539
    cleaning_beam_agent-0_mean: 461.82
    cleaning_beam_agent-0_min: 296
    cleaning_beam_agent-1_max: 390
    cleaning_beam_agent-1_mean: 235.04
    cleaning_beam_agent-1_min: 117
    cleaning_beam_agent-2_max: 549
    cleaning_beam_agent-2_mean: 344.11
    cleaning_beam_agent-2_min: 170
    cleaning_beam_agent-3_max: 119
    cleaning_beam_agent-3_mean: 14.97
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 523
    cleaning_beam_agent-4_mean: 431.26
    cleaning_beam_agent-4_min: 322
    cleaning_beam_agent-5_max: 309
    cleaning_beam_agent-5_mean: 40.41
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-02-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1140.0
  episode_reward_mean: 1009.8199999999883
  episode_reward_min: 470.0000000000123
  episodes_this_iter: 96
  episodes_total: 31296
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20382.788
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.870337724685669
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014120852574706078
        model: {}
        policy_loss: -0.0030829820316284895
        total_loss: -0.002386742737144232
        vf_explained_var: 0.0073221176862716675
        vf_loss: 22.28034210205078
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1608192920684814
        entropy_coeff: 0.0017600000137463212
        kl: 0.001738972496241331
        model: {}
        policy_loss: -0.0037418268620967865
        total_loss: -0.0035265637561678886
        vf_explained_var: -0.0025477558374404907
        vf_loss: 22.58306312561035
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1502050161361694
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013183332048356533
        model: {}
        policy_loss: -0.003258305136114359
        total_loss: -0.003028491511940956
        vf_explained_var: -0.004441499710083008
        vf_loss: 22.541744232177734
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3660513758659363
        entropy_coeff: 0.0017600000137463212
        kl: 0.001111553399823606
        model: {}
        policy_loss: -0.0020957179367542267
        total_loss: -0.0007166336290538311
        vf_explained_var: 0.09968830645084381
        vf_loss: 20.233327865600586
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8928359746932983
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018739334773272276
        model: {}
        policy_loss: -0.004006015602499247
        total_loss: -0.0034663164988160133
        vf_explained_var: 0.06766623258590698
        vf_loss: 21.11090850830078
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.612432062625885
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009539831080473959
        model: {}
        policy_loss: -0.00317589845508337
        total_loss: -0.002247273689135909
        vf_explained_var: 0.10309760272502899
        vf_loss: 20.06509017944336
    load_time_ms: 26302.381
    num_steps_sampled: 31296000
    num_steps_trained: 31296000
    sample_time_ms: 96009.179
    update_time_ms: 20.63
  iterations_since_restore: 306
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.721052631578946
    ram_util_percent: 14.3321052631579
  pid: 4061
  policy_reward_max:
    agent-0: 189.9999999999999
    agent-1: 189.9999999999999
    agent-2: 189.9999999999999
    agent-3: 189.9999999999999
    agent-4: 189.9999999999999
    agent-5: 189.9999999999999
  policy_reward_mean:
    agent-0: 168.30333333333314
    agent-1: 168.30333333333314
    agent-2: 168.30333333333314
    agent-3: 168.30333333333314
    agent-4: 168.30333333333314
    agent-5: 168.30333333333314
  policy_reward_min:
    agent-0: 78.33333333333327
    agent-1: 78.33333333333327
    agent-2: 78.33333333333327
    agent-3: 78.33333333333327
    agent-4: 78.33333333333327
    agent-5: 78.33333333333327
  sampler_perf:
    mean_env_wait_ms: 24.009881218323844
    mean_inference_ms: 12.262447707401153
    mean_processing_ms: 50.63574182519448
  time_since_restore: 39355.37567615509
  time_this_iter_s: 133.25191712379456
  time_total_s: 42566.43936228752
  timestamp: 1637056932
  timesteps_since_restore: 29376000
  timesteps_this_iter: 96000
  timesteps_total: 31296000
  training_iteration: 326
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 23.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    326 |          42566.4 | 31296000 |  1009.82 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 1.01
    apples_agent-0_min: 0
    apples_agent-1_max: 107
    apples_agent-1_mean: 22.87
    apples_agent-1_min: 0
    apples_agent-2_max: 102
    apples_agent-2_mean: 14.24
    apples_agent-2_min: 0
    apples_agent-3_max: 82
    apples_agent-3_mean: 52.94
    apples_agent-3_min: 29
    apples_agent-4_max: 53
    apples_agent-4_mean: 1.83
    apples_agent-4_min: 0
    apples_agent-5_max: 162
    apples_agent-5_mean: 98.03
    apples_agent-5_min: 51
    cleaning_beam_agent-0_max: 598
    cleaning_beam_agent-0_mean: 468.32
    cleaning_beam_agent-0_min: 356
    cleaning_beam_agent-1_max: 359
    cleaning_beam_agent-1_mean: 228.98
    cleaning_beam_agent-1_min: 90
    cleaning_beam_agent-2_max: 529
    cleaning_beam_agent-2_mean: 356.33
    cleaning_beam_agent-2_min: 159
    cleaning_beam_agent-3_max: 48
    cleaning_beam_agent-3_mean: 13.17
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 533
    cleaning_beam_agent-4_mean: 444.79
    cleaning_beam_agent-4_min: 322
    cleaning_beam_agent-5_max: 256
    cleaning_beam_agent-5_mean: 45.69
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-04-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1175.000000000012
  episode_reward_mean: 1007.0499999999873
  episode_reward_min: 426.00000000000136
  episodes_this_iter: 96
  episodes_total: 31392
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20433.431
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8677613735198975
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015527779469266534
        model: {}
        policy_loss: -0.0033587515354156494
        total_loss: -0.0026818905025720596
        vf_explained_var: 0.006226494908332825
        vf_loss: 22.04123306274414
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1630003452301025
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016714615048840642
        model: {}
        policy_loss: -0.003662708681076765
        total_loss: -0.003466036170721054
        vf_explained_var: -0.00660368800163269
        vf_loss: 22.435510635375977
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1348141431808472
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017624032916501164
        model: {}
        policy_loss: -0.003564514685422182
        total_loss: -0.0033942414447665215
        vf_explained_var: 0.02359253168106079
        vf_loss: 21.675457000732422
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35736900568008423
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012297627981752157
        model: {}
        policy_loss: -0.002328291069716215
        total_loss: -0.0010059744818136096
        vf_explained_var: 0.1225186437368393
        vf_loss: 19.51288604736328
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8894491195678711
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018213274888694286
        model: {}
        policy_loss: -0.0037507815286517143
        total_loss: -0.0031883716583251953
        vf_explained_var: 0.0455254465341568
        vf_loss: 21.278396606445312
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6042043566703796
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011205095797777176
        model: {}
        policy_loss: -0.002900080755352974
        total_loss: -0.001988530158996582
        vf_explained_var: 0.10960976779460907
        vf_loss: 19.749513626098633
    load_time_ms: 26238.138
    num_steps_sampled: 31392000
    num_steps_trained: 31392000
    sample_time_ms: 96161.796
    update_time_ms: 20.712
  iterations_since_restore: 307
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.801092896174865
    ram_util_percent: 12.951912568306012
  pid: 4061
  policy_reward_max:
    agent-0: 195.83333333333258
    agent-1: 195.83333333333258
    agent-2: 195.83333333333258
    agent-3: 195.83333333333258
    agent-4: 195.83333333333258
    agent-5: 195.83333333333258
  policy_reward_mean:
    agent-0: 167.8416666666665
    agent-1: 167.8416666666665
    agent-2: 167.8416666666665
    agent-3: 167.8416666666665
    agent-4: 167.8416666666665
    agent-5: 167.8416666666665
  policy_reward_min:
    agent-0: 70.99999999999983
    agent-1: 70.99999999999983
    agent-2: 70.99999999999983
    agent-3: 70.99999999999983
    agent-4: 70.99999999999983
    agent-5: 70.99999999999983
  sampler_perf:
    mean_env_wait_ms: 24.014149664516253
    mean_inference_ms: 12.263219370826283
    mean_processing_ms: 50.63858883792581
  time_since_restore: 39483.63348507881
  time_this_iter_s: 128.2578089237213
  time_total_s: 42694.69717121124
  timestamp: 1637057060
  timesteps_since_restore: 29472000
  timesteps_this_iter: 96000
  timesteps_total: 31392000
  training_iteration: 327
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    327 |          42694.7 | 31392000 |  1007.05 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 54
    apples_agent-0_mean: 1.99
    apples_agent-0_min: 0
    apples_agent-1_max: 101
    apples_agent-1_mean: 22.14
    apples_agent-1_min: 0
    apples_agent-2_max: 144
    apples_agent-2_mean: 15.79
    apples_agent-2_min: 0
    apples_agent-3_max: 97
    apples_agent-3_mean: 52.88
    apples_agent-3_min: 26
    apples_agent-4_max: 49
    apples_agent-4_mean: 1.25
    apples_agent-4_min: 0
    apples_agent-5_max: 164
    apples_agent-5_mean: 94.75
    apples_agent-5_min: 16
    cleaning_beam_agent-0_max: 598
    cleaning_beam_agent-0_mean: 466.11
    cleaning_beam_agent-0_min: 313
    cleaning_beam_agent-1_max: 417
    cleaning_beam_agent-1_mean: 229.17
    cleaning_beam_agent-1_min: 118
    cleaning_beam_agent-2_max: 626
    cleaning_beam_agent-2_mean: 352.67
    cleaning_beam_agent-2_min: 144
    cleaning_beam_agent-3_max: 84
    cleaning_beam_agent-3_mean: 14.98
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 548
    cleaning_beam_agent-4_mean: 421.6
    cleaning_beam_agent-4_min: 276
    cleaning_beam_agent-5_max: 567
    cleaning_beam_agent-5_mean: 49.34
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-07-21
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1152.9999999999966
  episode_reward_mean: 984.1399999999854
  episode_reward_min: 569.0000000000072
  episodes_this_iter: 96
  episodes_total: 31488
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20429.658
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8701187968254089
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012946025235578418
        model: {}
        policy_loss: -0.0030604414641857147
        total_loss: -0.0024752295576035976
        vf_explained_var: 0.024964526295661926
        vf_loss: 21.166248321533203
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.163557529449463
        entropy_coeff: 0.0017600000137463212
        kl: 0.000952684786170721
        model: {}
        policy_loss: -0.0036947298794984818
        total_loss: -0.0034567471593618393
        vf_explained_var: -0.05277365446090698
        vf_loss: 22.8583984375
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1278544664382935
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009784131543710828
        model: {}
        policy_loss: -0.00304447952657938
        total_loss: -0.0029661301523447037
        vf_explained_var: 0.050084665417671204
        vf_loss: 20.633731842041016
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36785265803337097
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008882343536242843
        model: {}
        policy_loss: -0.0022069087717682123
        total_loss: -0.0009154104627668858
        vf_explained_var: 0.10712689161300659
        vf_loss: 19.389179229736328
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8978500962257385
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014187084743753076
        model: {}
        policy_loss: -0.0035257339477539062
        total_loss: -0.0031040427275002003
        vf_explained_var: 0.07791802287101746
        vf_loss: 20.019088745117188
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6338938474655151
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011514633661136031
        model: {}
        policy_loss: -0.0031799860298633575
        total_loss: -0.002324148779734969
        vf_explained_var: 0.09306712448596954
        vf_loss: 19.714933395385742
    load_time_ms: 27167.768
    num_steps_sampled: 31488000
    num_steps_trained: 31488000
    sample_time_ms: 100552.252
    update_time_ms: 20.868
  iterations_since_restore: 308
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.80700389105058
    ram_util_percent: 17.676264591439693
  pid: 4061
  policy_reward_max:
    agent-0: 192.1666666666664
    agent-1: 192.1666666666664
    agent-2: 192.1666666666664
    agent-3: 192.1666666666664
    agent-4: 192.1666666666664
    agent-5: 192.1666666666664
  policy_reward_mean:
    agent-0: 164.02333333333326
    agent-1: 164.02333333333326
    agent-2: 164.02333333333326
    agent-3: 164.02333333333326
    agent-4: 164.02333333333326
    agent-5: 164.02333333333326
  policy_reward_min:
    agent-0: 94.83333333333363
    agent-1: 94.83333333333363
    agent-2: 94.83333333333363
    agent-3: 94.83333333333363
    agent-4: 94.83333333333363
    agent-5: 94.83333333333363
  sampler_perf:
    mean_env_wait_ms: 24.021884355660205
    mean_inference_ms: 12.265652104140202
    mean_processing_ms: 50.64971982970718
  time_since_restore: 39663.805815696716
  time_this_iter_s: 180.17233061790466
  time_total_s: 42874.86950182915
  timestamp: 1637057241
  timesteps_since_restore: 29568000
  timesteps_this_iter: 96000
  timesteps_total: 31488000
  training_iteration: 328
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 32.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    328 |          42874.9 | 31488000 |   984.14 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 23
    apples_agent-0_mean: 0.86
    apples_agent-0_min: 0
    apples_agent-1_max: 95
    apples_agent-1_mean: 24.53
    apples_agent-1_min: 0
    apples_agent-2_max: 74
    apples_agent-2_mean: 12.52
    apples_agent-2_min: 0
    apples_agent-3_max: 127
    apples_agent-3_mean: 52.66
    apples_agent-3_min: 27
    apples_agent-4_max: 20
    apples_agent-4_mean: 0.46
    apples_agent-4_min: 0
    apples_agent-5_max: 190
    apples_agent-5_mean: 102.06
    apples_agent-5_min: 42
    cleaning_beam_agent-0_max: 578
    cleaning_beam_agent-0_mean: 473.44
    cleaning_beam_agent-0_min: 345
    cleaning_beam_agent-1_max: 417
    cleaning_beam_agent-1_mean: 232.91
    cleaning_beam_agent-1_min: 67
    cleaning_beam_agent-2_max: 540
    cleaning_beam_agent-2_mean: 374.2
    cleaning_beam_agent-2_min: 195
    cleaning_beam_agent-3_max: 75
    cleaning_beam_agent-3_mean: 11.75
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 536
    cleaning_beam_agent-4_mean: 433.21
    cleaning_beam_agent-4_min: 325
    cleaning_beam_agent-5_max: 319
    cleaning_beam_agent-5_mean: 36.67
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-09-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1193.9999999999927
  episode_reward_mean: 1025.3199999999879
  episode_reward_min: 584.0000000000058
  episodes_this_iter: 96
  episodes_total: 31584
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20437.198
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.865786612033844
        entropy_coeff: 0.0017600000137463212
        kl: 0.001597473630681634
        model: {}
        policy_loss: -0.0031941242050379515
        total_loss: -0.002618903061375022
        vf_explained_var: 0.00808127224445343
        vf_loss: 20.990070343017578
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1701927185058594
        entropy_coeff: 0.0017600000137463212
        kl: 0.0025726582389324903
        model: {}
        policy_loss: -0.0041141752153635025
        total_loss: -0.003957593813538551
        vf_explained_var: -0.041289836168289185
        vf_loss: 22.161222457885742
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1314985752105713
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011160236317664385
        model: {}
        policy_loss: -0.0030857729725539684
        total_loss: -0.003010324202477932
        vf_explained_var: 0.02272026240825653
        vf_loss: 20.66887092590332
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3481999635696411
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006332809571176767
        model: {}
        policy_loss: -0.001838939730077982
        total_loss: -0.00047054653987288475
        vf_explained_var: 0.06562326848506927
        vf_loss: 19.812273025512695
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8749826550483704
        entropy_coeff: 0.0017600000137463212
        kl: 0.001765497145242989
        model: {}
        policy_loss: -0.004091648850589991
        total_loss: -0.0036007757298648357
        vf_explained_var: 0.04227514564990997
        vf_loss: 20.30845069885254
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5999234318733215
        entropy_coeff: 0.0017600000137463212
        kl: 0.001415322651155293
        model: {}
        policy_loss: -0.002853558398783207
        total_loss: -0.001960105262696743
        vf_explained_var: 0.06916569173336029
        vf_loss: 19.493188858032227
    load_time_ms: 28459.313
    num_steps_sampled: 31584000
    num_steps_trained: 31584000
    sample_time_ms: 101589.65
    update_time_ms: 20.952
  iterations_since_restore: 309
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.730841121495324
    ram_util_percent: 17.975233644859813
  pid: 4061
  policy_reward_max:
    agent-0: 199.0
    agent-1: 199.0
    agent-2: 199.0
    agent-3: 199.0
    agent-4: 199.0
    agent-5: 199.0
  policy_reward_mean:
    agent-0: 170.88666666666657
    agent-1: 170.88666666666657
    agent-2: 170.88666666666657
    agent-3: 170.88666666666657
    agent-4: 170.88666666666657
    agent-5: 170.88666666666657
  policy_reward_min:
    agent-0: 97.33333333333343
    agent-1: 97.33333333333343
    agent-2: 97.33333333333343
    agent-3: 97.33333333333343
    agent-4: 97.33333333333343
    agent-5: 97.33333333333343
  sampler_perf:
    mean_env_wait_ms: 24.030306531712007
    mean_inference_ms: 12.268122177556897
    mean_processing_ms: 50.65986660895067
  time_since_restore: 39813.54803919792
  time_this_iter_s: 149.74222350120544
  time_total_s: 43024.61172533035
  timestamp: 1637057391
  timesteps_since_restore: 29664000
  timesteps_this_iter: 96000
  timesteps_total: 31584000
  training_iteration: 329
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 28.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    329 |          43024.6 | 31584000 |  1025.32 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 25
    apples_agent-0_mean: 1.49
    apples_agent-0_min: 0
    apples_agent-1_max: 79
    apples_agent-1_mean: 25.16
    apples_agent-1_min: 0
    apples_agent-2_max: 216
    apples_agent-2_mean: 14.93
    apples_agent-2_min: 0
    apples_agent-3_max: 105
    apples_agent-3_mean: 51.9
    apples_agent-3_min: 27
    apples_agent-4_max: 51
    apples_agent-4_mean: 0.65
    apples_agent-4_min: 0
    apples_agent-5_max: 241
    apples_agent-5_mean: 98.4
    apples_agent-5_min: 1
    cleaning_beam_agent-0_max: 765
    cleaning_beam_agent-0_mean: 468.0
    cleaning_beam_agent-0_min: 370
    cleaning_beam_agent-1_max: 340
    cleaning_beam_agent-1_mean: 220.23
    cleaning_beam_agent-1_min: 104
    cleaning_beam_agent-2_max: 527
    cleaning_beam_agent-2_mean: 365.39
    cleaning_beam_agent-2_min: 149
    cleaning_beam_agent-3_max: 77
    cleaning_beam_agent-3_mean: 16.97
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 546
    cleaning_beam_agent-4_mean: 434.81
    cleaning_beam_agent-4_min: 199
    cleaning_beam_agent-5_max: 671
    cleaning_beam_agent-5_mean: 49.85
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-12-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1179.000000000002
  episode_reward_mean: 1006.0499999999888
  episode_reward_min: 452.0000000000136
  episodes_this_iter: 96
  episodes_total: 31680
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20519.11
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.849610447883606
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012939376756548882
        model: {}
        policy_loss: -0.0028820543084293604
        total_loss: -0.002250830177217722
        vf_explained_var: 0.03585062921047211
        vf_loss: 21.2653751373291
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1618931293487549
        entropy_coeff: 0.0017600000137463212
        kl: 0.001691927551291883
        model: {}
        policy_loss: -0.003810941008850932
        total_loss: -0.003625779878348112
        vf_explained_var: -0.01306714117527008
        vf_loss: 22.300926208496094
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.143928050994873
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014622702728956938
        model: {}
        policy_loss: -0.003085792763158679
        total_loss: -0.002990337321534753
        vf_explained_var: 0.041647255420684814
        vf_loss: 21.087644577026367
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35484611988067627
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010478580370545387
        model: {}
        policy_loss: -0.0020716937724500895
        total_loss: -0.0006842585280537605
        vf_explained_var: 0.08336158096790314
        vf_loss: 20.119661331176758
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8806998133659363
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017271246761083603
        model: {}
        policy_loss: -0.003468206152319908
        total_loss: -0.0029275487177073956
        vf_explained_var: 0.048768967390060425
        vf_loss: 20.906879425048828
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5978444218635559
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007064647506922483
        model: {}
        policy_loss: -0.002779127098619938
        total_loss: -0.0018308007856830955
        vf_explained_var: 0.08816960453987122
        vf_loss: 20.00530242919922
    load_time_ms: 28502.815
    num_steps_sampled: 31680000
    num_steps_trained: 31680000
    sample_time_ms: 102620.501
    update_time_ms: 20.919
  iterations_since_restore: 310
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.378680203045686
    ram_util_percent: 17.656345177664974
  pid: 4061
  policy_reward_max:
    agent-0: 196.4999999999996
    agent-1: 196.4999999999996
    agent-2: 196.4999999999996
    agent-3: 196.4999999999996
    agent-4: 196.4999999999996
    agent-5: 196.4999999999996
  policy_reward_mean:
    agent-0: 167.67499999999984
    agent-1: 167.67499999999984
    agent-2: 167.67499999999984
    agent-3: 167.67499999999984
    agent-4: 167.67499999999984
    agent-5: 167.67499999999984
  policy_reward_min:
    agent-0: 75.33333333333333
    agent-1: 75.33333333333333
    agent-2: 75.33333333333333
    agent-3: 75.33333333333333
    agent-4: 75.33333333333333
    agent-5: 75.33333333333333
  sampler_perf:
    mean_env_wait_ms: 24.038632887652952
    mean_inference_ms: 12.270591803452096
    mean_processing_ms: 50.67077315780558
  time_since_restore: 39951.681282520294
  time_this_iter_s: 138.13324332237244
  time_total_s: 43162.744968652725
  timestamp: 1637057529
  timesteps_since_restore: 29760000
  timesteps_this_iter: 96000
  timesteps_total: 31680000
  training_iteration: 330
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 33.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    330 |          43162.7 | 31680000 |  1006.05 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 29
    apples_agent-0_mean: 1.01
    apples_agent-0_min: 0
    apples_agent-1_max: 119
    apples_agent-1_mean: 23.12
    apples_agent-1_min: 0
    apples_agent-2_max: 92
    apples_agent-2_mean: 14.44
    apples_agent-2_min: 0
    apples_agent-3_max: 118
    apples_agent-3_mean: 55.45
    apples_agent-3_min: 24
    apples_agent-4_max: 72
    apples_agent-4_mean: 1.25
    apples_agent-4_min: 0
    apples_agent-5_max: 176
    apples_agent-5_mean: 99.75
    apples_agent-5_min: 7
    cleaning_beam_agent-0_max: 572
    cleaning_beam_agent-0_mean: 467.78
    cleaning_beam_agent-0_min: 370
    cleaning_beam_agent-1_max: 330
    cleaning_beam_agent-1_mean: 217.69
    cleaning_beam_agent-1_min: 83
    cleaning_beam_agent-2_max: 534
    cleaning_beam_agent-2_mean: 366.58
    cleaning_beam_agent-2_min: 214
    cleaning_beam_agent-3_max: 132
    cleaning_beam_agent-3_mean: 14.6
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 554
    cleaning_beam_agent-4_mean: 442.78
    cleaning_beam_agent-4_min: 349
    cleaning_beam_agent-5_max: 749
    cleaning_beam_agent-5_mean: 46.88
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-14-56
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1189.0000000000184
  episode_reward_mean: 1016.5999999999875
  episode_reward_min: 616.0000000000048
  episodes_this_iter: 96
  episodes_total: 31776
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20565.282
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8445346355438232
        entropy_coeff: 0.0017600000137463212
        kl: 0.002056702272966504
        model: {}
        policy_loss: -0.0032357326708734035
        total_loss: -0.0026228977367281914
        vf_explained_var: -0.003621503710746765
        vf_loss: 20.992149353027344
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1588971614837646
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013098575873300433
        model: {}
        policy_loss: -0.0033270600251853466
        total_loss: -0.0032540771644562483
        vf_explained_var: -0.00893448293209076
        vf_loss: 21.126441955566406
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1399812698364258
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019357511773705482
        model: {}
        policy_loss: -0.003399948589503765
        total_loss: -0.003310421947389841
        vf_explained_var: -0.006329134106636047
        vf_loss: 20.958946228027344
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3478928208351135
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009136439184658229
        model: {}
        policy_loss: -0.0018195112934336066
        total_loss: -0.00042518042027950287
        vf_explained_var: 0.036331698298454285
        vf_loss: 20.066226959228516
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8798694610595703
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021307743154466152
        model: {}
        policy_loss: -0.003821566002443433
        total_loss: -0.0033469018526375294
        vf_explained_var: 0.033943548798561096
        vf_loss: 20.232358932495117
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5956452488899231
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008576805703341961
        model: {}
        policy_loss: -0.0026758480817079544
        total_loss: -0.0017535919323563576
        vf_explained_var: 0.0521722137928009
        vf_loss: 19.705928802490234
    load_time_ms: 29282.544
    num_steps_sampled: 31776000
    num_steps_trained: 31776000
    sample_time_ms: 102637.065
    update_time_ms: 21.001
  iterations_since_restore: 311
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.00675105485232
    ram_util_percent: 18.407172995780584
  pid: 4061
  policy_reward_max:
    agent-0: 198.16666666666666
    agent-1: 198.16666666666666
    agent-2: 198.16666666666666
    agent-3: 198.16666666666666
    agent-4: 198.16666666666666
    agent-5: 198.16666666666666
  policy_reward_mean:
    agent-0: 169.43333333333317
    agent-1: 169.43333333333317
    agent-2: 169.43333333333317
    agent-3: 169.43333333333317
    agent-4: 169.43333333333317
    agent-5: 169.43333333333317
  policy_reward_min:
    agent-0: 102.666666666667
    agent-1: 102.666666666667
    agent-2: 102.666666666667
    agent-3: 102.666666666667
    agent-4: 102.666666666667
    agent-5: 102.666666666667
  sampler_perf:
    mean_env_wait_ms: 24.047332445204006
    mean_inference_ms: 12.27298236567085
    mean_processing_ms: 50.683667818286246
  time_since_restore: 40118.45882678032
  time_this_iter_s: 166.77754426002502
  time_total_s: 43329.52251291275
  timestamp: 1637057696
  timesteps_since_restore: 29856000
  timesteps_this_iter: 96000
  timesteps_total: 31776000
  training_iteration: 331
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    331 |          43329.5 | 31776000 |   1016.6 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 25
    apples_agent-0_mean: 0.8
    apples_agent-0_min: 0
    apples_agent-1_max: 93
    apples_agent-1_mean: 23.29
    apples_agent-1_min: 0
    apples_agent-2_max: 143
    apples_agent-2_mean: 14.13
    apples_agent-2_min: 0
    apples_agent-3_max: 190
    apples_agent-3_mean: 57.07
    apples_agent-3_min: 31
    apples_agent-4_max: 37
    apples_agent-4_mean: 0.73
    apples_agent-4_min: 0
    apples_agent-5_max: 237
    apples_agent-5_mean: 98.57
    apples_agent-5_min: 26
    cleaning_beam_agent-0_max: 668
    cleaning_beam_agent-0_mean: 474.81
    cleaning_beam_agent-0_min: 302
    cleaning_beam_agent-1_max: 382
    cleaning_beam_agent-1_mean: 207.55
    cleaning_beam_agent-1_min: 69
    cleaning_beam_agent-2_max: 538
    cleaning_beam_agent-2_mean: 358.71
    cleaning_beam_agent-2_min: 176
    cleaning_beam_agent-3_max: 58
    cleaning_beam_agent-3_mean: 17.0
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 526
    cleaning_beam_agent-4_mean: 434.78
    cleaning_beam_agent-4_min: 274
    cleaning_beam_agent-5_max: 412
    cleaning_beam_agent-5_mean: 42.27
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-17-41
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1194.0000000000182
  episode_reward_mean: 1005.5599999999862
  episode_reward_min: 251.99999999999707
  episodes_this_iter: 96
  episodes_total: 31872
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20552.366
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8400380611419678
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013153317850083113
        model: {}
        policy_loss: -0.002888256683945656
        total_loss: -0.0021876844111829996
        vf_explained_var: 0.004519164562225342
        vf_loss: 21.79038429260254
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1554609537124634
        entropy_coeff: 0.0017600000137463212
        kl: 0.001329934224486351
        model: {}
        policy_loss: -0.0038805711083114147
        total_loss: -0.003748910268768668
        vf_explained_var: 0.012477710843086243
        vf_loss: 21.652729034423828
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1363952159881592
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011894310591742396
        model: {}
        policy_loss: -0.0031703109852969646
        total_loss: -0.0030608996748924255
        vf_explained_var: 0.0331176221370697
        vf_loss: 21.094654083251953
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.355942040681839
        entropy_coeff: 0.0017600000137463212
        kl: 0.001195848686620593
        model: {}
        policy_loss: -0.002218923531472683
        total_loss: -0.0008566000615246594
        vf_explained_var: 0.08987374603748322
        vf_loss: 19.88780975341797
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8930500149726868
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019689176697283983
        model: {}
        policy_loss: -0.004054346587508917
        total_loss: -0.0035263397730886936
        vf_explained_var: 0.040843233466148376
        vf_loss: 20.997766494750977
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6129363775253296
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013681069249287248
        model: {}
        policy_loss: -0.0033332370221614838
        total_loss: -0.0023971968330442905
        vf_explained_var: 0.07866537570953369
        vf_loss: 20.148094177246094
    load_time_ms: 28941.128
    num_steps_sampled: 31872000
    num_steps_trained: 31872000
    sample_time_ms: 103113.552
    update_time_ms: 20.722
  iterations_since_restore: 312
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.841525423728815
    ram_util_percent: 18.485169491525422
  pid: 4061
  policy_reward_max:
    agent-0: 198.9999999999999
    agent-1: 198.9999999999999
    agent-2: 198.9999999999999
    agent-3: 198.9999999999999
    agent-4: 198.9999999999999
    agent-5: 198.9999999999999
  policy_reward_mean:
    agent-0: 167.59333333333316
    agent-1: 167.59333333333316
    agent-2: 167.59333333333316
    agent-3: 167.59333333333316
    agent-4: 167.59333333333316
    agent-5: 167.59333333333316
  policy_reward_min:
    agent-0: 41.999999999999964
    agent-1: 41.999999999999964
    agent-2: 41.999999999999964
    agent-3: 41.999999999999964
    agent-4: 41.999999999999964
    agent-5: 41.999999999999964
  sampler_perf:
    mean_env_wait_ms: 24.055610607158872
    mean_inference_ms: 12.275540340624273
    mean_processing_ms: 50.694507119224
  time_since_restore: 40283.391041994095
  time_this_iter_s: 164.93221521377563
  time_total_s: 43494.454728126526
  timestamp: 1637057861
  timesteps_since_restore: 29952000
  timesteps_this_iter: 96000
  timesteps_total: 31872000
  training_iteration: 332
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    332 |          43494.5 | 31872000 |  1005.56 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 34
    apples_agent-0_mean: 1.09
    apples_agent-0_min: 0
    apples_agent-1_max: 93
    apples_agent-1_mean: 27.96
    apples_agent-1_min: 0
    apples_agent-2_max: 98
    apples_agent-2_mean: 13.46
    apples_agent-2_min: 0
    apples_agent-3_max: 183
    apples_agent-3_mean: 54.95
    apples_agent-3_min: 19
    apples_agent-4_max: 84
    apples_agent-4_mean: 1.72
    apples_agent-4_min: 0
    apples_agent-5_max: 242
    apples_agent-5_mean: 96.26
    apples_agent-5_min: 33
    cleaning_beam_agent-0_max: 575
    cleaning_beam_agent-0_mean: 461.32
    cleaning_beam_agent-0_min: 249
    cleaning_beam_agent-1_max: 456
    cleaning_beam_agent-1_mean: 208.46
    cleaning_beam_agent-1_min: 72
    cleaning_beam_agent-2_max: 589
    cleaning_beam_agent-2_mean: 359.62
    cleaning_beam_agent-2_min: 190
    cleaning_beam_agent-3_max: 58
    cleaning_beam_agent-3_mean: 16.94
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 547
    cleaning_beam_agent-4_mean: 430.77
    cleaning_beam_agent-4_min: 232
    cleaning_beam_agent-5_max: 264
    cleaning_beam_agent-5_mean: 44.44
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-20-15
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1180.9999999999861
  episode_reward_mean: 998.9799999999861
  episode_reward_min: 286.99999999999767
  episodes_this_iter: 96
  episodes_total: 31968
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20530.263
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8502299189567566
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015276642516255379
        model: {}
        policy_loss: -0.0028590019792318344
        total_loss: -0.0020777839235961437
        vf_explained_var: 0.015377208590507507
        vf_loss: 22.776241302490234
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1559935808181763
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015262998640537262
        model: {}
        policy_loss: -0.003878396935760975
        total_loss: -0.0035868696868419647
        vf_explained_var: -0.002636462450027466
        vf_loss: 23.260746002197266
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1374562978744507
        entropy_coeff: 0.0017600000137463212
        kl: 0.001737471902742982
        model: {}
        policy_loss: -0.0034457372967153788
        total_loss: -0.003186536021530628
        vf_explained_var: 0.022263631224632263
        vf_loss: 22.61128044128418
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3627488315105438
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009217456681653857
        model: {}
        policy_loss: -0.0024868305772542953
        total_loss: -0.0010834401473402977
        vf_explained_var: 0.11701357364654541
        vf_loss: 20.41827392578125
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8924269676208496
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021863109432160854
        model: {}
        policy_loss: -0.004180297255516052
        total_loss: -0.003649270161986351
        vf_explained_var: 0.09206749498844147
        vf_loss: 21.016977310180664
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6246764659881592
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008515813387930393
        model: {}
        policy_loss: -0.0032855928875505924
        total_loss: -0.002314321929588914
        vf_explained_var: 0.10413782298564911
        vf_loss: 20.70700454711914
    load_time_ms: 29401.162
    num_steps_sampled: 31968000
    num_steps_trained: 31968000
    sample_time_ms: 103000.318
    update_time_ms: 19.802
  iterations_since_restore: 313
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.111363636363638
    ram_util_percent: 18.91681818181818
  pid: 4061
  policy_reward_max:
    agent-0: 196.8333333333331
    agent-1: 196.8333333333331
    agent-2: 196.8333333333331
    agent-3: 196.8333333333331
    agent-4: 196.8333333333331
    agent-5: 196.8333333333331
  policy_reward_mean:
    agent-0: 166.4966666666665
    agent-1: 166.4966666666665
    agent-2: 166.4966666666665
    agent-3: 166.4966666666665
    agent-4: 166.4966666666665
    agent-5: 166.4966666666665
  policy_reward_min:
    agent-0: 47.833333333333265
    agent-1: 47.833333333333265
    agent-2: 47.833333333333265
    agent-3: 47.833333333333265
    agent-4: 47.833333333333265
    agent-5: 47.833333333333265
  sampler_perf:
    mean_env_wait_ms: 24.063828096814312
    mean_inference_ms: 12.278106128310755
    mean_processing_ms: 50.707050234434774
  time_since_restore: 40437.77875638008
  time_this_iter_s: 154.38771438598633
  time_total_s: 43648.84244251251
  timestamp: 1637058015
  timesteps_since_restore: 30048000
  timesteps_this_iter: 96000
  timesteps_total: 31968000
  training_iteration: 333
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    333 |          43648.8 | 31968000 |   998.98 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 34
    apples_agent-0_mean: 1.78
    apples_agent-0_min: 0
    apples_agent-1_max: 88
    apples_agent-1_mean: 23.78
    apples_agent-1_min: 0
    apples_agent-2_max: 169
    apples_agent-2_mean: 15.28
    apples_agent-2_min: 0
    apples_agent-3_max: 149
    apples_agent-3_mean: 54.11
    apples_agent-3_min: 33
    apples_agent-4_max: 57
    apples_agent-4_mean: 1.31
    apples_agent-4_min: 0
    apples_agent-5_max: 189
    apples_agent-5_mean: 98.2
    apples_agent-5_min: 33
    cleaning_beam_agent-0_max: 573
    cleaning_beam_agent-0_mean: 447.71
    cleaning_beam_agent-0_min: 249
    cleaning_beam_agent-1_max: 399
    cleaning_beam_agent-1_mean: 223.54
    cleaning_beam_agent-1_min: 84
    cleaning_beam_agent-2_max: 522
    cleaning_beam_agent-2_mean: 357.26
    cleaning_beam_agent-2_min: 173
    cleaning_beam_agent-3_max: 51
    cleaning_beam_agent-3_mean: 13.95
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 542
    cleaning_beam_agent-4_mean: 436.82
    cleaning_beam_agent-4_min: 265
    cleaning_beam_agent-5_max: 177
    cleaning_beam_agent-5_mean: 37.4
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-22-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1273.99999999999
  episode_reward_mean: 999.4499999999872
  episode_reward_min: 533.0000000000074
  episodes_this_iter: 96
  episodes_total: 32064
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20503.681
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8330655694007874
        entropy_coeff: 0.0017600000137463212
        kl: 0.001621021656319499
        model: {}
        policy_loss: -0.0032221809960901737
        total_loss: -0.0023935227654874325
        vf_explained_var: 0.04052148759365082
        vf_loss: 22.948524475097656
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.177168846130371
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015922629972919822
        model: {}
        policy_loss: -0.003807620145380497
        total_loss: -0.0034830281510949135
        vf_explained_var: -0.0048350989818573
        vf_loss: 23.964101791381836
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1365480422973633
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011284035863354802
        model: {}
        policy_loss: -0.003080730326473713
        total_loss: -0.002848935779184103
        vf_explained_var: 0.062346458435058594
        vf_loss: 22.321186065673828
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.355434387922287
        entropy_coeff: 0.0017600000137463212
        kl: 0.000884644512552768
        model: {}
        policy_loss: -0.0021159653551876545
        total_loss: -0.0006602140492759645
        vf_explained_var: 0.12794581055641174
        vf_loss: 20.81314468383789
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8854129314422607
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015306056011468172
        model: {}
        policy_loss: -0.003738217055797577
        total_loss: -0.0030377795919775963
        vf_explained_var: 0.05205613374710083
        vf_loss: 22.587635040283203
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6101516485214233
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013532484881579876
        model: {}
        policy_loss: -0.0030814274214208126
        total_loss: -0.0020917034707963467
        vf_explained_var: 0.13260601460933685
        vf_loss: 20.63591957092285
    load_time_ms: 27443.361
    num_steps_sampled: 32064000
    num_steps_trained: 32064000
    sample_time_ms: 102939.973
    update_time_ms: 19.376
  iterations_since_restore: 314
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.56567164179105
    ram_util_percent: 17.54726368159204
  pid: 4061
  policy_reward_max:
    agent-0: 212.3333333333329
    agent-1: 212.3333333333329
    agent-2: 212.3333333333329
    agent-3: 212.3333333333329
    agent-4: 212.3333333333329
    agent-5: 212.3333333333329
  policy_reward_mean:
    agent-0: 166.5749999999998
    agent-1: 166.5749999999998
    agent-2: 166.5749999999998
    agent-3: 166.5749999999998
    agent-4: 166.5749999999998
    agent-5: 166.5749999999998
  policy_reward_min:
    agent-0: 88.83333333333364
    agent-1: 88.83333333333364
    agent-2: 88.83333333333364
    agent-3: 88.83333333333364
    agent-4: 88.83333333333364
    agent-5: 88.83333333333364
  sampler_perf:
    mean_env_wait_ms: 24.071229300937713
    mean_inference_ms: 12.280252410920744
    mean_processing_ms: 50.71698948446484
  time_since_restore: 40578.69801211357
  time_this_iter_s: 140.91925573349
  time_total_s: 43789.761698246
  timestamp: 1637058156
  timesteps_since_restore: 30144000
  timesteps_this_iter: 96000
  timesteps_total: 32064000
  training_iteration: 334
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    334 |          43789.8 | 32064000 |   999.45 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 23
    apples_agent-0_mean: 0.92
    apples_agent-0_min: 0
    apples_agent-1_max: 77
    apples_agent-1_mean: 19.24
    apples_agent-1_min: 0
    apples_agent-2_max: 86
    apples_agent-2_mean: 13.93
    apples_agent-2_min: 0
    apples_agent-3_max: 134
    apples_agent-3_mean: 54.03
    apples_agent-3_min: 26
    apples_agent-4_max: 75
    apples_agent-4_mean: 1.52
    apples_agent-4_min: 0
    apples_agent-5_max: 209
    apples_agent-5_mean: 98.72
    apples_agent-5_min: 50
    cleaning_beam_agent-0_max: 667
    cleaning_beam_agent-0_mean: 461.6
    cleaning_beam_agent-0_min: 340
    cleaning_beam_agent-1_max: 378
    cleaning_beam_agent-1_mean: 232.1
    cleaning_beam_agent-1_min: 84
    cleaning_beam_agent-2_max: 543
    cleaning_beam_agent-2_mean: 372.35
    cleaning_beam_agent-2_min: 164
    cleaning_beam_agent-3_max: 60
    cleaning_beam_agent-3_mean: 15.63
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 554
    cleaning_beam_agent-4_mean: 436.36
    cleaning_beam_agent-4_min: 329
    cleaning_beam_agent-5_max: 357
    cleaning_beam_agent-5_mean: 30.63
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-25-17
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1173.000000000004
  episode_reward_mean: 1024.8099999999881
  episode_reward_min: 599.9999999999986
  episodes_this_iter: 96
  episodes_total: 32160
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20482.335
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.828314483165741
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013895503943786025
        model: {}
        policy_loss: -0.0032200561836361885
        total_loss: -0.002497962210327387
        vf_explained_var: 0.019783630967140198
        vf_loss: 21.79926872253418
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1681280136108398
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015488759381696582
        model: {}
        policy_loss: -0.004051569849252701
        total_loss: -0.0038920463994145393
        vf_explained_var: 0.000896453857421875
        vf_loss: 22.154285430908203
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1223461627960205
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013135654153302312
        model: {}
        policy_loss: -0.0032118174713104963
        total_loss: -0.003035485977306962
        vf_explained_var: 0.024406760931015015
        vf_loss: 21.5166015625
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3537757098674774
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007238822290673852
        model: {}
        policy_loss: -0.0019641155377030373
        total_loss: -0.000527690164744854
        vf_explained_var: 0.06617054343223572
        vf_loss: 20.590686798095703
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8914813995361328
        entropy_coeff: 0.0017600000137463212
        kl: 0.002073037438094616
        model: {}
        policy_loss: -0.004011181183159351
        total_loss: -0.003482123836874962
        vf_explained_var: 0.055405616760253906
        vf_loss: 20.980661392211914
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5667221546173096
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016802026657387614
        model: {}
        policy_loss: -0.003061147639527917
        total_loss: -0.0020214966498315334
        vf_explained_var: 0.07526583969593048
        vf_loss: 20.370803833007812
    load_time_ms: 28325.685
    num_steps_sampled: 32160000
    num_steps_trained: 32160000
    sample_time_ms: 102776.124
    update_time_ms: 19.604
  iterations_since_restore: 315
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.53755458515284
    ram_util_percent: 18.71353711790393
  pid: 4061
  policy_reward_max:
    agent-0: 195.49999999999955
    agent-1: 195.49999999999955
    agent-2: 195.49999999999955
    agent-3: 195.49999999999955
    agent-4: 195.49999999999955
    agent-5: 195.49999999999955
  policy_reward_mean:
    agent-0: 170.8016666666665
    agent-1: 170.8016666666665
    agent-2: 170.8016666666665
    agent-3: 170.8016666666665
    agent-4: 170.8016666666665
    agent-5: 170.8016666666665
  policy_reward_min:
    agent-0: 100.00000000000001
    agent-1: 100.00000000000001
    agent-2: 100.00000000000001
    agent-3: 100.00000000000001
    agent-4: 100.00000000000001
    agent-5: 100.00000000000001
  sampler_perf:
    mean_env_wait_ms: 24.078727423792312
    mean_inference_ms: 12.28240936711284
    mean_processing_ms: 50.727855450419675
  time_since_restore: 40739.431557655334
  time_this_iter_s: 160.7335455417633
  time_total_s: 43950.495243787766
  timestamp: 1637058317
  timesteps_since_restore: 30240000
  timesteps_this_iter: 96000
  timesteps_total: 32160000
  training_iteration: 335
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 31.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    335 |          43950.5 | 32160000 |  1024.81 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 0.83
    apples_agent-0_min: 0
    apples_agent-1_max: 73
    apples_agent-1_mean: 20.05
    apples_agent-1_min: 0
    apples_agent-2_max: 351
    apples_agent-2_mean: 13.52
    apples_agent-2_min: 0
    apples_agent-3_max: 140
    apples_agent-3_mean: 54.1
    apples_agent-3_min: 27
    apples_agent-4_max: 77
    apples_agent-4_mean: 2.35
    apples_agent-4_min: 0
    apples_agent-5_max: 286
    apples_agent-5_mean: 100.19
    apples_agent-5_min: 36
    cleaning_beam_agent-0_max: 550
    cleaning_beam_agent-0_mean: 455.64
    cleaning_beam_agent-0_min: 345
    cleaning_beam_agent-1_max: 394
    cleaning_beam_agent-1_mean: 231.3
    cleaning_beam_agent-1_min: 79
    cleaning_beam_agent-2_max: 552
    cleaning_beam_agent-2_mean: 379.56
    cleaning_beam_agent-2_min: 133
    cleaning_beam_agent-3_max: 63
    cleaning_beam_agent-3_mean: 17.03
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 578
    cleaning_beam_agent-4_mean: 427.11
    cleaning_beam_agent-4_min: 271
    cleaning_beam_agent-5_max: 219
    cleaning_beam_agent-5_mean: 36.19
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-28-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1184.9999999999939
  episode_reward_mean: 1008.3599999999877
  episode_reward_min: 287.9999999999995
  episodes_this_iter: 96
  episodes_total: 32256
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20503.046
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8313884139060974
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017483287956565619
        model: {}
        policy_loss: -0.0032370155677199364
        total_loss: -0.00226573646068573
        vf_explained_var: 0.0033981502056121826
        vf_loss: 24.345239639282227
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1607195138931274
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015318052610382438
        model: {}
        policy_loss: -0.00361389247700572
        total_loss: -0.0031519888434559107
        vf_explained_var: -0.022712141275405884
        vf_loss: 25.047727584838867
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1202338933944702
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014742516214028
        model: {}
        policy_loss: -0.0035600578412413597
        total_loss: -0.003160734660923481
        vf_explained_var: 0.026902928948402405
        vf_loss: 23.709339141845703
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3513267934322357
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009825368179008365
        model: {}
        policy_loss: -0.002259845845401287
        total_loss: -0.0006923703476786613
        vf_explained_var: 0.10359323024749756
        vf_loss: 21.858097076416016
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8886771202087402
        entropy_coeff: 0.0017600000137463212
        kl: 0.002247195690870285
        model: {}
        policy_loss: -0.003688952187076211
        total_loss: -0.00304244551807642
        vf_explained_var: 0.09358690679073334
        vf_loss: 22.105810165405273
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5696703195571899
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011986038880422711
        model: {}
        policy_loss: -0.0031921002082526684
        total_loss: -0.0019832986872643232
        vf_explained_var: 0.0918266624212265
        vf_loss: 22.11419105529785
    load_time_ms: 30668.177
    num_steps_sampled: 32256000
    num_steps_trained: 32256000
    sample_time_ms: 104099.465
    update_time_ms: 20.329
  iterations_since_restore: 316
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.978600823045266
    ram_util_percent: 19.477777777777778
  pid: 4061
  policy_reward_max:
    agent-0: 197.49999999999977
    agent-1: 197.49999999999977
    agent-2: 197.49999999999977
    agent-3: 197.49999999999977
    agent-4: 197.49999999999977
    agent-5: 197.49999999999977
  policy_reward_mean:
    agent-0: 168.05999999999986
    agent-1: 168.05999999999986
    agent-2: 168.05999999999986
    agent-3: 168.05999999999986
    agent-4: 168.05999999999986
    agent-5: 168.05999999999986
  policy_reward_min:
    agent-0: 47.99999999999981
    agent-1: 47.99999999999981
    agent-2: 47.99999999999981
    agent-3: 47.99999999999981
    agent-4: 47.99999999999981
    agent-5: 47.99999999999981
  sampler_perf:
    mean_env_wait_ms: 24.087121724904286
    mean_inference_ms: 12.284858845225578
    mean_processing_ms: 50.739148866109275
  time_since_restore: 40909.62719750404
  time_this_iter_s: 170.1956398487091
  time_total_s: 44120.690883636475
  timestamp: 1637058488
  timesteps_since_restore: 30336000
  timesteps_this_iter: 96000
  timesteps_total: 32256000
  training_iteration: 336
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    336 |          44120.7 | 32256000 |  1008.36 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 31
    apples_agent-0_mean: 2.3
    apples_agent-0_min: 0
    apples_agent-1_max: 78
    apples_agent-1_mean: 18.58
    apples_agent-1_min: 0
    apples_agent-2_max: 101
    apples_agent-2_mean: 14.98
    apples_agent-2_min: 0
    apples_agent-3_max: 101
    apples_agent-3_mean: 52.39
    apples_agent-3_min: 26
    apples_agent-4_max: 61
    apples_agent-4_mean: 1.23
    apples_agent-4_min: 0
    apples_agent-5_max: 197
    apples_agent-5_mean: 100.58
    apples_agent-5_min: 24
    cleaning_beam_agent-0_max: 566
    cleaning_beam_agent-0_mean: 453.18
    cleaning_beam_agent-0_min: 256
    cleaning_beam_agent-1_max: 379
    cleaning_beam_agent-1_mean: 229.92
    cleaning_beam_agent-1_min: 93
    cleaning_beam_agent-2_max: 549
    cleaning_beam_agent-2_mean: 366.33
    cleaning_beam_agent-2_min: 165
    cleaning_beam_agent-3_max: 66
    cleaning_beam_agent-3_mean: 18.13
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 562
    cleaning_beam_agent-4_mean: 437.94
    cleaning_beam_agent-4_min: 327
    cleaning_beam_agent-5_max: 354
    cleaning_beam_agent-5_mean: 34.6
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-30-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1186.9999999999861
  episode_reward_mean: 1014.0299999999899
  episode_reward_min: 417.0000000000086
  episodes_this_iter: 96
  episodes_total: 32352
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20519.212
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8518407344818115
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018029094208031893
        model: {}
        policy_loss: -0.0032844538800418377
        total_loss: -0.002410593442618847
        vf_explained_var: 0.0057382285594940186
        vf_loss: 23.730989456176758
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1505458354949951
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015988743398338556
        model: {}
        policy_loss: -0.0038085058331489563
        total_loss: -0.003400471992790699
        vf_explained_var: -0.01995629072189331
        vf_loss: 24.329973220825195
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1274640560150146
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014132471987977624
        model: {}
        policy_loss: -0.0031817834824323654
        total_loss: -0.002885952591896057
        vf_explained_var: 0.04096890985965729
        vf_loss: 22.801637649536133
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37455153465270996
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012727979337796569
        model: {}
        policy_loss: -0.002396647585555911
        total_loss: -0.0008693288546055555
        vf_explained_var: 0.08311544358730316
        vf_loss: 21.865291595458984
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8761200308799744
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020752353593707085
        model: {}
        policy_loss: -0.003877660259604454
        total_loss: -0.0031450586393475533
        vf_explained_var: 0.045687466859817505
        vf_loss: 22.745737075805664
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5847597122192383
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011052561458200216
        model: {}
        policy_loss: -0.003275735303759575
        total_loss: -0.002202174859121442
        vf_explained_var: 0.11637416481971741
        vf_loss: 21.027366638183594
    load_time_ms: 31590.167
    num_steps_sampled: 32352000
    num_steps_trained: 32352000
    sample_time_ms: 104992.746
    update_time_ms: 20.976
  iterations_since_restore: 317
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.63827751196172
    ram_util_percent: 19.581818181818182
  pid: 4061
  policy_reward_max:
    agent-0: 197.8333333333333
    agent-1: 197.8333333333333
    agent-2: 197.8333333333333
    agent-3: 197.8333333333333
    agent-4: 197.8333333333333
    agent-5: 197.8333333333333
  policy_reward_mean:
    agent-0: 169.00499999999982
    agent-1: 169.00499999999982
    agent-2: 169.00499999999982
    agent-3: 169.00499999999982
    agent-4: 169.00499999999982
    agent-5: 169.00499999999982
  policy_reward_min:
    agent-0: 69.49999999999991
    agent-1: 69.49999999999991
    agent-2: 69.49999999999991
    agent-3: 69.49999999999991
    agent-4: 69.49999999999991
    agent-5: 69.49999999999991
  sampler_perf:
    mean_env_wait_ms: 24.095624826919153
    mean_inference_ms: 12.287403075602917
    mean_processing_ms: 50.75091829755647
  time_since_restore: 41056.22108864784
  time_this_iter_s: 146.59389114379883
  time_total_s: 44267.28477478027
  timestamp: 1637058634
  timesteps_since_restore: 30432000
  timesteps_this_iter: 96000
  timesteps_total: 32352000
  training_iteration: 337
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    337 |          44267.3 | 32352000 |  1014.03 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 0.64
    apples_agent-0_min: 0
    apples_agent-1_max: 80
    apples_agent-1_mean: 23.85
    apples_agent-1_min: 0
    apples_agent-2_max: 156
    apples_agent-2_mean: 11.02
    apples_agent-2_min: 0
    apples_agent-3_max: 89
    apples_agent-3_mean: 53.84
    apples_agent-3_min: 18
    apples_agent-4_max: 75
    apples_agent-4_mean: 1.55
    apples_agent-4_min: 0
    apples_agent-5_max: 369
    apples_agent-5_mean: 100.84
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 551
    cleaning_beam_agent-0_mean: 450.92
    cleaning_beam_agent-0_min: 342
    cleaning_beam_agent-1_max: 350
    cleaning_beam_agent-1_mean: 226.05
    cleaning_beam_agent-1_min: 99
    cleaning_beam_agent-2_max: 555
    cleaning_beam_agent-2_mean: 374.92
    cleaning_beam_agent-2_min: 192
    cleaning_beam_agent-3_max: 74
    cleaning_beam_agent-3_mean: 17.99
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 550
    cleaning_beam_agent-4_mean: 443.16
    cleaning_beam_agent-4_min: 327
    cleaning_beam_agent-5_max: 431
    cleaning_beam_agent-5_mean: 38.56
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-32-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1166.0000000000175
  episode_reward_mean: 1009.7299999999867
  episode_reward_min: 555.0000000000042
  episodes_this_iter: 96
  episodes_total: 32448
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20589.24
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8441135883331299
        entropy_coeff: 0.0017600000137463212
        kl: 0.001229036832228303
        model: {}
        policy_loss: -0.002737619448453188
        total_loss: -0.0019428156083449721
        vf_explained_var: 0.034285128116607666
        vf_loss: 22.80445098876953
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1624646186828613
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015429573832079768
        model: {}
        policy_loss: -0.0036224927753210068
        total_loss: -0.0032681827433407307
        vf_explained_var: -0.014640092849731445
        vf_loss: 24.002492904663086
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1209105253219604
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013169909361749887
        model: {}
        policy_loss: -0.00311666214838624
        total_loss: -0.002778154332190752
        vf_explained_var: 0.02014423906803131
        vf_loss: 23.11313247680664
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36220329999923706
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010384664637967944
        model: {}
        policy_loss: -0.002301377011463046
        total_loss: -0.0007917510811239481
        vf_explained_var: 0.08996020257472992
        vf_loss: 21.471036911010742
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8740050196647644
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016625653952360153
        model: {}
        policy_loss: -0.004038167651742697
        total_loss: -0.00329160806722939
        vf_explained_var: 0.032395124435424805
        vf_loss: 22.848068237304688
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5777279734611511
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010803438490256667
        model: {}
        policy_loss: -0.0028516510501503944
        total_loss: -0.0017410609871149063
        vf_explained_var: 0.09913341701030731
        vf_loss: 21.27388572692871
    load_time_ms: 30566.759
    num_steps_sampled: 32448000
    num_steps_trained: 32448000
    sample_time_ms: 101612.745
    update_time_ms: 20.928
  iterations_since_restore: 318
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.50205128205128
    ram_util_percent: 19.42666666666667
  pid: 4061
  policy_reward_max:
    agent-0: 194.33333333333283
    agent-1: 194.33333333333283
    agent-2: 194.33333333333283
    agent-3: 194.33333333333283
    agent-4: 194.33333333333283
    agent-5: 194.33333333333283
  policy_reward_mean:
    agent-0: 168.28833333333318
    agent-1: 168.28833333333318
    agent-2: 168.28833333333318
    agent-3: 168.28833333333318
    agent-4: 168.28833333333318
    agent-5: 168.28833333333318
  policy_reward_min:
    agent-0: 92.49999999999987
    agent-1: 92.49999999999987
    agent-2: 92.49999999999987
    agent-3: 92.49999999999987
    agent-4: 92.49999999999987
    agent-5: 92.49999999999987
  sampler_perf:
    mean_env_wait_ms: 24.103970640608964
    mean_inference_ms: 12.289742931839408
    mean_processing_ms: 50.7620083182058
  time_since_restore: 41192.95588541031
  time_this_iter_s: 136.73479676246643
  time_total_s: 44404.01957154274
  timestamp: 1637058771
  timesteps_since_restore: 30528000
  timesteps_this_iter: 96000
  timesteps_total: 32448000
  training_iteration: 338
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    338 |            44404 | 32448000 |  1009.73 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 30
    apples_agent-0_mean: 1.36
    apples_agent-0_min: 0
    apples_agent-1_max: 113
    apples_agent-1_mean: 20.82
    apples_agent-1_min: 0
    apples_agent-2_max: 156
    apples_agent-2_mean: 13.94
    apples_agent-2_min: 0
    apples_agent-3_max: 104
    apples_agent-3_mean: 53.01
    apples_agent-3_min: 20
    apples_agent-4_max: 74
    apples_agent-4_mean: 2.29
    apples_agent-4_min: 0
    apples_agent-5_max: 185
    apples_agent-5_mean: 99.23
    apples_agent-5_min: 44
    cleaning_beam_agent-0_max: 610
    cleaning_beam_agent-0_mean: 439.45
    cleaning_beam_agent-0_min: 331
    cleaning_beam_agent-1_max: 381
    cleaning_beam_agent-1_mean: 242.94
    cleaning_beam_agent-1_min: 93
    cleaning_beam_agent-2_max: 541
    cleaning_beam_agent-2_mean: 364.63
    cleaning_beam_agent-2_min: 195
    cleaning_beam_agent-3_max: 69
    cleaning_beam_agent-3_mean: 15.91
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 552
    cleaning_beam_agent-4_mean: 449.45
    cleaning_beam_agent-4_min: 191
    cleaning_beam_agent-5_max: 363
    cleaning_beam_agent-5_mean: 36.28
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-35-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1221.0000000000025
  episode_reward_mean: 1029.8299999999915
  episode_reward_min: 432.0000000000094
  episodes_this_iter: 96
  episodes_total: 32544
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20622.283
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8386910557746887
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020655435509979725
        model: {}
        policy_loss: -0.0029562131967395544
        total_loss: -0.002022113185375929
        vf_explained_var: 0.008905261754989624
        vf_loss: 24.10198211669922
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1600704193115234
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015683902893215418
        model: {}
        policy_loss: -0.003936376888304949
        total_loss: -0.0034658857621252537
        vf_explained_var: -0.03718671202659607
        vf_loss: 25.122148513793945
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.126609206199646
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011315243318676949
        model: {}
        policy_loss: -0.0030663067009299994
        total_loss: -0.0026579839177429676
        vf_explained_var: 0.00418342649936676
        vf_loss: 23.911510467529297
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36165302991867065
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010059999767690897
        model: {}
        policy_loss: -0.002368855755776167
        total_loss: -0.0008648736402392387
        vf_explained_var: 0.11101102828979492
        vf_loss: 21.4049015045166
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8763889074325562
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023209713399410248
        model: {}
        policy_loss: -0.004337482154369354
        total_loss: -0.0036953696981072426
        vf_explained_var: 0.09532208740711212
        vf_loss: 21.845561981201172
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5723959803581238
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005783896194770932
        model: {}
        policy_loss: -0.0028994991444051266
        total_loss: -0.0017998188268393278
        vf_explained_var: 0.12583984434604645
        vf_loss: 21.070955276489258
    load_time_ms: 30945.248
    num_steps_sampled: 32544000
    num_steps_trained: 32544000
    sample_time_ms: 101360.068
    update_time_ms: 20.53
  iterations_since_restore: 319
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.16759259259259
    ram_util_percent: 19.74537037037037
  pid: 4061
  policy_reward_max:
    agent-0: 203.49999999999994
    agent-1: 203.49999999999994
    agent-2: 203.49999999999994
    agent-3: 203.49999999999994
    agent-4: 203.49999999999994
    agent-5: 203.49999999999994
  policy_reward_mean:
    agent-0: 171.63833333333318
    agent-1: 171.63833333333318
    agent-2: 171.63833333333318
    agent-3: 171.63833333333318
    agent-4: 171.63833333333318
    agent-5: 171.63833333333318
  policy_reward_min:
    agent-0: 71.99999999999996
    agent-1: 71.99999999999996
    agent-2: 71.99999999999996
    agent-3: 71.99999999999996
    agent-4: 71.99999999999996
    agent-5: 71.99999999999996
  sampler_perf:
    mean_env_wait_ms: 24.112418851442953
    mean_inference_ms: 12.292117202055197
    mean_processing_ms: 50.7738797029247
  time_since_restore: 41344.33261203766
  time_this_iter_s: 151.37672662734985
  time_total_s: 44555.39629817009
  timestamp: 1637058923
  timesteps_since_restore: 30624000
  timesteps_this_iter: 96000
  timesteps_total: 32544000
  training_iteration: 339
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    339 |          44555.4 | 32544000 |  1029.83 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 41
    apples_agent-0_mean: 1.61
    apples_agent-0_min: 0
    apples_agent-1_max: 98
    apples_agent-1_mean: 25.87
    apples_agent-1_min: 0
    apples_agent-2_max: 119
    apples_agent-2_mean: 15.6
    apples_agent-2_min: 0
    apples_agent-3_max: 206
    apples_agent-3_mean: 54.83
    apples_agent-3_min: 29
    apples_agent-4_max: 42
    apples_agent-4_mean: 0.84
    apples_agent-4_min: 0
    apples_agent-5_max: 308
    apples_agent-5_mean: 98.8
    apples_agent-5_min: 24
    cleaning_beam_agent-0_max: 562
    cleaning_beam_agent-0_mean: 422.0
    cleaning_beam_agent-0_min: 282
    cleaning_beam_agent-1_max: 363
    cleaning_beam_agent-1_mean: 234.7
    cleaning_beam_agent-1_min: 113
    cleaning_beam_agent-2_max: 584
    cleaning_beam_agent-2_mean: 359.32
    cleaning_beam_agent-2_min: 196
    cleaning_beam_agent-3_max: 102
    cleaning_beam_agent-3_mean: 20.27
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 587
    cleaning_beam_agent-4_mean: 466.96
    cleaning_beam_agent-4_min: 349
    cleaning_beam_agent-5_max: 408
    cleaning_beam_agent-5_mean: 37.36
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-38-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1164.999999999992
  episode_reward_mean: 1017.4699999999888
  episode_reward_min: 491.0000000000107
  episodes_this_iter: 96
  episodes_total: 32640
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20590.94
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8415704965591431
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011182419257238507
        model: {}
        policy_loss: -0.0031543043442070484
        total_loss: -0.0024816240184009075
        vf_explained_var: 0.04675912857055664
        vf_loss: 21.538450241088867
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1398967504501343
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009782169945538044
        model: {}
        policy_loss: -0.00348953390493989
        total_loss: -0.003178190439939499
        vf_explained_var: -0.022152632474899292
        vf_loss: 23.17563247680664
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1280757188796997
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014013680629432201
        model: {}
        policy_loss: -0.0032044616527855396
        total_loss: -0.0029643410816788673
        vf_explained_var: 0.013419777154922485
        vf_loss: 22.255348205566406
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37439337372779846
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011364773381501436
        model: {}
        policy_loss: -0.0022165165282785892
        total_loss: -0.0008895387873053551
        vf_explained_var: 0.1165703535079956
        vf_loss: 19.85915756225586
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8640772700309753
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016739030834287405
        model: {}
        policy_loss: -0.0036545393522828817
        total_loss: -0.003080800175666809
        vf_explained_var: 0.06619808077812195
        vf_loss: 20.945144653320312
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5582262277603149
        entropy_coeff: 0.0017600000137463212
        kl: 0.000956499483436346
        model: {}
        policy_loss: -0.002954800147563219
        total_loss: -0.0019002784974873066
        vf_explained_var: 0.09512893855571747
        vf_loss: 20.369983673095703
    load_time_ms: 33645.63
    num_steps_sampled: 32640000
    num_steps_trained: 32640000
    sample_time_ms: 101421.261
    update_time_ms: 19.951
  iterations_since_restore: 320
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.490254237288134
    ram_util_percent: 19.731779661016947
  pid: 4061
  policy_reward_max:
    agent-0: 194.16666666666634
    agent-1: 194.16666666666634
    agent-2: 194.16666666666634
    agent-3: 194.16666666666634
    agent-4: 194.16666666666634
    agent-5: 194.16666666666634
  policy_reward_mean:
    agent-0: 169.5783333333332
    agent-1: 169.5783333333332
    agent-2: 169.5783333333332
    agent-3: 169.5783333333332
    agent-4: 169.5783333333332
    agent-5: 169.5783333333332
  policy_reward_min:
    agent-0: 81.8333333333336
    agent-1: 81.8333333333336
    agent-2: 81.8333333333336
    agent-3: 81.8333333333336
    agent-4: 81.8333333333336
    agent-5: 81.8333333333336
  sampler_perf:
    mean_env_wait_ms: 24.121304658209233
    mean_inference_ms: 12.294760279913515
    mean_processing_ms: 50.7860112273436
  time_since_restore: 41509.784049510956
  time_this_iter_s: 165.45143747329712
  time_total_s: 44720.84773564339
  timestamp: 1637059089
  timesteps_since_restore: 30720000
  timesteps_this_iter: 96000
  timesteps_total: 32640000
  training_iteration: 340
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 36.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    340 |          44720.8 | 32640000 |  1017.47 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 31
    apples_agent-0_mean: 1.46
    apples_agent-0_min: 0
    apples_agent-1_max: 102
    apples_agent-1_mean: 24.27
    apples_agent-1_min: 0
    apples_agent-2_max: 238
    apples_agent-2_mean: 14.94
    apples_agent-2_min: 0
    apples_agent-3_max: 158
    apples_agent-3_mean: 57.58
    apples_agent-3_min: 27
    apples_agent-4_max: 66
    apples_agent-4_mean: 2.08
    apples_agent-4_min: 0
    apples_agent-5_max: 159
    apples_agent-5_mean: 94.35
    apples_agent-5_min: 31
    cleaning_beam_agent-0_max: 571
    cleaning_beam_agent-0_mean: 426.42
    cleaning_beam_agent-0_min: 284
    cleaning_beam_agent-1_max: 403
    cleaning_beam_agent-1_mean: 246.67
    cleaning_beam_agent-1_min: 89
    cleaning_beam_agent-2_max: 593
    cleaning_beam_agent-2_mean: 364.15
    cleaning_beam_agent-2_min: 140
    cleaning_beam_agent-3_max: 71
    cleaning_beam_agent-3_mean: 17.95
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 576
    cleaning_beam_agent-4_mean: 467.32
    cleaning_beam_agent-4_min: 365
    cleaning_beam_agent-5_max: 423
    cleaning_beam_agent-5_mean: 32.01
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-40-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1183.0000000000023
  episode_reward_mean: 1021.2499999999895
  episode_reward_min: 553.0000000000061
  episodes_this_iter: 96
  episodes_total: 32736
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20590.803
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8476182222366333
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019705425947904587
        model: {}
        policy_loss: -0.0029821437783539295
        total_loss: -0.0022598248906433582
        vf_explained_var: 0.011677190661430359
        vf_loss: 22.141284942626953
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1579228639602661
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015279725193977356
        model: {}
        policy_loss: -0.0037233438342809677
        total_loss: -0.003516506403684616
        vf_explained_var: -0.00020080804824829102
        vf_loss: 22.447811126708984
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1179285049438477
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010400875471532345
        model: {}
        policy_loss: -0.0030992028769105673
        total_loss: -0.0028983100783079863
        vf_explained_var: 0.03222768008708954
        vf_loss: 21.684463500976562
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3706350326538086
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012275164481252432
        model: {}
        policy_loss: -0.002268056385219097
        total_loss: -0.0008624470792710781
        vf_explained_var: 0.08160132169723511
        vf_loss: 20.57921600341797
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8785160183906555
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019654640927910805
        model: {}
        policy_loss: -0.004034672863781452
        total_loss: -0.003474581055343151
        vf_explained_var: 0.06025420129299164
        vf_loss: 21.062820434570312
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5697057247161865
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010895597515627742
        model: {}
        policy_loss: -0.002907959744334221
        total_loss: -0.0018552883993834257
        vf_explained_var: 0.08220067620277405
        vf_loss: 20.55352020263672
    load_time_ms: 33753.938
    num_steps_sampled: 32736000
    num_steps_trained: 32736000
    sample_time_ms: 101161.892
    update_time_ms: 21.028
  iterations_since_restore: 321
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.991949152542375
    ram_util_percent: 18.36652542372882
  pid: 4061
  policy_reward_max:
    agent-0: 197.16666666666683
    agent-1: 197.16666666666683
    agent-2: 197.16666666666683
    agent-3: 197.16666666666683
    agent-4: 197.16666666666683
    agent-5: 197.16666666666683
  policy_reward_mean:
    agent-0: 170.2083333333332
    agent-1: 170.2083333333332
    agent-2: 170.2083333333332
    agent-3: 170.2083333333332
    agent-4: 170.2083333333332
    agent-5: 170.2083333333332
  policy_reward_min:
    agent-0: 92.16666666666691
    agent-1: 92.16666666666691
    agent-2: 92.16666666666691
    agent-3: 92.16666666666691
    agent-4: 92.16666666666691
    agent-5: 92.16666666666691
  sampler_perf:
    mean_env_wait_ms: 24.127964826665853
    mean_inference_ms: 12.29678012089544
    mean_processing_ms: 50.79509616358993
  time_since_restore: 41675.09025859833
  time_this_iter_s: 165.30620908737183
  time_total_s: 44886.15394473076
  timestamp: 1637059255
  timesteps_since_restore: 30816000
  timesteps_this_iter: 96000
  timesteps_total: 32736000
  training_iteration: 341
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 36.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    341 |          44886.2 | 32736000 |  1021.25 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 50
    apples_agent-0_mean: 1.59
    apples_agent-0_min: 0
    apples_agent-1_max: 108
    apples_agent-1_mean: 24.73
    apples_agent-1_min: 0
    apples_agent-2_max: 134
    apples_agent-2_mean: 13.91
    apples_agent-2_min: 0
    apples_agent-3_max: 133
    apples_agent-3_mean: 54.89
    apples_agent-3_min: 22
    apples_agent-4_max: 51
    apples_agent-4_mean: 1.61
    apples_agent-4_min: 0
    apples_agent-5_max: 194
    apples_agent-5_mean: 93.17
    apples_agent-5_min: 21
    cleaning_beam_agent-0_max: 540
    cleaning_beam_agent-0_mean: 429.28
    cleaning_beam_agent-0_min: 301
    cleaning_beam_agent-1_max: 374
    cleaning_beam_agent-1_mean: 245.84
    cleaning_beam_agent-1_min: 132
    cleaning_beam_agent-2_max: 613
    cleaning_beam_agent-2_mean: 369.65
    cleaning_beam_agent-2_min: 186
    cleaning_beam_agent-3_max: 74
    cleaning_beam_agent-3_mean: 18.46
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 622
    cleaning_beam_agent-4_mean: 455.26
    cleaning_beam_agent-4_min: 287
    cleaning_beam_agent-5_max: 292
    cleaning_beam_agent-5_mean: 32.13
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-43-39
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1194.0000000000155
  episode_reward_mean: 1017.1099999999902
  episode_reward_min: 285.999999999997
  episodes_this_iter: 96
  episodes_total: 32832
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20658.825
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8553009033203125
        entropy_coeff: 0.0017600000137463212
        kl: 0.001307500759139657
        model: {}
        policy_loss: -0.00280056893825531
        total_loss: -0.001924271578900516
        vf_explained_var: 0.016057193279266357
        vf_loss: 23.81625747680664
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1542556285858154
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015644319355487823
        model: {}
        policy_loss: -0.003579310607165098
        total_loss: -0.003157655242830515
        vf_explained_var: -0.011417672038078308
        vf_loss: 24.53146743774414
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1240460872650146
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016855723224580288
        model: {}
        policy_loss: -0.0033782203681766987
        total_loss: -0.0030047395266592503
        vf_explained_var: 0.028163298964500427
        vf_loss: 23.518035888671875
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37804174423217773
        entropy_coeff: 0.0017600000137463212
        kl: 0.000920135120395571
        model: {}
        policy_loss: -0.002381934318691492
        total_loss: -0.0008650841191411018
        vf_explained_var: 0.0986994057893753
        vf_loss: 21.82205581665039
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8837597370147705
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019462057389318943
        model: {}
        policy_loss: -0.0040684654377400875
        total_loss: -0.003413025289773941
        vf_explained_var: 0.08694148063659668
        vf_loss: 22.10858726501465
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.559291422367096
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008699194877408445
        model: {}
        policy_loss: -0.002829132601618767
        total_loss: -0.0016722003929316998
        vf_explained_var: 0.11474725604057312
        vf_loss: 21.412887573242188
    load_time_ms: 34296.295
    num_steps_sampled: 32832000
    num_steps_trained: 32832000
    sample_time_ms: 100536.242
    update_time_ms: 22.289
  iterations_since_restore: 322
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.85702127659574
    ram_util_percent: 19.3331914893617
  pid: 4061
  policy_reward_max:
    agent-0: 198.99999999999955
    agent-1: 198.99999999999955
    agent-2: 198.99999999999955
    agent-3: 198.99999999999955
    agent-4: 198.99999999999955
    agent-5: 198.99999999999955
  policy_reward_mean:
    agent-0: 169.51833333333315
    agent-1: 169.51833333333315
    agent-2: 169.51833333333315
    agent-3: 169.51833333333315
    agent-4: 169.51833333333315
    agent-5: 169.51833333333315
  policy_reward_min:
    agent-0: 47.66666666666655
    agent-1: 47.66666666666655
    agent-2: 47.66666666666655
    agent-3: 47.66666666666655
    agent-4: 47.66666666666655
    agent-5: 47.66666666666655
  sampler_perf:
    mean_env_wait_ms: 24.134651679538962
    mean_inference_ms: 12.29882212181655
    mean_processing_ms: 50.80435220468294
  time_since_restore: 41839.912291526794
  time_this_iter_s: 164.8220329284668
  time_total_s: 45050.975977659225
  timestamp: 1637059419
  timesteps_since_restore: 30912000
  timesteps_this_iter: 96000
  timesteps_total: 32832000
  training_iteration: 342
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 36.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    342 |            45051 | 32832000 |  1017.11 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 25
    apples_agent-0_mean: 1.08
    apples_agent-0_min: 0
    apples_agent-1_max: 77
    apples_agent-1_mean: 23.18
    apples_agent-1_min: 0
    apples_agent-2_max: 92
    apples_agent-2_mean: 12.75
    apples_agent-2_min: 0
    apples_agent-3_max: 180
    apples_agent-3_mean: 56.53
    apples_agent-3_min: 28
    apples_agent-4_max: 65
    apples_agent-4_mean: 1.98
    apples_agent-4_min: 0
    apples_agent-5_max: 241
    apples_agent-5_mean: 97.18
    apples_agent-5_min: 58
    cleaning_beam_agent-0_max: 536
    cleaning_beam_agent-0_mean: 430.47
    cleaning_beam_agent-0_min: 334
    cleaning_beam_agent-1_max: 433
    cleaning_beam_agent-1_mean: 263.08
    cleaning_beam_agent-1_min: 139
    cleaning_beam_agent-2_max: 560
    cleaning_beam_agent-2_mean: 359.65
    cleaning_beam_agent-2_min: 179
    cleaning_beam_agent-3_max: 119
    cleaning_beam_agent-3_mean: 21.38
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 559
    cleaning_beam_agent-4_mean: 441.87
    cleaning_beam_agent-4_min: 291
    cleaning_beam_agent-5_max: 274
    cleaning_beam_agent-5_mean: 39.28
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-46-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1194.0000000000155
  episode_reward_mean: 1029.869999999989
  episode_reward_min: 565.9999999999961
  episodes_this_iter: 96
  episodes_total: 32928
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20710.59
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8488987684249878
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014934843638911843
        model: {}
        policy_loss: -0.003099510446190834
        total_loss: -0.0022636172361671925
        vf_explained_var: 0.0074170976877212524
        vf_loss: 23.299501419067383
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1537531614303589
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014921885449439287
        model: {}
        policy_loss: -0.003591954242438078
        total_loss: -0.0032496924977749586
        vf_explained_var: -0.009656906127929688
        vf_loss: 23.728689193725586
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.127502679824829
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010076405014842749
        model: {}
        policy_loss: -0.0029586395248770714
        total_loss: -0.0027235522866249084
        vf_explained_var: 0.052838295698165894
        vf_loss: 22.194923400878906
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3674042820930481
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009332224726676941
        model: {}
        policy_loss: -0.002280163113027811
        total_loss: -0.0007939650677144527
        vf_explained_var: 0.09142851829528809
        vf_loss: 21.32831573486328
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8909234404563904
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016792500391602516
        model: {}
        policy_loss: -0.003740357467904687
        total_loss: -0.003069800091907382
        vf_explained_var: 0.04518982768058777
        vf_loss: 22.385852813720703
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5623468160629272
        entropy_coeff: 0.0017600000137463212
        kl: 0.000987748266197741
        model: {}
        policy_loss: -0.003110587364062667
        total_loss: -0.0019591168966144323
        vf_explained_var: 0.08759500086307526
        vf_loss: 21.41199493408203
    load_time_ms: 34788.733
    num_steps_sampled: 32928000
    num_steps_trained: 32928000
    sample_time_ms: 100456.884
    update_time_ms: 22.528
  iterations_since_restore: 323
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.626431718061674
    ram_util_percent: 20.25814977973568
  pid: 4061
  policy_reward_max:
    agent-0: 198.99999999999955
    agent-1: 198.99999999999955
    agent-2: 198.99999999999955
    agent-3: 198.99999999999955
    agent-4: 198.99999999999955
    agent-5: 198.99999999999955
  policy_reward_mean:
    agent-0: 171.6449999999998
    agent-1: 171.6449999999998
    agent-2: 171.6449999999998
    agent-3: 171.6449999999998
    agent-4: 171.6449999999998
    agent-5: 171.6449999999998
  policy_reward_min:
    agent-0: 94.33333333333358
    agent-1: 94.33333333333358
    agent-2: 94.33333333333358
    agent-3: 94.33333333333358
    agent-4: 94.33333333333358
    agent-5: 94.33333333333358
  sampler_perf:
    mean_env_wait_ms: 24.14262787762146
    mean_inference_ms: 12.301078205132935
    mean_processing_ms: 50.81484582011203
  time_since_restore: 41998.92327642441
  time_this_iter_s: 159.01098489761353
  time_total_s: 45209.98696255684
  timestamp: 1637059579
  timesteps_since_restore: 31008000
  timesteps_this_iter: 96000
  timesteps_total: 32928000
  training_iteration: 343
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 36.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    343 |            45210 | 32928000 |  1029.87 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 25
    apples_agent-0_mean: 0.99
    apples_agent-0_min: 0
    apples_agent-1_max: 90
    apples_agent-1_mean: 22.96
    apples_agent-1_min: 0
    apples_agent-2_max: 109
    apples_agent-2_mean: 10.6
    apples_agent-2_min: 0
    apples_agent-3_max: 114
    apples_agent-3_mean: 56.63
    apples_agent-3_min: 29
    apples_agent-4_max: 38
    apples_agent-4_mean: 0.78
    apples_agent-4_min: 0
    apples_agent-5_max: 176
    apples_agent-5_mean: 97.93
    apples_agent-5_min: 42
    cleaning_beam_agent-0_max: 559
    cleaning_beam_agent-0_mean: 433.96
    cleaning_beam_agent-0_min: 327
    cleaning_beam_agent-1_max: 405
    cleaning_beam_agent-1_mean: 244.45
    cleaning_beam_agent-1_min: 111
    cleaning_beam_agent-2_max: 550
    cleaning_beam_agent-2_mean: 384.41
    cleaning_beam_agent-2_min: 171
    cleaning_beam_agent-3_max: 60
    cleaning_beam_agent-3_mean: 18.15
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 572
    cleaning_beam_agent-4_mean: 452.53
    cleaning_beam_agent-4_min: 245
    cleaning_beam_agent-5_max: 370
    cleaning_beam_agent-5_mean: 34.77
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-48-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1219.9999999999886
  episode_reward_mean: 1050.0199999999902
  episode_reward_min: 525.0000000000163
  episodes_this_iter: 96
  episodes_total: 33024
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20738.592
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8337786793708801
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016149356961250305
        model: {}
        policy_loss: -0.0028274795040488243
        total_loss: -0.002237842418253422
        vf_explained_var: 0.05502714216709137
        vf_loss: 20.570863723754883
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1462717056274414
        entropy_coeff: 0.0017600000137463212
        kl: 0.00199930090457201
        model: {}
        policy_loss: -0.0038330545648932457
        total_loss: -0.0036191819235682487
        vf_explained_var: -0.020917966961860657
        vf_loss: 22.31313705444336
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1267011165618896
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012296047061681747
        model: {}
        policy_loss: -0.0031769927591085434
        total_loss: -0.003096942789852619
        vf_explained_var: 0.039979174733161926
        vf_loss: 20.630443572998047
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35200828313827515
        entropy_coeff: 0.0017600000137463212
        kl: 0.0004907676484435797
        model: {}
        policy_loss: -0.001743772067129612
        total_loss: -0.00031904992647469044
        vf_explained_var: 0.053770512342453
        vf_loss: 20.442520141601562
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8685858249664307
        entropy_coeff: 0.0017600000137463212
        kl: 0.001742358785122633
        model: {}
        policy_loss: -0.0037288055755198
        total_loss: -0.003195675555616617
        vf_explained_var: 0.05607305467128754
        vf_loss: 20.618423461914062
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5394371747970581
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005398184875957668
        model: {}
        policy_loss: -0.002382836304605007
        total_loss: -0.001266269013285637
        vf_explained_var: 0.04445444047451019
        vf_loss: 20.65978240966797
    load_time_ms: 34106.045
    num_steps_sampled: 33024000
    num_steps_trained: 33024000
    sample_time_ms: 100526.924
    update_time_ms: 24.464
  iterations_since_restore: 324
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.5
    ram_util_percent: 20.17046632124352
  pid: 4061
  policy_reward_max:
    agent-0: 203.33333333333292
    agent-1: 203.33333333333292
    agent-2: 203.33333333333292
    agent-3: 203.33333333333292
    agent-4: 203.33333333333292
    agent-5: 203.33333333333292
  policy_reward_mean:
    agent-0: 175.00333333333313
    agent-1: 175.00333333333313
    agent-2: 175.00333333333313
    agent-3: 175.00333333333313
    agent-4: 175.00333333333313
    agent-5: 175.00333333333313
  policy_reward_min:
    agent-0: 87.50000000000011
    agent-1: 87.50000000000011
    agent-2: 87.50000000000011
    agent-3: 87.50000000000011
    agent-4: 87.50000000000011
    agent-5: 87.50000000000011
  sampler_perf:
    mean_env_wait_ms: 24.1509156828108
    mean_inference_ms: 12.303900755104099
    mean_processing_ms: 50.826907566193505
  time_since_restore: 42133.99763560295
  time_this_iter_s: 135.0743591785431
  time_total_s: 45345.06132173538
  timestamp: 1637059714
  timesteps_since_restore: 31104000
  timesteps_this_iter: 96000
  timesteps_total: 33024000
  training_iteration: 344
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 37.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    344 |          45345.1 | 33024000 |  1050.02 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 47
    apples_agent-0_mean: 1.54
    apples_agent-0_min: 0
    apples_agent-1_max: 136
    apples_agent-1_mean: 24.03
    apples_agent-1_min: 0
    apples_agent-2_max: 99
    apples_agent-2_mean: 13.24
    apples_agent-2_min: 0
    apples_agent-3_max: 124
    apples_agent-3_mean: 55.77
    apples_agent-3_min: 24
    apples_agent-4_max: 29
    apples_agent-4_mean: 0.72
    apples_agent-4_min: 0
    apples_agent-5_max: 173
    apples_agent-5_mean: 94.92
    apples_agent-5_min: 41
    cleaning_beam_agent-0_max: 520
    cleaning_beam_agent-0_mean: 435.71
    cleaning_beam_agent-0_min: 263
    cleaning_beam_agent-1_max: 465
    cleaning_beam_agent-1_mean: 258.45
    cleaning_beam_agent-1_min: 128
    cleaning_beam_agent-2_max: 579
    cleaning_beam_agent-2_mean: 383.38
    cleaning_beam_agent-2_min: 204
    cleaning_beam_agent-3_max: 147
    cleaning_beam_agent-3_mean: 17.57
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 577
    cleaning_beam_agent-4_mean: 445.22
    cleaning_beam_agent-4_min: 348
    cleaning_beam_agent-5_max: 347
    cleaning_beam_agent-5_mean: 35.41
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-50-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1248.0000000000114
  episode_reward_mean: 1038.879999999991
  episode_reward_min: 424.000000000011
  episodes_this_iter: 96
  episodes_total: 33120
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20748.911
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8373816609382629
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012133391574025154
        model: {}
        policy_loss: -0.0027293572202324867
        total_loss: -0.0019835825078189373
        vf_explained_var: 0.024543628096580505
        vf_loss: 22.19569206237793
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1469467878341675
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018477891571819782
        model: {}
        policy_loss: -0.003705739276483655
        total_loss: -0.0034395745024085045
        vf_explained_var: -0.0030903220176696777
        vf_loss: 22.8478946685791
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.117523431777954
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008151605725288391
        model: {}
        policy_loss: -0.0028236517682671547
        total_loss: -0.0025558690540492535
        vf_explained_var: 0.014717668294906616
        vf_loss: 22.346237182617188
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36223918199539185
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011259010061621666
        model: {}
        policy_loss: -0.0020672259852290154
        total_loss: -0.000568813644349575
        vf_explained_var: 0.059781670570373535
        vf_loss: 21.359556198120117
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8860028386116028
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016977160703390837
        model: {}
        policy_loss: -0.0034981556236743927
        total_loss: -0.0029160138219594955
        vf_explained_var: 0.05865177512168884
        vf_loss: 21.41510009765625
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5517740249633789
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006766695878468454
        model: {}
        policy_loss: -0.002504961332306266
        total_loss: -0.0014251149259507656
        vf_explained_var: 0.09590205550193787
        vf_loss: 20.50969123840332
    load_time_ms: 31623.709
    num_steps_sampled: 33120000
    num_steps_trained: 33120000
    sample_time_ms: 100642.117
    update_time_ms: 24.22
  iterations_since_restore: 325
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.38061224489796
    ram_util_percent: 20.201020408163263
  pid: 4061
  policy_reward_max:
    agent-0: 207.99999999999955
    agent-1: 207.99999999999955
    agent-2: 207.99999999999955
    agent-3: 207.99999999999955
    agent-4: 207.99999999999955
    agent-5: 207.99999999999955
  policy_reward_mean:
    agent-0: 173.1466666666665
    agent-1: 173.1466666666665
    agent-2: 173.1466666666665
    agent-3: 173.1466666666665
    agent-4: 173.1466666666665
    agent-5: 173.1466666666665
  policy_reward_min:
    agent-0: 70.66666666666661
    agent-1: 70.66666666666661
    agent-2: 70.66666666666661
    agent-3: 70.66666666666661
    agent-4: 70.66666666666661
    agent-5: 70.66666666666661
  sampler_perf:
    mean_env_wait_ms: 24.159255511111397
    mean_inference_ms: 12.306067466498373
    mean_processing_ms: 50.838108845288545
  time_since_restore: 42271.20516371727
  time_this_iter_s: 137.20752811431885
  time_total_s: 45482.2688498497
  timestamp: 1637059851
  timesteps_since_restore: 31200000
  timesteps_this_iter: 96000
  timesteps_total: 33120000
  training_iteration: 345
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 37.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    345 |          45482.3 | 33120000 |  1038.88 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 18
    apples_agent-0_mean: 0.92
    apples_agent-0_min: 0
    apples_agent-1_max: 95
    apples_agent-1_mean: 23.91
    apples_agent-1_min: 0
    apples_agent-2_max: 249
    apples_agent-2_mean: 17.26
    apples_agent-2_min: 0
    apples_agent-3_max: 120
    apples_agent-3_mean: 53.72
    apples_agent-3_min: 25
    apples_agent-4_max: 43
    apples_agent-4_mean: 1.4
    apples_agent-4_min: 0
    apples_agent-5_max: 258
    apples_agent-5_mean: 97.94
    apples_agent-5_min: 27
    cleaning_beam_agent-0_max: 529
    cleaning_beam_agent-0_mean: 432.78
    cleaning_beam_agent-0_min: 329
    cleaning_beam_agent-1_max: 427
    cleaning_beam_agent-1_mean: 260.16
    cleaning_beam_agent-1_min: 136
    cleaning_beam_agent-2_max: 561
    cleaning_beam_agent-2_mean: 385.4
    cleaning_beam_agent-2_min: 226
    cleaning_beam_agent-3_max: 63
    cleaning_beam_agent-3_mean: 17.76
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 572
    cleaning_beam_agent-4_mean: 436.34
    cleaning_beam_agent-4_min: 240
    cleaning_beam_agent-5_max: 399
    cleaning_beam_agent-5_mean: 32.1
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-53-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1218.0000000000164
  episode_reward_mean: 1026.2599999999904
  episode_reward_min: 449.0000000000027
  episodes_this_iter: 96
  episodes_total: 33216
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20724.245
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8337043523788452
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010293037630617619
        model: {}
        policy_loss: -0.0026318253949284554
        total_loss: -0.0016959886997938156
        vf_explained_var: 0.02156543731689453
        vf_loss: 24.031553268432617
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.142015814781189
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023796360474079847
        model: {}
        policy_loss: -0.003931101411581039
        total_loss: -0.0034538975451141596
        vf_explained_var: -0.01125374436378479
        vf_loss: 24.871509552001953
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1191980838775635
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016874379944056273
        model: {}
        policy_loss: -0.003165013622492552
        total_loss: -0.0028396467678248882
        vf_explained_var: 0.06366001069545746
        vf_loss: 22.951541900634766
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38158583641052246
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013839026214554906
        model: {}
        policy_loss: -0.0022720806300640106
        total_loss: -0.0007568275323137641
        vf_explained_var: 0.11082629859447479
        vf_loss: 21.86844825744629
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8854348659515381
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016648635501042008
        model: {}
        policy_loss: -0.00391623517498374
        total_loss: -0.0031782956793904305
        vf_explained_var: 0.06670671701431274
        vf_loss: 22.96302604675293
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5490974187850952
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007424973882734776
        model: {}
        policy_loss: -0.00272973976098001
        total_loss: -0.0015857166144996881
        vf_explained_var: 0.14047937095165253
        vf_loss: 21.10436248779297
    load_time_ms: 29226.55
    num_steps_sampled: 33216000
    num_steps_trained: 33216000
    sample_time_ms: 99745.783
    update_time_ms: 24.126
  iterations_since_restore: 326
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.29179487179487
    ram_util_percent: 20.21794871794872
  pid: 4061
  policy_reward_max:
    agent-0: 202.9999999999997
    agent-1: 202.9999999999997
    agent-2: 202.9999999999997
    agent-3: 202.9999999999997
    agent-4: 202.9999999999997
    agent-5: 202.9999999999997
  policy_reward_mean:
    agent-0: 171.04333333333318
    agent-1: 171.04333333333318
    agent-2: 171.04333333333318
    agent-3: 171.04333333333318
    agent-4: 171.04333333333318
    agent-5: 171.04333333333318
  policy_reward_min:
    agent-0: 74.83333333333319
    agent-1: 74.83333333333319
    agent-2: 74.83333333333319
    agent-3: 74.83333333333319
    agent-4: 74.83333333333319
    agent-5: 74.83333333333319
  sampler_perf:
    mean_env_wait_ms: 24.167613876642836
    mean_inference_ms: 12.308393026211343
    mean_processing_ms: 50.84921940485434
  time_since_restore: 42408.15923023224
  time_this_iter_s: 136.95406651496887
  time_total_s: 45619.22291636467
  timestamp: 1637059988
  timesteps_since_restore: 31296000
  timesteps_this_iter: 96000
  timesteps_total: 33216000
  training_iteration: 346
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 37.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    346 |          45619.2 | 33216000 |  1026.26 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 29
    apples_agent-0_mean: 0.47
    apples_agent-0_min: 0
    apples_agent-1_max: 90
    apples_agent-1_mean: 23.02
    apples_agent-1_min: 0
    apples_agent-2_max: 140
    apples_agent-2_mean: 17.87
    apples_agent-2_min: 0
    apples_agent-3_max: 99
    apples_agent-3_mean: 59.07
    apples_agent-3_min: 25
    apples_agent-4_max: 64
    apples_agent-4_mean: 1.49
    apples_agent-4_min: 0
    apples_agent-5_max: 197
    apples_agent-5_mean: 95.27
    apples_agent-5_min: 17
    cleaning_beam_agent-0_max: 577
    cleaning_beam_agent-0_mean: 434.83
    cleaning_beam_agent-0_min: 321
    cleaning_beam_agent-1_max: 482
    cleaning_beam_agent-1_mean: 275.84
    cleaning_beam_agent-1_min: 121
    cleaning_beam_agent-2_max: 521
    cleaning_beam_agent-2_mean: 366.36
    cleaning_beam_agent-2_min: 184
    cleaning_beam_agent-3_max: 72
    cleaning_beam_agent-3_mean: 19.92
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 582
    cleaning_beam_agent-4_mean: 432.68
    cleaning_beam_agent-4_min: 287
    cleaning_beam_agent-5_max: 435
    cleaning_beam_agent-5_mean: 39.73
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 4
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-55-25
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1193.9999999999864
  episode_reward_mean: 1030.66999999999
  episode_reward_min: 538.000000000012
  episodes_this_iter: 96
  episodes_total: 33312
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20712.132
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8379061222076416
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013406556099653244
        model: {}
        policy_loss: -0.0027764197438955307
        total_loss: -0.0020752577111124992
        vf_explained_var: 0.035290107131004333
        vf_loss: 21.758756637573242
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.137845754623413
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015044669853523374
        model: {}
        policy_loss: -0.004005846567451954
        total_loss: -0.00369854923337698
        vf_explained_var: -0.023271113634109497
        vf_loss: 23.099069595336914
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1358873844146729
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015312164323404431
        model: {}
        policy_loss: -0.002997638890519738
        total_loss: -0.002872987650334835
        vf_explained_var: 0.06117124855518341
        vf_loss: 21.23811149597168
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3759605288505554
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008649713709019125
        model: {}
        policy_loss: -0.0022093411535024643
        total_loss: -0.0007480429485440254
        vf_explained_var: 0.058819159865379333
        vf_loss: 21.229888916015625
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8850328326225281
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016868480015546083
        model: {}
        policy_loss: -0.003574513830244541
        total_loss: -0.003015106078237295
        vf_explained_var: 0.060997262597084045
        vf_loss: 21.170631408691406
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5555484890937805
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008396606426686049
        model: {}
        policy_loss: -0.0028558429330587387
        total_loss: -0.0017525008879601955
        vf_explained_var: 0.08054350316524506
        vf_loss: 20.811092376708984
    load_time_ms: 28287.762
    num_steps_sampled: 33312000
    num_steps_trained: 33312000
    sample_time_ms: 99670.103
    update_time_ms: 23.445
  iterations_since_restore: 327
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.981443298969076
    ram_util_percent: 20.161855670103094
  pid: 4061
  policy_reward_max:
    agent-0: 198.99999999999974
    agent-1: 198.99999999999974
    agent-2: 198.99999999999974
    agent-3: 198.99999999999974
    agent-4: 198.99999999999974
    agent-5: 198.99999999999974
  policy_reward_mean:
    agent-0: 171.77833333333317
    agent-1: 171.77833333333317
    agent-2: 171.77833333333317
    agent-3: 171.77833333333317
    agent-4: 171.77833333333317
    agent-5: 171.77833333333317
  policy_reward_min:
    agent-0: 89.6666666666668
    agent-1: 89.6666666666668
    agent-2: 89.6666666666668
    agent-3: 89.6666666666668
    agent-4: 89.6666666666668
    agent-5: 89.6666666666668
  sampler_perf:
    mean_env_wait_ms: 24.17578794927445
    mean_inference_ms: 12.31066901089577
    mean_processing_ms: 50.86024312260315
  time_since_restore: 42544.47489237785
  time_this_iter_s: 136.31566214561462
  time_total_s: 45755.538578510284
  timestamp: 1637060125
  timesteps_since_restore: 31392000
  timesteps_this_iter: 96000
  timesteps_total: 33312000
  training_iteration: 347
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 37.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    347 |          45755.5 | 33312000 |  1030.67 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 39
    apples_agent-0_mean: 1.19
    apples_agent-0_min: 0
    apples_agent-1_max: 77
    apples_agent-1_mean: 19.12
    apples_agent-1_min: 0
    apples_agent-2_max: 194
    apples_agent-2_mean: 20.83
    apples_agent-2_min: 0
    apples_agent-3_max: 184
    apples_agent-3_mean: 57.11
    apples_agent-3_min: 24
    apples_agent-4_max: 46
    apples_agent-4_mean: 0.94
    apples_agent-4_min: 0
    apples_agent-5_max: 136
    apples_agent-5_mean: 91.05
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 549
    cleaning_beam_agent-0_mean: 443.01
    cleaning_beam_agent-0_min: 358
    cleaning_beam_agent-1_max: 462
    cleaning_beam_agent-1_mean: 284.68
    cleaning_beam_agent-1_min: 101
    cleaning_beam_agent-2_max: 584
    cleaning_beam_agent-2_mean: 381.74
    cleaning_beam_agent-2_min: 177
    cleaning_beam_agent-3_max: 61
    cleaning_beam_agent-3_mean: 19.59
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 553
    cleaning_beam_agent-4_mean: 440.36
    cleaning_beam_agent-4_min: 304
    cleaning_beam_agent-5_max: 648
    cleaning_beam_agent-5_mean: 47.75
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-58-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1199.0000000000018
  episode_reward_mean: 1033.6499999999924
  episode_reward_min: 577.0000000000081
  episodes_this_iter: 96
  episodes_total: 33408
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20684.576
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8408018350601196
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016124558169394732
        model: {}
        policy_loss: -0.0026399269700050354
        total_loss: -0.0020080162212252617
        vf_explained_var: -0.00374564528465271
        vf_loss: 21.11721420288086
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1363563537597656
        entropy_coeff: 0.0017600000137463212
        kl: 0.001300549367442727
        model: {}
        policy_loss: -0.0036015803925693035
        total_loss: -0.003474133089184761
        vf_explained_var: -0.009598016738891602
        vf_loss: 21.274328231811523
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1153351068496704
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015371870249509811
        model: {}
        policy_loss: -0.0034097707830369473
        total_loss: -0.003341543022543192
        vf_explained_var: 0.03564724326133728
        vf_loss: 20.312170028686523
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.368010014295578
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013216838706284761
        model: {}
        policy_loss: -0.0020896978676319122
        total_loss: -0.0007043927907943726
        vf_explained_var: 0.032952532172203064
        vf_loss: 20.330080032348633
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8866925835609436
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018881377764046192
        model: {}
        policy_loss: -0.003942691255360842
        total_loss: -0.003479211125522852
        vf_explained_var: 0.0374857634305954
        vf_loss: 20.24056625366211
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5622433423995972
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008645437192171812
        model: {}
        policy_loss: -0.002347377361729741
        total_loss: -0.0013338278513401747
        vf_explained_var: 0.050351887941360474
        vf_loss: 20.030986785888672
    load_time_ms: 30579.248
    num_steps_sampled: 33408000
    num_steps_trained: 33408000
    sample_time_ms: 99834.131
    update_time_ms: 23.737
  iterations_since_restore: 328
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.358695652173914
    ram_util_percent: 20.31260869565217
  pid: 4061
  policy_reward_max:
    agent-0: 199.83333333333286
    agent-1: 199.83333333333286
    agent-2: 199.83333333333286
    agent-3: 199.83333333333286
    agent-4: 199.83333333333286
    agent-5: 199.83333333333286
  policy_reward_mean:
    agent-0: 172.27499999999986
    agent-1: 172.27499999999986
    agent-2: 172.27499999999986
    agent-3: 172.27499999999986
    agent-4: 172.27499999999986
    agent-5: 172.27499999999986
  policy_reward_min:
    agent-0: 96.1666666666669
    agent-1: 96.1666666666669
    agent-2: 96.1666666666669
    agent-3: 96.1666666666669
    agent-4: 96.1666666666669
    agent-5: 96.1666666666669
  sampler_perf:
    mean_env_wait_ms: 24.184681758216986
    mean_inference_ms: 12.312969565029121
    mean_processing_ms: 50.8720582246013
  time_since_restore: 42705.56844139099
  time_this_iter_s: 161.09354901313782
  time_total_s: 45916.63212752342
  timestamp: 1637060286
  timesteps_since_restore: 31488000
  timesteps_this_iter: 96000
  timesteps_total: 33408000
  training_iteration: 348
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 33.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    348 |          45916.6 | 33408000 |  1033.65 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 27
    apples_agent-0_mean: 1.22
    apples_agent-0_min: 0
    apples_agent-1_max: 90
    apples_agent-1_mean: 22.11
    apples_agent-1_min: 0
    apples_agent-2_max: 108
    apples_agent-2_mean: 17.48
    apples_agent-2_min: 0
    apples_agent-3_max: 124
    apples_agent-3_mean: 61.33
    apples_agent-3_min: 31
    apples_agent-4_max: 72
    apples_agent-4_mean: 1.84
    apples_agent-4_min: 0
    apples_agent-5_max: 162
    apples_agent-5_mean: 96.58
    apples_agent-5_min: 36
    cleaning_beam_agent-0_max: 567
    cleaning_beam_agent-0_mean: 445.34
    cleaning_beam_agent-0_min: 266
    cleaning_beam_agent-1_max: 477
    cleaning_beam_agent-1_mean: 279.56
    cleaning_beam_agent-1_min: 112
    cleaning_beam_agent-2_max: 547
    cleaning_beam_agent-2_mean: 371.72
    cleaning_beam_agent-2_min: 193
    cleaning_beam_agent-3_max: 65
    cleaning_beam_agent-3_mean: 19.86
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 566
    cleaning_beam_agent-4_mean: 443.62
    cleaning_beam_agent-4_min: 268
    cleaning_beam_agent-5_max: 503
    cleaning_beam_agent-5_mean: 39.86
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-00-39
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1213.9999999999961
  episode_reward_mean: 1032.3599999999885
  episode_reward_min: 433.0000000000075
  episodes_this_iter: 96
  episodes_total: 33504
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20687.309
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8421474099159241
        entropy_coeff: 0.0017600000137463212
        kl: 0.001447636866942048
        model: {}
        policy_loss: -0.0030295485630631447
        total_loss: -0.0021635303273797035
        vf_explained_var: 0.03722845017910004
        vf_loss: 23.481969833374023
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1330974102020264
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013729719212278724
        model: {}
        policy_loss: -0.0036624909844249487
        total_loss: -0.003183920169249177
        vf_explained_var: -0.011561602354049683
        vf_loss: 24.72820281982422
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1159459352493286
        entropy_coeff: 0.0017600000137463212
        kl: 0.001382562331855297
        model: {}
        policy_loss: -0.003255816176533699
        total_loss: -0.002877507358789444
        vf_explained_var: 0.04167681932449341
        vf_loss: 23.423749923706055
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38488757610321045
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008835984044708312
        model: {}
        policy_loss: -0.00239395909011364
        total_loss: -0.0007985914126038551
        vf_explained_var: 0.0684712678194046
        vf_loss: 22.727731704711914
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8873463273048401
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019151453161612153
        model: {}
        policy_loss: -0.0038466351106762886
        total_loss: -0.003160094376653433
        vf_explained_var: 0.08093370497226715
        vf_loss: 22.482723236083984
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5561548471450806
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012965532951056957
        model: {}
        policy_loss: -0.002999700605869293
        total_loss: -0.0018700920045375824
        vf_explained_var: 0.13716167211532593
        vf_loss: 21.084409713745117
    load_time_ms: 30764.743
    num_steps_sampled: 33504000
    num_steps_trained: 33504000
    sample_time_ms: 99798.186
    update_time_ms: 23.6
  iterations_since_restore: 329
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.26009174311926
    ram_util_percent: 19.486238532110093
  pid: 4061
  policy_reward_max:
    agent-0: 202.33333333333346
    agent-1: 202.33333333333346
    agent-2: 202.33333333333346
    agent-3: 202.33333333333346
    agent-4: 202.33333333333346
    agent-5: 202.33333333333346
  policy_reward_mean:
    agent-0: 172.05999999999975
    agent-1: 172.05999999999975
    agent-2: 172.05999999999975
    agent-3: 172.05999999999975
    agent-4: 172.05999999999975
    agent-5: 172.05999999999975
  policy_reward_min:
    agent-0: 72.1666666666667
    agent-1: 72.1666666666667
    agent-2: 72.1666666666667
    agent-3: 72.1666666666667
    agent-4: 72.1666666666667
    agent-5: 72.1666666666667
  sampler_perf:
    mean_env_wait_ms: 24.192142830184515
    mean_inference_ms: 12.314931648526986
    mean_processing_ms: 50.881598408122315
  time_since_restore: 42858.48993897438
  time_this_iter_s: 152.92149758338928
  time_total_s: 46069.55362510681
  timestamp: 1637060439
  timesteps_since_restore: 31584000
  timesteps_this_iter: 96000
  timesteps_total: 33504000
  training_iteration: 349
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 33.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    349 |          46069.6 | 33504000 |  1032.36 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 42
    apples_agent-0_mean: 1.55
    apples_agent-0_min: 0
    apples_agent-1_max: 82
    apples_agent-1_mean: 21.73
    apples_agent-1_min: 0
    apples_agent-2_max: 291
    apples_agent-2_mean: 18.41
    apples_agent-2_min: 0
    apples_agent-3_max: 145
    apples_agent-3_mean: 60.53
    apples_agent-3_min: 30
    apples_agent-4_max: 49
    apples_agent-4_mean: 2.38
    apples_agent-4_min: 0
    apples_agent-5_max: 399
    apples_agent-5_mean: 101.4
    apples_agent-5_min: 23
    cleaning_beam_agent-0_max: 602
    cleaning_beam_agent-0_mean: 441.92
    cleaning_beam_agent-0_min: 336
    cleaning_beam_agent-1_max: 430
    cleaning_beam_agent-1_mean: 278.81
    cleaning_beam_agent-1_min: 122
    cleaning_beam_agent-2_max: 583
    cleaning_beam_agent-2_mean: 380.71
    cleaning_beam_agent-2_min: 196
    cleaning_beam_agent-3_max: 86
    cleaning_beam_agent-3_mean: 20.32
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 541
    cleaning_beam_agent-4_mean: 444.86
    cleaning_beam_agent-4_min: 284
    cleaning_beam_agent-5_max: 207
    cleaning_beam_agent-5_mean: 35.48
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-03-17
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1171.9999999999998
  episode_reward_mean: 1018.2399999999893
  episode_reward_min: 287.9999999999992
  episodes_this_iter: 96
  episodes_total: 33600
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20674.451
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8341250419616699
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014224438928067684
        model: {}
        policy_loss: -0.0028620520606637
        total_loss: -0.002031319309026003
        vf_explained_var: 0.04580758512020111
        vf_loss: 22.987926483154297
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1369211673736572
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013527090195566416
        model: {}
        policy_loss: -0.003541051410138607
        total_loss: -0.00302094011567533
        vf_explained_var: -0.04586973786354065
        vf_loss: 25.21091079711914
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1178723573684692
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013269433984532952
        model: {}
        policy_loss: -0.003119372995570302
        total_loss: -0.002697055460885167
        vf_explained_var: 0.008988291025161743
        vf_loss: 23.89773941040039
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39301374554634094
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013474291190505028
        model: {}
        policy_loss: -0.0024887400213629007
        total_loss: -0.0009608219843357801
        vf_explained_var: 0.07903727889060974
        vf_loss: 22.19625473022461
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8909209370613098
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017395762261003256
        model: {}
        policy_loss: -0.003688385244458914
        total_loss: -0.003019975032657385
        vf_explained_var: 0.07275386154651642
        vf_loss: 22.364322662353516
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5577098727226257
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008091242052614689
        model: {}
        policy_loss: -0.0030320100486278534
        total_loss: -0.0018584663048386574
        vf_explained_var: 0.1061621755361557
        vf_loss: 21.551128387451172
    load_time_ms: 30122.83
    num_steps_sampled: 33600000
    num_steps_trained: 33600000
    sample_time_ms: 99670.35
    update_time_ms: 23.748
  iterations_since_restore: 330
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.487999999999996
    ram_util_percent: 20.551555555555556
  pid: 4061
  policy_reward_max:
    agent-0: 195.3333333333331
    agent-1: 195.3333333333331
    agent-2: 195.3333333333331
    agent-3: 195.3333333333331
    agent-4: 195.3333333333331
    agent-5: 195.3333333333331
  policy_reward_mean:
    agent-0: 169.7066666666665
    agent-1: 169.7066666666665
    agent-2: 169.7066666666665
    agent-3: 169.7066666666665
    agent-4: 169.7066666666665
    agent-5: 169.7066666666665
  policy_reward_min:
    agent-0: 47.99999999999995
    agent-1: 47.99999999999995
    agent-2: 47.99999999999995
    agent-3: 47.99999999999995
    agent-4: 47.99999999999995
    agent-5: 47.99999999999995
  sampler_perf:
    mean_env_wait_ms: 24.200601187293238
    mean_inference_ms: 12.317291784770573
    mean_processing_ms: 50.89181063249281
  time_since_restore: 43016.084349393845
  time_this_iter_s: 157.5944104194641
  time_total_s: 46227.148035526276
  timestamp: 1637060597
  timesteps_since_restore: 31680000
  timesteps_this_iter: 96000
  timesteps_total: 33600000
  training_iteration: 350
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 38.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    350 |          46227.1 | 33600000 |  1018.24 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 38
    apples_agent-0_mean: 1.63
    apples_agent-0_min: 0
    apples_agent-1_max: 76
    apples_agent-1_mean: 20.22
    apples_agent-1_min: 0
    apples_agent-2_max: 152
    apples_agent-2_mean: 12.4
    apples_agent-2_min: 0
    apples_agent-3_max: 124
    apples_agent-3_mean: 62.06
    apples_agent-3_min: 33
    apples_agent-4_max: 69
    apples_agent-4_mean: 1.34
    apples_agent-4_min: 0
    apples_agent-5_max: 353
    apples_agent-5_mean: 101.91
    apples_agent-5_min: 10
    cleaning_beam_agent-0_max: 572
    cleaning_beam_agent-0_mean: 437.6
    cleaning_beam_agent-0_min: 329
    cleaning_beam_agent-1_max: 461
    cleaning_beam_agent-1_mean: 294.23
    cleaning_beam_agent-1_min: 132
    cleaning_beam_agent-2_max: 583
    cleaning_beam_agent-2_mean: 390.35
    cleaning_beam_agent-2_min: 176
    cleaning_beam_agent-3_max: 69
    cleaning_beam_agent-3_mean: 17.47
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 539
    cleaning_beam_agent-4_mean: 431.31
    cleaning_beam_agent-4_min: 322
    cleaning_beam_agent-5_max: 356
    cleaning_beam_agent-5_mean: 42.34
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-06-00
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1209.0000000000048
  episode_reward_mean: 1039.5399999999922
  episode_reward_min: 525.0000000000101
  episodes_this_iter: 96
  episodes_total: 33696
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20637.347
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.84372878074646
        entropy_coeff: 0.0017600000137463212
        kl: 0.001282086013816297
        model: {}
        policy_loss: -0.0030302563682198524
        total_loss: -0.0022597634233534336
        vf_explained_var: 0.0338401198387146
        vf_loss: 22.554492950439453
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.137871265411377
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016772549133747816
        model: {}
        policy_loss: -0.00413518026471138
        total_loss: -0.003788312431424856
        vf_explained_var: -0.004991978406906128
        vf_loss: 23.495182037353516
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1317620277404785
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014178126584738493
        model: {}
        policy_loss: -0.003019146155565977
        total_loss: -0.002716030227020383
        vf_explained_var: 0.015831291675567627
        vf_loss: 22.950176239013672
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3657950162887573
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008053994388319552
        model: {}
        policy_loss: -0.0020616184920072556
        total_loss: -0.0005631009116768837
        vf_explained_var: 0.08448618650436401
        vf_loss: 21.42317771911621
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9040315747261047
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016791848465800285
        model: {}
        policy_loss: -0.003644252195954323
        total_loss: -0.0030552579555660486
        vf_explained_var: 0.06593331694602966
        vf_loss: 21.80089569091797
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5379931330680847
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009037853451445699
        model: {}
        policy_loss: -0.0025633974000811577
        total_loss: -0.0013717503752559423
        vf_explained_var: 0.08684995770454407
        vf_loss: 21.385169982910156
    load_time_ms: 29639.416
    num_steps_sampled: 33696000
    num_steps_trained: 33696000
    sample_time_ms: 99965.295
    update_time_ms: 22.694
  iterations_since_restore: 331
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.468534482758617
    ram_util_percent: 20.709913793103446
  pid: 4061
  policy_reward_max:
    agent-0: 201.50000000000017
    agent-1: 201.50000000000017
    agent-2: 201.50000000000017
    agent-3: 201.50000000000017
    agent-4: 201.50000000000017
    agent-5: 201.50000000000017
  policy_reward_mean:
    agent-0: 173.25666666666658
    agent-1: 173.25666666666658
    agent-2: 173.25666666666658
    agent-3: 173.25666666666658
    agent-4: 173.25666666666658
    agent-5: 173.25666666666658
  policy_reward_min:
    agent-0: 87.50000000000037
    agent-1: 87.50000000000037
    agent-2: 87.50000000000037
    agent-3: 87.50000000000037
    agent-4: 87.50000000000037
    agent-5: 87.50000000000037
  sampler_perf:
    mean_env_wait_ms: 24.209384631340164
    mean_inference_ms: 12.319741831153513
    mean_processing_ms: 50.903368870738376
  time_since_restore: 43179.080882787704
  time_this_iter_s: 162.99653339385986
  time_total_s: 46390.144568920135
  timestamp: 1637060760
  timesteps_since_restore: 31776000
  timesteps_this_iter: 96000
  timesteps_total: 33696000
  training_iteration: 351
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 39.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    351 |          46390.1 | 33696000 |  1039.54 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 28
    apples_agent-0_mean: 1.19
    apples_agent-0_min: 0
    apples_agent-1_max: 87
    apples_agent-1_mean: 19.29
    apples_agent-1_min: 0
    apples_agent-2_max: 152
    apples_agent-2_mean: 19.46
    apples_agent-2_min: 0
    apples_agent-3_max: 103
    apples_agent-3_mean: 55.74
    apples_agent-3_min: 27
    apples_agent-4_max: 65
    apples_agent-4_mean: 0.88
    apples_agent-4_min: 0
    apples_agent-5_max: 142
    apples_agent-5_mean: 94.98
    apples_agent-5_min: 37
    cleaning_beam_agent-0_max: 536
    cleaning_beam_agent-0_mean: 432.96
    cleaning_beam_agent-0_min: 311
    cleaning_beam_agent-1_max: 434
    cleaning_beam_agent-1_mean: 288.55
    cleaning_beam_agent-1_min: 76
    cleaning_beam_agent-2_max: 598
    cleaning_beam_agent-2_mean: 357.0
    cleaning_beam_agent-2_min: 179
    cleaning_beam_agent-3_max: 150
    cleaning_beam_agent-3_mean: 21.3
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 535
    cleaning_beam_agent-4_mean: 426.38
    cleaning_beam_agent-4_min: 332
    cleaning_beam_agent-5_max: 538
    cleaning_beam_agent-5_mean: 43.71
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-08-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1190.999999999994
  episode_reward_mean: 1024.3499999999904
  episode_reward_min: 609.0000000000111
  episodes_this_iter: 96
  episodes_total: 33792
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20587.806
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8503849506378174
        entropy_coeff: 0.0017600000137463212
        kl: 0.001457505626603961
        model: {}
        policy_loss: -0.0028260257095098495
        total_loss: -0.0022112152073532343
        vf_explained_var: 0.02890115976333618
        vf_loss: 21.114864349365234
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1418946981430054
        entropy_coeff: 0.0017600000137463212
        kl: 0.001621975563466549
        model: {}
        policy_loss: -0.003850660054013133
        total_loss: -0.003651315812021494
        vf_explained_var: -0.015850991010665894
        vf_loss: 22.090791702270508
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1219515800476074
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014597696717828512
        model: {}
        policy_loss: -0.0034199105575680733
        total_loss: -0.0032494491897523403
        vf_explained_var: 0.016301721334457397
        vf_loss: 21.450984954833984
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3814617097377777
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009219373459927738
        model: {}
        policy_loss: -0.0018270984292030334
        total_loss: -0.00043804431334137917
        vf_explained_var: 0.05320720374584198
        vf_loss: 20.60424041748047
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.89509516954422
        entropy_coeff: 0.0017600000137463212
        kl: 0.002679546596482396
        model: {}
        policy_loss: -0.004144434817135334
        total_loss: -0.0036251337733119726
        vf_explained_var: 0.03639853000640869
        vf_loss: 20.946672439575195
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.56166011095047
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007212265627458692
        model: {}
        policy_loss: -0.00252933450974524
        total_loss: -0.0015453789383172989
        vf_explained_var: 0.09359502792358398
        vf_loss: 19.724769592285156
    load_time_ms: 28641.664
    num_steps_sampled: 33792000
    num_steps_trained: 33792000
    sample_time_ms: 99873.033
    update_time_ms: 21.856
  iterations_since_restore: 332
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.660273972602738
    ram_util_percent: 20.78767123287671
  pid: 4061
  policy_reward_max:
    agent-0: 198.49999999999994
    agent-1: 198.49999999999994
    agent-2: 198.49999999999994
    agent-3: 198.49999999999994
    agent-4: 198.49999999999994
    agent-5: 198.49999999999994
  policy_reward_mean:
    agent-0: 170.7249999999998
    agent-1: 170.7249999999998
    agent-2: 170.7249999999998
    agent-3: 170.7249999999998
    agent-4: 170.7249999999998
    agent-5: 170.7249999999998
  policy_reward_min:
    agent-0: 101.5000000000001
    agent-1: 101.5000000000001
    agent-2: 101.5000000000001
    agent-3: 101.5000000000001
    agent-4: 101.5000000000001
    agent-5: 101.5000000000001
  sampler_perf:
    mean_env_wait_ms: 24.216764290074572
    mean_inference_ms: 12.321831792320149
    mean_processing_ms: 50.91260453399758
  time_since_restore: 43332.378272295
  time_this_iter_s: 153.2973895072937
  time_total_s: 46543.44195842743
  timestamp: 1637060914
  timesteps_since_restore: 31872000
  timesteps_this_iter: 96000
  timesteps_total: 33792000
  training_iteration: 352
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 39.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    352 |          46543.4 | 33792000 |  1024.35 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 1.35
    apples_agent-0_min: 0
    apples_agent-1_max: 104
    apples_agent-1_mean: 22.94
    apples_agent-1_min: 0
    apples_agent-2_max: 160
    apples_agent-2_mean: 14.07
    apples_agent-2_min: 0
    apples_agent-3_max: 137
    apples_agent-3_mean: 59.33
    apples_agent-3_min: 27
    apples_agent-4_max: 42
    apples_agent-4_mean: 1.45
    apples_agent-4_min: 0
    apples_agent-5_max: 158
    apples_agent-5_mean: 97.78
    apples_agent-5_min: 50
    cleaning_beam_agent-0_max: 556
    cleaning_beam_agent-0_mean: 429.87
    cleaning_beam_agent-0_min: 299
    cleaning_beam_agent-1_max: 445
    cleaning_beam_agent-1_mean: 278.92
    cleaning_beam_agent-1_min: 114
    cleaning_beam_agent-2_max: 540
    cleaning_beam_agent-2_mean: 375.23
    cleaning_beam_agent-2_min: 185
    cleaning_beam_agent-3_max: 85
    cleaning_beam_agent-3_mean: 20.89
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 531
    cleaning_beam_agent-4_mean: 444.01
    cleaning_beam_agent-4_min: 301
    cleaning_beam_agent-5_max: 317
    cleaning_beam_agent-5_mean: 39.41
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-11-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1186.9999999999782
  episode_reward_mean: 1046.6499999999899
  episode_reward_min: 628.0000000000025
  episodes_this_iter: 96
  episodes_total: 33888
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20556.649
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8472344279289246
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014903226401656866
        model: {}
        policy_loss: -0.0026149065233767033
        total_loss: -0.0019885655492544174
        vf_explained_var: 0.04253123700618744
        vf_loss: 21.174745559692383
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1329643726348877
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014729605754837394
        model: {}
        policy_loss: -0.0037851850502192974
        total_loss: -0.0035235576797276735
        vf_explained_var: -0.009330809116363525
        vf_loss: 22.556461334228516
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.140967607498169
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011311033740639687
        model: {}
        policy_loss: -0.00309002841822803
        total_loss: -0.002913296688348055
        vf_explained_var: 0.020228534936904907
        vf_loss: 21.848360061645508
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37676846981048584
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012357726227492094
        model: {}
        policy_loss: -0.0020848577842116356
        total_loss: -0.0007084456738084555
        vf_explained_var: 0.0811232179403305
        vf_loss: 20.395244598388672
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8925569653511047
        entropy_coeff: 0.0017600000137463212
        kl: 0.001668707700446248
        model: {}
        policy_loss: -0.003472973359748721
        total_loss: -0.003012533765286207
        vf_explained_var: 0.08218666911125183
        vf_loss: 20.313425064086914
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5448974370956421
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010651074117049575
        model: {}
        policy_loss: -0.0025230743922293186
        total_loss: -0.0014383236411958933
        vf_explained_var: 0.07754938304424286
        vf_loss: 20.43771743774414
    load_time_ms: 29664.307
    num_steps_sampled: 33888000
    num_steps_trained: 33888000
    sample_time_ms: 100000.266
    update_time_ms: 21.981
  iterations_since_restore: 333
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.089711934156384
    ram_util_percent: 21.70164609053498
  pid: 4061
  policy_reward_max:
    agent-0: 197.83333333333331
    agent-1: 197.83333333333331
    agent-2: 197.83333333333331
    agent-3: 197.83333333333331
    agent-4: 197.83333333333331
    agent-5: 197.83333333333331
  policy_reward_mean:
    agent-0: 174.44166666666652
    agent-1: 174.44166666666652
    agent-2: 174.44166666666652
    agent-3: 174.44166666666652
    agent-4: 174.44166666666652
    agent-5: 174.44166666666652
  policy_reward_min:
    agent-0: 104.66666666666696
    agent-1: 104.66666666666696
    agent-2: 104.66666666666696
    agent-3: 104.66666666666696
    agent-4: 104.66666666666696
    agent-5: 104.66666666666696
  sampler_perf:
    mean_env_wait_ms: 24.22474044108339
    mean_inference_ms: 12.324004759136699
    mean_processing_ms: 50.92323653034302
  time_since_restore: 43502.64626479149
  time_this_iter_s: 170.26799249649048
  time_total_s: 46713.70995092392
  timestamp: 1637061084
  timesteps_since_restore: 31968000
  timesteps_this_iter: 96000
  timesteps_total: 33888000
  training_iteration: 353
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 39.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    353 |          46713.7 | 33888000 |  1046.65 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 0.57
    apples_agent-0_min: 0
    apples_agent-1_max: 73
    apples_agent-1_mean: 20.45
    apples_agent-1_min: 0
    apples_agent-2_max: 184
    apples_agent-2_mean: 22.08
    apples_agent-2_min: 0
    apples_agent-3_max: 125
    apples_agent-3_mean: 60.58
    apples_agent-3_min: 21
    apples_agent-4_max: 84
    apples_agent-4_mean: 1.69
    apples_agent-4_min: 0
    apples_agent-5_max: 196
    apples_agent-5_mean: 96.04
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 545
    cleaning_beam_agent-0_mean: 429.04
    cleaning_beam_agent-0_min: 310
    cleaning_beam_agent-1_max: 454
    cleaning_beam_agent-1_mean: 295.54
    cleaning_beam_agent-1_min: 144
    cleaning_beam_agent-2_max: 510
    cleaning_beam_agent-2_mean: 355.89
    cleaning_beam_agent-2_min: 147
    cleaning_beam_agent-3_max: 103
    cleaning_beam_agent-3_mean: 22.19
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 550
    cleaning_beam_agent-4_mean: 444.92
    cleaning_beam_agent-4_min: 274
    cleaning_beam_agent-5_max: 576
    cleaning_beam_agent-5_mean: 41.41
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-13-54
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1188.0000000000016
  episode_reward_mean: 1031.2399999999893
  episode_reward_min: 607.9999999999997
  episodes_this_iter: 96
  episodes_total: 33984
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20577.882
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8567632436752319
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012370373588055372
        model: {}
        policy_loss: -0.002567351795732975
        total_loss: -0.0019390005618333817
        vf_explained_var: 0.013985231518745422
        vf_loss: 21.362550735473633
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.129623532295227
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012927664211019874
        model: {}
        policy_loss: -0.003938470501452684
        total_loss: -0.00372126791626215
        vf_explained_var: -0.016514599323272705
        vf_loss: 22.053428649902344
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1234016418457031
        entropy_coeff: 0.0017600000137463212
        kl: 0.001006014528684318
        model: {}
        policy_loss: -0.003126072697341442
        total_loss: -0.002955464646220207
        vf_explained_var: 0.009846523404121399
        vf_loss: 21.47796058654785
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38425213098526
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010869167745113373
        model: {}
        policy_loss: -0.002188934711739421
        total_loss: -0.0008719642646610737
        vf_explained_var: 0.08239582180976868
        vf_loss: 19.932525634765625
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8932693004608154
        entropy_coeff: 0.0017600000137463212
        kl: 0.00215469510294497
        model: {}
        policy_loss: -0.0036097345873713493
        total_loss: -0.003095364198088646
        vf_explained_var: 0.036520376801490784
        vf_loss: 20.86526107788086
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5469686985015869
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008964561275206506
        model: {}
        policy_loss: -0.0027075475081801414
        total_loss: -0.0017213420942425728
        vf_explained_var: 0.10192634165287018
        vf_loss: 19.488666534423828
    load_time_ms: 30942.368
    num_steps_sampled: 33984000
    num_steps_trained: 33984000
    sample_time_ms: 100159.305
    update_time_ms: 20.686
  iterations_since_restore: 334
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.285915492957752
    ram_util_percent: 21.565727699530516
  pid: 4061
  policy_reward_max:
    agent-0: 197.99999999999991
    agent-1: 197.99999999999991
    agent-2: 197.99999999999991
    agent-3: 197.99999999999991
    agent-4: 197.99999999999991
    agent-5: 197.99999999999991
  policy_reward_mean:
    agent-0: 171.87333333333322
    agent-1: 171.87333333333322
    agent-2: 171.87333333333322
    agent-3: 171.87333333333322
    agent-4: 171.87333333333322
    agent-5: 171.87333333333322
  policy_reward_min:
    agent-0: 101.33333333333339
    agent-1: 101.33333333333339
    agent-2: 101.33333333333339
    agent-3: 101.33333333333339
    agent-4: 101.33333333333339
    agent-5: 101.33333333333339
  sampler_perf:
    mean_env_wait_ms: 24.23220503197399
    mean_inference_ms: 12.325931236247275
    mean_processing_ms: 50.933142373777684
  time_since_restore: 43652.29856824875
  time_this_iter_s: 149.65230345726013
  time_total_s: 46863.36225438118
  timestamp: 1637061234
  timesteps_since_restore: 32064000
  timesteps_this_iter: 96000
  timesteps_total: 33984000
  training_iteration: 354
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 39.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    354 |          46863.4 | 33984000 |  1031.24 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 46
    apples_agent-0_mean: 1.79
    apples_agent-0_min: 0
    apples_agent-1_max: 62
    apples_agent-1_mean: 19.33
    apples_agent-1_min: 0
    apples_agent-2_max: 145
    apples_agent-2_mean: 13.94
    apples_agent-2_min: 0
    apples_agent-3_max: 366
    apples_agent-3_mean: 61.87
    apples_agent-3_min: 31
    apples_agent-4_max: 33
    apples_agent-4_mean: 0.41
    apples_agent-4_min: 0
    apples_agent-5_max: 382
    apples_agent-5_mean: 102.3
    apples_agent-5_min: 36
    cleaning_beam_agent-0_max: 521
    cleaning_beam_agent-0_mean: 418.14
    cleaning_beam_agent-0_min: 289
    cleaning_beam_agent-1_max: 419
    cleaning_beam_agent-1_mean: 288.1
    cleaning_beam_agent-1_min: 102
    cleaning_beam_agent-2_max: 553
    cleaning_beam_agent-2_mean: 363.93
    cleaning_beam_agent-2_min: 179
    cleaning_beam_agent-3_max: 71
    cleaning_beam_agent-3_mean: 20.09
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 571
    cleaning_beam_agent-4_mean: 453.38
    cleaning_beam_agent-4_min: 363
    cleaning_beam_agent-5_max: 357
    cleaning_beam_agent-5_mean: 35.66
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-16-10
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1212.0000000000045
  episode_reward_mean: 1037.1399999999894
  episode_reward_min: 682.9999999999994
  episodes_this_iter: 96
  episodes_total: 34080
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20636.325
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8712427020072937
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013790130615234375
        model: {}
        policy_loss: -0.00328847486525774
        total_loss: -0.002604359295219183
        vf_explained_var: 0.014064803719520569
        vf_loss: 22.175006866455078
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1233769655227661
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017224516486749053
        model: {}
        policy_loss: -0.0037193680182099342
        total_loss: -0.003372219391167164
        vf_explained_var: -0.032403528690338135
        vf_loss: 23.2429141998291
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1424909830093384
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017565189627930522
        model: {}
        policy_loss: -0.0032440745271742344
        total_loss: -0.0030832039192318916
        vf_explained_var: 0.033668309450149536
        vf_loss: 21.716575622558594
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3772386312484741
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009583453647792339
        model: {}
        policy_loss: -0.0019312440417706966
        total_loss: -0.000535682775080204
        vf_explained_var: 0.0842752605676651
        vf_loss: 20.594985961914062
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8773102760314941
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017605348257347941
        model: {}
        policy_loss: -0.0037004868499934673
        total_loss: -0.003110774327069521
        vf_explained_var: 0.05060406029224396
        vf_loss: 21.337783813476562
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5415204167366028
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012084129266440868
        model: {}
        policy_loss: -0.0027329737786203623
        total_loss: -0.0016268750187009573
        vf_explained_var: 0.08403770625591278
        vf_loss: 20.59174156188965
    load_time_ms: 30853.337
    num_steps_sampled: 34080000
    num_steps_trained: 34080000
    sample_time_ms: 100032.582
    update_time_ms: 20.953
  iterations_since_restore: 335
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.688659793814434
    ram_util_percent: 20.320103092783505
  pid: 4061
  policy_reward_max:
    agent-0: 202.00000000000034
    agent-1: 202.00000000000034
    agent-2: 202.00000000000034
    agent-3: 202.00000000000034
    agent-4: 202.00000000000034
    agent-5: 202.00000000000034
  policy_reward_mean:
    agent-0: 172.85666666666657
    agent-1: 172.85666666666657
    agent-2: 172.85666666666657
    agent-3: 172.85666666666657
    agent-4: 172.85666666666657
    agent-5: 172.85666666666657
  policy_reward_min:
    agent-0: 113.83333333333343
    agent-1: 113.83333333333343
    agent-2: 113.83333333333343
    agent-3: 113.83333333333343
    agent-4: 113.83333333333343
    agent-5: 113.83333333333343
  sampler_perf:
    mean_env_wait_ms: 24.239633107341103
    mean_inference_ms: 12.327787138800975
    mean_processing_ms: 50.9432889383755
  time_since_restore: 43787.885848760605
  time_this_iter_s: 135.58728051185608
  time_total_s: 46998.949534893036
  timestamp: 1637061370
  timesteps_since_restore: 32160000
  timesteps_this_iter: 96000
  timesteps_total: 34080000
  training_iteration: 355
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 40.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    355 |          46998.9 | 34080000 |  1037.14 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 35
    apples_agent-0_mean: 1.8
    apples_agent-0_min: 0
    apples_agent-1_max: 94
    apples_agent-1_mean: 24.34
    apples_agent-1_min: 0
    apples_agent-2_max: 117
    apples_agent-2_mean: 16.25
    apples_agent-2_min: 0
    apples_agent-3_max: 112
    apples_agent-3_mean: 56.21
    apples_agent-3_min: 30
    apples_agent-4_max: 109
    apples_agent-4_mean: 2.41
    apples_agent-4_min: 0
    apples_agent-5_max: 200
    apples_agent-5_mean: 100.44
    apples_agent-5_min: 37
    cleaning_beam_agent-0_max: 546
    cleaning_beam_agent-0_mean: 412.45
    cleaning_beam_agent-0_min: 287
    cleaning_beam_agent-1_max: 453
    cleaning_beam_agent-1_mean: 290.41
    cleaning_beam_agent-1_min: 141
    cleaning_beam_agent-2_max: 520
    cleaning_beam_agent-2_mean: 345.64
    cleaning_beam_agent-2_min: 144
    cleaning_beam_agent-3_max: 122
    cleaning_beam_agent-3_mean: 20.92
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 596
    cleaning_beam_agent-4_mean: 447.98
    cleaning_beam_agent-4_min: 298
    cleaning_beam_agent-5_max: 378
    cleaning_beam_agent-5_mean: 34.14
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-18-48
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1202.9999999999998
  episode_reward_mean: 1038.389999999991
  episode_reward_min: 429.0000000000104
  episodes_this_iter: 96
  episodes_total: 34176
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20648.1
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8685295581817627
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018164910143241286
        model: {}
        policy_loss: -0.0028168894350528717
        total_loss: -0.002023345325142145
        vf_explained_var: 0.028526678681373596
        vf_loss: 23.22158432006836
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1454381942749023
        entropy_coeff: 0.0017600000137463212
        kl: 0.00109796691685915
        model: {}
        policy_loss: -0.003722837194800377
        total_loss: -0.003328487742692232
        vf_explained_var: -0.004573017358779907
        vf_loss: 24.103235244750977
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.142234444618225
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018349899910390377
        model: {}
        policy_loss: -0.003393653780221939
        total_loss: -0.003087339922785759
        vf_explained_var: 0.03290434181690216
        vf_loss: 23.16646957397461
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37697523832321167
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010847784578800201
        model: {}
        policy_loss: -0.002141868229955435
        total_loss: -0.000620656181126833
        vf_explained_var: 0.09142179787158966
        vf_loss: 21.846878051757812
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8971985578536987
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014246134087443352
        model: {}
        policy_loss: -0.003548718523234129
        total_loss: -0.002873482648283243
        vf_explained_var: 0.05878424644470215
        vf_loss: 22.543067932128906
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5657367706298828
        entropy_coeff: 0.0017600000137463212
        kl: 0.001319674076512456
        model: {}
        policy_loss: -0.0032880096696317196
        total_loss: -0.0021217497996985912
        vf_explained_var: 0.09948642551898956
        vf_loss: 21.61957550048828
    load_time_ms: 33192.044
    num_steps_sampled: 34176000
    num_steps_trained: 34176000
    sample_time_ms: 99781.172
    update_time_ms: 20.949
  iterations_since_restore: 336
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.068
    ram_util_percent: 20.210666666666665
  pid: 4061
  policy_reward_max:
    agent-0: 200.49999999999963
    agent-1: 200.49999999999963
    agent-2: 200.49999999999963
    agent-3: 200.49999999999963
    agent-4: 200.49999999999963
    agent-5: 200.49999999999963
  policy_reward_mean:
    agent-0: 173.06499999999977
    agent-1: 173.06499999999977
    agent-2: 173.06499999999977
    agent-3: 173.06499999999977
    agent-4: 173.06499999999977
    agent-5: 173.06499999999977
  policy_reward_min:
    agent-0: 71.49999999999996
    agent-1: 71.49999999999996
    agent-2: 71.49999999999996
    agent-3: 71.49999999999996
    agent-4: 71.49999999999996
    agent-5: 71.49999999999996
  sampler_perf:
    mean_env_wait_ms: 24.246177561772182
    mean_inference_ms: 12.329772966059354
    mean_processing_ms: 50.95442553434109
  time_since_restore: 43945.93522310257
  time_this_iter_s: 158.04937434196472
  time_total_s: 47156.998909235
  timestamp: 1637061528
  timesteps_since_restore: 32256000
  timesteps_this_iter: 96000
  timesteps_total: 34176000
  training_iteration: 356
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 32.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    356 |            47157 | 34176000 |  1038.39 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 22
    apples_agent-0_mean: 1.27
    apples_agent-0_min: 0
    apples_agent-1_max: 72
    apples_agent-1_mean: 21.86
    apples_agent-1_min: 0
    apples_agent-2_max: 239
    apples_agent-2_mean: 15.8
    apples_agent-2_min: 0
    apples_agent-3_max: 147
    apples_agent-3_mean: 55.01
    apples_agent-3_min: 26
    apples_agent-4_max: 42
    apples_agent-4_mean: 0.42
    apples_agent-4_min: 0
    apples_agent-5_max: 167
    apples_agent-5_mean: 100.54
    apples_agent-5_min: 5
    cleaning_beam_agent-0_max: 532
    cleaning_beam_agent-0_mean: 415.5
    cleaning_beam_agent-0_min: 328
    cleaning_beam_agent-1_max: 428
    cleaning_beam_agent-1_mean: 289.47
    cleaning_beam_agent-1_min: 134
    cleaning_beam_agent-2_max: 569
    cleaning_beam_agent-2_mean: 352.52
    cleaning_beam_agent-2_min: 189
    cleaning_beam_agent-3_max: 159
    cleaning_beam_agent-3_mean: 20.87
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 560
    cleaning_beam_agent-4_mean: 454.24
    cleaning_beam_agent-4_min: 330
    cleaning_beam_agent-5_max: 479
    cleaning_beam_agent-5_mean: 37.53
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-21-11
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1209.999999999985
  episode_reward_mean: 1043.149999999989
  episode_reward_min: 582.0000000000045
  episodes_this_iter: 96
  episodes_total: 34272
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20600.859
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8713163137435913
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017951425397768617
        model: {}
        policy_loss: -0.003010400803759694
        total_loss: -0.002402229467406869
        vf_explained_var: 0.01692788302898407
        vf_loss: 21.41687774658203
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1418589353561401
        entropy_coeff: 0.0017600000137463212
        kl: 0.001946528209373355
        model: {}
        policy_loss: -0.003650947008281946
        total_loss: -0.003440107684582472
        vf_explained_var: -0.015072077512741089
        vf_loss: 22.205127716064453
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1495237350463867
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017309124814346433
        model: {}
        policy_loss: -0.0033216034062206745
        total_loss: -0.003155916463583708
        vf_explained_var: -0.001513853669166565
        vf_loss: 21.888490676879883
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36232343316078186
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009536490542814136
        model: {}
        policy_loss: -0.0021396521478891373
        total_loss: -0.0007475358434021473
        vf_explained_var: 0.07328948378562927
        vf_loss: 20.298057556152344
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8833913207054138
        entropy_coeff: 0.0017600000137463212
        kl: 0.002388350199908018
        model: {}
        policy_loss: -0.004121582955121994
        total_loss: -0.003591093234717846
        vf_explained_var: 0.04300485551357269
        vf_loss: 20.852577209472656
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5515544414520264
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013584101106971502
        model: {}
        policy_loss: -0.00282403570599854
        total_loss: -0.0017826317343860865
        vf_explained_var: 0.07920937240123749
        vf_loss: 20.121379852294922
    load_time_ms: 34756.173
    num_steps_sampled: 34272000
    num_steps_trained: 34272000
    sample_time_ms: 98932.304
    update_time_ms: 21.158
  iterations_since_restore: 337
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 16.452941176470592
    ram_util_percent: 16.380882352941175
  pid: 4061
  policy_reward_max:
    agent-0: 201.66666666666674
    agent-1: 201.66666666666674
    agent-2: 201.66666666666674
    agent-3: 201.66666666666674
    agent-4: 201.66666666666674
    agent-5: 201.66666666666674
  policy_reward_mean:
    agent-0: 173.8583333333332
    agent-1: 173.8583333333332
    agent-2: 173.8583333333332
    agent-3: 173.8583333333332
    agent-4: 173.8583333333332
    agent-5: 173.8583333333332
  policy_reward_min:
    agent-0: 97.00000000000006
    agent-1: 97.00000000000006
    agent-2: 97.00000000000006
    agent-3: 97.00000000000006
    agent-4: 97.00000000000006
    agent-5: 97.00000000000006
  sampler_perf:
    mean_env_wait_ms: 24.249812648550495
    mean_inference_ms: 12.330138986115085
    mean_processing_ms: 50.955391245987045
  time_since_restore: 44088.91072201729
  time_this_iter_s: 142.97549891471863
  time_total_s: 47299.97440814972
  timestamp: 1637061671
  timesteps_since_restore: 32352000
  timesteps_this_iter: 96000
  timesteps_total: 34272000
  training_iteration: 357
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    357 |            47300 | 34272000 |  1043.15 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 32
    apples_agent-0_mean: 1.61
    apples_agent-0_min: 0
    apples_agent-1_max: 86
    apples_agent-1_mean: 16.76
    apples_agent-1_min: 0
    apples_agent-2_max: 89
    apples_agent-2_mean: 17.15
    apples_agent-2_min: 0
    apples_agent-3_max: 138
    apples_agent-3_mean: 55.42
    apples_agent-3_min: 25
    apples_agent-4_max: 30
    apples_agent-4_mean: 0.82
    apples_agent-4_min: 0
    apples_agent-5_max: 173
    apples_agent-5_mean: 99.04
    apples_agent-5_min: 42
    cleaning_beam_agent-0_max: 519
    cleaning_beam_agent-0_mean: 412.3
    cleaning_beam_agent-0_min: 288
    cleaning_beam_agent-1_max: 479
    cleaning_beam_agent-1_mean: 328.03
    cleaning_beam_agent-1_min: 148
    cleaning_beam_agent-2_max: 571
    cleaning_beam_agent-2_mean: 335.55
    cleaning_beam_agent-2_min: 137
    cleaning_beam_agent-3_max: 82
    cleaning_beam_agent-3_mean: 18.64
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 584
    cleaning_beam_agent-4_mean: 454.67
    cleaning_beam_agent-4_min: 365
    cleaning_beam_agent-5_max: 328
    cleaning_beam_agent-5_mean: 32.34
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-23-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1208.999999999998
  episode_reward_mean: 1040.3299999999904
  episode_reward_min: 619.0000000000023
  episodes_this_iter: 96
  episodes_total: 34368
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20598.558
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8764570951461792
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014483514241874218
        model: {}
        policy_loss: -0.0028642676770687103
        total_loss: -0.0021056956611573696
        vf_explained_var: 0.004756838083267212
        vf_loss: 23.011356353759766
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1254303455352783
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018039514543488622
        model: {}
        policy_loss: -0.004020433872938156
        total_loss: -0.0036693932488560677
        vf_explained_var: -0.006153106689453125
        vf_loss: 23.31793975830078
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.161965250968933
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010229282779619098
        model: {}
        policy_loss: -0.0033454562071710825
        total_loss: -0.0031515443697571754
        vf_explained_var: 0.037660449743270874
        vf_loss: 22.38970184326172
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37769615650177
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008909793104976416
        model: {}
        policy_loss: -0.002228386467322707
        total_loss: -0.0007895769085735083
        vf_explained_var: 0.09212088584899902
        vf_loss: 21.035585403442383
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8913930654525757
        entropy_coeff: 0.0017600000137463212
        kl: 0.001540225581265986
        model: {}
        policy_loss: -0.0033340491354465485
        total_loss: -0.002702265977859497
        vf_explained_var: 0.047117605805397034
        vf_loss: 22.00635528564453
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5584933757781982
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008102047140710056
        model: {}
        policy_loss: -0.0029673660174012184
        total_loss: -0.0019151493906974792
        vf_explained_var: 0.12225934863090515
        vf_loss: 20.351627349853516
    load_time_ms: 32730.355
    num_steps_sampled: 34368000
    num_steps_trained: 34368000
    sample_time_ms: 97756.086
    update_time_ms: 20.774
  iterations_since_restore: 338
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 14.958695652173914
    ram_util_percent: 15.689673913043476
  pid: 4061
  policy_reward_max:
    agent-0: 201.49999999999991
    agent-1: 201.49999999999991
    agent-2: 201.49999999999991
    agent-3: 201.49999999999991
    agent-4: 201.49999999999991
    agent-5: 201.49999999999991
  policy_reward_mean:
    agent-0: 173.3883333333332
    agent-1: 173.3883333333332
    agent-2: 173.3883333333332
    agent-3: 173.3883333333332
    agent-4: 173.3883333333332
    agent-5: 173.3883333333332
  policy_reward_min:
    agent-0: 103.1666666666668
    agent-1: 103.1666666666668
    agent-2: 103.1666666666668
    agent-3: 103.1666666666668
    agent-4: 103.1666666666668
    agent-5: 103.1666666666668
  sampler_perf:
    mean_env_wait_ms: 24.251900639095705
    mean_inference_ms: 12.329694493815346
    mean_processing_ms: 50.95325224400586
  time_since_restore: 44217.96690917015
  time_this_iter_s: 129.05618715286255
  time_total_s: 47429.03059530258
  timestamp: 1637061800
  timesteps_since_restore: 32448000
  timesteps_this_iter: 96000
  timesteps_total: 34368000
  training_iteration: 358
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    358 |            47429 | 34368000 |  1040.33 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 31
    apples_agent-0_mean: 1.57
    apples_agent-0_min: 0
    apples_agent-1_max: 89
    apples_agent-1_mean: 21.79
    apples_agent-1_min: 0
    apples_agent-2_max: 78
    apples_agent-2_mean: 14.11
    apples_agent-2_min: 0
    apples_agent-3_max: 128
    apples_agent-3_mean: 61.38
    apples_agent-3_min: 31
    apples_agent-4_max: 50
    apples_agent-4_mean: 1.06
    apples_agent-4_min: 0
    apples_agent-5_max: 214
    apples_agent-5_mean: 97.18
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 515
    cleaning_beam_agent-0_mean: 412.67
    cleaning_beam_agent-0_min: 273
    cleaning_beam_agent-1_max: 473
    cleaning_beam_agent-1_mean: 302.99
    cleaning_beam_agent-1_min: 160
    cleaning_beam_agent-2_max: 595
    cleaning_beam_agent-2_mean: 330.33
    cleaning_beam_agent-2_min: 133
    cleaning_beam_agent-3_max: 83
    cleaning_beam_agent-3_mean: 19.95
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 557
    cleaning_beam_agent-4_mean: 450.84
    cleaning_beam_agent-4_min: 367
    cleaning_beam_agent-5_max: 718
    cleaning_beam_agent-5_mean: 40.66
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-25-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1202.9999999999895
  episode_reward_mean: 1044.2899999999902
  episode_reward_min: 533.0000000000133
  episodes_this_iter: 96
  episodes_total: 34464
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20546.327
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8622720241546631
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016234409995377064
        model: {}
        policy_loss: -0.002607547678053379
        total_loss: -0.0019669383764266968
        vf_explained_var: 0.034789055585861206
        vf_loss: 21.582073211669922
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1292701959609985
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013985602417960763
        model: {}
        policy_loss: -0.003661255817860365
        total_loss: -0.003339019138365984
        vf_explained_var: -0.030321776866912842
        vf_loss: 23.09752082824707
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1555123329162598
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018557336879894137
        model: {}
        policy_loss: -0.00335739366710186
        total_loss: -0.003274211660027504
        vf_explained_var: 0.05535244941711426
        vf_loss: 21.168851852416992
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35512492060661316
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011837637284770608
        model: {}
        policy_loss: -0.00214006076566875
        total_loss: -0.0006815923843532801
        vf_explained_var: 0.06705451011657715
        vf_loss: 20.834901809692383
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8881633281707764
        entropy_coeff: 0.0017600000137463212
        kl: 0.002165523124858737
        model: {}
        policy_loss: -0.003801445011049509
        total_loss: -0.003175860270857811
        vf_explained_var: 0.01841902732849121
        vf_loss: 21.887500762939453
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5433149337768555
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012917593121528625
        model: {}
        policy_loss: -0.0028949370607733727
        total_loss: -0.0017697508446872234
        vf_explained_var: 0.0730721652507782
        vf_loss: 20.814231872558594
    load_time_ms: 30865.407
    num_steps_sampled: 34464000
    num_steps_trained: 34464000
    sample_time_ms: 96938.429
    update_time_ms: 22.231
  iterations_since_restore: 339
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.21508379888268
    ram_util_percent: 15.605027932960894
  pid: 4061
  policy_reward_max:
    agent-0: 200.4999999999998
    agent-1: 200.4999999999998
    agent-2: 200.4999999999998
    agent-3: 200.4999999999998
    agent-4: 200.4999999999998
    agent-5: 200.4999999999998
  policy_reward_mean:
    agent-0: 174.04833333333315
    agent-1: 174.04833333333315
    agent-2: 174.04833333333315
    agent-3: 174.04833333333315
    agent-4: 174.04833333333315
    agent-5: 174.04833333333315
  policy_reward_min:
    agent-0: 88.83333333333343
    agent-1: 88.83333333333343
    agent-2: 88.83333333333343
    agent-3: 88.83333333333343
    agent-4: 88.83333333333343
    agent-5: 88.83333333333343
  sampler_perf:
    mean_env_wait_ms: 24.253281837442998
    mean_inference_ms: 12.329068455092258
    mean_processing_ms: 50.95011272507484
  time_since_restore: 44343.48650121689
  time_this_iter_s: 125.51959204673767
  time_total_s: 47554.55018734932
  timestamp: 1637061926
  timesteps_since_restore: 32544000
  timesteps_this_iter: 96000
  timesteps_total: 34464000
  training_iteration: 359
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    359 |          47554.6 | 34464000 |  1044.29 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 40
    apples_agent-0_mean: 1.81
    apples_agent-0_min: 0
    apples_agent-1_max: 111
    apples_agent-1_mean: 21.71
    apples_agent-1_min: 0
    apples_agent-2_max: 124
    apples_agent-2_mean: 9.81
    apples_agent-2_min: 0
    apples_agent-3_max: 107
    apples_agent-3_mean: 55.13
    apples_agent-3_min: 30
    apples_agent-4_max: 85
    apples_agent-4_mean: 2.34
    apples_agent-4_min: 0
    apples_agent-5_max: 142
    apples_agent-5_mean: 99.66
    apples_agent-5_min: 52
    cleaning_beam_agent-0_max: 522
    cleaning_beam_agent-0_mean: 425.63
    cleaning_beam_agent-0_min: 304
    cleaning_beam_agent-1_max: 487
    cleaning_beam_agent-1_mean: 300.87
    cleaning_beam_agent-1_min: 176
    cleaning_beam_agent-2_max: 533
    cleaning_beam_agent-2_mean: 355.41
    cleaning_beam_agent-2_min: 136
    cleaning_beam_agent-3_max: 138
    cleaning_beam_agent-3_mean: 24.62
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 556
    cleaning_beam_agent-4_mean: 455.1
    cleaning_beam_agent-4_min: 348
    cleaning_beam_agent-5_max: 252
    cleaning_beam_agent-5_mean: 25.6
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-27-33
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1217.9999999999973
  episode_reward_mean: 1044.5999999999922
  episode_reward_min: 524.000000000008
  episodes_this_iter: 96
  episodes_total: 34560
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20493.472
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8541712760925293
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014491456095129251
        model: {}
        policy_loss: -0.0031479066237807274
        total_loss: -0.002250963356345892
        vf_explained_var: 0.03487005829811096
        vf_loss: 24.00286102294922
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1284334659576416
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013444422511383891
        model: {}
        policy_loss: -0.0036034411750733852
        total_loss: -0.003070003818720579
        vf_explained_var: -0.012397810816764832
        vf_loss: 25.19480323791504
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1428614854812622
        entropy_coeff: 0.0017600000137463212
        kl: 0.001156162703409791
        model: {}
        policy_loss: -0.003308923915028572
        total_loss: -0.002981756813824177
        vf_explained_var: 0.05797766149044037
        vf_loss: 23.38600730895996
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3771342635154724
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007306452607735991
        model: {}
        policy_loss: -0.0022522038780152798
        total_loss: -0.0007323225727304816
        vf_explained_var: 0.12153206765651703
        vf_loss: 21.836366653442383
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8822854161262512
        entropy_coeff: 0.0017600000137463212
        kl: 0.001485332497395575
        model: {}
        policy_loss: -0.0035744840279221535
        total_loss: -0.002790318801999092
        vf_explained_var: 0.05892854928970337
        vf_loss: 23.369897842407227
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5517444610595703
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012171500129625201
        model: {}
        policy_loss: -0.002904385793954134
        total_loss: -0.001732897013425827
        vf_explained_var: 0.13828319311141968
        vf_loss: 21.425588607788086
    load_time_ms: 28811.039
    num_steps_sampled: 34560000
    num_steps_trained: 34560000
    sample_time_ms: 95950.741
    update_time_ms: 22.573
  iterations_since_restore: 340
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.15138121546961
    ram_util_percent: 15.67624309392265
  pid: 4061
  policy_reward_max:
    agent-0: 202.9999999999993
    agent-1: 202.9999999999993
    agent-2: 202.9999999999993
    agent-3: 202.9999999999993
    agent-4: 202.9999999999993
    agent-5: 202.9999999999993
  policy_reward_mean:
    agent-0: 174.09999999999985
    agent-1: 174.09999999999985
    agent-2: 174.09999999999985
    agent-3: 174.09999999999985
    agent-4: 174.09999999999985
    agent-5: 174.09999999999985
  policy_reward_min:
    agent-0: 87.33333333333356
    agent-1: 87.33333333333356
    agent-2: 87.33333333333356
    agent-3: 87.33333333333356
    agent-4: 87.33333333333356
    agent-5: 87.33333333333356
  sampler_perf:
    mean_env_wait_ms: 24.255016387465197
    mean_inference_ms: 12.328615005287011
    mean_processing_ms: 50.94782076683075
  time_since_restore: 44470.16445970535
  time_this_iter_s: 126.67795848846436
  time_total_s: 47681.228145837784
  timestamp: 1637062053
  timesteps_since_restore: 32640000
  timesteps_this_iter: 96000
  timesteps_total: 34560000
  training_iteration: 360
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 28.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    360 |          47681.2 | 34560000 |   1044.6 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 48
    apples_agent-0_mean: 1.36
    apples_agent-0_min: 0
    apples_agent-1_max: 94
    apples_agent-1_mean: 17.41
    apples_agent-1_min: 0
    apples_agent-2_max: 121
    apples_agent-2_mean: 12.37
    apples_agent-2_min: 0
    apples_agent-3_max: 124
    apples_agent-3_mean: 54.78
    apples_agent-3_min: 31
    apples_agent-4_max: 63
    apples_agent-4_mean: 2.04
    apples_agent-4_min: 0
    apples_agent-5_max: 156
    apples_agent-5_mean: 101.7
    apples_agent-5_min: 3
    cleaning_beam_agent-0_max: 527
    cleaning_beam_agent-0_mean: 437.4
    cleaning_beam_agent-0_min: 336
    cleaning_beam_agent-1_max: 476
    cleaning_beam_agent-1_mean: 318.57
    cleaning_beam_agent-1_min: 136
    cleaning_beam_agent-2_max: 510
    cleaning_beam_agent-2_mean: 369.29
    cleaning_beam_agent-2_min: 191
    cleaning_beam_agent-3_max: 138
    cleaning_beam_agent-3_mean: 20.24
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 595
    cleaning_beam_agent-4_mean: 455.5
    cleaning_beam_agent-4_min: 344
    cleaning_beam_agent-5_max: 257
    cleaning_beam_agent-5_mean: 23.08
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-29-39
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1223.9999999999866
  episode_reward_mean: 1051.9299999999937
  episode_reward_min: 340.00000000000404
  episodes_this_iter: 96
  episodes_total: 34656
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20476.307
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8548853397369385
        entropy_coeff: 0.0017600000137463212
        kl: 0.001394059625454247
        model: {}
        policy_loss: -0.0027440721169114113
        total_loss: -0.0018876753747463226
        vf_explained_var: 0.04317367076873779
        vf_loss: 23.609966278076172
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1246237754821777
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021531200036406517
        model: {}
        policy_loss: -0.004062147345393896
        total_loss: -0.0035491513554006815
        vf_explained_var: -0.008505254983901978
        vf_loss: 24.923343658447266
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1200894117355347
        entropy_coeff: 0.0017600000137463212
        kl: 0.001647442695684731
        model: {}
        policy_loss: -0.0031970373820513487
        total_loss: -0.002810174599289894
        vf_explained_var: 0.043171390891075134
        vf_loss: 23.582183837890625
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3553968667984009
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010739329736679792
        model: {}
        policy_loss: -0.002134747104719281
        total_loss: -0.0005165752954781055
        vf_explained_var: 0.08668270707130432
        vf_loss: 22.436708450317383
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8855882883071899
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015010754577815533
        model: {}
        policy_loss: -0.00336416345089674
        total_loss: -0.0026343557983636856
        vf_explained_var: 0.07542990148067474
        vf_loss: 22.88442039489746
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5407035946846008
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016342506278306246
        model: {}
        policy_loss: -0.0030389209277927876
        total_loss: -0.001758715370669961
        vf_explained_var: 0.09409855306148529
        vf_loss: 22.318431854248047
    load_time_ms: 25989.826
    num_steps_sampled: 34656000
    num_steps_trained: 34656000
    sample_time_ms: 95045.712
    update_time_ms: 23.059
  iterations_since_restore: 341
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.684357541899441
    ram_util_percent: 15.597765363128492
  pid: 4061
  policy_reward_max:
    agent-0: 203.9999999999997
    agent-1: 203.9999999999997
    agent-2: 203.9999999999997
    agent-3: 203.9999999999997
    agent-4: 203.9999999999997
    agent-5: 203.9999999999997
  policy_reward_mean:
    agent-0: 175.32166666666654
    agent-1: 175.32166666666654
    agent-2: 175.32166666666654
    agent-3: 175.32166666666654
    agent-4: 175.32166666666654
    agent-5: 175.32166666666654
  policy_reward_min:
    agent-0: 56.666666666666444
    agent-1: 56.666666666666444
    agent-2: 56.666666666666444
    agent-3: 56.666666666666444
    agent-4: 56.666666666666444
    agent-5: 56.666666666666444
  sampler_perf:
    mean_env_wait_ms: 24.25771678148117
    mean_inference_ms: 12.32827948359655
    mean_processing_ms: 50.945875476396104
  time_since_restore: 44595.683406829834
  time_this_iter_s: 125.5189471244812
  time_total_s: 47806.747092962265
  timestamp: 1637062179
  timesteps_since_restore: 32736000
  timesteps_this_iter: 96000
  timesteps_total: 34656000
  training_iteration: 361
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    361 |          47806.7 | 34656000 |  1051.93 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 1.03
    apples_agent-0_min: 0
    apples_agent-1_max: 77
    apples_agent-1_mean: 21.88
    apples_agent-1_min: 0
    apples_agent-2_max: 100
    apples_agent-2_mean: 19.18
    apples_agent-2_min: 0
    apples_agent-3_max: 175
    apples_agent-3_mean: 56.73
    apples_agent-3_min: 25
    apples_agent-4_max: 56
    apples_agent-4_mean: 1.38
    apples_agent-4_min: 0
    apples_agent-5_max: 160
    apples_agent-5_mean: 101.08
    apples_agent-5_min: 39
    cleaning_beam_agent-0_max: 573
    cleaning_beam_agent-0_mean: 437.35
    cleaning_beam_agent-0_min: 317
    cleaning_beam_agent-1_max: 467
    cleaning_beam_agent-1_mean: 311.11
    cleaning_beam_agent-1_min: 138
    cleaning_beam_agent-2_max: 553
    cleaning_beam_agent-2_mean: 358.79
    cleaning_beam_agent-2_min: 152
    cleaning_beam_agent-3_max: 69
    cleaning_beam_agent-3_mean: 19.47
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 555
    cleaning_beam_agent-4_mean: 464.42
    cleaning_beam_agent-4_min: 314
    cleaning_beam_agent-5_max: 271
    cleaning_beam_agent-5_mean: 28.34
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-31-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1201.0000000000132
  episode_reward_mean: 1056.729999999991
  episode_reward_min: 474.00000000001177
  episodes_this_iter: 96
  episodes_total: 34752
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20474.953
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8332034945487976
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016919600311666727
        model: {}
        policy_loss: -0.002862471854314208
        total_loss: -0.002147708786651492
        vf_explained_var: 0.03011062741279602
        vf_loss: 21.812000274658203
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1262727975845337
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016543110832571983
        model: {}
        policy_loss: -0.004023019224405289
        total_loss: -0.003726462135091424
        vf_explained_var: -0.01281370222568512
        vf_loss: 22.7879695892334
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.130373239517212
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009845925960689783
        model: {}
        policy_loss: -0.0031130053102970123
        total_loss: -0.002850954420864582
        vf_explained_var: -0.002688974142074585
        vf_loss: 22.515064239501953
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3618689775466919
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007623806595802307
        model: {}
        policy_loss: -0.002124509774148464
        total_loss: -0.0006986157968640327
        vf_explained_var: 0.08041056990623474
        vf_loss: 20.62782859802246
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8863739371299744
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017099955584853888
        model: {}
        policy_loss: -0.003581757890060544
        total_loss: -0.0030257946345955133
        vf_explained_var: 0.06030374765396118
        vf_loss: 21.159820556640625
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5323848724365234
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007419633329845965
        model: {}
        policy_loss: -0.0025332081131637096
        total_loss: -0.0014017075300216675
        vf_explained_var: 0.0788697749376297
        vf_loss: 20.68499755859375
    load_time_ms: 24082.649
    num_steps_sampled: 34752000
    num_steps_trained: 34752000
    sample_time_ms: 94226.019
    update_time_ms: 22.833
  iterations_since_restore: 342
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 16.755
    ram_util_percent: 15.764999999999999
  pid: 4061
  policy_reward_max:
    agent-0: 200.16666666666632
    agent-1: 200.16666666666632
    agent-2: 200.16666666666632
    agent-3: 200.16666666666632
    agent-4: 200.16666666666632
    agent-5: 200.16666666666632
  policy_reward_mean:
    agent-0: 176.12166666666653
    agent-1: 176.12166666666653
    agent-2: 176.12166666666653
    agent-3: 176.12166666666653
    agent-4: 176.12166666666653
    agent-5: 176.12166666666653
  policy_reward_min:
    agent-0: 79.00000000000011
    agent-1: 79.00000000000011
    agent-2: 79.00000000000011
    agent-3: 79.00000000000011
    agent-4: 79.00000000000011
    agent-5: 79.00000000000011
  sampler_perf:
    mean_env_wait_ms: 24.261067039646804
    mean_inference_ms: 12.328444932916254
    mean_processing_ms: 50.94538790404263
  time_since_restore: 44721.69908905029
  time_this_iter_s: 126.01568222045898
  time_total_s: 47932.762775182724
  timestamp: 1637062305
  timesteps_since_restore: 32832000
  timesteps_this_iter: 96000
  timesteps_total: 34752000
  training_iteration: 362
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    362 |          47932.8 | 34752000 |  1056.73 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 40
    apples_agent-0_mean: 1.81
    apples_agent-0_min: 0
    apples_agent-1_max: 76
    apples_agent-1_mean: 18.96
    apples_agent-1_min: 0
    apples_agent-2_max: 202
    apples_agent-2_mean: 18.57
    apples_agent-2_min: 0
    apples_agent-3_max: 108
    apples_agent-3_mean: 55.82
    apples_agent-3_min: 17
    apples_agent-4_max: 100
    apples_agent-4_mean: 2.07
    apples_agent-4_min: 0
    apples_agent-5_max: 175
    apples_agent-5_mean: 96.31
    apples_agent-5_min: 17
    cleaning_beam_agent-0_max: 586
    cleaning_beam_agent-0_mean: 444.8
    cleaning_beam_agent-0_min: 330
    cleaning_beam_agent-1_max: 521
    cleaning_beam_agent-1_mean: 330.55
    cleaning_beam_agent-1_min: 163
    cleaning_beam_agent-2_max: 571
    cleaning_beam_agent-2_mean: 359.62
    cleaning_beam_agent-2_min: 153
    cleaning_beam_agent-3_max: 222
    cleaning_beam_agent-3_mean: 20.52
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 552
    cleaning_beam_agent-4_mean: 444.44
    cleaning_beam_agent-4_min: 259
    cleaning_beam_agent-5_max: 348
    cleaning_beam_agent-5_mean: 35.2
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-33-49
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1249.000000000005
  episode_reward_mean: 1034.2299999999898
  episode_reward_min: 559.0000000000007
  episodes_this_iter: 96
  episodes_total: 34848
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20437.676
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8382409811019897
        entropy_coeff: 0.0017600000137463212
        kl: 0.002066558925434947
        model: {}
        policy_loss: -0.0030583455227315426
        total_loss: -0.002244384028017521
        vf_explained_var: 0.025511130690574646
        vf_loss: 22.892675399780273
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1169943809509277
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010699064005166292
        model: {}
        policy_loss: -0.0033538038842380047
        total_loss: -0.002934566233307123
        vf_explained_var: -0.01503242552280426
        vf_loss: 23.85148811340332
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1324777603149414
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017429294530302286
        model: {}
        policy_loss: -0.0033344291150569916
        total_loss: -0.0029890290461480618
        vf_explained_var: 0.0059167444705963135
        vf_loss: 23.385587692260742
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36458534002304077
        entropy_coeff: 0.0017600000137463212
        kl: 0.000915276468731463
        model: {}
        policy_loss: -0.0021541216410696507
        total_loss: -0.0006871889345347881
        vf_explained_var: 0.10286176204681396
        vf_loss: 21.0860595703125
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9050867557525635
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018530595116317272
        model: {}
        policy_loss: -0.0036339201033115387
        total_loss: -0.0029870469588786364
        vf_explained_var: 0.04707559943199158
        vf_loss: 22.398303985595703
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5488607883453369
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009555006981827319
        model: {}
        policy_loss: -0.00282092671841383
        total_loss: -0.001588764600455761
        vf_explained_var: 0.06522250175476074
        vf_loss: 21.981597900390625
    load_time_ms: 20586.895
    num_steps_sampled: 34848000
    num_steps_trained: 34848000
    sample_time_ms: 93130.222
    update_time_ms: 22.672
  iterations_since_restore: 343
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.409039548022601
    ram_util_percent: 15.615254237288134
  pid: 4061
  policy_reward_max:
    agent-0: 208.16666666666666
    agent-1: 208.16666666666666
    agent-2: 208.16666666666666
    agent-3: 208.16666666666666
    agent-4: 208.16666666666666
    agent-5: 208.16666666666666
  policy_reward_mean:
    agent-0: 172.3716666666665
    agent-1: 172.3716666666665
    agent-2: 172.3716666666665
    agent-3: 172.3716666666665
    agent-4: 172.3716666666665
    agent-5: 172.3716666666665
  policy_reward_min:
    agent-0: 93.1666666666669
    agent-1: 93.1666666666669
    agent-2: 93.1666666666669
    agent-3: 93.1666666666669
    agent-4: 93.1666666666669
    agent-5: 93.1666666666669
  sampler_perf:
    mean_env_wait_ms: 24.263228026645937
    mean_inference_ms: 12.327823537260988
    mean_processing_ms: 50.94285040022932
  time_since_restore: 44845.57012677193
  time_this_iter_s: 123.87103772163391
  time_total_s: 48056.63381290436
  timestamp: 1637062429
  timesteps_since_restore: 32928000
  timesteps_this_iter: 96000
  timesteps_total: 34848000
  training_iteration: 363
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    363 |          48056.6 | 34848000 |  1034.23 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 27
    apples_agent-0_mean: 1.33
    apples_agent-0_min: 0
    apples_agent-1_max: 74
    apples_agent-1_mean: 16.01
    apples_agent-1_min: 0
    apples_agent-2_max: 105
    apples_agent-2_mean: 16.05
    apples_agent-2_min: 0
    apples_agent-3_max: 114
    apples_agent-3_mean: 56.41
    apples_agent-3_min: 29
    apples_agent-4_max: 48
    apples_agent-4_mean: 0.91
    apples_agent-4_min: 0
    apples_agent-5_max: 145
    apples_agent-5_mean: 97.72
    apples_agent-5_min: 25
    cleaning_beam_agent-0_max: 547
    cleaning_beam_agent-0_mean: 457.7
    cleaning_beam_agent-0_min: 346
    cleaning_beam_agent-1_max: 534
    cleaning_beam_agent-1_mean: 340.29
    cleaning_beam_agent-1_min: 215
    cleaning_beam_agent-2_max: 624
    cleaning_beam_agent-2_mean: 363.31
    cleaning_beam_agent-2_min: 180
    cleaning_beam_agent-3_max: 85
    cleaning_beam_agent-3_mean: 19.42
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 559
    cleaning_beam_agent-4_mean: 445.72
    cleaning_beam_agent-4_min: 281
    cleaning_beam_agent-5_max: 521
    cleaning_beam_agent-5_mean: 34.68
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-35-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1261.0000000000068
  episode_reward_mean: 1049.96999999999
  episode_reward_min: 512.0000000000109
  episodes_this_iter: 96
  episodes_total: 34944
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20375.184
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8558940887451172
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012278774520382285
        model: {}
        policy_loss: -0.0027658017352223396
        total_loss: -0.0020458505023270845
        vf_explained_var: 0.04409755766391754
        vf_loss: 22.26326560974121
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1208043098449707
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015362222911790013
        model: {}
        policy_loss: -0.0036219176836311817
        total_loss: -0.0032338809687644243
        vf_explained_var: -0.01032741367816925
        vf_loss: 23.606494903564453
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1293737888336182
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017120083793997765
        model: {}
        policy_loss: -0.003303570207208395
        total_loss: -0.003036101581528783
        vf_explained_var: 0.03355817496776581
        vf_loss: 22.551660537719727
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3632926642894745
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008036602521315217
        model: {}
        policy_loss: -0.0020420027431100607
        total_loss: -0.0006141421617940068
        vf_explained_var: 0.113608717918396
        vf_loss: 20.672569274902344
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.894221305847168
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018565758364275098
        model: {}
        policy_loss: -0.003379572182893753
        total_loss: -0.002823031507432461
        vf_explained_var: 0.08651626110076904
        vf_loss: 21.30370330810547
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5362370014190674
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007636196678504348
        model: {}
        policy_loss: -0.002751362742856145
        total_loss: -0.0016015166183933616
        vf_explained_var: 0.10446092486381531
        vf_loss: 20.936248779296875
    load_time_ms: 19166.096
    num_steps_sampled: 34944000
    num_steps_trained: 34944000
    sample_time_ms: 92056.125
    update_time_ms: 22.062
  iterations_since_restore: 344
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.403389830508473
    ram_util_percent: 15.602259887005646
  pid: 4061
  policy_reward_max:
    agent-0: 210.16666666666632
    agent-1: 210.16666666666632
    agent-2: 210.16666666666632
    agent-3: 210.16666666666632
    agent-4: 210.16666666666632
    agent-5: 210.16666666666632
  policy_reward_mean:
    agent-0: 174.99499999999986
    agent-1: 174.99499999999986
    agent-2: 174.99499999999986
    agent-3: 174.99499999999986
    agent-4: 174.99499999999986
    agent-5: 174.99499999999986
  policy_reward_min:
    agent-0: 85.33333333333337
    agent-1: 85.33333333333337
    agent-2: 85.33333333333337
    agent-3: 85.33333333333337
    agent-4: 85.33333333333337
    agent-5: 85.33333333333337
  sampler_perf:
    mean_env_wait_ms: 24.265567197829665
    mean_inference_ms: 12.327135984973674
    mean_processing_ms: 50.939859524945994
  time_since_restore: 44969.70197892189
  time_this_iter_s: 124.13185214996338
  time_total_s: 48180.76566505432
  timestamp: 1637062553
  timesteps_since_restore: 33024000
  timesteps_this_iter: 96000
  timesteps_total: 34944000
  training_iteration: 364
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    364 |          48180.8 | 34944000 |  1049.97 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 36
    apples_agent-0_mean: 1.19
    apples_agent-0_min: 0
    apples_agent-1_max: 79
    apples_agent-1_mean: 19.67
    apples_agent-1_min: 0
    apples_agent-2_max: 112
    apples_agent-2_mean: 12.79
    apples_agent-2_min: 0
    apples_agent-3_max: 109
    apples_agent-3_mean: 54.8
    apples_agent-3_min: 27
    apples_agent-4_max: 67
    apples_agent-4_mean: 1.36
    apples_agent-4_min: 0
    apples_agent-5_max: 172
    apples_agent-5_mean: 101.9
    apples_agent-5_min: 43
    cleaning_beam_agent-0_max: 556
    cleaning_beam_agent-0_mean: 455.0
    cleaning_beam_agent-0_min: 300
    cleaning_beam_agent-1_max: 612
    cleaning_beam_agent-1_mean: 334.87
    cleaning_beam_agent-1_min: 167
    cleaning_beam_agent-2_max: 588
    cleaning_beam_agent-2_mean: 361.93
    cleaning_beam_agent-2_min: 112
    cleaning_beam_agent-3_max: 119
    cleaning_beam_agent-3_mean: 21.81
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 535
    cleaning_beam_agent-4_mean: 435.68
    cleaning_beam_agent-4_min: 320
    cleaning_beam_agent-5_max: 229
    cleaning_beam_agent-5_mean: 24.75
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-37-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1210.000000000007
  episode_reward_mean: 1053.5399999999904
  episode_reward_min: 613.0000000000018
  episodes_this_iter: 96
  episodes_total: 35040
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20295.13
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8541965484619141
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017447971040382981
        model: {}
        policy_loss: -0.003095904365181923
        total_loss: -0.00248719472438097
        vf_explained_var: 0.04356302320957184
        vf_loss: 21.12094497680664
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1189827919006348
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017672986723482609
        model: {}
        policy_loss: -0.003668808378279209
        total_loss: -0.003310935804620385
        vf_explained_var: -0.05230036377906799
        vf_loss: 23.2728271484375
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1254692077636719
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014090692857280374
        model: {}
        policy_loss: -0.0033123891334980726
        total_loss: -0.003111961530521512
        vf_explained_var: 0.01617412269115448
        vf_loss: 21.81254768371582
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37044933438301086
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011198227293789387
        model: {}
        policy_loss: -0.0020734169520437717
        total_loss: -0.0007163956761360168
        vf_explained_var: 0.09057791531085968
        vf_loss: 20.09010124206543
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.90653395652771
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016078540356829762
        model: {}
        policy_loss: -0.0036373839247971773
        total_loss: -0.0031208819709718227
        vf_explained_var: 0.04456472396850586
        vf_loss: 21.120038986206055
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5266019105911255
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014703602064400911
        model: {}
        policy_loss: -0.002773337997496128
        total_loss: -0.0016559120267629623
        vf_explained_var: 0.07453250885009766
        vf_loss: 20.44244956970215
    load_time_ms: 19030.747
    num_steps_sampled: 35040000
    num_steps_trained: 35040000
    sample_time_ms: 91136.209
    update_time_ms: 22.341
  iterations_since_restore: 345
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.312994350282487
    ram_util_percent: 15.65875706214689
  pid: 4061
  policy_reward_max:
    agent-0: 201.66666666666623
    agent-1: 201.66666666666623
    agent-2: 201.66666666666623
    agent-3: 201.66666666666623
    agent-4: 201.66666666666623
    agent-5: 201.66666666666623
  policy_reward_mean:
    agent-0: 175.58999999999992
    agent-1: 175.58999999999992
    agent-2: 175.58999999999992
    agent-3: 175.58999999999992
    agent-4: 175.58999999999992
    agent-5: 175.58999999999992
  policy_reward_min:
    agent-0: 102.16666666666691
    agent-1: 102.16666666666691
    agent-2: 102.16666666666691
    agent-3: 102.16666666666691
    agent-4: 102.16666666666691
    agent-5: 102.16666666666691
  sampler_perf:
    mean_env_wait_ms: 24.26768590339344
    mean_inference_ms: 12.326622644456947
    mean_processing_ms: 50.937377248280626
  time_since_restore: 45093.926615953445
  time_this_iter_s: 124.22463703155518
  time_total_s: 48304.99030208588
  timestamp: 1637062678
  timesteps_since_restore: 33120000
  timesteps_this_iter: 96000
  timesteps_total: 35040000
  training_iteration: 365
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 28.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    365 |            48305 | 35040000 |  1053.54 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 28
    apples_agent-0_mean: 1.95
    apples_agent-0_min: 0
    apples_agent-1_max: 76
    apples_agent-1_mean: 20.42
    apples_agent-1_min: 0
    apples_agent-2_max: 105
    apples_agent-2_mean: 12.32
    apples_agent-2_min: 0
    apples_agent-3_max: 87
    apples_agent-3_mean: 55.41
    apples_agent-3_min: 27
    apples_agent-4_max: 64
    apples_agent-4_mean: 1.41
    apples_agent-4_min: 0
    apples_agent-5_max: 189
    apples_agent-5_mean: 98.94
    apples_agent-5_min: 40
    cleaning_beam_agent-0_max: 548
    cleaning_beam_agent-0_mean: 452.95
    cleaning_beam_agent-0_min: 316
    cleaning_beam_agent-1_max: 485
    cleaning_beam_agent-1_mean: 322.44
    cleaning_beam_agent-1_min: 196
    cleaning_beam_agent-2_max: 572
    cleaning_beam_agent-2_mean: 365.07
    cleaning_beam_agent-2_min: 166
    cleaning_beam_agent-3_max: 89
    cleaning_beam_agent-3_mean: 21.02
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 571
    cleaning_beam_agent-4_mean: 435.12
    cleaning_beam_agent-4_min: 319
    cleaning_beam_agent-5_max: 417
    cleaning_beam_agent-5_mean: 27.81
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-40-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1196.999999999992
  episode_reward_mean: 1042.8999999999903
  episode_reward_min: 493.0000000000155
  episodes_this_iter: 96
  episodes_total: 35136
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20257.298
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8516320586204529
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017335201846435666
        model: {}
        policy_loss: -0.002785621676594019
        total_loss: -0.002003698144108057
        vf_explained_var: 0.01256309449672699
        vf_loss: 22.807964324951172
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1257320642471313
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017122175777330995
        model: {}
        policy_loss: -0.003960456699132919
        total_loss: -0.0036018271930515766
        vf_explained_var: -0.011559337377548218
        vf_loss: 23.39919090270996
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.126176118850708
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016279863193631172
        model: {}
        policy_loss: -0.0034240754321217537
        total_loss: -0.0032346658408641815
        vf_explained_var: 0.06041726469993591
        vf_loss: 21.714799880981445
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3671531081199646
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007363430340774357
        model: {}
        policy_loss: -0.002119947923347354
        total_loss: -0.0006831750506535172
        vf_explained_var: 0.09875042736530304
        vf_loss: 20.82962989807129
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9063637256622314
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016097259940579534
        model: {}
        policy_loss: -0.0035065561532974243
        total_loss: -0.002863724483177066
        vf_explained_var: 0.03153812885284424
        vf_loss: 22.38031578063965
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5245566368103027
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007296365802176297
        model: {}
        policy_loss: -0.0027690152637660503
        total_loss: -0.0016202004626393318
        vf_explained_var: 0.1052800863981247
        vf_loss: 20.720327377319336
    load_time_ms: 16600.579
    num_steps_sampled: 35136000
    num_steps_trained: 35136000
    sample_time_ms: 90319.524
    update_time_ms: 21.7
  iterations_since_restore: 346
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 16.237078651685394
    ram_util_percent: 15.590449438202247
  pid: 4061
  policy_reward_max:
    agent-0: 199.4999999999998
    agent-1: 199.4999999999998
    agent-2: 199.4999999999998
    agent-3: 199.4999999999998
    agent-4: 199.4999999999998
    agent-5: 199.4999999999998
  policy_reward_mean:
    agent-0: 173.81666666666652
    agent-1: 173.81666666666652
    agent-2: 173.81666666666652
    agent-3: 173.81666666666652
    agent-4: 173.81666666666652
    agent-5: 173.81666666666652
  policy_reward_min:
    agent-0: 82.16666666666686
    agent-1: 82.16666666666686
    agent-2: 82.16666666666686
    agent-3: 82.16666666666686
    agent-4: 82.16666666666686
    agent-5: 82.16666666666686
  sampler_perf:
    mean_env_wait_ms: 24.26983896910373
    mean_inference_ms: 12.326311167370807
    mean_processing_ms: 50.935721598921525
  time_since_restore: 45219.02277779579
  time_this_iter_s: 125.09616184234619
  time_total_s: 48430.08646392822
  timestamp: 1637062803
  timesteps_since_restore: 33216000
  timesteps_this_iter: 96000
  timesteps_total: 35136000
  training_iteration: 366
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    366 |          48430.1 | 35136000 |   1042.9 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 0.46
    apples_agent-0_min: 0
    apples_agent-1_max: 83
    apples_agent-1_mean: 19.6
    apples_agent-1_min: 0
    apples_agent-2_max: 125
    apples_agent-2_mean: 18.4
    apples_agent-2_min: 0
    apples_agent-3_max: 112
    apples_agent-3_mean: 57.49
    apples_agent-3_min: 27
    apples_agent-4_max: 76
    apples_agent-4_mean: 1.84
    apples_agent-4_min: 0
    apples_agent-5_max: 141
    apples_agent-5_mean: 97.83
    apples_agent-5_min: 30
    cleaning_beam_agent-0_max: 572
    cleaning_beam_agent-0_mean: 473.5
    cleaning_beam_agent-0_min: 364
    cleaning_beam_agent-1_max: 476
    cleaning_beam_agent-1_mean: 319.13
    cleaning_beam_agent-1_min: 159
    cleaning_beam_agent-2_max: 565
    cleaning_beam_agent-2_mean: 367.53
    cleaning_beam_agent-2_min: 164
    cleaning_beam_agent-3_max: 52
    cleaning_beam_agent-3_mean: 18.18
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 559
    cleaning_beam_agent-4_mean: 439.67
    cleaning_beam_agent-4_min: 343
    cleaning_beam_agent-5_max: 293
    cleaning_beam_agent-5_mean: 27.64
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-42-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1211.99999999999
  episode_reward_mean: 1055.1099999999926
  episode_reward_min: 521.0000000000109
  episodes_this_iter: 96
  episodes_total: 35232
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20255.154
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8401557207107544
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012381235137581825
        model: {}
        policy_loss: -0.0027511927764862776
        total_loss: -0.0021115122362971306
        vf_explained_var: 0.01784050464630127
        vf_loss: 21.183542251586914
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1243356466293335
        entropy_coeff: 0.0017600000137463212
        kl: 0.001454729586839676
        model: {}
        policy_loss: -0.0034471817780286074
        total_loss: -0.0032430500723421574
        vf_explained_var: -0.005767405033111572
        vf_loss: 21.82964324951172
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1191188097000122
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010640015825629234
        model: {}
        policy_loss: -0.003180038882419467
        total_loss: -0.0029869331046938896
        vf_explained_var: 0.001550927758216858
        vf_loss: 21.627532958984375
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35301002860069275
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010934972669929266
        model: {}
        policy_loss: -0.002149578183889389
        total_loss: -0.0007522122468799353
        vf_explained_var: 0.06597037613391876
        vf_loss: 20.186647415161133
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9001598954200745
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014761367347091436
        model: {}
        policy_loss: -0.0035395894665271044
        total_loss: -0.0030647283419966698
        vf_explained_var: 0.046675294637680054
        vf_loss: 20.591405868530273
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5143078565597534
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010939370840787888
        model: {}
        policy_loss: -0.0027369381859898567
        total_loss: -0.001577199436724186
        vf_explained_var: 0.046978503465652466
        vf_loss: 20.649221420288086
    load_time_ms: 14809.976
    num_steps_sampled: 35232000
    num_steps_trained: 35232000
    sample_time_ms: 90134.758
    update_time_ms: 21.684
  iterations_since_restore: 347
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.544886363636364
    ram_util_percent: 15.65056818181818
  pid: 4061
  policy_reward_max:
    agent-0: 201.99999999999977
    agent-1: 201.99999999999977
    agent-2: 201.99999999999977
    agent-3: 201.99999999999977
    agent-4: 201.99999999999977
    agent-5: 201.99999999999977
  policy_reward_mean:
    agent-0: 175.8516666666665
    agent-1: 175.8516666666665
    agent-2: 175.8516666666665
    agent-3: 175.8516666666665
    agent-4: 175.8516666666665
    agent-5: 175.8516666666665
  policy_reward_min:
    agent-0: 86.83333333333331
    agent-1: 86.83333333333331
    agent-2: 86.83333333333331
    agent-3: 86.83333333333331
    agent-4: 86.83333333333331
    agent-5: 86.83333333333331
  sampler_perf:
    mean_env_wait_ms: 24.272306378535344
    mean_inference_ms: 12.325970353094291
    mean_processing_ms: 50.933378628770186
  time_since_restore: 45342.21772646904
  time_this_iter_s: 123.19494867324829
  time_total_s: 48553.28141260147
  timestamp: 1637062926
  timesteps_since_restore: 33312000
  timesteps_this_iter: 96000
  timesteps_total: 35232000
  training_iteration: 367
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    367 |          48553.3 | 35232000 |  1055.11 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 28
    apples_agent-0_mean: 1.17
    apples_agent-0_min: 0
    apples_agent-1_max: 89
    apples_agent-1_mean: 18.78
    apples_agent-1_min: 0
    apples_agent-2_max: 85
    apples_agent-2_mean: 18.04
    apples_agent-2_min: 0
    apples_agent-3_max: 136
    apples_agent-3_mean: 55.11
    apples_agent-3_min: 24
    apples_agent-4_max: 57
    apples_agent-4_mean: 0.92
    apples_agent-4_min: 0
    apples_agent-5_max: 175
    apples_agent-5_mean: 100.41
    apples_agent-5_min: 57
    cleaning_beam_agent-0_max: 554
    cleaning_beam_agent-0_mean: 472.83
    cleaning_beam_agent-0_min: 319
    cleaning_beam_agent-1_max: 486
    cleaning_beam_agent-1_mean: 338.37
    cleaning_beam_agent-1_min: 121
    cleaning_beam_agent-2_max: 534
    cleaning_beam_agent-2_mean: 363.19
    cleaning_beam_agent-2_min: 130
    cleaning_beam_agent-3_max: 100
    cleaning_beam_agent-3_mean: 25.98
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 543
    cleaning_beam_agent-4_mean: 434.14
    cleaning_beam_agent-4_min: 317
    cleaning_beam_agent-5_max: 306
    cleaning_beam_agent-5_mean: 24.09
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-44-11
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1228.0000000000014
  episode_reward_mean: 1055.96999999999
  episode_reward_min: 545.0000000000028
  episodes_this_iter: 96
  episodes_total: 35328
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20227.315
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8544588088989258
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010661279084160924
        model: {}
        policy_loss: -0.002668909262865782
        total_loss: -0.0019512518774718046
        vf_explained_var: 0.04931303858757019
        vf_loss: 22.21506118774414
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1083078384399414
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017343010986223817
        model: {}
        policy_loss: -0.003592338413000107
        total_loss: -0.003140571992844343
        vf_explained_var: -0.025650739669799805
        vf_loss: 24.02387237548828
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1238939762115479
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014671084936708212
        model: {}
        policy_loss: -0.0032154391519725323
        total_loss: -0.002883410081267357
        vf_explained_var: 0.012853473424911499
        vf_loss: 23.100753784179688
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38188910484313965
        entropy_coeff: 0.0017600000137463212
        kl: 0.000985633465461433
        model: {}
        policy_loss: -0.0022495961748063564
        total_loss: -0.00080906692892313
        vf_explained_var: 0.09551672637462616
        vf_loss: 21.126544952392578
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9022471904754639
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014915729407221079
        model: {}
        policy_loss: -0.0034585935063660145
        total_loss: -0.002789195394143462
        vf_explained_var: 0.03394003212451935
        vf_loss: 22.57352638244629
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4964858889579773
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011044587008655071
        model: {}
        policy_loss: -0.0024790428578853607
        total_loss: -0.0012627625837922096
        vf_explained_var: 0.1049012839794159
        vf_loss: 20.900978088378906
    load_time_ms: 14315.31
    num_steps_sampled: 35328000
    num_steps_trained: 35328000
    sample_time_ms: 90171.176
    update_time_ms: 21.508
  iterations_since_restore: 348
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.41129943502825
    ram_util_percent: 15.590960451977399
  pid: 4061
  policy_reward_max:
    agent-0: 204.6666666666666
    agent-1: 204.6666666666666
    agent-2: 204.6666666666666
    agent-3: 204.6666666666666
    agent-4: 204.6666666666666
    agent-5: 204.6666666666666
  policy_reward_mean:
    agent-0: 175.9949999999998
    agent-1: 175.9949999999998
    agent-2: 175.9949999999998
    agent-3: 175.9949999999998
    agent-4: 175.9949999999998
    agent-5: 175.9949999999998
  policy_reward_min:
    agent-0: 90.83333333333333
    agent-1: 90.83333333333333
    agent-2: 90.83333333333333
    agent-3: 90.83333333333333
    agent-4: 90.83333333333333
    agent-5: 90.83333333333333
  sampler_perf:
    mean_env_wait_ms: 24.274607204187646
    mean_inference_ms: 12.325529860531105
    mean_processing_ms: 50.93085590775277
  time_since_restore: 45466.342849731445
  time_this_iter_s: 124.1251232624054
  time_total_s: 48677.406535863876
  timestamp: 1637063051
  timesteps_since_restore: 33408000
  timesteps_this_iter: 96000
  timesteps_total: 35328000
  training_iteration: 368
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    368 |          48677.4 | 35328000 |  1055.97 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 37
    apples_agent-0_mean: 2.48
    apples_agent-0_min: 0
    apples_agent-1_max: 82
    apples_agent-1_mean: 19.36
    apples_agent-1_min: 0
    apples_agent-2_max: 195
    apples_agent-2_mean: 20.32
    apples_agent-2_min: 0
    apples_agent-3_max: 103
    apples_agent-3_mean: 58.83
    apples_agent-3_min: 24
    apples_agent-4_max: 80
    apples_agent-4_mean: 1.06
    apples_agent-4_min: 0
    apples_agent-5_max: 160
    apples_agent-5_mean: 95.31
    apples_agent-5_min: 51
    cleaning_beam_agent-0_max: 591
    cleaning_beam_agent-0_mean: 470.1
    cleaning_beam_agent-0_min: 297
    cleaning_beam_agent-1_max: 463
    cleaning_beam_agent-1_mean: 322.63
    cleaning_beam_agent-1_min: 164
    cleaning_beam_agent-2_max: 558
    cleaning_beam_agent-2_mean: 347.86
    cleaning_beam_agent-2_min: 127
    cleaning_beam_agent-3_max: 69
    cleaning_beam_agent-3_mean: 21.94
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 573
    cleaning_beam_agent-4_mean: 435.61
    cleaning_beam_agent-4_min: 339
    cleaning_beam_agent-5_max: 138
    cleaning_beam_agent-5_mean: 25.99
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-46-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1201.9999999999907
  episode_reward_mean: 1030.029999999989
  episode_reward_min: 537.0000000000085
  episodes_this_iter: 96
  episodes_total: 35424
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20217.873
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8620341420173645
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012411666102707386
        model: {}
        policy_loss: -0.0028313903603702784
        total_loss: -0.0019963779486715794
        vf_explained_var: 0.036476075649261475
        vf_loss: 23.52191925048828
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1097909212112427
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014842657838016748
        model: {}
        policy_loss: -0.0037610125727951527
        total_loss: -0.0032569661270827055
        vf_explained_var: -0.00863654911518097
        vf_loss: 24.572797775268555
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1365643739700317
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013590003363788128
        model: {}
        policy_loss: -0.003079427871853113
        total_loss: -0.0027039803098887205
        vf_explained_var: 0.0250442773103714
        vf_loss: 23.757972717285156
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39179179072380066
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009439928107894957
        model: {}
        policy_loss: -0.0024870638735592365
        total_loss: -0.0009858389385044575
        vf_explained_var: 0.10155656933784485
        vf_loss: 21.907777786254883
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8949955105781555
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015106763457879424
        model: {}
        policy_loss: -0.0033603282645344734
        total_loss: -0.0025969576090574265
        vf_explained_var: 0.04136541485786438
        vf_loss: 23.385616302490234
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5195399522781372
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012688027927652001
        model: {}
        policy_loss: -0.0032187006436288357
        total_loss: -0.0019888347014784813
        vf_explained_var: 0.1200675219297409
        vf_loss: 21.442588806152344
    load_time_ms: 14085.761
    num_steps_sampled: 35424000
    num_steps_trained: 35424000
    sample_time_ms: 90184.214
    update_time_ms: 20.341
  iterations_since_restore: 349
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.490857142857141
    ram_util_percent: 15.638285714285713
  pid: 4061
  policy_reward_max:
    agent-0: 200.33333333333312
    agent-1: 200.33333333333312
    agent-2: 200.33333333333312
    agent-3: 200.33333333333312
    agent-4: 200.33333333333312
    agent-5: 200.33333333333312
  policy_reward_mean:
    agent-0: 171.67166666666654
    agent-1: 171.67166666666654
    agent-2: 171.67166666666654
    agent-3: 171.67166666666654
    agent-4: 171.67166666666654
    agent-5: 171.67166666666654
  policy_reward_min:
    agent-0: 89.50000000000034
    agent-1: 89.50000000000034
    agent-2: 89.50000000000034
    agent-3: 89.50000000000034
    agent-4: 89.50000000000034
    agent-5: 89.50000000000034
  sampler_perf:
    mean_env_wait_ms: 24.276357724154863
    mean_inference_ms: 12.325040754940686
    mean_processing_ms: 50.92805242140964
  time_since_restore: 45589.58066248894
  time_this_iter_s: 123.23781275749207
  time_total_s: 48800.64434862137
  timestamp: 1637063174
  timesteps_since_restore: 33504000
  timesteps_this_iter: 96000
  timesteps_total: 35424000
  training_iteration: 369
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    369 |          48800.6 | 35424000 |  1030.03 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 44
    apples_agent-0_mean: 1.54
    apples_agent-0_min: 0
    apples_agent-1_max: 99
    apples_agent-1_mean: 19.58
    apples_agent-1_min: 0
    apples_agent-2_max: 167
    apples_agent-2_mean: 17.28
    apples_agent-2_min: 0
    apples_agent-3_max: 93
    apples_agent-3_mean: 55.58
    apples_agent-3_min: 24
    apples_agent-4_max: 64
    apples_agent-4_mean: 1.94
    apples_agent-4_min: 0
    apples_agent-5_max: 216
    apples_agent-5_mean: 100.96
    apples_agent-5_min: 50
    cleaning_beam_agent-0_max: 607
    cleaning_beam_agent-0_mean: 465.37
    cleaning_beam_agent-0_min: 351
    cleaning_beam_agent-1_max: 519
    cleaning_beam_agent-1_mean: 326.35
    cleaning_beam_agent-1_min: 144
    cleaning_beam_agent-2_max: 509
    cleaning_beam_agent-2_mean: 343.65
    cleaning_beam_agent-2_min: 160
    cleaning_beam_agent-3_max: 52
    cleaning_beam_agent-3_mean: 19.28
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 574
    cleaning_beam_agent-4_mean: 444.95
    cleaning_beam_agent-4_min: 295
    cleaning_beam_agent-5_max: 232
    cleaning_beam_agent-5_mean: 19.94
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-48-18
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1235.999999999998
  episode_reward_mean: 1058.5499999999918
  episode_reward_min: 546.000000000011
  episodes_this_iter: 96
  episodes_total: 35520
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20211.834
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8428331613540649
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013343766331672668
        model: {}
        policy_loss: -0.0026557808741927147
        total_loss: -0.001836919691413641
        vf_explained_var: 0.00617586076259613
        vf_loss: 23.0224552154541
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1057982444763184
        entropy_coeff: 0.0017600000137463212
        kl: 0.001435832935385406
        model: {}
        policy_loss: -0.0036562420427799225
        total_loss: -0.0033107418566942215
        vf_explained_var: 0.015317335724830627
        vf_loss: 22.917043685913086
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.139620065689087
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016470886766910553
        model: {}
        policy_loss: -0.0033416058868169785
        total_loss: -0.003048162441700697
        vf_explained_var: 0.013707324862480164
        vf_loss: 22.99172592163086
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37971362471580505
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012618948239833117
        model: {}
        policy_loss: -0.002353066112846136
        total_loss: -0.0009090239182114601
        vf_explained_var: 0.08761674165725708
        vf_loss: 21.123367309570312
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8837261199951172
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015733614563941956
        model: {}
        policy_loss: -0.003715158673003316
        total_loss: -0.003029024228453636
        vf_explained_var: 0.03506965935230255
        vf_loss: 22.414947509765625
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4974096119403839
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008317576721310616
        model: {}
        policy_loss: -0.00276567367836833
        total_loss: -0.0014898041263222694
        vf_explained_var: 0.07293559610843658
        vf_loss: 21.513090133666992
    load_time_ms: 13811.652
    num_steps_sampled: 35520000
    num_steps_trained: 35520000
    sample_time_ms: 90139.497
    update_time_ms: 20.593
  iterations_since_restore: 350
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.401136363636363
    ram_util_percent: 15.590340909090907
  pid: 4061
  policy_reward_max:
    agent-0: 206.00000000000026
    agent-1: 206.00000000000026
    agent-2: 206.00000000000026
    agent-3: 206.00000000000026
    agent-4: 206.00000000000026
    agent-5: 206.00000000000026
  policy_reward_mean:
    agent-0: 176.4249999999998
    agent-1: 176.4249999999998
    agent-2: 176.4249999999998
    agent-3: 176.4249999999998
    agent-4: 176.4249999999998
    agent-5: 176.4249999999998
  policy_reward_min:
    agent-0: 91.0000000000003
    agent-1: 91.0000000000003
    agent-2: 91.0000000000003
    agent-3: 91.0000000000003
    agent-4: 91.0000000000003
    agent-5: 91.0000000000003
  sampler_perf:
    mean_env_wait_ms: 24.278022966916275
    mean_inference_ms: 12.324419321197924
    mean_processing_ms: 50.9256043203674
  time_since_restore: 45712.96156215668
  time_this_iter_s: 123.38089966773987
  time_total_s: 48924.02524828911
  timestamp: 1637063298
  timesteps_since_restore: 33600000
  timesteps_this_iter: 96000
  timesteps_total: 35520000
  training_iteration: 370
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 28.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    370 |            48924 | 35520000 |  1058.55 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 60
    apples_agent-0_mean: 1.92
    apples_agent-0_min: 0
    apples_agent-1_max: 81
    apples_agent-1_mean: 21.46
    apples_agent-1_min: 0
    apples_agent-2_max: 398
    apples_agent-2_mean: 23.62
    apples_agent-2_min: 0
    apples_agent-3_max: 334
    apples_agent-3_mean: 58.25
    apples_agent-3_min: 25
    apples_agent-4_max: 54
    apples_agent-4_mean: 1.29
    apples_agent-4_min: 0
    apples_agent-5_max: 188
    apples_agent-5_mean: 102.63
    apples_agent-5_min: 55
    cleaning_beam_agent-0_max: 592
    cleaning_beam_agent-0_mean: 470.58
    cleaning_beam_agent-0_min: 295
    cleaning_beam_agent-1_max: 605
    cleaning_beam_agent-1_mean: 334.42
    cleaning_beam_agent-1_min: 182
    cleaning_beam_agent-2_max: 490
    cleaning_beam_agent-2_mean: 352.7
    cleaning_beam_agent-2_min: 80
    cleaning_beam_agent-3_max: 143
    cleaning_beam_agent-3_mean: 21.34
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 581
    cleaning_beam_agent-4_mean: 455.2
    cleaning_beam_agent-4_min: 369
    cleaning_beam_agent-5_max: 326
    cleaning_beam_agent-5_mean: 22.04
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-50-22
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1232.000000000001
  episode_reward_mean: 1050.369999999993
  episode_reward_min: 563.0000000000016
  episodes_this_iter: 96
  episodes_total: 35616
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20200.433
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8352492451667786
        entropy_coeff: 0.0017600000137463212
        kl: 0.001532213413156569
        model: {}
        policy_loss: -0.0028739923145622015
        total_loss: -0.0019207994919270277
        vf_explained_var: 0.04131230711936951
        vf_loss: 24.232330322265625
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1099796295166016
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017299557803198695
        model: {}
        policy_loss: -0.003754024626687169
        total_loss: -0.003085626754909754
        vf_explained_var: -0.03663194179534912
        vf_loss: 26.219619750976562
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1289575099945068
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013507872354239225
        model: {}
        policy_loss: -0.0030082659795880318
        total_loss: -0.0025622909888625145
        vf_explained_var: 0.037087708711624146
        vf_loss: 24.32939910888672
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3773570954799652
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008861623937264085
        model: {}
        policy_loss: -0.002138985786587
        total_loss: -0.000510183337610215
        vf_explained_var: 0.09195619821548462
        vf_loss: 22.929519653320312
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8856809139251709
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013257669052109122
        model: {}
        policy_loss: -0.00331446947529912
        total_loss: -0.0024435119703412056
        vf_explained_var: 0.03791581094264984
        vf_loss: 24.29752540588379
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5093458890914917
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007275119423866272
        model: {}
        policy_loss: -0.0024517744313925505
        total_loss: -0.0010005926014855504
        vf_explained_var: 0.0713251382112503
        vf_loss: 23.476303100585938
    load_time_ms: 13718.525
    num_steps_sampled: 35616000
    num_steps_trained: 35616000
    sample_time_ms: 90140.819
    update_time_ms: 20.197
  iterations_since_restore: 351
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 16.224719101123597
    ram_util_percent: 15.621348314606742
  pid: 4061
  policy_reward_max:
    agent-0: 205.33333333333272
    agent-1: 205.33333333333272
    agent-2: 205.33333333333272
    agent-3: 205.33333333333272
    agent-4: 205.33333333333272
    agent-5: 205.33333333333272
  policy_reward_mean:
    agent-0: 175.06166666666653
    agent-1: 175.06166666666653
    agent-2: 175.06166666666653
    agent-3: 175.06166666666653
    agent-4: 175.06166666666653
    agent-5: 175.06166666666653
  policy_reward_min:
    agent-0: 93.83333333333371
    agent-1: 93.83333333333371
    agent-2: 93.83333333333371
    agent-3: 93.83333333333371
    agent-4: 93.83333333333371
    agent-5: 93.83333333333371
  sampler_perf:
    mean_env_wait_ms: 24.280426033149915
    mean_inference_ms: 12.324032843885563
    mean_processing_ms: 50.923502083570675
  time_since_restore: 45837.457169771194
  time_this_iter_s: 124.49560761451721
  time_total_s: 49048.520855903625
  timestamp: 1637063422
  timesteps_since_restore: 33696000
  timesteps_this_iter: 96000
  timesteps_total: 35616000
  training_iteration: 371
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    371 |          49048.5 | 35616000 |  1050.37 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 58
    apples_agent-0_mean: 2.02
    apples_agent-0_min: 0
    apples_agent-1_max: 109
    apples_agent-1_mean: 19.42
    apples_agent-1_min: 0
    apples_agent-2_max: 191
    apples_agent-2_mean: 21.93
    apples_agent-2_min: 0
    apples_agent-3_max: 103
    apples_agent-3_mean: 53.94
    apples_agent-3_min: 26
    apples_agent-4_max: 28
    apples_agent-4_mean: 0.71
    apples_agent-4_min: 0
    apples_agent-5_max: 199
    apples_agent-5_mean: 99.47
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 569
    cleaning_beam_agent-0_mean: 470.49
    cleaning_beam_agent-0_min: 301
    cleaning_beam_agent-1_max: 586
    cleaning_beam_agent-1_mean: 337.77
    cleaning_beam_agent-1_min: 163
    cleaning_beam_agent-2_max: 545
    cleaning_beam_agent-2_mean: 355.83
    cleaning_beam_agent-2_min: 141
    cleaning_beam_agent-3_max: 70
    cleaning_beam_agent-3_mean: 16.81
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 581
    cleaning_beam_agent-4_mean: 455.7
    cleaning_beam_agent-4_min: 321
    cleaning_beam_agent-5_max: 769
    cleaning_beam_agent-5_mean: 31.24
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-52-27
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1253.0000000000014
  episode_reward_mean: 1067.199999999992
  episode_reward_min: 539.000000000012
  episodes_this_iter: 96
  episodes_total: 35712
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20172.615
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8419455885887146
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017586714820936322
        model: {}
        policy_loss: -0.002626067725941539
        total_loss: -0.0018356011714786291
        vf_explained_var: 0.007687047123908997
        vf_loss: 22.722919464111328
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1109092235565186
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016657044179737568
        model: {}
        policy_loss: -0.003669409779831767
        total_loss: -0.0033422221895307302
        vf_explained_var: 0.009305164217948914
        vf_loss: 22.82386589050293
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1226623058319092
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017783257644623518
        model: {}
        policy_loss: -0.0033964496105909348
        total_loss: -0.003114636056125164
        vf_explained_var: 0.0219058096408844
        vf_loss: 22.57697868347168
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3530004322528839
        entropy_coeff: 0.0017600000137463212
        kl: 0.001234266092069447
        model: {}
        policy_loss: -0.002261507324874401
        total_loss: -0.0008297402528114617
        vf_explained_var: 0.10236240923404694
        vf_loss: 20.530481338500977
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.892866849899292
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016332088271155953
        model: {}
        policy_loss: -0.003818938974291086
        total_loss: -0.0032288727816194296
        vf_explained_var: 0.05784377455711365
        vf_loss: 21.615102767944336
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5172812938690186
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011418939102441072
        model: {}
        policy_loss: -0.0029377900063991547
        total_loss: -0.001708689145743847
        vf_explained_var: 0.07513497769832611
        vf_loss: 21.395156860351562
    load_time_ms: 13671.805
    num_steps_sampled: 35712000
    num_steps_trained: 35712000
    sample_time_ms: 90092.546
    update_time_ms: 20.773
  iterations_since_restore: 352
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.321910112359552
    ram_util_percent: 15.664044943820224
  pid: 4061
  policy_reward_max:
    agent-0: 208.83333333333331
    agent-1: 208.83333333333331
    agent-2: 208.83333333333331
    agent-3: 208.83333333333331
    agent-4: 208.83333333333331
    agent-5: 208.83333333333331
  policy_reward_mean:
    agent-0: 177.8666666666665
    agent-1: 177.8666666666665
    agent-2: 177.8666666666665
    agent-3: 177.8666666666665
    agent-4: 177.8666666666665
    agent-5: 177.8666666666665
  policy_reward_min:
    agent-0: 89.8333333333336
    agent-1: 89.8333333333336
    agent-2: 89.8333333333336
    agent-3: 89.8333333333336
    agent-4: 89.8333333333336
    agent-5: 89.8333333333336
  sampler_perf:
    mean_env_wait_ms: 24.28295056713954
    mean_inference_ms: 12.32367122118385
    mean_processing_ms: 50.92173764570976
  time_since_restore: 45962.25882792473
  time_this_iter_s: 124.80165815353394
  time_total_s: 49173.32251405716
  timestamp: 1637063547
  timesteps_since_restore: 33792000
  timesteps_this_iter: 96000
  timesteps_total: 35712000
  training_iteration: 372
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    372 |          49173.3 | 35712000 |   1067.2 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 35
    apples_agent-0_mean: 1.36
    apples_agent-0_min: 0
    apples_agent-1_max: 119
    apples_agent-1_mean: 21.17
    apples_agent-1_min: 0
    apples_agent-2_max: 96
    apples_agent-2_mean: 12.37
    apples_agent-2_min: 0
    apples_agent-3_max: 103
    apples_agent-3_mean: 57.46
    apples_agent-3_min: 30
    apples_agent-4_max: 28
    apples_agent-4_mean: 0.28
    apples_agent-4_min: 0
    apples_agent-5_max: 187
    apples_agent-5_mean: 99.94
    apples_agent-5_min: 36
    cleaning_beam_agent-0_max: 581
    cleaning_beam_agent-0_mean: 487.28
    cleaning_beam_agent-0_min: 368
    cleaning_beam_agent-1_max: 494
    cleaning_beam_agent-1_mean: 322.27
    cleaning_beam_agent-1_min: 137
    cleaning_beam_agent-2_max: 540
    cleaning_beam_agent-2_mean: 377.15
    cleaning_beam_agent-2_min: 197
    cleaning_beam_agent-3_max: 127
    cleaning_beam_agent-3_mean: 19.13
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 655
    cleaning_beam_agent-4_mean: 467.5
    cleaning_beam_agent-4_min: 351
    cleaning_beam_agent-5_max: 351
    cleaning_beam_agent-5_mean: 20.05
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-54-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1246.0000000000084
  episode_reward_mean: 1077.0899999999942
  episode_reward_min: 491.0000000000009
  episodes_this_iter: 96
  episodes_total: 35808
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20160.026
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8401115536689758
        entropy_coeff: 0.0017600000137463212
        kl: 0.001612763269804418
        model: {}
        policy_loss: -0.003081765491515398
        total_loss: -0.002279390348121524
        vf_explained_var: 0.028346389532089233
        vf_loss: 22.809690475463867
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1234087944030762
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014369761338457465
        model: {}
        policy_loss: -0.003454918973147869
        total_loss: -0.0030173114500939846
        vf_explained_var: -0.016492798924446106
        vf_loss: 24.148090362548828
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1242754459381104
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016570001607760787
        model: {}
        policy_loss: -0.0032207982148975134
        total_loss: -0.0028586057014763355
        vf_explained_var: 0.004214972257614136
        vf_loss: 23.409143447875977
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.34985625743865967
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008807572303339839
        model: {}
        policy_loss: -0.0020247837528586388
        total_loss: -0.00048720836639404297
        vf_explained_var: 0.08061885833740234
        vf_loss: 21.533214569091797
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8752384185791016
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022879927419126034
        model: {}
        policy_loss: -0.0037632985040545464
        total_loss: -0.003001593053340912
        vf_explained_var: 0.020561188459396362
        vf_loss: 23.02126693725586
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49460282921791077
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005367012927308679
        model: {}
        policy_loss: -0.0024813925847411156
        total_loss: -0.0012321006506681442
        vf_explained_var: 0.09929637610912323
        vf_loss: 21.197940826416016
    load_time_ms: 13656.835
    num_steps_sampled: 35808000
    num_steps_trained: 35808000
    sample_time_ms: 90233.065
    update_time_ms: 20.62
  iterations_since_restore: 353
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.303932584269662
    ram_util_percent: 15.573033707865168
  pid: 4061
  policy_reward_max:
    agent-0: 207.6666666666662
    agent-1: 207.6666666666662
    agent-2: 207.6666666666662
    agent-3: 207.6666666666662
    agent-4: 207.6666666666662
    agent-5: 207.6666666666662
  policy_reward_mean:
    agent-0: 179.51499999999982
    agent-1: 179.51499999999982
    agent-2: 179.51499999999982
    agent-3: 179.51499999999982
    agent-4: 179.51499999999982
    agent-5: 179.51499999999982
  policy_reward_min:
    agent-0: 81.83333333333316
    agent-1: 81.83333333333316
    agent-2: 81.83333333333316
    agent-3: 81.83333333333316
    agent-4: 81.83333333333316
    agent-5: 81.83333333333316
  sampler_perf:
    mean_env_wait_ms: 24.286322981983826
    mean_inference_ms: 12.323229441116977
    mean_processing_ms: 50.92005476127454
  time_since_restore: 46087.261053323746
  time_this_iter_s: 125.00222539901733
  time_total_s: 49298.32473945618
  timestamp: 1637063672
  timesteps_since_restore: 33888000
  timesteps_this_iter: 96000
  timesteps_total: 35808000
  training_iteration: 373
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    373 |          49298.3 | 35808000 |  1077.09 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 0.74
    apples_agent-0_min: 0
    apples_agent-1_max: 82
    apples_agent-1_mean: 21.39
    apples_agent-1_min: 0
    apples_agent-2_max: 255
    apples_agent-2_mean: 18.09
    apples_agent-2_min: 0
    apples_agent-3_max: 84
    apples_agent-3_mean: 54.18
    apples_agent-3_min: 24
    apples_agent-4_max: 63
    apples_agent-4_mean: 1.18
    apples_agent-4_min: 0
    apples_agent-5_max: 259
    apples_agent-5_mean: 101.95
    apples_agent-5_min: 64
    cleaning_beam_agent-0_max: 593
    cleaning_beam_agent-0_mean: 479.42
    cleaning_beam_agent-0_min: 399
    cleaning_beam_agent-1_max: 492
    cleaning_beam_agent-1_mean: 309.95
    cleaning_beam_agent-1_min: 151
    cleaning_beam_agent-2_max: 540
    cleaning_beam_agent-2_mean: 373.81
    cleaning_beam_agent-2_min: 187
    cleaning_beam_agent-3_max: 81
    cleaning_beam_agent-3_mean: 18.33
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 571
    cleaning_beam_agent-4_mean: 469.65
    cleaning_beam_agent-4_min: 252
    cleaning_beam_agent-5_max: 116
    cleaning_beam_agent-5_mean: 17.09
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-56-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1224.999999999992
  episode_reward_mean: 1077.2099999999937
  episode_reward_min: 457.0000000000115
  episodes_this_iter: 96
  episodes_total: 35904
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20164.91
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8519531488418579
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013240724802017212
        model: {}
        policy_loss: -0.002815328538417816
        total_loss: -0.0021291952580213547
        vf_explained_var: 0.06043654680252075
        vf_loss: 21.85565757751465
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.125084638595581
        entropy_coeff: 0.0017600000137463212
        kl: 0.001311447937041521
        model: {}
        policy_loss: -0.003853198140859604
        total_loss: -0.003461413085460663
        vf_explained_var: -0.011372670531272888
        vf_loss: 23.719327926635742
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1142048835754395
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012899747816845775
        model: {}
        policy_loss: -0.003209786955267191
        total_loss: -0.00296147633343935
        vf_explained_var: 0.05137917399406433
        vf_loss: 22.09310531616211
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3592710793018341
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009272942552343011
        model: {}
        policy_loss: -0.002309731673449278
        total_loss: -0.0007941932417452335
        vf_explained_var: 0.07425457239151001
        vf_loss: 21.478567123413086
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.884943962097168
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017387664411216974
        model: {}
        policy_loss: -0.0037228951696306467
        total_loss: -0.0030205773655325174
        vf_explained_var: 0.03760711848735809
        vf_loss: 22.598201751708984
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5078811645507812
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009041879675351083
        model: {}
        policy_loss: -0.0026658261194825172
        total_loss: -0.0014459970407187939
        vf_explained_var: 0.09565271437168121
        vf_loss: 21.13699722290039
    load_time_ms: 13635.312
    num_steps_sampled: 35904000
    num_steps_trained: 35904000
    sample_time_ms: 90261.075
    update_time_ms: 21.223
  iterations_since_restore: 354
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.393785310734462
    ram_util_percent: 15.654237288135592
  pid: 4061
  policy_reward_max:
    agent-0: 204.1666666666661
    agent-1: 204.1666666666661
    agent-2: 204.1666666666661
    agent-3: 204.1666666666661
    agent-4: 204.1666666666661
    agent-5: 204.1666666666661
  policy_reward_mean:
    agent-0: 179.53499999999977
    agent-1: 179.53499999999977
    agent-2: 179.53499999999977
    agent-3: 179.53499999999977
    agent-4: 179.53499999999977
    agent-5: 179.53499999999977
  policy_reward_min:
    agent-0: 76.16666666666669
    agent-1: 76.16666666666669
    agent-2: 76.16666666666669
    agent-3: 76.16666666666669
    agent-4: 76.16666666666669
    agent-5: 76.16666666666669
  sampler_perf:
    mean_env_wait_ms: 24.288983716369703
    mean_inference_ms: 12.322696798789666
    mean_processing_ms: 50.91773187025987
  time_since_restore: 46211.48470425606
  time_this_iter_s: 124.22365093231201
  time_total_s: 49422.54839038849
  timestamp: 1637063797
  timesteps_since_restore: 33984000
  timesteps_this_iter: 96000
  timesteps_total: 35904000
  training_iteration: 374
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    374 |          49422.5 | 35904000 |  1077.21 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 50
    apples_agent-0_mean: 0.71
    apples_agent-0_min: 0
    apples_agent-1_max: 103
    apples_agent-1_mean: 22.12
    apples_agent-1_min: 0
    apples_agent-2_max: 193
    apples_agent-2_mean: 17.23
    apples_agent-2_min: 0
    apples_agent-3_max: 475
    apples_agent-3_mean: 63.67
    apples_agent-3_min: 34
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 510
    apples_agent-5_mean: 104.03
    apples_agent-5_min: 48
    cleaning_beam_agent-0_max: 579
    cleaning_beam_agent-0_mean: 473.36
    cleaning_beam_agent-0_min: 387
    cleaning_beam_agent-1_max: 483
    cleaning_beam_agent-1_mean: 307.14
    cleaning_beam_agent-1_min: 136
    cleaning_beam_agent-2_max: 569
    cleaning_beam_agent-2_mean: 384.58
    cleaning_beam_agent-2_min: 225
    cleaning_beam_agent-3_max: 98
    cleaning_beam_agent-3_mean: 17.18
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 610
    cleaning_beam_agent-4_mean: 473.29
    cleaning_beam_agent-4_min: 364
    cleaning_beam_agent-5_max: 397
    cleaning_beam_agent-5_mean: 23.66
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-58-41
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1203.000000000001
  episode_reward_mean: 1075.70999999999
  episode_reward_min: 594.9999999999965
  episodes_this_iter: 96
  episodes_total: 36000
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20170.968
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.852923572063446
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020027239806950092
        model: {}
        policy_loss: -0.00277193752117455
        total_loss: -0.002195102861151099
        vf_explained_var: 0.027983814477920532
        vf_loss: 20.779775619506836
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.132265329360962
        entropy_coeff: 0.0017600000137463212
        kl: 0.001177361817099154
        model: {}
        policy_loss: -0.003242696635425091
        total_loss: -0.003040055278688669
        vf_explained_var: -0.019025757908821106
        vf_loss: 21.954282760620117
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1000432968139648
        entropy_coeff: 0.0017600000137463212
        kl: 0.001528394059278071
        model: {}
        policy_loss: -0.003034734632819891
        total_loss: -0.002866523340344429
        vf_explained_var: 0.014936238527297974
        vf_loss: 21.042871475219727
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.33238083124160767
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009099884191527963
        model: {}
        policy_loss: -0.0021022665314376354
        total_loss: -0.0006552827544510365
        vf_explained_var: 0.0465877503156662
        vf_loss: 20.319759368896484
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.883590817451477
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018052710220217705
        model: {}
        policy_loss: -0.003514499869197607
        total_loss: -0.00291281845420599
        vf_explained_var: -0.010809481143951416
        vf_loss: 21.567991256713867
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.501512885093689
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012286320561543107
        model: {}
        policy_loss: -0.002509506419301033
        total_loss: -0.001341479830443859
        vf_explained_var: 0.041429728269577026
        vf_loss: 20.50689125061035
    load_time_ms: 13597.168
    num_steps_sampled: 36000000
    num_steps_trained: 36000000
    sample_time_ms: 90275.82
    update_time_ms: 21.741
  iterations_since_restore: 355
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.278531073446329
    ram_util_percent: 15.585310734463274
  pid: 4061
  policy_reward_max:
    agent-0: 200.49999999999972
    agent-1: 200.49999999999972
    agent-2: 200.49999999999972
    agent-3: 200.49999999999972
    agent-4: 200.49999999999972
    agent-5: 200.49999999999972
  policy_reward_mean:
    agent-0: 179.28499999999983
    agent-1: 179.28499999999983
    agent-2: 179.28499999999983
    agent-3: 179.28499999999983
    agent-4: 179.28499999999983
    agent-5: 179.28499999999983
  policy_reward_min:
    agent-0: 99.16666666666681
    agent-1: 99.16666666666681
    agent-2: 99.16666666666681
    agent-3: 99.16666666666681
    agent-4: 99.16666666666681
    agent-5: 99.16666666666681
  sampler_perf:
    mean_env_wait_ms: 24.291936438848037
    mean_inference_ms: 12.322184423863836
    mean_processing_ms: 50.91545524868988
  time_since_restore: 46335.54236412048
  time_this_iter_s: 124.05765986442566
  time_total_s: 49546.606050252914
  timestamp: 1637063921
  timesteps_since_restore: 34080000
  timesteps_this_iter: 96000
  timesteps_total: 36000000
  training_iteration: 375
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    375 |          49546.6 | 36000000 |  1075.71 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 74
    apples_agent-0_mean: 1.53
    apples_agent-0_min: 0
    apples_agent-1_max: 87
    apples_agent-1_mean: 23.16
    apples_agent-1_min: 0
    apples_agent-2_max: 191
    apples_agent-2_mean: 13.85
    apples_agent-2_min: 0
    apples_agent-3_max: 103
    apples_agent-3_mean: 54.44
    apples_agent-3_min: 16
    apples_agent-4_max: 68
    apples_agent-4_mean: 3.26
    apples_agent-4_min: 0
    apples_agent-5_max: 186
    apples_agent-5_mean: 100.0
    apples_agent-5_min: 39
    cleaning_beam_agent-0_max: 545
    cleaning_beam_agent-0_mean: 466.82
    cleaning_beam_agent-0_min: 258
    cleaning_beam_agent-1_max: 506
    cleaning_beam_agent-1_mean: 305.96
    cleaning_beam_agent-1_min: 137
    cleaning_beam_agent-2_max: 569
    cleaning_beam_agent-2_mean: 398.55
    cleaning_beam_agent-2_min: 252
    cleaning_beam_agent-3_max: 94
    cleaning_beam_agent-3_mean: 17.43
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 587
    cleaning_beam_agent-4_mean: 474.14
    cleaning_beam_agent-4_min: 351
    cleaning_beam_agent-5_max: 225
    cleaning_beam_agent-5_mean: 22.92
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-00-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1234.9999999999868
  episode_reward_mean: 1040.8599999999917
  episode_reward_min: 330.0000000000002
  episodes_this_iter: 96
  episodes_total: 36096
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20198.31
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8665062785148621
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016295196255668998
        model: {}
        policy_loss: -0.0028029796667397022
        total_loss: -0.0017168247140944004
        vf_explained_var: 0.04170075058937073
        vf_loss: 26.11206817626953
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1191802024841309
        entropy_coeff: 0.0017600000137463212
        kl: 0.001725375885143876
        model: {}
        policy_loss: -0.003608167404308915
        total_loss: -0.002796929096803069
        vf_explained_var: -0.023292481899261475
        vf_loss: 27.809972763061523
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.095725417137146
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012181976344436407
        model: {}
        policy_loss: -0.003071570536121726
        total_loss: -0.0023544016294181347
        vf_explained_var: 0.031122609972953796
        vf_loss: 26.456451416015625
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3689518868923187
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010375104611739516
        model: {}
        policy_loss: -0.0024242454674094915
        total_loss: -0.0007254478987306356
        vf_explained_var: 0.13752979040145874
        vf_loss: 23.48151969909668
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8871762752532959
        entropy_coeff: 0.0017600000137463212
        kl: 0.001598304370418191
        model: {}
        policy_loss: -0.0036648069508373737
        total_loss: -0.0027583590708673
        vf_explained_var: 0.09225846827030182
        vf_loss: 24.67875862121582
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5240745544433594
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010377977741882205
        model: {}
        policy_loss: -0.0029399851337075233
        total_loss: -0.0014685706701129675
        vf_explained_var: 0.11940377950668335
        vf_loss: 23.93787956237793
    load_time_ms: 13591.942
    num_steps_sampled: 36096000
    num_steps_trained: 36096000
    sample_time_ms: 90202.765
    update_time_ms: 21.948
  iterations_since_restore: 356
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 17.084831460674156
    ram_util_percent: 15.725842696629213
  pid: 4061
  policy_reward_max:
    agent-0: 205.83333333333314
    agent-1: 205.83333333333314
    agent-2: 205.83333333333314
    agent-3: 205.83333333333314
    agent-4: 205.83333333333314
    agent-5: 205.83333333333314
  policy_reward_mean:
    agent-0: 173.47666666666655
    agent-1: 173.47666666666655
    agent-2: 173.47666666666655
    agent-3: 173.47666666666655
    agent-4: 173.47666666666655
    agent-5: 173.47666666666655
  policy_reward_min:
    agent-0: 54.99999999999993
    agent-1: 54.99999999999993
    agent-2: 54.99999999999993
    agent-3: 54.99999999999993
    agent-4: 54.99999999999993
    agent-5: 54.99999999999993
  sampler_perf:
    mean_env_wait_ms: 24.294766697228077
    mean_inference_ms: 12.321800221108465
    mean_processing_ms: 50.91367283440227
  time_since_restore: 46460.14829945564
  time_this_iter_s: 124.6059353351593
  time_total_s: 49671.211985588074
  timestamp: 1637064046
  timesteps_since_restore: 34176000
  timesteps_this_iter: 96000
  timesteps_total: 36096000
  training_iteration: 376
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    376 |          49671.2 | 36096000 |  1040.86 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 59
    apples_agent-0_mean: 1.46
    apples_agent-0_min: 0
    apples_agent-1_max: 112
    apples_agent-1_mean: 20.32
    apples_agent-1_min: 0
    apples_agent-2_max: 333
    apples_agent-2_mean: 19.91
    apples_agent-2_min: 0
    apples_agent-3_max: 104
    apples_agent-3_mean: 56.6
    apples_agent-3_min: 24
    apples_agent-4_max: 58
    apples_agent-4_mean: 0.58
    apples_agent-4_min: 0
    apples_agent-5_max: 352
    apples_agent-5_mean: 97.8
    apples_agent-5_min: 24
    cleaning_beam_agent-0_max: 579
    cleaning_beam_agent-0_mean: 466.28
    cleaning_beam_agent-0_min: 287
    cleaning_beam_agent-1_max: 457
    cleaning_beam_agent-1_mean: 316.68
    cleaning_beam_agent-1_min: 174
    cleaning_beam_agent-2_max: 615
    cleaning_beam_agent-2_mean: 387.01
    cleaning_beam_agent-2_min: 156
    cleaning_beam_agent-3_max: 92
    cleaning_beam_agent-3_mean: 15.24
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 586
    cleaning_beam_agent-4_mean: 468.48
    cleaning_beam_agent-4_min: 369
    cleaning_beam_agent-5_max: 333
    cleaning_beam_agent-5_mean: 22.28
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-02-50
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1206.9999999999995
  episode_reward_mean: 1065.3399999999938
  episode_reward_min: 441.0000000000104
  episodes_this_iter: 96
  episodes_total: 36192
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20190.25
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8612149953842163
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014311808627098799
        model: {}
        policy_loss: -0.002717209979891777
        total_loss: -0.0020144470036029816
        vf_explained_var: 0.03847073018550873
        vf_loss: 22.185016632080078
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1274775266647339
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017086346633732319
        model: {}
        policy_loss: -0.0040008071810007095
        total_loss: -0.003617700422182679
        vf_explained_var: -0.02193590998649597
        vf_loss: 23.674623489379883
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.092584490776062
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011883293045684695
        model: {}
        policy_loss: -0.0032455408945679665
        total_loss: -0.0028293514624238014
        vf_explained_var: -0.011942476034164429
        vf_loss: 23.391389846801758
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3500154912471771
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013877315213903785
        model: {}
        policy_loss: -0.002384041203185916
        total_loss: -0.0009080870077013969
        vf_explained_var: 0.09284667670726776
        vf_loss: 20.919815063476562
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8977677822113037
        entropy_coeff: 0.0017600000137463212
        kl: 0.002204130869358778
        model: {}
        policy_loss: -0.0038016303442418575
        total_loss: -0.00314156012609601
        vf_explained_var: 0.03207141160964966
        vf_loss: 22.40143585205078
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4996407628059387
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008625878253951669
        model: {}
        policy_loss: -0.0029771802946925163
        total_loss: -0.001667385920882225
        vf_explained_var: 0.05487854778766632
        vf_loss: 21.891626358032227
    load_time_ms: 13609.102
    num_steps_sampled: 36192000
    num_steps_trained: 36192000
    sample_time_ms: 90285.047
    update_time_ms: 22.258
  iterations_since_restore: 357
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.51920903954802
    ram_util_percent: 15.6045197740113
  pid: 4061
  policy_reward_max:
    agent-0: 201.16666666666646
    agent-1: 201.16666666666646
    agent-2: 201.16666666666646
    agent-3: 201.16666666666646
    agent-4: 201.16666666666646
    agent-5: 201.16666666666646
  policy_reward_mean:
    agent-0: 177.55666666666653
    agent-1: 177.55666666666653
    agent-2: 177.55666666666653
    agent-3: 177.55666666666653
    agent-4: 177.55666666666653
    agent-5: 177.55666666666653
  policy_reward_min:
    agent-0: 73.4999999999999
    agent-1: 73.4999999999999
    agent-2: 73.4999999999999
    agent-3: 73.4999999999999
    agent-4: 73.4999999999999
    agent-5: 73.4999999999999
  sampler_perf:
    mean_env_wait_ms: 24.297208327906706
    mean_inference_ms: 12.321513810830092
    mean_processing_ms: 50.911373654368035
  time_since_restore: 46584.254932165146
  time_this_iter_s: 124.10663270950317
  time_total_s: 49795.31861829758
  timestamp: 1637064170
  timesteps_since_restore: 34272000
  timesteps_this_iter: 96000
  timesteps_total: 36192000
  training_iteration: 377
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    377 |          49795.3 | 36192000 |  1065.34 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 34
    apples_agent-0_mean: 1.59
    apples_agent-0_min: 0
    apples_agent-1_max: 78
    apples_agent-1_mean: 23.8
    apples_agent-1_min: 0
    apples_agent-2_max: 175
    apples_agent-2_mean: 13.56
    apples_agent-2_min: 0
    apples_agent-3_max: 115
    apples_agent-3_mean: 54.27
    apples_agent-3_min: 22
    apples_agent-4_max: 84
    apples_agent-4_mean: 2.69
    apples_agent-4_min: 0
    apples_agent-5_max: 235
    apples_agent-5_mean: 101.92
    apples_agent-5_min: 46
    cleaning_beam_agent-0_max: 603
    cleaning_beam_agent-0_mean: 466.12
    cleaning_beam_agent-0_min: 308
    cleaning_beam_agent-1_max: 467
    cleaning_beam_agent-1_mean: 298.84
    cleaning_beam_agent-1_min: 146
    cleaning_beam_agent-2_max: 595
    cleaning_beam_agent-2_mean: 405.54
    cleaning_beam_agent-2_min: 233
    cleaning_beam_agent-3_max: 62
    cleaning_beam_agent-3_mean: 17.16
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 575
    cleaning_beam_agent-4_mean: 456.6
    cleaning_beam_agent-4_min: 334
    cleaning_beam_agent-5_max: 205
    cleaning_beam_agent-5_mean: 16.73
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-04-54
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1220.999999999999
  episode_reward_mean: 1060.5499999999925
  episode_reward_min: 391.00000000000864
  episodes_this_iter: 96
  episodes_total: 36288
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20168.256
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8559311032295227
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020233779214322567
        model: {}
        policy_loss: -0.0027549967635422945
        total_loss: -0.0019038619939237833
        vf_explained_var: 0.05047635734081268
        vf_loss: 23.575733184814453
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1261045932769775
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011867271969094872
        model: {}
        policy_loss: -0.0034877608995884657
        total_loss: -0.003003771649673581
        vf_explained_var: 0.008922845125198364
        vf_loss: 24.659366607666016
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0933337211608887
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015766392461955547
        model: {}
        policy_loss: -0.003167790826410055
        total_loss: -0.0026493766345083714
        vf_explained_var: 0.014963582158088684
        vf_loss: 24.426830291748047
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3571225702762604
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011913715861737728
        model: {}
        policy_loss: -0.0024492687080055475
        total_loss: -0.0008919958490878344
        vf_explained_var: 0.11806206405162811
        vf_loss: 21.858078002929688
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9064924716949463
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017566930036991835
        model: {}
        policy_loss: -0.003790736198425293
        total_loss: -0.0030400706455111504
        vf_explained_var: 0.06050869822502136
        vf_loss: 23.460906982421875
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5052657723426819
        entropy_coeff: 0.0017600000137463212
        kl: 0.00147745362482965
        model: {}
        policy_loss: -0.0031890743412077427
        total_loss: -0.0018853164510801435
        vf_explained_var: 0.11933892965316772
        vf_loss: 21.930267333984375
    load_time_ms: 13555.239
    num_steps_sampled: 36288000
    num_steps_trained: 36288000
    sample_time_ms: 90336.358
    update_time_ms: 22.839
  iterations_since_restore: 358
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.301694915254235
    ram_util_percent: 15.635593220338977
  pid: 4061
  policy_reward_max:
    agent-0: 203.50000000000017
    agent-1: 203.50000000000017
    agent-2: 203.50000000000017
    agent-3: 203.50000000000017
    agent-4: 203.50000000000017
    agent-5: 203.50000000000017
  policy_reward_mean:
    agent-0: 176.75833333333318
    agent-1: 176.75833333333318
    agent-2: 176.75833333333318
    agent-3: 176.75833333333318
    agent-4: 176.75833333333318
    agent-5: 176.75833333333318
  policy_reward_min:
    agent-0: 65.16666666666643
    agent-1: 65.16666666666643
    agent-2: 65.16666666666643
    agent-3: 65.16666666666643
    agent-4: 65.16666666666643
    agent-5: 65.16666666666643
  sampler_perf:
    mean_env_wait_ms: 24.29950079798373
    mean_inference_ms: 12.320921300176542
    mean_processing_ms: 50.90889406066201
  time_since_restore: 46708.18678689003
  time_this_iter_s: 123.93185472488403
  time_total_s: 49919.25047302246
  timestamp: 1637064294
  timesteps_since_restore: 34368000
  timesteps_this_iter: 96000
  timesteps_total: 36288000
  training_iteration: 378
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 28.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    378 |          49919.3 | 36288000 |  1060.55 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 56
    apples_agent-0_mean: 1.21
    apples_agent-0_min: 0
    apples_agent-1_max: 118
    apples_agent-1_mean: 21.09
    apples_agent-1_min: 0
    apples_agent-2_max: 133
    apples_agent-2_mean: 14.24
    apples_agent-2_min: 0
    apples_agent-3_max: 114
    apples_agent-3_mean: 56.48
    apples_agent-3_min: 25
    apples_agent-4_max: 54
    apples_agent-4_mean: 0.76
    apples_agent-4_min: 0
    apples_agent-5_max: 205
    apples_agent-5_mean: 101.84
    apples_agent-5_min: 49
    cleaning_beam_agent-0_max: 567
    cleaning_beam_agent-0_mean: 469.35
    cleaning_beam_agent-0_min: 355
    cleaning_beam_agent-1_max: 447
    cleaning_beam_agent-1_mean: 300.52
    cleaning_beam_agent-1_min: 151
    cleaning_beam_agent-2_max: 576
    cleaning_beam_agent-2_mean: 375.64
    cleaning_beam_agent-2_min: 127
    cleaning_beam_agent-3_max: 80
    cleaning_beam_agent-3_mean: 15.47
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 596
    cleaning_beam_agent-4_mean: 455.96
    cleaning_beam_agent-4_min: 372
    cleaning_beam_agent-5_max: 331
    cleaning_beam_agent-5_mean: 25.06
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-06-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1199.000000000001
  episode_reward_mean: 1062.2299999999925
  episode_reward_min: 475.0000000000084
  episodes_this_iter: 96
  episodes_total: 36384
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20136.165
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8669996857643127
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013552833115682006
        model: {}
        policy_loss: -0.0025183893740177155
        total_loss: -0.0018193293362855911
        vf_explained_var: 0.05032792687416077
        vf_loss: 22.249784469604492
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1290550231933594
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014814683236181736
        model: {}
        policy_loss: -0.003765779547393322
        total_loss: -0.0034157559275627136
        vf_explained_var: 0.006626680493354797
        vf_loss: 23.37160873413086
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1087346076965332
        entropy_coeff: 0.0017600000137463212
        kl: 0.001572042121551931
        model: {}
        policy_loss: -0.0033744757529348135
        total_loss: -0.002944809850305319
        vf_explained_var: -0.01572062075138092
        vf_loss: 23.810375213623047
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3573808968067169
        entropy_coeff: 0.0017600000137463212
        kl: 0.001261992147192359
        model: {}
        policy_loss: -0.002291393466293812
        total_loss: -0.0008139843121170998
        vf_explained_var: 0.10095392167568207
        vf_loss: 21.063974380493164
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8952215313911438
        entropy_coeff: 0.0017600000137463212
        kl: 0.001409689662978053
        model: {}
        policy_loss: -0.003397975582629442
        total_loss: -0.002745585050433874
        vf_explained_var: 0.051548272371292114
        vf_loss: 22.279800415039062
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5157583355903625
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009274855256080627
        model: {}
        policy_loss: -0.00276804156601429
        total_loss: -0.001470375806093216
        vf_explained_var: 0.06806670129299164
        vf_loss: 22.053993225097656
    load_time_ms: 13622.748
    num_steps_sampled: 36384000
    num_steps_trained: 36384000
    sample_time_ms: 90440.149
    update_time_ms: 23.565
  iterations_since_restore: 359
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.437853107344633
    ram_util_percent: 15.648587570621466
  pid: 4061
  policy_reward_max:
    agent-0: 199.83333333333331
    agent-1: 199.83333333333331
    agent-2: 199.83333333333331
    agent-3: 199.83333333333331
    agent-4: 199.83333333333331
    agent-5: 199.83333333333331
  policy_reward_mean:
    agent-0: 177.0383333333331
    agent-1: 177.0383333333331
    agent-2: 177.0383333333331
    agent-3: 177.0383333333331
    agent-4: 177.0383333333331
    agent-5: 177.0383333333331
  policy_reward_min:
    agent-0: 79.16666666666659
    agent-1: 79.16666666666659
    agent-2: 79.16666666666659
    agent-3: 79.16666666666659
    agent-4: 79.16666666666659
    agent-5: 79.16666666666659
  sampler_perf:
    mean_env_wait_ms: 24.301671822577998
    mean_inference_ms: 12.320515511155506
    mean_processing_ms: 50.90738126476981
  time_since_restore: 46832.8279235363
  time_this_iter_s: 124.64113664627075
  time_total_s: 50043.89160966873
  timestamp: 1637064419
  timesteps_since_restore: 34464000
  timesteps_this_iter: 96000
  timesteps_total: 36384000
  training_iteration: 379
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    379 |          50043.9 | 36384000 |  1062.23 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 47
    apples_agent-0_mean: 1.74
    apples_agent-0_min: 0
    apples_agent-1_max: 99
    apples_agent-1_mean: 21.74
    apples_agent-1_min: 0
    apples_agent-2_max: 91
    apples_agent-2_mean: 15.48
    apples_agent-2_min: 0
    apples_agent-3_max: 116
    apples_agent-3_mean: 56.53
    apples_agent-3_min: 23
    apples_agent-4_max: 99
    apples_agent-4_mean: 1.28
    apples_agent-4_min: 0
    apples_agent-5_max: 149
    apples_agent-5_mean: 100.34
    apples_agent-5_min: 42
    cleaning_beam_agent-0_max: 539
    cleaning_beam_agent-0_mean: 449.74
    cleaning_beam_agent-0_min: 333
    cleaning_beam_agent-1_max: 425
    cleaning_beam_agent-1_mean: 302.09
    cleaning_beam_agent-1_min: 151
    cleaning_beam_agent-2_max: 552
    cleaning_beam_agent-2_mean: 367.57
    cleaning_beam_agent-2_min: 175
    cleaning_beam_agent-3_max: 74
    cleaning_beam_agent-3_mean: 15.3
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 599
    cleaning_beam_agent-4_mean: 472.78
    cleaning_beam_agent-4_min: 377
    cleaning_beam_agent-5_max: 295
    cleaning_beam_agent-5_mean: 22.35
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-09-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1216.9999999999966
  episode_reward_mean: 1056.3499999999926
  episode_reward_min: 549.0000000000133
  episodes_this_iter: 96
  episodes_total: 36480
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20163.35
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8769830465316772
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014090915210545063
        model: {}
        policy_loss: -0.0028328835032880306
        total_loss: -0.002151278080418706
        vf_explained_var: 0.00011695921421051025
        vf_loss: 22.250965118408203
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.132124662399292
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017132062930613756
        model: {}
        policy_loss: -0.0038671521469950676
        total_loss: -0.0036302534863352776
        vf_explained_var: -0.000970989465713501
        vf_loss: 22.294395446777344
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1212310791015625
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012717435602098703
        model: {}
        policy_loss: -0.003275220515206456
        total_loss: -0.003023087978363037
        vf_explained_var: -0.00032223761081695557
        vf_loss: 22.254987716674805
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3534131944179535
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006800844566896558
        model: {}
        policy_loss: -0.0019211610779166222
        total_loss: -0.00042190379463136196
        vf_explained_var: 0.046770527958869934
        vf_loss: 21.212635040283203
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.876204788684845
        entropy_coeff: 0.0017600000137463212
        kl: 0.001968034543097019
        model: {}
        policy_loss: -0.0037410769145935774
        total_loss: -0.003148886142298579
        vf_explained_var: 0.040285542607307434
        vf_loss: 21.3431396484375
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5333869457244873
        entropy_coeff: 0.0017600000137463212
        kl: 0.001056867535226047
        model: {}
        policy_loss: -0.003085767850279808
        total_loss: -0.0019015278667211533
        vf_explained_var: 0.05022266507148743
        vf_loss: 21.23002052307129
    load_time_ms: 13581.828
    num_steps_sampled: 36480000
    num_steps_trained: 36480000
    sample_time_ms: 90557.209
    update_time_ms: 23.868
  iterations_since_restore: 360
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.323595505617977
    ram_util_percent: 15.585955056179774
  pid: 4061
  policy_reward_max:
    agent-0: 202.83333333333314
    agent-1: 202.83333333333314
    agent-2: 202.83333333333314
    agent-3: 202.83333333333314
    agent-4: 202.83333333333314
    agent-5: 202.83333333333314
  policy_reward_mean:
    agent-0: 176.05833333333314
    agent-1: 176.05833333333314
    agent-2: 176.05833333333314
    agent-3: 176.05833333333314
    agent-4: 176.05833333333314
    agent-5: 176.05833333333314
  policy_reward_min:
    agent-0: 91.50000000000016
    agent-1: 91.50000000000016
    agent-2: 91.50000000000016
    agent-3: 91.50000000000016
    agent-4: 91.50000000000016
    agent-5: 91.50000000000016
  sampler_perf:
    mean_env_wait_ms: 24.303584968520855
    mean_inference_ms: 12.319941186991635
    mean_processing_ms: 50.90591437199125
  time_since_restore: 46957.25142407417
  time_this_iter_s: 124.42350053787231
  time_total_s: 50168.315110206604
  timestamp: 1637064543
  timesteps_since_restore: 34560000
  timesteps_this_iter: 96000
  timesteps_total: 36480000
  training_iteration: 380
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    380 |          50168.3 | 36480000 |  1056.35 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 22
    apples_agent-0_mean: 0.91
    apples_agent-0_min: 0
    apples_agent-1_max: 91
    apples_agent-1_mean: 21.98
    apples_agent-1_min: 0
    apples_agent-2_max: 200
    apples_agent-2_mean: 17.39
    apples_agent-2_min: 0
    apples_agent-3_max: 101
    apples_agent-3_mean: 57.19
    apples_agent-3_min: 29
    apples_agent-4_max: 49
    apples_agent-4_mean: 0.5
    apples_agent-4_min: 0
    apples_agent-5_max: 170
    apples_agent-5_mean: 101.0
    apples_agent-5_min: 55
    cleaning_beam_agent-0_max: 540
    cleaning_beam_agent-0_mean: 448.66
    cleaning_beam_agent-0_min: 303
    cleaning_beam_agent-1_max: 514
    cleaning_beam_agent-1_mean: 311.29
    cleaning_beam_agent-1_min: 148
    cleaning_beam_agent-2_max: 528
    cleaning_beam_agent-2_mean: 364.02
    cleaning_beam_agent-2_min: 184
    cleaning_beam_agent-3_max: 50
    cleaning_beam_agent-3_mean: 12.87
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 588
    cleaning_beam_agent-4_mean: 468.18
    cleaning_beam_agent-4_min: 354
    cleaning_beam_agent-5_max: 267
    cleaning_beam_agent-5_mean: 22.66
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-11-10
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1241.000000000019
  episode_reward_mean: 1071.3799999999894
  episode_reward_min: 637.9999999999968
  episodes_this_iter: 96
  episodes_total: 36576
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20181.129
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8760334253311157
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015229808632284403
        model: {}
        policy_loss: -0.0029347713571041822
        total_loss: -0.0023397228214889765
        vf_explained_var: 0.00830163061618805
        vf_loss: 21.36866569519043
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1267704963684082
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013930933782830834
        model: {}
        policy_loss: -0.0035583460703492165
        total_loss: -0.0033410266041755676
        vf_explained_var: -0.013394087553024292
        vf_loss: 22.004369735717773
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1115937232971191
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014216002309694886
        model: {}
        policy_loss: -0.0030975081026554108
        total_loss: -0.002904776483774185
        vf_explained_var: 0.008431553840637207
        vf_loss: 21.49138069152832
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.327928364276886
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011411852901801467
        model: {}
        policy_loss: -0.0020503122359514236
        total_loss: -0.0006098505109548569
        vf_explained_var: 0.06204347312450409
        vf_loss: 20.176185607910156
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8866011500358582
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013091819128021598
        model: {}
        policy_loss: -0.003613470820710063
        total_loss: -0.003074158914387226
        vf_explained_var: 0.026427999138832092
        vf_loss: 20.997303009033203
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5214860439300537
        entropy_coeff: 0.0017600000137463212
        kl: 0.000620758393779397
        model: {}
        policy_loss: -0.002691894769668579
        total_loss: -0.0016032867133617401
        vf_explained_var: 0.07481534779071808
        vf_loss: 20.064231872558594
    load_time_ms: 13611.532
    num_steps_sampled: 36576000
    num_steps_trained: 36576000
    sample_time_ms: 90637.649
    update_time_ms: 24.359
  iterations_since_restore: 361
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 16.09722222222222
    ram_util_percent: 15.661111111111108
  pid: 4061
  policy_reward_max:
    agent-0: 206.83333333333297
    agent-1: 206.83333333333297
    agent-2: 206.83333333333297
    agent-3: 206.83333333333297
    agent-4: 206.83333333333297
    agent-5: 206.83333333333297
  policy_reward_mean:
    agent-0: 178.56333333333313
    agent-1: 178.56333333333313
    agent-2: 178.56333333333313
    agent-3: 178.56333333333313
    agent-4: 178.56333333333313
    agent-5: 178.56333333333313
  policy_reward_min:
    agent-0: 106.33333333333394
    agent-1: 106.33333333333394
    agent-2: 106.33333333333394
    agent-3: 106.33333333333394
    agent-4: 106.33333333333394
    agent-5: 106.33333333333394
  sampler_perf:
    mean_env_wait_ms: 24.305842281779963
    mean_inference_ms: 12.319562159685658
    mean_processing_ms: 50.90453536471826
  time_since_restore: 47083.022859334946
  time_this_iter_s: 125.7714352607727
  time_total_s: 50294.08654546738
  timestamp: 1637064670
  timesteps_since_restore: 34656000
  timesteps_this_iter: 96000
  timesteps_total: 36576000
  training_iteration: 381
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 28.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    381 |          50294.1 | 36576000 |  1071.38 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 45
    apples_agent-0_mean: 1.23
    apples_agent-0_min: 0
    apples_agent-1_max: 111
    apples_agent-1_mean: 25.59
    apples_agent-1_min: 0
    apples_agent-2_max: 125
    apples_agent-2_mean: 16.79
    apples_agent-2_min: 0
    apples_agent-3_max: 116
    apples_agent-3_mean: 58.31
    apples_agent-3_min: 23
    apples_agent-4_max: 61
    apples_agent-4_mean: 1.16
    apples_agent-4_min: 0
    apples_agent-5_max: 195
    apples_agent-5_mean: 104.99
    apples_agent-5_min: 66
    cleaning_beam_agent-0_max: 550
    cleaning_beam_agent-0_mean: 457.54
    cleaning_beam_agent-0_min: 331
    cleaning_beam_agent-1_max: 459
    cleaning_beam_agent-1_mean: 293.57
    cleaning_beam_agent-1_min: 120
    cleaning_beam_agent-2_max: 581
    cleaning_beam_agent-2_mean: 366.0
    cleaning_beam_agent-2_min: 153
    cleaning_beam_agent-3_max: 61
    cleaning_beam_agent-3_mean: 15.27
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 588
    cleaning_beam_agent-4_mean: 478.86
    cleaning_beam_agent-4_min: 358
    cleaning_beam_agent-5_max: 202
    cleaning_beam_agent-5_mean: 19.8
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-13-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1218.000000000003
  episode_reward_mean: 1059.4399999999905
  episode_reward_min: 637.9999999999968
  episodes_this_iter: 96
  episodes_total: 36672
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20191.411
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8995608687400818
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019767843186855316
        model: {}
        policy_loss: -0.00290660304017365
        total_loss: -0.002316426718607545
        vf_explained_var: 0.0024007856845855713
        vf_loss: 21.734033584594727
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1380536556243896
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015575174475088716
        model: {}
        policy_loss: -0.004042644519358873
        total_loss: -0.003850579960271716
        vf_explained_var: -0.000301554799079895
        vf_loss: 21.95038414001465
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.115921974182129
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013847211375832558
        model: {}
        policy_loss: -0.0034670880995690823
        total_loss: -0.003226595465093851
        vf_explained_var: -0.005616173148155212
        vf_loss: 22.045164108276367
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3455929160118103
        entropy_coeff: 0.0017600000137463212
        kl: 0.000735588138923049
        model: {}
        policy_loss: -0.0019840330351144075
        total_loss: -0.0005489860195666552
        vf_explained_var: 0.06371740996837616
        vf_loss: 20.432903289794922
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8835443258285522
        entropy_coeff: 0.0017600000137463212
        kl: 0.001750578754581511
        model: {}
        policy_loss: -0.003612752305343747
        total_loss: -0.0030709116254001856
        vf_explained_var: 0.03710050880908966
        vf_loss: 20.968782424926758
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.526940107345581
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011550671188160777
        model: {}
        policy_loss: -0.0027332454919815063
        total_loss: -0.0015679094940423965
        vf_explained_var: 0.04688292741775513
        vf_loss: 20.927518844604492
    load_time_ms: 13560.237
    num_steps_sampled: 36672000
    num_steps_trained: 36672000
    sample_time_ms: 90583.839
    update_time_ms: 24.418
  iterations_since_restore: 362
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.400000000000004
    ram_util_percent: 15.649431818181814
  pid: 4061
  policy_reward_max:
    agent-0: 202.99999999999935
    agent-1: 202.99999999999935
    agent-2: 202.99999999999935
    agent-3: 202.99999999999935
    agent-4: 202.99999999999935
    agent-5: 202.99999999999935
  policy_reward_mean:
    agent-0: 176.5733333333331
    agent-1: 176.5733333333331
    agent-2: 176.5733333333331
    agent-3: 176.5733333333331
    agent-4: 176.5733333333331
    agent-5: 176.5733333333331
  policy_reward_min:
    agent-0: 106.33333333333394
    agent-1: 106.33333333333394
    agent-2: 106.33333333333394
    agent-3: 106.33333333333394
    agent-4: 106.33333333333394
    agent-5: 106.33333333333394
  sampler_perf:
    mean_env_wait_ms: 24.308140803342823
    mean_inference_ms: 12.31908642799687
    mean_processing_ms: 50.90279033049625
  time_since_restore: 47206.908500909805
  time_this_iter_s: 123.88564157485962
  time_total_s: 50417.972187042236
  timestamp: 1637064794
  timesteps_since_restore: 34752000
  timesteps_this_iter: 96000
  timesteps_total: 36672000
  training_iteration: 382
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    382 |            50418 | 36672000 |  1059.44 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 49
    apples_agent-0_mean: 2.83
    apples_agent-0_min: 0
    apples_agent-1_max: 104
    apples_agent-1_mean: 20.72
    apples_agent-1_min: 0
    apples_agent-2_max: 156
    apples_agent-2_mean: 10.53
    apples_agent-2_min: 0
    apples_agent-3_max: 119
    apples_agent-3_mean: 56.44
    apples_agent-3_min: 26
    apples_agent-4_max: 10
    apples_agent-4_mean: 0.14
    apples_agent-4_min: 0
    apples_agent-5_max: 195
    apples_agent-5_mean: 101.33
    apples_agent-5_min: 36
    cleaning_beam_agent-0_max: 529
    cleaning_beam_agent-0_mean: 428.29
    cleaning_beam_agent-0_min: 329
    cleaning_beam_agent-1_max: 451
    cleaning_beam_agent-1_mean: 299.59
    cleaning_beam_agent-1_min: 168
    cleaning_beam_agent-2_max: 598
    cleaning_beam_agent-2_mean: 391.63
    cleaning_beam_agent-2_min: 174
    cleaning_beam_agent-3_max: 32
    cleaning_beam_agent-3_mean: 13.38
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 589
    cleaning_beam_agent-4_mean: 466.77
    cleaning_beam_agent-4_min: 372
    cleaning_beam_agent-5_max: 195
    cleaning_beam_agent-5_mean: 20.16
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-15-21
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1231.999999999989
  episode_reward_mean: 1072.9699999999903
  episode_reward_min: 573.0000000000082
  episodes_this_iter: 96
  episodes_total: 36768
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20185.505
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8969578742980957
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017520047258585691
        model: {}
        policy_loss: -0.002978456672281027
        total_loss: -0.002444822806864977
        vf_explained_var: 0.04013986885547638
        vf_loss: 21.122804641723633
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1399893760681152
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022030824329704046
        model: {}
        policy_loss: -0.004148977808654308
        total_loss: -0.003935913555324078
        vf_explained_var: 0.0001592785120010376
        vf_loss: 22.19446563720703
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1157002449035645
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022173370234668255
        model: {}
        policy_loss: -0.0034357234835624695
        total_loss: -0.00313754053786397
        vf_explained_var: -0.030119329690933228
        vf_loss: 22.618135452270508
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3279530107975006
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007189520401880145
        model: {}
        policy_loss: -0.002013652352616191
        total_loss: -0.0005497212987393141
        vf_explained_var: 0.06830522418022156
        vf_loss: 20.4112548828125
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8826749920845032
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019325260072946548
        model: {}
        policy_loss: -0.0038469822611659765
        total_loss: -0.003293385496363044
        vf_explained_var: 0.039041876792907715
        vf_loss: 21.07103729248047
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5149336457252502
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010257824324071407
        model: {}
        policy_loss: -0.002675686962902546
        total_loss: -0.0015376601368188858
        vf_explained_var: 0.07461690902709961
        vf_loss: 20.443115234375
    load_time_ms: 13844.452
    num_steps_sampled: 36768000
    num_steps_trained: 36768000
    sample_time_ms: 90544.023
    update_time_ms: 24.713
  iterations_since_restore: 363
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 14.946153846153845
    ram_util_percent: 15.62307692307692
  pid: 4061
  policy_reward_max:
    agent-0: 205.33333333333323
    agent-1: 205.33333333333323
    agent-2: 205.33333333333323
    agent-3: 205.33333333333323
    agent-4: 205.33333333333323
    agent-5: 205.33333333333323
  policy_reward_mean:
    agent-0: 178.8283333333332
    agent-1: 178.8283333333332
    agent-2: 178.8283333333332
    agent-3: 178.8283333333332
    agent-4: 178.8283333333332
    agent-5: 178.8283333333332
  policy_reward_min:
    agent-0: 95.50000000000016
    agent-1: 95.50000000000016
    agent-2: 95.50000000000016
    agent-3: 95.50000000000016
    agent-4: 95.50000000000016
    agent-5: 95.50000000000016
  sampler_perf:
    mean_env_wait_ms: 24.310004245340092
    mean_inference_ms: 12.318588351144562
    mean_processing_ms: 50.90032344147074
  time_since_restore: 47334.31788134575
  time_this_iter_s: 127.4093804359436
  time_total_s: 50545.38156747818
  timestamp: 1637064921
  timesteps_since_restore: 34848000
  timesteps_this_iter: 96000
  timesteps_total: 36768000
  training_iteration: 383
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    383 |          50545.4 | 36768000 |  1072.97 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 0.61
    apples_agent-0_min: 0
    apples_agent-1_max: 85
    apples_agent-1_mean: 22.09
    apples_agent-1_min: 0
    apples_agent-2_max: 87
    apples_agent-2_mean: 15.19
    apples_agent-2_min: 0
    apples_agent-3_max: 105
    apples_agent-3_mean: 54.86
    apples_agent-3_min: 27
    apples_agent-4_max: 42
    apples_agent-4_mean: 0.62
    apples_agent-4_min: 0
    apples_agent-5_max: 172
    apples_agent-5_mean: 101.17
    apples_agent-5_min: 39
    cleaning_beam_agent-0_max: 579
    cleaning_beam_agent-0_mean: 442.41
    cleaning_beam_agent-0_min: 246
    cleaning_beam_agent-1_max: 456
    cleaning_beam_agent-1_mean: 283.96
    cleaning_beam_agent-1_min: 145
    cleaning_beam_agent-2_max: 537
    cleaning_beam_agent-2_mean: 357.51
    cleaning_beam_agent-2_min: 125
    cleaning_beam_agent-3_max: 80
    cleaning_beam_agent-3_mean: 15.96
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 560
    cleaning_beam_agent-4_mean: 454.86
    cleaning_beam_agent-4_min: 365
    cleaning_beam_agent-5_max: 169
    cleaning_beam_agent-5_mean: 16.54
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-17-28
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1251.000000000009
  episode_reward_mean: 1062.5799999999917
  episode_reward_min: 584.0000000000014
  episodes_this_iter: 96
  episodes_total: 36864
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20173.212
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8945431709289551
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017925857100635767
        model: {}
        policy_loss: -0.0029403381049633026
        total_loss: -0.002257451880723238
        vf_explained_var: 0.007528826594352722
        vf_loss: 22.572839736938477
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1432583332061768
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017357439501211047
        model: {}
        policy_loss: -0.0039725881069898605
        total_loss: -0.003712791483849287
        vf_explained_var: 0.009132221341133118
        vf_loss: 22.71929931640625
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1355483531951904
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017740536713972688
        model: {}
        policy_loss: -0.0037632298190146685
        total_loss: -0.0035072097089141607
        vf_explained_var: 0.013760223984718323
        vf_loss: 22.545841217041016
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3369167447090149
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013006380759179592
        model: {}
        policy_loss: -0.0022923231590539217
        total_loss: -0.0008593652164563537
        vf_explained_var: 0.10825911164283752
        vf_loss: 20.259300231933594
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8902628421783447
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015592868439853191
        model: {}
        policy_loss: -0.0037059644237160683
        total_loss: -0.003085447708144784
        vf_explained_var: 0.038627296686172485
        vf_loss: 21.87380599975586
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5150853395462036
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006111690308898687
        model: {}
        policy_loss: -0.002574586309492588
        total_loss: -0.0014304481446743011
        vf_explained_var: 0.10033701360225677
        vf_loss: 20.50689697265625
    load_time_ms: 14071.935
    num_steps_sampled: 36864000
    num_steps_trained: 36864000
    sample_time_ms: 90509.842
    update_time_ms: 24.475
  iterations_since_restore: 364
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.11277777777778
    ram_util_percent: 15.698333333333332
  pid: 4061
  policy_reward_max:
    agent-0: 208.49999999999986
    agent-1: 208.49999999999986
    agent-2: 208.49999999999986
    agent-3: 208.49999999999986
    agent-4: 208.49999999999986
    agent-5: 208.49999999999986
  policy_reward_mean:
    agent-0: 177.0966666666665
    agent-1: 177.0966666666665
    agent-2: 177.0966666666665
    agent-3: 177.0966666666665
    agent-4: 177.0966666666665
    agent-5: 177.0966666666665
  policy_reward_min:
    agent-0: 97.33333333333348
    agent-1: 97.33333333333348
    agent-2: 97.33333333333348
    agent-3: 97.33333333333348
    agent-4: 97.33333333333348
    agent-5: 97.33333333333348
  sampler_perf:
    mean_env_wait_ms: 24.311532237651434
    mean_inference_ms: 12.318074856514004
    mean_processing_ms: 50.8978543872322
  time_since_restore: 47460.33751273155
  time_this_iter_s: 126.01963138580322
  time_total_s: 50671.40119886398
  timestamp: 1637065048
  timesteps_since_restore: 34944000
  timesteps_this_iter: 96000
  timesteps_total: 36864000
  training_iteration: 384
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 28.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    384 |          50671.4 | 36864000 |  1062.58 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 28
    apples_agent-0_mean: 1.81
    apples_agent-0_min: 0
    apples_agent-1_max: 98
    apples_agent-1_mean: 18.98
    apples_agent-1_min: 0
    apples_agent-2_max: 97
    apples_agent-2_mean: 13.5
    apples_agent-2_min: 0
    apples_agent-3_max: 246
    apples_agent-3_mean: 53.34
    apples_agent-3_min: 26
    apples_agent-4_max: 12
    apples_agent-4_mean: 0.2
    apples_agent-4_min: 0
    apples_agent-5_max: 268
    apples_agent-5_mean: 101.18
    apples_agent-5_min: 58
    cleaning_beam_agent-0_max: 532
    cleaning_beam_agent-0_mean: 433.2
    cleaning_beam_agent-0_min: 329
    cleaning_beam_agent-1_max: 470
    cleaning_beam_agent-1_mean: 289.63
    cleaning_beam_agent-1_min: 145
    cleaning_beam_agent-2_max: 533
    cleaning_beam_agent-2_mean: 363.45
    cleaning_beam_agent-2_min: 153
    cleaning_beam_agent-3_max: 63
    cleaning_beam_agent-3_mean: 14.83
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 590
    cleaning_beam_agent-4_mean: 458.61
    cleaning_beam_agent-4_min: 347
    cleaning_beam_agent-5_max: 228
    cleaning_beam_agent-5_mean: 23.87
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-19-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1222.0000000000095
  episode_reward_mean: 1047.329999999989
  episode_reward_min: 708.99999999999
  episodes_this_iter: 96
  episodes_total: 36960
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20184.775
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.88711017370224
        entropy_coeff: 0.0017600000137463212
        kl: 0.002193463733419776
        model: {}
        policy_loss: -0.0032142633572220802
        total_loss: -0.0026833764277398586
        vf_explained_var: 0.015735909342765808
        vf_loss: 20.922019958496094
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1422337293624878
        entropy_coeff: 0.0017600000137463212
        kl: 0.002505919896066189
        model: {}
        policy_loss: -0.004034026060253382
        total_loss: -0.0038875089958310127
        vf_explained_var: -0.01178981363773346
        vf_loss: 21.568470001220703
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.114126443862915
        entropy_coeff: 0.0017600000137463212
        kl: 0.001499233883805573
        model: {}
        policy_loss: -0.0032319279853254557
        total_loss: -0.0030887271277606487
        vf_explained_var: 0.011692851781845093
        vf_loss: 21.0406494140625
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.32761338353157043
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006418150733225048
        model: {}
        policy_loss: -0.0017264769412577152
        total_loss: -0.0003400833811610937
        vf_explained_var: 0.07864336669445038
        vf_loss: 19.629928588867188
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8875715732574463
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021381275728344917
        model: {}
        policy_loss: -0.003990045748651028
        total_loss: -0.003509421134367585
        vf_explained_var: 0.04075579345226288
        vf_loss: 20.427539825439453
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5445677638053894
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009021515143103898
        model: {}
        policy_loss: -0.002819150686264038
        total_loss: -0.0017032623291015625
        vf_explained_var: 0.029412388801574707
        vf_loss: 20.74325942993164
    load_time_ms: 14069.337
    num_steps_sampled: 36960000
    num_steps_trained: 36960000
    sample_time_ms: 90453.42
    update_time_ms: 24.266
  iterations_since_restore: 365
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.80625
    ram_util_percent: 15.645454545454543
  pid: 4061
  policy_reward_max:
    agent-0: 203.66666666666686
    agent-1: 203.66666666666686
    agent-2: 203.66666666666686
    agent-3: 203.66666666666686
    agent-4: 203.66666666666686
    agent-5: 203.66666666666686
  policy_reward_mean:
    agent-0: 174.55499999999986
    agent-1: 174.55499999999986
    agent-2: 174.55499999999986
    agent-3: 174.55499999999986
    agent-4: 174.55499999999986
    agent-5: 174.55499999999986
  policy_reward_min:
    agent-0: 118.16666666666693
    agent-1: 118.16666666666693
    agent-2: 118.16666666666693
    agent-3: 118.16666666666693
    agent-4: 118.16666666666693
    agent-5: 118.16666666666693
  sampler_perf:
    mean_env_wait_ms: 24.31314923274363
    mean_inference_ms: 12.31754120444274
    mean_processing_ms: 50.896038125204996
  time_since_restore: 47583.898977041245
  time_this_iter_s: 123.56146430969238
  time_total_s: 50794.962663173676
  timestamp: 1637065171
  timesteps_since_restore: 35040000
  timesteps_this_iter: 96000
  timesteps_total: 36960000
  training_iteration: 385
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    385 |            50795 | 36960000 |  1047.33 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 51
    apples_agent-0_mean: 2.03
    apples_agent-0_min: 0
    apples_agent-1_max: 114
    apples_agent-1_mean: 22.03
    apples_agent-1_min: 0
    apples_agent-2_max: 123
    apples_agent-2_mean: 15.82
    apples_agent-2_min: 0
    apples_agent-3_max: 97
    apples_agent-3_mean: 53.93
    apples_agent-3_min: 32
    apples_agent-4_max: 44
    apples_agent-4_mean: 0.59
    apples_agent-4_min: 0
    apples_agent-5_max: 240
    apples_agent-5_mean: 100.97
    apples_agent-5_min: 58
    cleaning_beam_agent-0_max: 545
    cleaning_beam_agent-0_mean: 439.45
    cleaning_beam_agent-0_min: 304
    cleaning_beam_agent-1_max: 475
    cleaning_beam_agent-1_mean: 287.3
    cleaning_beam_agent-1_min: 176
    cleaning_beam_agent-2_max: 525
    cleaning_beam_agent-2_mean: 352.91
    cleaning_beam_agent-2_min: 193
    cleaning_beam_agent-3_max: 53
    cleaning_beam_agent-3_mean: 14.29
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 592
    cleaning_beam_agent-4_mean: 450.87
    cleaning_beam_agent-4_min: 339
    cleaning_beam_agent-5_max: 184
    cleaning_beam_agent-5_mean: 18.57
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-21-35
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1251.999999999996
  episode_reward_mean: 1075.97999999999
  episode_reward_min: 767.0000000000038
  episodes_this_iter: 96
  episodes_total: 37056
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20172.893
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.89858078956604
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022073788568377495
        model: {}
        policy_loss: -0.003144832793623209
        total_loss: -0.0025623843539506197
        vf_explained_var: 0.02215336263179779
        vf_loss: 21.6395263671875
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1287198066711426
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021752011962234974
        model: {}
        policy_loss: -0.004377993755042553
        total_loss: -0.004195262212306261
        vf_explained_var: 0.02414083480834961
        vf_loss: 21.692747116088867
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.116051197052002
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021452810615301132
        model: {}
        policy_loss: -0.003852285910397768
        total_loss: -0.0036603789776563644
        vf_explained_var: 0.027624770998954773
        vf_loss: 21.561580657958984
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3159905672073364
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008434758055955172
        model: {}
        policy_loss: -0.0017871672753244638
        total_loss: -0.000305469729937613
        vf_explained_var: 0.07644174993038177
        vf_loss: 20.378406524658203
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8943411111831665
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016474156873300672
        model: {}
        policy_loss: -0.0038279485888779163
        total_loss: -0.0032867989502847195
        vf_explained_var: 0.04259409010410309
        vf_loss: 21.15186882019043
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5209550857543945
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007115420303307474
        model: {}
        policy_loss: -0.002649860456585884
        total_loss: -0.0014996950048953295
        vf_explained_var: 0.07023067772388458
        vf_loss: 20.670488357543945
    load_time_ms: 13954.503
    num_steps_sampled: 37056000
    num_steps_trained: 37056000
    sample_time_ms: 90422.321
    update_time_ms: 24.544
  iterations_since_restore: 366
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 16.05371428571429
    ram_util_percent: 15.586285714285717
  pid: 4061
  policy_reward_max:
    agent-0: 208.66666666666643
    agent-1: 208.66666666666643
    agent-2: 208.66666666666643
    agent-3: 208.66666666666643
    agent-4: 208.66666666666643
    agent-5: 208.66666666666643
  policy_reward_mean:
    agent-0: 179.3299999999998
    agent-1: 179.3299999999998
    agent-2: 179.3299999999998
    agent-3: 179.3299999999998
    agent-4: 179.3299999999998
    agent-5: 179.3299999999998
  policy_reward_min:
    agent-0: 127.83333333333357
    agent-1: 127.83333333333357
    agent-2: 127.83333333333357
    agent-3: 127.83333333333357
    agent-4: 127.83333333333357
    agent-5: 127.83333333333357
  sampler_perf:
    mean_env_wait_ms: 24.314574768205713
    mean_inference_ms: 12.317024911184367
    mean_processing_ms: 50.89369788740053
  time_since_restore: 47706.943271160126
  time_this_iter_s: 123.04429411888123
  time_total_s: 50918.00695729256
  timestamp: 1637065295
  timesteps_since_restore: 35136000
  timesteps_this_iter: 96000
  timesteps_total: 37056000
  training_iteration: 386
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 28.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    386 |            50918 | 37056000 |  1075.98 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 40
    apples_agent-0_mean: 1.09
    apples_agent-0_min: 0
    apples_agent-1_max: 109
    apples_agent-1_mean: 21.02
    apples_agent-1_min: 0
    apples_agent-2_max: 110
    apples_agent-2_mean: 10.42
    apples_agent-2_min: 0
    apples_agent-3_max: 170
    apples_agent-3_mean: 57.73
    apples_agent-3_min: 16
    apples_agent-4_max: 81
    apples_agent-4_mean: 1.2
    apples_agent-4_min: 0
    apples_agent-5_max: 216
    apples_agent-5_mean: 105.25
    apples_agent-5_min: 48
    cleaning_beam_agent-0_max: 530
    cleaning_beam_agent-0_mean: 445.7
    cleaning_beam_agent-0_min: 325
    cleaning_beam_agent-1_max: 481
    cleaning_beam_agent-1_mean: 302.92
    cleaning_beam_agent-1_min: 147
    cleaning_beam_agent-2_max: 578
    cleaning_beam_agent-2_mean: 361.31
    cleaning_beam_agent-2_min: 149
    cleaning_beam_agent-3_max: 181
    cleaning_beam_agent-3_mean: 16.42
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 560
    cleaning_beam_agent-4_mean: 454.66
    cleaning_beam_agent-4_min: 314
    cleaning_beam_agent-5_max: 181
    cleaning_beam_agent-5_mean: 19.13
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-23-39
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1233.0000000000032
  episode_reward_mean: 1074.9099999999924
  episode_reward_min: 731.9999999999926
  episodes_this_iter: 96
  episodes_total: 37152
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20187.373
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8866761922836304
        entropy_coeff: 0.0017600000137463212
        kl: 0.001446505426429212
        model: {}
        policy_loss: -0.002799347275868058
        total_loss: -0.0020008094143122435
        vf_explained_var: -0.009884953498840332
        vf_loss: 23.590869903564453
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1310460567474365
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014374192105606198
        model: {}
        policy_loss: -0.003644188866019249
        total_loss: -0.0033157430589199066
        vf_explained_var: 0.012042924761772156
        vf_loss: 23.190837860107422
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1223288774490356
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010608115699142218
        model: {}
        policy_loss: -0.0031979770865291357
        total_loss: -0.0028584476094692945
        vf_explained_var: 0.010856181383132935
        vf_loss: 23.148256301879883
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.32508382201194763
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006576855084858835
        model: {}
        policy_loss: -0.0020197152625769377
        total_loss: -0.00045356122427619994
        vf_explained_var: 0.08110207319259644
        vf_loss: 21.38302230834961
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8811893463134766
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017212005332112312
        model: {}
        policy_loss: -0.003894285997375846
        total_loss: -0.0032201989088207483
        vf_explained_var: 0.045986488461494446
        vf_loss: 22.249807357788086
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5131964683532715
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006408726912923157
        model: {}
        policy_loss: -0.0025399161968380213
        total_loss: -0.0012122699990868568
        vf_explained_var: 0.05139629542827606
        vf_loss: 22.308719635009766
    load_time_ms: 14030.085
    num_steps_sampled: 37152000
    num_steps_trained: 37152000
    sample_time_ms: 90372.987
    update_time_ms: 24.104
  iterations_since_restore: 367
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.474157303370784
    ram_util_percent: 15.669662921348314
  pid: 4061
  policy_reward_max:
    agent-0: 205.50000000000006
    agent-1: 205.50000000000006
    agent-2: 205.50000000000006
    agent-3: 205.50000000000006
    agent-4: 205.50000000000006
    agent-5: 205.50000000000006
  policy_reward_mean:
    agent-0: 179.15166666666647
    agent-1: 179.15166666666647
    agent-2: 179.15166666666647
    agent-3: 179.15166666666647
    agent-4: 179.15166666666647
    agent-5: 179.15166666666647
  policy_reward_min:
    agent-0: 121.99999999999997
    agent-1: 121.99999999999997
    agent-2: 121.99999999999997
    agent-3: 121.99999999999997
    agent-4: 121.99999999999997
    agent-5: 121.99999999999997
  sampler_perf:
    mean_env_wait_ms: 24.31666078886981
    mean_inference_ms: 12.316821813042681
    mean_processing_ms: 50.892321183211834
  time_since_restore: 47831.456771850586
  time_this_iter_s: 124.5135006904602
  time_total_s: 51042.52045798302
  timestamp: 1637065419
  timesteps_since_restore: 35232000
  timesteps_this_iter: 96000
  timesteps_total: 37152000
  training_iteration: 387
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    387 |          51042.5 | 37152000 |  1074.91 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 22
    apples_agent-0_mean: 0.39
    apples_agent-0_min: 0
    apples_agent-1_max: 125
    apples_agent-1_mean: 23.05
    apples_agent-1_min: 0
    apples_agent-2_max: 111
    apples_agent-2_mean: 14.47
    apples_agent-2_min: 0
    apples_agent-3_max: 89
    apples_agent-3_mean: 52.07
    apples_agent-3_min: 30
    apples_agent-4_max: 59
    apples_agent-4_mean: 1.21
    apples_agent-4_min: 0
    apples_agent-5_max: 182
    apples_agent-5_mean: 102.45
    apples_agent-5_min: 31
    cleaning_beam_agent-0_max: 569
    cleaning_beam_agent-0_mean: 462.09
    cleaning_beam_agent-0_min: 355
    cleaning_beam_agent-1_max: 453
    cleaning_beam_agent-1_mean: 285.85
    cleaning_beam_agent-1_min: 162
    cleaning_beam_agent-2_max: 537
    cleaning_beam_agent-2_mean: 355.62
    cleaning_beam_agent-2_min: 173
    cleaning_beam_agent-3_max: 153
    cleaning_beam_agent-3_mean: 17.56
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 571
    cleaning_beam_agent-4_mean: 452.34
    cleaning_beam_agent-4_min: 275
    cleaning_beam_agent-5_max: 181
    cleaning_beam_agent-5_mean: 16.81
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-25-44
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1250.000000000016
  episode_reward_mean: 1069.0099999999923
  episode_reward_min: 451.0000000000075
  episodes_this_iter: 96
  episodes_total: 37248
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20206.705
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8750321269035339
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013933554291725159
        model: {}
        policy_loss: -0.0028229891322553158
        total_loss: -0.002098554279655218
        vf_explained_var: 0.03731921315193176
        vf_loss: 22.644935607910156
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1426520347595215
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017818729393184185
        model: {}
        policy_loss: -0.004058189690113068
        total_loss: -0.0037993937730789185
        vf_explained_var: 0.03772565722465515
        vf_loss: 22.698650360107422
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.120222568511963
        entropy_coeff: 0.0017600000137463212
        kl: 0.00134948396589607
        model: {}
        policy_loss: -0.003224264830350876
        total_loss: -0.0028645293787121773
        vf_explained_var: 0.013026773929595947
        vf_loss: 23.313247680664062
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3277932405471802
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007977999630384147
        model: {}
        policy_loss: -0.002188295591622591
        total_loss: -0.0006300397217273712
        vf_explained_var: 0.09088596701622009
        vf_loss: 21.351722717285156
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8966052532196045
        entropy_coeff: 0.0017600000137463212
        kl: 0.001452922122552991
        model: {}
        policy_loss: -0.0037930766120553017
        total_loss: -0.003145602997392416
        vf_explained_var: 0.053052157163619995
        vf_loss: 22.25503158569336
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5178525447845459
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008535798988305032
        model: {}
        policy_loss: -0.002804785966873169
        total_loss: -0.0015824688598513603
        vf_explained_var: 0.09401480853557587
        vf_loss: 21.33734703063965
    load_time_ms: 14046.356
    num_steps_sampled: 37248000
    num_steps_trained: 37248000
    sample_time_ms: 90406.16
    update_time_ms: 23.881
  iterations_since_restore: 368
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.3954802259887
    ram_util_percent: 15.64745762711864
  pid: 4061
  policy_reward_max:
    agent-0: 208.3333333333331
    agent-1: 208.3333333333331
    agent-2: 208.3333333333331
    agent-3: 208.3333333333331
    agent-4: 208.3333333333331
    agent-5: 208.3333333333331
  policy_reward_mean:
    agent-0: 178.16833333333315
    agent-1: 178.16833333333315
    agent-2: 178.16833333333315
    agent-3: 178.16833333333315
    agent-4: 178.16833333333315
    agent-5: 178.16833333333315
  policy_reward_min:
    agent-0: 75.16666666666677
    agent-1: 75.16666666666677
    agent-2: 75.16666666666677
    agent-3: 75.16666666666677
    agent-4: 75.16666666666677
    agent-5: 75.16666666666677
  sampler_perf:
    mean_env_wait_ms: 24.31872296388225
    mean_inference_ms: 12.3164734812092
    mean_processing_ms: 50.890723138443285
  time_since_restore: 47956.03423857689
  time_this_iter_s: 124.5774667263031
  time_total_s: 51167.09792470932
  timestamp: 1637065544
  timesteps_since_restore: 35328000
  timesteps_this_iter: 96000
  timesteps_total: 37248000
  training_iteration: 388
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    388 |          51167.1 | 37248000 |  1069.01 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 49
    apples_agent-0_mean: 2.06
    apples_agent-0_min: 0
    apples_agent-1_max: 80
    apples_agent-1_mean: 20.54
    apples_agent-1_min: 0
    apples_agent-2_max: 87
    apples_agent-2_mean: 10.58
    apples_agent-2_min: 0
    apples_agent-3_max: 91
    apples_agent-3_mean: 52.93
    apples_agent-3_min: 28
    apples_agent-4_max: 62
    apples_agent-4_mean: 2.37
    apples_agent-4_min: 0
    apples_agent-5_max: 140
    apples_agent-5_mean: 100.9
    apples_agent-5_min: 65
    cleaning_beam_agent-0_max: 555
    cleaning_beam_agent-0_mean: 460.79
    cleaning_beam_agent-0_min: 347
    cleaning_beam_agent-1_max: 467
    cleaning_beam_agent-1_mean: 302.48
    cleaning_beam_agent-1_min: 178
    cleaning_beam_agent-2_max: 499
    cleaning_beam_agent-2_mean: 361.2
    cleaning_beam_agent-2_min: 161
    cleaning_beam_agent-3_max: 48
    cleaning_beam_agent-3_mean: 13.89
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 600
    cleaning_beam_agent-4_mean: 453.14
    cleaning_beam_agent-4_min: 327
    cleaning_beam_agent-5_max: 103
    cleaning_beam_agent-5_mean: 15.56
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-27-50
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1250.000000000016
  episode_reward_mean: 1065.759999999992
  episode_reward_min: 407.00000000000733
  episodes_this_iter: 96
  episodes_total: 37344
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20266.507
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8831331729888916
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012466717744246125
        model: {}
        policy_loss: -0.0028979319613426924
        total_loss: -0.002138568088412285
        vf_explained_var: 0.029249578714370728
        vf_loss: 23.136777877807617
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1443548202514648
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014692338882014155
        model: {}
        policy_loss: -0.0038503925316035748
        total_loss: -0.003482759464532137
        vf_explained_var: 0.0029668807983398438
        vf_loss: 23.81702995300293
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1327519416809082
        entropy_coeff: 0.0017600000137463212
        kl: 0.002299183513969183
        model: {}
        policy_loss: -0.003650844097137451
        total_loss: -0.0032645482569932938
        vf_explained_var: 0.0033062100410461426
        vf_loss: 23.799375534057617
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3283693194389343
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007339437725022435
        model: {}
        policy_loss: -0.0019780434668064117
        total_loss: -0.0004037506878376007
        vf_explained_var: 0.09515677392482758
        vf_loss: 21.522125244140625
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8933691382408142
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018137504812330008
        model: {}
        policy_loss: -0.003991159610450268
        total_loss: -0.003323757089674473
        vf_explained_var: 0.059972405433654785
        vf_loss: 22.397315979003906
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5300569534301758
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015827083261683583
        model: {}
        policy_loss: -0.0030262870714068413
        total_loss: -0.0017795613966882229
        vf_explained_var: 0.08741718530654907
        vf_loss: 21.79627227783203
    load_time_ms: 14051.266
    num_steps_sampled: 37344000
    num_steps_trained: 37344000
    sample_time_ms: 90432.611
    update_time_ms: 23.4
  iterations_since_restore: 369
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.233333333333333
    ram_util_percent: 15.583888888888886
  pid: 4061
  policy_reward_max:
    agent-0: 208.3333333333331
    agent-1: 208.3333333333331
    agent-2: 208.3333333333331
    agent-3: 208.3333333333331
    agent-4: 208.3333333333331
    agent-5: 208.3333333333331
  policy_reward_mean:
    agent-0: 177.62666666666658
    agent-1: 177.62666666666658
    agent-2: 177.62666666666658
    agent-3: 177.62666666666658
    agent-4: 177.62666666666658
    agent-5: 177.62666666666658
  policy_reward_min:
    agent-0: 67.83333333333333
    agent-1: 67.83333333333333
    agent-2: 67.83333333333333
    agent-3: 67.83333333333333
    agent-4: 67.83333333333333
    agent-5: 67.83333333333333
  sampler_perf:
    mean_env_wait_ms: 24.320741348570788
    mean_inference_ms: 12.316080417365917
    mean_processing_ms: 50.88948843786543
  time_since_restore: 48081.58142518997
  time_this_iter_s: 125.54718661308289
  time_total_s: 51292.6451113224
  timestamp: 1637065670
  timesteps_since_restore: 35424000
  timesteps_this_iter: 96000
  timesteps_total: 37344000
  training_iteration: 389
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    389 |          51292.6 | 37344000 |  1065.76 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 51
    apples_agent-0_mean: 2.05
    apples_agent-0_min: 0
    apples_agent-1_max: 99
    apples_agent-1_mean: 21.41
    apples_agent-1_min: 0
    apples_agent-2_max: 112
    apples_agent-2_mean: 12.42
    apples_agent-2_min: 0
    apples_agent-3_max: 168
    apples_agent-3_mean: 51.95
    apples_agent-3_min: 15
    apples_agent-4_max: 25
    apples_agent-4_mean: 0.68
    apples_agent-4_min: 0
    apples_agent-5_max: 179
    apples_agent-5_mean: 103.51
    apples_agent-5_min: 55
    cleaning_beam_agent-0_max: 599
    cleaning_beam_agent-0_mean: 460.33
    cleaning_beam_agent-0_min: 322
    cleaning_beam_agent-1_max: 473
    cleaning_beam_agent-1_mean: 294.72
    cleaning_beam_agent-1_min: 74
    cleaning_beam_agent-2_max: 505
    cleaning_beam_agent-2_mean: 342.96
    cleaning_beam_agent-2_min: 206
    cleaning_beam_agent-3_max: 145
    cleaning_beam_agent-3_mean: 19.7
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 566
    cleaning_beam_agent-4_mean: 454.04
    cleaning_beam_agent-4_min: 343
    cleaning_beam_agent-5_max: 133
    cleaning_beam_agent-5_mean: 18.65
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-29-54
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1226.0000000000218
  episode_reward_mean: 1054.799999999991
  episode_reward_min: 455.0000000000121
  episodes_this_iter: 96
  episodes_total: 37440
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20231.125
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9041184782981873
        entropy_coeff: 0.0017600000137463212
        kl: 0.001636922126635909
        model: {}
        policy_loss: -0.0031306413002312183
        total_loss: -0.002313169650733471
        vf_explained_var: 0.0627034455537796
        vf_loss: 24.08717155456543
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1403048038482666
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014739497564733028
        model: {}
        policy_loss: -0.003529996844008565
        total_loss: -0.002940703881904483
        vf_explained_var: -0.008722707629203796
        vf_loss: 25.96231460571289
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1242657899856567
        entropy_coeff: 0.0017600000137463212
        kl: 0.001555457478389144
        model: {}
        policy_loss: -0.0032603396102786064
        total_loss: -0.0026724012568593025
        vf_explained_var: 0.0015433579683303833
        vf_loss: 25.666446685791016
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35011744499206543
        entropy_coeff: 0.0017600000137463212
        kl: 0.000970955821685493
        model: {}
        policy_loss: -0.002436397597193718
        total_loss: -0.0008023809641599655
        vf_explained_var: 0.12503427267074585
        vf_loss: 22.502222061157227
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8842577934265137
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019408694934099913
        model: {}
        policy_loss: -0.0036219684407114983
        total_loss: -0.002686186693608761
        vf_explained_var: 0.030655637383461
        vf_loss: 24.920698165893555
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.521348237991333
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012306986609473825
        model: {}
        policy_loss: -0.0032572101335972548
        total_loss: -0.0019111412111669779
        vf_explained_var: 0.12057405710220337
        vf_loss: 22.636383056640625
    load_time_ms: 14087.259
    num_steps_sampled: 37440000
    num_steps_trained: 37440000
    sample_time_ms: 90377.358
    update_time_ms: 22.51
  iterations_since_restore: 370
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.840909090909092
    ram_util_percent: 15.65681818181818
  pid: 4061
  policy_reward_max:
    agent-0: 204.3333333333335
    agent-1: 204.3333333333335
    agent-2: 204.3333333333335
    agent-3: 204.3333333333335
    agent-4: 204.3333333333335
    agent-5: 204.3333333333335
  policy_reward_mean:
    agent-0: 175.7999999999998
    agent-1: 175.7999999999998
    agent-2: 175.7999999999998
    agent-3: 175.7999999999998
    agent-4: 175.7999999999998
    agent-5: 175.7999999999998
  policy_reward_min:
    agent-0: 75.83333333333324
    agent-1: 75.83333333333324
    agent-2: 75.83333333333324
    agent-3: 75.83333333333324
    agent-4: 75.83333333333324
    agent-5: 75.83333333333324
  sampler_perf:
    mean_env_wait_ms: 24.32248370798222
    mean_inference_ms: 12.315799041040604
    mean_processing_ms: 50.88825607650869
  time_since_restore: 48205.44104361534
  time_this_iter_s: 123.85961842536926
  time_total_s: 51416.50472974777
  timestamp: 1637065794
  timesteps_since_restore: 35520000
  timesteps_this_iter: 96000
  timesteps_total: 37440000
  training_iteration: 390
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    390 |          51416.5 | 37440000 |   1054.8 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 44
    apples_agent-0_mean: 1.07
    apples_agent-0_min: 0
    apples_agent-1_max: 82
    apples_agent-1_mean: 20.27
    apples_agent-1_min: 0
    apples_agent-2_max: 168
    apples_agent-2_mean: 15.38
    apples_agent-2_min: 0
    apples_agent-3_max: 153
    apples_agent-3_mean: 56.42
    apples_agent-3_min: 26
    apples_agent-4_max: 50
    apples_agent-4_mean: 1.4
    apples_agent-4_min: 0
    apples_agent-5_max: 213
    apples_agent-5_mean: 107.74
    apples_agent-5_min: 39
    cleaning_beam_agent-0_max: 558
    cleaning_beam_agent-0_mean: 459.81
    cleaning_beam_agent-0_min: 333
    cleaning_beam_agent-1_max: 463
    cleaning_beam_agent-1_mean: 299.07
    cleaning_beam_agent-1_min: 145
    cleaning_beam_agent-2_max: 585
    cleaning_beam_agent-2_mean: 338.38
    cleaning_beam_agent-2_min: 121
    cleaning_beam_agent-3_max: 89
    cleaning_beam_agent-3_mean: 16.86
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 552
    cleaning_beam_agent-4_mean: 454.12
    cleaning_beam_agent-4_min: 346
    cleaning_beam_agent-5_max: 121
    cleaning_beam_agent-5_mean: 14.07
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-31-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1206.0000000000146
  episode_reward_mean: 1048.1899999999916
  episode_reward_min: 532.0000000000093
  episodes_this_iter: 96
  episodes_total: 37536
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20218.753
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9014917612075806
        entropy_coeff: 0.0017600000137463212
        kl: 0.002032129094004631
        model: {}
        policy_loss: -0.002930564107373357
        total_loss: -0.002109268680214882
        vf_explained_var: 0.00393812358379364
        vf_loss: 24.079212188720703
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1413811445236206
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009520036983303726
        model: {}
        policy_loss: -0.003364815376698971
        total_loss: -0.0029460927471518517
        vf_explained_var: -0.003191232681274414
        vf_loss: 24.275524139404297
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1344809532165527
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022362254094332457
        model: {}
        policy_loss: -0.0035338043235242367
        total_loss: -0.003064938820898533
        vf_explained_var: -0.01962466537952423
        vf_loss: 24.65552520751953
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3410305380821228
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007026100065559149
        model: {}
        policy_loss: -0.002054347889497876
        total_loss: -0.0005039415555074811
        vf_explained_var: 0.11126601696014404
        vf_loss: 21.506187438964844
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.893402099609375
        entropy_coeff: 0.0017600000137463212
        kl: 0.001438421430066228
        model: {}
        policy_loss: -0.0034926454536616802
        total_loss: -0.0027427170425653458
        vf_explained_var: 0.04007042944431305
        vf_loss: 23.223154067993164
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5375206470489502
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010535053443163633
        model: {}
        policy_loss: -0.002933848649263382
        total_loss: -0.0016800407320261002
        vf_explained_var: 0.08942767977714539
        vf_loss: 21.998443603515625
    load_time_ms: 14065.555
    num_steps_sampled: 37536000
    num_steps_trained: 37536000
    sample_time_ms: 90271.367
    update_time_ms: 22.13
  iterations_since_restore: 371
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 16.505084745762712
    ram_util_percent: 15.718644067796609
  pid: 4061
  policy_reward_max:
    agent-0: 200.99999999999974
    agent-1: 200.99999999999974
    agent-2: 200.99999999999974
    agent-3: 200.99999999999974
    agent-4: 200.99999999999974
    agent-5: 200.99999999999974
  policy_reward_mean:
    agent-0: 174.69833333333315
    agent-1: 174.69833333333315
    agent-2: 174.69833333333315
    agent-3: 174.69833333333315
    agent-4: 174.69833333333315
    agent-5: 174.69833333333315
  policy_reward_min:
    agent-0: 88.66666666666669
    agent-1: 88.66666666666669
    agent-2: 88.66666666666669
    agent-3: 88.66666666666669
    agent-4: 88.66666666666669
    agent-5: 88.66666666666669
  sampler_perf:
    mean_env_wait_ms: 24.324646144018992
    mean_inference_ms: 12.315946773700551
    mean_processing_ms: 50.887364143314855
  time_since_restore: 48329.802243709564
  time_this_iter_s: 124.36120009422302
  time_total_s: 51540.865929841995
  timestamp: 1637065918
  timesteps_since_restore: 35616000
  timesteps_this_iter: 96000
  timesteps_total: 37536000
  training_iteration: 391
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 28.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    391 |          51540.9 | 37536000 |  1048.19 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 79
    apples_agent-0_mean: 2.61
    apples_agent-0_min: 0
    apples_agent-1_max: 88
    apples_agent-1_mean: 24.34
    apples_agent-1_min: 0
    apples_agent-2_max: 132
    apples_agent-2_mean: 11.62
    apples_agent-2_min: 0
    apples_agent-3_max: 90
    apples_agent-3_mean: 50.56
    apples_agent-3_min: 25
    apples_agent-4_max: 59
    apples_agent-4_mean: 0.69
    apples_agent-4_min: 0
    apples_agent-5_max: 196
    apples_agent-5_mean: 103.85
    apples_agent-5_min: 32
    cleaning_beam_agent-0_max: 592
    cleaning_beam_agent-0_mean: 471.83
    cleaning_beam_agent-0_min: 280
    cleaning_beam_agent-1_max: 458
    cleaning_beam_agent-1_mean: 287.05
    cleaning_beam_agent-1_min: 126
    cleaning_beam_agent-2_max: 524
    cleaning_beam_agent-2_mean: 359.3
    cleaning_beam_agent-2_min: 188
    cleaning_beam_agent-3_max: 69
    cleaning_beam_agent-3_mean: 17.37
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 566
    cleaning_beam_agent-4_mean: 447.12
    cleaning_beam_agent-4_min: 317
    cleaning_beam_agent-5_max: 373
    cleaning_beam_agent-5_mean: 22.95
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-34-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1205.9999999999952
  episode_reward_mean: 1051.8999999999915
  episode_reward_min: 667.999999999995
  episodes_this_iter: 96
  episodes_total: 37632
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20213.934
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8962478041648865
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015517605934292078
        model: {}
        policy_loss: -0.0030868814792484045
        total_loss: -0.0025244976859539747
        vf_explained_var: 0.021158576011657715
        vf_loss: 21.3978271484375
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1502056121826172
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014728684909641743
        model: {}
        policy_loss: -0.003680327907204628
        total_loss: -0.0034914053976535797
        vf_explained_var: -0.011568188667297363
        vf_loss: 22.132797241210938
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.128849983215332
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015839524567127228
        model: {}
        policy_loss: -0.0033555584959685802
        total_loss: -0.003155290614813566
        vf_explained_var: -0.00030410289764404297
        vf_loss: 21.87044334411621
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.33912593126296997
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009643330122344196
        model: {}
        policy_loss: -0.0022745842579752207
        total_loss: -0.0008754488080739975
        vf_explained_var: 0.08687415719032288
        vf_loss: 19.959970474243164
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8889623284339905
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019191887695342302
        model: {}
        policy_loss: -0.003908931277692318
        total_loss: -0.003348373807966709
        vf_explained_var: 0.02757561206817627
        vf_loss: 21.25128936767578
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5411644577980042
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010897901374846697
        model: {}
        policy_loss: -0.0029302090406417847
        total_loss: -0.0018051211955025792
        vf_explained_var: 0.05077308416366577
        vf_loss: 20.775375366210938
    load_time_ms: 14104.98
    num_steps_sampled: 37632000
    num_steps_trained: 37632000
    sample_time_ms: 90279.422
    update_time_ms: 22.295
  iterations_since_restore: 372
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.320903954802262
    ram_util_percent: 15.646327683615818
  pid: 4061
  policy_reward_max:
    agent-0: 200.99999999999997
    agent-1: 200.99999999999997
    agent-2: 200.99999999999997
    agent-3: 200.99999999999997
    agent-4: 200.99999999999997
    agent-5: 200.99999999999997
  policy_reward_mean:
    agent-0: 175.31666666666652
    agent-1: 175.31666666666652
    agent-2: 175.31666666666652
    agent-3: 175.31666666666652
    agent-4: 175.31666666666652
    agent-5: 175.31666666666652
  policy_reward_min:
    agent-0: 111.33333333333329
    agent-1: 111.33333333333329
    agent-2: 111.33333333333329
    agent-3: 111.33333333333329
    agent-4: 111.33333333333329
    agent-5: 111.33333333333329
  sampler_perf:
    mean_env_wait_ms: 24.325897874220004
    mean_inference_ms: 12.315468005513502
    mean_processing_ms: 50.885256224283985
  time_since_restore: 48454.07330417633
  time_this_iter_s: 124.27106046676636
  time_total_s: 51665.13699030876
  timestamp: 1637066043
  timesteps_since_restore: 35712000
  timesteps_this_iter: 96000
  timesteps_total: 37632000
  training_iteration: 392
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    392 |          51665.1 | 37632000 |   1051.9 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 36
    apples_agent-0_mean: 1.29
    apples_agent-0_min: 0
    apples_agent-1_max: 96
    apples_agent-1_mean: 24.59
    apples_agent-1_min: 0
    apples_agent-2_max: 133
    apples_agent-2_mean: 9.81
    apples_agent-2_min: 0
    apples_agent-3_max: 139
    apples_agent-3_mean: 52.72
    apples_agent-3_min: 24
    apples_agent-4_max: 60
    apples_agent-4_mean: 1.82
    apples_agent-4_min: 0
    apples_agent-5_max: 203
    apples_agent-5_mean: 101.45
    apples_agent-5_min: 66
    cleaning_beam_agent-0_max: 602
    cleaning_beam_agent-0_mean: 471.52
    cleaning_beam_agent-0_min: 347
    cleaning_beam_agent-1_max: 433
    cleaning_beam_agent-1_mean: 284.81
    cleaning_beam_agent-1_min: 173
    cleaning_beam_agent-2_max: 581
    cleaning_beam_agent-2_mean: 348.78
    cleaning_beam_agent-2_min: 189
    cleaning_beam_agent-3_max: 50
    cleaning_beam_agent-3_mean: 17.62
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 571
    cleaning_beam_agent-4_mean: 446.4
    cleaning_beam_agent-4_min: 362
    cleaning_beam_agent-5_max: 142
    cleaning_beam_agent-5_mean: 14.34
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-36-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1205.0000000000173
  episode_reward_mean: 1034.3099999999902
  episode_reward_min: 592.0000000000031
  episodes_this_iter: 96
  episodes_total: 37728
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20222.322
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9062780141830444
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017173776868730783
        model: {}
        policy_loss: -0.0031120593193918467
        total_loss: -0.0024466870818287134
        vf_explained_var: 0.026193618774414062
        vf_loss: 22.604217529296875
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1446229219436646
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013709614286199212
        model: {}
        policy_loss: -0.0036488021723926067
        total_loss: -0.003409892786294222
        vf_explained_var: 0.028905197978019714
        vf_loss: 22.53448486328125
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.127502202987671
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012800208060070872
        model: {}
        policy_loss: -0.003396755550056696
        total_loss: -0.0030614244751632214
        vf_explained_var: 0.0006453990936279297
        vf_loss: 23.197357177734375
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3506868779659271
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011462053516879678
        model: {}
        policy_loss: -0.0023647707421332598
        total_loss: -0.0008798079215921462
        vf_explained_var: 0.09467960894107819
        vf_loss: 21.021718978881836
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8908663392066956
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016076097963377833
        model: {}
        policy_loss: -0.0034750131890177727
        total_loss: -0.0028195595368742943
        vf_explained_var: 0.04345625638961792
        vf_loss: 22.233787536621094
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5302870869636536
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014785295352339745
        model: {}
        policy_loss: -0.0030139272566884756
        total_loss: -0.0018454724922776222
        vf_explained_var: 0.0943349152803421
        vf_loss: 21.017566680908203
    load_time_ms: 13818.257
    num_steps_sampled: 37728000
    num_steps_trained: 37728000
    sample_time_ms: 90176.071
    update_time_ms: 22.249
  iterations_since_restore: 373
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.37175141242938
    ram_util_percent: 15.663841807909602
  pid: 4061
  policy_reward_max:
    agent-0: 200.8333333333333
    agent-1: 200.8333333333333
    agent-2: 200.8333333333333
    agent-3: 200.8333333333333
    agent-4: 200.8333333333333
    agent-5: 200.8333333333333
  policy_reward_mean:
    agent-0: 172.38499999999988
    agent-1: 172.38499999999988
    agent-2: 172.38499999999988
    agent-3: 172.38499999999988
    agent-4: 172.38499999999988
    agent-5: 172.38499999999988
  policy_reward_min:
    agent-0: 98.66666666666683
    agent-1: 98.66666666666683
    agent-2: 98.66666666666683
    agent-3: 98.66666666666683
    agent-4: 98.66666666666683
    agent-5: 98.66666666666683
  sampler_perf:
    mean_env_wait_ms: 24.32710724373206
    mean_inference_ms: 12.315090060230554
    mean_processing_ms: 50.88307979482125
  time_since_restore: 48577.65163707733
  time_this_iter_s: 123.57833290100098
  time_total_s: 51788.71532320976
  timestamp: 1637066166
  timesteps_since_restore: 35808000
  timesteps_this_iter: 96000
  timesteps_total: 37728000
  training_iteration: 393
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    393 |          51788.7 | 37728000 |  1034.31 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 38
    apples_agent-0_mean: 2.03
    apples_agent-0_min: 0
    apples_agent-1_max: 84
    apples_agent-1_mean: 23.06
    apples_agent-1_min: 0
    apples_agent-2_max: 79
    apples_agent-2_mean: 15.06
    apples_agent-2_min: 0
    apples_agent-3_max: 88
    apples_agent-3_mean: 53.67
    apples_agent-3_min: 29
    apples_agent-4_max: 41
    apples_agent-4_mean: 0.76
    apples_agent-4_min: 0
    apples_agent-5_max: 189
    apples_agent-5_mean: 103.78
    apples_agent-5_min: 34
    cleaning_beam_agent-0_max: 569
    cleaning_beam_agent-0_mean: 459.07
    cleaning_beam_agent-0_min: 293
    cleaning_beam_agent-1_max: 459
    cleaning_beam_agent-1_mean: 291.8
    cleaning_beam_agent-1_min: 151
    cleaning_beam_agent-2_max: 498
    cleaning_beam_agent-2_mean: 338.86
    cleaning_beam_agent-2_min: 138
    cleaning_beam_agent-3_max: 91
    cleaning_beam_agent-3_mean: 17.86
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 539
    cleaning_beam_agent-4_mean: 444.34
    cleaning_beam_agent-4_min: 353
    cleaning_beam_agent-5_max: 211
    cleaning_beam_agent-5_mean: 13.93
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-38-10
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1209.9999999999943
  episode_reward_mean: 1044.5699999999913
  episode_reward_min: 474.0000000000167
  episodes_this_iter: 96
  episodes_total: 37824
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20206.514
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9197748899459839
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014987546019256115
        model: {}
        policy_loss: -0.0032939836382865906
        total_loss: -0.0026832809671759605
        vf_explained_var: 0.01622845232486725
        vf_loss: 22.29505157470703
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1433556079864502
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015496860723942518
        model: {}
        policy_loss: -0.003734833560883999
        total_loss: -0.0034542055800557137
        vf_explained_var: -0.012403503060340881
        vf_loss: 22.929367065429688
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1248197555541992
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009062933968380094
        model: {}
        policy_loss: -0.0030764942057430744
        total_loss: -0.0028439778834581375
        vf_explained_var: 0.021931365132331848
        vf_loss: 22.12197494506836
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.34944966435432434
        entropy_coeff: 0.0017600000137463212
        kl: 0.001309625105932355
        model: {}
        policy_loss: -0.0020324718207120895
        total_loss: -0.0006305687129497528
        vf_explained_var: 0.10769154131412506
        vf_loss: 20.16931915283203
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8909651041030884
        entropy_coeff: 0.0017600000137463212
        kl: 0.002330018673092127
        model: {}
        policy_loss: -0.003882275428622961
        total_loss: -0.0032781572081148624
        vf_explained_var: 0.040107086300849915
        vf_loss: 21.722192764282227
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.536475658416748
        entropy_coeff: 0.0017600000137463212
        kl: 0.00128281326033175
        model: {}
        policy_loss: -0.0030736096668988466
        total_loss: -0.0019062929786741734
        vf_explained_var: 0.06662298738956451
        vf_loss: 21.115127563476562
    load_time_ms: 13592.223
    num_steps_sampled: 37824000
    num_steps_trained: 37824000
    sample_time_ms: 90199.59
    update_time_ms: 22.355
  iterations_since_restore: 374
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.338636363636363
    ram_util_percent: 15.590340909090907
  pid: 4061
  policy_reward_max:
    agent-0: 201.66666666666686
    agent-1: 201.66666666666686
    agent-2: 201.66666666666686
    agent-3: 201.66666666666686
    agent-4: 201.66666666666686
    agent-5: 201.66666666666686
  policy_reward_mean:
    agent-0: 174.09499999999977
    agent-1: 174.09499999999977
    agent-2: 174.09499999999977
    agent-3: 174.09499999999977
    agent-4: 174.09499999999977
    agent-5: 174.09499999999977
  policy_reward_min:
    agent-0: 79.00000000000003
    agent-1: 79.00000000000003
    agent-2: 79.00000000000003
    agent-3: 79.00000000000003
    agent-4: 79.00000000000003
    agent-5: 79.00000000000003
  sampler_perf:
    mean_env_wait_ms: 24.328475160671164
    mean_inference_ms: 12.314736005127859
    mean_processing_ms: 50.88113899299517
  time_since_restore: 48701.49802112579
  time_this_iter_s: 123.84638404846191
  time_total_s: 51912.561707258224
  timestamp: 1637066290
  timesteps_since_restore: 35904000
  timesteps_this_iter: 96000
  timesteps_total: 37824000
  training_iteration: 394
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    394 |          51912.6 | 37824000 |  1044.57 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 25
    apples_agent-0_mean: 1.13
    apples_agent-0_min: 0
    apples_agent-1_max: 70
    apples_agent-1_mean: 18.53
    apples_agent-1_min: 0
    apples_agent-2_max: 104
    apples_agent-2_mean: 11.07
    apples_agent-2_min: 0
    apples_agent-3_max: 152
    apples_agent-3_mean: 51.88
    apples_agent-3_min: 26
    apples_agent-4_max: 105
    apples_agent-4_mean: 2.36
    apples_agent-4_min: 0
    apples_agent-5_max: 190
    apples_agent-5_mean: 99.01
    apples_agent-5_min: 45
    cleaning_beam_agent-0_max: 570
    cleaning_beam_agent-0_mean: 457.12
    cleaning_beam_agent-0_min: 327
    cleaning_beam_agent-1_max: 483
    cleaning_beam_agent-1_mean: 301.59
    cleaning_beam_agent-1_min: 171
    cleaning_beam_agent-2_max: 484
    cleaning_beam_agent-2_mean: 342.68
    cleaning_beam_agent-2_min: 150
    cleaning_beam_agent-3_max: 50
    cleaning_beam_agent-3_mean: 15.97
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 546
    cleaning_beam_agent-4_mean: 433.56
    cleaning_beam_agent-4_min: 297
    cleaning_beam_agent-5_max: 127
    cleaning_beam_agent-5_mean: 15.09
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-40-15
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1196.0000000000014
  episode_reward_mean: 1045.769999999993
  episode_reward_min: 488.0000000000041
  episodes_this_iter: 96
  episodes_total: 37920
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20188.214
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9220815896987915
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013959936331957579
        model: {}
        policy_loss: -0.002830689074471593
        total_loss: -0.0020106874871999025
        vf_explained_var: 0.028110235929489136
        vf_loss: 24.42868423461914
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.144642949104309
        entropy_coeff: 0.0017600000137463212
        kl: 0.001714828540571034
        model: {}
        policy_loss: -0.004023024346679449
        total_loss: -0.0035522887483239174
        vf_explained_var: 0.011224821209907532
        vf_loss: 24.853050231933594
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1443963050842285
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011092823697254062
        model: {}
        policy_loss: -0.0032077645882964134
        total_loss: -0.002762037329375744
        vf_explained_var: 0.02178516983985901
        vf_loss: 24.598644256591797
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.34188517928123474
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008538189576938748
        model: {}
        policy_loss: -0.0021798028610646725
        total_loss: -0.0006080404855310917
        vf_explained_var: 0.13478682935237885
        vf_loss: 21.73481559753418
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9096561670303345
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020861285738646984
        model: {}
        policy_loss: -0.0037789717316627502
        total_loss: -0.0030198823660612106
        vf_explained_var: 0.06064248085021973
        vf_loss: 23.6008243560791
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5116959810256958
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009004825260490179
        model: {}
        policy_loss: -0.003061281982809305
        total_loss: -0.0017370902933180332
        vf_explained_var: 0.11491091549396515
        vf_loss: 22.24779510498047
    load_time_ms: 13684.334
    num_steps_sampled: 37920000
    num_steps_trained: 37920000
    sample_time_ms: 90179.056
    update_time_ms: 21.898
  iterations_since_restore: 375
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 16.228651685393256
    ram_util_percent: 15.649438202247188
  pid: 4061
  policy_reward_max:
    agent-0: 199.33333333333294
    agent-1: 199.33333333333294
    agent-2: 199.33333333333294
    agent-3: 199.33333333333294
    agent-4: 199.33333333333294
    agent-5: 199.33333333333294
  policy_reward_mean:
    agent-0: 174.29499999999982
    agent-1: 174.29499999999982
    agent-2: 174.29499999999982
    agent-3: 174.29499999999982
    agent-4: 174.29499999999982
    agent-5: 174.29499999999982
  policy_reward_min:
    agent-0: 81.33333333333321
    agent-1: 81.33333333333321
    agent-2: 81.33333333333321
    agent-3: 81.33333333333321
    agent-4: 81.33333333333321
    agent-5: 81.33333333333321
  sampler_perf:
    mean_env_wait_ms: 24.330247052050154
    mean_inference_ms: 12.314385293071826
    mean_processing_ms: 50.87943841602786
  time_since_restore: 48825.58508181572
  time_this_iter_s: 124.08706068992615
  time_total_s: 52036.64876794815
  timestamp: 1637066415
  timesteps_since_restore: 36000000
  timesteps_this_iter: 96000
  timesteps_total: 37920000
  training_iteration: 395
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    395 |          52036.6 | 37920000 |  1045.77 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 40
    apples_agent-0_mean: 0.6
    apples_agent-0_min: 0
    apples_agent-1_max: 84
    apples_agent-1_mean: 22.05
    apples_agent-1_min: 0
    apples_agent-2_max: 178
    apples_agent-2_mean: 13.23
    apples_agent-2_min: 0
    apples_agent-3_max: 104
    apples_agent-3_mean: 52.1
    apples_agent-3_min: 22
    apples_agent-4_max: 89
    apples_agent-4_mean: 4.06
    apples_agent-4_min: 0
    apples_agent-5_max: 212
    apples_agent-5_mean: 101.37
    apples_agent-5_min: 28
    cleaning_beam_agent-0_max: 596
    cleaning_beam_agent-0_mean: 469.09
    cleaning_beam_agent-0_min: 308
    cleaning_beam_agent-1_max: 417
    cleaning_beam_agent-1_mean: 287.24
    cleaning_beam_agent-1_min: 119
    cleaning_beam_agent-2_max: 659
    cleaning_beam_agent-2_mean: 341.76
    cleaning_beam_agent-2_min: 106
    cleaning_beam_agent-3_max: 55
    cleaning_beam_agent-3_mean: 16.18
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 590
    cleaning_beam_agent-4_mean: 443.58
    cleaning_beam_agent-4_min: 294
    cleaning_beam_agent-5_max: 164
    cleaning_beam_agent-5_mean: 14.88
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-42-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1214.9999999999925
  episode_reward_mean: 1054.019999999991
  episode_reward_min: 325.0000000000023
  episodes_this_iter: 96
  episodes_total: 38016
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20197.199
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.916382908821106
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008695058641023934
        model: {}
        policy_loss: -0.002970918081700802
        total_loss: -0.002286571078002453
        vf_explained_var: 0.00910237431526184
        vf_loss: 22.971826553344727
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1590867042541504
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021548266522586346
        model: {}
        policy_loss: -0.004387070424854755
        total_loss: -0.004072363954037428
        vf_explained_var: -0.012430667877197266
        vf_loss: 23.54701805114746
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1429154872894287
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016945598181337118
        model: {}
        policy_loss: -0.0033091427758336067
        total_loss: -0.0029881016816943884
        vf_explained_var: -0.007462441921234131
        vf_loss: 23.32570457458496
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3484404683113098
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012200225610285997
        model: {}
        policy_loss: -0.002048670779913664
        total_loss: -0.0005520803388208151
        vf_explained_var: 0.08859264850616455
        vf_loss: 21.098451614379883
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8919715881347656
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017304255161434412
        model: {}
        policy_loss: -0.0036145104095339775
        total_loss: -0.003068513236939907
        vf_explained_var: 0.08381471037864685
        vf_loss: 21.158695220947266
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.520064115524292
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009807436726987362
        model: {}
        policy_loss: -0.003304258454591036
        total_loss: -0.0021132673136889935
        vf_explained_var: 0.08862832188606262
        vf_loss: 21.063039779663086
    load_time_ms: 13780.577
    num_steps_sampled: 38016000
    num_steps_trained: 38016000
    sample_time_ms: 90264.831
    update_time_ms: 21.546
  iterations_since_restore: 376
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.528813559322034
    ram_util_percent: 15.587570621468927
  pid: 4061
  policy_reward_max:
    agent-0: 202.49999999999952
    agent-1: 202.49999999999952
    agent-2: 202.49999999999952
    agent-3: 202.49999999999952
    agent-4: 202.49999999999952
    agent-5: 202.49999999999952
  policy_reward_mean:
    agent-0: 175.66999999999982
    agent-1: 175.66999999999982
    agent-2: 175.66999999999982
    agent-3: 175.66999999999982
    agent-4: 175.66999999999982
    agent-5: 175.66999999999982
  policy_reward_min:
    agent-0: 54.1666666666665
    agent-1: 54.1666666666665
    agent-2: 54.1666666666665
    agent-3: 54.1666666666665
    agent-4: 54.1666666666665
    agent-5: 54.1666666666665
  sampler_perf:
    mean_env_wait_ms: 24.332016402303775
    mean_inference_ms: 12.313984501889351
    mean_processing_ms: 50.878169091234994
  time_since_restore: 48950.50659656525
  time_this_iter_s: 124.92151474952698
  time_total_s: 52161.57028269768
  timestamp: 1637066540
  timesteps_since_restore: 36096000
  timesteps_this_iter: 96000
  timesteps_total: 38016000
  training_iteration: 396
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    396 |          52161.6 | 38016000 |  1054.02 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 36
    apples_agent-0_mean: 1.94
    apples_agent-0_min: 0
    apples_agent-1_max: 91
    apples_agent-1_mean: 20.47
    apples_agent-1_min: 0
    apples_agent-2_max: 100
    apples_agent-2_mean: 11.22
    apples_agent-2_min: 0
    apples_agent-3_max: 96
    apples_agent-3_mean: 52.67
    apples_agent-3_min: 29
    apples_agent-4_max: 61
    apples_agent-4_mean: 1.19
    apples_agent-4_min: 0
    apples_agent-5_max: 139
    apples_agent-5_mean: 99.12
    apples_agent-5_min: 35
    cleaning_beam_agent-0_max: 571
    cleaning_beam_agent-0_mean: 462.24
    cleaning_beam_agent-0_min: 322
    cleaning_beam_agent-1_max: 432
    cleaning_beam_agent-1_mean: 288.74
    cleaning_beam_agent-1_min: 160
    cleaning_beam_agent-2_max: 659
    cleaning_beam_agent-2_mean: 343.14
    cleaning_beam_agent-2_min: 106
    cleaning_beam_agent-3_max: 61
    cleaning_beam_agent-3_mean: 14.92
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 563
    cleaning_beam_agent-4_mean: 442.47
    cleaning_beam_agent-4_min: 328
    cleaning_beam_agent-5_max: 134
    cleaning_beam_agent-5_mean: 12.18
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-44-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1206.9999999999782
  episode_reward_mean: 1061.5099999999914
  episode_reward_min: 659.99999999999
  episodes_this_iter: 96
  episodes_total: 38112
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20200.341
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9241968393325806
        entropy_coeff: 0.0017600000137463212
        kl: 0.001748824492096901
        model: {}
        policy_loss: -0.003333712462335825
        total_loss: -0.00278960051946342
        vf_explained_var: 0.03872522711753845
        vf_loss: 21.70696258544922
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1510651111602783
        entropy_coeff: 0.0017600000137463212
        kl: 0.001606829115189612
        model: {}
        policy_loss: -0.003968796692788601
        total_loss: -0.0037662070244550705
        vf_explained_var: 0.01575826108455658
        vf_loss: 22.284629821777344
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1381750106811523
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017351536080241203
        model: {}
        policy_loss: -0.003353128209710121
        total_loss: -0.0030967146158218384
        vf_explained_var: 0.0006004422903060913
        vf_loss: 22.596004486083984
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.34497565031051636
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008174970280379057
        model: {}
        policy_loss: -0.002094342838972807
        total_loss: -0.0006668283604085445
        vf_explained_var: 0.09891459345817566
        vf_loss: 20.34674072265625
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8954556584358215
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018373148050159216
        model: {}
        policy_loss: -0.003715513739734888
        total_loss: -0.0031460062600672245
        vf_explained_var: 0.048301249742507935
        vf_loss: 21.4550838470459
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5060994625091553
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009968254016712308
        model: {}
        policy_loss: -0.0031191837042570114
        total_loss: -0.001919223926961422
        vf_explained_var: 0.07379941642284393
        vf_loss: 20.906970977783203
    load_time_ms: 13768.486
    num_steps_sampled: 38112000
    num_steps_trained: 38112000
    sample_time_ms: 90235.595
    update_time_ms: 21.608
  iterations_since_restore: 377
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.410112359550562
    ram_util_percent: 15.600561797752807
  pid: 4061
  policy_reward_max:
    agent-0: 201.16666666666663
    agent-1: 201.16666666666663
    agent-2: 201.16666666666663
    agent-3: 201.16666666666663
    agent-4: 201.16666666666663
    agent-5: 201.16666666666663
  policy_reward_mean:
    agent-0: 176.91833333333318
    agent-1: 176.91833333333318
    agent-2: 176.91833333333318
    agent-3: 176.91833333333318
    agent-4: 176.91833333333318
    agent-5: 176.91833333333318
  policy_reward_min:
    agent-0: 110.00000000000043
    agent-1: 110.00000000000043
    agent-2: 110.00000000000043
    agent-3: 110.00000000000043
    agent-4: 110.00000000000043
    agent-5: 110.00000000000043
  sampler_perf:
    mean_env_wait_ms: 24.333052650486263
    mean_inference_ms: 12.313580366781816
    mean_processing_ms: 50.87629854496547
  time_since_restore: 49074.64222764969
  time_this_iter_s: 124.13563108444214
  time_total_s: 52285.70591378212
  timestamp: 1637066664
  timesteps_since_restore: 36192000
  timesteps_this_iter: 96000
  timesteps_total: 38112000
  training_iteration: 397
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    397 |          52285.7 | 38112000 |  1061.51 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 38
    apples_agent-0_mean: 1.5
    apples_agent-0_min: 0
    apples_agent-1_max: 112
    apples_agent-1_mean: 23.67
    apples_agent-1_min: 0
    apples_agent-2_max: 122
    apples_agent-2_mean: 13.19
    apples_agent-2_min: 0
    apples_agent-3_max: 153
    apples_agent-3_mean: 52.02
    apples_agent-3_min: 30
    apples_agent-4_max: 42
    apples_agent-4_mean: 0.91
    apples_agent-4_min: 0
    apples_agent-5_max: 159
    apples_agent-5_mean: 99.04
    apples_agent-5_min: 42
    cleaning_beam_agent-0_max: 589
    cleaning_beam_agent-0_mean: 465.86
    cleaning_beam_agent-0_min: 349
    cleaning_beam_agent-1_max: 432
    cleaning_beam_agent-1_mean: 284.9
    cleaning_beam_agent-1_min: 139
    cleaning_beam_agent-2_max: 550
    cleaning_beam_agent-2_mean: 344.46
    cleaning_beam_agent-2_min: 149
    cleaning_beam_agent-3_max: 110
    cleaning_beam_agent-3_mean: 16.82
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 565
    cleaning_beam_agent-4_mean: 439.25
    cleaning_beam_agent-4_min: 334
    cleaning_beam_agent-5_max: 256
    cleaning_beam_agent-5_mean: 12.75
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-46-28
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1207.0000000000086
  episode_reward_mean: 1064.339999999993
  episode_reward_min: 665.0000000000006
  episodes_this_iter: 96
  episodes_total: 38208
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20202.45
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9177110195159912
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018207973334938288
        model: {}
        policy_loss: -0.0029113334603607655
        total_loss: -0.0023149685002863407
        vf_explained_var: 0.01954585313796997
        vf_loss: 22.1153564453125
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1512233018875122
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014619596768170595
        model: {}
        policy_loss: -0.0036344369873404503
        total_loss: -0.0034002503380179405
        vf_explained_var: -0.0006413906812667847
        vf_loss: 22.603410720825195
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1359316110610962
        entropy_coeff: 0.0017600000137463212
        kl: 0.001967902760952711
        model: {}
        policy_loss: -0.0038268687203526497
        total_loss: -0.003488512709736824
        vf_explained_var: -0.03433924913406372
        vf_loss: 23.375896453857422
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.34625208377838135
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008296392625197768
        model: {}
        policy_loss: -0.0020011616870760918
        total_loss: -0.0005354476161301136
        vf_explained_var: 0.08213692903518677
        vf_loss: 20.751142501831055
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8988317847251892
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019226763397455215
        model: {}
        policy_loss: -0.003557223128154874
        total_loss: -0.0029711585957556963
        vf_explained_var: 0.036725208163261414
        vf_loss: 21.68008804321289
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4827827215194702
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007418846362270415
        model: {}
        policy_loss: -0.0025793351233005524
        total_loss: -0.0013169738231226802
        vf_explained_var: 0.06373012065887451
        vf_loss: 21.120595932006836
    load_time_ms: 13819.975
    num_steps_sampled: 38208000
    num_steps_trained: 38208000
    sample_time_ms: 90101.829
    update_time_ms: 21.0
  iterations_since_restore: 378
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.500568181818181
    ram_util_percent: 15.591477272727273
  pid: 4061
  policy_reward_max:
    agent-0: 201.16666666666663
    agent-1: 201.16666666666663
    agent-2: 201.16666666666663
    agent-3: 201.16666666666663
    agent-4: 201.16666666666663
    agent-5: 201.16666666666663
  policy_reward_mean:
    agent-0: 177.38999999999982
    agent-1: 177.38999999999982
    agent-2: 177.38999999999982
    agent-3: 177.38999999999982
    agent-4: 177.38999999999982
    agent-5: 177.38999999999982
  policy_reward_min:
    agent-0: 110.83333333333358
    agent-1: 110.83333333333358
    agent-2: 110.83333333333358
    agent-3: 110.83333333333358
    agent-4: 110.83333333333358
    agent-5: 110.83333333333358
  sampler_perf:
    mean_env_wait_ms: 24.333924941555228
    mean_inference_ms: 12.312932485544131
    mean_processing_ms: 50.87371006581543
  time_since_restore: 49198.42011165619
  time_this_iter_s: 123.77788400650024
  time_total_s: 52409.48379778862
  timestamp: 1637066788
  timesteps_since_restore: 36288000
  timesteps_this_iter: 96000
  timesteps_total: 38208000
  training_iteration: 398
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 28.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    398 |          52409.5 | 38208000 |  1064.34 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 45
    apples_agent-0_mean: 1.28
    apples_agent-0_min: 0
    apples_agent-1_max: 91
    apples_agent-1_mean: 22.59
    apples_agent-1_min: 0
    apples_agent-2_max: 110
    apples_agent-2_mean: 15.28
    apples_agent-2_min: 0
    apples_agent-3_max: 134
    apples_agent-3_mean: 53.14
    apples_agent-3_min: 20
    apples_agent-4_max: 140
    apples_agent-4_mean: 2.05
    apples_agent-4_min: 0
    apples_agent-5_max: 213
    apples_agent-5_mean: 98.04
    apples_agent-5_min: 40
    cleaning_beam_agent-0_max: 572
    cleaning_beam_agent-0_mean: 459.49
    cleaning_beam_agent-0_min: 329
    cleaning_beam_agent-1_max: 460
    cleaning_beam_agent-1_mean: 291.95
    cleaning_beam_agent-1_min: 110
    cleaning_beam_agent-2_max: 550
    cleaning_beam_agent-2_mean: 334.06
    cleaning_beam_agent-2_min: 144
    cleaning_beam_agent-3_max: 65
    cleaning_beam_agent-3_mean: 18.77
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 548
    cleaning_beam_agent-4_mean: 446.96
    cleaning_beam_agent-4_min: 302
    cleaning_beam_agent-5_max: 211
    cleaning_beam_agent-5_mean: 19.37
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-48-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1206.9999999999918
  episode_reward_mean: 1046.40999999999
  episode_reward_min: 517.00000000001
  episodes_this_iter: 96
  episodes_total: 38304
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20155.868
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9053570032119751
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015789568424224854
        model: {}
        policy_loss: -0.002994319424033165
        total_loss: -0.00228549400344491
        vf_explained_var: 0.032881662249565125
        vf_loss: 23.022525787353516
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.152392029762268
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019930005073547363
        model: {}
        policy_loss: -0.0039376625791192055
        total_loss: -0.003610859625041485
        vf_explained_var: 0.00907832384109497
        vf_loss: 23.550151824951172
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1402499675750732
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011387791018933058
        model: {}
        policy_loss: -0.003438187064602971
        total_loss: -0.002966518746688962
        vf_explained_var: -0.04070371389389038
        vf_loss: 24.785062789916992
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35253533720970154
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008390221046283841
        model: {}
        policy_loss: -0.002067230176180601
        total_loss: -0.0005546740721911192
        vf_explained_var: 0.1023622453212738
        vf_loss: 21.330183029174805
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8784201741218567
        entropy_coeff: 0.0017600000137463212
        kl: 0.001777593046426773
        model: {}
        policy_loss: -0.003912885673344135
        total_loss: -0.0031497422605752945
        vf_explained_var: 0.027362078428268433
        vf_loss: 23.091646194458008
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5106678009033203
        entropy_coeff: 0.0017600000137463212
        kl: 0.001604370423592627
        model: {}
        policy_loss: -0.0030571864917874336
        total_loss: -0.00178506039083004
        vf_explained_var: 0.08706890046596527
        vf_loss: 21.709007263183594
    load_time_ms: 13808.708
    num_steps_sampled: 38304000
    num_steps_trained: 38304000
    sample_time_ms: 89930.183
    update_time_ms: 21.122
  iterations_since_restore: 379
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.529545454545454
    ram_util_percent: 15.65681818181818
  pid: 4061
  policy_reward_max:
    agent-0: 201.16666666666677
    agent-1: 201.16666666666677
    agent-2: 201.16666666666677
    agent-3: 201.16666666666677
    agent-4: 201.16666666666677
    agent-5: 201.16666666666677
  policy_reward_mean:
    agent-0: 174.40166666666642
    agent-1: 174.40166666666642
    agent-2: 174.40166666666642
    agent-3: 174.40166666666642
    agent-4: 174.40166666666642
    agent-5: 174.40166666666642
  policy_reward_min:
    agent-0: 86.16666666666657
    agent-1: 86.16666666666657
    agent-2: 86.16666666666657
    agent-3: 86.16666666666657
    agent-4: 86.16666666666657
    agent-5: 86.16666666666657
  sampler_perf:
    mean_env_wait_ms: 24.335015264094395
    mean_inference_ms: 12.312374546804614
    mean_processing_ms: 50.87168094392111
  time_since_restore: 49321.66426920891
  time_this_iter_s: 123.24415755271912
  time_total_s: 52532.72795534134
  timestamp: 1637066911
  timesteps_since_restore: 36384000
  timesteps_this_iter: 96000
  timesteps_total: 38304000
  training_iteration: 399
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    399 |          52532.7 | 38304000 |  1046.41 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 53
    apples_agent-0_mean: 1.89
    apples_agent-0_min: 0
    apples_agent-1_max: 77
    apples_agent-1_mean: 20.59
    apples_agent-1_min: 0
    apples_agent-2_max: 91
    apples_agent-2_mean: 15.93
    apples_agent-2_min: 0
    apples_agent-3_max: 91
    apples_agent-3_mean: 52.71
    apples_agent-3_min: 28
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 153
    apples_agent-5_mean: 99.1
    apples_agent-5_min: 43
    cleaning_beam_agent-0_max: 638
    cleaning_beam_agent-0_mean: 472.04
    cleaning_beam_agent-0_min: 329
    cleaning_beam_agent-1_max: 508
    cleaning_beam_agent-1_mean: 295.05
    cleaning_beam_agent-1_min: 145
    cleaning_beam_agent-2_max: 507
    cleaning_beam_agent-2_mean: 323.04
    cleaning_beam_agent-2_min: 157
    cleaning_beam_agent-3_max: 55
    cleaning_beam_agent-3_mean: 15.23
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 577
    cleaning_beam_agent-4_mean: 445.17
    cleaning_beam_agent-4_min: 334
    cleaning_beam_agent-5_max: 211
    cleaning_beam_agent-5_mean: 13.89
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-50-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1220.9999999999968
  episode_reward_mean: 1057.1399999999912
  episode_reward_min: 517.00000000001
  episodes_this_iter: 96
  episodes_total: 38400
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20169.214
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9101966619491577
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018820599652826786
        model: {}
        policy_loss: -0.0030456185340881348
        total_loss: -0.002489152830094099
        vf_explained_var: 0.05646517872810364
        vf_loss: 21.584110260009766
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1640512943267822
        entropy_coeff: 0.0017600000137463212
        kl: 0.002286216476932168
        model: {}
        policy_loss: -0.004124191123992205
        total_loss: -0.003891869680956006
        vf_explained_var: 0.0016216784715652466
        vf_loss: 22.81052017211914
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1471259593963623
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013619890669360757
        model: {}
        policy_loss: -0.003463044064119458
        total_loss: -0.0031998390331864357
        vf_explained_var: 0.002582177519798279
        vf_loss: 22.82145881652832
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.34559378027915955
        entropy_coeff: 0.0017600000137463212
        kl: 0.001130705582909286
        model: {}
        policy_loss: -0.002150436397641897
        total_loss: -0.0006927028298377991
        vf_explained_var: 0.09486298263072968
        vf_loss: 20.659753799438477
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8913760185241699
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021536988206207752
        model: {}
        policy_loss: -0.0038929665461182594
        total_loss: -0.0032474719919264317
        vf_explained_var: 0.025736674666404724
        vf_loss: 22.143177032470703
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48749491572380066
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011551561765372753
        model: {}
        policy_loss: -0.0029700160957872868
        total_loss: -0.0017627343768253922
        vf_explained_var: 0.0936013013124466
        vf_loss: 20.652719497680664
    load_time_ms: 13813.976
    num_steps_sampled: 38400000
    num_steps_trained: 38400000
    sample_time_ms: 89967.648
    update_time_ms: 21.426
  iterations_since_restore: 380
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.70056179775281
    ram_util_percent: 15.573595505617975
  pid: 4061
  policy_reward_max:
    agent-0: 203.4999999999997
    agent-1: 203.4999999999997
    agent-2: 203.4999999999997
    agent-3: 203.4999999999997
    agent-4: 203.4999999999997
    agent-5: 203.4999999999997
  policy_reward_mean:
    agent-0: 176.18999999999988
    agent-1: 176.18999999999988
    agent-2: 176.18999999999988
    agent-3: 176.18999999999988
    agent-4: 176.18999999999988
    agent-5: 176.18999999999988
  policy_reward_min:
    agent-0: 86.16666666666657
    agent-1: 86.16666666666657
    agent-2: 86.16666666666657
    agent-3: 86.16666666666657
    agent-4: 86.16666666666657
    agent-5: 86.16666666666657
  sampler_perf:
    mean_env_wait_ms: 24.3366345177373
    mean_inference_ms: 12.31200060406226
    mean_processing_ms: 50.87058953143837
  time_since_restore: 49446.06390428543
  time_this_iter_s: 124.39963507652283
  time_total_s: 52657.12759041786
  timestamp: 1637067036
  timesteps_since_restore: 36480000
  timesteps_this_iter: 96000
  timesteps_total: 38400000
  training_iteration: 400
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    400 |          52657.1 | 38400000 |  1057.14 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 54
    apples_agent-0_mean: 1.41
    apples_agent-0_min: 0
    apples_agent-1_max: 100
    apples_agent-1_mean: 24.78
    apples_agent-1_min: 0
    apples_agent-2_max: 136
    apples_agent-2_mean: 10.24
    apples_agent-2_min: 0
    apples_agent-3_max: 90
    apples_agent-3_mean: 50.3
    apples_agent-3_min: 27
    apples_agent-4_max: 40
    apples_agent-4_mean: 0.75
    apples_agent-4_min: 0
    apples_agent-5_max: 147
    apples_agent-5_mean: 98.42
    apples_agent-5_min: 51
    cleaning_beam_agent-0_max: 574
    cleaning_beam_agent-0_mean: 465.66
    cleaning_beam_agent-0_min: 348
    cleaning_beam_agent-1_max: 452
    cleaning_beam_agent-1_mean: 272.51
    cleaning_beam_agent-1_min: 120
    cleaning_beam_agent-2_max: 528
    cleaning_beam_agent-2_mean: 345.19
    cleaning_beam_agent-2_min: 182
    cleaning_beam_agent-3_max: 50
    cleaning_beam_agent-3_mean: 17.68
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 574
    cleaning_beam_agent-4_mean: 445.67
    cleaning_beam_agent-4_min: 328
    cleaning_beam_agent-5_max: 127
    cleaning_beam_agent-5_mean: 10.84
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-52-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1187.9999999999934
  episode_reward_mean: 1058.8099999999902
  episode_reward_min: 832.0000000000151
  episodes_this_iter: 96
  episodes_total: 38496
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20154.433
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9303138256072998
        entropy_coeff: 0.0017600000137463212
        kl: 0.002094237133860588
        model: {}
        policy_loss: -0.002900478895753622
        total_loss: -0.0023690187372267246
        vf_explained_var: -0.018287450075149536
        vf_loss: 21.68813133239746
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1540188789367676
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020587267354130745
        model: {}
        policy_loss: -0.00403103232383728
        total_loss: -0.0038993721827864647
        vf_explained_var: -0.016203835606575012
        vf_loss: 21.627328872680664
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1405539512634277
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012141058687120676
        model: {}
        policy_loss: -0.0033580251038074493
        total_loss: -0.003195352852344513
        vf_explained_var: -0.02168731391429901
        vf_loss: 21.70047950744629
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3484671711921692
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007403237977996469
        model: {}
        policy_loss: -0.0017525681760162115
        total_loss: -0.0003862064331769943
        vf_explained_var: 0.06813696026802063
        vf_loss: 19.796630859375
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9075793027877808
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023309066891670227
        model: {}
        policy_loss: -0.00409392174333334
        total_loss: -0.003681827336549759
        vf_explained_var: 0.05299675464630127
        vf_loss: 20.094341278076172
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4934313893318176
        entropy_coeff: 0.0017600000137463212
        kl: 0.000703678757417947
        model: {}
        policy_loss: -0.0025609363801777363
        total_loss: -0.0014392060693353415
        vf_explained_var: 0.06342382729053497
        vf_loss: 19.901683807373047
    load_time_ms: 13859.769
    num_steps_sampled: 38496000
    num_steps_trained: 38496000
    sample_time_ms: 89844.667
    update_time_ms: 21.279
  iterations_since_restore: 381
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.431818181818182
    ram_util_percent: 15.657954545454544
  pid: 4061
  policy_reward_max:
    agent-0: 197.9999999999996
    agent-1: 197.9999999999996
    agent-2: 197.9999999999996
    agent-3: 197.9999999999996
    agent-4: 197.9999999999996
    agent-5: 197.9999999999996
  policy_reward_mean:
    agent-0: 176.46833333333316
    agent-1: 176.46833333333316
    agent-2: 176.46833333333316
    agent-3: 176.46833333333316
    agent-4: 176.46833333333316
    agent-5: 176.46833333333316
  policy_reward_min:
    agent-0: 138.66666666666663
    agent-1: 138.66666666666663
    agent-2: 138.66666666666663
    agent-3: 138.66666666666663
    agent-4: 138.66666666666663
    agent-5: 138.66666666666663
  sampler_perf:
    mean_env_wait_ms: 24.337552078647786
    mean_inference_ms: 12.311553397727957
    mean_processing_ms: 50.8682247455534
  time_since_restore: 49569.53239989281
  time_this_iter_s: 123.4684956073761
  time_total_s: 52780.59608602524
  timestamp: 1637067160
  timesteps_since_restore: 36576000
  timesteps_this_iter: 96000
  timesteps_total: 38496000
  training_iteration: 401
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    401 |          52780.6 | 38496000 |  1058.81 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 73
    apples_agent-0_mean: 1.97
    apples_agent-0_min: 0
    apples_agent-1_max: 76
    apples_agent-1_mean: 19.63
    apples_agent-1_min: 0
    apples_agent-2_max: 117
    apples_agent-2_mean: 12.85
    apples_agent-2_min: 0
    apples_agent-3_max: 152
    apples_agent-3_mean: 51.27
    apples_agent-3_min: 26
    apples_agent-4_max: 67
    apples_agent-4_mean: 1.56
    apples_agent-4_min: 0
    apples_agent-5_max: 158
    apples_agent-5_mean: 99.44
    apples_agent-5_min: 32
    cleaning_beam_agent-0_max: 585
    cleaning_beam_agent-0_mean: 450.34
    cleaning_beam_agent-0_min: 317
    cleaning_beam_agent-1_max: 455
    cleaning_beam_agent-1_mean: 297.45
    cleaning_beam_agent-1_min: 132
    cleaning_beam_agent-2_max: 536
    cleaning_beam_agent-2_mean: 335.8
    cleaning_beam_agent-2_min: 137
    cleaning_beam_agent-3_max: 99
    cleaning_beam_agent-3_mean: 19.09
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 522
    cleaning_beam_agent-4_mean: 418.94
    cleaning_beam_agent-4_min: 301
    cleaning_beam_agent-5_max: 224
    cleaning_beam_agent-5_mean: 15.91
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-54-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1244.0000000000073
  episode_reward_mean: 1050.6999999999916
  episode_reward_min: 563.0000000000059
  episodes_this_iter: 96
  episodes_total: 38592
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20136.281
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9211227893829346
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012154878349974751
        model: {}
        policy_loss: -0.003027389058843255
        total_loss: -0.002350094262510538
        vf_explained_var: 0.05032704770565033
        vf_loss: 22.984703063964844
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1434224843978882
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017743287608027458
        model: {}
        policy_loss: -0.0036454068031162024
        total_loss: -0.003268847707659006
        vf_explained_var: 0.01348121464252472
        vf_loss: 23.889854431152344
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1467903852462769
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012700813822448254
        model: {}
        policy_loss: -0.0036485083401203156
        total_loss: -0.003233164083212614
        vf_explained_var: -0.004126846790313721
        vf_loss: 24.336923599243164
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35448455810546875
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008248172816820443
        model: {}
        policy_loss: -0.0022925245575606823
        total_loss: -0.000771896680817008
        vf_explained_var: 0.11441631615161896
        vf_loss: 21.445192337036133
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9137095212936401
        entropy_coeff: 0.0017600000137463212
        kl: 0.001436443068087101
        model: {}
        policy_loss: -0.003669468918815255
        total_loss: -0.0029960782267153263
        vf_explained_var: 0.05646418035030365
        vf_loss: 22.81519889831543
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4901324212551117
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012653630692511797
        model: {}
        policy_loss: -0.0026528649032115936
        total_loss: -0.0013233590871095657
        vf_explained_var: 0.0945991575717926
        vf_loss: 21.92136001586914
    load_time_ms: 13775.299
    num_steps_sampled: 38592000
    num_steps_trained: 38592000
    sample_time_ms: 89787.435
    update_time_ms: 20.986
  iterations_since_restore: 382
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.474857142857143
    ram_util_percent: 15.572000000000003
  pid: 4061
  policy_reward_max:
    agent-0: 207.3333333333331
    agent-1: 207.3333333333331
    agent-2: 207.3333333333331
    agent-3: 207.3333333333331
    agent-4: 207.3333333333331
    agent-5: 207.3333333333331
  policy_reward_mean:
    agent-0: 175.11666666666653
    agent-1: 175.11666666666653
    agent-2: 175.11666666666653
    agent-3: 175.11666666666653
    agent-4: 175.11666666666653
    agent-5: 175.11666666666653
  policy_reward_min:
    agent-0: 93.83333333333346
    agent-1: 93.83333333333346
    agent-2: 93.83333333333346
    agent-3: 93.83333333333346
    agent-4: 93.83333333333346
    agent-5: 93.83333333333346
  sampler_perf:
    mean_env_wait_ms: 24.338337631843324
    mean_inference_ms: 12.311281424874764
    mean_processing_ms: 50.86637245249534
  time_since_restore: 49692.19223213196
  time_this_iter_s: 122.659832239151
  time_total_s: 52903.25591826439
  timestamp: 1637067283
  timesteps_since_restore: 36672000
  timesteps_this_iter: 96000
  timesteps_total: 38592000
  training_iteration: 402
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    402 |          52903.3 | 38592000 |   1050.7 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 50
    apples_agent-0_mean: 1.6
    apples_agent-0_min: 0
    apples_agent-1_max: 80
    apples_agent-1_mean: 21.85
    apples_agent-1_min: 0
    apples_agent-2_max: 103
    apples_agent-2_mean: 11.67
    apples_agent-2_min: 0
    apples_agent-3_max: 95
    apples_agent-3_mean: 51.73
    apples_agent-3_min: 27
    apples_agent-4_max: 82
    apples_agent-4_mean: 2.46
    apples_agent-4_min: 0
    apples_agent-5_max: 150
    apples_agent-5_mean: 98.34
    apples_agent-5_min: 53
    cleaning_beam_agent-0_max: 558
    cleaning_beam_agent-0_mean: 454.8
    cleaning_beam_agent-0_min: 222
    cleaning_beam_agent-1_max: 500
    cleaning_beam_agent-1_mean: 310.12
    cleaning_beam_agent-1_min: 167
    cleaning_beam_agent-2_max: 510
    cleaning_beam_agent-2_mean: 350.78
    cleaning_beam_agent-2_min: 144
    cleaning_beam_agent-3_max: 61
    cleaning_beam_agent-3_mean: 17.91
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 546
    cleaning_beam_agent-4_mean: 428.0
    cleaning_beam_agent-4_min: 323
    cleaning_beam_agent-5_max: 81
    cleaning_beam_agent-5_mean: 10.8
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-56-48
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1234.0000000000073
  episode_reward_mean: 1068.3599999999956
  episode_reward_min: 570.0000000000102
  episodes_this_iter: 96
  episodes_total: 38688
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20168.373
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9282742142677307
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015499527798965573
        model: {}
        policy_loss: -0.0028618592768907547
        total_loss: -0.0022601503878831863
        vf_explained_var: 0.04845583438873291
        vf_loss: 22.35470962524414
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1465060710906982
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014961842680349946
        model: {}
        policy_loss: -0.0036049033515155315
        total_loss: -0.0032240203581750393
        vf_explained_var: -0.01928618550300598
        vf_loss: 23.987346649169922
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1383239030838013
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010850385297089815
        model: {}
        policy_loss: -0.003269046312198043
        total_loss: -0.002903764136135578
        vf_explained_var: -0.007487595081329346
        vf_loss: 23.68730926513672
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3452848792076111
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009980557952076197
        model: {}
        policy_loss: -0.002523808740079403
        total_loss: -0.0010154792107641697
        vf_explained_var: 0.09983578324317932
        vf_loss: 21.160293579101562
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9169011116027832
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015701630618423223
        model: {}
        policy_loss: -0.0035679321736097336
        total_loss: -0.0028875377029180527
        vf_explained_var: 0.02323538064956665
        vf_loss: 22.941404342651367
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4760701656341553
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008459578966721892
        model: {}
        policy_loss: -0.0026765877846628428
        total_loss: -0.0013627642765641212
        vf_explained_var: 0.08625274896621704
        vf_loss: 21.517057418823242
    load_time_ms: 13806.982
    num_steps_sampled: 38688000
    num_steps_trained: 38688000
    sample_time_ms: 89869.072
    update_time_ms: 20.799
  iterations_since_restore: 383
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 18.110674157303368
    ram_util_percent: 16.225280898876402
  pid: 4061
  policy_reward_max:
    agent-0: 205.66666666666663
    agent-1: 205.66666666666663
    agent-2: 205.66666666666663
    agent-3: 205.66666666666663
    agent-4: 205.66666666666663
    agent-5: 205.66666666666663
  policy_reward_mean:
    agent-0: 178.05999999999986
    agent-1: 178.05999999999986
    agent-2: 178.05999999999986
    agent-3: 178.05999999999986
    agent-4: 178.05999999999986
    agent-5: 178.05999999999986
  policy_reward_min:
    agent-0: 95.00000000000018
    agent-1: 95.00000000000018
    agent-2: 95.00000000000018
    agent-3: 95.00000000000018
    agent-4: 95.00000000000018
    agent-5: 95.00000000000018
  sampler_perf:
    mean_env_wait_ms: 24.339945198489794
    mean_inference_ms: 12.311042307375306
    mean_processing_ms: 50.86596259847676
  time_since_restore: 49817.243335962296
  time_this_iter_s: 125.05110383033752
  time_total_s: 53028.30702209473
  timestamp: 1637067408
  timesteps_since_restore: 36768000
  timesteps_this_iter: 96000
  timesteps_total: 38688000
  training_iteration: 403
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    403 |          53028.3 | 38688000 |  1068.36 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 39
    apples_agent-0_mean: 2.11
    apples_agent-0_min: 0
    apples_agent-1_max: 91
    apples_agent-1_mean: 21.92
    apples_agent-1_min: 0
    apples_agent-2_max: 126
    apples_agent-2_mean: 12.29
    apples_agent-2_min: 0
    apples_agent-3_max: 104
    apples_agent-3_mean: 55.5
    apples_agent-3_min: 34
    apples_agent-4_max: 72
    apples_agent-4_mean: 1.14
    apples_agent-4_min: 0
    apples_agent-5_max: 204
    apples_agent-5_mean: 103.04
    apples_agent-5_min: 57
    cleaning_beam_agent-0_max: 573
    cleaning_beam_agent-0_mean: 443.9
    cleaning_beam_agent-0_min: 295
    cleaning_beam_agent-1_max: 498
    cleaning_beam_agent-1_mean: 309.02
    cleaning_beam_agent-1_min: 161
    cleaning_beam_agent-2_max: 533
    cleaning_beam_agent-2_mean: 357.35
    cleaning_beam_agent-2_min: 159
    cleaning_beam_agent-3_max: 93
    cleaning_beam_agent-3_mean: 15.98
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 543
    cleaning_beam_agent-4_mean: 431.81
    cleaning_beam_agent-4_min: 323
    cleaning_beam_agent-5_max: 89
    cleaning_beam_agent-5_mean: 12.91
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-58-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1221.9999999999925
  episode_reward_mean: 1070.289999999992
  episode_reward_min: 594.9999999999992
  episodes_this_iter: 96
  episodes_total: 38784
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20205.362
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9200372099876404
        entropy_coeff: 0.0017600000137463212
        kl: 0.002116581192240119
        model: {}
        policy_loss: -0.0030163421761244535
        total_loss: -0.002404164057224989
        vf_explained_var: 0.054568007588386536
        vf_loss: 22.314455032348633
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1396631002426147
        entropy_coeff: 0.0017600000137463212
        kl: 0.001316939014941454
        model: {}
        policy_loss: -0.0035452779848128557
        total_loss: -0.0032517253421247005
        vf_explained_var: 0.02501063048839569
        vf_loss: 22.993629455566406
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.143855333328247
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012175296433269978
        model: {}
        policy_loss: -0.003532819217070937
        total_loss: -0.003166070906445384
        vf_explained_var: -0.010441884398460388
        vf_loss: 23.799365997314453
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.33386847376823425
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010781624587252736
        model: {}
        policy_loss: -0.0021838266402482986
        total_loss: -0.0006599975749850273
        vf_explained_var: 0.1012687236070633
        vf_loss: 21.114377975463867
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.91463303565979
        entropy_coeff: 0.0017600000137463212
        kl: 0.001649327459745109
        model: {}
        policy_loss: -0.003943228162825108
        total_loss: -0.003318425267934799
        vf_explained_var: 0.05117860436439514
        vf_loss: 22.345577239990234
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.475367933511734
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006741166580468416
        model: {}
        policy_loss: -0.002590694697573781
        total_loss: -0.0013305963948369026
        vf_explained_var: 0.1107160896062851
        vf_loss: 20.96746253967285
    load_time_ms: 13865.459
    num_steps_sampled: 38784000
    num_steps_trained: 38784000
    sample_time_ms: 90519.06
    update_time_ms: 20.345
  iterations_since_restore: 384
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.949468085106382
    ram_util_percent: 19.23351063829787
  pid: 4061
  policy_reward_max:
    agent-0: 203.6666666666666
    agent-1: 203.6666666666666
    agent-2: 203.6666666666666
    agent-3: 203.6666666666666
    agent-4: 203.6666666666666
    agent-5: 203.6666666666666
  policy_reward_mean:
    agent-0: 178.38166666666646
    agent-1: 178.38166666666646
    agent-2: 178.38166666666646
    agent-3: 178.38166666666646
    agent-4: 178.38166666666646
    agent-5: 178.38166666666646
  policy_reward_min:
    agent-0: 99.16666666666676
    agent-1: 99.16666666666676
    agent-2: 99.16666666666676
    agent-3: 99.16666666666676
    agent-4: 99.16666666666676
    agent-5: 99.16666666666676
  sampler_perf:
    mean_env_wait_ms: 24.343552298641107
    mean_inference_ms: 12.312009753954257
    mean_processing_ms: 50.86923657486153
  time_since_restore: 49948.506959438324
  time_this_iter_s: 131.26362347602844
  time_total_s: 53159.570645570755
  timestamp: 1637067539
  timesteps_since_restore: 36864000
  timesteps_this_iter: 96000
  timesteps_total: 38784000
  training_iteration: 404
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    404 |          53159.6 | 38784000 |  1070.29 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 28
    apples_agent-0_mean: 1.16
    apples_agent-0_min: 0
    apples_agent-1_max: 108
    apples_agent-1_mean: 22.33
    apples_agent-1_min: 0
    apples_agent-2_max: 119
    apples_agent-2_mean: 9.09
    apples_agent-2_min: 0
    apples_agent-3_max: 167
    apples_agent-3_mean: 53.84
    apples_agent-3_min: 30
    apples_agent-4_max: 68
    apples_agent-4_mean: 1.82
    apples_agent-4_min: 0
    apples_agent-5_max: 179
    apples_agent-5_mean: 100.7
    apples_agent-5_min: 62
    cleaning_beam_agent-0_max: 560
    cleaning_beam_agent-0_mean: 441.15
    cleaning_beam_agent-0_min: 298
    cleaning_beam_agent-1_max: 465
    cleaning_beam_agent-1_mean: 303.68
    cleaning_beam_agent-1_min: 92
    cleaning_beam_agent-2_max: 528
    cleaning_beam_agent-2_mean: 353.99
    cleaning_beam_agent-2_min: 168
    cleaning_beam_agent-3_max: 60
    cleaning_beam_agent-3_mean: 16.3
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 530
    cleaning_beam_agent-4_mean: 431.12
    cleaning_beam_agent-4_min: 303
    cleaning_beam_agent-5_max: 89
    cleaning_beam_agent-5_mean: 9.98
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-01-11
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1236.9999999999832
  episode_reward_mean: 1074.6699999999923
  episode_reward_min: 626.9999999999992
  episodes_this_iter: 96
  episodes_total: 38880
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20231.578
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9080628156661987
        entropy_coeff: 0.0017600000137463212
        kl: 0.001715758815407753
        model: {}
        policy_loss: -0.003053806722164154
        total_loss: -0.0024020164273679256
        vf_explained_var: 0.04136669635772705
        vf_loss: 22.49979019165039
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1461114883422852
        entropy_coeff: 0.0017600000137463212
        kl: 0.001391648082062602
        model: {}
        policy_loss: -0.004007292445749044
        total_loss: -0.0036334360484033823
        vf_explained_var: -0.01698407530784607
        vf_loss: 23.910125732421875
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1362810134887695
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015697440830990672
        model: {}
        policy_loss: -0.0034301145933568478
        total_loss: -0.0030474967788904905
        vf_explained_var: -0.014876306056976318
        vf_loss: 23.824726104736328
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.342543363571167
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011153676314279437
        model: {}
        policy_loss: -0.0020383812952786684
        total_loss: -0.0005532526411116123
        vf_explained_var: 0.10858914256095886
        vf_loss: 20.880054473876953
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9035234451293945
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017785625532269478
        model: {}
        policy_loss: -0.0036810096353292465
        total_loss: -0.0030549392104148865
        vf_explained_var: 0.05315782129764557
        vf_loss: 22.162736892700195
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46357274055480957
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007230712799355388
        model: {}
        policy_loss: -0.0024609442334622145
        total_loss: -0.0011360740754753351
        vf_explained_var: 0.08928267657756805
        vf_loss: 21.407564163208008
    load_time_ms: 13850.965
    num_steps_sampled: 38880000
    num_steps_trained: 38880000
    sample_time_ms: 91235.097
    update_time_ms: 19.916
  iterations_since_restore: 385
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.08930481283423
    ram_util_percent: 19.166310160427805
  pid: 4061
  policy_reward_max:
    agent-0: 206.16666666666674
    agent-1: 206.16666666666674
    agent-2: 206.16666666666674
    agent-3: 206.16666666666674
    agent-4: 206.16666666666674
    agent-5: 206.16666666666674
  policy_reward_mean:
    agent-0: 179.1116666666665
    agent-1: 179.1116666666665
    agent-2: 179.1116666666665
    agent-3: 179.1116666666665
    agent-4: 179.1116666666665
    agent-5: 179.1116666666665
  policy_reward_min:
    agent-0: 104.50000000000033
    agent-1: 104.50000000000033
    agent-2: 104.50000000000033
    agent-3: 104.50000000000033
    agent-4: 104.50000000000033
    agent-5: 104.50000000000033
  sampler_perf:
    mean_env_wait_ms: 24.347742538728436
    mean_inference_ms: 12.313043595023167
    mean_processing_ms: 50.87397754546633
  time_since_restore: 50079.899476766586
  time_this_iter_s: 131.39251732826233
  time_total_s: 53290.96316289902
  timestamp: 1637067671
  timesteps_since_restore: 36960000
  timesteps_this_iter: 96000
  timesteps_total: 38880000
  training_iteration: 405
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    405 |            53291 | 38880000 |  1074.67 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 18
    apples_agent-0_mean: 0.79
    apples_agent-0_min: 0
    apples_agent-1_max: 81
    apples_agent-1_mean: 21.54
    apples_agent-1_min: 0
    apples_agent-2_max: 157
    apples_agent-2_mean: 14.11
    apples_agent-2_min: 0
    apples_agent-3_max: 81
    apples_agent-3_mean: 52.72
    apples_agent-3_min: 28
    apples_agent-4_max: 52
    apples_agent-4_mean: 2.02
    apples_agent-4_min: 0
    apples_agent-5_max: 356
    apples_agent-5_mean: 100.71
    apples_agent-5_min: 52
    cleaning_beam_agent-0_max: 566
    cleaning_beam_agent-0_mean: 449.62
    cleaning_beam_agent-0_min: 338
    cleaning_beam_agent-1_max: 448
    cleaning_beam_agent-1_mean: 297.78
    cleaning_beam_agent-1_min: 134
    cleaning_beam_agent-2_max: 558
    cleaning_beam_agent-2_mean: 358.7
    cleaning_beam_agent-2_min: 143
    cleaning_beam_agent-3_max: 46
    cleaning_beam_agent-3_mean: 16.14
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 563
    cleaning_beam_agent-4_mean: 440.4
    cleaning_beam_agent-4_min: 337
    cleaning_beam_agent-5_max: 207
    cleaning_beam_agent-5_mean: 12.63
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-03-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1227.000000000003
  episode_reward_mean: 1065.099999999992
  episode_reward_min: 559.0000000000141
  episodes_this_iter: 96
  episodes_total: 38976
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20216.766
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8961223363876343
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013996621128171682
        model: {}
        policy_loss: -0.002971269190311432
        total_loss: -0.0021795995999127626
        vf_explained_var: 0.01832030713558197
        vf_loss: 23.68844223022461
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1464300155639648
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014996810350567102
        model: {}
        policy_loss: -0.00343046011403203
        total_loss: -0.0030160145834088326
        vf_explained_var: -0.006064057350158691
        vf_loss: 24.32162857055664
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1317944526672363
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017407427076250315
        model: {}
        policy_loss: -0.0035924939438700676
        total_loss: -0.003067052224650979
        vf_explained_var: -0.04361313581466675
        vf_loss: 25.173992156982422
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35053718090057373
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006113356212154031
        model: {}
        policy_loss: -0.0022178329527378082
        total_loss: -0.0007067595142871141
        vf_explained_var: 0.11662279069423676
        vf_loss: 21.280174255371094
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8913423418998718
        entropy_coeff: 0.0017600000137463212
        kl: 0.00211710250005126
        model: {}
        policy_loss: -0.0038523543626070023
        total_loss: -0.003094225190579891
        vf_explained_var: 0.03445503115653992
        vf_loss: 23.268949508666992
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47976189851760864
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010670543415471911
        model: {}
        policy_loss: -0.002924349159002304
        total_loss: -0.0015849699266254902
        vf_explained_var: 0.09491530060768127
        vf_loss: 21.837623596191406
    load_time_ms: 13809.81
    num_steps_sampled: 38976000
    num_steps_trained: 38976000
    sample_time_ms: 91696.176
    update_time_ms: 20.013
  iterations_since_restore: 386
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.078804347826086
    ram_util_percent: 19.21630434782609
  pid: 4061
  policy_reward_max:
    agent-0: 204.49999999999994
    agent-1: 204.49999999999994
    agent-2: 204.49999999999994
    agent-3: 204.49999999999994
    agent-4: 204.49999999999994
    agent-5: 204.49999999999994
  policy_reward_mean:
    agent-0: 177.51666666666657
    agent-1: 177.51666666666657
    agent-2: 177.51666666666657
    agent-3: 177.51666666666657
    agent-4: 177.51666666666657
    agent-5: 177.51666666666657
  policy_reward_min:
    agent-0: 93.166666666667
    agent-1: 93.166666666667
    agent-2: 93.166666666667
    agent-3: 93.166666666667
    agent-4: 93.166666666667
    agent-5: 93.166666666667
  sampler_perf:
    mean_env_wait_ms: 24.351153137044147
    mean_inference_ms: 12.31405646665355
    mean_processing_ms: 50.876908372350115
  time_since_restore: 50208.85631799698
  time_this_iter_s: 128.95684123039246
  time_total_s: 53419.92000412941
  timestamp: 1637067800
  timesteps_since_restore: 37056000
  timesteps_this_iter: 96000
  timesteps_total: 38976000
  training_iteration: 406
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    406 |          53419.9 | 38976000 |   1065.1 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 33
    apples_agent-0_mean: 1.09
    apples_agent-0_min: 0
    apples_agent-1_max: 91
    apples_agent-1_mean: 24.03
    apples_agent-1_min: 0
    apples_agent-2_max: 146
    apples_agent-2_mean: 12.52
    apples_agent-2_min: 0
    apples_agent-3_max: 135
    apples_agent-3_mean: 55.7
    apples_agent-3_min: 31
    apples_agent-4_max: 63
    apples_agent-4_mean: 0.73
    apples_agent-4_min: 0
    apples_agent-5_max: 210
    apples_agent-5_mean: 102.92
    apples_agent-5_min: 62
    cleaning_beam_agent-0_max: 573
    cleaning_beam_agent-0_mean: 457.79
    cleaning_beam_agent-0_min: 342
    cleaning_beam_agent-1_max: 442
    cleaning_beam_agent-1_mean: 295.4
    cleaning_beam_agent-1_min: 105
    cleaning_beam_agent-2_max: 558
    cleaning_beam_agent-2_mean: 346.81
    cleaning_beam_agent-2_min: 100
    cleaning_beam_agent-3_max: 115
    cleaning_beam_agent-3_mean: 17.07
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 588
    cleaning_beam_agent-4_mean: 451.46
    cleaning_beam_agent-4_min: 362
    cleaning_beam_agent-5_max: 54
    cleaning_beam_agent-5_mean: 9.73
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-05-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1225.0000000000082
  episode_reward_mean: 1085.7699999999936
  episode_reward_min: 552.0000000000074
  episodes_this_iter: 96
  episodes_total: 39072
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20214.993
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8973772525787354
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016275240341201425
        model: {}
        policy_loss: -0.0029945787973701954
        total_loss: -0.0022697285749018192
        vf_explained_var: 0.021307945251464844
        vf_loss: 23.042339324951172
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1400327682495117
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015246433904394507
        model: {}
        policy_loss: -0.0034307469613850117
        total_loss: -0.0031059016473591328
        vf_explained_var: 0.010901689529418945
        vf_loss: 23.31302261352539
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1417783498764038
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012194636510685086
        model: {}
        policy_loss: -0.0034393230453133583
        total_loss: -0.0031035589054226875
        vf_explained_var: 0.010905742645263672
        vf_loss: 23.452926635742188
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3409802317619324
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008908549207262695
        model: {}
        policy_loss: -0.0020685000345110893
        total_loss: -0.0005393312312662601
        vf_explained_var: 0.09019783139228821
        vf_loss: 21.29288101196289
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8989330530166626
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013193811755627394
        model: {}
        policy_loss: -0.003272663801908493
        total_loss: -0.0026675034314393997
        vf_explained_var: 0.06162711977958679
        vf_loss: 21.872806549072266
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4554896652698517
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007007949170656502
        model: {}
        policy_loss: -0.0023199468851089478
        total_loss: -0.000989717897027731
        vf_explained_var: 0.09054692089557648
        vf_loss: 21.318931579589844
    load_time_ms: 13808.545
    num_steps_sampled: 39072000
    num_steps_trained: 39072000
    sample_time_ms: 92376.174
    update_time_ms: 20.11
  iterations_since_restore: 387
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.908021390374326
    ram_util_percent: 19.172192513368984
  pid: 4061
  policy_reward_max:
    agent-0: 204.1666666666667
    agent-1: 204.1666666666667
    agent-2: 204.1666666666667
    agent-3: 204.1666666666667
    agent-4: 204.1666666666667
    agent-5: 204.1666666666667
  policy_reward_mean:
    agent-0: 180.96166666666653
    agent-1: 180.96166666666653
    agent-2: 180.96166666666653
    agent-3: 180.96166666666653
    agent-4: 180.96166666666653
    agent-5: 180.96166666666653
  policy_reward_min:
    agent-0: 92.00000000000013
    agent-1: 92.00000000000013
    agent-2: 92.00000000000013
    agent-3: 92.00000000000013
    agent-4: 92.00000000000013
    agent-5: 92.00000000000013
  sampler_perf:
    mean_env_wait_ms: 24.3547321567375
    mean_inference_ms: 12.315002490893601
    mean_processing_ms: 50.880222453515124
  time_since_restore: 50339.7589969635
  time_this_iter_s: 130.90267896652222
  time_total_s: 53550.82268309593
  timestamp: 1637067931
  timesteps_since_restore: 37152000
  timesteps_this_iter: 96000
  timesteps_total: 39072000
  training_iteration: 407
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    407 |          53550.8 | 39072000 |  1085.77 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 63
    apples_agent-0_mean: 3.48
    apples_agent-0_min: 0
    apples_agent-1_max: 92
    apples_agent-1_mean: 18.34
    apples_agent-1_min: 0
    apples_agent-2_max: 112
    apples_agent-2_mean: 13.05
    apples_agent-2_min: 0
    apples_agent-3_max: 123
    apples_agent-3_mean: 53.42
    apples_agent-3_min: 28
    apples_agent-4_max: 56
    apples_agent-4_mean: 1.36
    apples_agent-4_min: 0
    apples_agent-5_max: 165
    apples_agent-5_mean: 99.73
    apples_agent-5_min: 52
    cleaning_beam_agent-0_max: 551
    cleaning_beam_agent-0_mean: 459.78
    cleaning_beam_agent-0_min: 331
    cleaning_beam_agent-1_max: 470
    cleaning_beam_agent-1_mean: 314.46
    cleaning_beam_agent-1_min: 151
    cleaning_beam_agent-2_max: 540
    cleaning_beam_agent-2_mean: 326.8
    cleaning_beam_agent-2_min: 132
    cleaning_beam_agent-3_max: 74
    cleaning_beam_agent-3_mean: 15.83
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 577
    cleaning_beam_agent-4_mean: 455.67
    cleaning_beam_agent-4_min: 333
    cleaning_beam_agent-5_max: 308
    cleaning_beam_agent-5_mean: 15.85
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-07-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1237.0000000000034
  episode_reward_mean: 1078.8899999999928
  episode_reward_min: 607.9999999999995
  episodes_this_iter: 96
  episodes_total: 39168
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20195.969
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9051433801651001
        entropy_coeff: 0.0017600000137463212
        kl: 0.002395299496129155
        model: {}
        policy_loss: -0.0030399071983993053
        total_loss: -0.002390169072896242
        vf_explained_var: 0.03705050051212311
        vf_loss: 22.427906036376953
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1507350206375122
        entropy_coeff: 0.0017600000137463212
        kl: 0.001199672115035355
        model: {}
        policy_loss: -0.0035705985501408577
        total_loss: -0.0033119218423962593
        vf_explained_var: 0.021775051951408386
        vf_loss: 22.839698791503906
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1424787044525146
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010112766176462173
        model: {}
        policy_loss: -0.0034673779737204313
        total_loss: -0.003023632103577256
        vf_explained_var: -0.04506474733352661
        vf_loss: 24.545066833496094
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.34525272250175476
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013151944149285555
        model: {}
        policy_loss: -0.0021012574434280396
        total_loss: -0.0005944222211837769
        vf_explained_var: 0.09004926681518555
        vf_loss: 21.14481544494629
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8919424414634705
        entropy_coeff: 0.0017600000137463212
        kl: 0.002215882996097207
        model: {}
        policy_loss: -0.003912708722054958
        total_loss: -0.003247483866289258
        vf_explained_var: 0.040008366107940674
        vf_loss: 22.350452423095703
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.472059965133667
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010805437341332436
        model: {}
        policy_loss: -0.002573243109509349
        total_loss: -0.0012959418818354607
        vf_explained_var: 0.09833377599716187
        vf_loss: 21.081275939941406
    load_time_ms: 13721.742
    num_steps_sampled: 39168000
    num_steps_trained: 39168000
    sample_time_ms: 93240.539
    update_time_ms: 21.265
  iterations_since_restore: 388
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.002673796791445
    ram_util_percent: 19.207486631016042
  pid: 4061
  policy_reward_max:
    agent-0: 206.16666666666674
    agent-1: 206.16666666666674
    agent-2: 206.16666666666674
    agent-3: 206.16666666666674
    agent-4: 206.16666666666674
    agent-5: 206.16666666666674
  policy_reward_mean:
    agent-0: 179.81499999999983
    agent-1: 179.81499999999983
    agent-2: 179.81499999999983
    agent-3: 179.81499999999983
    agent-4: 179.81499999999983
    agent-5: 179.81499999999983
  policy_reward_min:
    agent-0: 101.33333333333346
    agent-1: 101.33333333333346
    agent-2: 101.33333333333346
    agent-3: 101.33333333333346
    agent-4: 101.33333333333346
    agent-5: 101.33333333333346
  sampler_perf:
    mean_env_wait_ms: 24.35841591468379
    mean_inference_ms: 12.315936068557795
    mean_processing_ms: 50.88348837620083
  time_since_restore: 50471.10041189194
  time_this_iter_s: 131.34141492843628
  time_total_s: 53682.16409802437
  timestamp: 1637068063
  timesteps_since_restore: 37248000
  timesteps_this_iter: 96000
  timesteps_total: 39168000
  training_iteration: 408
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    408 |          53682.2 | 39168000 |  1078.89 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 41
    apples_agent-0_mean: 1.52
    apples_agent-0_min: 0
    apples_agent-1_max: 93
    apples_agent-1_mean: 16.95
    apples_agent-1_min: 0
    apples_agent-2_max: 573
    apples_agent-2_mean: 19.07
    apples_agent-2_min: 0
    apples_agent-3_max: 129
    apples_agent-3_mean: 54.83
    apples_agent-3_min: 26
    apples_agent-4_max: 56
    apples_agent-4_mean: 1.41
    apples_agent-4_min: 0
    apples_agent-5_max: 488
    apples_agent-5_mean: 104.05
    apples_agent-5_min: 65
    cleaning_beam_agent-0_max: 560
    cleaning_beam_agent-0_mean: 461.31
    cleaning_beam_agent-0_min: 254
    cleaning_beam_agent-1_max: 477
    cleaning_beam_agent-1_mean: 313.47
    cleaning_beam_agent-1_min: 152
    cleaning_beam_agent-2_max: 554
    cleaning_beam_agent-2_mean: 328.25
    cleaning_beam_agent-2_min: 118
    cleaning_beam_agent-3_max: 113
    cleaning_beam_agent-3_mean: 16.9
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 545
    cleaning_beam_agent-4_mean: 459.25
    cleaning_beam_agent-4_min: 337
    cleaning_beam_agent-5_max: 71
    cleaning_beam_agent-5_mean: 11.75
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-09-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1250.0000000000086
  episode_reward_mean: 1076.9699999999943
  episode_reward_min: 649.999999999998
  episodes_this_iter: 96
  episodes_total: 39264
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20222.822
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8918790221214294
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015668238047510386
        model: {}
        policy_loss: -0.0028502449858933687
        total_loss: -0.0021042581647634506
        vf_explained_var: 0.02586844563484192
        vf_loss: 23.156917572021484
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1560293436050415
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012249208521097898
        model: {}
        policy_loss: -0.003671381389722228
        total_loss: -0.0034091889392584562
        vf_explained_var: 0.03183084726333618
        vf_loss: 22.967987060546875
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1467604637145996
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014698767336085439
        model: {}
        policy_loss: -0.0034795966930687428
        total_loss: -0.0030258574988693
        vf_explained_var: -0.03382468223571777
        vf_loss: 24.720394134521484
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.34440574049949646
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007632972556166351
        model: {}
        policy_loss: -0.0018140199827030301
        total_loss: -0.00028528645634651184
        vf_explained_var: 0.09798678755760193
        vf_loss: 21.348896026611328
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8819189071655273
        entropy_coeff: 0.0017600000137463212
        kl: 0.001566006918437779
        model: {}
        policy_loss: -0.0034684021957218647
        total_loss: -0.0027063568122684956
        vf_explained_var: 0.022715970873832703
        vf_loss: 23.142227172851562
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4806159734725952
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010656205704435706
        model: {}
        policy_loss: -0.0025980332866311073
        total_loss: -0.0012646294198930264
        vf_explained_var: 0.08280423283576965
        vf_loss: 21.792865753173828
    load_time_ms: 13763.384
    num_steps_sampled: 39264000
    num_steps_trained: 39264000
    sample_time_ms: 93908.443
    update_time_ms: 20.938
  iterations_since_restore: 389
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.349462365591396
    ram_util_percent: 19.175806451612903
  pid: 4061
  policy_reward_max:
    agent-0: 208.33333333333326
    agent-1: 208.33333333333326
    agent-2: 208.33333333333326
    agent-3: 208.33333333333326
    agent-4: 208.33333333333326
    agent-5: 208.33333333333326
  policy_reward_mean:
    agent-0: 179.4949999999999
    agent-1: 179.4949999999999
    agent-2: 179.4949999999999
    agent-3: 179.4949999999999
    agent-4: 179.4949999999999
    agent-5: 179.4949999999999
  policy_reward_min:
    agent-0: 108.33333333333422
    agent-1: 108.33333333333422
    agent-2: 108.33333333333422
    agent-3: 108.33333333333422
    agent-4: 108.33333333333422
    agent-5: 108.33333333333422
  sampler_perf:
    mean_env_wait_ms: 24.362541593440973
    mean_inference_ms: 12.316768231334331
    mean_processing_ms: 50.88690768520055
  time_since_restore: 50601.733288526535
  time_this_iter_s: 130.63287663459778
  time_total_s: 53812.796974658966
  timestamp: 1637068193
  timesteps_since_restore: 37344000
  timesteps_this_iter: 96000
  timesteps_total: 39264000
  training_iteration: 409
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    409 |          53812.8 | 39264000 |  1076.97 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 40
    apples_agent-0_mean: 1.21
    apples_agent-0_min: 0
    apples_agent-1_max: 91
    apples_agent-1_mean: 21.91
    apples_agent-1_min: 0
    apples_agent-2_max: 95
    apples_agent-2_mean: 10.64
    apples_agent-2_min: 0
    apples_agent-3_max: 90
    apples_agent-3_mean: 55.15
    apples_agent-3_min: 31
    apples_agent-4_max: 47
    apples_agent-4_mean: 1.3
    apples_agent-4_min: 0
    apples_agent-5_max: 223
    apples_agent-5_mean: 99.49
    apples_agent-5_min: 47
    cleaning_beam_agent-0_max: 582
    cleaning_beam_agent-0_mean: 463.22
    cleaning_beam_agent-0_min: 307
    cleaning_beam_agent-1_max: 486
    cleaning_beam_agent-1_mean: 310.07
    cleaning_beam_agent-1_min: 168
    cleaning_beam_agent-2_max: 548
    cleaning_beam_agent-2_mean: 339.72
    cleaning_beam_agent-2_min: 122
    cleaning_beam_agent-3_max: 75
    cleaning_beam_agent-3_mean: 14.42
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 566
    cleaning_beam_agent-4_mean: 461.12
    cleaning_beam_agent-4_min: 351
    cleaning_beam_agent-5_max: 171
    cleaning_beam_agent-5_mean: 14.27
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-12-05
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1253.9999999999902
  episode_reward_mean: 1093.1399999999949
  episode_reward_min: 723.0000000000053
  episodes_this_iter: 96
  episodes_total: 39360
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20237.169
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8932784199714661
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012173595605418086
        model: {}
        policy_loss: -0.0027270375285297632
        total_loss: -0.0019332087831571698
        vf_explained_var: 0.028030723333358765
        vf_loss: 23.660001754760742
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1566795110702515
        entropy_coeff: 0.0017600000137463212
        kl: 0.001888495753519237
        model: {}
        policy_loss: -0.003942002542316914
        total_loss: -0.00353130791336298
        vf_explained_var: 0.0031483322381973267
        vf_loss: 24.464519500732422
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.140670895576477
        entropy_coeff: 0.0017600000137463212
        kl: 0.001661811606027186
        model: {}
        policy_loss: -0.003492752555757761
        total_loss: -0.003050129860639572
        vf_explained_var: 0.0015162527561187744
        vf_loss: 24.50203514099121
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.326516717672348
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011893194168806076
        model: {}
        policy_loss: -0.002173507818952203
        total_loss: -0.0005485927686095238
        vf_explained_var: 0.09351181983947754
        vf_loss: 21.995824813842773
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8844903111457825
        entropy_coeff: 0.0017600000137463212
        kl: 0.002026199596002698
        model: {}
        policy_loss: -0.0035931244492530823
        total_loss: -0.0028496570885181427
        vf_explained_var: 0.05513449013233185
        vf_loss: 23.001708984375
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4859858751296997
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010379401501268148
        model: {}
        policy_loss: -0.0029633198864758015
        total_loss: -0.0016246038721874356
        vf_explained_var: 0.1094498485326767
        vf_loss: 21.94052505493164
    load_time_ms: 13794.147
    num_steps_sampled: 39360000
    num_steps_trained: 39360000
    sample_time_ms: 94521.932
    update_time_ms: 21.58
  iterations_since_restore: 390
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.279144385026743
    ram_util_percent: 19.23903743315508
  pid: 4061
  policy_reward_max:
    agent-0: 209.00000000000014
    agent-1: 209.00000000000014
    agent-2: 209.00000000000014
    agent-3: 209.00000000000014
    agent-4: 209.00000000000014
    agent-5: 209.00000000000014
  policy_reward_mean:
    agent-0: 182.18999999999977
    agent-1: 182.18999999999977
    agent-2: 182.18999999999977
    agent-3: 182.18999999999977
    agent-4: 182.18999999999977
    agent-5: 182.18999999999977
  policy_reward_min:
    agent-0: 120.49999999999996
    agent-1: 120.49999999999996
    agent-2: 120.49999999999996
    agent-3: 120.49999999999996
    agent-4: 120.49999999999996
    agent-5: 120.49999999999996
  sampler_perf:
    mean_env_wait_ms: 24.366716190369207
    mean_inference_ms: 12.317725435519137
    mean_processing_ms: 50.890186083671
  time_since_restore: 50732.71626162529
  time_this_iter_s: 130.98297309875488
  time_total_s: 53943.77994775772
  timestamp: 1637068325
  timesteps_since_restore: 37440000
  timesteps_this_iter: 96000
  timesteps_total: 39360000
  training_iteration: 410
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    410 |          53943.8 | 39360000 |  1093.14 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 69
    apples_agent-0_mean: 3.85
    apples_agent-0_min: 0
    apples_agent-1_max: 77
    apples_agent-1_mean: 21.86
    apples_agent-1_min: 0
    apples_agent-2_max: 180
    apples_agent-2_mean: 17.69
    apples_agent-2_min: 0
    apples_agent-3_max: 119
    apples_agent-3_mean: 54.04
    apples_agent-3_min: 23
    apples_agent-4_max: 107
    apples_agent-4_mean: 1.07
    apples_agent-4_min: 0
    apples_agent-5_max: 209
    apples_agent-5_mean: 99.77
    apples_agent-5_min: 30
    cleaning_beam_agent-0_max: 554
    cleaning_beam_agent-0_mean: 454.74
    cleaning_beam_agent-0_min: 320
    cleaning_beam_agent-1_max: 515
    cleaning_beam_agent-1_mean: 300.23
    cleaning_beam_agent-1_min: 119
    cleaning_beam_agent-2_max: 493
    cleaning_beam_agent-2_mean: 336.93
    cleaning_beam_agent-2_min: 144
    cleaning_beam_agent-3_max: 159
    cleaning_beam_agent-3_mean: 16.74
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 590
    cleaning_beam_agent-4_mean: 467.24
    cleaning_beam_agent-4_min: 351
    cleaning_beam_agent-5_max: 147
    cleaning_beam_agent-5_mean: 16.7
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-14-15
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1243.0000000000014
  episode_reward_mean: 1058.9099999999942
  episode_reward_min: 561.0000000000069
  episodes_this_iter: 96
  episodes_total: 39456
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20256.831
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9261484146118164
        entropy_coeff: 0.0017600000137463212
        kl: 0.002745280973613262
        model: {}
        policy_loss: -0.003151754615828395
        total_loss: -0.0023926522117108107
        vf_explained_var: 0.05870486795902252
        vf_loss: 23.891237258911133
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1663410663604736
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016911705024540424
        model: {}
        policy_loss: -0.003916152752935886
        total_loss: -0.003438385669142008
        vf_explained_var: 0.0025976598262786865
        vf_loss: 25.305278778076172
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1405929327011108
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013838671147823334
        model: {}
        policy_loss: -0.003416790161281824
        total_loss: -0.002957724966108799
        vf_explained_var: 0.0301198810338974
        vf_loss: 24.665069580078125
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35401004552841187
        entropy_coeff: 0.0017600000137463212
        kl: 0.00102097331546247
        model: {}
        policy_loss: -0.0022078752517700195
        total_loss: -0.0005359956994652748
        vf_explained_var: 0.09599775075912476
        vf_loss: 22.949359893798828
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.875633716583252
        entropy_coeff: 0.0017600000137463212
        kl: 0.002073851879686117
        model: {}
        policy_loss: -0.0038656038232147694
        total_loss: -0.0029249724466353655
        vf_explained_var: 0.02125382423400879
        vf_loss: 24.81745147705078
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5083708167076111
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007490444695577025
        model: {}
        policy_loss: -0.00301593029871583
        total_loss: -0.0016784206964075565
        vf_explained_var: 0.12183244526386261
        vf_loss: 22.322418212890625
    load_time_ms: 13792.836
    num_steps_sampled: 39456000
    num_steps_trained: 39456000
    sample_time_ms: 95206.96
    update_time_ms: 21.184
  iterations_since_restore: 391
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.177956989247313
    ram_util_percent: 19.224731182795693
  pid: 4061
  policy_reward_max:
    agent-0: 207.16666666666643
    agent-1: 207.16666666666643
    agent-2: 207.16666666666643
    agent-3: 207.16666666666643
    agent-4: 207.16666666666643
    agent-5: 207.16666666666643
  policy_reward_mean:
    agent-0: 176.48499999999981
    agent-1: 176.48499999999981
    agent-2: 176.48499999999981
    agent-3: 176.48499999999981
    agent-4: 176.48499999999981
    agent-5: 176.48499999999981
  policy_reward_min:
    agent-0: 93.50000000000013
    agent-1: 93.50000000000013
    agent-2: 93.50000000000013
    agent-3: 93.50000000000013
    agent-4: 93.50000000000013
    agent-5: 93.50000000000013
  sampler_perf:
    mean_env_wait_ms: 24.370412114884303
    mean_inference_ms: 12.31855289332089
    mean_processing_ms: 50.89360383567763
  time_since_restore: 50863.2038705349
  time_this_iter_s: 130.48760890960693
  time_total_s: 54074.26755666733
  timestamp: 1637068455
  timesteps_since_restore: 37536000
  timesteps_this_iter: 96000
  timesteps_total: 39456000
  training_iteration: 411
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 36.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    411 |          54074.3 | 39456000 |  1058.91 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 37
    apples_agent-0_mean: 1.61
    apples_agent-0_min: 0
    apples_agent-1_max: 74
    apples_agent-1_mean: 21.27
    apples_agent-1_min: 0
    apples_agent-2_max: 240
    apples_agent-2_mean: 16.79
    apples_agent-2_min: 0
    apples_agent-3_max: 216
    apples_agent-3_mean: 54.0
    apples_agent-3_min: 23
    apples_agent-4_max: 38
    apples_agent-4_mean: 1.05
    apples_agent-4_min: 0
    apples_agent-5_max: 230
    apples_agent-5_mean: 103.22
    apples_agent-5_min: 50
    cleaning_beam_agent-0_max: 544
    cleaning_beam_agent-0_mean: 432.54
    cleaning_beam_agent-0_min: 252
    cleaning_beam_agent-1_max: 486
    cleaning_beam_agent-1_mean: 293.19
    cleaning_beam_agent-1_min: 162
    cleaning_beam_agent-2_max: 527
    cleaning_beam_agent-2_mean: 334.91
    cleaning_beam_agent-2_min: 119
    cleaning_beam_agent-3_max: 85
    cleaning_beam_agent-3_mean: 14.92
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 585
    cleaning_beam_agent-4_mean: 479.49
    cleaning_beam_agent-4_min: 341
    cleaning_beam_agent-5_max: 198
    cleaning_beam_agent-5_mean: 14.45
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-17-01
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1205.9999999999975
  episode_reward_mean: 1068.1799999999937
  episode_reward_min: 508.00000000001074
  episodes_this_iter: 96
  episodes_total: 39552
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20253.004
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9159650802612305
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017835597973316908
        model: {}
        policy_loss: -0.002974805422127247
        total_loss: -0.002256941981613636
        vf_explained_var: 0.04927392303943634
        vf_loss: 23.29960823059082
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1647160053253174
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018015984678640962
        model: {}
        policy_loss: -0.003836868330836296
        total_loss: -0.003466845490038395
        vf_explained_var: 0.01106967031955719
        vf_loss: 24.19925308227539
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.137459635734558
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012802055571228266
        model: {}
        policy_loss: -0.0034583252854645252
        total_loss: -0.003028921317309141
        vf_explained_var: 0.007453829050064087
        vf_loss: 24.313331604003906
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3338112235069275
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010704078013077378
        model: {}
        policy_loss: -0.0024050436913967133
        total_loss: -0.0008121272549033165
        vf_explained_var: 0.10745863616466522
        vf_loss: 21.804227828979492
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8798683881759644
        entropy_coeff: 0.0017600000137463212
        kl: 0.001937161316163838
        model: {}
        policy_loss: -0.003582295263186097
        total_loss: -0.0027606566436588764
        vf_explained_var: 0.03015582263469696
        vf_loss: 23.702068328857422
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4874374270439148
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012193634174764156
        model: {}
        policy_loss: -0.002552531659603119
        total_loss: -0.0012046676129102707
        vf_explained_var: 0.097353994846344
        vf_loss: 22.057540893554688
    load_time_ms: 16186.946
    num_steps_sampled: 39552000
    num_steps_trained: 39552000
    sample_time_ms: 97100.83
    update_time_ms: 20.944
  iterations_since_restore: 392
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.235593220338984
    ram_util_percent: 20.51016949152542
  pid: 4061
  policy_reward_max:
    agent-0: 200.9999999999999
    agent-1: 200.9999999999999
    agent-2: 200.9999999999999
    agent-3: 200.9999999999999
    agent-4: 200.9999999999999
    agent-5: 200.9999999999999
  policy_reward_mean:
    agent-0: 178.0299999999999
    agent-1: 178.0299999999999
    agent-2: 178.0299999999999
    agent-3: 178.0299999999999
    agent-4: 178.0299999999999
    agent-5: 178.0299999999999
  policy_reward_min:
    agent-0: 84.66666666666704
    agent-1: 84.66666666666704
    agent-2: 84.66666666666704
    agent-3: 84.66666666666704
    agent-4: 84.66666666666704
    agent-5: 84.66666666666704
  sampler_perf:
    mean_env_wait_ms: 24.37358290804955
    mean_inference_ms: 12.319553727128605
    mean_processing_ms: 50.896225573672965
  time_since_restore: 51028.77888131142
  time_this_iter_s: 165.57501077651978
  time_total_s: 54239.84256744385
  timestamp: 1637068621
  timesteps_since_restore: 37632000
  timesteps_this_iter: 96000
  timesteps_total: 39552000
  training_iteration: 412
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 38.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    412 |          54239.8 | 39552000 |  1068.18 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 42
    apples_agent-0_mean: 1.97
    apples_agent-0_min: 0
    apples_agent-1_max: 102
    apples_agent-1_mean: 22.14
    apples_agent-1_min: 0
    apples_agent-2_max: 194
    apples_agent-2_mean: 14.29
    apples_agent-2_min: 0
    apples_agent-3_max: 84
    apples_agent-3_mean: 50.9
    apples_agent-3_min: 29
    apples_agent-4_max: 97
    apples_agent-4_mean: 1.67
    apples_agent-4_min: 0
    apples_agent-5_max: 210
    apples_agent-5_mean: 101.54
    apples_agent-5_min: 12
    cleaning_beam_agent-0_max: 558
    cleaning_beam_agent-0_mean: 439.74
    cleaning_beam_agent-0_min: 332
    cleaning_beam_agent-1_max: 427
    cleaning_beam_agent-1_mean: 304.3
    cleaning_beam_agent-1_min: 166
    cleaning_beam_agent-2_max: 508
    cleaning_beam_agent-2_mean: 334.66
    cleaning_beam_agent-2_min: 118
    cleaning_beam_agent-3_max: 66
    cleaning_beam_agent-3_mean: 14.13
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 589
    cleaning_beam_agent-4_mean: 480.6
    cleaning_beam_agent-4_min: 358
    cleaning_beam_agent-5_max: 361
    cleaning_beam_agent-5_mean: 12.8
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-19-28
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1237.0000000000211
  episode_reward_mean: 1082.0999999999929
  episode_reward_min: 443.0000000000058
  episodes_this_iter: 96
  episodes_total: 39648
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20234.782
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9191119074821472
        entropy_coeff: 0.0017600000137463212
        kl: 0.001275732647627592
        model: {}
        policy_loss: -0.0029108095914125443
        total_loss: -0.0021606218069791794
        vf_explained_var: 0.01395384967327118
        vf_loss: 23.678253173828125
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1696572303771973
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014934116043150425
        model: {}
        policy_loss: -0.00357264606282115
        total_loss: -0.0032545537687838078
        vf_explained_var: 0.005526915192604065
        vf_loss: 23.766895294189453
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1460692882537842
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015442625153809786
        model: {}
        policy_loss: -0.003556248266249895
        total_loss: -0.0031554820016026497
        vf_explained_var: -0.005843713879585266
        vf_loss: 24.17848777770996
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.33884197473526
        entropy_coeff: 0.0017600000137463212
        kl: 0.00120077608153224
        model: {}
        policy_loss: -0.0020615062676370144
        total_loss: -0.000521810376085341
        vf_explained_var: 0.10285621881484985
        vf_loss: 21.360599517822266
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8761312365531921
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018522577593103051
        model: {}
        policy_loss: -0.004031028598546982
        total_loss: -0.003291493747383356
        vf_explained_var: 0.04530663788318634
        vf_loss: 22.81525421142578
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4831235408782959
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008622764144092798
        model: {}
        policy_loss: -0.002635256852954626
        total_loss: -0.0013437550514936447
        vf_explained_var: 0.10468505322933197
        vf_loss: 21.41800308227539
    load_time_ms: 17783.214
    num_steps_sampled: 39648000
    num_steps_trained: 39648000
    sample_time_ms: 97703.523
    update_time_ms: 21.313
  iterations_since_restore: 393
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.63492822966507
    ram_util_percent: 21.198564593301434
  pid: 4061
  policy_reward_max:
    agent-0: 206.16666666666623
    agent-1: 206.16666666666623
    agent-2: 206.16666666666623
    agent-3: 206.16666666666623
    agent-4: 206.16666666666623
    agent-5: 206.16666666666623
  policy_reward_mean:
    agent-0: 180.34999999999982
    agent-1: 180.34999999999982
    agent-2: 180.34999999999982
    agent-3: 180.34999999999982
    agent-4: 180.34999999999982
    agent-5: 180.34999999999982
  policy_reward_min:
    agent-0: 73.83333333333341
    agent-1: 73.83333333333341
    agent-2: 73.83333333333341
    agent-3: 73.83333333333341
    agent-4: 73.83333333333341
    agent-5: 73.83333333333341
  sampler_perf:
    mean_env_wait_ms: 24.377191195901574
    mean_inference_ms: 12.320519902189234
    mean_processing_ms: 50.89991479283776
  time_since_restore: 51175.624411821365
  time_this_iter_s: 146.84553050994873
  time_total_s: 54386.6880979538
  timestamp: 1637068768
  timesteps_since_restore: 37728000
  timesteps_this_iter: 96000
  timesteps_total: 39648000
  training_iteration: 413
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 36.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    413 |          54386.7 | 39648000 |   1082.1 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 29
    apples_agent-0_mean: 1.01
    apples_agent-0_min: 0
    apples_agent-1_max: 91
    apples_agent-1_mean: 19.55
    apples_agent-1_min: 0
    apples_agent-2_max: 154
    apples_agent-2_mean: 16.84
    apples_agent-2_min: 0
    apples_agent-3_max: 91
    apples_agent-3_mean: 53.45
    apples_agent-3_min: 26
    apples_agent-4_max: 190
    apples_agent-4_mean: 3.28
    apples_agent-4_min: 0
    apples_agent-5_max: 221
    apples_agent-5_mean: 99.0
    apples_agent-5_min: 59
    cleaning_beam_agent-0_max: 562
    cleaning_beam_agent-0_mean: 448.38
    cleaning_beam_agent-0_min: 313
    cleaning_beam_agent-1_max: 482
    cleaning_beam_agent-1_mean: 314.73
    cleaning_beam_agent-1_min: 156
    cleaning_beam_agent-2_max: 490
    cleaning_beam_agent-2_mean: 312.83
    cleaning_beam_agent-2_min: 138
    cleaning_beam_agent-3_max: 186
    cleaning_beam_agent-3_mean: 15.46
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 578
    cleaning_beam_agent-4_mean: 479.42
    cleaning_beam_agent-4_min: 372
    cleaning_beam_agent-5_max: 292
    cleaning_beam_agent-5_mean: 16.62
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-21-41
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1267.0000000000036
  episode_reward_mean: 1070.909999999995
  episode_reward_min: 482.0000000000129
  episodes_this_iter: 96
  episodes_total: 39744
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20234.945
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8994556665420532
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017739981412887573
        model: {}
        policy_loss: -0.0029003822710365057
        total_loss: -0.0021659829653799534
        vf_explained_var: 0.05209393799304962
        vf_loss: 23.17443084716797
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1656460762023926
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021053876262158155
        model: {}
        policy_loss: -0.0040454259142279625
        total_loss: -0.0037019287701696157
        vf_explained_var: 0.02016925811767578
        vf_loss: 23.95032501220703
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1536659002304077
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018227354157716036
        model: {}
        policy_loss: -0.00372995063662529
        total_loss: -0.0031903539784252644
        vf_explained_var: -0.04307660460472107
        vf_loss: 25.700477600097656
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3491905927658081
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008224482880905271
        model: {}
        policy_loss: -0.001993926940485835
        total_loss: -0.0004151705070398748
        vf_explained_var: 0.10210053622722626
        vf_loss: 21.933292388916016
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8770193457603455
        entropy_coeff: 0.0017600000137463212
        kl: 0.001839753007516265
        model: {}
        policy_loss: -0.003422623500227928
        total_loss: -0.0026940740644931793
        vf_explained_var: 0.0690678060054779
        vf_loss: 22.721036911010742
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49826115369796753
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010164107661694288
        model: {}
        policy_loss: -0.0026984545402228832
        total_loss: -0.0013693415094166994
        vf_explained_var: 0.0987958014011383
        vf_loss: 22.060504913330078
    load_time_ms: 18024.525
    num_steps_sampled: 39744000
    num_steps_trained: 39744000
    sample_time_ms: 97641.669
    update_time_ms: 21.809
  iterations_since_restore: 394
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.08
    ram_util_percent: 20.15157894736842
  pid: 4061
  policy_reward_max:
    agent-0: 211.16666666666686
    agent-1: 211.16666666666686
    agent-2: 211.16666666666686
    agent-3: 211.16666666666686
    agent-4: 211.16666666666686
    agent-5: 211.16666666666686
  policy_reward_mean:
    agent-0: 178.48499999999984
    agent-1: 178.48499999999984
    agent-2: 178.48499999999984
    agent-3: 178.48499999999984
    agent-4: 178.48499999999984
    agent-5: 178.48499999999984
  policy_reward_min:
    agent-0: 80.3333333333333
    agent-1: 80.3333333333333
    agent-2: 80.3333333333333
    agent-3: 80.3333333333333
    agent-4: 80.3333333333333
    agent-5: 80.3333333333333
  sampler_perf:
    mean_env_wait_ms: 24.380521400134896
    mean_inference_ms: 12.32143935368505
    mean_processing_ms: 50.90278951814435
  time_since_restore: 51308.72201132774
  time_this_iter_s: 133.09759950637817
  time_total_s: 54519.785697460175
  timestamp: 1637068901
  timesteps_since_restore: 37824000
  timesteps_this_iter: 96000
  timesteps_total: 39744000
  training_iteration: 414
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 38.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    414 |          54519.8 | 39744000 |  1070.91 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 40
    apples_agent-0_mean: 2.27
    apples_agent-0_min: 0
    apples_agent-1_max: 65
    apples_agent-1_mean: 16.04
    apples_agent-1_min: 0
    apples_agent-2_max: 116
    apples_agent-2_mean: 10.7
    apples_agent-2_min: 0
    apples_agent-3_max: 86
    apples_agent-3_mean: 52.48
    apples_agent-3_min: 26
    apples_agent-4_max: 84
    apples_agent-4_mean: 2.73
    apples_agent-4_min: 0
    apples_agent-5_max: 202
    apples_agent-5_mean: 100.41
    apples_agent-5_min: 36
    cleaning_beam_agent-0_max: 566
    cleaning_beam_agent-0_mean: 456.49
    cleaning_beam_agent-0_min: 332
    cleaning_beam_agent-1_max: 478
    cleaning_beam_agent-1_mean: 306.13
    cleaning_beam_agent-1_min: 104
    cleaning_beam_agent-2_max: 502
    cleaning_beam_agent-2_mean: 329.21
    cleaning_beam_agent-2_min: 168
    cleaning_beam_agent-3_max: 96
    cleaning_beam_agent-3_mean: 14.91
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 601
    cleaning_beam_agent-4_mean: 469.4
    cleaning_beam_agent-4_min: 358
    cleaning_beam_agent-5_max: 289
    cleaning_beam_agent-5_mean: 17.54
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-23-56
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1260.000000000006
  episode_reward_mean: 1049.9699999999912
  episode_reward_min: 265.99999999999585
  episodes_this_iter: 96
  episodes_total: 39840
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20233.931
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9010201692581177
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019518048502504826
        model: {}
        policy_loss: -0.003093724139034748
        total_loss: -0.002299318555742502
        vf_explained_var: 0.06025338172912598
        vf_loss: 23.8020076751709
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1618629693984985
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012617758475244045
        model: {}
        policy_loss: -0.003516959957778454
        total_loss: -0.003031289204955101
        vf_explained_var: 0.0018670707941055298
        vf_loss: 25.30553436279297
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.154978632926941
        entropy_coeff: 0.0017600000137463212
        kl: 0.001453595468774438
        model: {}
        policy_loss: -0.0035738167352974415
        total_loss: -0.0030465032905340195
        vf_explained_var: -0.010749861598014832
        vf_loss: 25.600784301757812
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35315370559692383
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007983410614542663
        model: {}
        policy_loss: -0.0022547580301761627
        total_loss: -0.0006720046512782574
        vf_explained_var: 0.1321202963590622
        vf_loss: 22.043052673339844
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8836716413497925
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013278158148750663
        model: {}
        policy_loss: -0.0034845140762627125
        total_loss: -0.0026840954087674618
        vf_explained_var: 0.07079541683197021
        vf_loss: 23.556804656982422
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.517589271068573
        entropy_coeff: 0.0017600000137463212
        kl: 0.000797355838585645
        model: {}
        policy_loss: -0.002669153967872262
        total_loss: -0.001351799350231886
        vf_explained_var: 0.12111589312553406
        vf_loss: 22.283153533935547
    load_time_ms: 18378.334
    num_steps_sampled: 39840000
    num_steps_trained: 39840000
    sample_time_ms: 97603.03
    update_time_ms: 22.506
  iterations_since_restore: 395
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.319791666666664
    ram_util_percent: 20.53125
  pid: 4061
  policy_reward_max:
    agent-0: 210.00000000000009
    agent-1: 210.00000000000009
    agent-2: 210.00000000000009
    agent-3: 210.00000000000009
    agent-4: 210.00000000000009
    agent-5: 210.00000000000009
  policy_reward_mean:
    agent-0: 174.9949999999998
    agent-1: 174.9949999999998
    agent-2: 174.9949999999998
    agent-3: 174.9949999999998
    agent-4: 174.9949999999998
    agent-5: 174.9949999999998
  policy_reward_min:
    agent-0: 44.33333333333327
    agent-1: 44.33333333333327
    agent-2: 44.33333333333327
    agent-3: 44.33333333333327
    agent-4: 44.33333333333327
    agent-5: 44.33333333333327
  sampler_perf:
    mean_env_wait_ms: 24.38412157545922
    mean_inference_ms: 12.322401599703774
    mean_processing_ms: 50.905670769846495
  time_since_restore: 51443.23370838165
  time_this_iter_s: 134.5116970539093
  time_total_s: 54654.297394514084
  timestamp: 1637069036
  timesteps_since_restore: 37920000
  timesteps_this_iter: 96000
  timesteps_total: 39840000
  training_iteration: 415
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 38.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    415 |          54654.3 | 39840000 |  1049.97 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 43
    apples_agent-0_mean: 1.51
    apples_agent-0_min: 0
    apples_agent-1_max: 91
    apples_agent-1_mean: 16.95
    apples_agent-1_min: 0
    apples_agent-2_max: 133
    apples_agent-2_mean: 15.31
    apples_agent-2_min: 0
    apples_agent-3_max: 126
    apples_agent-3_mean: 54.99
    apples_agent-3_min: 26
    apples_agent-4_max: 31
    apples_agent-4_mean: 0.44
    apples_agent-4_min: 0
    apples_agent-5_max: 162
    apples_agent-5_mean: 100.3
    apples_agent-5_min: 58
    cleaning_beam_agent-0_max: 569
    cleaning_beam_agent-0_mean: 445.85
    cleaning_beam_agent-0_min: 334
    cleaning_beam_agent-1_max: 506
    cleaning_beam_agent-1_mean: 298.41
    cleaning_beam_agent-1_min: 104
    cleaning_beam_agent-2_max: 500
    cleaning_beam_agent-2_mean: 316.38
    cleaning_beam_agent-2_min: 160
    cleaning_beam_agent-3_max: 76
    cleaning_beam_agent-3_mean: 13.59
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 603
    cleaning_beam_agent-4_mean: 482.14
    cleaning_beam_agent-4_min: 381
    cleaning_beam_agent-5_max: 93
    cleaning_beam_agent-5_mean: 12.54
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-26-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1233.9999999999995
  episode_reward_mean: 1077.429999999994
  episode_reward_min: 497.00000000001125
  episodes_this_iter: 96
  episodes_total: 39936
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20232.383
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9159576296806335
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018209510017186403
        model: {}
        policy_loss: -0.003223709762096405
        total_loss: -0.0025132978335022926
        vf_explained_var: 0.025303438305854797
        vf_loss: 23.224977493286133
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.16879403591156
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020069931633770466
        model: {}
        policy_loss: -0.0040135979652404785
        total_loss: -0.003693648613989353
        vf_explained_var: -0.0004626959562301636
        vf_loss: 23.770309448242188
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1605877876281738
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015771553153172135
        model: {}
        policy_loss: -0.0035417391918599606
        total_loss: -0.0031837504357099533
        vf_explained_var: -0.00392952561378479
        vf_loss: 24.006216049194336
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.34492966532707214
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014815953327342868
        model: {}
        policy_loss: -0.002099110744893551
        total_loss: -0.0005840519443154335
        vf_explained_var: 0.10524120926856995
        vf_loss: 21.22136688232422
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8674699068069458
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017056514043360949
        model: {}
        policy_loss: -0.0037528923712670803
        total_loss: -0.0029648886993527412
        vf_explained_var: 0.02400362491607666
        vf_loss: 23.147476196289062
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4891698658466339
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011749241966754198
        model: {}
        policy_loss: -0.0025235507637262344
        total_loss: -0.0012412830255925655
        vf_explained_var: 0.09613870084285736
        vf_loss: 21.43207359313965
    load_time_ms: 18445.523
    num_steps_sampled: 39936000
    num_steps_trained: 39936000
    sample_time_ms: 97662.052
    update_time_ms: 23.08
  iterations_since_restore: 396
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.961827956989247
    ram_util_percent: 20.404301075268815
  pid: 4061
  policy_reward_max:
    agent-0: 205.66666666666654
    agent-1: 205.66666666666654
    agent-2: 205.66666666666654
    agent-3: 205.66666666666654
    agent-4: 205.66666666666654
    agent-5: 205.66666666666654
  policy_reward_mean:
    agent-0: 179.57166666666654
    agent-1: 179.57166666666654
    agent-2: 179.57166666666654
    agent-3: 179.57166666666654
    agent-4: 179.57166666666654
    agent-5: 179.57166666666654
  policy_reward_min:
    agent-0: 82.83333333333348
    agent-1: 82.83333333333348
    agent-2: 82.83333333333348
    agent-3: 82.83333333333348
    agent-4: 82.83333333333348
    agent-5: 82.83333333333348
  sampler_perf:
    mean_env_wait_ms: 24.387343559934525
    mean_inference_ms: 12.323200136129362
    mean_processing_ms: 50.90920333768178
  time_since_restore: 51573.444996118546
  time_this_iter_s: 130.2112877368927
  time_total_s: 54784.50868225098
  timestamp: 1637069166
  timesteps_since_restore: 38016000
  timesteps_this_iter: 96000
  timesteps_total: 39936000
  training_iteration: 416
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 37.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    416 |          54784.5 | 39936000 |  1077.43 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 55
    apples_agent-0_mean: 2.49
    apples_agent-0_min: 0
    apples_agent-1_max: 111
    apples_agent-1_mean: 17.65
    apples_agent-1_min: 0
    apples_agent-2_max: 72
    apples_agent-2_mean: 12.55
    apples_agent-2_min: 0
    apples_agent-3_max: 98
    apples_agent-3_mean: 51.8
    apples_agent-3_min: 27
    apples_agent-4_max: 62
    apples_agent-4_mean: 1.52
    apples_agent-4_min: 0
    apples_agent-5_max: 191
    apples_agent-5_mean: 99.63
    apples_agent-5_min: 56
    cleaning_beam_agent-0_max: 563
    cleaning_beam_agent-0_mean: 447.5
    cleaning_beam_agent-0_min: 275
    cleaning_beam_agent-1_max: 454
    cleaning_beam_agent-1_mean: 307.35
    cleaning_beam_agent-1_min: 56
    cleaning_beam_agent-2_max: 468
    cleaning_beam_agent-2_mean: 319.18
    cleaning_beam_agent-2_min: 132
    cleaning_beam_agent-3_max: 54
    cleaning_beam_agent-3_mean: 15.63
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 600
    cleaning_beam_agent-4_mean: 494.62
    cleaning_beam_agent-4_min: 414
    cleaning_beam_agent-5_max: 121
    cleaning_beam_agent-5_mean: 13.7
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-28-18
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1241.0
  episode_reward_mean: 1066.1799999999937
  episode_reward_min: 565.0000000000051
  episodes_this_iter: 96
  episodes_total: 40032
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20219.771
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9081146717071533
        entropy_coeff: 0.0017600000137463212
        kl: 0.001273977104574442
        model: {}
        policy_loss: -0.0029200557619333267
        total_loss: -0.0021477574482560158
        vf_explained_var: 0.04129520058631897
        vf_loss: 23.705793380737305
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1743403673171997
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014257049188017845
        model: {}
        policy_loss: -0.003548827487975359
        total_loss: -0.0031910683028399944
        vf_explained_var: 0.015772148966789246
        vf_loss: 24.245996475219727
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1587986946105957
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017836987972259521
        model: {}
        policy_loss: -0.0036009824834764004
        total_loss: -0.0031755329109728336
        vf_explained_var: 0.003151416778564453
        vf_loss: 24.649364471435547
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3552612066268921
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008906753500923514
        model: {}
        policy_loss: -0.0021285363472998142
        total_loss: -0.0005921102128922939
        vf_explained_var: 0.12352579832077026
        vf_loss: 21.61688995361328
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8600102066993713
        entropy_coeff: 0.0017600000137463212
        kl: 0.001540355384349823
        model: {}
        policy_loss: -0.003452111966907978
        total_loss: -0.0025837374851107597
        vf_explained_var: 0.032837167382240295
        vf_loss: 23.819929122924805
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49887895584106445
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014894434716552496
        model: {}
        policy_loss: -0.0027552405372262
        total_loss: -0.0014880320522934198
        vf_explained_var: 0.12994836270809174
        vf_loss: 21.452367782592773
    load_time_ms: 18479.609
    num_steps_sampled: 40032000
    num_steps_trained: 40032000
    sample_time_ms: 97694.758
    update_time_ms: 22.628
  iterations_since_restore: 397
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.703208556149733
    ram_util_percent: 20.471122994652404
  pid: 4061
  policy_reward_max:
    agent-0: 206.8333333333339
    agent-1: 206.8333333333339
    agent-2: 206.8333333333339
    agent-3: 206.8333333333339
    agent-4: 206.8333333333339
    agent-5: 206.8333333333339
  policy_reward_mean:
    agent-0: 177.6966666666665
    agent-1: 177.6966666666665
    agent-2: 177.6966666666665
    agent-3: 177.6966666666665
    agent-4: 177.6966666666665
    agent-5: 177.6966666666665
  policy_reward_min:
    agent-0: 94.16666666666706
    agent-1: 94.16666666666706
    agent-2: 94.16666666666706
    agent-3: 94.16666666666706
    agent-4: 94.16666666666706
    agent-5: 94.16666666666706
  sampler_perf:
    mean_env_wait_ms: 24.391319490465552
    mean_inference_ms: 12.324141742711253
    mean_processing_ms: 50.91333572413467
  time_since_restore: 51704.9053299427
  time_this_iter_s: 131.46033382415771
  time_total_s: 54915.969016075134
  timestamp: 1637069298
  timesteps_since_restore: 38112000
  timesteps_this_iter: 96000
  timesteps_total: 40032000
  training_iteration: 417
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 38.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    417 |            54916 | 40032000 |  1066.18 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 66
    apples_agent-0_mean: 3.32
    apples_agent-0_min: 0
    apples_agent-1_max: 58
    apples_agent-1_mean: 12.53
    apples_agent-1_min: 0
    apples_agent-2_max: 110
    apples_agent-2_mean: 20.34
    apples_agent-2_min: 0
    apples_agent-3_max: 90
    apples_agent-3_mean: 53.5
    apples_agent-3_min: 32
    apples_agent-4_max: 62
    apples_agent-4_mean: 1.67
    apples_agent-4_min: 0
    apples_agent-5_max: 161
    apples_agent-5_mean: 98.64
    apples_agent-5_min: 10
    cleaning_beam_agent-0_max: 566
    cleaning_beam_agent-0_mean: 448.26
    cleaning_beam_agent-0_min: 256
    cleaning_beam_agent-1_max: 457
    cleaning_beam_agent-1_mean: 322.86
    cleaning_beam_agent-1_min: 167
    cleaning_beam_agent-2_max: 491
    cleaning_beam_agent-2_mean: 290.91
    cleaning_beam_agent-2_min: 115
    cleaning_beam_agent-3_max: 109
    cleaning_beam_agent-3_mean: 15.05
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 626
    cleaning_beam_agent-4_mean: 497.16
    cleaning_beam_agent-4_min: 341
    cleaning_beam_agent-5_max: 110
    cleaning_beam_agent-5_mean: 13.34
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 3
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-30-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1277.0000000000034
  episode_reward_mean: 1073.0299999999936
  episode_reward_min: 508.00000000001495
  episodes_this_iter: 96
  episodes_total: 40128
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20254.553
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9190911054611206
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012827017344534397
        model: {}
        policy_loss: -0.002829174045473337
        total_loss: -0.0021625570952892303
        vf_explained_var: 0.06739877164363861
        vf_loss: 22.842185974121094
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1775097846984863
        entropy_coeff: 0.0017600000137463212
        kl: 0.001447648974135518
        model: {}
        policy_loss: -0.003435581922531128
        total_loss: -0.0030423267744481564
        vf_explained_var: -0.015289664268493652
        vf_loss: 24.65673828125
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1576844453811646
        entropy_coeff: 0.0017600000137463212
        kl: 0.001332573127001524
        model: {}
        policy_loss: -0.0034523927606642246
        total_loss: -0.0030349022708833218
        vf_explained_var: -0.0005862265825271606
        vf_loss: 24.550155639648438
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3485262393951416
        entropy_coeff: 0.0017600000137463212
        kl: 0.000940744939725846
        model: {}
        policy_loss: -0.0019305013120174408
        total_loss: -0.000334116630256176
        vf_explained_var: 0.09089471399784088
        vf_loss: 22.097917556762695
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8605219721794128
        entropy_coeff: 0.0017600000137463212
        kl: 0.001831861911341548
        model: {}
        policy_loss: -0.003643339965492487
        total_loss: -0.0028481450863182545
        vf_explained_var: 0.0489327609539032
        vf_loss: 23.09714698791504
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5124677419662476
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009701285744085908
        model: {}
        policy_loss: -0.0029569007456302643
        total_loss: -0.0016823066398501396
        vf_explained_var: 0.10853719711303711
        vf_loss: 21.765390396118164
    load_time_ms: 18564.687
    num_steps_sampled: 40128000
    num_steps_trained: 40128000
    sample_time_ms: 97282.965
    update_time_ms: 21.647
  iterations_since_restore: 398
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.260655737704916
    ram_util_percent: 17.404371584699454
  pid: 4061
  policy_reward_max:
    agent-0: 212.83333333333346
    agent-1: 212.83333333333346
    agent-2: 212.83333333333346
    agent-3: 212.83333333333346
    agent-4: 212.83333333333346
    agent-5: 212.83333333333346
  policy_reward_mean:
    agent-0: 178.83833333333314
    agent-1: 178.83833333333314
    agent-2: 178.83833333333314
    agent-3: 178.83833333333314
    agent-4: 178.83833333333314
    agent-5: 178.83833333333314
  policy_reward_min:
    agent-0: 84.66666666666683
    agent-1: 84.66666666666683
    agent-2: 84.66666666666683
    agent-3: 84.66666666666683
    agent-4: 84.66666666666683
    agent-5: 84.66666666666683
  sampler_perf:
    mean_env_wait_ms: 24.394114607193636
    mean_inference_ms: 12.324368916613544
    mean_processing_ms: 50.91373145759205
  time_since_restore: 51833.312395334244
  time_this_iter_s: 128.40706539154053
  time_total_s: 55044.376081466675
  timestamp: 1637069426
  timesteps_since_restore: 38208000
  timesteps_this_iter: 96000
  timesteps_total: 40128000
  training_iteration: 418
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    418 |          55044.4 | 40128000 |  1073.03 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 77
    apples_agent-0_mean: 2.4
    apples_agent-0_min: 0
    apples_agent-1_max: 85
    apples_agent-1_mean: 17.31
    apples_agent-1_min: 0
    apples_agent-2_max: 250
    apples_agent-2_mean: 16.41
    apples_agent-2_min: 0
    apples_agent-3_max: 114
    apples_agent-3_mean: 51.01
    apples_agent-3_min: 32
    apples_agent-4_max: 53
    apples_agent-4_mean: 1.73
    apples_agent-4_min: 0
    apples_agent-5_max: 171
    apples_agent-5_mean: 99.9
    apples_agent-5_min: 45
    cleaning_beam_agent-0_max: 545
    cleaning_beam_agent-0_mean: 437.89
    cleaning_beam_agent-0_min: 240
    cleaning_beam_agent-1_max: 539
    cleaning_beam_agent-1_mean: 308.02
    cleaning_beam_agent-1_min: 153
    cleaning_beam_agent-2_max: 576
    cleaning_beam_agent-2_mean: 324.15
    cleaning_beam_agent-2_min: 139
    cleaning_beam_agent-3_max: 57
    cleaning_beam_agent-3_mean: 16.74
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 627
    cleaning_beam_agent-4_mean: 500.74
    cleaning_beam_agent-4_min: 355
    cleaning_beam_agent-5_max: 145
    cleaning_beam_agent-5_mean: 16.75
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-32-39
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1250.000000000004
  episode_reward_mean: 1056.3199999999927
  episode_reward_min: 418.00000000000693
  episodes_this_iter: 96
  episodes_total: 40224
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20230.466
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9371947646141052
        entropy_coeff: 0.0017600000137463212
        kl: 0.001984486822038889
        model: {}
        policy_loss: -0.003401512745767832
        total_loss: -0.0025854771956801414
        vf_explained_var: 0.06286929547786713
        vf_loss: 24.654998779296875
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1716079711914062
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013202311238273978
        model: {}
        policy_loss: -0.00338542927056551
        total_loss: -0.0028122132644057274
        vf_explained_var: -0.0017973631620407104
        vf_loss: 26.35245704650879
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1505454778671265
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018028023187071085
        model: {}
        policy_loss: -0.0033211903646588326
        total_loss: -0.0027545816265046597
        vf_explained_var: 0.014532849192619324
        vf_loss: 25.915708541870117
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3588692247867584
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006919319275766611
        model: {}
        policy_loss: -0.0024391652550548315
        total_loss: -0.0007878124015405774
        vf_explained_var: 0.13115398585796356
        vf_loss: 22.82962417602539
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8696889281272888
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015852557262405753
        model: {}
        policy_loss: -0.003711362136527896
        total_loss: -0.002766275079920888
        vf_explained_var: 0.0578874796628952
        vf_loss: 24.757394790649414
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5142666101455688
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009226242545992136
        model: {}
        policy_loss: -0.0030265478417277336
        total_loss: -0.0016297847032546997
        vf_explained_var: 0.12346908450126648
        vf_loss: 23.01874351501465
    load_time_ms: 18571.948
    num_steps_sampled: 40224000
    num_steps_trained: 40224000
    sample_time_ms: 97458.804
    update_time_ms: 21.738
  iterations_since_restore: 399
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.285714285714285
    ram_util_percent: 18.371957671957674
  pid: 4061
  policy_reward_max:
    agent-0: 208.33333333333366
    agent-1: 208.33333333333366
    agent-2: 208.33333333333366
    agent-3: 208.33333333333366
    agent-4: 208.33333333333366
    agent-5: 208.33333333333366
  policy_reward_mean:
    agent-0: 176.05333333333317
    agent-1: 176.05333333333317
    agent-2: 176.05333333333317
    agent-3: 176.05333333333317
    agent-4: 176.05333333333317
    agent-5: 176.05333333333317
  policy_reward_min:
    agent-0: 69.66666666666663
    agent-1: 69.66666666666663
    agent-2: 69.66666666666663
    agent-3: 69.66666666666663
    agent-4: 69.66666666666663
    agent-5: 69.66666666666663
  sampler_perf:
    mean_env_wait_ms: 24.398645791639243
    mean_inference_ms: 12.32562211822159
    mean_processing_ms: 50.91864082469766
  time_since_restore: 51965.53113436699
  time_this_iter_s: 132.21873903274536
  time_total_s: 55176.59482049942
  timestamp: 1637069559
  timesteps_since_restore: 38304000
  timesteps_this_iter: 96000
  timesteps_total: 40224000
  training_iteration: 419
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    419 |          55176.6 | 40224000 |  1056.32 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 37
    apples_agent-0_mean: 1.67
    apples_agent-0_min: 0
    apples_agent-1_max: 80
    apples_agent-1_mean: 19.25
    apples_agent-1_min: 0
    apples_agent-2_max: 122
    apples_agent-2_mean: 10.17
    apples_agent-2_min: 0
    apples_agent-3_max: 165
    apples_agent-3_mean: 54.31
    apples_agent-3_min: 22
    apples_agent-4_max: 70
    apples_agent-4_mean: 0.7
    apples_agent-4_min: 0
    apples_agent-5_max: 185
    apples_agent-5_mean: 101.41
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 536
    cleaning_beam_agent-0_mean: 436.37
    cleaning_beam_agent-0_min: 358
    cleaning_beam_agent-1_max: 451
    cleaning_beam_agent-1_mean: 308.66
    cleaning_beam_agent-1_min: 127
    cleaning_beam_agent-2_max: 576
    cleaning_beam_agent-2_mean: 337.86
    cleaning_beam_agent-2_min: 109
    cleaning_beam_agent-3_max: 44
    cleaning_beam_agent-3_mean: 12.93
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 589
    cleaning_beam_agent-4_mean: 496.79
    cleaning_beam_agent-4_min: 369
    cleaning_beam_agent-5_max: 490
    cleaning_beam_agent-5_mean: 23.01
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-34-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1238.0000000000005
  episode_reward_mean: 1085.4199999999928
  episode_reward_min: 499.0000000000051
  episodes_this_iter: 96
  episodes_total: 40320
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20254.887
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9315102696418762
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014270603423938155
        model: {}
        policy_loss: -0.0031568182166665792
        total_loss: -0.0025425555650144815
        vf_explained_var: 0.04056398570537567
        vf_loss: 22.53721046447754
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1786788702011108
        entropy_coeff: 0.0017600000137463212
        kl: 0.001270540989935398
        model: {}
        policy_loss: -0.003474883735179901
        total_loss: -0.0032947417348623276
        vf_explained_var: 0.031104177236557007
        vf_loss: 22.546157836914062
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1493545770645142
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015507638454437256
        model: {}
        policy_loss: -0.003626032965257764
        total_loss: -0.003269700100645423
        vf_explained_var: -0.02284151315689087
        vf_loss: 23.79197883605957
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3325033187866211
        entropy_coeff: 0.0017600000137463212
        kl: 0.00117519311606884
        model: {}
        policy_loss: -0.001978966873139143
        total_loss: -0.000413648784160614
        vf_explained_var: 0.0725378692150116
        vf_loss: 21.505247116088867
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8712598085403442
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019714385271072388
        model: {}
        policy_loss: -0.0035478798672556877
        total_loss: -0.0028597768396139145
        vf_explained_var: 0.046319589018821716
        vf_loss: 22.215208053588867
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5016898512840271
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007354708504863083
        model: {}
        policy_loss: -0.0025672726333141327
        total_loss: -0.001310007181018591
        vf_explained_var: 0.08018097281455994
        vf_loss: 21.402420043945312
    load_time_ms: 18589.02
    num_steps_sampled: 40320000
    num_steps_trained: 40320000
    sample_time_ms: 97485.728
    update_time_ms: 21.05
  iterations_since_restore: 400
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.662765957446805
    ram_util_percent: 19.53244680851064
  pid: 4061
  policy_reward_max:
    agent-0: 206.33333333333331
    agent-1: 206.33333333333331
    agent-2: 206.33333333333331
    agent-3: 206.33333333333331
    agent-4: 206.33333333333331
    agent-5: 206.33333333333331
  policy_reward_mean:
    agent-0: 180.90333333333314
    agent-1: 180.90333333333314
    agent-2: 180.90333333333314
    agent-3: 180.90333333333314
    agent-4: 180.90333333333314
    agent-5: 180.90333333333314
  policy_reward_min:
    agent-0: 83.16666666666667
    agent-1: 83.16666666666667
    agent-2: 83.16666666666667
    agent-3: 83.16666666666667
    agent-4: 83.16666666666667
    agent-5: 83.16666666666667
  sampler_perf:
    mean_env_wait_ms: 24.402266716776875
    mean_inference_ms: 12.3265171052403
    mean_processing_ms: 50.921518640917604
  time_since_restore: 52097.195038080215
  time_this_iter_s: 131.66390371322632
  time_total_s: 55308.25872421265
  timestamp: 1637069691
  timesteps_since_restore: 38400000
  timesteps_this_iter: 96000
  timesteps_total: 40320000
  training_iteration: 420
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 37.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    420 |          55308.3 | 40320000 |  1085.42 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 38
    apples_agent-0_mean: 2.76
    apples_agent-0_min: 0
    apples_agent-1_max: 88
    apples_agent-1_mean: 20.04
    apples_agent-1_min: 0
    apples_agent-2_max: 109
    apples_agent-2_mean: 12.01
    apples_agent-2_min: 0
    apples_agent-3_max: 126
    apples_agent-3_mean: 56.66
    apples_agent-3_min: 28
    apples_agent-4_max: 39
    apples_agent-4_mean: 0.45
    apples_agent-4_min: 0
    apples_agent-5_max: 164
    apples_agent-5_mean: 100.54
    apples_agent-5_min: 46
    cleaning_beam_agent-0_max: 528
    cleaning_beam_agent-0_mean: 415.02
    cleaning_beam_agent-0_min: 318
    cleaning_beam_agent-1_max: 511
    cleaning_beam_agent-1_mean: 317.06
    cleaning_beam_agent-1_min: 127
    cleaning_beam_agent-2_max: 465
    cleaning_beam_agent-2_mean: 319.14
    cleaning_beam_agent-2_min: 150
    cleaning_beam_agent-3_max: 54
    cleaning_beam_agent-3_mean: 14.93
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 592
    cleaning_beam_agent-4_mean: 489.1
    cleaning_beam_agent-4_min: 394
    cleaning_beam_agent-5_max: 134
    cleaning_beam_agent-5_mean: 14.28
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-37-02
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1214.9999999999927
  episode_reward_mean: 1069.8999999999928
  episode_reward_min: 549.0000000000001
  episodes_this_iter: 96
  episodes_total: 40416
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20246.905
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9362465143203735
        entropy_coeff: 0.0017600000137463212
        kl: 0.001557668554596603
        model: {}
        policy_loss: -0.002788000740110874
        total_loss: -0.00208474975079298
        vf_explained_var: 0.026885420083999634
        vf_loss: 23.510486602783203
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1743417978286743
        entropy_coeff: 0.0017600000137463212
        kl: 0.001305443700402975
        model: {}
        policy_loss: -0.003502500243484974
        total_loss: -0.0031799315474927425
        vf_explained_var: 0.0035753995180130005
        vf_loss: 23.894084930419922
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1565101146697998
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010541153606027365
        model: {}
        policy_loss: -0.003106830408796668
        total_loss: -0.002755028661340475
        vf_explained_var: 0.006739363074302673
        vf_loss: 23.87261390686035
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3506913483142853
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008602965972386301
        model: {}
        policy_loss: -0.0021947273053228855
        total_loss: -0.0006772088818252087
        vf_explained_var: 0.11078079044818878
        vf_loss: 21.34734344482422
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8731368184089661
        entropy_coeff: 0.0017600000137463212
        kl: 0.001839475822634995
        model: {}
        policy_loss: -0.0034140339121222496
        total_loss: -0.002613483462482691
        vf_explained_var: 0.02689981460571289
        vf_loss: 23.372713088989258
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.498593807220459
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016990252770483494
        model: {}
        policy_loss: -0.0027556726709008217
        total_loss: -0.0014603491872549057
        vf_explained_var: 0.09394818544387817
        vf_loss: 21.728487014770508
    load_time_ms: 18631.884
    num_steps_sampled: 40416000
    num_steps_trained: 40416000
    sample_time_ms: 97445.97
    update_time_ms: 21.184
  iterations_since_restore: 401
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.670588235294115
    ram_util_percent: 18.528877005347592
  pid: 4061
  policy_reward_max:
    agent-0: 202.50000000000014
    agent-1: 202.50000000000014
    agent-2: 202.50000000000014
    agent-3: 202.50000000000014
    agent-4: 202.50000000000014
    agent-5: 202.50000000000014
  policy_reward_mean:
    agent-0: 178.31666666666658
    agent-1: 178.31666666666658
    agent-2: 178.31666666666658
    agent-3: 178.31666666666658
    agent-4: 178.31666666666658
    agent-5: 178.31666666666658
  policy_reward_min:
    agent-0: 91.50000000000016
    agent-1: 91.50000000000016
    agent-2: 91.50000000000016
    agent-3: 91.50000000000016
    agent-4: 91.50000000000016
    agent-5: 91.50000000000016
  sampler_perf:
    mean_env_wait_ms: 24.405486779093952
    mean_inference_ms: 12.327245732106059
    mean_processing_ms: 50.92408658018876
  time_since_restore: 52227.64549255371
  time_this_iter_s: 130.45045447349548
  time_total_s: 55438.70917868614
  timestamp: 1637069822
  timesteps_since_restore: 38496000
  timesteps_this_iter: 96000
  timesteps_total: 40416000
  training_iteration: 421
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    421 |          55438.7 | 40416000 |   1069.9 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 103
    apples_agent-0_mean: 2.48
    apples_agent-0_min: 0
    apples_agent-1_max: 87
    apples_agent-1_mean: 17.03
    apples_agent-1_min: 0
    apples_agent-2_max: 84
    apples_agent-2_mean: 10.56
    apples_agent-2_min: 0
    apples_agent-3_max: 126
    apples_agent-3_mean: 55.76
    apples_agent-3_min: 25
    apples_agent-4_max: 20
    apples_agent-4_mean: 0.24
    apples_agent-4_min: 0
    apples_agent-5_max: 164
    apples_agent-5_mean: 99.48
    apples_agent-5_min: 57
    cleaning_beam_agent-0_max: 528
    cleaning_beam_agent-0_mean: 426.93
    cleaning_beam_agent-0_min: 333
    cleaning_beam_agent-1_max: 553
    cleaning_beam_agent-1_mean: 308.35
    cleaning_beam_agent-1_min: 159
    cleaning_beam_agent-2_max: 521
    cleaning_beam_agent-2_mean: 318.36
    cleaning_beam_agent-2_min: 143
    cleaning_beam_agent-3_max: 83
    cleaning_beam_agent-3_mean: 16.22
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 585
    cleaning_beam_agent-4_mean: 484.48
    cleaning_beam_agent-4_min: 330
    cleaning_beam_agent-5_max: 123
    cleaning_beam_agent-5_mean: 14.37
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-39-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1212.99999999999
  episode_reward_mean: 1093.0299999999936
  episode_reward_min: 624.9999999999911
  episodes_this_iter: 96
  episodes_total: 40512
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20273.817
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9417036175727844
        entropy_coeff: 0.0017600000137463212
        kl: 0.001590602332726121
        model: {}
        policy_loss: -0.0028558371122926474
        total_loss: -0.002267751842737198
        vf_explained_var: 0.025182455778121948
        vf_loss: 22.45482635498047
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.172135829925537
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014692128170281649
        model: {}
        policy_loss: -0.003487157868221402
        total_loss: -0.003303154371678829
        vf_explained_var: 0.006614223122596741
        vf_loss: 22.469635009765625
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1448252201080322
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018974908161908388
        model: {}
        policy_loss: -0.0034319590777158737
        total_loss: -0.0031769892666488886
        vf_explained_var: -0.002533823251724243
        vf_loss: 22.698623657226562
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.33985549211502075
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007833834970369935
        model: {}
        policy_loss: -0.0020104222930967808
        total_loss: -0.000505028641782701
        vf_explained_var: 0.06631647050380707
        vf_loss: 21.035388946533203
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8834095001220703
        entropy_coeff: 0.0017600000137463212
        kl: 0.002278480678796768
        model: {}
        policy_loss: -0.0037234770134091377
        total_loss: -0.0030816085636615753
        vf_explained_var: 0.03231737017631531
        vf_loss: 21.966686248779297
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4707239866256714
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007899737684056163
        model: {}
        policy_loss: -0.0025461858604103327
        total_loss: -0.0013015634613111615
        vf_explained_var: 0.08221253752708435
        vf_loss: 20.730953216552734
    load_time_ms: 16555.277
    num_steps_sampled: 40512000
    num_steps_trained: 40512000
    sample_time_ms: 95591.147
    update_time_ms: 21.143
  iterations_since_restore: 402
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.292265193370167
    ram_util_percent: 15.356906077348066
  pid: 4061
  policy_reward_max:
    agent-0: 202.16666666666657
    agent-1: 202.16666666666657
    agent-2: 202.16666666666657
    agent-3: 202.16666666666657
    agent-4: 202.16666666666657
    agent-5: 202.16666666666657
  policy_reward_mean:
    agent-0: 182.1716666666665
    agent-1: 182.1716666666665
    agent-2: 182.1716666666665
    agent-3: 182.1716666666665
    agent-4: 182.1716666666665
    agent-5: 182.1716666666665
  policy_reward_min:
    agent-0: 104.16666666666715
    agent-1: 104.16666666666715
    agent-2: 104.16666666666715
    agent-3: 104.16666666666715
    agent-4: 104.16666666666715
    agent-5: 104.16666666666715
  sampler_perf:
    mean_env_wait_ms: 24.40669499555644
    mean_inference_ms: 12.326715267207323
    mean_processing_ms: 50.92284237864527
  time_since_restore: 52354.092732429504
  time_this_iter_s: 126.44723987579346
  time_total_s: 55565.156418561935
  timestamp: 1637069949
  timesteps_since_restore: 38592000
  timesteps_this_iter: 96000
  timesteps_total: 40512000
  training_iteration: 422
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 28.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    422 |          55565.2 | 40512000 |  1093.03 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 44
    apples_agent-0_mean: 1.89
    apples_agent-0_min: 0
    apples_agent-1_max: 109
    apples_agent-1_mean: 18.21
    apples_agent-1_min: 0
    apples_agent-2_max: 106
    apples_agent-2_mean: 12.8
    apples_agent-2_min: 0
    apples_agent-3_max: 109
    apples_agent-3_mean: 55.1
    apples_agent-3_min: 28
    apples_agent-4_max: 80
    apples_agent-4_mean: 1.6
    apples_agent-4_min: 0
    apples_agent-5_max: 201
    apples_agent-5_mean: 104.03
    apples_agent-5_min: 30
    cleaning_beam_agent-0_max: 512
    cleaning_beam_agent-0_mean: 418.75
    cleaning_beam_agent-0_min: 247
    cleaning_beam_agent-1_max: 553
    cleaning_beam_agent-1_mean: 315.44
    cleaning_beam_agent-1_min: 140
    cleaning_beam_agent-2_max: 486
    cleaning_beam_agent-2_mean: 320.53
    cleaning_beam_agent-2_min: 138
    cleaning_beam_agent-3_max: 126
    cleaning_beam_agent-3_mean: 17.15
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 597
    cleaning_beam_agent-4_mean: 471.97
    cleaning_beam_agent-4_min: 281
    cleaning_beam_agent-5_max: 92
    cleaning_beam_agent-5_mean: 12.87
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-41-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1236.999999999995
  episode_reward_mean: 1068.9899999999916
  episode_reward_min: 477.0000000000067
  episodes_this_iter: 96
  episodes_total: 40608
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20300.986
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9502882957458496
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015967024955898523
        model: {}
        policy_loss: -0.0033837100490927696
        total_loss: -0.002758149290457368
        vf_explained_var: 0.008269175887107849
        vf_loss: 22.980674743652344
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1713473796844482
        entropy_coeff: 0.0017600000137463212
        kl: 0.001647443976253271
        model: {}
        policy_loss: -0.0036919135600328445
        total_loss: -0.003435875289142132
        vf_explained_var: -0.0015011727809906006
        vf_loss: 23.17612075805664
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1441915035247803
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012714897748082876
        model: {}
        policy_loss: -0.0031112032011151314
        total_loss: -0.002818708773702383
        vf_explained_var: 0.003844216465950012
        vf_loss: 23.06267547607422
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35429757833480835
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012752664042636752
        model: {}
        policy_loss: -0.002219440881162882
        total_loss: -0.000740664079785347
        vf_explained_var: 0.09224240481853485
        vf_loss: 21.023418426513672
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8815358877182007
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015775340143591166
        model: {}
        policy_loss: -0.0034975616727024317
        total_loss: -0.002817588159814477
        vf_explained_var: 0.036571577191352844
        vf_loss: 22.314754486083984
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4871457815170288
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009521888568997383
        model: {}
        policy_loss: -0.002691226080060005
        total_loss: -0.0014349971897900105
        vf_explained_var: 0.08735319972038269
        vf_loss: 21.136058807373047
    load_time_ms: 15007.464
    num_steps_sampled: 40608000
    num_steps_trained: 40608000
    sample_time_ms: 95375.454
    update_time_ms: 20.771
  iterations_since_restore: 403
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.73695652173913
    ram_util_percent: 17.121195652173913
  pid: 4061
  policy_reward_max:
    agent-0: 206.16666666666643
    agent-1: 206.16666666666643
    agent-2: 206.16666666666643
    agent-3: 206.16666666666643
    agent-4: 206.16666666666643
    agent-5: 206.16666666666643
  policy_reward_mean:
    agent-0: 178.16499999999985
    agent-1: 178.16499999999985
    agent-2: 178.16499999999985
    agent-3: 178.16499999999985
    agent-4: 178.16499999999985
    agent-5: 178.16499999999985
  policy_reward_min:
    agent-0: 79.49999999999993
    agent-1: 79.49999999999993
    agent-2: 79.49999999999993
    agent-3: 79.49999999999993
    agent-4: 79.49999999999993
    agent-5: 79.49999999999993
  sampler_perf:
    mean_env_wait_ms: 24.40897064939855
    mean_inference_ms: 12.327297334366454
    mean_processing_ms: 50.924207162028296
  time_since_restore: 52483.5845644474
  time_this_iter_s: 129.49183201789856
  time_total_s: 55694.648250579834
  timestamp: 1637070079
  timesteps_since_restore: 38688000
  timesteps_this_iter: 96000
  timesteps_total: 40608000
  training_iteration: 423
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    423 |          55694.6 | 40608000 |  1068.99 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 44
    apples_agent-0_mean: 2.45
    apples_agent-0_min: 0
    apples_agent-1_max: 76
    apples_agent-1_mean: 17.32
    apples_agent-1_min: 0
    apples_agent-2_max: 218
    apples_agent-2_mean: 12.48
    apples_agent-2_min: 0
    apples_agent-3_max: 170
    apples_agent-3_mean: 56.54
    apples_agent-3_min: 29
    apples_agent-4_max: 95
    apples_agent-4_mean: 2.25
    apples_agent-4_min: 0
    apples_agent-5_max: 185
    apples_agent-5_mean: 99.2
    apples_agent-5_min: 43
    cleaning_beam_agent-0_max: 578
    cleaning_beam_agent-0_mean: 432.31
    cleaning_beam_agent-0_min: 282
    cleaning_beam_agent-1_max: 493
    cleaning_beam_agent-1_mean: 310.21
    cleaning_beam_agent-1_min: 147
    cleaning_beam_agent-2_max: 557
    cleaning_beam_agent-2_mean: 328.5
    cleaning_beam_agent-2_min: 138
    cleaning_beam_agent-3_max: 100
    cleaning_beam_agent-3_mean: 16.59
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 605
    cleaning_beam_agent-4_mean: 481.36
    cleaning_beam_agent-4_min: 379
    cleaning_beam_agent-5_max: 150
    cleaning_beam_agent-5_mean: 14.88
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-46-41
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1234.9999999999823
  episode_reward_mean: 1070.1399999999935
  episode_reward_min: 579.000000000004
  episodes_this_iter: 96
  episodes_total: 40704
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20307.265
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9354792833328247
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015588442329317331
        model: {}
        policy_loss: -0.0030001734849065542
        total_loss: -0.0023408648557960987
        vf_explained_var: 0.06551668047904968
        vf_loss: 23.057571411132812
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1788537502288818
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022643376141786575
        model: {}
        policy_loss: -0.004051045514643192
        total_loss: -0.003671238897368312
        vf_explained_var: 0.0039905160665512085
        vf_loss: 24.54591178894043
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1315510272979736
        entropy_coeff: 0.0017600000137463212
        kl: 0.00212981179356575
        model: {}
        policy_loss: -0.0034585553221404552
        total_loss: -0.00299077108502388
        vf_explained_var: 0.0015942752361297607
        vf_loss: 24.59317970275879
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35318028926849365
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009587371023371816
        model: {}
        policy_loss: -0.0019926042295992374
        total_loss: -0.00042074848897755146
        vf_explained_var: 0.1097174733877182
        vf_loss: 21.93455696105957
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8775589466094971
        entropy_coeff: 0.0017600000137463212
        kl: 0.002409125678241253
        model: {}
        policy_loss: -0.0038990769535303116
        total_loss: -0.003103043884038925
        vf_explained_var: 0.05208112299442291
        vf_loss: 23.40530014038086
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4947412312030792
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005416489439085126
        model: {}
        policy_loss: -0.0024788551963865757
        total_loss: -0.00115688843652606
        vf_explained_var: 0.11004558205604553
        vf_loss: 21.927148818969727
    load_time_ms: 20772.383
    num_steps_sampled: 40704000
    num_steps_trained: 40704000
    sample_time_ms: 108548.196
    update_time_ms: 20.6
  iterations_since_restore: 404
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 10.020869565217392
    ram_util_percent: 29.10478260869565
  pid: 4061
  policy_reward_max:
    agent-0: 205.8333333333334
    agent-1: 205.8333333333334
    agent-2: 205.8333333333334
    agent-3: 205.8333333333334
    agent-4: 205.8333333333334
    agent-5: 205.8333333333334
  policy_reward_mean:
    agent-0: 178.35666666666657
    agent-1: 178.35666666666657
    agent-2: 178.35666666666657
    agent-3: 178.35666666666657
    agent-4: 178.35666666666657
    agent-5: 178.35666666666657
  policy_reward_min:
    agent-0: 96.50000000000031
    agent-1: 96.50000000000031
    agent-2: 96.50000000000031
    agent-3: 96.50000000000031
    agent-4: 96.50000000000031
    agent-5: 96.50000000000031
  sampler_perf:
    mean_env_wait_ms: 24.411689434990368
    mean_inference_ms: 12.327785896118847
    mean_processing_ms: 50.92535321772892
  time_since_restore: 52806.154168605804
  time_this_iter_s: 322.5696041584015
  time_total_s: 56017.217854738235
  timestamp: 1637070401
  timesteps_since_restore: 38784000
  timesteps_this_iter: 96000
  timesteps_total: 40704000
  training_iteration: 424
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 48.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    424 |          56017.2 | 40704000 |  1070.14 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 50
    apples_agent-0_mean: 2.16
    apples_agent-0_min: 0
    apples_agent-1_max: 64
    apples_agent-1_mean: 15.96
    apples_agent-1_min: 0
    apples_agent-2_max: 159
    apples_agent-2_mean: 16.56
    apples_agent-2_min: 0
    apples_agent-3_max: 143
    apples_agent-3_mean: 54.46
    apples_agent-3_min: 23
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 141
    apples_agent-5_mean: 95.86
    apples_agent-5_min: 34
    cleaning_beam_agent-0_max: 529
    cleaning_beam_agent-0_mean: 422.37
    cleaning_beam_agent-0_min: 282
    cleaning_beam_agent-1_max: 490
    cleaning_beam_agent-1_mean: 310.14
    cleaning_beam_agent-1_min: 165
    cleaning_beam_agent-2_max: 546
    cleaning_beam_agent-2_mean: 320.86
    cleaning_beam_agent-2_min: 157
    cleaning_beam_agent-3_max: 62
    cleaning_beam_agent-3_mean: 15.9
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 599
    cleaning_beam_agent-4_mean: 474.42
    cleaning_beam_agent-4_min: 366
    cleaning_beam_agent-5_max: 137
    cleaning_beam_agent-5_mean: 16.37
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-48-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1256.9999999999973
  episode_reward_mean: 1067.3999999999928
  episode_reward_min: 436.0000000000043
  episodes_this_iter: 96
  episodes_total: 40800
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20296.137
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9428533911705017
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015623207436874509
        model: {}
        policy_loss: -0.0031030382961034775
        total_loss: -0.0025077280588448048
        vf_explained_var: 0.03227818012237549
        vf_loss: 22.547321319580078
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.163236379623413
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018187880050390959
        model: {}
        policy_loss: -0.00397256575524807
        total_loss: -0.0037407074123620987
        vf_explained_var: 0.019280865788459778
        vf_loss: 22.79155731201172
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.142148733139038
        entropy_coeff: 0.0017600000137463212
        kl: 0.001149649266153574
        model: {}
        policy_loss: -0.003280875738710165
        total_loss: -0.002874998841434717
        vf_explained_var: -0.0384613573551178
        vf_loss: 24.160547256469727
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3521314859390259
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007870845729485154
        model: {}
        policy_loss: -0.002004563808441162
        total_loss: -0.0005408907309174538
        vf_explained_var: 0.10361027717590332
        vf_loss: 20.83422088623047
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8794893622398376
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015912659000605345
        model: {}
        policy_loss: -0.0035021293442696333
        total_loss: -0.0028013631235808134
        vf_explained_var: 0.032343342900276184
        vf_loss: 22.486675262451172
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48958268761634827
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013290474889799953
        model: {}
        policy_loss: -0.0026385686360299587
        total_loss: -0.0013335919938981533
        vf_explained_var: 0.0683659017086029
        vf_loss: 21.66641616821289
    load_time_ms: 20560.267
    num_steps_sampled: 40800000
    num_steps_trained: 40800000
    sample_time_ms: 108238.679
    update_time_ms: 20.14
  iterations_since_restore: 405
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.633695652173916
    ram_util_percent: 19.347826086956523
  pid: 4061
  policy_reward_max:
    agent-0: 209.4999999999998
    agent-1: 209.4999999999998
    agent-2: 209.4999999999998
    agent-3: 209.4999999999998
    agent-4: 209.4999999999998
    agent-5: 209.4999999999998
  policy_reward_mean:
    agent-0: 177.8999999999998
    agent-1: 177.8999999999998
    agent-2: 177.8999999999998
    agent-3: 177.8999999999998
    agent-4: 177.8999999999998
    agent-5: 177.8999999999998
  policy_reward_min:
    agent-0: 72.66666666666649
    agent-1: 72.66666666666649
    agent-2: 72.66666666666649
    agent-3: 72.66666666666649
    agent-4: 72.66666666666649
    agent-5: 72.66666666666649
  sampler_perf:
    mean_env_wait_ms: 24.414155517288794
    mean_inference_ms: 12.328247718305
    mean_processing_ms: 50.927099172434275
  time_since_restore: 52935.35921525955
  time_this_iter_s: 129.20504665374756
  time_total_s: 56146.42290139198
  timestamp: 1637070531
  timesteps_since_restore: 38880000
  timesteps_this_iter: 96000
  timesteps_total: 40800000
  training_iteration: 425
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 33.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    425 |          56146.4 | 40800000 |   1067.4 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 33
    apples_agent-0_mean: 1.29
    apples_agent-0_min: 0
    apples_agent-1_max: 77
    apples_agent-1_mean: 19.68
    apples_agent-1_min: 0
    apples_agent-2_max: 211
    apples_agent-2_mean: 15.68
    apples_agent-2_min: 0
    apples_agent-3_max: 97
    apples_agent-3_mean: 55.54
    apples_agent-3_min: 28
    apples_agent-4_max: 31
    apples_agent-4_mean: 0.78
    apples_agent-4_min: 0
    apples_agent-5_max: 160
    apples_agent-5_mean: 99.79
    apples_agent-5_min: 55
    cleaning_beam_agent-0_max: 499
    cleaning_beam_agent-0_mean: 403.59
    cleaning_beam_agent-0_min: 293
    cleaning_beam_agent-1_max: 471
    cleaning_beam_agent-1_mean: 314.72
    cleaning_beam_agent-1_min: 160
    cleaning_beam_agent-2_max: 550
    cleaning_beam_agent-2_mean: 335.79
    cleaning_beam_agent-2_min: 164
    cleaning_beam_agent-3_max: 212
    cleaning_beam_agent-3_mean: 18.84
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 591
    cleaning_beam_agent-4_mean: 466.12
    cleaning_beam_agent-4_min: 363
    cleaning_beam_agent-5_max: 102
    cleaning_beam_agent-5_mean: 12.88
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-50-56
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1245.0000000000057
  episode_reward_mean: 1066.6899999999912
  episode_reward_min: 580.9999999999994
  episodes_this_iter: 96
  episodes_total: 40896
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20326.298
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9474343061447144
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014063384151086211
        model: {}
        policy_loss: -0.003263020422309637
        total_loss: -0.002695433096960187
        vf_explained_var: 0.03304708003997803
        vf_loss: 22.3507080078125
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1720603704452515
        entropy_coeff: 0.0017600000137463212
        kl: 0.00135811825748533
        model: {}
        policy_loss: -0.003393100807443261
        total_loss: -0.003121898276731372
        vf_explained_var: -0.013766765594482422
        vf_loss: 23.34029197692871
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.133784294128418
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014939132379367948
        model: {}
        policy_loss: -0.003325425321236253
        total_loss: -0.002990422770380974
        vf_explained_var: -0.011636972427368164
        vf_loss: 23.304636001586914
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35611817240715027
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010284874588251114
        model: {}
        policy_loss: -0.002205869648605585
        total_loss: -0.0007807551883161068
        vf_explained_var: 0.1079658716917038
        vf_loss: 20.518829345703125
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8696441054344177
        entropy_coeff: 0.0017600000137463212
        kl: 0.002117368159815669
        model: {}
        policy_loss: -0.003713213838636875
        total_loss: -0.0029988004826009274
        vf_explained_var: 0.025397345423698425
        vf_loss: 22.449886322021484
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4887431859970093
        entropy_coeff: 0.0017600000137463212
        kl: 0.000677278614602983
        model: {}
        policy_loss: -0.0026682871393859386
        total_loss: -0.001402548048645258
        vf_explained_var: 0.07593756914138794
        vf_loss: 21.259286880493164
    load_time_ms: 20474.017
    num_steps_sampled: 40896000
    num_steps_trained: 40896000
    sample_time_ms: 107761.472
    update_time_ms: 19.586
  iterations_since_restore: 406
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.879775280898876
    ram_util_percent: 19.070224719101123
  pid: 4061
  policy_reward_max:
    agent-0: 207.49999999999997
    agent-1: 207.49999999999997
    agent-2: 207.49999999999997
    agent-3: 207.49999999999997
    agent-4: 207.49999999999997
    agent-5: 207.49999999999997
  policy_reward_mean:
    agent-0: 177.78166666666652
    agent-1: 177.78166666666652
    agent-2: 177.78166666666652
    agent-3: 177.78166666666652
    agent-4: 177.78166666666652
    agent-5: 177.78166666666652
  policy_reward_min:
    agent-0: 96.8333333333338
    agent-1: 96.8333333333338
    agent-2: 96.8333333333338
    agent-3: 96.8333333333338
    agent-4: 96.8333333333338
    agent-5: 96.8333333333338
  sampler_perf:
    mean_env_wait_ms: 24.416123305961147
    mean_inference_ms: 12.328511207480904
    mean_processing_ms: 50.927622415781386
  time_since_restore: 53060.30418086052
  time_this_iter_s: 124.94496560096741
  time_total_s: 56271.36786699295
  timestamp: 1637070656
  timesteps_since_restore: 38976000
  timesteps_this_iter: 96000
  timesteps_total: 40896000
  training_iteration: 426
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 35.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    426 |          56271.4 | 40896000 |  1066.69 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 59
    apples_agent-0_mean: 2.76
    apples_agent-0_min: 0
    apples_agent-1_max: 71
    apples_agent-1_mean: 17.33
    apples_agent-1_min: 0
    apples_agent-2_max: 180
    apples_agent-2_mean: 13.92
    apples_agent-2_min: 0
    apples_agent-3_max: 174
    apples_agent-3_mean: 57.7
    apples_agent-3_min: 32
    apples_agent-4_max: 79
    apples_agent-4_mean: 1.53
    apples_agent-4_min: 0
    apples_agent-5_max: 206
    apples_agent-5_mean: 100.5
    apples_agent-5_min: 59
    cleaning_beam_agent-0_max: 554
    cleaning_beam_agent-0_mean: 408.14
    cleaning_beam_agent-0_min: 251
    cleaning_beam_agent-1_max: 490
    cleaning_beam_agent-1_mean: 313.05
    cleaning_beam_agent-1_min: 137
    cleaning_beam_agent-2_max: 481
    cleaning_beam_agent-2_mean: 335.03
    cleaning_beam_agent-2_min: 118
    cleaning_beam_agent-3_max: 70
    cleaning_beam_agent-3_mean: 15.21
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 610
    cleaning_beam_agent-4_mean: 490.29
    cleaning_beam_agent-4_min: 322
    cleaning_beam_agent-5_max: 89
    cleaning_beam_agent-5_mean: 13.46
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-53-01
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1228.0000000000064
  episode_reward_mean: 1075.5399999999938
  episode_reward_min: 713.9999999999936
  episodes_this_iter: 96
  episodes_total: 40992
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20340.101
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9370459318161011
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016270158812403679
        model: {}
        policy_loss: -0.0030669018160551786
        total_loss: -0.0025322474539279938
        vf_explained_var: 0.06649823486804962
        vf_loss: 21.838539123535156
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1854214668273926
        entropy_coeff: 0.0017600000137463212
        kl: 0.002066823188215494
        model: {}
        policy_loss: -0.004095039330422878
        total_loss: -0.003803820349276066
        vf_explained_var: -0.01684732735157013
        vf_loss: 23.775651931762695
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1511644124984741
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014009536243975163
        model: {}
        policy_loss: -0.0033301720395684242
        total_loss: -0.0030284938402473927
        vf_explained_var: 0.002271890640258789
        vf_loss: 23.27729606628418
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.34800010919570923
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010733435628935695
        model: {}
        policy_loss: -0.0019948240369558334
        total_loss: -0.0005188181530684233
        vf_explained_var: 0.10312537848949432
        vf_loss: 20.884836196899414
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8630430698394775
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022196329664438963
        model: {}
        policy_loss: -0.0039598471485078335
        total_loss: -0.0032666176557540894
        vf_explained_var: 0.05204816162586212
        vf_loss: 22.121854782104492
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49286767840385437
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010524283861741424
        model: {}
        policy_loss: -0.002748283091932535
        total_loss: -0.001489813905209303
        vf_explained_var: 0.08944264054298401
        vf_loss: 21.2591609954834
    load_time_ms: 20354.762
    num_steps_sampled: 40992000
    num_steps_trained: 40992000
    sample_time_ms: 107227.015
    update_time_ms: 21.114
  iterations_since_restore: 407
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 19.81741573033708
    ram_util_percent: 19.219662921348313
  pid: 4061
  policy_reward_max:
    agent-0: 204.666666666666
    agent-1: 204.666666666666
    agent-2: 204.666666666666
    agent-3: 204.666666666666
    agent-4: 204.666666666666
    agent-5: 204.666666666666
  policy_reward_mean:
    agent-0: 179.2566666666665
    agent-1: 179.2566666666665
    agent-2: 179.2566666666665
    agent-3: 179.2566666666665
    agent-4: 179.2566666666665
    agent-5: 179.2566666666665
  policy_reward_min:
    agent-0: 119.00000000000043
    agent-1: 119.00000000000043
    agent-2: 119.00000000000043
    agent-3: 119.00000000000043
    agent-4: 119.00000000000043
    agent-5: 119.00000000000043
  sampler_perf:
    mean_env_wait_ms: 24.4182353663247
    mean_inference_ms: 12.328660306057326
    mean_processing_ms: 50.927454360494224
  time_since_restore: 53185.358731508255
  time_this_iter_s: 125.0545506477356
  time_total_s: 56396.422417640686
  timestamp: 1637070781
  timesteps_since_restore: 39072000
  timesteps_this_iter: 96000
  timesteps_total: 40992000
  training_iteration: 427
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    427 |          56396.4 | 40992000 |  1075.54 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 57
    apples_agent-0_mean: 1.28
    apples_agent-0_min: 0
    apples_agent-1_max: 99
    apples_agent-1_mean: 17.11
    apples_agent-1_min: 0
    apples_agent-2_max: 194
    apples_agent-2_mean: 14.94
    apples_agent-2_min: 0
    apples_agent-3_max: 100
    apples_agent-3_mean: 54.17
    apples_agent-3_min: 29
    apples_agent-4_max: 63
    apples_agent-4_mean: 0.63
    apples_agent-4_min: 0
    apples_agent-5_max: 181
    apples_agent-5_mean: 98.7
    apples_agent-5_min: 61
    cleaning_beam_agent-0_max: 497
    cleaning_beam_agent-0_mean: 406.33
    cleaning_beam_agent-0_min: 278
    cleaning_beam_agent-1_max: 475
    cleaning_beam_agent-1_mean: 310.96
    cleaning_beam_agent-1_min: 111
    cleaning_beam_agent-2_max: 500
    cleaning_beam_agent-2_mean: 330.2
    cleaning_beam_agent-2_min: 122
    cleaning_beam_agent-3_max: 55
    cleaning_beam_agent-3_mean: 13.92
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 593
    cleaning_beam_agent-4_mean: 497.02
    cleaning_beam_agent-4_min: 394
    cleaning_beam_agent-5_max: 174
    cleaning_beam_agent-5_mean: 15.52
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-55-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1245.9999999999952
  episode_reward_mean: 1087.0599999999938
  episode_reward_min: 554.0000000000053
  episodes_this_iter: 96
  episodes_total: 41088
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20318.768
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.953044056892395
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014550163177773356
        model: {}
        policy_loss: -0.0030003879219293594
        total_loss: -0.0024862713180482388
        vf_explained_var: 0.039529502391815186
        vf_loss: 21.91474151611328
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1719887256622314
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021545225754380226
        model: {}
        policy_loss: -0.003425147384405136
        total_loss: -0.0031900620087981224
        vf_explained_var: -0.006710410118103027
        vf_loss: 22.977855682373047
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1483808755874634
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013284040614962578
        model: {}
        policy_loss: -0.0035604480654001236
        total_loss: -0.0032995501533150673
        vf_explained_var: -0.0018634200096130371
        vf_loss: 22.820465087890625
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3288775086402893
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011153637897223234
        model: {}
        policy_loss: -0.0023569324985146523
        total_loss: -0.0008639218285679817
        vf_explained_var: 0.08782629668712616
        vf_loss: 20.718353271484375
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.874518096446991
        entropy_coeff: 0.0017600000137463212
        kl: 0.002438153838738799
        model: {}
        policy_loss: -0.004055222030729055
        total_loss: -0.003341744653880596
        vf_explained_var: 0.012422725558280945
        vf_loss: 22.526287078857422
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48339831829071045
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006674489704892039
        model: {}
        policy_loss: -0.0025849381927400827
        total_loss: -0.001313342247158289
        vf_explained_var: 0.06697608530521393
        vf_loss: 21.223756790161133
    load_time_ms: 20271.018
    num_steps_sampled: 41088000
    num_steps_trained: 41088000
    sample_time_ms: 106955.751
    update_time_ms: 21.467
  iterations_since_restore: 408
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.25337078651685
    ram_util_percent: 19.18876404494382
  pid: 4061
  policy_reward_max:
    agent-0: 207.66666666666634
    agent-1: 207.66666666666634
    agent-2: 207.66666666666634
    agent-3: 207.66666666666634
    agent-4: 207.66666666666634
    agent-5: 207.66666666666634
  policy_reward_mean:
    agent-0: 181.1766666666665
    agent-1: 181.1766666666665
    agent-2: 181.1766666666665
    agent-3: 181.1766666666665
    agent-4: 181.1766666666665
    agent-5: 181.1766666666665
  policy_reward_min:
    agent-0: 92.33333333333368
    agent-1: 92.33333333333368
    agent-2: 92.33333333333368
    agent-3: 92.33333333333368
    agent-4: 92.33333333333368
    agent-5: 92.33333333333368
  sampler_perf:
    mean_env_wait_ms: 24.420313823419736
    mean_inference_ms: 12.328733955792364
    mean_processing_ms: 50.92734681848893
  time_since_restore: 53310.012832164764
  time_this_iter_s: 124.6541006565094
  time_total_s: 56521.076518297195
  timestamp: 1637070906
  timesteps_since_restore: 39168000
  timesteps_this_iter: 96000
  timesteps_total: 41088000
  training_iteration: 428
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    428 |          56521.1 | 41088000 |  1087.06 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 22
    apples_agent-0_mean: 0.73
    apples_agent-0_min: 0
    apples_agent-1_max: 91
    apples_agent-1_mean: 21.08
    apples_agent-1_min: 0
    apples_agent-2_max: 122
    apples_agent-2_mean: 15.18
    apples_agent-2_min: 0
    apples_agent-3_max: 125
    apples_agent-3_mean: 55.72
    apples_agent-3_min: 30
    apples_agent-4_max: 54
    apples_agent-4_mean: 0.97
    apples_agent-4_min: 0
    apples_agent-5_max: 171
    apples_agent-5_mean: 99.39
    apples_agent-5_min: 57
    cleaning_beam_agent-0_max: 519
    cleaning_beam_agent-0_mean: 413.66
    cleaning_beam_agent-0_min: 308
    cleaning_beam_agent-1_max: 518
    cleaning_beam_agent-1_mean: 306.25
    cleaning_beam_agent-1_min: 149
    cleaning_beam_agent-2_max: 460
    cleaning_beam_agent-2_mean: 313.46
    cleaning_beam_agent-2_min: 120
    cleaning_beam_agent-3_max: 59
    cleaning_beam_agent-3_mean: 15.98
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 602
    cleaning_beam_agent-4_mean: 478.62
    cleaning_beam_agent-4_min: 333
    cleaning_beam_agent-5_max: 118
    cleaning_beam_agent-5_mean: 13.64
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-57-10
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1267.999999999993
  episode_reward_mean: 1089.1099999999954
  episode_reward_min: 460.0000000000025
  episodes_this_iter: 96
  episodes_total: 41184
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20334.751
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9447699189186096
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020090951584279537
        model: {}
        policy_loss: -0.0030388832092285156
        total_loss: -0.0025120945647358894
        vf_explained_var: 0.04659487307071686
        vf_loss: 21.895835876464844
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1718474626541138
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018406511517241597
        model: {}
        policy_loss: -0.0038681020960211754
        total_loss: -0.003650144673883915
        vf_explained_var: 0.00582374632358551
        vf_loss: 22.80413055419922
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.15886390209198
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014220713637769222
        model: {}
        policy_loss: -0.003504271851852536
        total_loss: -0.00327152106910944
        vf_explained_var: 0.013373807072639465
        vf_loss: 22.723487854003906
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.33441972732543945
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009539039456285536
        model: {}
        policy_loss: -0.0020802372600883245
        total_loss: -0.0005556331016123295
        vf_explained_var: 0.07628823816776276
        vf_loss: 21.13181495666504
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8868605494499207
        entropy_coeff: 0.0017600000137463212
        kl: 0.00160278903786093
        model: {}
        policy_loss: -0.0037973537109792233
        total_loss: -0.003191546071320772
        vf_explained_var: 0.05573831498622894
        vf_loss: 21.66681480407715
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4853816032409668
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008991560898721218
        model: {}
        policy_loss: -0.0028925463557243347
        total_loss: -0.0016251467168331146
        vf_explained_var: 0.07612147927284241
        vf_loss: 21.21670150756836
    load_time_ms: 20165.949
    num_steps_sampled: 41184000
    num_steps_trained: 41184000
    sample_time_ms: 106254.657
    update_time_ms: 21.173
  iterations_since_restore: 409
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.281818181818185
    ram_util_percent: 19.088636363636365
  pid: 4061
  policy_reward_max:
    agent-0: 211.3333333333333
    agent-1: 211.3333333333333
    agent-2: 211.3333333333333
    agent-3: 211.3333333333333
    agent-4: 211.3333333333333
    agent-5: 211.3333333333333
  policy_reward_mean:
    agent-0: 181.5183333333331
    agent-1: 181.5183333333331
    agent-2: 181.5183333333331
    agent-3: 181.5183333333331
    agent-4: 181.5183333333331
    agent-5: 181.5183333333331
  policy_reward_min:
    agent-0: 76.66666666666674
    agent-1: 76.66666666666674
    agent-2: 76.66666666666674
    agent-3: 76.66666666666674
    agent-4: 76.66666666666674
    agent-5: 76.66666666666674
  sampler_perf:
    mean_env_wait_ms: 24.42207764088719
    mean_inference_ms: 12.32883870690973
    mean_processing_ms: 50.92757572072866
  time_since_restore: 53434.334070682526
  time_this_iter_s: 124.32123851776123
  time_total_s: 56645.39775681496
  timestamp: 1637071030
  timesteps_since_restore: 39264000
  timesteps_this_iter: 96000
  timesteps_total: 41184000
  training_iteration: 429
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    429 |          56645.4 | 41184000 |  1089.11 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 85
    apples_agent-0_mean: 2.84
    apples_agent-0_min: 0
    apples_agent-1_max: 76
    apples_agent-1_mean: 17.35
    apples_agent-1_min: 0
    apples_agent-2_max: 171
    apples_agent-2_mean: 16.93
    apples_agent-2_min: 0
    apples_agent-3_max: 152
    apples_agent-3_mean: 57.86
    apples_agent-3_min: 30
    apples_agent-4_max: 23
    apples_agent-4_mean: 0.38
    apples_agent-4_min: 0
    apples_agent-5_max: 189
    apples_agent-5_mean: 104.13
    apples_agent-5_min: 50
    cleaning_beam_agent-0_max: 492
    cleaning_beam_agent-0_mean: 398.8
    cleaning_beam_agent-0_min: 261
    cleaning_beam_agent-1_max: 472
    cleaning_beam_agent-1_mean: 313.25
    cleaning_beam_agent-1_min: 141
    cleaning_beam_agent-2_max: 479
    cleaning_beam_agent-2_mean: 318.04
    cleaning_beam_agent-2_min: 153
    cleaning_beam_agent-3_max: 63
    cleaning_beam_agent-3_mean: 14.72
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 599
    cleaning_beam_agent-4_mean: 472.41
    cleaning_beam_agent-4_min: 363
    cleaning_beam_agent-5_max: 107
    cleaning_beam_agent-5_mean: 12.15
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-59-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1260.0000000000125
  episode_reward_mean: 1080.4799999999925
  episode_reward_min: 789.999999999994
  episodes_this_iter: 96
  episodes_total: 41280
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20308.301
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9617959260940552
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021963356994092464
        model: {}
        policy_loss: -0.0033447195310145617
        total_loss: -0.002860517241060734
        vf_explained_var: 0.012304455041885376
        vf_loss: 21.769630432128906
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1733516454696655
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020936254877597094
        model: {}
        policy_loss: -0.003836654359474778
        total_loss: -0.0036883505526930094
        vf_explained_var: -0.001368880271911621
        vf_loss: 22.13404083251953
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.134575366973877
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017089867033064365
        model: {}
        policy_loss: -0.0033284537494182587
        total_loss: -0.003023087978363037
        vf_explained_var: -0.04366287589073181
        vf_loss: 23.02216911315918
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.34802788496017456
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012863115407526493
        model: {}
        policy_loss: -0.0021327307913452387
        total_loss: -0.0006945707718841732
        vf_explained_var: 0.07037873566150665
        vf_loss: 20.506916046142578
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8877173662185669
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020626643672585487
        model: {}
        policy_loss: -0.0037452909164130688
        total_loss: -0.0031472849659621716
        vf_explained_var: 0.0226031094789505
        vf_loss: 21.603870391845703
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4973480701446533
        entropy_coeff: 0.0017600000137463212
        kl: 0.001157945953309536
        model: {}
        policy_loss: -0.002593021374195814
        total_loss: -0.001408543437719345
        vf_explained_var: 0.06962871551513672
        vf_loss: 20.598114013671875
    load_time_ms: 20053.252
    num_steps_sampled: 41280000
    num_steps_trained: 41280000
    sample_time_ms: 105604.845
    update_time_ms: 21.817
  iterations_since_restore: 410
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.23806818181818
    ram_util_percent: 19.116477272727273
  pid: 4061
  policy_reward_max:
    agent-0: 209.99999999999983
    agent-1: 209.99999999999983
    agent-2: 209.99999999999983
    agent-3: 209.99999999999983
    agent-4: 209.99999999999983
    agent-5: 209.99999999999983
  policy_reward_mean:
    agent-0: 180.07999999999979
    agent-1: 180.07999999999979
    agent-2: 180.07999999999979
    agent-3: 180.07999999999979
    agent-4: 180.07999999999979
    agent-5: 180.07999999999979
  policy_reward_min:
    agent-0: 131.66666666666688
    agent-1: 131.66666666666688
    agent-2: 131.66666666666688
    agent-3: 131.66666666666688
    agent-4: 131.66666666666688
    agent-5: 131.66666666666688
  sampler_perf:
    mean_env_wait_ms: 24.42336004556306
    mean_inference_ms: 12.328761480039754
    mean_processing_ms: 50.92660036056138
  time_since_restore: 53558.11285448074
  time_this_iter_s: 123.77878379821777
  time_total_s: 56769.176540613174
  timestamp: 1637071154
  timesteps_since_restore: 39360000
  timesteps_this_iter: 96000
  timesteps_total: 41280000
  training_iteration: 430
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 35.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    430 |          56769.2 | 41280000 |  1080.48 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 55
    apples_agent-0_mean: 1.72
    apples_agent-0_min: 0
    apples_agent-1_max: 86
    apples_agent-1_mean: 19.97
    apples_agent-1_min: 0
    apples_agent-2_max: 112
    apples_agent-2_mean: 11.7
    apples_agent-2_min: 0
    apples_agent-3_max: 87
    apples_agent-3_mean: 55.37
    apples_agent-3_min: 29
    apples_agent-4_max: 58
    apples_agent-4_mean: 0.97
    apples_agent-4_min: 0
    apples_agent-5_max: 149
    apples_agent-5_mean: 99.86
    apples_agent-5_min: 32
    cleaning_beam_agent-0_max: 500
    cleaning_beam_agent-0_mean: 390.94
    cleaning_beam_agent-0_min: 197
    cleaning_beam_agent-1_max: 471
    cleaning_beam_agent-1_mean: 302.71
    cleaning_beam_agent-1_min: 175
    cleaning_beam_agent-2_max: 524
    cleaning_beam_agent-2_mean: 323.65
    cleaning_beam_agent-2_min: 163
    cleaning_beam_agent-3_max: 39
    cleaning_beam_agent-3_mean: 13.6
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 558
    cleaning_beam_agent-4_mean: 465.51
    cleaning_beam_agent-4_min: 375
    cleaning_beam_agent-5_max: 76
    cleaning_beam_agent-5_mean: 11.98
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-01-18
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1232.000000000002
  episode_reward_mean: 1064.7199999999928
  episode_reward_min: 405.00000000000813
  episodes_this_iter: 96
  episodes_total: 41376
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20331.114
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9585353136062622
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011511098127812147
        model: {}
        policy_loss: -0.003260104451328516
        total_loss: -0.0025220918469130993
        vf_explained_var: 0.007691860198974609
        vf_loss: 24.250329971313477
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1878535747528076
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014948564348742366
        model: {}
        policy_loss: -0.003526977961882949
        total_loss: -0.0032855505123734474
        vf_explained_var: 0.04573005437850952
        vf_loss: 23.320520401000977
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.135363221168518
        entropy_coeff: 0.0017600000137463212
        kl: 0.001580305164679885
        model: {}
        policy_loss: -0.003300656331703067
        total_loss: -0.002885381691157818
        vf_explained_var: 0.012673109769821167
        vf_loss: 24.135128021240234
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3573296070098877
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008680238388478756
        model: {}
        policy_loss: -0.002117299474775791
        total_loss: -0.0006062963511794806
        vf_explained_var: 0.12436316907405853
        vf_loss: 21.399005889892578
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8802534937858582
        entropy_coeff: 0.0017600000137463212
        kl: 0.002657703123986721
        model: {}
        policy_loss: -0.0038686555344611406
        total_loss: -0.003035167697817087
        vf_explained_var: 0.024288654327392578
        vf_loss: 23.827320098876953
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5109339952468872
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011432182509452105
        model: {}
        policy_loss: -0.0031105219386518
        total_loss: -0.0018186212982982397
        vf_explained_var: 0.10415259003639221
        vf_loss: 21.911447525024414
    load_time_ms: 19920.052
    num_steps_sampled: 41376000
    num_steps_trained: 41376000
    sample_time_ms: 105063.555
    update_time_ms: 22.096
  iterations_since_restore: 411
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.507344632768362
    ram_util_percent: 19.21412429378531
  pid: 4061
  policy_reward_max:
    agent-0: 205.33333333333348
    agent-1: 205.33333333333348
    agent-2: 205.33333333333348
    agent-3: 205.33333333333348
    agent-4: 205.33333333333348
    agent-5: 205.33333333333348
  policy_reward_mean:
    agent-0: 177.45333333333318
    agent-1: 177.45333333333318
    agent-2: 177.45333333333318
    agent-3: 177.45333333333318
    agent-4: 177.45333333333318
    agent-5: 177.45333333333318
  policy_reward_min:
    agent-0: 67.49999999999989
    agent-1: 67.49999999999989
    agent-2: 67.49999999999989
    agent-3: 67.49999999999989
    agent-4: 67.49999999999989
    agent-5: 67.49999999999989
  sampler_perf:
    mean_env_wait_ms: 24.42472781203934
    mean_inference_ms: 12.328699460968965
    mean_processing_ms: 50.92600830149813
  time_since_restore: 53682.06380224228
  time_this_iter_s: 123.95094776153564
  time_total_s: 56893.12748837471
  timestamp: 1637071278
  timesteps_since_restore: 39456000
  timesteps_this_iter: 96000
  timesteps_total: 41376000
  training_iteration: 431
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    431 |          56893.1 | 41376000 |  1064.72 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 46
    apples_agent-0_mean: 1.47
    apples_agent-0_min: 0
    apples_agent-1_max: 104
    apples_agent-1_mean: 17.11
    apples_agent-1_min: 0
    apples_agent-2_max: 141
    apples_agent-2_mean: 13.34
    apples_agent-2_min: 0
    apples_agent-3_max: 96
    apples_agent-3_mean: 56.69
    apples_agent-3_min: 28
    apples_agent-4_max: 21
    apples_agent-4_mean: 0.4
    apples_agent-4_min: 0
    apples_agent-5_max: 165
    apples_agent-5_mean: 102.01
    apples_agent-5_min: 60
    cleaning_beam_agent-0_max: 499
    cleaning_beam_agent-0_mean: 394.55
    cleaning_beam_agent-0_min: 297
    cleaning_beam_agent-1_max: 473
    cleaning_beam_agent-1_mean: 305.26
    cleaning_beam_agent-1_min: 115
    cleaning_beam_agent-2_max: 537
    cleaning_beam_agent-2_mean: 333.37
    cleaning_beam_agent-2_min: 157
    cleaning_beam_agent-3_max: 46
    cleaning_beam_agent-3_mean: 13.03
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 584
    cleaning_beam_agent-4_mean: 478.82
    cleaning_beam_agent-4_min: 378
    cleaning_beam_agent-5_max: 206
    cleaning_beam_agent-5_mean: 11.02
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-03-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1237.0000000000043
  episode_reward_mean: 1092.4299999999928
  episode_reward_min: 624.9999999999893
  episodes_this_iter: 96
  episodes_total: 41472
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20316.426
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9639990925788879
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020793145522475243
        model: {}
        policy_loss: -0.003158566541969776
        total_loss: -0.0026662470772862434
        vf_explained_var: 0.024763032793998718
        vf_loss: 21.88958168029785
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.178687334060669
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018291539745405316
        model: {}
        policy_loss: -0.003807495813816786
        total_loss: -0.0036873691715300083
        vf_explained_var: 0.029211387038230896
        vf_loss: 21.946151733398438
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1511611938476562
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012800630647689104
        model: {}
        policy_loss: -0.003109864890575409
        total_loss: -0.00289527652785182
        vf_explained_var: 0.004268467426300049
        vf_loss: 22.406343460083008
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.33970803022384644
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009920669253915548
        model: {}
        policy_loss: -0.001963499002158642
        total_loss: -0.0004968240391463041
        vf_explained_var: 0.08107662200927734
        vf_loss: 20.64561653137207
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8784775137901306
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020161066204309464
        model: {}
        policy_loss: -0.003916692920029163
        total_loss: -0.0032933682668954134
        vf_explained_var: 0.03471823036670685
        vf_loss: 21.6944522857666
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47455501556396484
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010454592993482947
        model: {}
        policy_loss: -0.0026867464184761047
        total_loss: -0.0013925060629844666
        vf_explained_var: 0.05586472153663635
        vf_loss: 21.294597625732422
    load_time_ms: 19597.855
    num_steps_sampled: 41472000
    num_steps_trained: 41472000
    sample_time_ms: 105167.76
    update_time_ms: 22.347
  iterations_since_restore: 412
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.875706214689266
    ram_util_percent: 19.146892655367232
  pid: 4061
  policy_reward_max:
    agent-0: 206.16666666666637
    agent-1: 206.16666666666637
    agent-2: 206.16666666666637
    agent-3: 206.16666666666637
    agent-4: 206.16666666666637
    agent-5: 206.16666666666637
  policy_reward_mean:
    agent-0: 182.0716666666665
    agent-1: 182.0716666666665
    agent-2: 182.0716666666665
    agent-3: 182.0716666666665
    agent-4: 182.0716666666665
    agent-5: 182.0716666666665
  policy_reward_min:
    agent-0: 104.16666666666724
    agent-1: 104.16666666666724
    agent-2: 104.16666666666724
    agent-3: 104.16666666666724
    agent-4: 104.16666666666724
    agent-5: 104.16666666666724
  sampler_perf:
    mean_env_wait_ms: 24.426450983426392
    mean_inference_ms: 12.328749462365993
    mean_processing_ms: 50.92586288962693
  time_since_restore: 53806.18484735489
  time_this_iter_s: 124.12104511260986
  time_total_s: 57017.24853348732
  timestamp: 1637071403
  timesteps_since_restore: 39552000
  timesteps_this_iter: 96000
  timesteps_total: 41472000
  training_iteration: 432
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    432 |          57017.2 | 41472000 |  1092.43 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 59
    apples_agent-0_mean: 2.7
    apples_agent-0_min: 0
    apples_agent-1_max: 95
    apples_agent-1_mean: 20.51
    apples_agent-1_min: 0
    apples_agent-2_max: 262
    apples_agent-2_mean: 10.88
    apples_agent-2_min: 0
    apples_agent-3_max: 146
    apples_agent-3_mean: 58.53
    apples_agent-3_min: 26
    apples_agent-4_max: 48
    apples_agent-4_mean: 0.87
    apples_agent-4_min: 0
    apples_agent-5_max: 262
    apples_agent-5_mean: 101.95
    apples_agent-5_min: 38
    cleaning_beam_agent-0_max: 494
    cleaning_beam_agent-0_mean: 385.25
    cleaning_beam_agent-0_min: 221
    cleaning_beam_agent-1_max: 419
    cleaning_beam_agent-1_mean: 294.92
    cleaning_beam_agent-1_min: 165
    cleaning_beam_agent-2_max: 462
    cleaning_beam_agent-2_mean: 344.76
    cleaning_beam_agent-2_min: 98
    cleaning_beam_agent-3_max: 71
    cleaning_beam_agent-3_mean: 15.43
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 581
    cleaning_beam_agent-4_mean: 487.27
    cleaning_beam_agent-4_min: 387
    cleaning_beam_agent-5_max: 95
    cleaning_beam_agent-5_mean: 15.75
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-05-28
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1222.9999999999825
  episode_reward_mean: 1054.7099999999941
  episode_reward_min: 324.0000000000015
  episodes_this_iter: 96
  episodes_total: 41568
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20301.025
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9748445749282837
        entropy_coeff: 0.0017600000137463212
        kl: 0.002064469736069441
        model: {}
        policy_loss: -0.0031211404129862785
        total_loss: -0.0024750535376369953
        vf_explained_var: 0.06695325672626495
        vf_loss: 23.61811637878418
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.18339204788208
        entropy_coeff: 0.0017600000137463212
        kl: 0.001629229518584907
        model: {}
        policy_loss: -0.0034798174165189266
        total_loss: -0.0030789407901465893
        vf_explained_var: 0.01312631368637085
        vf_loss: 24.836448669433594
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1251497268676758
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018553391564637423
        model: {}
        policy_loss: -0.0033426061272621155
        total_loss: -0.0027602834161370993
        vf_explained_var: -0.016219958662986755
        vf_loss: 25.625904083251953
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3671569526195526
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007750312797725201
        model: {}
        policy_loss: -0.00248753372579813
        total_loss: -0.0009249458089470863
        vf_explained_var: 0.12407004833221436
        vf_loss: 22.08785629272461
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8790279626846313
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018918367568403482
        model: {}
        policy_loss: -0.003710180753841996
        total_loss: -0.002871559699997306
        vf_explained_var: 0.05342666804790497
        vf_loss: 23.85710906982422
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5129855275154114
        entropy_coeff: 0.0017600000137463212
        kl: 0.000962069898378104
        model: {}
        policy_loss: -0.002918712794780731
        total_loss: -0.0016027344390749931
        vf_explained_var: 0.11879609525203705
        vf_loss: 22.18832015991211
    load_time_ms: 19478.817
    num_steps_sampled: 41568000
    num_steps_trained: 41568000
    sample_time_ms: 104832.195
    update_time_ms: 22.124
  iterations_since_restore: 413
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.658988764044942
    ram_util_percent: 19.01797752808989
  pid: 4061
  policy_reward_max:
    agent-0: 203.8333333333335
    agent-1: 203.8333333333335
    agent-2: 203.8333333333335
    agent-3: 203.8333333333335
    agent-4: 203.8333333333335
    agent-5: 203.8333333333335
  policy_reward_mean:
    agent-0: 175.78499999999988
    agent-1: 175.78499999999988
    agent-2: 175.78499999999988
    agent-3: 175.78499999999988
    agent-4: 175.78499999999988
    agent-5: 175.78499999999988
  policy_reward_min:
    agent-0: 53.99999999999987
    agent-1: 53.99999999999987
    agent-2: 53.99999999999987
    agent-3: 53.99999999999987
    agent-4: 53.99999999999987
    agent-5: 53.99999999999987
  sampler_perf:
    mean_env_wait_ms: 24.428294048325643
    mean_inference_ms: 12.32891859311826
    mean_processing_ms: 50.92598458884656
  time_since_restore: 53930.97845244408
  time_this_iter_s: 124.79360508918762
  time_total_s: 57142.04213857651
  timestamp: 1637071528
  timesteps_since_restore: 39648000
  timesteps_this_iter: 96000
  timesteps_total: 41568000
  training_iteration: 433
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    433 |            57142 | 41568000 |  1054.71 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 27
    apples_agent-0_mean: 2.27
    apples_agent-0_min: 0
    apples_agent-1_max: 103
    apples_agent-1_mean: 17.63
    apples_agent-1_min: 0
    apples_agent-2_max: 111
    apples_agent-2_mean: 14.61
    apples_agent-2_min: 0
    apples_agent-3_max: 106
    apples_agent-3_mean: 55.36
    apples_agent-3_min: 21
    apples_agent-4_max: 32
    apples_agent-4_mean: 0.53
    apples_agent-4_min: 0
    apples_agent-5_max: 242
    apples_agent-5_mean: 101.19
    apples_agent-5_min: 41
    cleaning_beam_agent-0_max: 465
    cleaning_beam_agent-0_mean: 371.11
    cleaning_beam_agent-0_min: 235
    cleaning_beam_agent-1_max: 549
    cleaning_beam_agent-1_mean: 294.23
    cleaning_beam_agent-1_min: 96
    cleaning_beam_agent-2_max: 475
    cleaning_beam_agent-2_mean: 345.96
    cleaning_beam_agent-2_min: 140
    cleaning_beam_agent-3_max: 46
    cleaning_beam_agent-3_mean: 13.75
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 599
    cleaning_beam_agent-4_mean: 476.14
    cleaning_beam_agent-4_min: 381
    cleaning_beam_agent-5_max: 81
    cleaning_beam_agent-5_mean: 10.68
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-07-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1239.0000000000082
  episode_reward_mean: 1071.9399999999944
  episode_reward_min: 439.00000000000495
  episodes_this_iter: 96
  episodes_total: 41664
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20276.597
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9719258546829224
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013864540960639715
        model: {}
        policy_loss: -0.0031861690804362297
        total_loss: -0.002574507612735033
        vf_explained_var: 0.050622835755348206
        vf_loss: 23.22251319885254
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1834886074066162
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013876307057216763
        model: {}
        policy_loss: -0.003651807550340891
        total_loss: -0.0033175034914165735
        vf_explained_var: 0.01289975643157959
        vf_loss: 24.172447204589844
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1305599212646484
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011516171507537365
        model: {}
        policy_loss: -0.0031637512147426605
        total_loss: -0.0027290014550089836
        vf_explained_var: 0.006908386945724487
        vf_loss: 24.245342254638672
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3610420823097229
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006580897606909275
        model: {}
        policy_loss: -0.002058526501059532
        total_loss: -0.0005349498242139816
        vf_explained_var: 0.117961585521698
        vf_loss: 21.590078353881836
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8848711252212524
        entropy_coeff: 0.0017600000137463212
        kl: 0.002049056114628911
        model: {}
        policy_loss: -0.003573538037016988
        total_loss: -0.0027307497803121805
        vf_explained_var: 0.019122809171676636
        vf_loss: 24.001605987548828
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4980573058128357
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008921771659515798
        model: {}
        policy_loss: -0.0029812827706336975
        total_loss: -0.0016916203312575817
        vf_explained_var: 0.11432996392250061
        vf_loss: 21.662445068359375
    load_time_ms: 13344.919
    num_steps_sampled: 41664000
    num_steps_trained: 41664000
    sample_time_ms: 91176.439
    update_time_ms: 22.823
  iterations_since_restore: 414
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.84034090909091
    ram_util_percent: 19.05909090909091
  pid: 4061
  policy_reward_max:
    agent-0: 206.50000000000017
    agent-1: 206.50000000000017
    agent-2: 206.50000000000017
    agent-3: 206.50000000000017
    agent-4: 206.50000000000017
    agent-5: 206.50000000000017
  policy_reward_mean:
    agent-0: 178.65666666666652
    agent-1: 178.65666666666652
    agent-2: 178.65666666666652
    agent-3: 178.65666666666652
    agent-4: 178.65666666666652
    agent-5: 178.65666666666652
  policy_reward_min:
    agent-0: 73.1666666666666
    agent-1: 73.1666666666666
    agent-2: 73.1666666666666
    agent-3: 73.1666666666666
    agent-4: 73.1666666666666
    agent-5: 73.1666666666666
  sampler_perf:
    mean_env_wait_ms: 24.42938025451986
    mean_inference_ms: 12.328847174285299
    mean_processing_ms: 50.92508732170642
  time_since_restore: 54055.33915686607
  time_this_iter_s: 124.36070442199707
  time_total_s: 57266.402842998505
  timestamp: 1637071652
  timesteps_since_restore: 39744000
  timesteps_this_iter: 96000
  timesteps_total: 41664000
  training_iteration: 434
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    434 |          57266.4 | 41664000 |  1071.94 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 53
    apples_agent-0_mean: 2.14
    apples_agent-0_min: 0
    apples_agent-1_max: 73
    apples_agent-1_mean: 22.15
    apples_agent-1_min: 0
    apples_agent-2_max: 67
    apples_agent-2_mean: 9.28
    apples_agent-2_min: 0
    apples_agent-3_max: 104
    apples_agent-3_mean: 54.42
    apples_agent-3_min: 31
    apples_agent-4_max: 30
    apples_agent-4_mean: 1.03
    apples_agent-4_min: 0
    apples_agent-5_max: 164
    apples_agent-5_mean: 101.34
    apples_agent-5_min: 18
    cleaning_beam_agent-0_max: 479
    cleaning_beam_agent-0_mean: 381.16
    cleaning_beam_agent-0_min: 299
    cleaning_beam_agent-1_max: 486
    cleaning_beam_agent-1_mean: 292.01
    cleaning_beam_agent-1_min: 168
    cleaning_beam_agent-2_max: 591
    cleaning_beam_agent-2_mean: 356.02
    cleaning_beam_agent-2_min: 140
    cleaning_beam_agent-3_max: 36
    cleaning_beam_agent-3_mean: 14.26
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 600
    cleaning_beam_agent-4_mean: 468.96
    cleaning_beam_agent-4_min: 369
    cleaning_beam_agent-5_max: 99
    cleaning_beam_agent-5_mean: 11.36
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-09-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1247.0000000000014
  episode_reward_mean: 1082.0099999999932
  episode_reward_min: 333.00000000000335
  episodes_this_iter: 96
  episodes_total: 41760
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20259.935
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9550817012786865
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012572817504405975
        model: {}
        policy_loss: -0.002753033535555005
        total_loss: -0.0021624525543302298
        vf_explained_var: 0.009421154856681824
        vf_loss: 22.71526336669922
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1965336799621582
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018499032594263554
        model: {}
        policy_loss: -0.0034622985403984785
        total_loss: -0.00328528112731874
        vf_explained_var: 0.008172377943992615
        vf_loss: 22.829151153564453
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1309922933578491
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018723157700151205
        model: {}
        policy_loss: -0.0034480609465390444
        total_loss: -0.003183439141139388
        vf_explained_var: 0.019770532846450806
        vf_loss: 22.551647186279297
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.351839542388916
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009739425731822848
        model: {}
        policy_loss: -0.0020904329139739275
        total_loss: -0.0006624446250498295
        vf_explained_var: 0.10919944941997528
        vf_loss: 20.47226333618164
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8863887190818787
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023431521840393543
        model: {}
        policy_loss: -0.004074648953974247
        total_loss: -0.003398043103516102
        vf_explained_var: 0.03099827468395233
        vf_loss: 22.366455078125
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5027880668640137
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006691931048408151
        model: {}
        policy_loss: -0.002854435471817851
        total_loss: -0.0016321553848683834
        vf_explained_var: 0.08787940442562103
        vf_loss: 21.071847915649414
    load_time_ms: 13137.04
    num_steps_sampled: 41760000
    num_steps_trained: 41760000
    sample_time_ms: 91014.599
    update_time_ms: 22.947
  iterations_since_restore: 415
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 19.49608938547486
    ram_util_percent: 18.95418994413408
  pid: 4061
  policy_reward_max:
    agent-0: 207.83333333333337
    agent-1: 207.83333333333337
    agent-2: 207.83333333333337
    agent-3: 207.83333333333337
    agent-4: 207.83333333333337
    agent-5: 207.83333333333337
  policy_reward_mean:
    agent-0: 180.33499999999987
    agent-1: 180.33499999999987
    agent-2: 180.33499999999987
    agent-3: 180.33499999999987
    agent-4: 180.33499999999987
    agent-5: 180.33499999999987
  policy_reward_min:
    agent-0: 55.49999999999979
    agent-1: 55.49999999999979
    agent-2: 55.49999999999979
    agent-3: 55.49999999999979
    agent-4: 55.49999999999979
    agent-5: 55.49999999999979
  sampler_perf:
    mean_env_wait_ms: 24.43080081509808
    mean_inference_ms: 12.328833005368253
    mean_processing_ms: 50.92483516434362
  time_since_restore: 54180.65587592125
  time_this_iter_s: 125.31671905517578
  time_total_s: 57391.71956205368
  timestamp: 1637071778
  timesteps_since_restore: 39840000
  timesteps_this_iter: 96000
  timesteps_total: 41760000
  training_iteration: 435
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 31.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    435 |          57391.7 | 41760000 |  1082.01 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 34
    apples_agent-0_mean: 1.18
    apples_agent-0_min: 0
    apples_agent-1_max: 90
    apples_agent-1_mean: 22.57
    apples_agent-1_min: 0
    apples_agent-2_max: 163
    apples_agent-2_mean: 20.65
    apples_agent-2_min: 0
    apples_agent-3_max: 125
    apples_agent-3_mean: 57.37
    apples_agent-3_min: 26
    apples_agent-4_max: 16
    apples_agent-4_mean: 0.16
    apples_agent-4_min: 0
    apples_agent-5_max: 233
    apples_agent-5_mean: 103.07
    apples_agent-5_min: 40
    cleaning_beam_agent-0_max: 484
    cleaning_beam_agent-0_mean: 378.98
    cleaning_beam_agent-0_min: 251
    cleaning_beam_agent-1_max: 489
    cleaning_beam_agent-1_mean: 271.3
    cleaning_beam_agent-1_min: 165
    cleaning_beam_agent-2_max: 497
    cleaning_beam_agent-2_mean: 328.75
    cleaning_beam_agent-2_min: 124
    cleaning_beam_agent-3_max: 41
    cleaning_beam_agent-3_mean: 12.7
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 578
    cleaning_beam_agent-4_mean: 468.33
    cleaning_beam_agent-4_min: 373
    cleaning_beam_agent-5_max: 296
    cleaning_beam_agent-5_mean: 16.74
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-11-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1227.0000000000146
  episode_reward_mean: 1076.0199999999925
  episode_reward_min: 822.9999999999707
  episodes_this_iter: 96
  episodes_total: 41856
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20217.19
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9430624842643738
        entropy_coeff: 0.0017600000137463212
        kl: 0.001047888188622892
        model: {}
        policy_loss: -0.002829241333529353
        total_loss: -0.0022654500789940357
        vf_explained_var: -0.012522146105766296
        vf_loss: 22.23583984375
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.195433497428894
        entropy_coeff: 0.0017600000137463212
        kl: 0.002157737733796239
        model: {}
        policy_loss: -0.0040959580801427364
        total_loss: -0.004023886285722256
        vf_explained_var: 0.011484086513519287
        vf_loss: 21.7603759765625
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1403214931488037
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019554353784769773
        model: {}
        policy_loss: -0.0032904380932450294
        total_loss: -0.0031549264676868916
        vf_explained_var: 0.025799468159675598
        vf_loss: 21.42477798461914
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3477622866630554
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006453756359405816
        model: {}
        policy_loss: -0.0019023055210709572
        total_loss: -0.000494824955239892
        vf_explained_var: 0.08180077373981476
        vf_loss: 20.195404052734375
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8868916630744934
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016404015477746725
        model: {}
        policy_loss: -0.00376534485258162
        total_loss: -0.003185005858540535
        vf_explained_var: 0.02600148320198059
        vf_loss: 21.412696838378906
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5150216817855835
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008332115830853581
        model: {}
        policy_loss: -0.002668287605047226
        total_loss: -0.0014967600582167506
        vf_explained_var: 0.059896573424339294
        vf_loss: 20.7796630859375
    load_time_ms: 13128.306
    num_steps_sampled: 41856000
    num_steps_trained: 41856000
    sample_time_ms: 91344.714
    update_time_ms: 22.636
  iterations_since_restore: 416
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.196153846153845
    ram_util_percent: 17.526923076923076
  pid: 4061
  policy_reward_max:
    agent-0: 204.49999999999983
    agent-1: 204.49999999999983
    agent-2: 204.49999999999983
    agent-3: 204.49999999999983
    agent-4: 204.49999999999983
    agent-5: 204.49999999999983
  policy_reward_mean:
    agent-0: 179.3366666666665
    agent-1: 179.3366666666665
    agent-2: 179.3366666666665
    agent-3: 179.3366666666665
    agent-4: 179.3366666666665
    agent-5: 179.3366666666665
  policy_reward_min:
    agent-0: 137.16666666666674
    agent-1: 137.16666666666674
    agent-2: 137.16666666666674
    agent-3: 137.16666666666674
    agent-4: 137.16666666666674
    agent-5: 137.16666666666674
  sampler_perf:
    mean_env_wait_ms: 24.432878927552164
    mean_inference_ms: 12.329646387474456
    mean_processing_ms: 50.928316021590966
  time_since_restore: 54308.33607029915
  time_this_iter_s: 127.68019437789917
  time_total_s: 57519.39975643158
  timestamp: 1637071905
  timesteps_since_restore: 39936000
  timesteps_this_iter: 96000
  timesteps_total: 41856000
  training_iteration: 436
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 33.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    436 |          57519.4 | 41856000 |  1076.02 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 37
    apples_agent-0_mean: 1.76
    apples_agent-0_min: 0
    apples_agent-1_max: 113
    apples_agent-1_mean: 23.24
    apples_agent-1_min: 0
    apples_agent-2_max: 302
    apples_agent-2_mean: 12.46
    apples_agent-2_min: 0
    apples_agent-3_max: 93
    apples_agent-3_mean: 55.54
    apples_agent-3_min: 32
    apples_agent-4_max: 22
    apples_agent-4_mean: 0.44
    apples_agent-4_min: 0
    apples_agent-5_max: 306
    apples_agent-5_mean: 100.56
    apples_agent-5_min: 42
    cleaning_beam_agent-0_max: 483
    cleaning_beam_agent-0_mean: 377.38
    cleaning_beam_agent-0_min: 231
    cleaning_beam_agent-1_max: 403
    cleaning_beam_agent-1_mean: 272.23
    cleaning_beam_agent-1_min: 122
    cleaning_beam_agent-2_max: 577
    cleaning_beam_agent-2_mean: 366.16
    cleaning_beam_agent-2_min: 174
    cleaning_beam_agent-3_max: 75
    cleaning_beam_agent-3_mean: 13.11
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 553
    cleaning_beam_agent-4_mean: 451.86
    cleaning_beam_agent-4_min: 379
    cleaning_beam_agent-5_max: 115
    cleaning_beam_agent-5_mean: 12.09
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-13-52
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1242.0000000000114
  episode_reward_mean: 1082.259999999992
  episode_reward_min: 716.0000000000057
  episodes_this_iter: 96
  episodes_total: 41952
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20215.994
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9472368955612183
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018667837139219046
        model: {}
        policy_loss: -0.003312790533527732
        total_loss: -0.002847174182534218
        vf_explained_var: 0.02906087040901184
        vf_loss: 21.3275203704834
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1879403591156006
        entropy_coeff: 0.0017600000137463212
        kl: 0.002018023282289505
        model: {}
        policy_loss: -0.0038759326562285423
        total_loss: -0.003766228910535574
        vf_explained_var: 0.003603518009185791
        vf_loss: 22.004779815673828
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.127058506011963
        entropy_coeff: 0.0017600000137463212
        kl: 0.001723693567328155
        model: {}
        policy_loss: -0.0034556626342236996
        total_loss: -0.003230595961213112
        vf_explained_var: -0.0039435625076293945
        vf_loss: 22.086894989013672
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3460645079612732
        entropy_coeff: 0.0017600000137463212
        kl: 0.000982071040198207
        model: {}
        policy_loss: -0.001976937521249056
        total_loss: -0.0005470700562000275
        vf_explained_var: 0.07479552924633026
        vf_loss: 20.389408111572266
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8840239644050598
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016449023969471455
        model: {}
        policy_loss: -0.003996910527348518
        total_loss: -0.003437797073274851
        vf_explained_var: 0.04436647891998291
        vf_loss: 21.149972915649414
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49098557233810425
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008539608679711819
        model: {}
        policy_loss: -0.002372672315686941
        total_loss: -0.0011303620412945747
        vf_explained_var: 0.049331456422805786
        vf_loss: 21.064468383789062
    load_time_ms: 13136.384
    num_steps_sampled: 41952000
    num_steps_trained: 41952000
    sample_time_ms: 91485.49
    update_time_ms: 21.624
  iterations_since_restore: 417
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.194999999999997
    ram_util_percent: 19.366666666666667
  pid: 4061
  policy_reward_max:
    agent-0: 207.00000000000003
    agent-1: 207.00000000000003
    agent-2: 207.00000000000003
    agent-3: 207.00000000000003
    agent-4: 207.00000000000003
    agent-5: 207.00000000000003
  policy_reward_mean:
    agent-0: 180.37666666666652
    agent-1: 180.37666666666652
    agent-2: 180.37666666666652
    agent-3: 180.37666666666652
    agent-4: 180.37666666666652
    agent-5: 180.37666666666652
  policy_reward_min:
    agent-0: 119.33333333333331
    agent-1: 119.33333333333331
    agent-2: 119.33333333333331
    agent-3: 119.33333333333331
    agent-4: 119.33333333333331
    agent-5: 119.33333333333331
  sampler_perf:
    mean_env_wait_ms: 24.435150776786898
    mean_inference_ms: 12.330168034042908
    mean_processing_ms: 50.9305518866614
  time_since_restore: 54434.86135482788
  time_this_iter_s: 126.5252845287323
  time_total_s: 57645.92504096031
  timestamp: 1637072032
  timesteps_since_restore: 40032000
  timesteps_this_iter: 96000
  timesteps_total: 41952000
  training_iteration: 437
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 37.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    437 |          57645.9 | 41952000 |  1082.26 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 95
    apples_agent-0_mean: 3.61
    apples_agent-0_min: 0
    apples_agent-1_max: 96
    apples_agent-1_mean: 17.59
    apples_agent-1_min: 0
    apples_agent-2_max: 222
    apples_agent-2_mean: 12.48
    apples_agent-2_min: 0
    apples_agent-3_max: 155
    apples_agent-3_mean: 56.81
    apples_agent-3_min: 23
    apples_agent-4_max: 18
    apples_agent-4_mean: 0.37
    apples_agent-4_min: 0
    apples_agent-5_max: 306
    apples_agent-5_mean: 100.0
    apples_agent-5_min: 39
    cleaning_beam_agent-0_max: 511
    cleaning_beam_agent-0_mean: 379.66
    cleaning_beam_agent-0_min: 276
    cleaning_beam_agent-1_max: 502
    cleaning_beam_agent-1_mean: 298.63
    cleaning_beam_agent-1_min: 150
    cleaning_beam_agent-2_max: 528
    cleaning_beam_agent-2_mean: 356.54
    cleaning_beam_agent-2_min: 170
    cleaning_beam_agent-3_max: 75
    cleaning_beam_agent-3_mean: 16.3
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 568
    cleaning_beam_agent-4_mean: 460.24
    cleaning_beam_agent-4_min: 363
    cleaning_beam_agent-5_max: 153
    cleaning_beam_agent-5_mean: 14.02
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-15-56
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1242.0000000000014
  episode_reward_mean: 1074.7699999999945
  episode_reward_min: 675.000000000004
  episodes_this_iter: 96
  episodes_total: 42048
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20218.803
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9504098892211914
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015410734340548515
        model: {}
        policy_loss: -0.0029076591599732637
        total_loss: -0.0022807999048382044
        vf_explained_var: 0.0463537722826004
        vf_loss: 22.995777130126953
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1858742237091064
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012544452911242843
        model: {}
        policy_loss: -0.00340855959802866
        total_loss: -0.00313620176166296
        vf_explained_var: 0.02136412262916565
        vf_loss: 23.594982147216797
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1422088146209717
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011910084867849946
        model: {}
        policy_loss: -0.0031816395930945873
        total_loss: -0.0028262389823794365
        vf_explained_var: 0.02008715271949768
        vf_loss: 23.656888961791992
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36094367504119873
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014188819332048297
        model: {}
        policy_loss: -0.002264047972857952
        total_loss: -0.0007579335942864418
        vf_explained_var: 0.11230993270874023
        vf_loss: 21.413745880126953
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8824130296707153
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012001700233668089
        model: {}
        policy_loss: -0.0034130762796849012
        total_loss: -0.0026189791969954967
        vf_explained_var: 0.02827613055706024
        vf_loss: 23.471406936645508
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4968411326408386
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013626444851979613
        model: {}
        policy_loss: -0.00289361085742712
        total_loss: -0.0015579711180180311
        vf_explained_var: 0.08551681041717529
        vf_loss: 22.10077667236328
    load_time_ms: 13132.034
    num_steps_sampled: 42048000
    num_steps_trained: 42048000
    sample_time_ms: 91444.917
    update_time_ms: 21.498
  iterations_since_restore: 418
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.133333333333336
    ram_util_percent: 19.203389830508474
  pid: 4061
  policy_reward_max:
    agent-0: 206.99999999999997
    agent-1: 206.99999999999997
    agent-2: 206.99999999999997
    agent-3: 206.99999999999997
    agent-4: 206.99999999999997
    agent-5: 206.99999999999997
  policy_reward_mean:
    agent-0: 179.1283333333332
    agent-1: 179.1283333333332
    agent-2: 179.1283333333332
    agent-3: 179.1283333333332
    agent-4: 179.1283333333332
    agent-5: 179.1283333333332
  policy_reward_min:
    agent-0: 112.50000000000004
    agent-1: 112.50000000000004
    agent-2: 112.50000000000004
    agent-3: 112.50000000000004
    agent-4: 112.50000000000004
    agent-5: 112.50000000000004
  sampler_perf:
    mean_env_wait_ms: 24.436676969397237
    mean_inference_ms: 12.33024739418273
    mean_processing_ms: 50.93046997837955
  time_since_restore: 54559.10725903511
  time_this_iter_s: 124.24590420722961
  time_total_s: 57770.17094516754
  timestamp: 1637072156
  timesteps_since_restore: 40128000
  timesteps_this_iter: 96000
  timesteps_total: 42048000
  training_iteration: 438
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    438 |          57770.2 | 42048000 |  1074.77 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 41
    apples_agent-0_mean: 1.61
    apples_agent-0_min: 0
    apples_agent-1_max: 66
    apples_agent-1_mean: 19.75
    apples_agent-1_min: 0
    apples_agent-2_max: 222
    apples_agent-2_mean: 15.97
    apples_agent-2_min: 0
    apples_agent-3_max: 103
    apples_agent-3_mean: 56.35
    apples_agent-3_min: 19
    apples_agent-4_max: 50
    apples_agent-4_mean: 1.15
    apples_agent-4_min: 0
    apples_agent-5_max: 306
    apples_agent-5_mean: 98.44
    apples_agent-5_min: 46
    cleaning_beam_agent-0_max: 483
    cleaning_beam_agent-0_mean: 377.36
    cleaning_beam_agent-0_min: 254
    cleaning_beam_agent-1_max: 459
    cleaning_beam_agent-1_mean: 289.23
    cleaning_beam_agent-1_min: 153
    cleaning_beam_agent-2_max: 515
    cleaning_beam_agent-2_mean: 346.77
    cleaning_beam_agent-2_min: 136
    cleaning_beam_agent-3_max: 59
    cleaning_beam_agent-3_mean: 14.53
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 564
    cleaning_beam_agent-4_mean: 452.42
    cleaning_beam_agent-4_min: 237
    cleaning_beam_agent-5_max: 263
    cleaning_beam_agent-5_mean: 14.14
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-18-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1208.0000000000127
  episode_reward_mean: 1061.479999999993
  episode_reward_min: 508.00000000001495
  episodes_this_iter: 96
  episodes_total: 42144
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20208.381
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9539976119995117
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013716459507122636
        model: {}
        policy_loss: -0.002952911425381899
        total_loss: -0.0021905526518821716
        vf_explained_var: 0.030652105808258057
        vf_loss: 24.413963317871094
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1963953971862793
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015341471880674362
        model: {}
        policy_loss: -0.0035398155450820923
        total_loss: -0.003147757612168789
        vf_explained_var: 0.005817800760269165
        vf_loss: 24.977163314819336
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1628648042678833
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021393110509961843
        model: {}
        policy_loss: -0.003589568194001913
        total_loss: -0.0031246738508343697
        vf_explained_var: 0.00023077428340911865
        vf_loss: 25.1153621673584
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3628242611885071
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011938747484236956
        model: {}
        policy_loss: -0.0023665293119847775
        total_loss: -0.0008363192901015282
        vf_explained_var: 0.1368597447872162
        vf_loss: 21.687774658203125
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8855639100074768
        entropy_coeff: 0.0017600000137463212
        kl: 0.001742296852171421
        model: {}
        policy_loss: -0.0041251275688409805
        total_loss: -0.0033488557673990726
        vf_explained_var: 0.07127454876899719
        vf_loss: 23.3486328125
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5154198408126831
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009861080907285213
        model: {}
        policy_loss: -0.0029690111987292767
        total_loss: -0.001686203759163618
        vf_explained_var: 0.13037323951721191
        vf_loss: 21.89948844909668
    load_time_ms: 13145.523
    num_steps_sampled: 42144000
    num_steps_trained: 42144000
    sample_time_ms: 91657.76
    update_time_ms: 21.56
  iterations_since_restore: 419
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.309444444444445
    ram_util_percent: 19.275555555555556
  pid: 4061
  policy_reward_max:
    agent-0: 201.33333333333343
    agent-1: 201.33333333333343
    agent-2: 201.33333333333343
    agent-3: 201.33333333333343
    agent-4: 201.33333333333343
    agent-5: 201.33333333333343
  policy_reward_mean:
    agent-0: 176.9133333333331
    agent-1: 176.9133333333331
    agent-2: 176.9133333333331
    agent-3: 176.9133333333331
    agent-4: 176.9133333333331
    agent-5: 176.9133333333331
  policy_reward_min:
    agent-0: 84.66666666666676
    agent-1: 84.66666666666676
    agent-2: 84.66666666666676
    agent-3: 84.66666666666676
    agent-4: 84.66666666666676
    agent-5: 84.66666666666676
  sampler_perf:
    mean_env_wait_ms: 24.438826600988346
    mean_inference_ms: 12.330740391462031
    mean_processing_ms: 50.93196919349317
  time_since_restore: 54685.55652952194
  time_this_iter_s: 126.44927048683167
  time_total_s: 57896.62021565437
  timestamp: 1637072283
  timesteps_since_restore: 40224000
  timesteps_this_iter: 96000
  timesteps_total: 42144000
  training_iteration: 439
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 35.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    439 |          57896.6 | 42144000 |  1061.48 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 33
    apples_agent-0_mean: 1.14
    apples_agent-0_min: 0
    apples_agent-1_max: 98
    apples_agent-1_mean: 18.69
    apples_agent-1_min: 0
    apples_agent-2_max: 185
    apples_agent-2_mean: 15.88
    apples_agent-2_min: 0
    apples_agent-3_max: 109
    apples_agent-3_mean: 56.09
    apples_agent-3_min: 21
    apples_agent-4_max: 39
    apples_agent-4_mean: 0.39
    apples_agent-4_min: 0
    apples_agent-5_max: 233
    apples_agent-5_mean: 101.99
    apples_agent-5_min: 60
    cleaning_beam_agent-0_max: 464
    cleaning_beam_agent-0_mean: 367.09
    cleaning_beam_agent-0_min: 277
    cleaning_beam_agent-1_max: 434
    cleaning_beam_agent-1_mean: 286.84
    cleaning_beam_agent-1_min: 128
    cleaning_beam_agent-2_max: 499
    cleaning_beam_agent-2_mean: 329.36
    cleaning_beam_agent-2_min: 142
    cleaning_beam_agent-3_max: 91
    cleaning_beam_agent-3_mean: 17.14
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 551
    cleaning_beam_agent-4_mean: 453.95
    cleaning_beam_agent-4_min: 352
    cleaning_beam_agent-5_max: 132
    cleaning_beam_agent-5_mean: 13.28
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-20-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1236.9999999999998
  episode_reward_mean: 1069.519999999993
  episode_reward_min: 780.9999999999967
  episodes_this_iter: 96
  episodes_total: 42240
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20219.447
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9588867425918579
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022127022966742516
        model: {}
        policy_loss: -0.003379917936399579
        total_loss: -0.0027835615910589695
        vf_explained_var: -0.005976736545562744
        vf_loss: 22.839962005615234
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1770292520523071
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016320075374096632
        model: {}
        policy_loss: -0.003273208625614643
        total_loss: -0.0031348243355751038
        vf_explained_var: 0.027230307459831238
        vf_loss: 22.099586486816406
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.157282829284668
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011212615063413978
        model: {}
        policy_loss: -0.0032726393546909094
        total_loss: -0.003119222354143858
        vf_explained_var: 0.036523088812828064
        vf_loss: 21.902374267578125
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35573211312294006
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009832705836743116
        model: {}
        policy_loss: -0.0023680459707975388
        total_loss: -0.0009970329701900482
        vf_explained_var: 0.12003540992736816
        vf_loss: 19.97100067138672
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8789432644844055
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018198293400928378
        model: {}
        policy_loss: -0.003534328890964389
        total_loss: -0.002881717635318637
        vf_explained_var: 0.030566826462745667
        vf_loss: 21.995515823364258
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5261285901069641
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008650639210827649
        model: {}
        policy_loss: -0.0029186848551034927
        total_loss: -0.0017701787874102592
        vf_explained_var: 0.09069937467575073
        vf_loss: 20.744903564453125
    load_time_ms: 13185.001
    num_steps_sampled: 42240000
    num_steps_trained: 42240000
    sample_time_ms: 91777.748
    update_time_ms: 21.464
  iterations_since_restore: 420
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.150279329608935
    ram_util_percent: 19.09441340782123
  pid: 4061
  policy_reward_max:
    agent-0: 206.16666666666663
    agent-1: 206.16666666666663
    agent-2: 206.16666666666663
    agent-3: 206.16666666666663
    agent-4: 206.16666666666663
    agent-5: 206.16666666666663
  policy_reward_mean:
    agent-0: 178.25333333333307
    agent-1: 178.25333333333307
    agent-2: 178.25333333333307
    agent-3: 178.25333333333307
    agent-4: 178.25333333333307
    agent-5: 178.25333333333307
  policy_reward_min:
    agent-0: 130.16666666666683
    agent-1: 130.16666666666683
    agent-2: 130.16666666666683
    agent-3: 130.16666666666683
    agent-4: 130.16666666666683
    agent-5: 130.16666666666683
  sampler_perf:
    mean_env_wait_ms: 24.439945606812458
    mean_inference_ms: 12.330942198734624
    mean_processing_ms: 50.93254610379122
  time_since_restore: 54811.11998081207
  time_this_iter_s: 125.56345129013062
  time_total_s: 58022.183666944504
  timestamp: 1637072409
  timesteps_since_restore: 40320000
  timesteps_this_iter: 96000
  timesteps_total: 42240000
  training_iteration: 440
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 35.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    440 |          58022.2 | 42240000 |  1069.52 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 46
    apples_agent-0_mean: 1.29
    apples_agent-0_min: 0
    apples_agent-1_max: 96
    apples_agent-1_mean: 20.56
    apples_agent-1_min: 0
    apples_agent-2_max: 94
    apples_agent-2_mean: 11.34
    apples_agent-2_min: 0
    apples_agent-3_max: 102
    apples_agent-3_mean: 55.53
    apples_agent-3_min: 27
    apples_agent-4_max: 39
    apples_agent-4_mean: 0.39
    apples_agent-4_min: 0
    apples_agent-5_max: 201
    apples_agent-5_mean: 99.58
    apples_agent-5_min: 48
    cleaning_beam_agent-0_max: 476
    cleaning_beam_agent-0_mean: 379.64
    cleaning_beam_agent-0_min: 276
    cleaning_beam_agent-1_max: 479
    cleaning_beam_agent-1_mean: 293.05
    cleaning_beam_agent-1_min: 162
    cleaning_beam_agent-2_max: 525
    cleaning_beam_agent-2_mean: 348.23
    cleaning_beam_agent-2_min: 142
    cleaning_beam_agent-3_max: 64
    cleaning_beam_agent-3_mean: 14.35
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 535
    cleaning_beam_agent-4_mean: 459.83
    cleaning_beam_agent-4_min: 371
    cleaning_beam_agent-5_max: 160
    cleaning_beam_agent-5_mean: 11.98
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-22-15
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1261.0000000000007
  episode_reward_mean: 1082.279999999993
  episode_reward_min: 661.0000000000014
  episodes_this_iter: 96
  episodes_total: 42336
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20185.87
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9396023154258728
        entropy_coeff: 0.0017600000137463212
        kl: 0.001240844838321209
        model: {}
        policy_loss: -0.002974972827360034
        total_loss: -0.002453649416565895
        vf_explained_var: 0.01583920419216156
        vf_loss: 21.750225067138672
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.180145025253296
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015321659157052636
        model: {}
        policy_loss: -0.0038275434635579586
        total_loss: -0.003678428940474987
        vf_explained_var: 0.00030243396759033203
        vf_loss: 22.261688232421875
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1387380361557007
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009090006351470947
        model: {}
        policy_loss: -0.003093119477853179
        total_loss: -0.002892657183110714
        vf_explained_var: 0.005460590124130249
        vf_loss: 22.046417236328125
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3453342318534851
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011842494131997228
        model: {}
        policy_loss: -0.0020522342529147863
        total_loss: -0.0006525909993797541
        vf_explained_var: 0.09605467319488525
        vf_loss: 20.074310302734375
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8765870332717896
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017170821083709598
        model: {}
        policy_loss: -0.0036441306583583355
        total_loss: -0.002989842090755701
        vf_explained_var: 0.013797372579574585
        vf_loss: 21.970787048339844
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5052511096000671
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007189278258010745
        model: {}
        policy_loss: -0.0025337282568216324
        total_loss: -0.0013394677080214024
        vf_explained_var: 0.06684635579586029
        vf_loss: 20.835044860839844
    load_time_ms: 13180.671
    num_steps_sampled: 42336000
    num_steps_trained: 42336000
    sample_time_ms: 91874.794
    update_time_ms: 21.246
  iterations_since_restore: 421
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.84722222222222
    ram_util_percent: 19.05777777777778
  pid: 4061
  policy_reward_max:
    agent-0: 210.16666666666677
    agent-1: 210.16666666666677
    agent-2: 210.16666666666677
    agent-3: 210.16666666666677
    agent-4: 210.16666666666677
    agent-5: 210.16666666666677
  policy_reward_mean:
    agent-0: 180.37999999999985
    agent-1: 180.37999999999985
    agent-2: 180.37999999999985
    agent-3: 180.37999999999985
    agent-4: 180.37999999999985
    agent-5: 180.37999999999985
  policy_reward_min:
    agent-0: 110.1666666666669
    agent-1: 110.1666666666669
    agent-2: 110.1666666666669
    agent-3: 110.1666666666669
    agent-4: 110.1666666666669
    agent-5: 110.1666666666669
  sampler_perf:
    mean_env_wait_ms: 24.441626172357196
    mean_inference_ms: 12.331220642062892
    mean_processing_ms: 50.93316803389423
  time_since_restore: 54935.62856435776
  time_this_iter_s: 124.50858354568481
  time_total_s: 58146.69225049019
  timestamp: 1637072535
  timesteps_since_restore: 40416000
  timesteps_this_iter: 96000
  timesteps_total: 42336000
  training_iteration: 441
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    441 |          58146.7 | 42336000 |  1082.28 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 50
    apples_agent-0_mean: 2.55
    apples_agent-0_min: 0
    apples_agent-1_max: 79
    apples_agent-1_mean: 21.43
    apples_agent-1_min: 0
    apples_agent-2_max: 158
    apples_agent-2_mean: 11.13
    apples_agent-2_min: 0
    apples_agent-3_max: 96
    apples_agent-3_mean: 56.83
    apples_agent-3_min: 27
    apples_agent-4_max: 68
    apples_agent-4_mean: 1.18
    apples_agent-4_min: 0
    apples_agent-5_max: 144
    apples_agent-5_mean: 96.87
    apples_agent-5_min: 50
    cleaning_beam_agent-0_max: 473
    cleaning_beam_agent-0_mean: 375.51
    cleaning_beam_agent-0_min: 235
    cleaning_beam_agent-1_max: 479
    cleaning_beam_agent-1_mean: 289.33
    cleaning_beam_agent-1_min: 142
    cleaning_beam_agent-2_max: 527
    cleaning_beam_agent-2_mean: 344.36
    cleaning_beam_agent-2_min: 134
    cleaning_beam_agent-3_max: 59
    cleaning_beam_agent-3_mean: 15.28
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 554
    cleaning_beam_agent-4_mean: 446.09
    cleaning_beam_agent-4_min: 309
    cleaning_beam_agent-5_max: 222
    cleaning_beam_agent-5_mean: 12.4
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-24-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1265.0000000000207
  episode_reward_mean: 1064.1899999999923
  episode_reward_min: 497.0000000000167
  episodes_this_iter: 96
  episodes_total: 42432
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20203.448
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9433822631835938
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012307061115279794
        model: {}
        policy_loss: -0.002844218397513032
        total_loss: -0.002296831924468279
        vf_explained_var: 0.0508221834897995
        vf_loss: 22.077404022216797
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1921495199203491
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024875961244106293
        model: {}
        policy_loss: -0.004025907721370459
        total_loss: -0.0037665371783077717
        vf_explained_var: -0.014773458242416382
        vf_loss: 23.57552719116211
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.140650749206543
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018100317101925611
        model: {}
        policy_loss: -0.003449545241892338
        total_loss: -0.0031225578859448433
        vf_explained_var: -0.005127757787704468
        vf_loss: 23.345338821411133
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3570227026939392
        entropy_coeff: 0.0017600000137463212
        kl: 0.00106626667547971
        model: {}
        policy_loss: -0.002190683037042618
        total_loss: -0.0007193316705524921
        vf_explained_var: 0.09615777432918549
        vf_loss: 20.997093200683594
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.884721577167511
        entropy_coeff: 0.0017600000137463212
        kl: 0.0031090709380805492
        model: {}
        policy_loss: -0.0042505282908678055
        total_loss: -0.003562638536095619
        vf_explained_var: 0.032830610871315
        vf_loss: 22.45004653930664
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.523373544216156
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013452633284032345
        model: {}
        policy_loss: -0.0028297980315983295
        total_loss: -0.0015830476768314838
        vf_explained_var: 0.06767350435256958
        vf_loss: 21.67886734008789
    load_time_ms: 13199.15
    num_steps_sampled: 42432000
    num_steps_trained: 42432000
    sample_time_ms: 91860.148
    update_time_ms: 21.24
  iterations_since_restore: 422
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.20561797752809
    ram_util_percent: 19.080337078651688
  pid: 4061
  policy_reward_max:
    agent-0: 210.83333333333323
    agent-1: 210.83333333333323
    agent-2: 210.83333333333323
    agent-3: 210.83333333333323
    agent-4: 210.83333333333323
    agent-5: 210.83333333333323
  policy_reward_mean:
    agent-0: 177.3649999999998
    agent-1: 177.3649999999998
    agent-2: 177.3649999999998
    agent-3: 177.3649999999998
    agent-4: 177.3649999999998
    agent-5: 177.3649999999998
  policy_reward_min:
    agent-0: 82.83333333333353
    agent-1: 82.83333333333353
    agent-2: 82.83333333333353
    agent-3: 82.83333333333353
    agent-4: 82.83333333333353
    agent-5: 82.83333333333353
  sampler_perf:
    mean_env_wait_ms: 24.442513422813704
    mean_inference_ms: 12.3311044718526
    mean_processing_ms: 50.93257971655648
  time_since_restore: 55059.968896865845
  time_this_iter_s: 124.34033250808716
  time_total_s: 58271.032582998276
  timestamp: 1637072660
  timesteps_since_restore: 40512000
  timesteps_this_iter: 96000
  timesteps_total: 42432000
  training_iteration: 442
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 34.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    442 |            58271 | 42432000 |  1064.19 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 35
    apples_agent-0_mean: 1.35
    apples_agent-0_min: 0
    apples_agent-1_max: 80
    apples_agent-1_mean: 21.91
    apples_agent-1_min: 0
    apples_agent-2_max: 336
    apples_agent-2_mean: 14.16
    apples_agent-2_min: 0
    apples_agent-3_max: 170
    apples_agent-3_mean: 57.09
    apples_agent-3_min: 27
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 358
    apples_agent-5_mean: 102.23
    apples_agent-5_min: 52
    cleaning_beam_agent-0_max: 475
    cleaning_beam_agent-0_mean: 384.72
    cleaning_beam_agent-0_min: 295
    cleaning_beam_agent-1_max: 432
    cleaning_beam_agent-1_mean: 277.71
    cleaning_beam_agent-1_min: 54
    cleaning_beam_agent-2_max: 529
    cleaning_beam_agent-2_mean: 331.12
    cleaning_beam_agent-2_min: 144
    cleaning_beam_agent-3_max: 56
    cleaning_beam_agent-3_mean: 14.94
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 528
    cleaning_beam_agent-4_mean: 457.5
    cleaning_beam_agent-4_min: 344
    cleaning_beam_agent-5_max: 188
    cleaning_beam_agent-5_mean: 13.88
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-26-25
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1261.9999999999889
  episode_reward_mean: 1078.1399999999926
  episode_reward_min: 718.999999999996
  episodes_this_iter: 96
  episodes_total: 42528
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20217.294
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9353929758071899
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018600285984575748
        model: {}
        policy_loss: -0.0031385840848088264
        total_loss: -0.002632689429447055
        vf_explained_var: 0.03641277551651001
        vf_loss: 21.521867752075195
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1760454177856445
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011441324604675174
        model: {}
        policy_loss: -0.003485141322016716
        total_loss: -0.0033116014674305916
        vf_explained_var: -0.0007783621549606323
        vf_loss: 22.43381690979004
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1539244651794434
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012692142045125365
        model: {}
        policy_loss: -0.0030745421536266804
        total_loss: -0.0028335340321063995
        vf_explained_var: -0.01637992262840271
        vf_loss: 22.719158172607422
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3440001904964447
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010125448461622
        model: {}
        policy_loss: -0.001973310485482216
        total_loss: -0.0005051875486969948
        vf_explained_var: 0.07320085167884827
        vf_loss: 20.735618591308594
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8901729583740234
        entropy_coeff: 0.0017600000137463212
        kl: 0.001706975162960589
        model: {}
        policy_loss: -0.0036192191764712334
        total_loss: -0.002945658750832081
        vf_explained_var: -0.001699984073638916
        vf_loss: 22.402650833129883
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5163211822509766
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010070903226733208
        model: {}
        policy_loss: -0.0027905902825295925
        total_loss: -0.0016383208567276597
        vf_explained_var: 0.08303341269493103
        vf_loss: 20.609954833984375
    load_time_ms: 13185.967
    num_steps_sampled: 42528000
    num_steps_trained: 42528000
    sample_time_ms: 91871.693
    update_time_ms: 22.225
  iterations_since_restore: 423
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.828651685393258
    ram_util_percent: 18.391573033707864
  pid: 4061
  policy_reward_max:
    agent-0: 210.3333333333332
    agent-1: 210.3333333333332
    agent-2: 210.3333333333332
    agent-3: 210.3333333333332
    agent-4: 210.3333333333332
    agent-5: 210.3333333333332
  policy_reward_mean:
    agent-0: 179.68999999999983
    agent-1: 179.68999999999983
    agent-2: 179.68999999999983
    agent-3: 179.68999999999983
    agent-4: 179.68999999999983
    agent-5: 179.68999999999983
  policy_reward_min:
    agent-0: 119.83333333333351
    agent-1: 119.83333333333351
    agent-2: 119.83333333333351
    agent-3: 119.83333333333351
    agent-4: 119.83333333333351
    agent-5: 119.83333333333351
  sampler_perf:
    mean_env_wait_ms: 24.44328841872206
    mean_inference_ms: 12.33120710053984
    mean_processing_ms: 50.93275061948266
  time_since_restore: 55184.885489702225
  time_this_iter_s: 124.91659283638
  time_total_s: 58395.949175834656
  timestamp: 1637072785
  timesteps_since_restore: 40608000
  timesteps_this_iter: 96000
  timesteps_total: 42528000
  training_iteration: 443
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    443 |          58395.9 | 42528000 |  1078.14 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 65
    apples_agent-0_mean: 2.06
    apples_agent-0_min: 0
    apples_agent-1_max: 106
    apples_agent-1_mean: 23.25
    apples_agent-1_min: 0
    apples_agent-2_max: 98
    apples_agent-2_mean: 13.15
    apples_agent-2_min: 0
    apples_agent-3_max: 89
    apples_agent-3_mean: 56.79
    apples_agent-3_min: 29
    apples_agent-4_max: 56
    apples_agent-4_mean: 1.76
    apples_agent-4_min: 0
    apples_agent-5_max: 149
    apples_agent-5_mean: 99.55
    apples_agent-5_min: 48
    cleaning_beam_agent-0_max: 502
    cleaning_beam_agent-0_mean: 396.31
    cleaning_beam_agent-0_min: 293
    cleaning_beam_agent-1_max: 457
    cleaning_beam_agent-1_mean: 279.7
    cleaning_beam_agent-1_min: 134
    cleaning_beam_agent-2_max: 475
    cleaning_beam_agent-2_mean: 329.13
    cleaning_beam_agent-2_min: 166
    cleaning_beam_agent-3_max: 71
    cleaning_beam_agent-3_mean: 16.89
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 553
    cleaning_beam_agent-4_mean: 465.6
    cleaning_beam_agent-4_min: 324
    cleaning_beam_agent-5_max: 224
    cleaning_beam_agent-5_mean: 14.42
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-28-30
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1242.000000000017
  episode_reward_mean: 1059.8299999999924
  episode_reward_min: 528.0000000000086
  episodes_this_iter: 96
  episodes_total: 42624
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20217.213
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9232134819030762
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022707993630319834
        model: {}
        policy_loss: -0.0030331681482493877
        total_loss: -0.002329729264602065
        vf_explained_var: 0.043847620487213135
        vf_loss: 23.282943725585938
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1839643716812134
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014366222312673926
        model: {}
        policy_loss: -0.0033754785545170307
        total_loss: -0.0030153640545904636
        vf_explained_var: -0.004187420010566711
        vf_loss: 24.43889617919922
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1525686979293823
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010506469989195466
        model: {}
        policy_loss: -0.00329104857519269
        total_loss: -0.002801122609525919
        vf_explained_var: -0.03552842140197754
        vf_loss: 25.18445587158203
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3624407649040222
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007143463590182364
        model: {}
        policy_loss: -0.00209313677623868
        total_loss: -0.0005546249449253082
        vf_explained_var: 0.10528038442134857
        vf_loss: 21.76409149169922
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8772463798522949
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021408305037766695
        model: {}
        policy_loss: -0.003595837391912937
        total_loss: -0.002832249738276005
        vf_explained_var: 0.05279594659805298
        vf_loss: 23.075407028198242
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5303796529769897
        entropy_coeff: 0.0017600000137463212
        kl: 0.000696660834364593
        model: {}
        policy_loss: -0.0026575997471809387
        total_loss: -0.001456183847039938
        vf_explained_var: 0.12524056434631348
        vf_loss: 21.348846435546875
    load_time_ms: 13179.559
    num_steps_sampled: 42624000
    num_steps_trained: 42624000
    sample_time_ms: 91962.368
    update_time_ms: 21.634
  iterations_since_restore: 424
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.2123595505618
    ram_util_percent: 19.223033707865167
  pid: 4061
  policy_reward_max:
    agent-0: 206.99999999999972
    agent-1: 206.99999999999972
    agent-2: 206.99999999999972
    agent-3: 206.99999999999972
    agent-4: 206.99999999999972
    agent-5: 206.99999999999972
  policy_reward_mean:
    agent-0: 176.63833333333318
    agent-1: 176.63833333333318
    agent-2: 176.63833333333318
    agent-3: 176.63833333333318
    agent-4: 176.63833333333318
    agent-5: 176.63833333333318
  policy_reward_min:
    agent-0: 88.00000000000036
    agent-1: 88.00000000000036
    agent-2: 88.00000000000036
    agent-3: 88.00000000000036
    agent-4: 88.00000000000036
    agent-5: 88.00000000000036
  sampler_perf:
    mean_env_wait_ms: 24.44505142380739
    mean_inference_ms: 12.331332686419314
    mean_processing_ms: 50.93397194697488
  time_since_restore: 55310.0841319561
  time_this_iter_s: 125.19864225387573
  time_total_s: 58521.14781808853
  timestamp: 1637072910
  timesteps_since_restore: 40704000
  timesteps_this_iter: 96000
  timesteps_total: 42624000
  training_iteration: 444
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    444 |          58521.1 | 42624000 |  1059.83 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 42
    apples_agent-0_mean: 1.6
    apples_agent-0_min: 0
    apples_agent-1_max: 90
    apples_agent-1_mean: 18.27
    apples_agent-1_min: 0
    apples_agent-2_max: 121
    apples_agent-2_mean: 14.32
    apples_agent-2_min: 0
    apples_agent-3_max: 118
    apples_agent-3_mean: 52.47
    apples_agent-3_min: 28
    apples_agent-4_max: 35
    apples_agent-4_mean: 0.35
    apples_agent-4_min: 0
    apples_agent-5_max: 310
    apples_agent-5_mean: 100.0
    apples_agent-5_min: 36
    cleaning_beam_agent-0_max: 502
    cleaning_beam_agent-0_mean: 393.63
    cleaning_beam_agent-0_min: 215
    cleaning_beam_agent-1_max: 497
    cleaning_beam_agent-1_mean: 289.4
    cleaning_beam_agent-1_min: 125
    cleaning_beam_agent-2_max: 582
    cleaning_beam_agent-2_mean: 340.07
    cleaning_beam_agent-2_min: 135
    cleaning_beam_agent-3_max: 55
    cleaning_beam_agent-3_mean: 15.23
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 544
    cleaning_beam_agent-4_mean: 472.42
    cleaning_beam_agent-4_min: 347
    cleaning_beam_agent-5_max: 255
    cleaning_beam_agent-5_mean: 14.75
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.04
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-30-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1268.99999999999
  episode_reward_mean: 1080.1599999999928
  episode_reward_min: 360.0000000000069
  episodes_this_iter: 96
  episodes_total: 42720
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20258.552
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9208252429962158
        entropy_coeff: 0.0017600000137463212
        kl: 0.001388705801218748
        model: {}
        policy_loss: -0.0029915859922766685
        total_loss: -0.002296043559908867
        vf_explained_var: 0.06205940246582031
        vf_loss: 23.161949157714844
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1759549379348755
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017870027804747224
        model: {}
        policy_loss: -0.0036399001255631447
        total_loss: -0.0032497942447662354
        vf_explained_var: 0.009800553321838379
        vf_loss: 24.59786605834961
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1427198648452759
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013998295180499554
        model: {}
        policy_loss: -0.0037152674049139023
        total_loss: -0.0032335780560970306
        vf_explained_var: -0.007900744676589966
        vf_loss: 24.928796768188477
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35521256923675537
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008300138870254159
        model: {}
        policy_loss: -0.001970963552594185
        total_loss: -0.0003873351961374283
        vf_explained_var: 0.10714103281497955
        vf_loss: 22.088035583496094
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.881938099861145
        entropy_coeff: 0.0017600000137463212
        kl: 0.00210306397639215
        model: {}
        policy_loss: -0.0039095766842365265
        total_loss: -0.00313912914134562
        vf_explained_var: 0.060513436794281006
        vf_loss: 23.226537704467773
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5086634755134583
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009751659817993641
        model: {}
        policy_loss: -0.002905868459492922
        total_loss: -0.001549920067191124
        vf_explained_var: 0.0955047458410263
        vf_loss: 22.511987686157227
    load_time_ms: 13162.801
    num_steps_sampled: 42720000
    num_steps_trained: 42720000
    sample_time_ms: 91874.116
    update_time_ms: 21.326
  iterations_since_restore: 425
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.90279329608938
    ram_util_percent: 19.018435754189944
  pid: 4061
  policy_reward_max:
    agent-0: 211.49999999999997
    agent-1: 211.49999999999997
    agent-2: 211.49999999999997
    agent-3: 211.49999999999997
    agent-4: 211.49999999999997
    agent-5: 211.49999999999997
  policy_reward_mean:
    agent-0: 180.02666666666647
    agent-1: 180.02666666666647
    agent-2: 180.02666666666647
    agent-3: 180.02666666666647
    agent-4: 180.02666666666647
    agent-5: 180.02666666666647
  policy_reward_min:
    agent-0: 59.99999999999986
    agent-1: 59.99999999999986
    agent-2: 59.99999999999986
    agent-3: 59.99999999999986
    agent-4: 59.99999999999986
    agent-5: 59.99999999999986
  sampler_perf:
    mean_env_wait_ms: 24.4467819693644
    mean_inference_ms: 12.33135916667721
    mean_processing_ms: 50.93411838777022
  time_since_restore: 55434.787257671356
  time_this_iter_s: 124.70312571525574
  time_total_s: 58645.85094380379
  timestamp: 1637073036
  timesteps_since_restore: 40800000
  timesteps_this_iter: 96000
  timesteps_total: 42720000
  training_iteration: 445
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 33.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    445 |          58645.9 | 42720000 |  1080.16 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 60
    apples_agent-0_mean: 2.29
    apples_agent-0_min: 0
    apples_agent-1_max: 75
    apples_agent-1_mean: 20.8
    apples_agent-1_min: 0
    apples_agent-2_max: 116
    apples_agent-2_mean: 15.86
    apples_agent-2_min: 0
    apples_agent-3_max: 103
    apples_agent-3_mean: 56.36
    apples_agent-3_min: 29
    apples_agent-4_max: 122
    apples_agent-4_mean: 2.34
    apples_agent-4_min: 0
    apples_agent-5_max: 206
    apples_agent-5_mean: 97.62
    apples_agent-5_min: 16
    cleaning_beam_agent-0_max: 490
    cleaning_beam_agent-0_mean: 391.11
    cleaning_beam_agent-0_min: 215
    cleaning_beam_agent-1_max: 439
    cleaning_beam_agent-1_mean: 275.55
    cleaning_beam_agent-1_min: 167
    cleaning_beam_agent-2_max: 477
    cleaning_beam_agent-2_mean: 322.03
    cleaning_beam_agent-2_min: 131
    cleaning_beam_agent-3_max: 47
    cleaning_beam_agent-3_mean: 16.35
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 561
    cleaning_beam_agent-4_mean: 478.8
    cleaning_beam_agent-4_min: 322
    cleaning_beam_agent-5_max: 470
    cleaning_beam_agent-5_mean: 20.51
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-32-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1248.9999999999804
  episode_reward_mean: 1059.0399999999913
  episode_reward_min: 572.0000000000038
  episodes_this_iter: 96
  episodes_total: 42816
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20267.462
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9107087254524231
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013099804054945707
        model: {}
        policy_loss: -0.0027485033497214317
        total_loss: -0.0019826022908091545
        vf_explained_var: 0.040795087814331055
        vf_loss: 23.687471389770508
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1687403917312622
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014439444057643414
        model: {}
        policy_loss: -0.0037025909405201674
        total_loss: -0.0033685441594570875
        vf_explained_var: 0.03018869459629059
        vf_loss: 23.91029930114746
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1426124572753906
        entropy_coeff: 0.0017600000137463212
        kl: 0.001227378030307591
        model: {}
        policy_loss: -0.003844865132123232
        total_loss: -0.003493832889944315
        vf_explained_var: 0.041576534509658813
        vf_loss: 23.62030029296875
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36676907539367676
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009386279270984232
        model: {}
        policy_loss: -0.001978701213374734
        total_loss: -0.00040863524191081524
        vf_explained_var: 0.10162682831287384
        vf_loss: 22.155778884887695
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8704302310943604
        entropy_coeff: 0.0017600000137463212
        kl: 0.00162359734531492
        model: {}
        policy_loss: -0.003769742790609598
        total_loss: -0.003001276170834899
        vf_explained_var: 0.06727573275566101
        vf_loss: 23.00421905517578
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.516060471534729
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010121562518179417
        model: {}
        policy_loss: -0.0030219298787415028
        total_loss: -0.0017497963272035122
        vf_explained_var: 0.11757226288318634
        vf_loss: 21.803983688354492
    load_time_ms: 13155.666
    num_steps_sampled: 42816000
    num_steps_trained: 42816000
    sample_time_ms: 91763.291
    update_time_ms: 21.604
  iterations_since_restore: 426
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.717222222222226
    ram_util_percent: 18.647777777777776
  pid: 4061
  policy_reward_max:
    agent-0: 208.1666666666666
    agent-1: 208.1666666666666
    agent-2: 208.1666666666666
    agent-3: 208.1666666666666
    agent-4: 208.1666666666666
    agent-5: 208.1666666666666
  policy_reward_mean:
    agent-0: 176.50666666666655
    agent-1: 176.50666666666655
    agent-2: 176.50666666666655
    agent-3: 176.50666666666655
    agent-4: 176.50666666666655
    agent-5: 176.50666666666655
  policy_reward_min:
    agent-0: 95.33333333333367
    agent-1: 95.33333333333367
    agent-2: 95.33333333333367
    agent-3: 95.33333333333367
    agent-4: 95.33333333333367
    agent-5: 95.33333333333367
  sampler_perf:
    mean_env_wait_ms: 24.4491736605856
    mean_inference_ms: 12.331877003138883
    mean_processing_ms: 50.936698709918254
  time_since_restore: 55561.35489654541
  time_this_iter_s: 126.56763887405396
  time_total_s: 58772.41858267784
  timestamp: 1637073162
  timesteps_since_restore: 40896000
  timesteps_this_iter: 96000
  timesteps_total: 42816000
  training_iteration: 446
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    446 |          58772.4 | 42816000 |  1059.04 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 38
    apples_agent-0_mean: 2.02
    apples_agent-0_min: 0
    apples_agent-1_max: 104
    apples_agent-1_mean: 21.66
    apples_agent-1_min: 0
    apples_agent-2_max: 93
    apples_agent-2_mean: 12.37
    apples_agent-2_min: 0
    apples_agent-3_max: 87
    apples_agent-3_mean: 54.45
    apples_agent-3_min: 28
    apples_agent-4_max: 62
    apples_agent-4_mean: 0.86
    apples_agent-4_min: 0
    apples_agent-5_max: 142
    apples_agent-5_mean: 98.06
    apples_agent-5_min: 42
    cleaning_beam_agent-0_max: 470
    cleaning_beam_agent-0_mean: 391.12
    cleaning_beam_agent-0_min: 291
    cleaning_beam_agent-1_max: 412
    cleaning_beam_agent-1_mean: 286.61
    cleaning_beam_agent-1_min: 111
    cleaning_beam_agent-2_max: 522
    cleaning_beam_agent-2_mean: 346.09
    cleaning_beam_agent-2_min: 163
    cleaning_beam_agent-3_max: 91
    cleaning_beam_agent-3_mean: 15.35
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 558
    cleaning_beam_agent-4_mean: 471.58
    cleaning_beam_agent-4_min: 324
    cleaning_beam_agent-5_max: 203
    cleaning_beam_agent-5_mean: 17.77
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-34-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1233.9999999999934
  episode_reward_mean: 1079.649999999994
  episode_reward_min: 743.9999999999953
  episodes_this_iter: 96
  episodes_total: 42912
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20286.724
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9226015210151672
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018025345634669065
        model: {}
        policy_loss: -0.0032892320305109024
        total_loss: -0.00269439909607172
        vf_explained_var: 0.04038895666599274
        vf_loss: 22.18610191345215
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1879605054855347
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015391591005027294
        model: {}
        policy_loss: -0.0035683850292116404
        total_loss: -0.0033680712804198265
        vf_explained_var: 0.017428219318389893
        vf_loss: 22.911235809326172
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.140778660774231
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012320470996201038
        model: {}
        policy_loss: -0.0033199158497154713
        total_loss: -0.003028213046491146
        vf_explained_var: 0.006009340286254883
        vf_loss: 22.9947566986084
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3525272607803345
        entropy_coeff: 0.0017600000137463212
        kl: 0.000830970355309546
        model: {}
        policy_loss: -0.001783682033419609
        total_loss: -0.00031152740120887756
        vf_explained_var: 0.09680555760860443
        vf_loss: 20.926025390625
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8761065602302551
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013842255575582385
        model: {}
        policy_loss: -0.0034646964631974697
        total_loss: -0.002815800718963146
        vf_explained_var: 0.05346471071243286
        vf_loss: 21.908437728881836
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5065685510635376
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006799554103054106
        model: {}
        policy_loss: -0.0026738853193819523
        total_loss: -0.0014395553153008223
        vf_explained_var: 0.08655259013175964
        vf_loss: 21.25892448425293
    load_time_ms: 13178.494
    num_steps_sampled: 42912000
    num_steps_trained: 42912000
    sample_time_ms: 91545.342
    update_time_ms: 21.827
  iterations_since_restore: 427
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.405617977528088
    ram_util_percent: 20.165168539325844
  pid: 4061
  policy_reward_max:
    agent-0: 205.66666666666666
    agent-1: 205.66666666666666
    agent-2: 205.66666666666666
    agent-3: 205.66666666666666
    agent-4: 205.66666666666666
    agent-5: 205.66666666666666
  policy_reward_mean:
    agent-0: 179.94166666666652
    agent-1: 179.94166666666652
    agent-2: 179.94166666666652
    agent-3: 179.94166666666652
    agent-4: 179.94166666666652
    agent-5: 179.94166666666652
  policy_reward_min:
    agent-0: 124.00000000000041
    agent-1: 124.00000000000041
    agent-2: 124.00000000000041
    agent-3: 124.00000000000041
    agent-4: 124.00000000000041
    agent-5: 124.00000000000041
  sampler_perf:
    mean_env_wait_ms: 24.450563683552073
    mean_inference_ms: 12.331931922507605
    mean_processing_ms: 50.93736496829163
  time_since_restore: 55686.14498972893
  time_this_iter_s: 124.79009318351746
  time_total_s: 58897.20867586136
  timestamp: 1637073287
  timesteps_since_restore: 40992000
  timesteps_this_iter: 96000
  timesteps_total: 42912000
  training_iteration: 447
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    447 |          58897.2 | 42912000 |  1079.65 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 21
    apples_agent-0_mean: 0.91
    apples_agent-0_min: 0
    apples_agent-1_max: 81
    apples_agent-1_mean: 23.1
    apples_agent-1_min: 0
    apples_agent-2_max: 227
    apples_agent-2_mean: 17.1
    apples_agent-2_min: 0
    apples_agent-3_max: 112
    apples_agent-3_mean: 57.3
    apples_agent-3_min: 37
    apples_agent-4_max: 10
    apples_agent-4_mean: 0.1
    apples_agent-4_min: 0
    apples_agent-5_max: 250
    apples_agent-5_mean: 101.57
    apples_agent-5_min: 63
    cleaning_beam_agent-0_max: 484
    cleaning_beam_agent-0_mean: 399.23
    cleaning_beam_agent-0_min: 288
    cleaning_beam_agent-1_max: 403
    cleaning_beam_agent-1_mean: 275.61
    cleaning_beam_agent-1_min: 111
    cleaning_beam_agent-2_max: 522
    cleaning_beam_agent-2_mean: 339.0
    cleaning_beam_agent-2_min: 137
    cleaning_beam_agent-3_max: 64
    cleaning_beam_agent-3_mean: 13.96
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 587
    cleaning_beam_agent-4_mean: 480.31
    cleaning_beam_agent-4_min: 405
    cleaning_beam_agent-5_max: 121
    cleaning_beam_agent-5_mean: 13.61
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-36-52
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1229.9999999999982
  episode_reward_mean: 1088.309999999992
  episode_reward_min: 673.9999999999909
  episodes_this_iter: 96
  episodes_total: 43008
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20278.443
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9177175760269165
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017421066295355558
        model: {}
        policy_loss: -0.0029028437566012144
        total_loss: -0.0023822167422622442
        vf_explained_var: 0.02905675768852234
        vf_loss: 21.358135223388672
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1720293760299683
        entropy_coeff: 0.0017600000137463212
        kl: 0.002107286360114813
        model: {}
        policy_loss: -0.0038363239727914333
        total_loss: -0.003647408215329051
        vf_explained_var: -0.016649872064590454
        vf_loss: 22.516891479492188
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1397870779037476
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017327854875475168
        model: {}
        policy_loss: -0.0036254478618502617
        total_loss: -0.003439296968281269
        vf_explained_var: 0.004835665225982666
        vf_loss: 21.921775817871094
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35374101996421814
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005637647700496018
        model: {}
        policy_loss: -0.0019249457400292158
        total_loss: -0.0005225148051977158
        vf_explained_var: 0.08133722841739655
        vf_loss: 20.25014877319336
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8670483827590942
        entropy_coeff: 0.0017600000137463212
        kl: 0.00158985226880759
        model: {}
        policy_loss: -0.003540254198014736
        total_loss: -0.0029303194023668766
        vf_explained_var: 0.030704855918884277
        vf_loss: 21.35938835144043
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49568188190460205
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008968045003712177
        model: {}
        policy_loss: -0.0025070111732929945
        total_loss: -0.0013398085720837116
        vf_explained_var: 0.07755215466022491
        vf_loss: 20.396041870117188
    load_time_ms: 13162.605
    num_steps_sampled: 43008000
    num_steps_trained: 43008000
    sample_time_ms: 91653.428
    update_time_ms: 22.015
  iterations_since_restore: 428
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.352808988764043
    ram_util_percent: 19.667977528089892
  pid: 4061
  policy_reward_max:
    agent-0: 205.00000000000006
    agent-1: 205.00000000000006
    agent-2: 205.00000000000006
    agent-3: 205.00000000000006
    agent-4: 205.00000000000006
    agent-5: 205.00000000000006
  policy_reward_mean:
    agent-0: 181.38499999999988
    agent-1: 181.38499999999988
    agent-2: 181.38499999999988
    agent-3: 181.38499999999988
    agent-4: 181.38499999999988
    agent-5: 181.38499999999988
  policy_reward_min:
    agent-0: 112.33333333333351
    agent-1: 112.33333333333351
    agent-2: 112.33333333333351
    agent-3: 112.33333333333351
    agent-4: 112.33333333333351
    agent-5: 112.33333333333351
  sampler_perf:
    mean_env_wait_ms: 24.452272895335344
    mean_inference_ms: 12.332167374830346
    mean_processing_ms: 50.93761567825132
  time_since_restore: 55811.20785284042
  time_this_iter_s: 125.06286311149597
  time_total_s: 59022.271538972855
  timestamp: 1637073412
  timesteps_since_restore: 41088000
  timesteps_this_iter: 96000
  timesteps_total: 43008000
  training_iteration: 448
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    448 |          59022.3 | 43008000 |  1088.31 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 7
    apples_agent-0_mean: 0.18
    apples_agent-0_min: 0
    apples_agent-1_max: 76
    apples_agent-1_mean: 23.74
    apples_agent-1_min: 0
    apples_agent-2_max: 93
    apples_agent-2_mean: 10.07
    apples_agent-2_min: 0
    apples_agent-3_max: 116
    apples_agent-3_mean: 58.41
    apples_agent-3_min: 32
    apples_agent-4_max: 70
    apples_agent-4_mean: 2.72
    apples_agent-4_min: 0
    apples_agent-5_max: 162
    apples_agent-5_mean: 100.14
    apples_agent-5_min: 35
    cleaning_beam_agent-0_max: 486
    cleaning_beam_agent-0_mean: 395.34
    cleaning_beam_agent-0_min: 284
    cleaning_beam_agent-1_max: 441
    cleaning_beam_agent-1_mean: 267.65
    cleaning_beam_agent-1_min: 94
    cleaning_beam_agent-2_max: 527
    cleaning_beam_agent-2_mean: 346.28
    cleaning_beam_agent-2_min: 108
    cleaning_beam_agent-3_max: 82
    cleaning_beam_agent-3_mean: 16.6
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 556
    cleaning_beam_agent-4_mean: 476.33
    cleaning_beam_agent-4_min: 360
    cleaning_beam_agent-5_max: 70
    cleaning_beam_agent-5_mean: 9.7
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-38-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1291.0000000000007
  episode_reward_mean: 1090.2099999999946
  episode_reward_min: 453.00000000000506
  episodes_this_iter: 96
  episodes_total: 43104
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20305.643
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9202072620391846
        entropy_coeff: 0.0017600000137463212
        kl: 0.001594153931364417
        model: {}
        policy_loss: -0.0027912280056625605
        total_loss: -0.0019058547914028168
        vf_explained_var: -0.0006213933229446411
        vf_loss: 25.04940414428711
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.190272569656372
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010953290620818734
        model: {}
        policy_loss: -0.0035308999940752983
        total_loss: -0.003101496957242489
        vf_explained_var: -0.0016448050737380981
        vf_loss: 25.24284553527832
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.125061273574829
        entropy_coeff: 0.0017600000137463212
        kl: 0.001411870471201837
        model: {}
        policy_loss: -0.0032548289746046066
        total_loss: -0.0027754195034503937
        vf_explained_var: 0.016814902424812317
        vf_loss: 24.59516143798828
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37131500244140625
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009324008133262396
        model: {}
        policy_loss: -0.00212795939296484
        total_loss: -0.0005933111533522606
        vf_explained_var: 0.1303347796201706
        vf_loss: 21.88165283203125
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8724907040596008
        entropy_coeff: 0.0017600000137463212
        kl: 0.001702033099718392
        model: {}
        policy_loss: -0.0035884121898561716
        total_loss: -0.0027917076367884874
        vf_explained_var: 0.07287243008613586
        vf_loss: 23.322893142700195
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4689641296863556
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008537221001461148
        model: {}
        policy_loss: -0.002418921561911702
        total_loss: -0.0009909076616168022
        vf_explained_var: 0.10244543850421906
        vf_loss: 22.53392219543457
    load_time_ms: 13133.606
    num_steps_sampled: 43104000
    num_steps_trained: 43104000
    sample_time_ms: 91446.174
    update_time_ms: 22.174
  iterations_since_restore: 429
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.44180790960452
    ram_util_percent: 19.859887005649718
  pid: 4061
  policy_reward_max:
    agent-0: 215.16666666666643
    agent-1: 215.16666666666643
    agent-2: 215.16666666666643
    agent-3: 215.16666666666643
    agent-4: 215.16666666666643
    agent-5: 215.16666666666643
  policy_reward_mean:
    agent-0: 181.70166666666648
    agent-1: 181.70166666666648
    agent-2: 181.70166666666648
    agent-3: 181.70166666666648
    agent-4: 181.70166666666648
    agent-5: 181.70166666666648
  policy_reward_min:
    agent-0: 75.49999999999986
    agent-1: 75.49999999999986
    agent-2: 75.49999999999986
    agent-3: 75.49999999999986
    agent-4: 75.49999999999986
    agent-5: 75.49999999999986
  sampler_perf:
    mean_env_wait_ms: 24.453792466571148
    mean_inference_ms: 12.332364719852608
    mean_processing_ms: 50.93807594189484
  time_since_restore: 55935.582924366
  time_this_iter_s: 124.37507152557373
  time_total_s: 59146.64661049843
  timestamp: 1637073537
  timesteps_since_restore: 41184000
  timesteps_this_iter: 96000
  timesteps_total: 43104000
  training_iteration: 449
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    449 |          59146.6 | 43104000 |  1090.21 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 41
    apples_agent-0_mean: 1.57
    apples_agent-0_min: 0
    apples_agent-1_max: 79
    apples_agent-1_mean: 21.22
    apples_agent-1_min: 0
    apples_agent-2_max: 225
    apples_agent-2_mean: 14.84
    apples_agent-2_min: 0
    apples_agent-3_max: 171
    apples_agent-3_mean: 59.58
    apples_agent-3_min: 30
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 272
    apples_agent-5_mean: 99.64
    apples_agent-5_min: 51
    cleaning_beam_agent-0_max: 495
    cleaning_beam_agent-0_mean: 390.08
    cleaning_beam_agent-0_min: 260
    cleaning_beam_agent-1_max: 449
    cleaning_beam_agent-1_mean: 267.46
    cleaning_beam_agent-1_min: 117
    cleaning_beam_agent-2_max: 513
    cleaning_beam_agent-2_mean: 348.25
    cleaning_beam_agent-2_min: 166
    cleaning_beam_agent-3_max: 69
    cleaning_beam_agent-3_mean: 15.39
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 566
    cleaning_beam_agent-4_mean: 474.31
    cleaning_beam_agent-4_min: 383
    cleaning_beam_agent-5_max: 167
    cleaning_beam_agent-5_mean: 14.14
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-41-01
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1251.9999999999986
  episode_reward_mean: 1070.259999999992
  episode_reward_min: 692.9999999999981
  episodes_this_iter: 96
  episodes_total: 43200
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20286.78
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9234200119972229
        entropy_coeff: 0.0017600000137463212
        kl: 0.001622166484594345
        model: {}
        policy_loss: -0.0030597466975450516
        total_loss: -0.0024506039917469025
        vf_explained_var: 0.060349151492118835
        vf_loss: 22.343624114990234
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1973698139190674
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015297778882086277
        model: {}
        policy_loss: -0.0037589389830827713
        total_loss: -0.0035296864807605743
        vf_explained_var: 0.018396466970443726
        vf_loss: 23.366226196289062
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1377066373825073
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020213164389133453
        model: {}
        policy_loss: -0.0038382913917303085
        total_loss: -0.0035068877041339874
        vf_explained_var: 0.018189549446105957
        vf_loss: 23.33767318725586
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3570299446582794
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007249833433888853
        model: {}
        policy_loss: -0.0019028573296964169
        total_loss: -0.0003687078133225441
        vf_explained_var: 0.09112358093261719
        vf_loss: 21.625200271606445
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8632292747497559
        entropy_coeff: 0.0017600000137463212
        kl: 0.002151289489120245
        model: {}
        policy_loss: -0.0038934764452278614
        total_loss: -0.0031096525490283966
        vf_explained_var: 0.03149932622909546
        vf_loss: 23.03107452392578
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5116170644760132
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008291552076116204
        model: {}
        policy_loss: -0.0026485794223845005
        total_loss: -0.0013927682302892208
        vf_explained_var: 0.09326551854610443
        vf_loss: 21.562572479248047
    load_time_ms: 13119.077
    num_steps_sampled: 43200000
    num_steps_trained: 43200000
    sample_time_ms: 91348.546
    update_time_ms: 22.02
  iterations_since_restore: 430
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.102259887005648
    ram_util_percent: 18.76440677966102
  pid: 4061
  policy_reward_max:
    agent-0: 208.66666666666632
    agent-1: 208.66666666666632
    agent-2: 208.66666666666632
    agent-3: 208.66666666666632
    agent-4: 208.66666666666632
    agent-5: 208.66666666666632
  policy_reward_mean:
    agent-0: 178.3766666666665
    agent-1: 178.3766666666665
    agent-2: 178.3766666666665
    agent-3: 178.3766666666665
    agent-4: 178.3766666666665
    agent-5: 178.3766666666665
  policy_reward_min:
    agent-0: 115.50000000000016
    agent-1: 115.50000000000016
    agent-2: 115.50000000000016
    agent-3: 115.50000000000016
    agent-4: 115.50000000000016
    agent-5: 115.50000000000016
  sampler_perf:
    mean_env_wait_ms: 24.45471214615688
    mean_inference_ms: 12.332409207321488
    mean_processing_ms: 50.93873933605148
  time_since_restore: 56059.79197216034
  time_this_iter_s: 124.20904779434204
  time_total_s: 59270.85565829277
  timestamp: 1637073661
  timesteps_since_restore: 41280000
  timesteps_this_iter: 96000
  timesteps_total: 43200000
  training_iteration: 450
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    450 |          59270.9 | 43200000 |  1070.26 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 48
    apples_agent-0_mean: 1.44
    apples_agent-0_min: 0
    apples_agent-1_max: 75
    apples_agent-1_mean: 20.17
    apples_agent-1_min: 0
    apples_agent-2_max: 298
    apples_agent-2_mean: 17.33
    apples_agent-2_min: 0
    apples_agent-3_max: 108
    apples_agent-3_mean: 56.62
    apples_agent-3_min: 26
    apples_agent-4_max: 41
    apples_agent-4_mean: 0.41
    apples_agent-4_min: 0
    apples_agent-5_max: 235
    apples_agent-5_mean: 99.96
    apples_agent-5_min: 30
    cleaning_beam_agent-0_max: 490
    cleaning_beam_agent-0_mean: 391.89
    cleaning_beam_agent-0_min: 255
    cleaning_beam_agent-1_max: 437
    cleaning_beam_agent-1_mean: 269.21
    cleaning_beam_agent-1_min: 83
    cleaning_beam_agent-2_max: 480
    cleaning_beam_agent-2_mean: 338.95
    cleaning_beam_agent-2_min: 160
    cleaning_beam_agent-3_max: 143
    cleaning_beam_agent-3_mean: 15.09
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 594
    cleaning_beam_agent-4_mean: 479.33
    cleaning_beam_agent-4_min: 383
    cleaning_beam_agent-5_max: 104
    cleaning_beam_agent-5_mean: 12.53
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-43-07
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1245.0000000000011
  episode_reward_mean: 1079.5699999999931
  episode_reward_min: 635.9999999999977
  episodes_this_iter: 96
  episodes_total: 43296
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20311.189
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9154050350189209
        entropy_coeff: 0.0017600000137463212
        kl: 0.001521622878499329
        model: {}
        policy_loss: -0.0030523210298269987
        total_loss: -0.0024043277371674776
        vf_explained_var: 0.07646630704402924
        vf_loss: 22.591083526611328
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1930220127105713
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020118695683777332
        model: {}
        policy_loss: -0.003885330632328987
        total_loss: -0.003544132225215435
        vf_explained_var: 0.006105154752731323
        vf_loss: 24.409164428710938
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1417230367660522
        entropy_coeff: 0.0017600000137463212
        kl: 0.001542986137792468
        model: {}
        policy_loss: -0.003051948035135865
        total_loss: -0.0025968037080019712
        vf_explained_var: -0.005874752998352051
        vf_loss: 24.645767211914062
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3489859104156494
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006254409090615809
        model: {}
        policy_loss: -0.0018698405474424362
        total_loss: -0.0003115586005151272
        vf_explained_var: 0.1142263412475586
        vf_loss: 21.7249698638916
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8685840368270874
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019500711932778358
        model: {}
        policy_loss: -0.003732931800186634
        total_loss: -0.002884694840759039
        vf_explained_var: 0.02839668095111847
        vf_loss: 23.769437789916992
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4951186180114746
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010818644659593701
        model: {}
        policy_loss: -0.002872578799724579
        total_loss: -0.00155631173402071
        vf_explained_var: 0.10650976002216339
        vf_loss: 21.876745223999023
    load_time_ms: 13133.445
    num_steps_sampled: 43296000
    num_steps_trained: 43296000
    sample_time_ms: 91372.624
    update_time_ms: 22.431
  iterations_since_restore: 431
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.52011173184357
    ram_util_percent: 19.360335195530727
  pid: 4061
  policy_reward_max:
    agent-0: 207.50000000000017
    agent-1: 207.50000000000017
    agent-2: 207.50000000000017
    agent-3: 207.50000000000017
    agent-4: 207.50000000000017
    agent-5: 207.50000000000017
  policy_reward_mean:
    agent-0: 179.92833333333326
    agent-1: 179.92833333333326
    agent-2: 179.92833333333326
    agent-3: 179.92833333333326
    agent-4: 179.92833333333326
    agent-5: 179.92833333333326
  policy_reward_min:
    agent-0: 106.00000000000057
    agent-1: 106.00000000000057
    agent-2: 106.00000000000057
    agent-3: 106.00000000000057
    agent-4: 106.00000000000057
    agent-5: 106.00000000000057
  sampler_perf:
    mean_env_wait_ms: 24.456440920307454
    mean_inference_ms: 12.332733455972628
    mean_processing_ms: 50.93945692159864
  time_since_restore: 56184.92549610138
  time_this_iter_s: 125.13352394104004
  time_total_s: 59395.98918223381
  timestamp: 1637073787
  timesteps_since_restore: 41376000
  timesteps_this_iter: 96000
  timesteps_total: 43296000
  training_iteration: 451
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 36.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    451 |            59396 | 43296000 |  1079.57 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 80
    apples_agent-0_mean: 2.39
    apples_agent-0_min: 0
    apples_agent-1_max: 84
    apples_agent-1_mean: 24.75
    apples_agent-1_min: 0
    apples_agent-2_max: 92
    apples_agent-2_mean: 6.92
    apples_agent-2_min: 0
    apples_agent-3_max: 109
    apples_agent-3_mean: 58.9
    apples_agent-3_min: 23
    apples_agent-4_max: 72
    apples_agent-4_mean: 1.27
    apples_agent-4_min: 0
    apples_agent-5_max: 159
    apples_agent-5_mean: 98.71
    apples_agent-5_min: 24
    cleaning_beam_agent-0_max: 532
    cleaning_beam_agent-0_mean: 387.62
    cleaning_beam_agent-0_min: 263
    cleaning_beam_agent-1_max: 434
    cleaning_beam_agent-1_mean: 254.47
    cleaning_beam_agent-1_min: 139
    cleaning_beam_agent-2_max: 567
    cleaning_beam_agent-2_mean: 365.46
    cleaning_beam_agent-2_min: 169
    cleaning_beam_agent-3_max: 36
    cleaning_beam_agent-3_mean: 13.22
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 560
    cleaning_beam_agent-4_mean: 473.16
    cleaning_beam_agent-4_min: 347
    cleaning_beam_agent-5_max: 201
    cleaning_beam_agent-5_mean: 10.99
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-45-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1269.000000000008
  episode_reward_mean: 1089.5599999999927
  episode_reward_min: 476.00000000000307
  episodes_this_iter: 96
  episodes_total: 43392
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20292.4
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9249646663665771
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013146568089723587
        model: {}
        policy_loss: -0.0027677100151777267
        total_loss: -0.001971272751688957
        vf_explained_var: 0.02821454405784607
        vf_loss: 24.243759155273438
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1861287355422974
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022308104671537876
        model: {}
        policy_loss: -0.004044076427817345
        total_loss: -0.0035482137463986874
        vf_explained_var: -0.031003624200820923
        vf_loss: 25.834484100341797
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1210976839065552
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013057604664936662
        model: {}
        policy_loss: -0.003120067063719034
        total_loss: -0.0024838135577738285
        vf_explained_var: -0.046122848987579346
        vf_loss: 26.093862533569336
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35412997007369995
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013842746848240495
        model: {}
        policy_loss: -0.0025297855027019978
        total_loss: -0.0009510526433587074
        vf_explained_var: 0.11893698573112488
        vf_loss: 22.020023345947266
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8746161460876465
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014421625528484583
        model: {}
        policy_loss: -0.0035695014521479607
        total_loss: -0.002681591548025608
        vf_explained_var: 0.029124096035957336
        vf_loss: 24.27236557006836
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4891204833984375
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008828629506751895
        model: {}
        policy_loss: -0.002933880779892206
        total_loss: -0.0015353127382695675
        vf_explained_var: 0.09624336659908295
        vf_loss: 22.594249725341797
    load_time_ms: 13122.934
    num_steps_sampled: 43392000
    num_steps_trained: 43392000
    sample_time_ms: 91447.579
    update_time_ms: 21.898
  iterations_since_restore: 432
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.2819209039548
    ram_util_percent: 19.298305084745763
  pid: 4061
  policy_reward_max:
    agent-0: 211.50000000000003
    agent-1: 211.50000000000003
    agent-2: 211.50000000000003
    agent-3: 211.50000000000003
    agent-4: 211.50000000000003
    agent-5: 211.50000000000003
  policy_reward_mean:
    agent-0: 181.59333333333316
    agent-1: 181.59333333333316
    agent-2: 181.59333333333316
    agent-3: 181.59333333333316
    agent-4: 181.59333333333316
    agent-5: 181.59333333333316
  policy_reward_min:
    agent-0: 79.3333333333335
    agent-1: 79.3333333333335
    agent-2: 79.3333333333335
    agent-3: 79.3333333333335
    agent-4: 79.3333333333335
    agent-5: 79.3333333333335
  sampler_perf:
    mean_env_wait_ms: 24.458143866894684
    mean_inference_ms: 12.332885727628454
    mean_processing_ms: 50.94029738168287
  time_since_restore: 56309.75403761864
  time_this_iter_s: 124.82854151725769
  time_total_s: 59520.81772375107
  timestamp: 1637073912
  timesteps_since_restore: 41472000
  timesteps_this_iter: 96000
  timesteps_total: 43392000
  training_iteration: 452
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    452 |          59520.8 | 43392000 |  1089.56 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 51
    apples_agent-0_mean: 1.3
    apples_agent-0_min: 0
    apples_agent-1_max: 107
    apples_agent-1_mean: 26.2
    apples_agent-1_min: 0
    apples_agent-2_max: 64
    apples_agent-2_mean: 8.93
    apples_agent-2_min: 0
    apples_agent-3_max: 129
    apples_agent-3_mean: 61.42
    apples_agent-3_min: 27
    apples_agent-4_max: 57
    apples_agent-4_mean: 1.63
    apples_agent-4_min: 0
    apples_agent-5_max: 189
    apples_agent-5_mean: 103.77
    apples_agent-5_min: 54
    cleaning_beam_agent-0_max: 471
    cleaning_beam_agent-0_mean: 393.97
    cleaning_beam_agent-0_min: 236
    cleaning_beam_agent-1_max: 369
    cleaning_beam_agent-1_mean: 250.35
    cleaning_beam_agent-1_min: 102
    cleaning_beam_agent-2_max: 545
    cleaning_beam_agent-2_mean: 360.34
    cleaning_beam_agent-2_min: 154
    cleaning_beam_agent-3_max: 88
    cleaning_beam_agent-3_mean: 15.27
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 570
    cleaning_beam_agent-4_mean: 476.23
    cleaning_beam_agent-4_min: 256
    cleaning_beam_agent-5_max: 87
    cleaning_beam_agent-5_mean: 12.05
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-47-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1231.0000000000052
  episode_reward_mean: 1077.789999999995
  episode_reward_min: 345.00000000000387
  episodes_this_iter: 96
  episodes_total: 43488
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20272.999
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9224456548690796
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014390836004167795
        model: {}
        policy_loss: -0.0028214529156684875
        total_loss: -0.0019747624173760414
        vf_explained_var: 0.03677643835544586
        vf_loss: 24.70193862915039
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1861456632614136
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014618011191487312
        model: {}
        policy_loss: -0.0036559465806931257
        total_loss: -0.0031630557496100664
        vf_explained_var: -0.004875198006629944
        vf_loss: 25.805091857910156
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1196513175964355
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012198850745335221
        model: {}
        policy_loss: -0.0032349219545722008
        total_loss: -0.0026299157179892063
        vf_explained_var: -0.006179898977279663
        vf_loss: 25.755956649780273
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3663855493068695
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008286300580948591
        model: {}
        policy_loss: -0.002270834520459175
        total_loss: -0.0006313461344689131
        vf_explained_var: 0.10896439850330353
        vf_loss: 22.843219757080078
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8661895990371704
        entropy_coeff: 0.0017600000137463212
        kl: 0.001953332219272852
        model: {}
        policy_loss: -0.003612142987549305
        total_loss: -0.0027377521619200706
        vf_explained_var: 0.06390474736690521
        vf_loss: 23.988853454589844
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49600523710250854
        entropy_coeff: 0.0017600000137463212
        kl: 0.00114043231587857
        model: {}
        policy_loss: -0.0027540894225239754
        total_loss: -0.00138942152261734
        vf_explained_var: 0.12632666528224945
        vf_loss: 22.376401901245117
    load_time_ms: 13172.632
    num_steps_sampled: 43488000
    num_steps_trained: 43488000
    sample_time_ms: 91680.383
    update_time_ms: 21.706
  iterations_since_restore: 433
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.05054945054945
    ram_util_percent: 19.225824175824176
  pid: 4061
  policy_reward_max:
    agent-0: 205.16666666666634
    agent-1: 205.16666666666634
    agent-2: 205.16666666666634
    agent-3: 205.16666666666634
    agent-4: 205.16666666666634
    agent-5: 205.16666666666634
  policy_reward_mean:
    agent-0: 179.63166666666658
    agent-1: 179.63166666666658
    agent-2: 179.63166666666658
    agent-3: 179.63166666666658
    agent-4: 179.63166666666658
    agent-5: 179.63166666666658
  policy_reward_min:
    agent-0: 57.49999999999984
    agent-1: 57.49999999999984
    agent-2: 57.49999999999984
    agent-3: 57.49999999999984
    agent-4: 57.49999999999984
    agent-5: 57.49999999999984
  sampler_perf:
    mean_env_wait_ms: 24.46011309514027
    mean_inference_ms: 12.333253372373246
    mean_processing_ms: 50.94200727437286
  time_since_restore: 56437.28513240814
  time_this_iter_s: 127.531094789505
  time_total_s: 59648.34881854057
  timestamp: 1637074039
  timesteps_since_restore: 41568000
  timesteps_this_iter: 96000
  timesteps_total: 43488000
  training_iteration: 453
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    453 |          59648.3 | 43488000 |  1077.79 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 36
    apples_agent-0_mean: 2.11
    apples_agent-0_min: 0
    apples_agent-1_max: 119
    apples_agent-1_mean: 22.78
    apples_agent-1_min: 0
    apples_agent-2_max: 166
    apples_agent-2_mean: 13.03
    apples_agent-2_min: 0
    apples_agent-3_max: 103
    apples_agent-3_mean: 57.26
    apples_agent-3_min: 20
    apples_agent-4_max: 34
    apples_agent-4_mean: 0.91
    apples_agent-4_min: 0
    apples_agent-5_max: 215
    apples_agent-5_mean: 97.35
    apples_agent-5_min: 18
    cleaning_beam_agent-0_max: 484
    cleaning_beam_agent-0_mean: 391.49
    cleaning_beam_agent-0_min: 202
    cleaning_beam_agent-1_max: 402
    cleaning_beam_agent-1_mean: 249.23
    cleaning_beam_agent-1_min: 100
    cleaning_beam_agent-2_max: 518
    cleaning_beam_agent-2_mean: 357.07
    cleaning_beam_agent-2_min: 154
    cleaning_beam_agent-3_max: 83
    cleaning_beam_agent-3_mean: 16.37
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 560
    cleaning_beam_agent-4_mean: 471.63
    cleaning_beam_agent-4_min: 348
    cleaning_beam_agent-5_max: 141
    cleaning_beam_agent-5_mean: 13.32
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 3
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-49-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1280.000000000005
  episode_reward_mean: 1076.8399999999947
  episode_reward_min: 202.99999999999756
  episodes_this_iter: 96
  episodes_total: 43584
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20298.248
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9108070135116577
        entropy_coeff: 0.0017600000137463212
        kl: 0.001043454511091113
        model: {}
        policy_loss: -0.002596180886030197
        total_loss: -0.001527195330709219
        vf_explained_var: 0.055463507771492004
        vf_loss: 26.72003936767578
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.191382646560669
        entropy_coeff: 0.0017600000137463212
        kl: 0.001490112510509789
        model: {}
        policy_loss: -0.0035858743358403444
        total_loss: -0.0027865716256201267
        vf_explained_var: -0.02277533710002899
        vf_loss: 28.961353302001953
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1298714876174927
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008534878143109381
        model: {}
        policy_loss: -0.0031102150678634644
        total_loss: -0.0022695008665323257
        vf_explained_var: 0.0010530948638916016
        vf_loss: 28.292882919311523
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3631284832954407
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009589382098056376
        model: {}
        policy_loss: -0.0024498661514371634
        total_loss: -0.0006937053985893726
        vf_explained_var: 0.1547936201095581
        vf_loss: 23.95269203186035
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8633050918579102
        entropy_coeff: 0.0017600000137463212
        kl: 0.002121821278706193
        model: {}
        policy_loss: -0.0037007287610322237
        total_loss: -0.0026005511172115803
        vf_explained_var: 0.0748303234577179
        vf_loss: 26.195945739746094
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4942532479763031
        entropy_coeff: 0.0017600000137463212
        kl: 0.001005075522698462
        model: {}
        policy_loss: -0.003086432348936796
        total_loss: -0.0014358451589941978
        vf_explained_var: 0.1086522787809372
        vf_loss: 25.204734802246094
    load_time_ms: 13176.214
    num_steps_sampled: 43584000
    num_steps_trained: 43584000
    sample_time_ms: 91620.579
    update_time_ms: 21.862
  iterations_since_restore: 434
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.49550561797753
    ram_util_percent: 19.174719101123593
  pid: 4061
  policy_reward_max:
    agent-0: 213.33333333333363
    agent-1: 213.33333333333363
    agent-2: 213.33333333333363
    agent-3: 213.33333333333363
    agent-4: 213.33333333333363
    agent-5: 213.33333333333363
  policy_reward_mean:
    agent-0: 179.47333333333322
    agent-1: 179.47333333333322
    agent-2: 179.47333333333322
    agent-3: 179.47333333333322
    agent-4: 179.47333333333322
    agent-5: 179.47333333333322
  policy_reward_min:
    agent-0: 33.833333333333385
    agent-1: 33.833333333333385
    agent-2: 33.833333333333385
    agent-3: 33.833333333333385
    agent-4: 33.833333333333385
    agent-5: 33.833333333333385
  sampler_perf:
    mean_env_wait_ms: 24.461688865880923
    mean_inference_ms: 12.333374519233736
    mean_processing_ms: 50.94290383603646
  time_since_restore: 56562.21708655357
  time_this_iter_s: 124.93195414543152
  time_total_s: 59773.280772686005
  timestamp: 1637074164
  timesteps_since_restore: 41664000
  timesteps_this_iter: 96000
  timesteps_total: 43584000
  training_iteration: 454
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    454 |          59773.3 | 43584000 |  1076.84 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 34
    apples_agent-0_mean: 1.91
    apples_agent-0_min: 0
    apples_agent-1_max: 87
    apples_agent-1_mean: 22.84
    apples_agent-1_min: 0
    apples_agent-2_max: 135
    apples_agent-2_mean: 14.71
    apples_agent-2_min: 0
    apples_agent-3_max: 121
    apples_agent-3_mean: 58.51
    apples_agent-3_min: 26
    apples_agent-4_max: 83
    apples_agent-4_mean: 1.23
    apples_agent-4_min: 0
    apples_agent-5_max: 153
    apples_agent-5_mean: 99.52
    apples_agent-5_min: 56
    cleaning_beam_agent-0_max: 529
    cleaning_beam_agent-0_mean: 399.01
    cleaning_beam_agent-0_min: 296
    cleaning_beam_agent-1_max: 412
    cleaning_beam_agent-1_mean: 236.64
    cleaning_beam_agent-1_min: 121
    cleaning_beam_agent-2_max: 514
    cleaning_beam_agent-2_mean: 354.71
    cleaning_beam_agent-2_min: 147
    cleaning_beam_agent-3_max: 95
    cleaning_beam_agent-3_mean: 15.27
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 583
    cleaning_beam_agent-4_mean: 477.95
    cleaning_beam_agent-4_min: 368
    cleaning_beam_agent-5_max: 141
    cleaning_beam_agent-5_mean: 10.55
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-51-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1240.9999999999864
  episode_reward_mean: 1081.6299999999933
  episode_reward_min: 500.00000000000784
  episodes_this_iter: 96
  episodes_total: 43680
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20279.786
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9163371324539185
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015192418359220028
        model: {}
        policy_loss: -0.0028731543570756912
        total_loss: -0.0021391534246504307
        vf_explained_var: 0.049015626311302185
        vf_loss: 23.467565536499023
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1849673986434937
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014714759308844805
        model: {}
        policy_loss: -0.003569734748452902
        total_loss: -0.0031864354386925697
        vf_explained_var: 0.0021997541189193726
        vf_loss: 24.688417434692383
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.126861572265625
        entropy_coeff: 0.0017600000137463212
        kl: 0.001088117714971304
        model: {}
        policy_loss: -0.0034311162307858467
        total_loss: -0.0029679937288165092
        vf_explained_var: 0.008259698748588562
        vf_loss: 24.464038848876953
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3671634793281555
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016015288420021534
        model: {}
        policy_loss: -0.0025184887927025557
        total_loss: -0.0009728208533488214
        vf_explained_var: 0.11400175094604492
        vf_loss: 21.918792724609375
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8658019304275513
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016394234262406826
        model: {}
        policy_loss: -0.0034172022715210915
        total_loss: -0.0025882162153720856
        vf_explained_var: 0.04588024318218231
        vf_loss: 23.52799415588379
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49241751432418823
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007744580507278442
        model: {}
        policy_loss: -0.0027114609256386757
        total_loss: -0.001384158618748188
        vf_explained_var: 0.11008816957473755
        vf_loss: 21.939603805541992
    load_time_ms: 13192.473
    num_steps_sampled: 43680000
    num_steps_trained: 43680000
    sample_time_ms: 91638.727
    update_time_ms: 21.9
  iterations_since_restore: 435
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.64804469273743
    ram_util_percent: 18.06536312849162
  pid: 4061
  policy_reward_max:
    agent-0: 206.83333333333354
    agent-1: 206.83333333333354
    agent-2: 206.83333333333354
    agent-3: 206.83333333333354
    agent-4: 206.83333333333354
    agent-5: 206.83333333333354
  policy_reward_mean:
    agent-0: 180.27166666666645
    agent-1: 180.27166666666645
    agent-2: 180.27166666666645
    agent-3: 180.27166666666645
    agent-4: 180.27166666666645
    agent-5: 180.27166666666645
  policy_reward_min:
    agent-0: 83.33333333333346
    agent-1: 83.33333333333346
    agent-2: 83.33333333333346
    agent-3: 83.33333333333346
    agent-4: 83.33333333333346
    agent-5: 83.33333333333346
  sampler_perf:
    mean_env_wait_ms: 24.4630509683928
    mean_inference_ms: 12.333516654361226
    mean_processing_ms: 50.94297639258119
  time_since_restore: 56687.05895471573
  time_this_iter_s: 124.84186816215515
  time_total_s: 59898.12264084816
  timestamp: 1637074289
  timesteps_since_restore: 41760000
  timesteps_this_iter: 96000
  timesteps_total: 43680000
  training_iteration: 455
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 32.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    455 |          59898.1 | 43680000 |  1081.63 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 35
    apples_agent-0_mean: 1.3
    apples_agent-0_min: 0
    apples_agent-1_max: 102
    apples_agent-1_mean: 25.95
    apples_agent-1_min: 0
    apples_agent-2_max: 92
    apples_agent-2_mean: 11.85
    apples_agent-2_min: 0
    apples_agent-3_max: 121
    apples_agent-3_mean: 58.23
    apples_agent-3_min: 29
    apples_agent-4_max: 72
    apples_agent-4_mean: 1.62
    apples_agent-4_min: 0
    apples_agent-5_max: 136
    apples_agent-5_mean: 97.28
    apples_agent-5_min: 64
    cleaning_beam_agent-0_max: 520
    cleaning_beam_agent-0_mean: 406.56
    cleaning_beam_agent-0_min: 270
    cleaning_beam_agent-1_max: 423
    cleaning_beam_agent-1_mean: 234.22
    cleaning_beam_agent-1_min: 119
    cleaning_beam_agent-2_max: 526
    cleaning_beam_agent-2_mean: 356.88
    cleaning_beam_agent-2_min: 154
    cleaning_beam_agent-3_max: 123
    cleaning_beam_agent-3_mean: 16.41
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 566
    cleaning_beam_agent-4_mean: 478.26
    cleaning_beam_agent-4_min: 359
    cleaning_beam_agent-5_max: 167
    cleaning_beam_agent-5_mean: 10.07
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-53-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1264.999999999999
  episode_reward_mean: 1094.629999999993
  episode_reward_min: 674.9999999999853
  episodes_this_iter: 96
  episodes_total: 43776
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20293.409
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9037095308303833
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017279301537200809
        model: {}
        policy_loss: -0.002971639856696129
        total_loss: -0.0023512328043580055
        vf_explained_var: 0.04630489647388458
        vf_loss: 22.109355926513672
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1900670528411865
        entropy_coeff: 0.0017600000137463212
        kl: 0.001813310431316495
        model: {}
        policy_loss: -0.003969109617173672
        total_loss: -0.003709588898345828
        vf_explained_var: -0.005674645304679871
        vf_loss: 23.54043197631836
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1318860054016113
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014345287345349789
        model: {}
        policy_loss: -0.0034500369802117348
        total_loss: -0.0030370373278856277
        vf_explained_var: -0.03556671738624573
        vf_loss: 24.051227569580078
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36372244358062744
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008553227526135743
        model: {}
        policy_loss: -0.0018256604671478271
        total_loss: -0.00033755600452423096
        vf_explained_var: 0.08593350648880005
        vf_loss: 21.282527923583984
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8688190579414368
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018918034620583057
        model: {}
        policy_loss: -0.0034808190539479256
        total_loss: -0.0027647383976727724
        vf_explained_var: 0.02947138249874115
        vf_loss: 22.452041625976562
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4877070486545563
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009387932950630784
        model: {}
        policy_loss: -0.0023297322914004326
        total_loss: -0.001038464717566967
        vf_explained_var: 0.0756540596485138
        vf_loss: 21.49628257751465
    load_time_ms: 13189.9
    num_steps_sampled: 43776000
    num_steps_trained: 43776000
    sample_time_ms: 91442.142
    update_time_ms: 22.397
  iterations_since_restore: 436
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.925842696629214
    ram_util_percent: 18.46685393258427
  pid: 4061
  policy_reward_max:
    agent-0: 210.8333333333334
    agent-1: 210.8333333333334
    agent-2: 210.8333333333334
    agent-3: 210.8333333333334
    agent-4: 210.8333333333334
    agent-5: 210.8333333333334
  policy_reward_mean:
    agent-0: 182.43833333333316
    agent-1: 182.43833333333316
    agent-2: 182.43833333333316
    agent-3: 182.43833333333316
    agent-4: 182.43833333333316
    agent-5: 182.43833333333316
  policy_reward_min:
    agent-0: 112.50000000000047
    agent-1: 112.50000000000047
    agent-2: 112.50000000000047
    agent-3: 112.50000000000047
    agent-4: 112.50000000000047
    agent-5: 112.50000000000047
  sampler_perf:
    mean_env_wait_ms: 24.464754306711402
    mean_inference_ms: 12.333779275719865
    mean_processing_ms: 50.94427147047786
  time_since_restore: 56811.77785563469
  time_this_iter_s: 124.71890091896057
  time_total_s: 60022.84154176712
  timestamp: 1637074414
  timesteps_since_restore: 41856000
  timesteps_this_iter: 96000
  timesteps_total: 43776000
  training_iteration: 456
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    456 |          60022.8 | 43776000 |  1094.63 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 45
    apples_agent-0_mean: 1.75
    apples_agent-0_min: 0
    apples_agent-1_max: 98
    apples_agent-1_mean: 23.66
    apples_agent-1_min: 0
    apples_agent-2_max: 106
    apples_agent-2_mean: 14.88
    apples_agent-2_min: 0
    apples_agent-3_max: 94
    apples_agent-3_mean: 58.1
    apples_agent-3_min: 34
    apples_agent-4_max: 67
    apples_agent-4_mean: 0.67
    apples_agent-4_min: 0
    apples_agent-5_max: 146
    apples_agent-5_mean: 100.43
    apples_agent-5_min: 43
    cleaning_beam_agent-0_max: 512
    cleaning_beam_agent-0_mean: 407.5
    cleaning_beam_agent-0_min: 276
    cleaning_beam_agent-1_max: 404
    cleaning_beam_agent-1_mean: 236.47
    cleaning_beam_agent-1_min: 106
    cleaning_beam_agent-2_max: 565
    cleaning_beam_agent-2_mean: 340.43
    cleaning_beam_agent-2_min: 149
    cleaning_beam_agent-3_max: 43
    cleaning_beam_agent-3_mean: 16.1
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 566
    cleaning_beam_agent-4_mean: 477.35
    cleaning_beam_agent-4_min: 382
    cleaning_beam_agent-5_max: 54
    cleaning_beam_agent-5_mean: 7.78
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-55-39
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1283.0000000000075
  episode_reward_mean: 1090.2299999999932
  episode_reward_min: 419.00000000000705
  episodes_this_iter: 96
  episodes_total: 43872
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20293.238
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9046487212181091
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016467118402943015
        model: {}
        policy_loss: -0.002844932023435831
        total_loss: -0.0021022488363087177
        vf_explained_var: 0.04059822857379913
        vf_loss: 23.348678588867188
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1798145771026611
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013743649469688535
        model: {}
        policy_loss: -0.0036912206560373306
        total_loss: -0.003283232916146517
        vf_explained_var: -0.016129270195961
        vf_loss: 24.84457778930664
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1334939002990723
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013240303378552198
        model: {}
        policy_loss: -0.003241993486881256
        total_loss: -0.002804249059408903
        vf_explained_var: -0.0008419305086135864
        vf_loss: 24.32693862915039
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36754852533340454
        entropy_coeff: 0.0017600000137463212
        kl: 0.001283098477870226
        model: {}
        policy_loss: -0.002478304784744978
        total_loss: -0.0009206128306686878
        vf_explained_var: 0.09471246600151062
        vf_loss: 22.045764923095703
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8477537035942078
        entropy_coeff: 0.0017600000137463212
        kl: 0.001971999416127801
        model: {}
        policy_loss: -0.003564423881471157
        total_loss: -0.0027136115822941065
        vf_explained_var: 0.03454340994358063
        vf_loss: 23.42859649658203
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49232661724090576
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009898939169943333
        model: {}
        policy_loss: -0.00271058501675725
        total_loss: -0.0013714071828871965
        vf_explained_var: 0.09287448227405548
        vf_loss: 22.05672836303711
    load_time_ms: 13168.355
    num_steps_sampled: 43872000
    num_steps_trained: 43872000
    sample_time_ms: 91488.193
    update_time_ms: 21.705
  iterations_since_restore: 437
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.536516853932582
    ram_util_percent: 18.752247191011236
  pid: 4061
  policy_reward_max:
    agent-0: 213.83333333333323
    agent-1: 213.83333333333323
    agent-2: 213.83333333333323
    agent-3: 213.83333333333323
    agent-4: 213.83333333333323
    agent-5: 213.83333333333323
  policy_reward_mean:
    agent-0: 181.70499999999984
    agent-1: 181.70499999999984
    agent-2: 181.70499999999984
    agent-3: 181.70499999999984
    agent-4: 181.70499999999984
    agent-5: 181.70499999999984
  policy_reward_min:
    agent-0: 69.83333333333316
    agent-1: 69.83333333333316
    agent-2: 69.83333333333316
    agent-3: 69.83333333333316
    agent-4: 69.83333333333316
    agent-5: 69.83333333333316
  sampler_perf:
    mean_env_wait_ms: 24.466146167444982
    mean_inference_ms: 12.33385980688993
    mean_processing_ms: 50.94469444003334
  time_since_restore: 56936.79712533951
  time_this_iter_s: 125.01926970481873
  time_total_s: 60147.86081147194
  timestamp: 1637074539
  timesteps_since_restore: 41952000
  timesteps_this_iter: 96000
  timesteps_total: 43872000
  training_iteration: 457
  trial_id: '00000'
  
[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
== Status ==
Memory usage on this node: 34.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    457 |          60147.9 | 43872000 |  1090.23 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 33
    apples_agent-0_mean: 0.96
    apples_agent-0_min: 0
    apples_agent-1_max: 78
    apples_agent-1_mean: 23.28
    apples_agent-1_min: 0
    apples_agent-2_max: 103
    apples_agent-2_mean: 9.94
    apples_agent-2_min: 0
    apples_agent-3_max: 132
    apples_agent-3_mean: 58.71
    apples_agent-3_min: 26
    apples_agent-4_max: 58
    apples_agent-4_mean: 1.75
    apples_agent-4_min: 0
    apples_agent-5_max: 173
    apples_agent-5_mean: 104.34
    apples_agent-5_min: 49
    cleaning_beam_agent-0_max: 507
    cleaning_beam_agent-0_mean: 412.68
    cleaning_beam_agent-0_min: 273
    cleaning_beam_agent-1_max: 417
    cleaning_beam_agent-1_mean: 240.43
    cleaning_beam_agent-1_min: 127
    cleaning_beam_agent-2_max: 510
    cleaning_beam_agent-2_mean: 365.91
    cleaning_beam_agent-2_min: 208
    cleaning_beam_agent-3_max: 58
    cleaning_beam_agent-3_mean: 15.17
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 586
    cleaning_beam_agent-4_mean: 483.54
    cleaning_beam_agent-4_min: 392
    cleaning_beam_agent-5_max: 146
    cleaning_beam_agent-5_mean: 10.19
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-57-44
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1252.9999999999955
  episode_reward_mean: 1105.699999999995
  episode_reward_min: 590.0000000000007
  episodes_this_iter: 96
  episodes_total: 43968
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20306.27
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8929232358932495
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020671342499554157
        model: {}
        policy_loss: -0.0029264171607792377
        total_loss: -0.002096631098538637
        vf_explained_var: 0.017040863633155823
        vf_loss: 24.013286590576172
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.183483600616455
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016226707957684994
        model: {}
        policy_loss: -0.0037325185257941484
        total_loss: -0.0032640162389725447
        vf_explained_var: -0.04002884030342102
        vf_loss: 25.514347076416016
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1351332664489746
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014023402472957969
        model: {}
        policy_loss: -0.0033065311145037413
        total_loss: -0.002870856784284115
        vf_explained_var: -0.0012502670288085938
        vf_loss: 24.335073471069336
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3566911220550537
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007540523074567318
        model: {}
        policy_loss: -0.0017943093553185463
        total_loss: -0.00021924171596765518
        vf_explained_var: 0.0979999303817749
        vf_loss: 22.02842140197754
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.868237316608429
        entropy_coeff: 0.0017600000137463212
        kl: 0.001851178938522935
        model: {}
        policy_loss: -0.003679413814097643
        total_loss: -0.0028296462260186672
        vf_explained_var: 0.026194095611572266
        vf_loss: 23.77862548828125
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4808558523654938
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011579832062125206
        model: {}
        policy_loss: -0.002594290766865015
        total_loss: -0.0012508220970630646
        vf_explained_var: 0.10382434725761414
        vf_loss: 21.89773941040039
    load_time_ms: 13184.58
    num_steps_sampled: 43968000
    num_steps_trained: 43968000
    sample_time_ms: 91418.02
    update_time_ms: 22.172
  iterations_since_restore: 438
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.2271186440678
    ram_util_percent: 18.77005649717514
  pid: 4061
  policy_reward_max:
    agent-0: 208.833333333333
    agent-1: 208.833333333333
    agent-2: 208.833333333333
    agent-3: 208.833333333333
    agent-4: 208.833333333333
    agent-5: 208.833333333333
  policy_reward_mean:
    agent-0: 184.28333333333313
    agent-1: 184.28333333333313
    agent-2: 184.28333333333313
    agent-3: 184.28333333333313
    agent-4: 184.28333333333313
    agent-5: 184.28333333333313
  policy_reward_min:
    agent-0: 98.33333333333368
    agent-1: 98.33333333333368
    agent-2: 98.33333333333368
    agent-3: 98.33333333333368
    agent-4: 98.33333333333368
    agent-5: 98.33333333333368
  sampler_perf:
    mean_env_wait_ms: 24.467613447192125
    mean_inference_ms: 12.333952894816825
    mean_processing_ms: 50.94379827975416
  time_since_restore: 57061.450224876404
  time_this_iter_s: 124.65309953689575
  time_total_s: 60272.513911008835
  timestamp: 1637074664
  timesteps_since_restore: 42048000
  timesteps_this_iter: 96000
  timesteps_total: 43968000
  training_iteration: 458
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    458 |          60272.5 | 43968000 |   1105.7 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 37
    apples_agent-0_mean: 0.84
    apples_agent-0_min: 0
    apples_agent-1_max: 125
    apples_agent-1_mean: 26.51
    apples_agent-1_min: 0
    apples_agent-2_max: 155
    apples_agent-2_mean: 18.33
    apples_agent-2_min: 0
    apples_agent-3_max: 129
    apples_agent-3_mean: 60.54
    apples_agent-3_min: 34
    apples_agent-4_max: 61
    apples_agent-4_mean: 1.65
    apples_agent-4_min: 0
    apples_agent-5_max: 189
    apples_agent-5_mean: 104.06
    apples_agent-5_min: 49
    cleaning_beam_agent-0_max: 482
    cleaning_beam_agent-0_mean: 396.58
    cleaning_beam_agent-0_min: 297
    cleaning_beam_agent-1_max: 360
    cleaning_beam_agent-1_mean: 245.14
    cleaning_beam_agent-1_min: 97
    cleaning_beam_agent-2_max: 521
    cleaning_beam_agent-2_mean: 353.77
    cleaning_beam_agent-2_min: 120
    cleaning_beam_agent-3_max: 55
    cleaning_beam_agent-3_mean: 15.7
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 583
    cleaning_beam_agent-4_mean: 472.05
    cleaning_beam_agent-4_min: 341
    cleaning_beam_agent-5_max: 146
    cleaning_beam_agent-5_mean: 9.32
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-59-49
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1235.000000000003
  episode_reward_mean: 1093.0399999999938
  episode_reward_min: 473.00000000000546
  episodes_this_iter: 96
  episodes_total: 44064
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20286.974
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9016506671905518
        entropy_coeff: 0.0017600000137463212
        kl: 0.002003780100494623
        model: {}
        policy_loss: -0.0027833341155201197
        total_loss: -0.00215929769910872
        vf_explained_var: 0.04177464544773102
        vf_loss: 22.109439849853516
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.179418683052063
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018619191832840443
        model: {}
        policy_loss: -0.0037957224994897842
        total_loss: -0.0035414020530879498
        vf_explained_var: -0.005216240882873535
        vf_loss: 23.300983428955078
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1258633136749268
        entropy_coeff: 0.0017600000137463212
        kl: 0.00145215995144099
        model: {}
        policy_loss: -0.0032099774107337
        total_loss: -0.0028976555913686752
        vf_explained_var: 0.005523398518562317
        vf_loss: 22.93840789794922
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3617507517337799
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008664315682835877
        model: {}
        policy_loss: -0.002147958381101489
        total_loss: -0.0006467518396675587
        vf_explained_var: 0.07294429838657379
        vf_loss: 21.37887191772461
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8775655627250671
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020391184370964766
        model: {}
        policy_loss: -0.003617105772718787
        total_loss: -0.0029229847714304924
        vf_explained_var: 0.030302688479423523
        vf_loss: 22.386371612548828
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48600244522094727
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006497227004729211
        model: {}
        policy_loss: -0.0025703031569719315
        total_loss: -0.001325484598055482
        vf_explained_var: 0.09011527895927429
        vf_loss: 21.001813888549805
    load_time_ms: 13207.93
    num_steps_sampled: 44064000
    num_steps_trained: 44064000
    sample_time_ms: 91390.463
    update_time_ms: 21.906
  iterations_since_restore: 439
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 19.285875706214693
    ram_util_percent: 17.907344632768364
  pid: 4061
  policy_reward_max:
    agent-0: 205.8333333333331
    agent-1: 205.8333333333331
    agent-2: 205.8333333333331
    agent-3: 205.8333333333331
    agent-4: 205.8333333333331
    agent-5: 205.8333333333331
  policy_reward_mean:
    agent-0: 182.1733333333332
    agent-1: 182.1733333333332
    agent-2: 182.1733333333332
    agent-3: 182.1733333333332
    agent-4: 182.1733333333332
    agent-5: 182.1733333333332
  policy_reward_min:
    agent-0: 78.83333333333356
    agent-1: 78.83333333333356
    agent-2: 78.83333333333356
    agent-3: 78.83333333333356
    agent-4: 78.83333333333356
    agent-5: 78.83333333333356
  sampler_perf:
    mean_env_wait_ms: 24.468632001839897
    mean_inference_ms: 12.333835132128886
    mean_processing_ms: 50.94338718861558
  time_since_restore: 57185.592574596405
  time_this_iter_s: 124.14234972000122
  time_total_s: 60396.656260728836
  timestamp: 1637074789
  timesteps_since_restore: 42144000
  timesteps_this_iter: 96000
  timesteps_total: 44064000
  training_iteration: 459
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 26.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    459 |          60396.7 | 44064000 |  1093.04 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 40
    apples_agent-0_mean: 2.11
    apples_agent-0_min: 0
    apples_agent-1_max: 99
    apples_agent-1_mean: 22.52
    apples_agent-1_min: 0
    apples_agent-2_max: 96
    apples_agent-2_mean: 14.03
    apples_agent-2_min: 0
    apples_agent-3_max: 129
    apples_agent-3_mean: 56.69
    apples_agent-3_min: 20
    apples_agent-4_max: 15
    apples_agent-4_mean: 0.15
    apples_agent-4_min: 0
    apples_agent-5_max: 158
    apples_agent-5_mean: 101.01
    apples_agent-5_min: 48
    cleaning_beam_agent-0_max: 477
    cleaning_beam_agent-0_mean: 385.47
    cleaning_beam_agent-0_min: 246
    cleaning_beam_agent-1_max: 417
    cleaning_beam_agent-1_mean: 237.19
    cleaning_beam_agent-1_min: 116
    cleaning_beam_agent-2_max: 561
    cleaning_beam_agent-2_mean: 363.67
    cleaning_beam_agent-2_min: 166
    cleaning_beam_agent-3_max: 43
    cleaning_beam_agent-3_mean: 16.36
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 576
    cleaning_beam_agent-4_mean: 456.1
    cleaning_beam_agent-4_min: 368
    cleaning_beam_agent-5_max: 173
    cleaning_beam_agent-5_mean: 10.97
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_10-01-56
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1242.0000000000055
  episode_reward_mean: 1084.0199999999923
  episode_reward_min: 628.99999999999
  episodes_this_iter: 96
  episodes_total: 44160
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20297.008
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9099729657173157
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021431189961731434
        model: {}
        policy_loss: -0.003299205331131816
        total_loss: -0.002503385301679373
        vf_explained_var: 0.014721229672431946
        vf_loss: 23.973726272583008
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.188051462173462
        entropy_coeff: 0.0017600000137463212
        kl: 0.001363340299576521
        model: {}
        policy_loss: -0.0035397601313889027
        total_loss: -0.003169475356116891
        vf_explained_var: -0.011519521474838257
        vf_loss: 24.612552642822266
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1378231048583984
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017704744823276997
        model: {}
        policy_loss: -0.003191729774698615
        total_loss: -0.0026983460411429405
        vf_explained_var: -0.026839569211006165
        vf_loss: 24.95953941345215
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36666518449783325
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008635612321086228
        model: {}
        policy_loss: -0.002261374145746231
        total_loss: -0.0007229875773191452
        vf_explained_var: 0.10226911306381226
        vf_loss: 21.837158203125
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8747239112854004
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017948783934116364
        model: {}
        policy_loss: -0.0035696537233889103
        total_loss: -0.002778771333396435
        vf_explained_var: 0.041173070669174194
        vf_loss: 23.303958892822266
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48703059554100037
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010635561775416136
        model: {}
        policy_loss: -0.0026644570752978325
        total_loss: -0.0013577891513705254
        vf_explained_var: 0.11087314784526825
        vf_loss: 21.638418197631836
    load_time_ms: 13214.292
    num_steps_sampled: 44160000
    num_steps_trained: 44160000
    sample_time_ms: 91670.185
    update_time_ms: 21.844
  iterations_since_restore: 440
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.10164835164835
    ram_util_percent: 16.10879120879121
  pid: 4061
  policy_reward_max:
    agent-0: 206.99999999999966
    agent-1: 206.99999999999966
    agent-2: 206.99999999999966
    agent-3: 206.99999999999966
    agent-4: 206.99999999999966
    agent-5: 206.99999999999966
  policy_reward_mean:
    agent-0: 180.6699999999999
    agent-1: 180.6699999999999
    agent-2: 180.6699999999999
    agent-3: 180.6699999999999
    agent-4: 180.6699999999999
    agent-5: 180.6699999999999
  policy_reward_min:
    agent-0: 104.83333333333364
    agent-1: 104.83333333333364
    agent-2: 104.83333333333364
    agent-3: 104.83333333333364
    agent-4: 104.83333333333364
    agent-5: 104.83333333333364
  sampler_perf:
    mean_env_wait_ms: 24.47023443974915
    mean_inference_ms: 12.334171381049504
    mean_processing_ms: 50.94539196857576
  time_since_restore: 57312.757892131805
  time_this_iter_s: 127.16531753540039
  time_total_s: 60523.82157826424
  timestamp: 1637074916
  timesteps_since_restore: 42240000
  timesteps_this_iter: 96000
  timesteps_total: 44160000
  training_iteration: 460
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 33.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    460 |          60523.8 | 44160000 |  1084.02 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 21
    apples_agent-0_mean: 1.13
    apples_agent-0_min: 0
    apples_agent-1_max: 92
    apples_agent-1_mean: 21.77
    apples_agent-1_min: 0
    apples_agent-2_max: 122
    apples_agent-2_mean: 14.87
    apples_agent-2_min: 0
    apples_agent-3_max: 97
    apples_agent-3_mean: 54.14
    apples_agent-3_min: 29
    apples_agent-4_max: 1
    apples_agent-4_mean: 0.01
    apples_agent-4_min: 0
    apples_agent-5_max: 140
    apples_agent-5_mean: 99.27
    apples_agent-5_min: 54
    cleaning_beam_agent-0_max: 483
    cleaning_beam_agent-0_mean: 381.13
    cleaning_beam_agent-0_min: 276
    cleaning_beam_agent-1_max: 399
    cleaning_beam_agent-1_mean: 244.76
    cleaning_beam_agent-1_min: 130
    cleaning_beam_agent-2_max: 520
    cleaning_beam_agent-2_mean: 364.12
    cleaning_beam_agent-2_min: 166
    cleaning_beam_agent-3_max: 50
    cleaning_beam_agent-3_mean: 14.26
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 547
    cleaning_beam_agent-4_mean: 459.27
    cleaning_beam_agent-4_min: 308
    cleaning_beam_agent-5_max: 130
    cleaning_beam_agent-5_mean: 14.34
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_10-04-02
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1241.0000000000075
  episode_reward_mean: 1097.249999999994
  episode_reward_min: 734.9999999999939
  episodes_this_iter: 96
  episodes_total: 44256
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20254.047
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9018891453742981
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015321971150115132
        model: {}
        policy_loss: -0.00286888238042593
        total_loss: -0.0021641086786985397
        vf_explained_var: 0.004295945167541504
        vf_loss: 22.920991897583008
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1872069835662842
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018660258501768112
        model: {}
        policy_loss: -0.0038243522867560387
        total_loss: -0.003628375008702278
        vf_explained_var: 0.010753408074378967
        vf_loss: 22.854616165161133
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1229726076126099
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011043297126889229
        model: {}
        policy_loss: -0.00332018849439919
        total_loss: -0.003060433082282543
        vf_explained_var: 0.025333955883979797
        vf_loss: 22.361835479736328
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.33640390634536743
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016290772473439574
        model: {}
        policy_loss: -0.001955673098564148
        total_loss: -0.0004406026564538479
        vf_explained_var: 0.0845111608505249
        vf_loss: 21.07142448425293
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.872797429561615
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017919635865837336
        model: {}
        policy_loss: -0.003714130260050297
        total_loss: -0.00300576607696712
        vf_explained_var: 0.02402573823928833
        vf_loss: 22.444883346557617
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4954761266708374
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010893687140196562
        model: {}
        policy_loss: -0.0028576201293617487
        total_loss: -0.0016491233836859465
        vf_explained_var: 0.09520573914051056
        vf_loss: 20.80535125732422
    load_time_ms: 13204.826
    num_steps_sampled: 44256000
    num_steps_trained: 44256000
    sample_time_ms: 91750.023
    update_time_ms: 21.321
  iterations_since_restore: 441
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.40279329608939
    ram_util_percent: 20.506703910614526
  pid: 4061
  policy_reward_max:
    agent-0: 206.83333333333368
    agent-1: 206.83333333333368
    agent-2: 206.83333333333368
    agent-3: 206.83333333333368
    agent-4: 206.83333333333368
    agent-5: 206.83333333333368
  policy_reward_mean:
    agent-0: 182.87499999999983
    agent-1: 182.87499999999983
    agent-2: 182.87499999999983
    agent-3: 182.87499999999983
    agent-4: 182.87499999999983
    agent-5: 182.87499999999983
  policy_reward_min:
    agent-0: 122.50000000000034
    agent-1: 122.50000000000034
    agent-2: 122.50000000000034
    agent-3: 122.50000000000034
    agent-4: 122.50000000000034
    agent-5: 122.50000000000034
  sampler_perf:
    mean_env_wait_ms: 24.472251879393365
    mean_inference_ms: 12.334657752057549
    mean_processing_ms: 50.947424451252715
  time_since_restore: 57438.1565322876
  time_this_iter_s: 125.39864015579224
  time_total_s: 60649.22021842003
  timestamp: 1637075042
  timesteps_since_restore: 42336000
  timesteps_this_iter: 96000
  timesteps_total: 44256000
  training_iteration: 461
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 26.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.124:4061 |    461 |          60649.2 | 44256000 |  1097.25 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=4061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f826b3ab588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 38
    apples_agent-0_mean: 1.87
    apples_agent-0_min: 0
    apples_agent-1_max: 83
    apples_agent-1_mean: 26.22
    apples_agent-1_min: 0
    apples_agent-2_max: 89
    apples_agent-2_mean: 15.18
    apples_agent-2_min: 0
    apples_agent-3_max: 121
    apples_agent-3_mean: 49.74
    apples_agent-3_min: 29
    apples_agent-4_max: 42
    apples_agent-4_mean: 0.78
    apples_agent-4_min: 0
    apples_agent-5_max: 154
    apples_agent-5_mean: 100.57
    apples_agent-5_min: 24
    cleaning_beam_agent-0_max: 483
    cleaning_beam_agent-0_mean: 390.49
    cleaning_beam_agent-0_min: 278
    cleaning_beam_agent-1_max: 444
    cleaning_beam_agent-1_mean: 230.28
    cleaning_beam_agent-1_min: 101
    cleaning_beam_agent-2_max: 616
    cleaning_beam_agent-2_mean: 369.98
    cleaning_beam_agent-2_min: 207
    cleaning_beam_agent-3_max: 48
    cleaning_beam_agent-3_mean: 16.9
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 550
    cleaning_beam_agent-4_mean: 451.71
    cleaning_beam_agent-4_min: 362
    cleaning_beam_agent-5_max: 216
    cleaning_beam_agent-5_mean: 14.43
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_10-06-05
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1256.999999999981
  episode_reward_mean: 1075.679999999993
  episode_reward_min: 405.00000000000597
  episodes_this_iter: 96
  episodes_total: 44352
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu124.cluster.local
  info:
    grad_time_ms: 20250.394
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9112504720687866
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011219190200790763
        model: {}
        policy_loss: -0.002851782599464059
        total_loss: -0.002055511111393571
        vf_explained_var: 0.061192288994789124
        vf_loss: 24.000736236572266
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1895610094070435
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013563852990046144
        model: {}
        policy_loss: -0.003741790074855089
        total_loss: -0.0032580620609223843
        vf_explained_var: -0.00769554078578949
        vf_loss: 25.773563385009766
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1356955766677856
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012857387773692608
        model: {}
        policy_loss: -0.003454434685409069
        total_loss: -0.0029107648879289627
        vf_explained_var: 0.005790457129478455
        vf_loss: 25.42496109008789
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35050734877586365
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014733364805579185
        model: {}
        policy_loss: -0.002242035698145628
        total_loss: -0.0006371899507939816
        vf_explained_var: 0.13121607899665833
        vf_loss: 22.217382431030273
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8766956925392151
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015397309325635433
        model: {}
        policy_loss: -0.003270483575761318
        total_loss: -0.002355919685214758
        vf_explained_var: 0.0397956520318985
        vf_loss: 24.575469970703125
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5157215595245361
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011827530106529593
        model: {}
        policy_loss: -0.003015562891960144
        total_loss: -0.0016771703958511353
        vf_explained_var: 0.12187747657299042
        vf_loss: 22.46060562133789
    load_time_ms: 13227.73
    num_steps_sampled: 44352000
    num_steps_trained: 44352000
    sample_time_ms: 91569.882
    update_time_ms: 21.297
  iterations_since_restore: 442
  node_ip: 172.17.8.124
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.451428571428572
    ram_util_percent: 14.836000000000004
  pid: 4061
  policy_reward_max:
    agent-0: 209.49999999999986
    agent-1: 209.49999999999986
    agent-2: 209.49999999999986
    agent-3: 209.49999999999986
    agent-4: 209.49999999999986
    agent-5: 209.49999999999986
  policy_reward_mean:
    agent-0: 179.2799999999999
    agent-1: 179.2799999999999
    agent-2: 179.2799999999999
    agent-3: 179.2799999999999
    agent-4: 179.2799999999999
    agent-5: 179.2799999999999
  policy_reward_min:
    agent-0: 67.4999999999999
    agent-1: 67.4999999999999
    agent-2: 67.4999999999999
    agent-3: 67.4999999999999
    agent-4: 67.4999999999999
    agent-5: 67.4999999999999
  sampler_perf:
    mean_env_wait_ms: 24.472571484031008
    mean_inference_ms: 12.3341506851731
    mean_processing_ms: 50.94604189829073
  time_since_restore: 57561.3258972168
  time_this_iter_s: 123.16936492919922
  time_total_s: 60772.38958334923
  timestamp: 1637075165
  timesteps_since_restore: 42432000
  timesteps_this_iter: 96000
  timesteps_total: 44352000
  training_iteration: 462
  trial_id: '00000'
  >>> RESTORING FROM:  /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics/BaselinePPOTrainer_cleanup_env_0_2021-11-15_17-00-45_bf59qjr/checkpoint_500
== Status ==
Memory usage on this node: 14.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------+--------+------------------+----------+----------+
| Trial name                           | status   | loc   |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  |       |    440 |          69193.5 | 42240000 |  1095.76 |
+--------------------------------------+----------+-------+--------+------------------+----------+----------+


[2m[36m(pid=38661)[0m 2021-11-16 16:37:34,009	INFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution
[2m[36m(pid=38661)[0m 2021-11-16 16:37:34,024	INFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=38661)[0m 2021-11-16 16:39:21,637	INFO trainable.py:180 -- _setup took 107.628 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(pid=38661)[0m 2021-11-16 16:39:21,637	INFO trainable.py:217 -- Getting current IP.
[2m[36m(pid=38661)[0m 2021-11-16 16:39:21,638	WARNING util.py:37 -- Install gputil for GPU system monitoring.
[2m[36m(pid=38661)[0m 2021-11-16 16:39:24,777	INFO trainable.py:217 -- Getting current IP.
[2m[36m(pid=38661)[0m 2021-11-16 16:39:24,777	INFO trainable.py:423 -- Restored on 172.17.8.131 from checkpoint: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics/BaselinePPOTrainer_cleanup_env_0_2021-11-15_17-00-45_bf59qjr/tmpm9gbp9fkrestore_from_object/checkpoint-440
[2m[36m(pid=38661)[0m 2021-11-16 16:39:24,777	INFO trainable.py:430 -- Current state after restoring: {'_iteration': 440, '_timesteps_total': 42240000, '_time_total': 69193.48181247711, '_episodes_total': 42240}
== Status ==
Memory usage on this node: 26.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------+--------+------------------+----------+----------+
| Trial name                           | status   | loc   |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  |       |    440 |          69193.5 | 42240000 |  1095.76 |
+--------------------------------------+----------+-------+--------+------------------+----------+----------+


[2m[36m(pid=38661)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff6f2e62518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 68
    apples_agent-0_mean: 2.1666666666666665
    apples_agent-0_min: 0
    apples_agent-1_max: 95
    apples_agent-1_mean: 22.59375
    apples_agent-1_min: 0
    apples_agent-2_max: 181
    apples_agent-2_mean: 4.885416666666667
    apples_agent-2_min: 0
    apples_agent-3_max: 214
    apples_agent-3_mean: 96.69791666666667
    apples_agent-3_min: 51
    apples_agent-4_max: 61
    apples_agent-4_mean: 1.21875
    apples_agent-4_min: 0
    apples_agent-5_max: 119
    apples_agent-5_mean: 76.0
    apples_agent-5_min: 41
    cleaning_beam_agent-0_max: 523
    cleaning_beam_agent-0_mean: 381.7708333333333
    cleaning_beam_agent-0_min: 120
    cleaning_beam_agent-1_max: 388
    cleaning_beam_agent-1_mean: 244.89583333333334
    cleaning_beam_agent-1_min: 112
    cleaning_beam_agent-2_max: 507
    cleaning_beam_agent-2_mean: 393.2291666666667
    cleaning_beam_agent-2_min: 199
    cleaning_beam_agent-3_max: 62
    cleaning_beam_agent-3_mean: 10.552083333333334
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 540
    cleaning_beam_agent-4_mean: 436.3020833333333
    cleaning_beam_agent-4_min: 183
    cleaning_beam_agent-5_max: 96
    cleaning_beam_agent-5_mean: 15.958333333333334
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.010416666666666666
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_16-43-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1242.9999999999889
  episode_reward_mean: 1078.9062499999934
  episode_reward_min: 418.0000000000037
  episodes_this_iter: 96
  episodes_total: 42336
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu131.cluster.local
  info:
    grad_time_ms: 34095.901
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9977748394012451
        entropy_coeff: 0.0017600000137463212
        kl: 0.001298933057114482
        model: {}
        policy_loss: -0.003728618612512946
        total_loss: -0.0027670504059642553
        vf_explained_var: 0.04632124304771423
        vf_loss: 24.578678131103516
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1442316770553589
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015446953475475311
        model: {}
        policy_loss: -0.004552600905299187
        total_loss: -0.0035768686793744564
        vf_explained_var: -0.03771832585334778
        vf_loss: 26.806446075439453
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0964514017105103
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013771296944469213
        model: {}
        policy_loss: -0.004121681675314903
        total_loss: -0.003174056066200137
        vf_explained_var: 0.00016626715660095215
        vf_loss: 26.019521713256836
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35557660460472107
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007081391522660851
        model: {}
        policy_loss: -0.0023885921109467745
        total_loss: -0.0005660222377628088
        vf_explained_var: 0.10768000781536102
        vf_loss: 23.067577362060547
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9551618099212646
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010816353606060147
        model: {}
        policy_loss: -0.003791447728872299
        total_loss: -0.00286306906491518
        vf_explained_var: 0.07282310724258423
        vf_loss: 23.931352615356445
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48923301696777344
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007408485398627818
        model: {}
        policy_loss: -0.0029225926846265793
        total_loss: -0.0013831937685608864
        vf_explained_var: 0.12796054780483246
        vf_loss: 22.522777557373047
    load_time_ms: 101943.37
    num_steps_sampled: 42336000
    num_steps_trained: 42336000
    sample_time_ms: 100048.739
    update_time_ms: 3198.962
  iterations_since_restore: 1
  node_ip: 172.17.8.131
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.775683060109287
    ram_util_percent: 18.515573770491805
  pid: 38661
  policy_reward_max:
    agent-0: 207.16666666666643
    agent-1: 207.16666666666643
    agent-2: 207.16666666666643
    agent-3: 207.16666666666643
    agent-4: 207.16666666666643
    agent-5: 207.16666666666643
  policy_reward_mean:
    agent-0: 179.81770833333317
    agent-1: 179.81770833333317
    agent-2: 179.81770833333317
    agent-3: 179.81770833333317
    agent-4: 179.81770833333317
    agent-5: 179.81770833333317
  policy_reward_min:
    agent-0: 69.66666666666669
    agent-1: 69.66666666666669
    agent-2: 69.66666666666669
    agent-3: 69.66666666666669
    agent-4: 69.66666666666669
    agent-5: 69.66666666666669
  sampler_perf:
    mean_env_wait_ms: 26.219521170650125
    mean_inference_ms: 13.640862602096691
    mean_processing_ms: 53.78795253646958
  time_since_restore: 247.79151368141174
  time_this_iter_s: 247.79151368141174
  time_total_s: 69441.27332615852
  timestamp: 1637099018
  timesteps_since_restore: 96000
  timesteps_this_iter: 96000
  timesteps_total: 42336000
  training_iteration: 441
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 33.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.131:38661 |    441 |          69441.3 | 42336000 |  1078.91 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=38661)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff6f2e62518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 81
    apples_agent-0_mean: 2.31
    apples_agent-0_min: 0
    apples_agent-1_max: 119
    apples_agent-1_mean: 31.0
    apples_agent-1_min: 0
    apples_agent-2_max: 68
    apples_agent-2_mean: 3.32
    apples_agent-2_min: 0
    apples_agent-3_max: 129
    apples_agent-3_mean: 93.71
    apples_agent-3_min: 46
    apples_agent-4_max: 69
    apples_agent-4_mean: 1.17
    apples_agent-4_min: 0
    apples_agent-5_max: 124
    apples_agent-5_mean: 78.62
    apples_agent-5_min: 52
    cleaning_beam_agent-0_max: 502
    cleaning_beam_agent-0_mean: 392.39
    cleaning_beam_agent-0_min: 177
    cleaning_beam_agent-1_max: 342
    cleaning_beam_agent-1_mean: 236.02
    cleaning_beam_agent-1_min: 132
    cleaning_beam_agent-2_max: 549
    cleaning_beam_agent-2_mean: 388.15
    cleaning_beam_agent-2_min: 224
    cleaning_beam_agent-3_max: 55
    cleaning_beam_agent-3_mean: 9.08
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 518
    cleaning_beam_agent-4_mean: 446.51
    cleaning_beam_agent-4_min: 161
    cleaning_beam_agent-5_max: 82
    cleaning_beam_agent-5_mean: 15.48
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_16-48-00
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1262.000000000001
  episode_reward_mean: 1094.9099999999942
  episode_reward_min: 676.0000000000053
  episodes_this_iter: 96
  episodes_total: 42432
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu131.cluster.local
  info:
    grad_time_ms: 26561.38
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0129386186599731
        entropy_coeff: 0.0017600000137463212
        kl: 0.001191302202641964
        model: {}
        policy_loss: -0.0029289182275533676
        total_loss: -0.0023164041340351105
        vf_explained_var: 0.03394336998462677
        vf_loss: 22.761558532714844
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1413894891738892
        entropy_coeff: 0.0017600000137463212
        kl: 0.00109661347232759
        model: {}
        policy_loss: -0.003791848197579384
        total_loss: -0.003182424232363701
        vf_explained_var: -0.05634239315986633
        vf_loss: 25.086078643798828
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 1.2000000424450263e-05
        entropy: 1.108323335647583
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014457181096076965
        model: {}
        policy_loss: -0.0034134946763515472
        total_loss: -0.002765282988548279
        vf_explained_var: -0.028976410627365112
        vf_loss: 24.542922973632812
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 1.2000000424450263e-05
        entropy: 0.332905650138855
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007994977640919387
        model: {}
        policy_loss: -0.001848398707807064
        total_loss: -0.00024580443277955055
        vf_explained_var: 0.10556237399578094
        vf_loss: 21.08559799194336
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9463567733764648
        entropy_coeff: 0.0017600000137463212
        kl: 0.001077916007488966
        model: {}
        policy_loss: -0.003387059550732374
        total_loss: -0.0026787351816892624
        vf_explained_var: 0.04230782389640808
        vf_loss: 22.661212921142578
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4916144013404846
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006784868892282248
        model: {}
        policy_loss: -0.002395637333393097
        total_loss: -0.0011219140142202377
        vf_explained_var: 0.12368758022785187
        vf_loss: 20.711132049560547
    load_time_ms: 78134.415
    num_steps_sampled: 42432000
    num_steps_trained: 42432000
    sample_time_ms: 136127.626
    update_time_ms: 1648.962
  iterations_since_restore: 2
  node_ip: 172.17.8.131
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.065819209039546
    ram_util_percent: 19.14774011299435
  pid: 38661
  policy_reward_max:
    agent-0: 210.3333333333336
    agent-1: 210.3333333333336
    agent-2: 210.3333333333336
    agent-3: 210.3333333333336
    agent-4: 210.3333333333336
    agent-5: 210.3333333333336
  policy_reward_mean:
    agent-0: 182.4849999999999
    agent-1: 182.4849999999999
    agent-2: 182.4849999999999
    agent-3: 182.4849999999999
    agent-4: 182.4849999999999
    agent-5: 182.4849999999999
  policy_reward_min:
    agent-0: 112.66666666666674
    agent-1: 112.66666666666674
    agent-2: 112.66666666666674
    agent-3: 112.66666666666674
    agent-4: 112.66666666666674
    agent-5: 112.66666666666674
  sampler_perf:
    mean_env_wait_ms: 27.285245077095492
    mean_inference_ms: 13.197067431018914
    mean_processing_ms: 54.47612557320835
  time_since_restore: 493.8784124851227
  time_this_iter_s: 246.08689880371094
  time_total_s: 69687.36022496223
  timestamp: 1637099280
  timesteps_since_restore: 192000
  timesteps_this_iter: 96000
  timesteps_total: 42432000
  training_iteration: 442
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 36.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.131:38661 |    442 |          69687.4 | 42432000 |  1094.91 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=38661)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff6f2e62518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 64
    apples_agent-0_mean: 2.77
    apples_agent-0_min: 0
    apples_agent-1_max: 114
    apples_agent-1_mean: 25.45
    apples_agent-1_min: 0
    apples_agent-2_max: 195
    apples_agent-2_mean: 11.38
    apples_agent-2_min: 0
    apples_agent-3_max: 147
    apples_agent-3_mean: 93.24
    apples_agent-3_min: 54
    apples_agent-4_max: 111
    apples_agent-4_mean: 2.57
    apples_agent-4_min: 0
    apples_agent-5_max: 178
    apples_agent-5_mean: 81.59
    apples_agent-5_min: 45
    cleaning_beam_agent-0_max: 494
    cleaning_beam_agent-0_mean: 399.72
    cleaning_beam_agent-0_min: 274
    cleaning_beam_agent-1_max: 415
    cleaning_beam_agent-1_mean: 252.7
    cleaning_beam_agent-1_min: 126
    cleaning_beam_agent-2_max: 499
    cleaning_beam_agent-2_mean: 370.67
    cleaning_beam_agent-2_min: 114
    cleaning_beam_agent-3_max: 48
    cleaning_beam_agent-3_mean: 9.99
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 536
    cleaning_beam_agent-4_mean: 441.02
    cleaning_beam_agent-4_min: 292
    cleaning_beam_agent-5_max: 149
    cleaning_beam_agent-5_mean: 18.37
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_16-51-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1246.9999999999911
  episode_reward_mean: 1065.4899999999936
  episode_reward_min: 521.0000000000097
  episodes_this_iter: 96
  episodes_total: 42528
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu131.cluster.local
  info:
    grad_time_ms: 23880.444
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 1.2000000424450263e-05
        entropy: 1.004581332206726
        entropy_coeff: 0.0017600000137463212
        kl: 0.0030683132354170084
        model: {}
        policy_loss: -0.003396023530513048
        total_loss: -0.0026826136745512486
        vf_explained_var: 0.012660607695579529
        vf_loss: 23.28058624267578
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1623938083648682
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014007129939273
        model: {}
        policy_loss: -0.0036339224316179752
        total_loss: -0.0032474123872816563
        vf_explained_var: -0.002337813377380371
        vf_loss: 23.622882843017578
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1066067218780518
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012820499250665307
        model: {}
        policy_loss: -0.003620691830292344
        total_loss: -0.00290029589086771
        vf_explained_var: -0.09799826145172119
        vf_loss: 26.039203643798828
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 1.2000000424450263e-05
        entropy: 0.34739869832992554
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008426120621152222
        model: {}
        policy_loss: -0.0018058712594211102
        total_loss: -0.00026990752667188644
        vf_explained_var: 0.10775609314441681
        vf_loss: 21.052597045898438
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9419800043106079
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018495949916541576
        model: {}
        policy_loss: -0.0034723179414868355
        total_loss: -0.0028237428050488234
        vf_explained_var: 0.06082804501056671
        vf_loss: 22.139806747436523
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4988382160663605
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006688894354738295
        model: {}
        policy_loss: -0.0023282887414097786
        total_loss: -0.0011055564973503351
        vf_explained_var: 0.12339936196804047
        vf_loss: 20.672443389892578
    load_time_ms: 74466.145
    num_steps_sampled: 42528000
    num_steps_trained: 42528000
    sample_time_ms: 137623.285
    update_time_ms: 1122.374
  iterations_since_restore: 3
  node_ip: 172.17.8.131
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.79192546583851
    ram_util_percent: 18.950310559006212
  pid: 38661
  policy_reward_max:
    agent-0: 207.83333333333366
    agent-1: 207.83333333333366
    agent-2: 207.83333333333366
    agent-3: 207.83333333333366
    agent-4: 207.83333333333366
    agent-5: 207.83333333333366
  policy_reward_mean:
    agent-0: 177.58166666666654
    agent-1: 177.58166666666654
    agent-2: 177.58166666666654
    agent-3: 177.58166666666654
    agent-4: 177.58166666666654
    agent-5: 177.58166666666654
  policy_reward_min:
    agent-0: 86.83333333333344
    agent-1: 86.83333333333344
    agent-2: 86.83333333333344
    agent-3: 86.83333333333344
    agent-4: 86.83333333333344
    agent-5: 86.83333333333344
  sampler_perf:
    mean_env_wait_ms: 26.928046435175364
    mean_inference_ms: 12.917028386411268
    mean_processing_ms: 54.176457537001816
  time_since_restore: 720.2979874610901
  time_this_iter_s: 226.4195749759674
  time_total_s: 69913.7797999382
  timestamp: 1637099507
  timesteps_since_restore: 288000
  timesteps_this_iter: 96000
  timesteps_total: 42528000
  training_iteration: 443
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 32.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.131:38661 |    443 |          69913.8 | 42528000 |  1065.49 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=38661)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff6f2e62518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 39
    apples_agent-0_mean: 2.06
    apples_agent-0_min: 0
    apples_agent-1_max: 104
    apples_agent-1_mean: 27.39
    apples_agent-1_min: 0
    apples_agent-2_max: 212
    apples_agent-2_mean: 7.17
    apples_agent-2_min: 0
    apples_agent-3_max: 137
    apples_agent-3_mean: 95.29
    apples_agent-3_min: 45
    apples_agent-4_max: 78
    apples_agent-4_mean: 1.74
    apples_agent-4_min: 0
    apples_agent-5_max: 225
    apples_agent-5_mean: 75.25
    apples_agent-5_min: 45
    cleaning_beam_agent-0_max: 553
    cleaning_beam_agent-0_mean: 408.56
    cleaning_beam_agent-0_min: 123
    cleaning_beam_agent-1_max: 440
    cleaning_beam_agent-1_mean: 237.66
    cleaning_beam_agent-1_min: 129
    cleaning_beam_agent-2_max: 589
    cleaning_beam_agent-2_mean: 407.23
    cleaning_beam_agent-2_min: 170
    cleaning_beam_agent-3_max: 70
    cleaning_beam_agent-3_mean: 10.82
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 553
    cleaning_beam_agent-4_mean: 443.78
    cleaning_beam_agent-4_min: 286
    cleaning_beam_agent-5_max: 95
    cleaning_beam_agent-5_mean: 17.8
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_16-55-35
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1254.0000000000118
  episode_reward_mean: 1070.259999999995
  episode_reward_min: 378.0000000000036
  episodes_this_iter: 96
  episodes_total: 42624
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu131.cluster.local
  info:
    grad_time_ms: 22514.512
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0076509714126587
        entropy_coeff: 0.0017600000137463212
        kl: 0.001707532093860209
        model: {}
        policy_loss: -0.0029801353812217712
        total_loss: -0.002106990898028016
        vf_explained_var: 0.048660263419151306
        vf_loss: 26.039213180541992
      agent-1:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1509214639663696
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015720970695838332
        model: {}
        policy_loss: -0.004125840496271849
        total_loss: -0.0033236437011510134
        vf_explained_var: -0.01801764965057373
        vf_loss: 27.885177612304688
      agent-2:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0882978439331055
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017386344261467457
        model: {}
        policy_loss: -0.0036916700191795826
        total_loss: -0.0028752246871590614
        vf_explained_var: 0.016849592328071594
        vf_loss: 26.88384437561035
      agent-3:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3496992886066437
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007381337927654386
        model: {}
        policy_loss: -0.0020675165578722954
        total_loss: -0.0002879370003938675
        vf_explained_var: 0.1306566298007965
        vf_loss: 23.76601219177246
      agent-4:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9411987066268921
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019229406025260687
        model: {}
        policy_loss: -0.0038660727441310883
        total_loss: -0.0029112817719578743
        vf_explained_var: 0.06222786009311676
        vf_loss: 25.63227653503418
      agent-5:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 1.2000000424450263e-05
        entropy: 0.494251012802124
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011285507353022695
        model: {}
        policy_loss: -0.002640369115397334
        total_loss: -0.0011136136017739773
        vf_explained_var: 0.13476251065731049
        vf_loss: 23.68425750732422
    load_time_ms: 69696.261
    num_steps_sampled: 42624000
    num_steps_trained: 42624000
    sample_time_ms: 141848.04
    update_time_ms: 854.119
  iterations_since_restore: 4
  node_ip: 172.17.8.131
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.10705521472393
    ram_util_percent: 18.691411042944782
  pid: 38661
  policy_reward_max:
    agent-0: 208.9999999999999
    agent-1: 208.9999999999999
    agent-2: 208.9999999999999
    agent-3: 208.9999999999999
    agent-4: 208.9999999999999
    agent-5: 208.9999999999999
  policy_reward_mean:
    agent-0: 178.3766666666665
    agent-1: 178.3766666666665
    agent-2: 178.3766666666665
    agent-3: 178.3766666666665
    agent-4: 178.3766666666665
    agent-5: 178.3766666666665
  policy_reward_min:
    agent-0: 62.99999999999977
    agent-1: 62.99999999999977
    agent-2: 62.99999999999977
    agent-3: 62.99999999999977
    agent-4: 62.99999999999977
    agent-5: 62.99999999999977
  sampler_perf:
    mean_env_wait_ms: 26.785081997311348
    mean_inference_ms: 12.775088551032654
    mean_processing_ms: 54.01354473143745
  time_since_restore: 948.7622761726379
  time_this_iter_s: 228.46428871154785
  time_total_s: 70142.24408864975
  timestamp: 1637099735
  timesteps_since_restore: 384000
  timesteps_this_iter: 96000
  timesteps_total: 42624000
  training_iteration: 444
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 36.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.131:38661 |    444 |          70142.2 | 42624000 |  1070.26 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=38661)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff6f2e62518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 64
    apples_agent-0_mean: 2.5
    apples_agent-0_min: 0
    apples_agent-1_max: 81
    apples_agent-1_mean: 22.1
    apples_agent-1_min: 0
    apples_agent-2_max: 128
    apples_agent-2_mean: 5.73
    apples_agent-2_min: 0
    apples_agent-3_max: 129
    apples_agent-3_mean: 93.71
    apples_agent-3_min: 38
    apples_agent-4_max: 32
    apples_agent-4_mean: 0.55
    apples_agent-4_min: 0
    apples_agent-5_max: 137
    apples_agent-5_mean: 76.9
    apples_agent-5_min: 47
    cleaning_beam_agent-0_max: 548
    cleaning_beam_agent-0_mean: 417.38
    cleaning_beam_agent-0_min: 185
    cleaning_beam_agent-1_max: 368
    cleaning_beam_agent-1_mean: 228.42
    cleaning_beam_agent-1_min: 105
    cleaning_beam_agent-2_max: 553
    cleaning_beam_agent-2_mean: 404.26
    cleaning_beam_agent-2_min: 160
    cleaning_beam_agent-3_max: 39
    cleaning_beam_agent-3_mean: 8.89
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 559
    cleaning_beam_agent-4_mean: 445.78
    cleaning_beam_agent-4_min: 284
    cleaning_beam_agent-5_max: 114
    cleaning_beam_agent-5_mean: 15.02
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_16-59-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1270.0000000000066
  episode_reward_mean: 1093.0099999999945
  episode_reward_min: 533.000000000003
  episodes_this_iter: 96
  episodes_total: 42720
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu131.cluster.local
  info:
    grad_time_ms: 21712.594
    learner:
      agent-0:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0047662258148193
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013923889491707087
        model: {}
        policy_loss: -0.0030466115567833185
        total_loss: -0.002429710701107979
        vf_explained_var: 0.01958593726158142
        vf_loss: 23.6788272857666
      agent-1:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1600247621536255
        entropy_coeff: 0.0017600000137463212
        kl: 0.001340592629276216
        model: {}
        policy_loss: -0.0035626408644020557
        total_loss: -0.003007294610142708
        vf_explained_var: -0.05957043170928955
        vf_loss: 25.802324295043945
      agent-2:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0888535976409912
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015678033232688904
        model: {}
        policy_loss: -0.003607101272791624
        total_loss: -0.003030605847015977
        vf_explained_var: -0.020896315574645996
        vf_loss: 24.7327880859375
      agent-3:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3240109384059906
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006526976358145475
        model: {}
        policy_loss: -0.0017435969784855843
        total_loss: -8.158572018146515e-05
        vf_explained_var: 0.08076310157775879
        vf_loss: 22.24114990234375
      agent-4:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9439414739608765
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015565380454063416
        model: {}
        policy_loss: -0.0034643993712961674
        total_loss: -0.002806863747537136
        vf_explained_var: 0.051107823848724365
        vf_loss: 22.9941463470459
      agent-5:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4829177260398865
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008162236190401018
        model: {}
        policy_loss: -0.0024683098308742046
        total_loss: -0.0011647450737655163
        vf_explained_var: 0.1133185625076294
        vf_loss: 21.43297576904297
    load_time_ms: 64987.515
    num_steps_sampled: 42720000
    num_steps_trained: 42720000
    sample_time_ms: 147256.151
    update_time_ms: 695.047
  iterations_since_restore: 5
  node_ip: 172.17.8.131
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.949849849849848
    ram_util_percent: 18.581381381381384
  pid: 38661
  policy_reward_max:
    agent-0: 211.66666666666683
    agent-1: 211.66666666666683
    agent-2: 211.66666666666683
    agent-3: 211.66666666666683
    agent-4: 211.66666666666683
    agent-5: 211.66666666666683
  policy_reward_mean:
    agent-0: 182.16833333333315
    agent-1: 182.16833333333315
    agent-2: 182.16833333333315
    agent-3: 182.16833333333315
    agent-4: 182.16833333333315
    agent-5: 182.16833333333315
  policy_reward_min:
    agent-0: 88.8333333333333
    agent-1: 88.8333333333333
    agent-2: 88.8333333333333
    agent-3: 88.8333333333333
    agent-4: 88.8333333333333
    agent-5: 88.8333333333333
  sampler_perf:
    mean_env_wait_ms: 26.688245248219587
    mean_inference_ms: 12.695719513642816
    mean_processing_ms: 53.94354108213063
  time_since_restore: 1182.4782433509827
  time_this_iter_s: 233.71596717834473
  time_total_s: 70375.9600558281
  timestamp: 1637099969
  timesteps_since_restore: 480000
  timesteps_this_iter: 96000
  timesteps_total: 42720000
  training_iteration: 445
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.131:38661 |    445 |            70376 | 42720000 |  1093.01 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=38661)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff6f2e62518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 41
    apples_agent-0_mean: 1.67
    apples_agent-0_min: 0
    apples_agent-1_max: 92
    apples_agent-1_mean: 29.89
    apples_agent-1_min: 0
    apples_agent-2_max: 105
    apples_agent-2_mean: 2.89
    apples_agent-2_min: 0
    apples_agent-3_max: 155
    apples_agent-3_mean: 95.21
    apples_agent-3_min: 52
    apples_agent-4_max: 101
    apples_agent-4_mean: 2.63
    apples_agent-4_min: 0
    apples_agent-5_max: 135
    apples_agent-5_mean: 78.03
    apples_agent-5_min: 47
    cleaning_beam_agent-0_max: 565
    cleaning_beam_agent-0_mean: 434.93
    cleaning_beam_agent-0_min: 311
    cleaning_beam_agent-1_max: 340
    cleaning_beam_agent-1_mean: 233.29
    cleaning_beam_agent-1_min: 125
    cleaning_beam_agent-2_max: 519
    cleaning_beam_agent-2_mean: 394.12
    cleaning_beam_agent-2_min: 199
    cleaning_beam_agent-3_max: 39
    cleaning_beam_agent-3_mean: 9.55
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 550
    cleaning_beam_agent-4_mean: 439.82
    cleaning_beam_agent-4_min: 236
    cleaning_beam_agent-5_max: 66
    cleaning_beam_agent-5_mean: 13.28
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_17-02-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1231.0000000000066
  episode_reward_mean: 1090.859999999995
  episode_reward_min: 627.0000000000089
  episodes_this_iter: 96
  episodes_total: 42816
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu131.cluster.local
  info:
    grad_time_ms: 21209.819
    learner:
      agent-0:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0164549350738525
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017281889449805021
        model: {}
        policy_loss: -0.003018410410732031
        total_loss: -0.0024285553954541683
        vf_explained_var: 0.028234779834747314
        vf_loss: 23.680133819580078
      agent-1:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 1.2000000424450263e-05
        entropy: 1.16153883934021
        entropy_coeff: 0.0017600000137463212
        kl: 0.002263442613184452
        model: {}
        policy_loss: -0.003992184530943632
        total_loss: -0.003418498206883669
        vf_explained_var: -0.06139439344406128
        vf_loss: 26.0384521484375
      agent-2:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1081886291503906
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015398409450426698
        model: {}
        policy_loss: -0.003523179329931736
        total_loss: -0.0029650419019162655
        vf_explained_var: -0.02116292715072632
        vf_loss: 24.9892520904541
      agent-3:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 1.2000000424450263e-05
        entropy: 0.33178824186325073
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009304795530624688
        model: {}
        policy_loss: -0.0018685627728700638
        total_loss: -0.00024605938233435154
        vf_explained_var: 0.09629921615123749
        vf_loss: 22.006370544433594
      agent-4:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9425395727157593
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016562964301556349
        model: {}
        policy_loss: -0.0035270974040031433
        total_loss: -0.0028838245198130608
        vf_explained_var: 0.0607120543718338
        vf_loss: 22.917938232421875
      agent-5:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4861892759799957
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009574461728334427
        model: {}
        policy_loss: -0.002359279664233327
        total_loss: -0.0010259812697768211
        vf_explained_var: 0.10578122735023499
        vf_loss: 21.830101013183594
    load_time_ms: 62657.924
    num_steps_sampled: 42816000
    num_steps_trained: 42816000
    sample_time_ms: 145676.299
    update_time_ms: 591.472
  iterations_since_restore: 6
  node_ip: 172.17.8.131
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.692905405405405
    ram_util_percent: 18.54054054054054
  pid: 38661
  policy_reward_max:
    agent-0: 205.1666666666664
    agent-1: 205.1666666666664
    agent-2: 205.1666666666664
    agent-3: 205.1666666666664
    agent-4: 205.1666666666664
    agent-5: 205.1666666666664
  policy_reward_mean:
    agent-0: 181.80999999999986
    agent-1: 181.80999999999986
    agent-2: 181.80999999999986
    agent-3: 181.80999999999986
    agent-4: 181.80999999999986
    agent-5: 181.80999999999986
  policy_reward_min:
    agent-0: 104.50000000000036
    agent-1: 104.50000000000036
    agent-2: 104.50000000000036
    agent-3: 104.50000000000036
    agent-4: 104.50000000000036
    agent-5: 104.50000000000036
  sampler_perf:
    mean_env_wait_ms: 26.630477925990768
    mean_inference_ms: 12.637356551432136
    mean_processing_ms: 53.88011870634974
  time_since_restore: 1390.117948293686
  time_this_iter_s: 207.63970494270325
  time_total_s: 70583.5997607708
  timestamp: 1637100177
  timesteps_since_restore: 576000
  timesteps_this_iter: 96000
  timesteps_total: 42816000
  training_iteration: 446
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.131:38661 |    446 |          70583.6 | 42816000 |  1090.86 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=38661)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff6f2e62518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 223
    apples_agent-0_mean: 4.21
    apples_agent-0_min: 0
    apples_agent-1_max: 107
    apples_agent-1_mean: 28.45
    apples_agent-1_min: 0
    apples_agent-2_max: 76
    apples_agent-2_mean: 5.07
    apples_agent-2_min: 0
    apples_agent-3_max: 196
    apples_agent-3_mean: 96.35
    apples_agent-3_min: 41
    apples_agent-4_max: 101
    apples_agent-4_mean: 3.75
    apples_agent-4_min: 0
    apples_agent-5_max: 165
    apples_agent-5_mean: 77.03
    apples_agent-5_min: 43
    cleaning_beam_agent-0_max: 535
    cleaning_beam_agent-0_mean: 401.18
    cleaning_beam_agent-0_min: 232
    cleaning_beam_agent-1_max: 406
    cleaning_beam_agent-1_mean: 224.23
    cleaning_beam_agent-1_min: 109
    cleaning_beam_agent-2_max: 515
    cleaning_beam_agent-2_mean: 380.97
    cleaning_beam_agent-2_min: 167
    cleaning_beam_agent-3_max: 60
    cleaning_beam_agent-3_mean: 10.67
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 563
    cleaning_beam_agent-4_mean: 438.67
    cleaning_beam_agent-4_min: 230
    cleaning_beam_agent-5_max: 129
    cleaning_beam_agent-5_mean: 16.13
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_17-06-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1273.9999999999939
  episode_reward_mean: 1056.7199999999953
  episode_reward_min: 382.00000000000557
  episodes_this_iter: 96
  episodes_total: 42912
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu131.cluster.local
  info:
    grad_time_ms: 20827.763
    learner:
      agent-0:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0145479440689087
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017615756951272488
        model: {}
        policy_loss: -0.003105836920440197
        total_loss: -0.0024654166772961617
        vf_explained_var: 0.08516745269298553
        vf_loss: 24.205196380615234
      agent-1:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1652791500091553
        entropy_coeff: 0.0017600000137463212
        kl: 0.001358051667921245
        model: {}
        policy_loss: -0.0036659499164670706
        total_loss: -0.0029486881103366613
        vf_explained_var: -0.04661479592323303
        vf_loss: 27.63909339904785
      agent-2:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 1.2000000424450263e-05
        entropy: 1.094428539276123
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016061927890405059
        model: {}
        policy_loss: -0.0037566483952105045
        total_loss: -0.0030232269782572985
        vf_explained_var: -0.004296436905860901
        vf_loss: 26.54597282409668
      agent-3:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3563653230667114
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009035156690515578
        model: {}
        policy_loss: -0.0021041538566350937
        total_loss: -0.00040178955532610416
        vf_explained_var: 0.11996220052242279
        vf_loss: 23.267471313476562
      agent-4:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9422643184661865
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020735205616801977
        model: {}
        policy_loss: -0.003953021019697189
        total_loss: -0.003160521388053894
        vf_explained_var: 0.07504074275493622
        vf_loss: 24.44408416748047
      agent-5:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5191183686256409
        entropy_coeff: 0.0017600000137463212
        kl: 0.001979225780814886
        model: {}
        policy_loss: -0.002837980631738901
        total_loss: -0.0015264359535649419
        vf_explained_var: 0.1607726663351059
        vf_loss: 22.190093994140625
    load_time_ms: 61523.456
    num_steps_sampled: 42912000
    num_steps_trained: 42912000
    sample_time_ms: 145052.834
    update_time_ms: 516.919
  iterations_since_restore: 7
  node_ip: 172.17.8.131
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.258688524590166
    ram_util_percent: 18.495409836065576
  pid: 38661
  policy_reward_max:
    agent-0: 212.33333333333323
    agent-1: 212.33333333333323
    agent-2: 212.33333333333323
    agent-3: 212.33333333333323
    agent-4: 212.33333333333323
    agent-5: 212.33333333333323
  policy_reward_mean:
    agent-0: 176.11999999999986
    agent-1: 176.11999999999986
    agent-2: 176.11999999999986
    agent-3: 176.11999999999986
    agent-4: 176.11999999999986
    agent-5: 176.11999999999986
  policy_reward_min:
    agent-0: 63.66666666666648
    agent-1: 63.66666666666648
    agent-2: 63.66666666666648
    agent-3: 63.66666666666648
    agent-4: 63.66666666666648
    agent-5: 63.66666666666648
  sampler_perf:
    mean_env_wait_ms: 26.56782646209128
    mean_inference_ms: 12.60382984306419
    mean_processing_ms: 53.86185337091991
  time_since_restore: 1604.8778400421143
  time_this_iter_s: 214.75989174842834
  time_total_s: 70798.35965251923
  timestamp: 1637100392
  timesteps_since_restore: 672000
  timesteps_this_iter: 96000
  timesteps_total: 42912000
  training_iteration: 447
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.131:38661 |    447 |          70798.4 | 42912000 |  1056.72 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[33m(pid=raylet)[0m F1116 17:09:42.071332 38568 node_manager.cc:559]  Check failed: node_id != self_node_id_ Exiting because this node manager has mistakenly been marked dead by the monitor.
[2m[33m(pid=raylet)[0m *** Check failure stack trace: ***
[2m[33m(pid=raylet)[0m     @     0x55bf4326376d  google::LogMessage::Fail()
[2m[33m(pid=raylet)[0m     @     0x55bf43264bdc  google::LogMessage::SendToLog()
[2m[33m(pid=raylet)[0m     @     0x55bf43263449  google::LogMessage::Flush()
[2m[33m(pid=raylet)[0m     @     0x55bf43263661  google::LogMessage::~LogMessage()
[2m[33m(pid=raylet)[0m     @     0x55bf42f24029  ray::RayLog::~RayLog()
[2m[33m(pid=raylet)[0m     @     0x55bf42d03810  ray::raylet::NodeManager::NodeRemoved()
[2m[33m(pid=raylet)[0m     @     0x55bf42d039ec  _ZNSt17_Function_handlerIFvRKN3ray8ClientIDERKNS0_3rpc11GcsNodeInfoEEZNS0_6raylet11NodeManager11RegisterGcsEvEUlS3_S7_E0_E9_M_invokeERKSt9_Any_dataS3_S7_
[2m[33m(pid=raylet)[0m     @     0x55bf42dbd682  ray::gcs::ClientTable::HandleNotification()
[2m[33m(pid=raylet)[0m     @     0x55bf42dbdb6b  _ZNSt17_Function_handlerIFvPN3ray3gcs14RedisGcsClientERKNS0_8ClientIDERKSt6vectorINS0_3rpc11GcsNodeInfoESaIS9_EEEZNS1_11ClientTable21SubscribeToNodeChangeERKSt8functionIFvS6_RKS9_EERKSG_IFvNS0_6StatusEEEEUlS3_RKNS0_8UniqueIDESD_E_E9_M_invokeERKSt9_Any_dataS3_S6_SD_
[2m[33m(pid=raylet)[0m     @     0x55bf42dc0908  _ZNSt17_Function_handlerIFvPN3ray3gcs14RedisGcsClientERKNS0_8ClientIDENS0_3rpc13GcsChangeModeERKSt6vectorINS7_11GcsNodeInfoESaISA_EEEZNS1_3LogIS4_SA_E9SubscribeERKNS0_5JobIDES6_RKSt8functionIFvS3_S6_SE_EERKSL_IFvS3_EEEUlS3_S6_S8_SE_E_E9_M_invokeERKSt9_Any_dataS3_S6_S8_SE_
[2m[33m(pid=raylet)[0m     @     0x55bf42dbe18e  _ZNSt17_Function_handlerIFvSt10shared_ptrIN3ray3gcs13CallbackReplyEEEZNS2_3LogINS1_8ClientIDENS1_3rpc11GcsNodeInfoEE9SubscribeERKNS1_5JobIDERKS7_RKSt8functionIFvPNS2_14RedisGcsClientESF_NS8_13GcsChangeModeERKSt6vectorIS9_SaIS9_EEEERKSG_IFvSI_EEEUlS4_E_E9_M_invokeERKSt9_Any_dataS4_
[2m[33m(pid=raylet)[0m     @     0x55bf42d9ad0b  _ZN5boost4asio6detail18completion_handlerIZN3ray3gcs20RedisCallbackManager12CallbackItem8DispatchERSt10shared_ptrINS4_13CallbackReplyEEEUlvE_E11do_completeEPvPNS1_19scheduler_operationERKNS_6system10error_codeEm
[2m[33m(pid=raylet)[0m     @     0x55bf431f541f  boost::asio::detail::scheduler::do_run_one()
[2m[33m(pid=raylet)[0m     @     0x55bf431f6921  boost::asio::detail::scheduler::run()
[2m[33m(pid=raylet)[0m     @     0x55bf431f77c2  boost::asio::io_context::run()
[2m[33m(pid=raylet)[0m     @     0x55bf42c73669  main
[2m[33m(pid=raylet)[0m     @     0x7f9b1bc9dbf7  __libc_start_main
[2m[33m(pid=raylet)[0m     @     0x55bf42c84331  (unknown)
[2m[36m(pid=38685)[0m E1116 17:09:50.046243 39421 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38719)[0m E1116 17:09:50.047011 39424 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38725)[0m E1116 17:09:50.045924 39420 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38728)[0m E1116 17:09:50.097761 39499 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38651)[0m E1116 17:09:50.007495 39230 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38657)[0m E1116 17:09:50.101214 39503 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38727)[0m E1116 17:09:50.097160 39496 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38695)[0m E1116 17:09:50.097960 39500 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38705)[0m E1116 17:09:50.098366 39501 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38708)[0m E1116 17:09:50.094899 39488 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38703)[0m E1116 17:09:50.096556 39494 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38687)[0m E1116 17:09:50.094635 39487 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38709)[0m E1116 17:09:50.095124 39489 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38697)[0m E1116 17:09:50.093338 39484 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38676)[0m E1116 17:09:50.096220 39493 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38701)[0m E1116 17:09:50.093744 39485 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38716)[0m E1116 17:09:50.096843 39495 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38723)[0m E1116 17:09:50.044857 39415 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38680)[0m E1116 17:09:50.097647 39498 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38683)[0m E1116 17:09:50.045192 39416 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38674)[0m E1116 17:09:50.095764 39492 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38689)[0m E1116 17:09:49.804538 38858 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38649)[0m E1116 17:09:49.870338 38926 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38681)[0m E1116 17:09:49.947759 39070 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38655)[0m E1116 17:09:49.913230 38909 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38693)[0m E1116 17:09:49.890708 38942 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38668)[0m E1116 17:09:49.870867 38928 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38659)[0m E1116 17:09:49.943075 39053 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38712)[0m E1116 17:09:49.963332 39069 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38672)[0m E1116 17:09:49.938596 39038 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38653)[0m E1116 17:09:49.939038 39040 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38665)[0m E1116 17:09:50.006472 39216 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38721)[0m E1116 17:09:49.954210 39129 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38678)[0m E1116 17:09:50.080376 39215 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38698)[0m E1116 17:09:50.007216 39227 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38691)[0m E1116 17:09:50.006713 39221 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38714)[0m E1116 17:09:50.045006 39414 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38663)[0m E1116 17:09:50.095301 39490 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38711)[0m E1116 17:09:50.095603 39491 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38661)[0m E1116 17:11:37.174477 39497 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38661)[0m 2021-11-16 17:15:07,722	WARNING metrics.py:70 -- WARNING: collected no metrics in 180 seconds
[2m[36m(pid=38661)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff6f2e62518> -> 0 episodes
