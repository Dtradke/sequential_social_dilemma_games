 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-15 12:17:38,699	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 52.85 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-15 12:17:39,012	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 16840179712 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-15 12:17:39,900	ERROR trial_runner.py:349 -- Trial Runner checkpointing failed.
Traceback (most recent call last):
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 347, in step
    self.checkpoint()
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 278, in checkpoint
    os.rename(tmp_file_name, self.checkpoint_file)
FileNotFoundError: [Errno 2] No such file or directory: '/h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics/.tmp_checkpoint' -> '/h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics/experiment_state-2021-11-15_12-17-39.json'
2021-11-15 12:22:51,316	WARNING util.py:137 -- The `fetch_result` operation took 1.3828554153442383 seconds to complete, which may be a performance bottleneck.
2021-11-15 12:22:53,144	WARNING util.py:137 -- The `process_trial` operation took 3.295743703842163 seconds to complete, which may be a performance bottleneck.
2021-11-15 12:22:53,747	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6015923023223877 seconds to complete, which may be a performance bottleneck.
2021-11-15 13:09:48,158	WARNING util.py:137 -- The `process_trial` operation took 1.3237783908843994 seconds to complete, which may be a performance bottleneck.
2021-11-15 13:11:50,471	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.933471918106079 seconds to complete, which may be a performance bottleneck.
slurmstepd: error: *** JOB 4981787 ON gpu125 CANCELLED AT 2021-11-15T14:46:27 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-15 14:48:38,801	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 53.82 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-15 14:48:39,106	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 10563571712 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-15 14:54:31,968	WARNING util.py:137 -- The `fetch_result` operation took 0.9057717323303223 seconds to complete, which may be a performance bottleneck.
2021-11-15 14:54:33,190	WARNING util.py:137 -- The `process_trial` operation took 2.238588809967041 seconds to complete, which may be a performance bottleneck.
2021-11-15 15:58:32,232	WARNING util.py:137 -- The `experiment_checkpoint` operation took 2.5486361980438232 seconds to complete, which may be a performance bottleneck.
slurmstepd: error: *** JOB 4981787 ON gpu057 CANCELLED AT 2021-11-15T16:56:18 ***
