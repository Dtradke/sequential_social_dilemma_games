>>> RESTORING FROM:  /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics/BaselinePPOTrainer_cleanup_env_0_2021-11-15_17-00-45_bf59qjr/checkpoint_60
== Status ==
Memory usage on this node: 26.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------+--------+------------------+---------+----------+
| Trial name                           | status   | loc   |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  |       |     60 |           9077.4 | 5760000 |   408.57 |
+--------------------------------------+----------+-------+--------+------------------+---------+----------+


[2m[36m(pid=24061)[0m 2021-11-15 19:51:08,157	INFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution
[2m[36m(pid=24061)[0m 2021-11-15 19:51:08,172	INFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=24061)[0m 2021-11-15 19:52:49,079	INFO trainable.py:180 -- _setup took 100.921 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(pid=24061)[0m 2021-11-15 19:52:49,079	INFO trainable.py:217 -- Getting current IP.
[2m[36m(pid=24061)[0m 2021-11-15 19:52:49,079	WARNING util.py:37 -- Install gputil for GPU system monitoring.
[2m[36m(pid=24061)[0m 2021-11-15 19:52:52,021	INFO trainable.py:217 -- Getting current IP.
[2m[36m(pid=24061)[0m 2021-11-15 19:52:52,021	INFO trainable.py:423 -- Restored on 172.17.8.138 from checkpoint: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics/BaselinePPOTrainer_cleanup_env_0_2021-11-15_17-00-45_bf59qjr/tmpkhthuqhmrestore_from_object/checkpoint-60
[2m[36m(pid=24061)[0m 2021-11-15 19:52:52,021	INFO trainable.py:430 -- Current state after restoring: {'_iteration': 60, '_timesteps_total': 5760000, '_time_total': 9126.011813879013, '_episodes_total': 5760}
== Status ==
Memory usage on this node: 32.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------+--------+------------------+---------+----------+
| Trial name                           | status   | loc   |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  |       |     60 |           9077.4 | 5760000 |   408.57 |
+--------------------------------------+----------+-------+--------+------------------+---------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 69
    apples_agent-0_mean: 6.770833333333333
    apples_agent-0_min: 0
    apples_agent-1_max: 145
    apples_agent-1_mean: 17.1875
    apples_agent-1_min: 0
    apples_agent-2_max: 161
    apples_agent-2_mean: 18.21875
    apples_agent-2_min: 0
    apples_agent-3_max: 159
    apples_agent-3_mean: 90.07291666666667
    apples_agent-3_min: 17
    apples_agent-4_max: 123
    apples_agent-4_mean: 7.489583333333333
    apples_agent-4_min: 0
    apples_agent-5_max: 146
    apples_agent-5_mean: 60.65625
    apples_agent-5_min: 2
    cleaning_beam_agent-0_max: 388
    cleaning_beam_agent-0_mean: 269.9791666666667
    cleaning_beam_agent-0_min: 150
    cleaning_beam_agent-1_max: 604
    cleaning_beam_agent-1_mean: 333.53125
    cleaning_beam_agent-1_min: 127
    cleaning_beam_agent-2_max: 515
    cleaning_beam_agent-2_mean: 307.59375
    cleaning_beam_agent-2_min: 129
    cleaning_beam_agent-3_max: 134
    cleaning_beam_agent-3_mean: 70.79166666666667
    cleaning_beam_agent-3_min: 38
    cleaning_beam_agent-4_max: 472
    cleaning_beam_agent-4_mean: 326.5104166666667
    cleaning_beam_agent-4_min: 130
    cleaning_beam_agent-5_max: 180
    cleaning_beam_agent-5_mean: 61.135416666666664
    cleaning_beam_agent-5_min: 18
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03125
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.0625
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.020833333333333332
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.010416666666666666
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.052083333333333336
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.08333333333333333
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_19-56-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 655.9999999999885
  episode_reward_mean: 461.57291666667305
  episode_reward_min: 171.99999999999923
  episodes_this_iter: 96
  episodes_total: 5856
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 32159.025
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0009005760075524449
        entropy: 1.3863615989685059
        entropy_coeff: 0.0017600000137463212
        kl: 0.007342920638620853
        model: {}
        policy_loss: -0.013872413896024227
        total_loss: -0.013833051547408104
        vf_explained_var: 0.058433711528778076
        vf_loss: 10.107736587524414
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0009005760075524449
        entropy: 1.284472942352295
        entropy_coeff: 0.0017600000137463212
        kl: 0.011414089240133762
        model: {}
        policy_loss: -0.027092939242720604
        total_loss: -0.02601570636034012
        vf_explained_var: 0.017155751585960388
        vf_loss: 10.550883293151855
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0009005760075524449
        entropy: 1.3263604640960693
        entropy_coeff: 0.0017600000137463212
        kl: 0.010290024802088737
        model: {}
        policy_loss: -0.023455506190657616
        total_loss: -0.022731252014636993
        vf_explained_var: 0.06763690710067749
        vf_loss: 10.006409645080566
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0009005760075524449
        entropy: 1.1558345556259155
        entropy_coeff: 0.0017600000137463212
        kl: 0.007929311133921146
        model: {}
        policy_loss: -0.01626187562942505
        total_loss: -0.015790479257702827
        vf_explained_var: 0.1427164375782013
        vf_loss: 9.198016166687012
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0009005760075524449
        entropy: 1.2311152219772339
        entropy_coeff: 0.0017600000137463212
        kl: 0.010216733440756798
        model: {}
        policy_loss: -0.026115596294403076
        total_loss: -0.02527124620974064
        vf_explained_var: 0.09803816676139832
        vf_loss: 9.677711486816406
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0009005760075524449
        entropy: 1.280975580215454
        entropy_coeff: 0.0017600000137463212
        kl: 0.010599158704280853
        model: {}
        policy_loss: -0.02478613145649433
        total_loss: -0.02403879165649414
        vf_explained_var: 0.1787981241941452
        vf_loss: 8.820221900939941
    load_time_ms: 73994.815
    num_steps_sampled: 5856000
    num_steps_trained: 5856000
    sample_time_ms: 103663.842
    update_time_ms: 3117.323
  iterations_since_restore: 1
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.11635802469136
    ram_util_percent: 19.501543209876544
  pid: 24061
  policy_reward_max:
    agent-0: 109.33333333333375
    agent-1: 109.33333333333375
    agent-2: 109.33333333333375
    agent-3: 109.33333333333375
    agent-4: 109.33333333333375
    agent-5: 109.33333333333375
  policy_reward_mean:
    agent-0: 76.92881944444451
    agent-1: 76.92881944444451
    agent-2: 76.92881944444451
    agent-3: 76.92881944444451
    agent-4: 76.92881944444451
    agent-5: 76.92881944444451
  policy_reward_min:
    agent-0: 28.66666666666673
    agent-1: 28.66666666666673
    agent-2: 28.66666666666673
    agent-3: 28.66666666666673
    agent-4: 28.66666666666673
    agent-5: 28.66666666666673
  sampler_perf:
    mean_env_wait_ms: 24.69171332074451
    mean_inference_ms: 13.908326605975608
    mean_processing_ms: 52.37446337829143
  time_since_restore: 218.42494702339172
  time_this_iter_s: 218.42494702339172
  time_total_s: 9344.436760902405
  timestamp: 1637024196
  timesteps_since_restore: 96000
  timesteps_this_iter: 96000
  timesteps_total: 5856000
  training_iteration: 61
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 36.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |     61 |          9344.44 | 5856000 |  461.573 |
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 40
    apples_agent-0_mean: 6.57
    apples_agent-0_min: 0
    apples_agent-1_max: 72
    apples_agent-1_mean: 19.97
    apples_agent-1_min: 0
    apples_agent-2_max: 201
    apples_agent-2_mean: 14.06
    apples_agent-2_min: 0
    apples_agent-3_max: 175
    apples_agent-3_mean: 87.24
    apples_agent-3_min: 23
    apples_agent-4_max: 67
    apples_agent-4_mean: 3.13
    apples_agent-4_min: 0
    apples_agent-5_max: 118
    apples_agent-5_mean: 62.95
    apples_agent-5_min: 10
    cleaning_beam_agent-0_max: 404
    cleaning_beam_agent-0_mean: 258.92
    cleaning_beam_agent-0_min: 152
    cleaning_beam_agent-1_max: 591
    cleaning_beam_agent-1_mean: 315.76
    cleaning_beam_agent-1_min: 133
    cleaning_beam_agent-2_max: 580
    cleaning_beam_agent-2_mean: 315.31
    cleaning_beam_agent-2_min: 148
    cleaning_beam_agent-3_max: 146
    cleaning_beam_agent-3_mean: 81.44
    cleaning_beam_agent-3_min: 30
    cleaning_beam_agent-4_max: 509
    cleaning_beam_agent-4_mean: 330.8
    cleaning_beam_agent-4_min: 128
    cleaning_beam_agent-5_max: 138
    cleaning_beam_agent-5_mean: 57.03
    cleaning_beam_agent-5_min: 17
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.05
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.04
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_19-59-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 676.9999999999942
  episode_reward_mean: 468.66000000000605
  episode_reward_min: 213.99999999999727
  episodes_this_iter: 96
  episodes_total: 5952
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 26222.08
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008945856243371964
        entropy: 1.3635210990905762
        entropy_coeff: 0.0017600000137463212
        kl: 0.008374778553843498
        model: {}
        policy_loss: -0.016237031668424606
        total_loss: -0.01593128778040409
        vf_explained_var: 0.09666739404201508
        vf_loss: 10.305865287780762
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008945856243371964
        entropy: 1.3160663843154907
        entropy_coeff: 0.0017600000137463212
        kl: 0.011197278276085854
        model: {}
        policy_loss: -0.02767017111182213
        total_loss: -0.026649564504623413
        vf_explained_var: 0.03748016059398651
        vf_loss: 10.974312782287598
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008945856243371964
        entropy: 1.3370332717895508
        entropy_coeff: 0.0017600000137463212
        kl: 0.010820334777235985
        model: {}
        policy_loss: -0.023644033819437027
        total_loss: -0.02278073877096176
        vf_explained_var: 0.07757796347141266
        vf_loss: 10.524067878723145
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008945856243371964
        entropy: 1.1498323678970337
        entropy_coeff: 0.0017600000137463212
        kl: 0.008773179724812508
        model: {}
        policy_loss: -0.019471053034067154
        total_loss: -0.018796298652887344
        vf_explained_var: 0.17275077104568481
        vf_loss: 9.43828010559082
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008945856243371964
        entropy: 1.23263680934906
        entropy_coeff: 0.0017600000137463212
        kl: 0.01114422082901001
        model: {}
        policy_loss: -0.02762303687632084
        total_loss: -0.02652471326291561
        vf_explained_var: 0.08925282955169678
        vf_loss: 10.38922119140625
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008945856243371964
        entropy: 1.256330132484436
        entropy_coeff: 0.0017600000137463212
        kl: 0.009961409494280815
        model: {}
        policy_loss: -0.025722481310367584
        total_loss: -0.025021275505423546
        vf_explained_var: 0.19323189556598663
        vf_loss: 9.20065689086914
    load_time_ms: 65075.865
    num_steps_sampled: 5952000
    num_steps_trained: 5952000
    sample_time_ms: 108925.082
    update_time_ms: 1680.999
  iterations_since_restore: 2
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.408303249097473
    ram_util_percent: 19.992779783393498
  pid: 24061
  policy_reward_max:
    agent-0: 112.83333333333397
    agent-1: 112.83333333333397
    agent-2: 112.83333333333397
    agent-3: 112.83333333333397
    agent-4: 112.83333333333397
    agent-5: 112.83333333333397
  policy_reward_mean:
    agent-0: 78.11000000000006
    agent-1: 78.11000000000006
    agent-2: 78.11000000000006
    agent-3: 78.11000000000006
    agent-4: 78.11000000000006
    agent-5: 78.11000000000006
  policy_reward_min:
    agent-0: 35.6666666666667
    agent-1: 35.6666666666667
    agent-2: 35.6666666666667
    agent-3: 35.6666666666667
    agent-4: 35.6666666666667
    agent-5: 35.6666666666667
  sampler_perf:
    mean_env_wait_ms: 24.788686359455223
    mean_inference_ms: 13.334896556535961
    mean_processing_ms: 52.13275433006848
  time_since_restore: 409.5409541130066
  time_this_iter_s: 191.11600708961487
  time_total_s: 9535.55276799202
  timestamp: 1637024393
  timesteps_since_restore: 192000
  timesteps_this_iter: 96000
  timesteps_total: 5952000
  training_iteration: 62
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |     62 |          9535.55 | 5952000 |   468.66 |
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 48
    apples_agent-0_mean: 8.03
    apples_agent-0_min: 0
    apples_agent-1_max: 94
    apples_agent-1_mean: 18.3
    apples_agent-1_min: 0
    apples_agent-2_max: 95
    apples_agent-2_mean: 11.5
    apples_agent-2_min: 0
    apples_agent-3_max: 129
    apples_agent-3_mean: 82.18
    apples_agent-3_min: 23
    apples_agent-4_max: 46
    apples_agent-4_mean: 2.12
    apples_agent-4_min: 0
    apples_agent-5_max: 101
    apples_agent-5_mean: 63.55
    apples_agent-5_min: 26
    cleaning_beam_agent-0_max: 394
    cleaning_beam_agent-0_mean: 260.2
    cleaning_beam_agent-0_min: 142
    cleaning_beam_agent-1_max: 531
    cleaning_beam_agent-1_mean: 274.46
    cleaning_beam_agent-1_min: 88
    cleaning_beam_agent-2_max: 623
    cleaning_beam_agent-2_mean: 348.32
    cleaning_beam_agent-2_min: 87
    cleaning_beam_agent-3_max: 140
    cleaning_beam_agent-3_mean: 77.48
    cleaning_beam_agent-3_min: 36
    cleaning_beam_agent-4_max: 481
    cleaning_beam_agent-4_mean: 325.68
    cleaning_beam_agent-4_min: 108
    cleaning_beam_agent-5_max: 118
    cleaning_beam_agent-5_mean: 62.32
    cleaning_beam_agent-5_min: 24
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.05
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.05
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.04
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.08
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-02-54
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 713.9999999999989
  episode_reward_mean: 461.1000000000068
  episode_reward_min: 158.00000000000088
  episodes_this_iter: 96
  episodes_total: 6048
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 24266.469
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008885951829142869
        entropy: 1.3299320936203003
        entropy_coeff: 0.0017600000137463212
        kl: 0.008440051227807999
        model: {}
        policy_loss: -0.01788449101150036
        total_loss: -0.017555490136146545
        vf_explained_var: 0.08189618587493896
        vf_loss: 9.816713333129883
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008885951829142869
        entropy: 1.2704501152038574
        entropy_coeff: 0.0017600000137463212
        kl: 0.012794731184840202
        model: {}
        policy_loss: -0.030706292018294334
        total_loss: -0.02936929278075695
        vf_explained_var: 0.05176198482513428
        vf_loss: 10.14048957824707
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008885951829142869
        entropy: 1.308664083480835
        entropy_coeff: 0.0017600000137463212
        kl: 0.01158118899911642
        model: {}
        policy_loss: -0.024242818355560303
        total_loss: -0.02324696257710457
        vf_explained_var: 0.08069804310798645
        vf_loss: 9.828678131103516
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008885951829142869
        entropy: 1.0922753810882568
        entropy_coeff: 0.0017600000137463212
        kl: 0.009123658761382103
        model: {}
        policy_loss: -0.018836891278624535
        total_loss: -0.01803051121532917
        vf_explained_var: 0.15477684140205383
        vf_loss: 9.040474891662598
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008885951829142869
        entropy: 1.237480640411377
        entropy_coeff: 0.0017600000137463212
        kl: 0.011359057389199734
        model: {}
        policy_loss: -0.028987016528844833
        total_loss: -0.027926864102482796
        vf_explained_var: 0.09670382738113403
        vf_loss: 9.663082122802734
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008885951829142869
        entropy: 1.2483447790145874
        entropy_coeff: 0.0017600000137463212
        kl: 0.011159057728946209
        model: {}
        policy_loss: -0.02697492204606533
        total_loss: -0.02606302499771118
        vf_explained_var: 0.18002575635910034
        vf_loss: 8.771724700927734
    load_time_ms: 58837.081
    num_steps_sampled: 6048000
    num_steps_trained: 6048000
    sample_time_ms: 110745.718
    update_time_ms: 1142.416
  iterations_since_restore: 3
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 16.310465116279072
    ram_util_percent: 19.799612403100774
  pid: 24061
  policy_reward_max:
    agent-0: 119.00000000000043
    agent-1: 119.00000000000043
    agent-2: 119.00000000000043
    agent-3: 119.00000000000043
    agent-4: 119.00000000000043
    agent-5: 119.00000000000043
  policy_reward_mean:
    agent-0: 76.85000000000004
    agent-1: 76.85000000000004
    agent-2: 76.85000000000004
    agent-3: 76.85000000000004
    agent-4: 76.85000000000004
    agent-5: 76.85000000000004
  policy_reward_min:
    agent-0: 26.333333333333346
    agent-1: 26.333333333333346
    agent-2: 26.333333333333346
    agent-3: 26.333333333333346
    agent-4: 26.333333333333346
    agent-5: 26.333333333333346
  sampler_perf:
    mean_env_wait_ms: 24.676786250123833
    mean_inference_ms: 13.098236840281174
    mean_processing_ms: 52.091174091535265
  time_since_restore: 590.8604056835175
  time_this_iter_s: 181.31945157051086
  time_total_s: 9716.87221956253
  timestamp: 1637024574
  timesteps_since_restore: 288000
  timesteps_this_iter: 96000
  timesteps_total: 6048000
  training_iteration: 63
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |     63 |          9716.87 | 6048000 |    461.1 |
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 35
    apples_agent-0_mean: 6.93
    apples_agent-0_min: 0
    apples_agent-1_max: 144
    apples_agent-1_mean: 23.61
    apples_agent-1_min: 0
    apples_agent-2_max: 73
    apples_agent-2_mean: 8.0
    apples_agent-2_min: 0
    apples_agent-3_max: 151
    apples_agent-3_mean: 84.65
    apples_agent-3_min: 18
    apples_agent-4_max: 64
    apples_agent-4_mean: 2.84
    apples_agent-4_min: 0
    apples_agent-5_max: 105
    apples_agent-5_mean: 61.35
    apples_agent-5_min: 10
    cleaning_beam_agent-0_max: 477
    cleaning_beam_agent-0_mean: 277.28
    cleaning_beam_agent-0_min: 152
    cleaning_beam_agent-1_max: 496
    cleaning_beam_agent-1_mean: 252.89
    cleaning_beam_agent-1_min: 86
    cleaning_beam_agent-2_max: 550
    cleaning_beam_agent-2_mean: 349.08
    cleaning_beam_agent-2_min: 174
    cleaning_beam_agent-3_max: 161
    cleaning_beam_agent-3_mean: 64.69
    cleaning_beam_agent-3_min: 23
    cleaning_beam_agent-4_max: 608
    cleaning_beam_agent-4_mean: 338.57
    cleaning_beam_agent-4_min: 134
    cleaning_beam_agent-5_max: 170
    cleaning_beam_agent-5_mean: 64.25
    cleaning_beam_agent-5_min: 23
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.05
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-05-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 695.9999999999928
  episode_reward_mean: 469.78000000000435
  episode_reward_min: 160.99999999999963
  episodes_this_iter: 96
  episodes_total: 6144
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 23307.313
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008826047996990383
        entropy: 1.3204721212387085
        entropy_coeff: 0.0017600000137463212
        kl: 0.008105676621198654
        model: {}
        policy_loss: -0.018263787031173706
        total_loss: -0.017895478755235672
        vf_explained_var: 0.07613243162631989
        vf_loss: 10.71202278137207
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008826047996990383
        entropy: 1.2873084545135498
        entropy_coeff: 0.0017600000137463212
        kl: 0.01372985728085041
        model: {}
        policy_loss: -0.028231525793671608
        total_loss: -0.026628928259015083
        vf_explained_var: 0.03347611427307129
        vf_loss: 11.222902297973633
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008826047996990383
        entropy: 1.3232784271240234
        entropy_coeff: 0.0017600000137463212
        kl: 0.010786189697682858
        model: {}
        policy_loss: -0.024504657834768295
        total_loss: -0.02361840009689331
        vf_explained_var: 0.08795475959777832
        vf_loss: 10.57990837097168
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008826047996990383
        entropy: 1.0817168951034546
        entropy_coeff: 0.0017600000137463212
        kl: 0.008818238973617554
        model: {}
        policy_loss: -0.019582320004701614
        total_loss: -0.018750544637441635
        vf_explained_var: 0.16210637986660004
        vf_loss: 9.71947193145752
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008826047996990383
        entropy: 1.2207595109939575
        entropy_coeff: 0.0017600000137463212
        kl: 0.012235458008944988
        model: {}
        policy_loss: -0.0300741009414196
        total_loss: -0.028717361390590668
        vf_explained_var: 0.08784578740596771
        vf_loss: 10.581840515136719
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008826047996990383
        entropy: 1.2600128650665283
        entropy_coeff: 0.0017600000137463212
        kl: 0.011282149702310562
        model: {}
        policy_loss: -0.029168883338570595
        total_loss: -0.02817034348845482
        vf_explained_var: 0.17213240265846252
        vf_loss: 9.597345352172852
    load_time_ms: 55954.981
    num_steps_sampled: 6144000
    num_steps_trained: 6144000
    sample_time_ms: 110703.038
    update_time_ms: 877.888
  iterations_since_restore: 4
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 16.070588235294117
    ram_util_percent: 19.72235294117647
  pid: 24061
  policy_reward_max:
    agent-0: 116.00000000000041
    agent-1: 116.00000000000041
    agent-2: 116.00000000000041
    agent-3: 116.00000000000041
    agent-4: 116.00000000000041
    agent-5: 116.00000000000041
  policy_reward_mean:
    agent-0: 78.29666666666674
    agent-1: 78.29666666666674
    agent-2: 78.29666666666674
    agent-3: 78.29666666666674
    agent-4: 78.29666666666674
    agent-5: 78.29666666666674
  policy_reward_min:
    agent-0: 26.83333333333337
    agent-1: 26.83333333333337
    agent-2: 26.83333333333337
    agent-3: 26.83333333333337
    agent-4: 26.83333333333337
    agent-5: 26.83333333333337
  sampler_perf:
    mean_env_wait_ms: 24.593807399046653
    mean_inference_ms: 12.93989566768078
    mean_processing_ms: 51.98287099728324
  time_since_restore: 769.4461636543274
  time_this_iter_s: 178.58575797080994
  time_total_s: 9895.45797753334
  timestamp: 1637024753
  timesteps_since_restore: 384000
  timesteps_this_iter: 96000
  timesteps_total: 6144000
  training_iteration: 64
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |     64 |          9895.46 | 6144000 |   469.78 |
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 62
    apples_agent-0_mean: 7.25
    apples_agent-0_min: 0
    apples_agent-1_max: 105
    apples_agent-1_mean: 19.0
    apples_agent-1_min: 0
    apples_agent-2_max: 109
    apples_agent-2_mean: 14.32
    apples_agent-2_min: 0
    apples_agent-3_max: 154
    apples_agent-3_mean: 86.05
    apples_agent-3_min: 22
    apples_agent-4_max: 80
    apples_agent-4_mean: 4.29
    apples_agent-4_min: 0
    apples_agent-5_max: 104
    apples_agent-5_mean: 64.37
    apples_agent-5_min: 4
    cleaning_beam_agent-0_max: 441
    cleaning_beam_agent-0_mean: 281.19
    cleaning_beam_agent-0_min: 135
    cleaning_beam_agent-1_max: 518
    cleaning_beam_agent-1_mean: 281.17
    cleaning_beam_agent-1_min: 79
    cleaning_beam_agent-2_max: 607
    cleaning_beam_agent-2_mean: 349.29
    cleaning_beam_agent-2_min: 145
    cleaning_beam_agent-3_max: 102
    cleaning_beam_agent-3_mean: 58.63
    cleaning_beam_agent-3_min: 23
    cleaning_beam_agent-4_max: 646
    cleaning_beam_agent-4_mean: 344.87
    cleaning_beam_agent-4_min: 162
    cleaning_beam_agent-5_max: 197
    cleaning_beam_agent-5_mean: 60.55
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.06
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-08-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 706.9999999999889
  episode_reward_mean: 486.78000000000515
  episode_reward_min: 155.9999999999995
  episodes_this_iter: 96
  episodes_total: 6240
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 22684.075
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008766144164837897
        entropy: 1.2986910343170166
        entropy_coeff: 0.0017600000137463212
        kl: 0.008343948051333427
        model: {}
        policy_loss: -0.018703438341617584
        total_loss: -0.018228722736239433
        vf_explained_var: 0.07715047895908356
        vf_loss: 10.91627311706543
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008766144164837897
        entropy: 1.2573771476745605
        entropy_coeff: 0.0017600000137463212
        kl: 0.012185374274849892
        model: {}
        policy_loss: -0.0285081434994936
        total_loss: -0.027152279391884804
        vf_explained_var: 0.04302401840686798
        vf_loss: 11.317752838134766
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008766144164837897
        entropy: 1.284094214439392
        entropy_coeff: 0.0017600000137463212
        kl: 0.012469673529267311
        model: {}
        policy_loss: -0.02361486852169037
        total_loss: -0.022290989756584167
        vf_explained_var: 0.07908068597316742
        vf_loss: 10.899492263793945
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008766144164837897
        entropy: 1.0713454484939575
        entropy_coeff: 0.0017600000137463212
        kl: 0.008873027749359608
        model: {}
        policy_loss: -0.02012944221496582
        total_loss: -0.01924559287726879
        vf_explained_var: 0.15925998985767365
        vf_loss: 9.948135375976562
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008766144164837897
        entropy: 1.198578119277954
        entropy_coeff: 0.0017600000137463212
        kl: 0.012284776195883751
        model: {}
        policy_loss: -0.030110640451312065
        total_loss: -0.028730865567922592
        vf_explained_var: 0.1283215433359146
        vf_loss: 10.32319450378418
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008766144164837897
        entropy: 1.250463843345642
        entropy_coeff: 0.0017600000137463212
        kl: 0.012076936662197113
        model: {}
        policy_loss: -0.0305346567183733
        total_loss: -0.02935101091861725
        vf_explained_var: 0.1813853681087494
        vf_loss: 9.69080924987793
    load_time_ms: 51728.273
    num_steps_sampled: 6240000
    num_steps_trained: 6240000
    sample_time_ms: 111122.538
    update_time_ms: 715.549
  iterations_since_restore: 5
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 16.64979079497908
    ram_util_percent: 19.643096234309628
  pid: 24061
  policy_reward_max:
    agent-0: 117.83333333333387
    agent-1: 117.83333333333387
    agent-2: 117.83333333333387
    agent-3: 117.83333333333387
    agent-4: 117.83333333333387
    agent-5: 117.83333333333387
  policy_reward_mean:
    agent-0: 81.13000000000008
    agent-1: 81.13000000000008
    agent-2: 81.13000000000008
    agent-3: 81.13000000000008
    agent-4: 81.13000000000008
    agent-5: 81.13000000000008
  policy_reward_min:
    agent-0: 26.00000000000006
    agent-1: 26.00000000000006
    agent-2: 26.00000000000006
    agent-3: 26.00000000000006
    agent-4: 26.00000000000006
    agent-5: 26.00000000000006
  sampler_perf:
    mean_env_wait_ms: 24.61043943582998
    mean_inference_ms: 12.872254832031139
    mean_processing_ms: 51.93175315166259
  time_since_restore: 937.443781375885
  time_this_iter_s: 167.99761772155762
  time_total_s: 10063.455595254898
  timestamp: 1637024922
  timesteps_since_restore: 480000
  timesteps_this_iter: 96000
  timesteps_total: 6240000
  training_iteration: 65
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |     65 |          10063.5 | 6240000 |   486.78 |
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 49
    apples_agent-0_mean: 7.91
    apples_agent-0_min: 0
    apples_agent-1_max: 102
    apples_agent-1_mean: 24.12
    apples_agent-1_min: 0
    apples_agent-2_max: 100
    apples_agent-2_mean: 8.72
    apples_agent-2_min: 0
    apples_agent-3_max: 171
    apples_agent-3_mean: 91.56
    apples_agent-3_min: 27
    apples_agent-4_max: 52
    apples_agent-4_mean: 3.58
    apples_agent-4_min: 0
    apples_agent-5_max: 110
    apples_agent-5_mean: 63.15
    apples_agent-5_min: 13
    cleaning_beam_agent-0_max: 402
    cleaning_beam_agent-0_mean: 282.13
    cleaning_beam_agent-0_min: 148
    cleaning_beam_agent-1_max: 527
    cleaning_beam_agent-1_mean: 258.99
    cleaning_beam_agent-1_min: 66
    cleaning_beam_agent-2_max: 594
    cleaning_beam_agent-2_mean: 381.34
    cleaning_beam_agent-2_min: 172
    cleaning_beam_agent-3_max: 97
    cleaning_beam_agent-3_mean: 54.39
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 601
    cleaning_beam_agent-4_mean: 362.23
    cleaning_beam_agent-4_min: 122
    cleaning_beam_agent-5_max: 121
    cleaning_beam_agent-5_mean: 56.51
    cleaning_beam_agent-5_min: 17
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.05
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 3
    fire_beam_agent-4_mean: 0.09
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-11-48
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 707.9999999999856
  episode_reward_mean: 490.28000000000446
  episode_reward_min: 212.9999999999971
  episodes_this_iter: 96
  episodes_total: 6336
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 22298.56
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008706239750608802
        entropy: 1.2705659866333008
        entropy_coeff: 0.0017600000137463212
        kl: 0.010364923626184464
        model: {}
        policy_loss: -0.01802057772874832
        total_loss: -0.01697452738881111
        vf_explained_var: 0.07764427363872528
        vf_loss: 12.09261703491211
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008706239750608802
        entropy: 1.2685589790344238
        entropy_coeff: 0.0017600000137463212
        kl: 0.012769547291100025
        model: {}
        policy_loss: -0.030450940132141113
        total_loss: -0.02886936068534851
        vf_explained_var: 0.03912699222564697
        vf_loss: 12.603324890136719
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008706239750608802
        entropy: 1.2805989980697632
        entropy_coeff: 0.0017600000137463212
        kl: 0.01096704788506031
        model: {}
        policy_loss: -0.02564491517841816
        total_loss: -0.02453179843723774
        vf_explained_var: 0.10496295988559723
        vf_loss: 11.735584259033203
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008706239750608802
        entropy: 1.0641155242919922
        entropy_coeff: 0.0017600000137463212
        kl: 0.008805613033473492
        model: {}
        policy_loss: -0.021098772063851357
        total_loss: -0.02014072984457016
        vf_explained_var: 0.18439705669879913
        vf_loss: 10.697625160217285
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008706239750608802
        entropy: 1.1816864013671875
        entropy_coeff: 0.0017600000137463212
        kl: 0.011597994714975357
        model: {}
        policy_loss: -0.031122786924242973
        total_loss: -0.02973948046565056
        vf_explained_var: 0.12808758020401
        vf_loss: 11.434761047363281
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008706239750608802
        entropy: 1.2284575700759888
        entropy_coeff: 0.0017600000137463212
        kl: 0.01152736321091652
        model: {}
        policy_loss: -0.028535008430480957
        total_loss: -0.027360081672668457
        vf_explained_var: 0.2135564088821411
        vf_loss: 10.315406799316406
    load_time_ms: 52838.567
    num_steps_sampled: 6336000
    num_steps_trained: 6336000
    sample_time_ms: 110425.958
    update_time_ms: 608.028
  iterations_since_restore: 6
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 17.124150943396224
    ram_util_percent: 21.331320754716977
  pid: 24061
  policy_reward_max:
    agent-0: 118.0000000000007
    agent-1: 118.0000000000007
    agent-2: 118.0000000000007
    agent-3: 118.0000000000007
    agent-4: 118.0000000000007
    agent-5: 118.0000000000007
  policy_reward_mean:
    agent-0: 81.71333333333344
    agent-1: 81.71333333333344
    agent-2: 81.71333333333344
    agent-3: 81.71333333333344
    agent-4: 81.71333333333344
    agent-5: 81.71333333333344
  policy_reward_min:
    agent-0: 35.50000000000005
    agent-1: 35.50000000000005
    agent-2: 35.50000000000005
    agent-3: 35.50000000000005
    agent-4: 35.50000000000005
    agent-5: 35.50000000000005
  sampler_perf:
    mean_env_wait_ms: 24.595086455463292
    mean_inference_ms: 12.81591157703644
    mean_processing_ms: 51.84611534961481
  time_since_restore: 1123.4166886806488
  time_this_iter_s: 185.9729073047638
  time_total_s: 10249.428502559662
  timestamp: 1637025108
  timesteps_since_restore: 576000
  timesteps_this_iter: 96000
  timesteps_total: 6336000
  training_iteration: 66
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 44.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |     66 |          10249.4 | 6336000 |   490.28 |
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 59
    apples_agent-0_mean: 7.99
    apples_agent-0_min: 0
    apples_agent-1_max: 100
    apples_agent-1_mean: 23.48
    apples_agent-1_min: 0
    apples_agent-2_max: 117
    apples_agent-2_mean: 15.46
    apples_agent-2_min: 0
    apples_agent-3_max: 187
    apples_agent-3_mean: 93.05
    apples_agent-3_min: 18
    apples_agent-4_max: 119
    apples_agent-4_mean: 4.92
    apples_agent-4_min: 0
    apples_agent-5_max: 118
    apples_agent-5_mean: 65.01
    apples_agent-5_min: 16
    cleaning_beam_agent-0_max: 427
    cleaning_beam_agent-0_mean: 278.86
    cleaning_beam_agent-0_min: 172
    cleaning_beam_agent-1_max: 528
    cleaning_beam_agent-1_mean: 265.11
    cleaning_beam_agent-1_min: 90
    cleaning_beam_agent-2_max: 533
    cleaning_beam_agent-2_mean: 341.3
    cleaning_beam_agent-2_min: 139
    cleaning_beam_agent-3_max: 102
    cleaning_beam_agent-3_mean: 54.09
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 622
    cleaning_beam_agent-4_mean: 376.95
    cleaning_beam_agent-4_min: 116
    cleaning_beam_agent-5_max: 150
    cleaning_beam_agent-5_mean: 54.46
    cleaning_beam_agent-5_min: 17
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.04
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.05
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 3
    fire_beam_agent-4_mean: 0.1
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-14-35
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 768.9999999999933
  episode_reward_mean: 495.59000000000447
  episode_reward_min: 200.99999999999744
  episodes_this_iter: 96
  episodes_total: 6432
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 22032.365
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008646335918456316
        entropy: 1.2734391689300537
        entropy_coeff: 0.0017600000137463212
        kl: 0.008299869485199451
        model: {}
        policy_loss: -0.019957372918725014
        total_loss: -0.019431591033935547
        vf_explained_var: 0.08739185333251953
        vf_loss: 11.0706205368042
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008646335918456316
        entropy: 1.268470287322998
        entropy_coeff: 0.0017600000137463212
        kl: 0.01343041192740202
        model: {}
        policy_loss: -0.032129526138305664
        total_loss: -0.030521199107170105
        vf_explained_var: 0.04850183427333832
        vf_loss: 11.547536849975586
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008646335918456316
        entropy: 1.2823971509933472
        entropy_coeff: 0.0017600000137463212
        kl: 0.01191750168800354
        model: {}
        policy_loss: -0.026909852400422096
        total_loss: -0.02572665922343731
        vf_explained_var: 0.1287810206413269
        vf_loss: 10.567137718200684
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008646335918456316
        entropy: 1.0890429019927979
        entropy_coeff: 0.0017600000137463212
        kl: 0.009551607072353363
        model: {}
        policy_loss: -0.021262086927890778
        total_loss: -0.02027839794754982
        vf_explained_var: 0.18381771445274353
        vf_loss: 9.90084457397461
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008646335918456316
        entropy: 1.1777056455612183
        entropy_coeff: 0.0017600000137463212
        kl: 0.013082705438137054
        model: {}
        policy_loss: -0.03232800215482712
        total_loss: -0.03069816157221794
        vf_explained_var: 0.10415913164615631
        vf_loss: 10.860613822937012
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008646335918456316
        entropy: 1.215975046157837
        entropy_coeff: 0.0017600000137463212
        kl: 0.012066337279975414
        model: {}
        policy_loss: -0.03041553683578968
        total_loss: -0.02914743684232235
        vf_explained_var: 0.17922081053256989
        vf_loss: 9.949485778808594
    load_time_ms: 50769.513
    num_steps_sampled: 6432000
    num_steps_trained: 6432000
    sample_time_ms: 110030.513
    update_time_ms: 533.242
  iterations_since_restore: 7
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 17.584033613445378
    ram_util_percent: 25.16176470588235
  pid: 24061
  policy_reward_max:
    agent-0: 128.16666666666708
    agent-1: 128.16666666666708
    agent-2: 128.16666666666708
    agent-3: 128.16666666666708
    agent-4: 128.16666666666708
    agent-5: 128.16666666666708
  policy_reward_mean:
    agent-0: 82.59833333333344
    agent-1: 82.59833333333344
    agent-2: 82.59833333333344
    agent-3: 82.59833333333344
    agent-4: 82.59833333333344
    agent-5: 82.59833333333344
  policy_reward_min:
    agent-0: 33.50000000000005
    agent-1: 33.50000000000005
    agent-2: 33.50000000000005
    agent-3: 33.50000000000005
    agent-4: 33.50000000000005
    agent-5: 33.50000000000005
  sampler_perf:
    mean_env_wait_ms: 24.59375505776869
    mean_inference_ms: 12.786963326394375
    mean_processing_ms: 51.85295440733473
  time_since_restore: 1290.0893800258636
  time_this_iter_s: 166.67269134521484
  time_total_s: 10416.101193904877
  timestamp: 1637025275
  timesteps_since_restore: 672000
  timesteps_this_iter: 96000
  timesteps_total: 6432000
  training_iteration: 67
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 46.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |     67 |          10416.1 | 6432000 |   495.59 |
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 47
    apples_agent-0_mean: 6.4
    apples_agent-0_min: 0
    apples_agent-1_max: 130
    apples_agent-1_mean: 20.73
    apples_agent-1_min: 0
    apples_agent-2_max: 154
    apples_agent-2_mean: 19.14
    apples_agent-2_min: 0
    apples_agent-3_max: 181
    apples_agent-3_mean: 91.37
    apples_agent-3_min: 26
    apples_agent-4_max: 30
    apples_agent-4_mean: 1.36
    apples_agent-4_min: 0
    apples_agent-5_max: 123
    apples_agent-5_mean: 69.99
    apples_agent-5_min: 17
    cleaning_beam_agent-0_max: 389
    cleaning_beam_agent-0_mean: 270.44
    cleaning_beam_agent-0_min: 167
    cleaning_beam_agent-1_max: 500
    cleaning_beam_agent-1_mean: 277.94
    cleaning_beam_agent-1_min: 108
    cleaning_beam_agent-2_max: 693
    cleaning_beam_agent-2_mean: 348.94
    cleaning_beam_agent-2_min: 116
    cleaning_beam_agent-3_max: 155
    cleaning_beam_agent-3_mean: 60.66
    cleaning_beam_agent-3_min: 24
    cleaning_beam_agent-4_max: 515
    cleaning_beam_agent-4_mean: 371.03
    cleaning_beam_agent-4_min: 193
    cleaning_beam_agent-5_max: 154
    cleaning_beam_agent-5_mean: 51.73
    cleaning_beam_agent-5_min: 20
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.06
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 6
    fire_beam_agent-5_mean: 0.08
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-17-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 743.9999999999951
  episode_reward_mean: 505.77000000000476
  episode_reward_min: 188.99999999999824
  episodes_this_iter: 96
  episodes_total: 6528
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 21789.846
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000858643208630383
        entropy: 1.2815301418304443
        entropy_coeff: 0.0017600000137463212
        kl: 0.009064346551895142
        model: {}
        policy_loss: -0.019580096006393433
        total_loss: -0.01893772929906845
        vf_explained_var: 0.10988037288188934
        vf_loss: 10.849895477294922
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000858643208630383
        entropy: 1.2297271490097046
        entropy_coeff: 0.0017600000137463212
        kl: 0.013569097965955734
        model: {}
        policy_loss: -0.033003099262714386
        total_loss: -0.031240878626704216
        vf_explained_var: 0.005791828036308289
        vf_loss: 12.127285957336426
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000858643208630383
        entropy: 1.2537370920181274
        entropy_coeff: 0.0017600000137463212
        kl: 0.011363664641976357
        model: {}
        policy_loss: -0.0259703416377306
        total_loss: -0.024824336171150208
        vf_explained_var: 0.1135166585445404
        vf_loss: 10.798497200012207
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000858643208630383
        entropy: 1.0429913997650146
        entropy_coeff: 0.0017600000137463212
        kl: 0.01034619566053152
        model: {}
        policy_loss: -0.01972765102982521
        total_loss: -0.01847553811967373
        vf_explained_var: 0.1642867624759674
        vf_loss: 10.185391426086426
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000858643208630383
        entropy: 1.187947392463684
        entropy_coeff: 0.0017600000137463212
        kl: 0.013824481517076492
        model: {}
        policy_loss: -0.03091140277683735
        total_loss: -0.0291183739900589
        vf_explained_var: 0.08124002814292908
        vf_loss: 11.18914794921875
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000858643208630383
        entropy: 1.1929184198379517
        entropy_coeff: 0.0017600000137463212
        kl: 0.012341711670160294
        model: {}
        policy_loss: -0.029676062986254692
        total_loss: -0.028314407914876938
        vf_explained_var: 0.18512125313282013
        vf_loss: 9.928481101989746
    load_time_ms: 49071.77
    num_steps_sampled: 6528000
    num_steps_trained: 6528000
    sample_time_ms: 110880.983
    update_time_ms: 473.113
  iterations_since_restore: 8
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 16.487550200803213
    ram_util_percent: 25.25381526104418
  pid: 24061
  policy_reward_max:
    agent-0: 124.00000000000055
    agent-1: 124.00000000000055
    agent-2: 124.00000000000055
    agent-3: 124.00000000000055
    agent-4: 124.00000000000055
    agent-5: 124.00000000000055
  policy_reward_mean:
    agent-0: 84.29500000000013
    agent-1: 84.29500000000013
    agent-2: 84.29500000000013
    agent-3: 84.29500000000013
    agent-4: 84.29500000000013
    agent-5: 84.29500000000013
  policy_reward_min:
    agent-0: 31.500000000000064
    agent-1: 31.500000000000064
    agent-2: 31.500000000000064
    agent-3: 31.500000000000064
    agent-4: 31.500000000000064
    agent-5: 31.500000000000064
  sampler_perf:
    mean_env_wait_ms: 24.59218733850245
    mean_inference_ms: 12.756627402689725
    mean_processing_ms: 51.829932651357964
  time_since_restore: 1464.3782033920288
  time_this_iter_s: 174.28882336616516
  time_total_s: 10590.390017271042
  timestamp: 1637025449
  timesteps_since_restore: 768000
  timesteps_this_iter: 96000
  timesteps_total: 6528000
  training_iteration: 68
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 46.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |     68 |          10590.4 | 6528000 |   505.77 |
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 56
    apples_agent-0_mean: 9.46
    apples_agent-0_min: 0
    apples_agent-1_max: 95
    apples_agent-1_mean: 21.33
    apples_agent-1_min: 0
    apples_agent-2_max: 156
    apples_agent-2_mean: 10.12
    apples_agent-2_min: 0
    apples_agent-3_max: 140
    apples_agent-3_mean: 79.87
    apples_agent-3_min: 30
    apples_agent-4_max: 99
    apples_agent-4_mean: 4.53
    apples_agent-4_min: 0
    apples_agent-5_max: 135
    apples_agent-5_mean: 68.94
    apples_agent-5_min: 23
    cleaning_beam_agent-0_max: 384
    cleaning_beam_agent-0_mean: 263.78
    cleaning_beam_agent-0_min: 82
    cleaning_beam_agent-1_max: 611
    cleaning_beam_agent-1_mean: 260.56
    cleaning_beam_agent-1_min: 97
    cleaning_beam_agent-2_max: 588
    cleaning_beam_agent-2_mean: 361.5
    cleaning_beam_agent-2_min: 84
    cleaning_beam_agent-3_max: 123
    cleaning_beam_agent-3_mean: 57.43
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 569
    cleaning_beam_agent-4_mean: 397.93
    cleaning_beam_agent-4_min: 128
    cleaning_beam_agent-5_max: 130
    cleaning_beam_agent-5_mean: 52.97
    cleaning_beam_agent-5_min: 13
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.08
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 42
    fire_beam_agent-5_mean: 0.65
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-20-17
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 743.9999999999942
  episode_reward_mean: 510.8300000000036
  episode_reward_min: 135.0000000000146
  episodes_this_iter: 96
  episodes_total: 6624
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 21643.163
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008526528254151344
        entropy: 1.2591423988342285
        entropy_coeff: 0.0017600000137463212
        kl: 0.008454529568552971
        model: {}
        policy_loss: -0.0185487549751997
        total_loss: -0.01772213540971279
        vf_explained_var: 0.05951815843582153
        vf_loss: 13.518028259277344
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008526528254151344
        entropy: 1.2448405027389526
        entropy_coeff: 0.0017600000137463212
        kl: 0.01311667449772358
        model: {}
        policy_loss: -0.0318521223962307
        total_loss: -0.030036495998501778
        vf_explained_var: 0.03769136965274811
        vf_loss: 13.832088470458984
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008526528254151344
        entropy: 1.2742187976837158
        entropy_coeff: 0.0017600000137463212
        kl: 0.01195044070482254
        model: {}
        policy_loss: -0.026577619835734367
        total_loss: -0.025137649849057198
        vf_explained_var: 0.10007396340370178
        vf_loss: 12.925071716308594
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008526528254151344
        entropy: 1.0000072717666626
        entropy_coeff: 0.0017600000137463212
        kl: 0.00912478007376194
        model: {}
        policy_loss: -0.02009524777531624
        total_loss: -0.01883583702147007
        vf_explained_var: 0.1726013571023941
        vf_loss: 11.944710731506348
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008526528254151344
        entropy: 1.1808429956436157
        entropy_coeff: 0.0017600000137463212
        kl: 0.011991996318101883
        model: {}
        policy_loss: -0.030828410759568214
        total_loss: -0.029199007898569107
        vf_explained_var: 0.08948315680027008
        vf_loss: 13.092872619628906
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008526528254151344
        entropy: 1.1972962617874146
        entropy_coeff: 0.0017600000137463212
        kl: 0.01131646241992712
        model: {}
        policy_loss: -0.029897186905145645
        total_loss: -0.028539219871163368
        vf_explained_var: 0.16021615266799927
        vf_loss: 12.019174575805664
    load_time_ms: 47392.887
    num_steps_sampled: 6624000
    num_steps_trained: 6624000
    sample_time_ms: 111130.52
    update_time_ms: 427.654
  iterations_since_restore: 9
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 16.77958333333333
    ram_util_percent: 25.180833333333332
  pid: 24061
  policy_reward_max:
    agent-0: 124.00000000000036
    agent-1: 124.00000000000036
    agent-2: 124.00000000000036
    agent-3: 124.00000000000036
    agent-4: 124.00000000000036
    agent-5: 124.00000000000036
  policy_reward_mean:
    agent-0: 85.1383333333335
    agent-1: 85.1383333333335
    agent-2: 85.1383333333335
    agent-3: 85.1383333333335
    agent-4: 85.1383333333335
    agent-5: 85.1383333333335
  policy_reward_min:
    agent-0: 22.500000000000007
    agent-1: 22.500000000000007
    agent-2: 22.500000000000007
    agent-3: 22.500000000000007
    agent-4: 22.500000000000007
    agent-5: 22.500000000000007
  sampler_perf:
    mean_env_wait_ms: 24.615999029828664
    mean_inference_ms: 12.733138216070328
    mean_processing_ms: 51.85767764793631
  time_since_restore: 1632.1865406036377
  time_this_iter_s: 167.8083372116089
  time_total_s: 10758.19835448265
  timestamp: 1637025617
  timesteps_since_restore: 864000
  timesteps_this_iter: 96000
  timesteps_total: 6624000
  training_iteration: 69
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 45.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |     69 |          10758.2 | 6624000 |   510.83 |
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 53
    apples_agent-0_mean: 9.55
    apples_agent-0_min: 0
    apples_agent-1_max: 130
    apples_agent-1_mean: 26.69
    apples_agent-1_min: 0
    apples_agent-2_max: 133
    apples_agent-2_mean: 11.56
    apples_agent-2_min: 0
    apples_agent-3_max: 159
    apples_agent-3_mean: 88.3
    apples_agent-3_min: 22
    apples_agent-4_max: 77
    apples_agent-4_mean: 5.0
    apples_agent-4_min: 0
    apples_agent-5_max: 112
    apples_agent-5_mean: 69.45
    apples_agent-5_min: 23
    cleaning_beam_agent-0_max: 417
    cleaning_beam_agent-0_mean: 261.61
    cleaning_beam_agent-0_min: 81
    cleaning_beam_agent-1_max: 495
    cleaning_beam_agent-1_mean: 252.45
    cleaning_beam_agent-1_min: 82
    cleaning_beam_agent-2_max: 611
    cleaning_beam_agent-2_mean: 345.05
    cleaning_beam_agent-2_min: 127
    cleaning_beam_agent-3_max: 144
    cleaning_beam_agent-3_mean: 63.85
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 630
    cleaning_beam_agent-4_mean: 431.52
    cleaning_beam_agent-4_min: 189
    cleaning_beam_agent-5_max: 144
    cleaning_beam_agent-5_mean: 51.64
    cleaning_beam_agent-5_min: 12
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-23-00
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 754.9999999999878
  episode_reward_mean: 523.7700000000025
  episode_reward_min: 216.99999999999736
  episodes_this_iter: 96
  episodes_total: 6720
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 21534.324
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008466623839922249
        entropy: 1.254894733428955
        entropy_coeff: 0.0017600000137463212
        kl: 0.008527345955371857
        model: {}
        policy_loss: -0.0199522003531456
        total_loss: -0.01918807625770569
        vf_explained_var: 0.055506691336631775
        vf_loss: 12.672717094421387
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008466623839922249
        entropy: 1.2460238933563232
        entropy_coeff: 0.0017600000137463212
        kl: 0.013506017625331879
        model: {}
        policy_loss: -0.03347592428326607
        total_loss: -0.03168847784399986
        vf_explained_var: 0.04717846214771271
        vf_loss: 12.792478561401367
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008466623839922249
        entropy: 1.2905250787734985
        entropy_coeff: 0.0017600000137463212
        kl: 0.013041982427239418
        model: {}
        policy_loss: -0.027017761021852493
        total_loss: -0.025480128824710846
        vf_explained_var: 0.10566776990890503
        vf_loss: 12.005610466003418
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008466623839922249
        entropy: 1.0540037155151367
        entropy_coeff: 0.0017600000137463212
        kl: 0.010124918073415756
        model: {}
        policy_loss: -0.02133522555232048
        total_loss: -0.020054815337061882
        vf_explained_var: 0.17277078330516815
        vf_loss: 11.10478687286377
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008466623839922249
        entropy: 1.142510175704956
        entropy_coeff: 0.0017600000137463212
        kl: 0.01319147553294897
        model: {}
        policy_loss: -0.03288612514734268
        total_loss: -0.03105778805911541
        vf_explained_var: 0.10505349934101105
        vf_loss: 12.008606910705566
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008466623839922249
        entropy: 1.1573700904846191
        entropy_coeff: 0.0017600000137463212
        kl: 0.011724297888576984
        model: {}
        policy_loss: -0.03023545816540718
        total_loss: -0.028848234564065933
        vf_explained_var: 0.196002796292305
        vf_loss: 10.793363571166992
    load_time_ms: 46429.725
    num_steps_sampled: 6720000
    num_steps_trained: 6720000
    sample_time_ms: 110357.705
    update_time_ms: 392.37
  iterations_since_restore: 10
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 17.25086956521739
    ram_util_percent: 25.12
  pid: 24061
  policy_reward_max:
    agent-0: 125.83333333333395
    agent-1: 125.83333333333395
    agent-2: 125.83333333333395
    agent-3: 125.83333333333395
    agent-4: 125.83333333333395
    agent-5: 125.83333333333395
  policy_reward_mean:
    agent-0: 87.29500000000019
    agent-1: 87.29500000000019
    agent-2: 87.29500000000019
    agent-3: 87.29500000000019
    agent-4: 87.29500000000019
    agent-5: 87.29500000000019
  policy_reward_min:
    agent-0: 36.166666666666664
    agent-1: 36.166666666666664
    agent-2: 36.166666666666664
    agent-3: 36.166666666666664
    agent-4: 36.166666666666664
    agent-5: 36.166666666666664
  sampler_perf:
    mean_env_wait_ms: 24.639261135193877
    mean_inference_ms: 12.717483712091074
    mean_processing_ms: 51.84571006927887
  time_since_restore: 1794.0843305587769
  time_this_iter_s: 161.89778995513916
  time_total_s: 10920.09614443779
  timestamp: 1637025780
  timesteps_since_restore: 960000
  timesteps_this_iter: 96000
  timesteps_total: 6720000
  training_iteration: 70
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 45.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |     70 |          10920.1 | 6720000 |   523.77 |
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 53
    apples_agent-0_mean: 7.83
    apples_agent-0_min: 0
    apples_agent-1_max: 73
    apples_agent-1_mean: 22.94
    apples_agent-1_min: 0
    apples_agent-2_max: 123
    apples_agent-2_mean: 10.19
    apples_agent-2_min: 0
    apples_agent-3_max: 168
    apples_agent-3_mean: 95.28
    apples_agent-3_min: 38
    apples_agent-4_max: 89
    apples_agent-4_mean: 3.84
    apples_agent-4_min: 0
    apples_agent-5_max: 200
    apples_agent-5_mean: 74.95
    apples_agent-5_min: 9
    cleaning_beam_agent-0_max: 417
    cleaning_beam_agent-0_mean: 266.19
    cleaning_beam_agent-0_min: 113
    cleaning_beam_agent-1_max: 520
    cleaning_beam_agent-1_mean: 265.79
    cleaning_beam_agent-1_min: 56
    cleaning_beam_agent-2_max: 545
    cleaning_beam_agent-2_mean: 330.17
    cleaning_beam_agent-2_min: 113
    cleaning_beam_agent-3_max: 126
    cleaning_beam_agent-3_mean: 63.2
    cleaning_beam_agent-3_min: 28
    cleaning_beam_agent-4_max: 605
    cleaning_beam_agent-4_mean: 411.44
    cleaning_beam_agent-4_min: 145
    cleaning_beam_agent-5_max: 163
    cleaning_beam_agent-5_mean: 55.75
    cleaning_beam_agent-5_min: 14
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 3
    fire_beam_agent-1_mean: 0.06
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-25-41
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 762.9999999999925
  episode_reward_mean: 542.1200000000033
  episode_reward_min: 241.99999999999596
  episodes_this_iter: 96
  episodes_total: 6816
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20354.452
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008406720007769763
        entropy: 1.2560617923736572
        entropy_coeff: 0.0017600000137463212
        kl: 0.009193994104862213
        model: {}
        policy_loss: -0.02025238797068596
        total_loss: -0.019461464136838913
        vf_explained_var: 0.07964007556438446
        vf_loss: 11.62796401977539
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008406720007769763
        entropy: 1.2252907752990723
        entropy_coeff: 0.0017600000137463212
        kl: 0.013409839943051338
        model: {}
        policy_loss: -0.03451075032353401
        total_loss: -0.03274039551615715
        vf_explained_var: 0.015956565737724304
        vf_loss: 12.449029922485352
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008406720007769763
        entropy: 1.2804999351501465
        entropy_coeff: 0.0017600000137463212
        kl: 0.013662455603480339
        model: {}
        policy_loss: -0.0270367581397295
        total_loss: -0.02539645880460739
        vf_explained_var: 0.08118614554405212
        vf_loss: 11.614888191223145
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008406720007769763
        entropy: 1.0578548908233643
        entropy_coeff: 0.0017600000137463212
        kl: 0.009637809358537197
        model: {}
        policy_loss: -0.02175419218838215
        total_loss: -0.020652318373322487
        vf_explained_var: 0.18013568222522736
        vf_loss: 10.361339569091797
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008406720007769763
        entropy: 1.1478571891784668
        entropy_coeff: 0.0017600000137463212
        kl: 0.01255942415446043
        model: {}
        policy_loss: -0.03314455598592758
        total_loss: -0.031495146453380585
        vf_explained_var: 0.08385753631591797
        vf_loss: 11.577531814575195
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008406720007769763
        entropy: 1.1479688882827759
        entropy_coeff: 0.0017600000137463212
        kl: 0.01210530661046505
        model: {}
        policy_loss: -0.03156256303191185
        total_loss: -0.03011714294552803
        vf_explained_var: 0.17322184145450592
        vf_loss: 10.447821617126465
    load_time_ms: 42903.21
    num_steps_sampled: 6816000
    num_steps_trained: 6816000
    sample_time_ms: 110223.046
    update_time_ms: 87.398
  iterations_since_restore: 11
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 17.0008658008658
    ram_util_percent: 25.0930735930736
  pid: 24061
  policy_reward_max:
    agent-0: 127.16666666666703
    agent-1: 127.16666666666703
    agent-2: 127.16666666666703
    agent-3: 127.16666666666703
    agent-4: 127.16666666666703
    agent-5: 127.16666666666703
  policy_reward_mean:
    agent-0: 90.35333333333352
    agent-1: 90.35333333333352
    agent-2: 90.35333333333352
    agent-3: 90.35333333333352
    agent-4: 90.35333333333352
    agent-5: 90.35333333333352
  policy_reward_min:
    agent-0: 40.333333333333314
    agent-1: 40.333333333333314
    agent-2: 40.333333333333314
    agent-3: 40.333333333333314
    agent-4: 40.333333333333314
    agent-5: 40.333333333333314
  sampler_perf:
    mean_env_wait_ms: 24.64364270308717
    mean_inference_ms: 12.69951494652989
    mean_processing_ms: 51.84510768825879
  time_since_restore: 1955.6886675357819
  time_this_iter_s: 161.604336977005
  time_total_s: 11081.700481414795
  timestamp: 1637025941
  timesteps_since_restore: 1056000
  timesteps_this_iter: 96000
  timesteps_total: 6816000
  training_iteration: 71
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 45.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |     71 |          11081.7 | 6816000 |   542.12 |
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 42
    apples_agent-0_mean: 8.47
    apples_agent-0_min: 0
    apples_agent-1_max: 140
    apples_agent-1_mean: 29.77
    apples_agent-1_min: 0
    apples_agent-2_max: 208
    apples_agent-2_mean: 10.51
    apples_agent-2_min: 0
    apples_agent-3_max: 164
    apples_agent-3_mean: 87.09
    apples_agent-3_min: 19
    apples_agent-4_max: 94
    apples_agent-4_mean: 5.16
    apples_agent-4_min: 0
    apples_agent-5_max: 157
    apples_agent-5_mean: 70.63
    apples_agent-5_min: 27
    cleaning_beam_agent-0_max: 375
    cleaning_beam_agent-0_mean: 256.52
    cleaning_beam_agent-0_min: 118
    cleaning_beam_agent-1_max: 430
    cleaning_beam_agent-1_mean: 253.31
    cleaning_beam_agent-1_min: 105
    cleaning_beam_agent-2_max: 551
    cleaning_beam_agent-2_mean: 353.89
    cleaning_beam_agent-2_min: 108
    cleaning_beam_agent-3_max: 138
    cleaning_beam_agent-3_mean: 69.07
    cleaning_beam_agent-3_min: 24
    cleaning_beam_agent-4_max: 528
    cleaning_beam_agent-4_mean: 392.11
    cleaning_beam_agent-4_min: 161
    cleaning_beam_agent-5_max: 147
    cleaning_beam_agent-5_mean: 55.37
    cleaning_beam_agent-5_min: 16
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.06
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-28-13
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 774.9999999999978
  episode_reward_mean: 529.1700000000045
  episode_reward_min: 245.9999999999953
  episodes_this_iter: 96
  episodes_total: 6912
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20360.069
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008346816175617278
        entropy: 1.239181637763977
        entropy_coeff: 0.0017600000137463212
        kl: 0.009198414161801338
        model: {}
        policy_loss: -0.021096063777804375
        total_loss: -0.0203440859913826
        vf_explained_var: 0.0512995719909668
        vf_loss: 10.932514190673828
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008346816175617278
        entropy: 1.2245874404907227
        entropy_coeff: 0.0017600000137463212
        kl: 0.014081915840506554
        model: {}
        policy_loss: -0.03449363633990288
        total_loss: -0.03271499276161194
        vf_explained_var: 0.030231967568397522
        vf_loss: 11.175357818603516
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008346816175617278
        entropy: 1.280132532119751
        entropy_coeff: 0.0017600000137463212
        kl: 0.012637993320822716
        model: {}
        policy_loss: -0.026587368920445442
        total_loss: -0.02521814964711666
        vf_explained_var: 0.04965488612651825
        vf_loss: 10.946557998657227
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008346816175617278
        entropy: 1.04838228225708
        entropy_coeff: 0.0017600000137463212
        kl: 0.010733011178672314
        model: {}
        policy_loss: -0.021467577666044235
        total_loss: -0.020197566598653793
        vf_explained_var: 0.15967905521392822
        vf_loss: 9.685620307922363
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008346816175617278
        entropy: 1.1462805271148682
        entropy_coeff: 0.0017600000137463212
        kl: 0.014746691100299358
        model: {}
        policy_loss: -0.030963458120822906
        total_loss: -0.028971577063202858
        vf_explained_var: 0.07980857789516449
        vf_loss: 10.599958419799805
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008346816175617278
        entropy: 1.1634429693222046
        entropy_coeff: 0.0017600000137463212
        kl: 0.012400043196976185
        model: {}
        policy_loss: -0.03222043439745903
        total_loss: -0.030816059559583664
        vf_explained_var: 0.15640710294246674
        vf_loss: 9.720270156860352
    load_time_ms: 40208.233
    num_steps_sampled: 6912000
    num_steps_trained: 6912000
    sample_time_ms: 108974.055
    update_time_ms: 66.855
  iterations_since_restore: 12
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 17.61302325581395
    ram_util_percent: 24.266976744186042
  pid: 24061
  policy_reward_max:
    agent-0: 129.1666666666668
    agent-1: 129.1666666666668
    agent-2: 129.1666666666668
    agent-3: 129.1666666666668
    agent-4: 129.1666666666668
    agent-5: 129.1666666666668
  policy_reward_mean:
    agent-0: 88.19500000000016
    agent-1: 88.19500000000016
    agent-2: 88.19500000000016
    agent-3: 88.19500000000016
    agent-4: 88.19500000000016
    agent-5: 88.19500000000016
  policy_reward_min:
    agent-0: 40.99999999999996
    agent-1: 40.99999999999996
    agent-2: 40.99999999999996
    agent-3: 40.99999999999996
    agent-4: 40.99999999999996
    agent-5: 40.99999999999996
  sampler_perf:
    mean_env_wait_ms: 24.62749445163294
    mean_inference_ms: 12.680386665817716
    mean_processing_ms: 52.04471210862053
  time_since_restore: 2107.0886850357056
  time_this_iter_s: 151.4000174999237
  time_total_s: 11233.100498914719
  timestamp: 1637026093
  timesteps_since_restore: 1152000
  timesteps_this_iter: 96000
  timesteps_total: 6912000
  training_iteration: 72
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |     72 |          11233.1 | 6912000 |   529.17 |
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 142
    apples_agent-0_mean: 9.87
    apples_agent-0_min: 0
    apples_agent-1_max: 110
    apples_agent-1_mean: 24.81
    apples_agent-1_min: 0
    apples_agent-2_max: 233
    apples_agent-2_mean: 17.75
    apples_agent-2_min: 0
    apples_agent-3_max: 157
    apples_agent-3_mean: 85.7
    apples_agent-3_min: 30
    apples_agent-4_max: 73
    apples_agent-4_mean: 2.98
    apples_agent-4_min: 0
    apples_agent-5_max: 130
    apples_agent-5_mean: 71.39
    apples_agent-5_min: 27
    cleaning_beam_agent-0_max: 375
    cleaning_beam_agent-0_mean: 262.25
    cleaning_beam_agent-0_min: 126
    cleaning_beam_agent-1_max: 464
    cleaning_beam_agent-1_mean: 302.12
    cleaning_beam_agent-1_min: 129
    cleaning_beam_agent-2_max: 535
    cleaning_beam_agent-2_mean: 341.0
    cleaning_beam_agent-2_min: 122
    cleaning_beam_agent-3_max: 154
    cleaning_beam_agent-3_mean: 62.44
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 570
    cleaning_beam_agent-4_mean: 404.44
    cleaning_beam_agent-4_min: 152
    cleaning_beam_agent-5_max: 133
    cleaning_beam_agent-5_mean: 54.39
    cleaning_beam_agent-5_min: 18
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-30-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 762.999999999973
  episode_reward_mean: 518.7900000000036
  episode_reward_min: 256.99999999999636
  episodes_this_iter: 96
  episodes_total: 7008
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20355.863
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008286911761388183
        entropy: 1.2238576412200928
        entropy_coeff: 0.0017600000137463212
        kl: 0.009450491517782211
        model: {}
        policy_loss: -0.021981947124004364
        total_loss: -0.02107449434697628
        vf_explained_var: 0.06953878700733185
        vf_loss: 11.71341323852539
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008286911761388183
        entropy: 1.2322187423706055
        entropy_coeff: 0.0017600000137463212
        kl: 0.013974231667816639
        model: {}
        policy_loss: -0.034359611570835114
        total_loss: -0.03250517696142197
        vf_explained_var: 0.024140089750289917
        vf_loss: 12.282930374145508
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008286911761388183
        entropy: 1.270670771598816
        entropy_coeff: 0.0017600000137463212
        kl: 0.012389401905238628
        model: {}
        policy_loss: -0.027146434411406517
        total_loss: -0.025773661211133003
        vf_explained_var: 0.10163792967796326
        vf_loss: 11.312729835510254
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008286911761388183
        entropy: 1.0490598678588867
        entropy_coeff: 0.0017600000137463212
        kl: 0.010635027661919594
        model: {}
        policy_loss: -0.022762339562177658
        total_loss: -0.02146545797586441
        vf_explained_var: 0.1924302726984024
        vf_loss: 10.162181854248047
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008286911761388183
        entropy: 1.160139560699463
        entropy_coeff: 0.0017600000137463212
        kl: 0.014419201761484146
        model: {}
        policy_loss: -0.03380536660552025
        total_loss: -0.03182924538850784
        vf_explained_var: 0.09912288188934326
        vf_loss: 11.341222763061523
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008286911761388183
        entropy: 1.1558847427368164
        entropy_coeff: 0.0017600000137463212
        kl: 0.012823463417589664
        model: {}
        policy_loss: -0.03226796165108681
        total_loss: -0.03071669675409794
        vf_explained_var: 0.18915101885795593
        vf_loss: 10.209282875061035
    load_time_ms: 38383.123
    num_steps_sampled: 7008000
    num_steps_trained: 7008000
    sample_time_ms: 107162.201
    update_time_ms: 67.065
  iterations_since_restore: 13
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 18.919806763285024
    ram_util_percent: 19.061835748792276
  pid: 24061
  policy_reward_max:
    agent-0: 127.16666666666761
    agent-1: 127.16666666666761
    agent-2: 127.16666666666761
    agent-3: 127.16666666666761
    agent-4: 127.16666666666761
    agent-5: 127.16666666666761
  policy_reward_mean:
    agent-0: 86.46500000000015
    agent-1: 86.46500000000015
    agent-2: 86.46500000000015
    agent-3: 86.46500000000015
    agent-4: 86.46500000000015
    agent-5: 86.46500000000015
  policy_reward_min:
    agent-0: 42.833333333333314
    agent-1: 42.833333333333314
    agent-2: 42.833333333333314
    agent-3: 42.833333333333314
    agent-4: 42.833333333333314
    agent-5: 42.833333333333314
  sampler_perf:
    mean_env_wait_ms: 24.59556520546123
    mean_inference_ms: 12.650769591142808
    mean_processing_ms: 51.91945083607308
  time_since_restore: 2251.9538509845734
  time_this_iter_s: 144.8651659488678
  time_total_s: 11377.965664863586
  timestamp: 1637026238
  timesteps_since_restore: 1248000
  timesteps_this_iter: 96000
  timesteps_total: 7008000
  training_iteration: 73
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |     73 |            11378 | 7008000 |   518.79 |
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 60
    apples_agent-0_mean: 9.88
    apples_agent-0_min: 0
    apples_agent-1_max: 108
    apples_agent-1_mean: 19.32
    apples_agent-1_min: 0
    apples_agent-2_max: 130
    apples_agent-2_mean: 11.63
    apples_agent-2_min: 0
    apples_agent-3_max: 169
    apples_agent-3_mean: 89.71
    apples_agent-3_min: 30
    apples_agent-4_max: 106
    apples_agent-4_mean: 3.59
    apples_agent-4_min: 0
    apples_agent-5_max: 151
    apples_agent-5_mean: 70.16
    apples_agent-5_min: 6
    cleaning_beam_agent-0_max: 363
    cleaning_beam_agent-0_mean: 244.25
    cleaning_beam_agent-0_min: 111
    cleaning_beam_agent-1_max: 579
    cleaning_beam_agent-1_mean: 346.5
    cleaning_beam_agent-1_min: 156
    cleaning_beam_agent-2_max: 606
    cleaning_beam_agent-2_mean: 345.52
    cleaning_beam_agent-2_min: 119
    cleaning_beam_agent-3_max: 130
    cleaning_beam_agent-3_mean: 62.31
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 588
    cleaning_beam_agent-4_mean: 431.2
    cleaning_beam_agent-4_min: 98
    cleaning_beam_agent-5_max: 175
    cleaning_beam_agent-5_mean: 57.61
    cleaning_beam_agent-5_min: 19
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 3
    fire_beam_agent-1_mean: 0.05
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.06
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-33-07
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 787.9999999999802
  episode_reward_mean: 541.3900000000018
  episode_reward_min: 131.0000000000008
  episodes_this_iter: 96
  episodes_total: 7104
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20319.406
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008227007929235697
        entropy: 1.2324624061584473
        entropy_coeff: 0.0017600000137463212
        kl: 0.008943046443164349
        model: {}
        policy_loss: -0.02170853316783905
        total_loss: -0.02087455429136753
        vf_explained_var: 0.10547295212745667
        vf_loss: 12.145042419433594
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008227007929235697
        entropy: 1.2082092761993408
        entropy_coeff: 0.0017600000137463212
        kl: 0.01454381924122572
        model: {}
        policy_loss: -0.03388262540102005
        total_loss: -0.03182137385010719
        vf_explained_var: 0.05878867208957672
        vf_loss: 12.789373397827148
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008227007929235697
        entropy: 1.2477664947509766
        entropy_coeff: 0.0017600000137463212
        kl: 0.012646473944187164
        model: {}
        policy_loss: -0.027875375002622604
        total_loss: -0.02629418857395649
        vf_explained_var: 0.08119142055511475
        vf_loss: 12.479618072509766
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008227007929235697
        entropy: 1.0041248798370361
        entropy_coeff: 0.0017600000137463212
        kl: 0.010163662023842335
        model: {}
        policy_loss: -0.02361128106713295
        total_loss: -0.022239787504076958
        vf_explained_var: 0.18542061746120453
        vf_loss: 11.060211181640625
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008227007929235697
        entropy: 1.1323392391204834
        entropy_coeff: 0.0017600000137463212
        kl: 0.015836134552955627
        model: {}
        policy_loss: -0.029486529529094696
        total_loss: -0.02704080380499363
        vf_explained_var: 0.06385453045368195
        vf_loss: 12.714189529418945
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008227007929235697
        entropy: 1.1435785293579102
        entropy_coeff: 0.0017600000137463212
        kl: 0.013941742479801178
        model: {}
        policy_loss: -0.0303265992552042
        total_loss: -0.028461340814828873
        vf_explained_var: 0.19711269438266754
        vf_loss: 10.896068572998047
    load_time_ms: 36502.881
    num_steps_sampled: 7104000
    num_steps_trained: 7104000
    sample_time_ms: 106103.302
    update_time_ms: 66.751
  iterations_since_restore: 14
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 18.70235849056604
    ram_util_percent: 19.05094339622642
  pid: 24061
  policy_reward_max:
    agent-0: 131.33333333333385
    agent-1: 131.33333333333385
    agent-2: 131.33333333333385
    agent-3: 131.33333333333385
    agent-4: 131.33333333333385
    agent-5: 131.33333333333385
  policy_reward_mean:
    agent-0: 90.23166666666685
    agent-1: 90.23166666666685
    agent-2: 90.23166666666685
    agent-3: 90.23166666666685
    agent-4: 90.23166666666685
    agent-5: 90.23166666666685
  policy_reward_min:
    agent-0: 21.833333333333336
    agent-1: 21.833333333333336
    agent-2: 21.833333333333336
    agent-3: 21.833333333333336
    agent-4: 21.833333333333336
    agent-5: 21.833333333333336
  sampler_perf:
    mean_env_wait_ms: 24.585869226054243
    mean_inference_ms: 12.61900826320595
    mean_processing_ms: 51.84376856199181
  time_since_restore: 2400.715560913086
  time_this_iter_s: 148.76170992851257
  time_total_s: 11526.727374792099
  timestamp: 1637026387
  timesteps_since_restore: 1344000
  timesteps_this_iter: 96000
  timesteps_total: 7104000
  training_iteration: 74
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |     74 |          11526.7 | 7104000 |   541.39 |
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 71
    apples_agent-0_mean: 9.38
    apples_agent-0_min: 0
    apples_agent-1_max: 75
    apples_agent-1_mean: 22.3
    apples_agent-1_min: 0
    apples_agent-2_max: 217
    apples_agent-2_mean: 13.51
    apples_agent-2_min: 0
    apples_agent-3_max: 169
    apples_agent-3_mean: 84.58
    apples_agent-3_min: 19
    apples_agent-4_max: 75
    apples_agent-4_mean: 4.42
    apples_agent-4_min: 0
    apples_agent-5_max: 129
    apples_agent-5_mean: 71.12
    apples_agent-5_min: 22
    cleaning_beam_agent-0_max: 463
    cleaning_beam_agent-0_mean: 271.25
    cleaning_beam_agent-0_min: 102
    cleaning_beam_agent-1_max: 508
    cleaning_beam_agent-1_mean: 318.08
    cleaning_beam_agent-1_min: 126
    cleaning_beam_agent-2_max: 601
    cleaning_beam_agent-2_mean: 337.7
    cleaning_beam_agent-2_min: 79
    cleaning_beam_agent-3_max: 166
    cleaning_beam_agent-3_mean: 62.38
    cleaning_beam_agent-3_min: 24
    cleaning_beam_agent-4_max: 626
    cleaning_beam_agent-4_mean: 420.32
    cleaning_beam_agent-4_min: 222
    cleaning_beam_agent-5_max: 127
    cleaning_beam_agent-5_mean: 52.46
    cleaning_beam_agent-5_min: 17
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.11
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-35-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 834.9999999999723
  episode_reward_mean: 544.6400000000033
  episode_reward_min: 178.99999999999852
  episodes_this_iter: 96
  episodes_total: 7200
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20324.862
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008167104097083211
        entropy: 1.2347033023834229
        entropy_coeff: 0.0017600000137463212
        kl: 0.009415734559297562
        model: {}
        policy_loss: -0.022351182997226715
        total_loss: -0.021342825144529343
        vf_explained_var: 0.05581575632095337
        vf_loss: 12.982879638671875
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008167104097083211
        entropy: 1.1909164190292358
        entropy_coeff: 0.0017600000137463212
        kl: 0.014964969828724861
        model: {}
        policy_loss: -0.032217539846897125
        total_loss: -0.029986107721924782
        vf_explained_var: 0.030143097043037415
        vf_loss: 13.344486236572266
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008167104097083211
        entropy: 1.2551695108413696
        entropy_coeff: 0.0017600000137463212
        kl: 0.011834241449832916
        model: {}
        policy_loss: -0.02769877016544342
        total_loss: -0.026330331340432167
        vf_explained_var: 0.11949034035205841
        vf_loss: 12.106902122497559
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008167104097083211
        entropy: 1.0247595310211182
        entropy_coeff: 0.0017600000137463212
        kl: 0.01001065131276846
        model: {}
        policy_loss: -0.02412053942680359
        total_loss: -0.02282598242163658
        vf_explained_var: 0.2036719024181366
        vf_loss: 10.959993362426758
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008167104097083211
        entropy: 1.137444257736206
        entropy_coeff: 0.0017600000137463212
        kl: 0.013227080926299095
        model: {}
        policy_loss: -0.03340485692024231
        total_loss: -0.03150802105665207
        vf_explained_var: 0.08860944211483002
        vf_loss: 12.533281326293945
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008167104097083211
        entropy: 1.1124989986419678
        entropy_coeff: 0.0017600000137463212
        kl: 0.012711668387055397
        model: {}
        policy_loss: -0.03269833326339722
        total_loss: -0.031049203127622604
        vf_explained_var: 0.22555536031723022
        vf_loss: 10.647933006286621
    load_time_ms: 35767.468
    num_steps_sampled: 7200000
    num_steps_trained: 7200000
    sample_time_ms: 104455.29
    update_time_ms: 62.986
  iterations_since_restore: 15
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 19.020388349514565
    ram_util_percent: 18.950970873786414
  pid: 24061
  policy_reward_max:
    agent-0: 139.16666666666703
    agent-1: 139.16666666666703
    agent-2: 139.16666666666703
    agent-3: 139.16666666666703
    agent-4: 139.16666666666703
    agent-5: 139.16666666666703
  policy_reward_mean:
    agent-0: 90.7733333333335
    agent-1: 90.7733333333335
    agent-2: 90.7733333333335
    agent-3: 90.7733333333335
    agent-4: 90.7733333333335
    agent-5: 90.7733333333335
  policy_reward_min:
    agent-0: 29.833333333333396
    agent-1: 29.833333333333396
    agent-2: 29.833333333333396
    agent-3: 29.833333333333396
    agent-4: 29.833333333333396
    agent-5: 29.833333333333396
  sampler_perf:
    mean_env_wait_ms: 24.574622281489386
    mean_inference_ms: 12.594377394584805
    mean_processing_ms: 51.75951697019487
  time_since_restore: 2544.942262649536
  time_this_iter_s: 144.2267017364502
  time_total_s: 11670.95407652855
  timestamp: 1637026531
  timesteps_since_restore: 1440000
  timesteps_this_iter: 96000
  timesteps_total: 7200000
  training_iteration: 75
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |     75 |            11671 | 7200000 |   544.64 |
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 57
    apples_agent-0_mean: 9.3
    apples_agent-0_min: 0
    apples_agent-1_max: 79
    apples_agent-1_mean: 21.51
    apples_agent-1_min: 0
    apples_agent-2_max: 197
    apples_agent-2_mean: 13.12
    apples_agent-2_min: 0
    apples_agent-3_max: 170
    apples_agent-3_mean: 88.67
    apples_agent-3_min: 23
    apples_agent-4_max: 120
    apples_agent-4_mean: 3.84
    apples_agent-4_min: 0
    apples_agent-5_max: 110
    apples_agent-5_mean: 71.55
    apples_agent-5_min: 13
    cleaning_beam_agent-0_max: 408
    cleaning_beam_agent-0_mean: 253.9
    cleaning_beam_agent-0_min: 92
    cleaning_beam_agent-1_max: 603
    cleaning_beam_agent-1_mean: 313.95
    cleaning_beam_agent-1_min: 160
    cleaning_beam_agent-2_max: 592
    cleaning_beam_agent-2_mean: 344.42
    cleaning_beam_agent-2_min: 110
    cleaning_beam_agent-3_max: 160
    cleaning_beam_agent-3_mean: 59.07
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 592
    cleaning_beam_agent-4_mean: 413.91
    cleaning_beam_agent-4_min: 117
    cleaning_beam_agent-5_max: 138
    cleaning_beam_agent-5_mean: 53.36
    cleaning_beam_agent-5_min: 17
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 8
    fire_beam_agent-4_mean: 0.14
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-38-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 792.999999999985
  episode_reward_mean: 557.3500000000018
  episode_reward_min: 117.00000000000067
  episodes_this_iter: 96
  episodes_total: 7296
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20288.08
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008107200264930725
        entropy: 1.2326029539108276
        entropy_coeff: 0.0017600000137463212
        kl: 0.00943848118185997
        model: {}
        policy_loss: -0.022134453058242798
        total_loss: -0.021220991387963295
        vf_explained_var: 0.061135873198509216
        vf_loss: 11.951465606689453
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008107200264930725
        entropy: 1.1715500354766846
        entropy_coeff: 0.0017600000137463212
        kl: 0.014169381000101566
        model: {}
        policy_loss: -0.033412110060453415
        total_loss: -0.031428396701812744
        vf_explained_var: 0.04925650358200073
        vf_loss: 12.117643356323242
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008107200264930725
        entropy: 1.256317377090454
        entropy_coeff: 0.0017600000137463212
        kl: 0.012838153168559074
        model: {}
        policy_loss: -0.028749408200383186
        total_loss: -0.027257295325398445
        vf_explained_var: 0.10858994722366333
        vf_loss: 11.35600471496582
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008107200264930725
        entropy: 1.0152003765106201
        entropy_coeff: 0.0017600000137463212
        kl: 0.010000744834542274
        model: {}
        policy_loss: -0.02322572097182274
        total_loss: -0.021941421553492546
        vf_explained_var: 0.15903125703334808
        vf_loss: 10.709043502807617
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008107200264930725
        entropy: 1.1449278593063354
        entropy_coeff: 0.0017600000137463212
        kl: 0.013513955287635326
        model: {}
        policy_loss: -0.033922262489795685
        total_loss: -0.03205873444676399
        vf_explained_var: 0.07651540637016296
        vf_loss: 11.758061408996582
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008107200264930725
        entropy: 1.0937906503677368
        entropy_coeff: 0.0017600000137463212
        kl: 0.012309659272432327
        model: {}
        policy_loss: -0.033527374267578125
        total_loss: -0.031921934336423874
        vf_explained_var: 0.16024163365364075
        vf_loss: 10.685784339904785
    load_time_ms: 34490.195
    num_steps_sampled: 7296000
    num_steps_trained: 7296000
    sample_time_ms: 102937.268
    update_time_ms: 58.492
  iterations_since_restore: 16
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 18.06294642857143
    ram_util_percent: 19.141517857142855
  pid: 24061
  policy_reward_max:
    agent-0: 132.16666666666728
    agent-1: 132.16666666666728
    agent-2: 132.16666666666728
    agent-3: 132.16666666666728
    agent-4: 132.16666666666728
    agent-5: 132.16666666666728
  policy_reward_mean:
    agent-0: 92.89166666666688
    agent-1: 92.89166666666688
    agent-2: 92.89166666666688
    agent-3: 92.89166666666688
    agent-4: 92.89166666666688
    agent-5: 92.89166666666688
  policy_reward_min:
    agent-0: 19.500000000000007
    agent-1: 19.500000000000007
    agent-2: 19.500000000000007
    agent-3: 19.500000000000007
    agent-4: 19.500000000000007
    agent-5: 19.500000000000007
  sampler_perf:
    mean_env_wait_ms: 24.54496598629985
    mean_inference_ms: 12.567447360817305
    mean_processing_ms: 51.670394801814844
  time_since_restore: 2702.4538476467133
  time_this_iter_s: 157.51158499717712
  time_total_s: 11828.465661525726
  timestamp: 1637026689
  timesteps_since_restore: 1536000
  timesteps_this_iter: 96000
  timesteps_total: 7296000
  training_iteration: 76
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |     76 |          11828.5 | 7296000 |   557.35 |
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 38
    apples_agent-0_mean: 9.57
    apples_agent-0_min: 0
    apples_agent-1_max: 97
    apples_agent-1_mean: 25.35
    apples_agent-1_min: 0
    apples_agent-2_max: 139
    apples_agent-2_mean: 12.83
    apples_agent-2_min: 0
    apples_agent-3_max: 152
    apples_agent-3_mean: 89.84
    apples_agent-3_min: 31
    apples_agent-4_max: 65
    apples_agent-4_mean: 2.26
    apples_agent-4_min: 0
    apples_agent-5_max: 118
    apples_agent-5_mean: 72.43
    apples_agent-5_min: 37
    cleaning_beam_agent-0_max: 413
    cleaning_beam_agent-0_mean: 267.34
    cleaning_beam_agent-0_min: 149
    cleaning_beam_agent-1_max: 439
    cleaning_beam_agent-1_mean: 297.19
    cleaning_beam_agent-1_min: 119
    cleaning_beam_agent-2_max: 549
    cleaning_beam_agent-2_mean: 352.85
    cleaning_beam_agent-2_min: 117
    cleaning_beam_agent-3_max: 137
    cleaning_beam_agent-3_mean: 55.42
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 617
    cleaning_beam_agent-4_mean: 456.47
    cleaning_beam_agent-4_min: 195
    cleaning_beam_agent-5_max: 131
    cleaning_beam_agent-5_mean: 54.91
    cleaning_beam_agent-5_min: 16
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.09
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-40-27
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 813.9999999999852
  episode_reward_mean: 573.5400000000012
  episode_reward_min: 223.99999999999727
  episodes_this_iter: 96
  episodes_total: 7392
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20251.99
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000804729585070163
        entropy: 1.239872932434082
        entropy_coeff: 0.0017600000137463212
        kl: 0.00993085652589798
        model: {}
        policy_loss: -0.02377753145992756
        total_loss: -0.022732047364115715
        vf_explained_var: 0.04373881220817566
        vf_loss: 12.41490364074707
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000804729585070163
        entropy: 1.1530970335006714
        entropy_coeff: 0.0017600000137463212
        kl: 0.014191685244441032
        model: {}
        policy_loss: -0.03392907977104187
        total_loss: -0.03186482936143875
        vf_explained_var: 0.034528300166130066
        vf_loss: 12.55364990234375
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000804729585070163
        entropy: 1.24247407913208
        entropy_coeff: 0.0017600000137463212
        kl: 0.013121893629431725
        model: {}
        policy_loss: -0.028782665729522705
        total_loss: -0.0271891001611948
        vf_explained_var: 0.11009375751018524
        vf_loss: 11.559425354003906
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000804729585070163
        entropy: 0.9863418340682983
        entropy_coeff: 0.0017600000137463212
        kl: 0.010050035081803799
        model: {}
        policy_loss: -0.02222016453742981
        total_loss: -0.020852770656347275
        vf_explained_var: 0.15899021923542023
        vf_loss: 10.93346881866455
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000804729585070163
        entropy: 1.0946261882781982
        entropy_coeff: 0.0017600000137463212
        kl: 0.01354097668081522
        model: {}
        policy_loss: -0.03268580138683319
        total_loss: -0.030667360872030258
        vf_explained_var: 0.04787828028202057
        vf_loss: 12.367851257324219
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000804729585070163
        entropy: 1.0819110870361328
        entropy_coeff: 0.0017600000137463212
        kl: 0.013548728078603745
        model: {}
        policy_loss: -0.03411521017551422
        total_loss: -0.03226546570658684
        vf_explained_var: 0.19609427452087402
        vf_loss: 10.44163703918457
    load_time_ms: 33333.233
    num_steps_sampled: 7392000
    num_steps_trained: 7392000
    sample_time_ms: 101240.677
    update_time_ms: 52.312
  iterations_since_restore: 17
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 19.35279187817259
    ram_util_percent: 18.78020304568528
  pid: 24061
  policy_reward_max:
    agent-0: 135.66666666666688
    agent-1: 135.66666666666688
    agent-2: 135.66666666666688
    agent-3: 135.66666666666688
    agent-4: 135.66666666666688
    agent-5: 135.66666666666688
  policy_reward_mean:
    agent-0: 95.59000000000026
    agent-1: 95.59000000000026
    agent-2: 95.59000000000026
    agent-3: 95.59000000000026
    agent-4: 95.59000000000026
    agent-5: 95.59000000000026
  policy_reward_min:
    agent-0: 37.333333333333364
    agent-1: 37.333333333333364
    agent-2: 37.333333333333364
    agent-3: 37.333333333333364
    agent-4: 37.333333333333364
    agent-5: 37.333333333333364
  sampler_perf:
    mean_env_wait_ms: 24.534221362409152
    mean_inference_ms: 12.545659533789468
    mean_processing_ms: 51.59383227010454
  time_since_restore: 2840.2140424251556
  time_this_iter_s: 137.76019477844238
  time_total_s: 11966.225856304169
  timestamp: 1637026827
  timesteps_since_restore: 1632000
  timesteps_this_iter: 96000
  timesteps_total: 7392000
  training_iteration: 77
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |     77 |          11966.2 | 7392000 |   573.54 |
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 55
    apples_agent-0_mean: 8.93
    apples_agent-0_min: 0
    apples_agent-1_max: 85
    apples_agent-1_mean: 25.3
    apples_agent-1_min: 0
    apples_agent-2_max: 204
    apples_agent-2_mean: 13.55
    apples_agent-2_min: 0
    apples_agent-3_max: 165
    apples_agent-3_mean: 87.28
    apples_agent-3_min: 31
    apples_agent-4_max: 101
    apples_agent-4_mean: 4.05
    apples_agent-4_min: 0
    apples_agent-5_max: 119
    apples_agent-5_mean: 71.97
    apples_agent-5_min: 27
    cleaning_beam_agent-0_max: 412
    cleaning_beam_agent-0_mean: 252.33
    cleaning_beam_agent-0_min: 105
    cleaning_beam_agent-1_max: 598
    cleaning_beam_agent-1_mean: 308.76
    cleaning_beam_agent-1_min: 184
    cleaning_beam_agent-2_max: 578
    cleaning_beam_agent-2_mean: 347.94
    cleaning_beam_agent-2_min: 126
    cleaning_beam_agent-3_max: 141
    cleaning_beam_agent-3_mean: 50.99
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 617
    cleaning_beam_agent-4_mean: 438.42
    cleaning_beam_agent-4_min: 115
    cleaning_beam_agent-5_max: 144
    cleaning_beam_agent-5_mean: 49.78
    cleaning_beam_agent-5_min: 15
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.07
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.07
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-42-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 842.999999999989
  episode_reward_mean: 563.4800000000013
  episode_reward_min: 178.99999999999872
  episodes_this_iter: 96
  episodes_total: 7488
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20258.448
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007987392018549144
        entropy: 1.246501088142395
        entropy_coeff: 0.0017600000137463212
        kl: 0.010716848075389862
        model: {}
        policy_loss: -0.02408769726753235
        total_loss: -0.02277614362537861
        vf_explained_var: 0.04993925988674164
        vf_loss: 13.62024211883545
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007987392018549144
        entropy: 1.198472499847412
        entropy_coeff: 0.0017600000137463212
        kl: 0.014159911312162876
        model: {}
        policy_loss: -0.03465354070067406
        total_loss: -0.03254721313714981
        vf_explained_var: 0.03540264070034027
        vf_loss: 13.83655834197998
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007987392018549144
        entropy: 1.2517170906066895
        entropy_coeff: 0.0017600000137463212
        kl: 0.01336525846272707
        model: {}
        policy_loss: -0.02852662093937397
        total_loss: -0.026799701154232025
        vf_explained_var: 0.1237901896238327
        vf_loss: 12.568873405456543
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007987392018549144
        entropy: 0.944399356842041
        entropy_coeff: 0.0017600000137463212
        kl: 0.010405807755887508
        model: {}
        policy_loss: -0.02127877250313759
        total_loss: -0.01969948783516884
        vf_explained_var: 0.19152116775512695
        vf_loss: 11.602642059326172
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007987392018549144
        entropy: 1.1105239391326904
        entropy_coeff: 0.0017600000137463212
        kl: 0.013948197476565838
        model: {}
        policy_loss: -0.03388547524809837
        total_loss: -0.03178386017680168
        vf_explained_var: 0.11612880229949951
        vf_loss: 12.664978981018066
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007987392018549144
        entropy: 1.0607292652130127
        entropy_coeff: 0.0017600000137463212
        kl: 0.01290481723845005
        model: {}
        policy_loss: -0.03195832669734955
        total_loss: -0.03011452592909336
        vf_explained_var: 0.21188262104988098
        vf_loss: 11.29725456237793
    load_time_ms: 30952.73
    num_steps_sampled: 7488000
    num_steps_trained: 7488000
    sample_time_ms: 98651.54
    update_time_ms: 48.918
  iterations_since_restore: 18
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.50505617977528
    ram_util_percent: 18.64213483146067
  pid: 24061
  policy_reward_max:
    agent-0: 140.5000000000004
    agent-1: 140.5000000000004
    agent-2: 140.5000000000004
    agent-3: 140.5000000000004
    agent-4: 140.5000000000004
    agent-5: 140.5000000000004
  policy_reward_mean:
    agent-0: 93.91333333333355
    agent-1: 93.91333333333355
    agent-2: 93.91333333333355
    agent-3: 93.91333333333355
    agent-4: 93.91333333333355
    agent-5: 93.91333333333355
  policy_reward_min:
    agent-0: 29.83333333333339
    agent-1: 29.83333333333339
    agent-2: 29.83333333333339
    agent-3: 29.83333333333339
    agent-4: 29.83333333333339
    agent-5: 29.83333333333339
  sampler_perf:
    mean_env_wait_ms: 24.524997007683456
    mean_inference_ms: 12.528298816422339
    mean_processing_ms: 51.52428021035495
  time_since_restore: 2964.8403792381287
  time_this_iter_s: 124.62633681297302
  time_total_s: 12090.852193117142
  timestamp: 1637026952
  timesteps_since_restore: 1728000
  timesteps_this_iter: 96000
  timesteps_total: 7488000
  training_iteration: 78
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |     78 |          12090.9 | 7488000 |   563.48 |
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 72
    apples_agent-0_mean: 9.72
    apples_agent-0_min: 0
    apples_agent-1_max: 148
    apples_agent-1_mean: 25.97
    apples_agent-1_min: 0
    apples_agent-2_max: 280
    apples_agent-2_mean: 11.1
    apples_agent-2_min: 0
    apples_agent-3_max: 177
    apples_agent-3_mean: 84.74
    apples_agent-3_min: 27
    apples_agent-4_max: 61
    apples_agent-4_mean: 4.35
    apples_agent-4_min: 0
    apples_agent-5_max: 117
    apples_agent-5_mean: 69.86
    apples_agent-5_min: 24
    cleaning_beam_agent-0_max: 348
    cleaning_beam_agent-0_mean: 250.94
    cleaning_beam_agent-0_min: 129
    cleaning_beam_agent-1_max: 498
    cleaning_beam_agent-1_mean: 294.93
    cleaning_beam_agent-1_min: 126
    cleaning_beam_agent-2_max: 544
    cleaning_beam_agent-2_mean: 334.11
    cleaning_beam_agent-2_min: 99
    cleaning_beam_agent-3_max: 122
    cleaning_beam_agent-3_mean: 45.03
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 603
    cleaning_beam_agent-4_mean: 449.98
    cleaning_beam_agent-4_min: 181
    cleaning_beam_agent-5_max: 138
    cleaning_beam_agent-5_mean: 46.12
    cleaning_beam_agent-5_min: 17
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.1
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-44-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 788.999999999981
  episode_reward_mean: 567.4400000000003
  episode_reward_min: 222.999999999997
  episodes_this_iter: 96
  episodes_total: 7584
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20231.244
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007927488186396658
        entropy: 1.2170624732971191
        entropy_coeff: 0.0017600000137463212
        kl: 0.010878930799663067
        model: {}
        policy_loss: -0.023369723930954933
        total_loss: -0.021918926388025284
        vf_explained_var: 0.06843727827072144
        vf_loss: 14.17040729522705
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007927488186396658
        entropy: 1.1608210802078247
        entropy_coeff: 0.0017600000137463212
        kl: 0.014910181052982807
        model: {}
        policy_loss: -0.0362173430621624
        total_loss: -0.03382178023457527
        vf_explained_var: 0.043895572423934937
        vf_loss: 14.565743446350098
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007927488186396658
        entropy: 1.262742280960083
        entropy_coeff: 0.0017600000137463212
        kl: 0.012583114206790924
        model: {}
        policy_loss: -0.030221782624721527
        total_loss: -0.028571842238307
        vf_explained_var: 0.10894735157489777
        vf_loss: 13.557453155517578
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007927488186396658
        entropy: 0.8951970338821411
        entropy_coeff: 0.0017600000137463212
        kl: 0.00925231259316206
        model: {}
        policy_loss: -0.02207334339618683
        total_loss: -0.020611638203263283
        vf_explained_var: 0.22033265233039856
        vf_loss: 11.867925643920898
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007927488186396658
        entropy: 1.0889068841934204
        entropy_coeff: 0.0017600000137463212
        kl: 0.013047192245721817
        model: {}
        policy_loss: -0.03450188785791397
        total_loss: -0.03240848332643509
        vf_explained_var: 0.07936500012874603
        vf_loss: 14.0044527053833
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007927488186396658
        entropy: 1.0453323125839233
        entropy_coeff: 0.0017600000137463212
        kl: 0.012485906481742859
        model: {}
        policy_loss: -0.033018842339515686
        total_loss: -0.031177086755633354
        vf_explained_var: 0.22091351449489594
        vf_loss: 11.843613624572754
    load_time_ms: 29278.885
    num_steps_sampled: 7584000
    num_steps_trained: 7584000
    sample_time_ms: 96566.765
    update_time_ms: 46.339
  iterations_since_restore: 19
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.095675675675675
    ram_util_percent: 18.625405405405406
  pid: 24061
  policy_reward_max:
    agent-0: 131.50000000000085
    agent-1: 131.50000000000085
    agent-2: 131.50000000000085
    agent-3: 131.50000000000085
    agent-4: 131.50000000000085
    agent-5: 131.50000000000085
  policy_reward_mean:
    agent-0: 94.57333333333355
    agent-1: 94.57333333333355
    agent-2: 94.57333333333355
    agent-3: 94.57333333333355
    agent-4: 94.57333333333355
    agent-5: 94.57333333333355
  policy_reward_min:
    agent-0: 37.16666666666665
    agent-1: 37.16666666666665
    agent-2: 37.16666666666665
    agent-3: 37.16666666666665
    agent-4: 37.16666666666665
    agent-5: 37.16666666666665
  sampler_perf:
    mean_env_wait_ms: 24.509325878400873
    mean_inference_ms: 12.512793497544862
    mean_processing_ms: 51.46343628912009
  time_since_restore: 3094.745022058487
  time_this_iter_s: 129.90464282035828
  time_total_s: 12220.7568359375
  timestamp: 1637027082
  timesteps_since_restore: 1824000
  timesteps_this_iter: 96000
  timesteps_total: 7584000
  training_iteration: 79
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 34.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |     79 |          12220.8 | 7584000 |   567.44 |
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 61
    apples_agent-0_mean: 8.36
    apples_agent-0_min: 0
    apples_agent-1_max: 115
    apples_agent-1_mean: 22.47
    apples_agent-1_min: 0
    apples_agent-2_max: 125
    apples_agent-2_mean: 12.11
    apples_agent-2_min: 0
    apples_agent-3_max: 166
    apples_agent-3_mean: 86.92
    apples_agent-3_min: 20
    apples_agent-4_max: 85
    apples_agent-4_mean: 4.27
    apples_agent-4_min: 0
    apples_agent-5_max: 133
    apples_agent-5_mean: 69.74
    apples_agent-5_min: 26
    cleaning_beam_agent-0_max: 425
    cleaning_beam_agent-0_mean: 264.24
    cleaning_beam_agent-0_min: 121
    cleaning_beam_agent-1_max: 531
    cleaning_beam_agent-1_mean: 263.3
    cleaning_beam_agent-1_min: 90
    cleaning_beam_agent-2_max: 638
    cleaning_beam_agent-2_mean: 342.67
    cleaning_beam_agent-2_min: 98
    cleaning_beam_agent-3_max: 132
    cleaning_beam_agent-3_mean: 44.02
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 629
    cleaning_beam_agent-4_mean: 427.83
    cleaning_beam_agent-4_min: 107
    cleaning_beam_agent-5_max: 135
    cleaning_beam_agent-5_mean: 49.02
    cleaning_beam_agent-5_min: 20
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.09
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-46-49
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 781.9999999999886
  episode_reward_mean: 563.7900000000009
  episode_reward_min: 119.0000000000008
  episodes_this_iter: 96
  episodes_total: 7680
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20187.287
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007867583772167563
        entropy: 1.214839220046997
        entropy_coeff: 0.0017600000137463212
        kl: 0.01026509515941143
        model: {}
        policy_loss: -0.02386767603456974
        total_loss: -0.022537706419825554
        vf_explained_var: 0.03990881145000458
        vf_loss: 14.150697708129883
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007867583772167563
        entropy: 1.1359400749206543
        entropy_coeff: 0.0017600000137463212
        kl: 0.014627588912844658
        model: {}
        policy_loss: -0.035496193915605545
        total_loss: -0.03315232694149017
        vf_explained_var: 0.040618449449539185
        vf_loss: 14.176051139831543
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007867583772167563
        entropy: 1.2416276931762695
        entropy_coeff: 0.0017600000137463212
        kl: 0.012866719625890255
        model: {}
        policy_loss: -0.030511554330587387
        total_loss: -0.028810705989599228
        vf_explained_var: 0.10995148122310638
        vf_loss: 13.127702713012695
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007867583772167563
        entropy: 0.8891377449035645
        entropy_coeff: 0.0017600000137463212
        kl: 0.00990645494312048
        model: {}
        policy_loss: -0.022745603695511818
        total_loss: -0.02115359716117382
        vf_explained_var: 0.20258885622024536
        vf_loss: 11.755970001220703
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007867583772167563
        entropy: 1.1051753759384155
        entropy_coeff: 0.0017600000137463212
        kl: 0.013777155429124832
        model: {}
        policy_loss: -0.03626110404729843
        total_loss: -0.034147653728723526
        vf_explained_var: 0.11617562174797058
        vf_loss: 13.0313138961792
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007867583772167563
        entropy: 1.042419672012329
        entropy_coeff: 0.0017600000137463212
        kl: 0.012675033882260323
        model: {}
        policy_loss: -0.03292509540915489
        total_loss: -0.031048331409692764
        vf_explained_var: 0.2024276703596115
        vf_loss: 11.764176368713379
    load_time_ms: 27175.379
    num_steps_sampled: 7680000
    num_steps_trained: 7680000
    sample_time_ms: 95198.426
    update_time_ms: 41.353
  iterations_since_restore: 20
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 16.429444444444446
    ram_util_percent: 16.787222222222223
  pid: 24061
  policy_reward_max:
    agent-0: 130.33333333333383
    agent-1: 130.33333333333383
    agent-2: 130.33333333333383
    agent-3: 130.33333333333383
    agent-4: 130.33333333333383
    agent-5: 130.33333333333383
  policy_reward_mean:
    agent-0: 93.96500000000019
    agent-1: 93.96500000000019
    agent-2: 93.96500000000019
    agent-3: 93.96500000000019
    agent-4: 93.96500000000019
    agent-5: 93.96500000000019
  policy_reward_min:
    agent-0: 19.833333333333332
    agent-1: 19.833333333333332
    agent-2: 19.833333333333332
    agent-3: 19.833333333333332
    agent-4: 19.833333333333332
    agent-5: 19.833333333333332
  sampler_perf:
    mean_env_wait_ms: 24.488464901038633
    mean_inference_ms: 12.495303302384718
    mean_processing_ms: 51.40902796748067
  time_since_restore: 3221.414525985718
  time_this_iter_s: 126.66950392723083
  time_total_s: 12347.42633986473
  timestamp: 1637027209
  timesteps_since_restore: 1920000
  timesteps_this_iter: 96000
  timesteps_total: 7680000
  training_iteration: 80
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |     80 |          12347.4 | 7680000 |   563.79 |
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 45
    apples_agent-0_mean: 8.86
    apples_agent-0_min: 0
    apples_agent-1_max: 164
    apples_agent-1_mean: 27.2
    apples_agent-1_min: 0
    apples_agent-2_max: 268
    apples_agent-2_mean: 15.82
    apples_agent-2_min: 0
    apples_agent-3_max: 168
    apples_agent-3_mean: 89.38
    apples_agent-3_min: 20
    apples_agent-4_max: 43
    apples_agent-4_mean: 3.17
    apples_agent-4_min: 0
    apples_agent-5_max: 132
    apples_agent-5_mean: 72.57
    apples_agent-5_min: 30
    cleaning_beam_agent-0_max: 455
    cleaning_beam_agent-0_mean: 268.55
    cleaning_beam_agent-0_min: 139
    cleaning_beam_agent-1_max: 558
    cleaning_beam_agent-1_mean: 243.32
    cleaning_beam_agent-1_min: 93
    cleaning_beam_agent-2_max: 599
    cleaning_beam_agent-2_mean: 359.49
    cleaning_beam_agent-2_min: 32
    cleaning_beam_agent-3_max: 154
    cleaning_beam_agent-3_mean: 47.27
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 637
    cleaning_beam_agent-4_mean: 454.7
    cleaning_beam_agent-4_min: 228
    cleaning_beam_agent-5_max: 144
    cleaning_beam_agent-5_mean: 48.3
    cleaning_beam_agent-5_min: 13
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.09
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 12
    fire_beam_agent-5_mean: 0.17
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-48-54
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 794.9999999999764
  episode_reward_mean: 573.3899999999995
  episode_reward_min: 309.0000000000032
  episodes_this_iter: 96
  episodes_total: 7776
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20153.724
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007807679940015078
        entropy: 1.2244466543197632
        entropy_coeff: 0.0017600000137463212
        kl: 0.010757636278867722
        model: {}
        policy_loss: -0.02387899160385132
        total_loss: -0.022521788254380226
        vf_explained_var: 0.08694292604923248
        vf_loss: 13.607027053833008
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007807679940015078
        entropy: 1.107555866241455
        entropy_coeff: 0.0017600000137463212
        kl: 0.014567237347364426
        model: {}
        policy_loss: -0.03634381294250488
        total_loss: -0.03396234288811684
        vf_explained_var: 0.05027619004249573
        vf_loss: 14.173151016235352
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007807679940015078
        entropy: 1.1815924644470215
        entropy_coeff: 0.0017600000137463212
        kl: 0.01211425568908453
        model: {}
        policy_loss: -0.029842030256986618
        total_loss: -0.028116125613451004
        vf_explained_var: 0.07266688346862793
        vf_loss: 13.826562881469727
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007807679940015078
        entropy: 0.8922643065452576
        entropy_coeff: 0.0017600000137463212
        kl: 0.009169836528599262
        model: {}
        policy_loss: -0.022770129144191742
        total_loss: -0.021322645246982574
        vf_explained_var: 0.20557035505771637
        vf_loss: 11.8390474319458
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007807679940015078
        entropy: 1.07993483543396
        entropy_coeff: 0.0017600000137463212
        kl: 0.012533394619822502
        model: {}
        policy_loss: -0.0344783253967762
        total_loss: -0.03252430632710457
        vf_explained_var: 0.09494456648826599
        vf_loss: 13.480263710021973
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007807679940015078
        entropy: 1.050196647644043
        entropy_coeff: 0.0017600000137463212
        kl: 0.014039261266589165
        model: {}
        policy_loss: -0.03177621215581894
        total_loss: -0.02960182912647724
        vf_explained_var: 0.18541166186332703
        vf_loss: 12.148792266845703
    load_time_ms: 24630.78
    num_steps_sampled: 7776000
    num_steps_trained: 7776000
    sample_time_ms: 93979.002
    update_time_ms: 37.182
  iterations_since_restore: 21
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.544382022471911
    ram_util_percent: 16.12584269662921
  pid: 24061
  policy_reward_max:
    agent-0: 132.5000000000005
    agent-1: 132.5000000000005
    agent-2: 132.5000000000005
    agent-3: 132.5000000000005
    agent-4: 132.5000000000005
    agent-5: 132.5000000000005
  policy_reward_mean:
    agent-0: 95.56500000000024
    agent-1: 95.56500000000024
    agent-2: 95.56500000000024
    agent-3: 95.56500000000024
    agent-4: 95.56500000000024
    agent-5: 95.56500000000024
  policy_reward_min:
    agent-0: 51.499999999999815
    agent-1: 51.499999999999815
    agent-2: 51.499999999999815
    agent-3: 51.499999999999815
    agent-4: 51.499999999999815
    agent-5: 51.499999999999815
  sampler_perf:
    mean_env_wait_ms: 24.476318730479637
    mean_inference_ms: 12.478185766645407
    mean_processing_ms: 51.35611342138685
  time_since_restore: 3345.023436307907
  time_this_iter_s: 123.60891032218933
  time_total_s: 12471.03525018692
  timestamp: 1637027334
  timesteps_since_restore: 2016000
  timesteps_this_iter: 96000
  timesteps_total: 7776000
  training_iteration: 81
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |     81 |            12471 | 7776000 |   573.39 |
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 67
    apples_agent-0_mean: 9.13
    apples_agent-0_min: 0
    apples_agent-1_max: 156
    apples_agent-1_mean: 26.04
    apples_agent-1_min: 0
    apples_agent-2_max: 143
    apples_agent-2_mean: 11.1
    apples_agent-2_min: 0
    apples_agent-3_max: 152
    apples_agent-3_mean: 83.06
    apples_agent-3_min: 0
    apples_agent-4_max: 52
    apples_agent-4_mean: 3.24
    apples_agent-4_min: 0
    apples_agent-5_max: 174
    apples_agent-5_mean: 74.7
    apples_agent-5_min: 31
    cleaning_beam_agent-0_max: 382
    cleaning_beam_agent-0_mean: 264.62
    cleaning_beam_agent-0_min: 116
    cleaning_beam_agent-1_max: 505
    cleaning_beam_agent-1_mean: 257.73
    cleaning_beam_agent-1_min: 139
    cleaning_beam_agent-2_max: 627
    cleaning_beam_agent-2_mean: 368.67
    cleaning_beam_agent-2_min: 154
    cleaning_beam_agent-3_max: 166
    cleaning_beam_agent-3_mean: 50.12
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 593
    cleaning_beam_agent-4_mean: 436.94
    cleaning_beam_agent-4_min: 224
    cleaning_beam_agent-5_max: 139
    cleaning_beam_agent-5_mean: 46.49
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.05
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-51-00
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 810.9999999999927
  episode_reward_mean: 572.1000000000001
  episode_reward_min: 202.99999999999773
  episodes_this_iter: 96
  episodes_total: 7872
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20163.165
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007747776107862592
        entropy: 1.2075520753860474
        entropy_coeff: 0.0017600000137463212
        kl: 0.009538383223116398
        model: {}
        policy_loss: -0.024310577660799026
        total_loss: -0.023221777752041817
        vf_explained_var: 0.09399305284023285
        vf_loss: 13.064139366149902
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007747776107862592
        entropy: 1.1468552350997925
        entropy_coeff: 0.0017600000137463212
        kl: 0.015323015861213207
        model: {}
        policy_loss: -0.03635241463780403
        total_loss: -0.033894360065460205
        vf_explained_var: 0.021541565656661987
        vf_loss: 14.119172096252441
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007747776107862592
        entropy: 1.202451229095459
        entropy_coeff: 0.0017600000137463212
        kl: 0.012139125727117062
        model: {}
        policy_loss: -0.029698165133595467
        total_loss: -0.02807067334651947
        vf_explained_var: 0.08766533434391022
        vf_loss: 13.159811973571777
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007747776107862592
        entropy: 0.8809323310852051
        entropy_coeff: 0.0017600000137463212
        kl: 0.009662788361310959
        model: {}
        policy_loss: -0.022790532559156418
        total_loss: -0.02126418612897396
        vf_explained_var: 0.20716403424739838
        vf_loss: 11.442285537719727
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007747776107862592
        entropy: 1.0925323963165283
        entropy_coeff: 0.0017600000137463212
        kl: 0.012776479125022888
        model: {}
        policy_loss: -0.03537876904010773
        total_loss: -0.033481113612651825
        vf_explained_var: 0.12261942028999329
        vf_loss: 12.652134895324707
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007747776107862592
        entropy: 1.0277581214904785
        entropy_coeff: 0.0017600000137463212
        kl: 0.013103155419230461
        model: {}
        policy_loss: -0.03364565595984459
        total_loss: -0.03165794909000397
        vf_explained_var: 0.1843019425868988
        vf_loss: 11.759326934814453
    load_time_ms: 23195.656
    num_steps_sampled: 7872000
    num_steps_trained: 7872000
    sample_time_ms: 92853.31
    update_time_ms: 35.802
  iterations_since_restore: 22
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.32388888888889
    ram_util_percent: 16.12111111111111
  pid: 24061
  policy_reward_max:
    agent-0: 135.16666666666714
    agent-1: 135.16666666666714
    agent-2: 135.16666666666714
    agent-3: 135.16666666666714
    agent-4: 135.16666666666714
    agent-5: 135.16666666666714
  policy_reward_mean:
    agent-0: 95.35000000000022
    agent-1: 95.35000000000022
    agent-2: 95.35000000000022
    agent-3: 95.35000000000022
    agent-4: 95.35000000000022
    agent-5: 95.35000000000022
  policy_reward_min:
    agent-0: 33.83333333333338
    agent-1: 33.83333333333338
    agent-2: 33.83333333333338
    agent-3: 33.83333333333338
    agent-4: 33.83333333333338
    agent-5: 33.83333333333338
  sampler_perf:
    mean_env_wait_ms: 24.459174132164673
    mean_inference_ms: 12.463561811497318
    mean_processing_ms: 51.31118079402772
  time_since_restore: 3470.911559820175
  time_this_iter_s: 125.88812351226807
  time_total_s: 12596.923373699188
  timestamp: 1637027460
  timesteps_since_restore: 2112000
  timesteps_this_iter: 96000
  timesteps_total: 7872000
  training_iteration: 82
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |     82 |          12596.9 | 7872000 |    572.1 |
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 72
    apples_agent-0_mean: 9.44
    apples_agent-0_min: 0
    apples_agent-1_max: 163
    apples_agent-1_mean: 31.85
    apples_agent-1_min: 0
    apples_agent-2_max: 176
    apples_agent-2_mean: 13.66
    apples_agent-2_min: 0
    apples_agent-3_max: 151
    apples_agent-3_mean: 91.89
    apples_agent-3_min: 22
    apples_agent-4_max: 102
    apples_agent-4_mean: 2.94
    apples_agent-4_min: 0
    apples_agent-5_max: 141
    apples_agent-5_mean: 73.67
    apples_agent-5_min: 25
    cleaning_beam_agent-0_max: 447
    cleaning_beam_agent-0_mean: 282.63
    cleaning_beam_agent-0_min: 101
    cleaning_beam_agent-1_max: 437
    cleaning_beam_agent-1_mean: 240.24
    cleaning_beam_agent-1_min: 109
    cleaning_beam_agent-2_max: 627
    cleaning_beam_agent-2_mean: 361.6
    cleaning_beam_agent-2_min: 82
    cleaning_beam_agent-3_max: 117
    cleaning_beam_agent-3_mean: 42.97
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 553
    cleaning_beam_agent-4_mean: 435.58
    cleaning_beam_agent-4_min: 191
    cleaning_beam_agent-5_max: 178
    cleaning_beam_agent-5_mean: 55.4
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 13
    fire_beam_agent-4_mean: 0.17
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-53-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 811.9999999999811
  episode_reward_mean: 580.5100000000003
  episode_reward_min: 206.99999999999838
  episodes_this_iter: 96
  episodes_total: 7968
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20134.106
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007687872275710106
        entropy: 1.1693894863128662
        entropy_coeff: 0.0017600000137463212
        kl: 0.009639555588364601
        model: {}
        policy_loss: -0.024402638897299767
        total_loss: -0.023181427270174026
        vf_explained_var: 0.09639245271682739
        vf_loss: 13.514293670654297
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007687872275710106
        entropy: 1.1357145309448242
        entropy_coeff: 0.0017600000137463212
        kl: 0.014384296722710133
        model: {}
        policy_loss: -0.03613882511854172
        total_loss: -0.03379031643271446
        vf_explained_var: 0.018601059913635254
        vf_loss: 14.705060958862305
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007687872275710106
        entropy: 1.204167127609253
        entropy_coeff: 0.0017600000137463212
        kl: 0.01244686171412468
        model: {}
        policy_loss: -0.02966815046966076
        total_loss: -0.027951745316386223
        vf_explained_var: 0.10101862251758575
        vf_loss: 13.463722229003906
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007687872275710106
        entropy: 0.8727110624313354
        entropy_coeff: 0.0017600000137463212
        kl: 0.009340371005237103
        model: {}
        policy_loss: -0.022216610610485077
        total_loss: -0.020624730736017227
        vf_explained_var: 0.15799486637115479
        vf_loss: 12.59781551361084
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007687872275710106
        entropy: 1.0924878120422363
        entropy_coeff: 0.0017600000137463212
        kl: 0.01317921094596386
        model: {}
        policy_loss: -0.03499938175082207
        total_loss: -0.03298071771860123
        vf_explained_var: 0.126325324177742
        vf_loss: 13.056031227111816
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007687872275710106
        entropy: 1.0555260181427002
        entropy_coeff: 0.0017600000137463212
        kl: 0.017450468614697456
        model: {}
        policy_loss: -0.0320378802716732
        total_loss: -0.02916320413351059
        vf_explained_var: 0.17153474688529968
        vf_loss: 12.423126220703125
    load_time_ms: 21694.501
    num_steps_sampled: 7968000
    num_steps_trained: 7968000
    sample_time_ms: 92271.22
    update_time_ms: 32.31
  iterations_since_restore: 23
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.448295454545455
    ram_util_percent: 16.135795454545452
  pid: 24061
  policy_reward_max:
    agent-0: 135.33333333333374
    agent-1: 135.33333333333374
    agent-2: 135.33333333333374
    agent-3: 135.33333333333374
    agent-4: 135.33333333333374
    agent-5: 135.33333333333374
  policy_reward_mean:
    agent-0: 96.75166666666694
    agent-1: 96.75166666666694
    agent-2: 96.75166666666694
    agent-3: 96.75166666666694
    agent-4: 96.75166666666694
    agent-5: 96.75166666666694
  policy_reward_min:
    agent-0: 34.50000000000001
    agent-1: 34.50000000000001
    agent-2: 34.50000000000001
    agent-3: 34.50000000000001
    agent-4: 34.50000000000001
    agent-5: 34.50000000000001
  sampler_perf:
    mean_env_wait_ms: 24.437344092603084
    mean_inference_ms: 12.447152738396744
    mean_processing_ms: 51.25867413293773
  time_since_restore: 3594.6011679172516
  time_this_iter_s: 123.68960809707642
  time_total_s: 12720.612981796265
  timestamp: 1637027584
  timesteps_since_restore: 2208000
  timesteps_this_iter: 96000
  timesteps_total: 7968000
  training_iteration: 83
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |     83 |          12720.6 | 7968000 |   580.51 |
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 39
    apples_agent-0_mean: 6.12
    apples_agent-0_min: 0
    apples_agent-1_max: 239
    apples_agent-1_mean: 31.72
    apples_agent-1_min: 0
    apples_agent-2_max: 145
    apples_agent-2_mean: 12.22
    apples_agent-2_min: 0
    apples_agent-3_max: 169
    apples_agent-3_mean: 89.34
    apples_agent-3_min: 22
    apples_agent-4_max: 59
    apples_agent-4_mean: 2.48
    apples_agent-4_min: 0
    apples_agent-5_max: 249
    apples_agent-5_mean: 75.89
    apples_agent-5_min: 29
    cleaning_beam_agent-0_max: 467
    cleaning_beam_agent-0_mean: 285.6
    cleaning_beam_agent-0_min: 144
    cleaning_beam_agent-1_max: 415
    cleaning_beam_agent-1_mean: 242.45
    cleaning_beam_agent-1_min: 120
    cleaning_beam_agent-2_max: 559
    cleaning_beam_agent-2_mean: 342.41
    cleaning_beam_agent-2_min: 72
    cleaning_beam_agent-3_max: 118
    cleaning_beam_agent-3_mean: 47.26
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 725
    cleaning_beam_agent-4_mean: 459.7
    cleaning_beam_agent-4_min: 147
    cleaning_beam_agent-5_max: 210
    cleaning_beam_agent-5_mean: 55.8
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-55-15
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 822.9999999999793
  episode_reward_mean: 580.9400000000003
  episode_reward_min: 176.9999999999999
  episodes_this_iter: 96
  episodes_total: 8064
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20090.213
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007627967861481011
        entropy: 1.2125089168548584
        entropy_coeff: 0.0017600000137463212
        kl: 0.009969774633646011
        model: {}
        policy_loss: -0.02587062306702137
        total_loss: -0.024736888706684113
        vf_explained_var: 0.0889500230550766
        vf_loss: 12.737885475158691
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007627967861481011
        entropy: 1.1360070705413818
        entropy_coeff: 0.0017600000137463212
        kl: 0.01452522724866867
        model: {}
        policy_loss: -0.03593933954834938
        total_loss: -0.03365481644868851
        vf_explained_var: 0.014528229832649231
        vf_loss: 13.788517951965332
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007627967861481011
        entropy: 1.2137346267700195
        entropy_coeff: 0.0017600000137463212
        kl: 0.013292192481458187
        model: {}
        policy_loss: -0.03108065389096737
        total_loss: -0.02932599186897278
        vf_explained_var: 0.11862537264823914
        vf_loss: 12.323959350585938
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007627967861481011
        entropy: 0.8593769073486328
        entropy_coeff: 0.0017600000137463212
        kl: 0.009354031644761562
        model: {}
        policy_loss: -0.0219883993268013
        total_loss: -0.020493831485509872
        vf_explained_var: 0.18670140206813812
        vf_loss: 11.362643241882324
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007627967861481011
        entropy: 1.0653051137924194
        entropy_coeff: 0.0017600000137463212
        kl: 0.013638635165989399
        model: {}
        policy_loss: -0.03413248062133789
        total_loss: -0.03204622492194176
        vf_explained_var: 0.11819863319396973
        vf_loss: 12.334660530090332
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007627967861481011
        entropy: 1.0523256063461304
        entropy_coeff: 0.0017600000137463212
        kl: 0.0129943722859025
        model: {}
        policy_loss: -0.035748764872550964
        total_loss: -0.03380934149026871
        vf_explained_var: 0.14701049029827118
        vf_loss: 11.926421165466309
    load_time_ms: 20336.945
    num_steps_sampled: 8064000
    num_steps_trained: 8064000
    sample_time_ms: 91902.456
    update_time_ms: 26.637
  iterations_since_restore: 24
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 14.759893048128344
    ram_util_percent: 16.158288770053474
  pid: 24061
  policy_reward_max:
    agent-0: 137.1666666666668
    agent-1: 137.1666666666668
    agent-2: 137.1666666666668
    agent-3: 137.1666666666668
    agent-4: 137.1666666666668
    agent-5: 137.1666666666668
  policy_reward_mean:
    agent-0: 96.82333333333361
    agent-1: 96.82333333333361
    agent-2: 96.82333333333361
    agent-3: 96.82333333333361
    agent-4: 96.82333333333361
    agent-5: 96.82333333333361
  policy_reward_min:
    agent-0: 29.50000000000005
    agent-1: 29.50000000000005
    agent-2: 29.50000000000005
    agent-3: 29.50000000000005
    agent-4: 29.50000000000005
    agent-5: 29.50000000000005
  sampler_perf:
    mean_env_wait_ms: 24.419471758290378
    mean_inference_ms: 12.43711324290284
    mean_processing_ms: 51.32009390200532
  time_since_restore: 3725.759135723114
  time_this_iter_s: 131.15796780586243
  time_total_s: 12851.770949602127
  timestamp: 1637027715
  timesteps_since_restore: 2304000
  timesteps_this_iter: 96000
  timesteps_total: 8064000
  training_iteration: 84
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |     84 |          12851.8 | 8064000 |   580.94 |
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 54
    apples_agent-0_mean: 8.14
    apples_agent-0_min: 0
    apples_agent-1_max: 114
    apples_agent-1_mean: 26.44
    apples_agent-1_min: 0
    apples_agent-2_max: 190
    apples_agent-2_mean: 15.19
    apples_agent-2_min: 0
    apples_agent-3_max: 171
    apples_agent-3_mean: 96.4
    apples_agent-3_min: 23
    apples_agent-4_max: 85
    apples_agent-4_mean: 5.27
    apples_agent-4_min: 0
    apples_agent-5_max: 134
    apples_agent-5_mean: 72.87
    apples_agent-5_min: 26
    cleaning_beam_agent-0_max: 441
    cleaning_beam_agent-0_mean: 244.67
    cleaning_beam_agent-0_min: 135
    cleaning_beam_agent-1_max: 481
    cleaning_beam_agent-1_mean: 261.58
    cleaning_beam_agent-1_min: 71
    cleaning_beam_agent-2_max: 569
    cleaning_beam_agent-2_mean: 364.11
    cleaning_beam_agent-2_min: 95
    cleaning_beam_agent-3_max: 154
    cleaning_beam_agent-3_mean: 50.14
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 598
    cleaning_beam_agent-4_mean: 433.41
    cleaning_beam_agent-4_min: 128
    cleaning_beam_agent-5_max: 122
    cleaning_beam_agent-5_mean: 51.56
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.06
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 16
    fire_beam_agent-5_mean: 0.19
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-57-21
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 817.9999999999699
  episode_reward_mean: 587.3300000000008
  episode_reward_min: 214.99999999999733
  episodes_this_iter: 96
  episodes_total: 8160
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20077.176
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007568064029328525
        entropy: 1.227211594581604
        entropy_coeff: 0.0017600000137463212
        kl: 0.011716961860656738
        model: {}
        policy_loss: -0.025498831644654274
        total_loss: -0.02407614141702652
        vf_explained_var: 0.08184505999088287
        vf_loss: 12.391951560974121
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007568064029328525
        entropy: 1.161872386932373
        entropy_coeff: 0.0017600000137463212
        kl: 0.015333711169660091
        model: {}
        policy_loss: -0.03430819511413574
        total_loss: -0.031976792961359024
        vf_explained_var: 0.02884387969970703
        vf_loss: 13.095550537109375
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007568064029328525
        entropy: 1.1636929512023926
        entropy_coeff: 0.0017600000137463212
        kl: 0.012724131345748901
        model: {}
        policy_loss: -0.030042504891753197
        total_loss: -0.028293635696172714
        vf_explained_var: 0.07153421640396118
        vf_loss: 12.521427154541016
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007568064029328525
        entropy: 0.8592671751976013
        entropy_coeff: 0.0017600000137463212
        kl: 0.009074266068637371
        model: {}
        policy_loss: -0.022061169147491455
        total_loss: -0.020622778683900833
        vf_explained_var: 0.15794864296913147
        vf_loss: 11.358461380004883
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007568064029328525
        entropy: 1.09551203250885
        entropy_coeff: 0.0017600000137463212
        kl: 0.0155949080362916
        model: {}
        policy_loss: -0.03324481472373009
        total_loss: -0.030852191150188446
        vf_explained_var: 0.10933856666088104
        vf_loss: 12.01740837097168
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007568064029328525
        entropy: 1.0142126083374023
        entropy_coeff: 0.0017600000137463212
        kl: 0.01304832473397255
        model: {}
        policy_loss: -0.035358697175979614
        total_loss: -0.033408865332603455
        vf_explained_var: 0.1658620834350586
        vf_loss: 11.251845359802246
    load_time_ms: 19067.617
    num_steps_sampled: 8160000
    num_steps_trained: 8160000
    sample_time_ms: 91355.879
    update_time_ms: 27.881
  iterations_since_restore: 25
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.314444444444444
    ram_util_percent: 16.10333333333333
  pid: 24061
  policy_reward_max:
    agent-0: 136.33333333333357
    agent-1: 136.33333333333357
    agent-2: 136.33333333333357
    agent-3: 136.33333333333357
    agent-4: 136.33333333333357
    agent-5: 136.33333333333357
  policy_reward_mean:
    agent-0: 97.88833333333358
    agent-1: 97.88833333333358
    agent-2: 97.88833333333358
    agent-3: 97.88833333333358
    agent-4: 97.88833333333358
    agent-5: 97.88833333333358
  policy_reward_min:
    agent-0: 35.833333333333364
    agent-1: 35.833333333333364
    agent-2: 35.833333333333364
    agent-3: 35.833333333333364
    agent-4: 35.833333333333364
    agent-5: 35.833333333333364
  sampler_perf:
    mean_env_wait_ms: 24.404570944100772
    mean_inference_ms: 12.425648075807828
    mean_processing_ms: 51.277024432602275
  time_since_restore: 3851.650668144226
  time_this_iter_s: 125.89153242111206
  time_total_s: 12977.66248202324
  timestamp: 1637027841
  timesteps_since_restore: 2400000
  timesteps_this_iter: 96000
  timesteps_total: 8160000
  training_iteration: 85
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |     85 |          12977.7 | 8160000 |   587.33 |
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 50
    apples_agent-0_mean: 7.84
    apples_agent-0_min: 0
    apples_agent-1_max: 93
    apples_agent-1_mean: 25.78
    apples_agent-1_min: 0
    apples_agent-2_max: 171
    apples_agent-2_mean: 15.74
    apples_agent-2_min: 0
    apples_agent-3_max: 183
    apples_agent-3_mean: 93.88
    apples_agent-3_min: 38
    apples_agent-4_max: 45
    apples_agent-4_mean: 2.61
    apples_agent-4_min: 0
    apples_agent-5_max: 134
    apples_agent-5_mean: 72.63
    apples_agent-5_min: 20
    cleaning_beam_agent-0_max: 436
    cleaning_beam_agent-0_mean: 254.91
    cleaning_beam_agent-0_min: 96
    cleaning_beam_agent-1_max: 527
    cleaning_beam_agent-1_mean: 267.77
    cleaning_beam_agent-1_min: 102
    cleaning_beam_agent-2_max: 527
    cleaning_beam_agent-2_mean: 347.32
    cleaning_beam_agent-2_min: 61
    cleaning_beam_agent-3_max: 172
    cleaning_beam_agent-3_mean: 49.07
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 587
    cleaning_beam_agent-4_mean: 444.6
    cleaning_beam_agent-4_min: 145
    cleaning_beam_agent-5_max: 202
    cleaning_beam_agent-5_mean: 51.06
    cleaning_beam_agent-5_min: 16
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.07
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 4
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-59-27
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 848.9999999999887
  episode_reward_mean: 595.0899999999997
  episode_reward_min: 250.9999999999967
  episodes_this_iter: 96
  episodes_total: 8256
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20089.809
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007508160197176039
        entropy: 1.2766412496566772
        entropy_coeff: 0.0017600000137463212
        kl: 0.013588011264801025
        model: {}
        policy_loss: -0.025601305067539215
        total_loss: -0.023682888597249985
        vf_explained_var: 0.060326457023620605
        vf_loss: 14.477006912231445
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007508160197176039
        entropy: 1.1461384296417236
        entropy_coeff: 0.0017600000137463212
        kl: 0.015334690921008587
        model: {}
        policy_loss: -0.03494967892765999
        total_loss: -0.032387278974056244
        vf_explained_var: 0.01814529299736023
        vf_loss: 15.12666130065918
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007508160197176039
        entropy: 1.2008447647094727
        entropy_coeff: 0.0017600000137463212
        kl: 0.013161560520529747
        model: {}
        policy_loss: -0.03208666294813156
        total_loss: -0.030232612043619156
        vf_explained_var: 0.13281138241291046
        vf_loss: 13.35224723815918
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007508160197176039
        entropy: 0.8483838438987732
        entropy_coeff: 0.0017600000137463212
        kl: 0.009887488558888435
        model: {}
        policy_loss: -0.022471372038125992
        total_loss: -0.020746279507875443
        vf_explained_var: 0.19487327337265015
        vf_loss: 12.407493591308594
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007508160197176039
        entropy: 1.0734102725982666
        entropy_coeff: 0.0017600000137463212
        kl: 0.014475883916020393
        model: {}
        policy_loss: -0.03286907449364662
        total_loss: -0.030424918979406357
        vf_explained_var: 0.06587491929531097
        vf_loss: 14.381847381591797
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007508160197176039
        entropy: 1.024875521659851
        entropy_coeff: 0.0017600000137463212
        kl: 0.013927970081567764
        model: {}
        policy_loss: -0.03532418608665466
        total_loss: -0.03307849541306496
        vf_explained_var: 0.17950645089149475
        vf_loss: 12.638779640197754
    load_time_ms: 15902.435
    num_steps_sampled: 8256000
    num_steps_trained: 8256000
    sample_time_ms: 91222.286
    update_time_ms: 27.943
  iterations_since_restore: 26
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.381460674157301
    ram_util_percent: 16.228651685393256
  pid: 24061
  policy_reward_max:
    agent-0: 141.50000000000028
    agent-1: 141.50000000000028
    agent-2: 141.50000000000028
    agent-3: 141.50000000000028
    agent-4: 141.50000000000028
    agent-5: 141.50000000000028
  policy_reward_mean:
    agent-0: 99.18166666666693
    agent-1: 99.18166666666693
    agent-2: 99.18166666666693
    agent-3: 99.18166666666693
    agent-4: 99.18166666666693
    agent-5: 99.18166666666693
  policy_reward_min:
    agent-0: 41.83333333333331
    agent-1: 41.83333333333331
    agent-2: 41.83333333333331
    agent-3: 41.83333333333331
    agent-4: 41.83333333333331
    agent-5: 41.83333333333331
  sampler_perf:
    mean_env_wait_ms: 24.392505400432686
    mean_inference_ms: 12.41441630190963
    mean_processing_ms: 51.24253152816342
  time_since_restore: 3976.3046379089355
  time_this_iter_s: 124.65396976470947
  time_total_s: 13102.316451787949
  timestamp: 1637027967
  timesteps_since_restore: 2496000
  timesteps_this_iter: 96000
  timesteps_total: 8256000
  training_iteration: 86
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 29.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |     86 |          13102.3 | 8256000 |   595.09 |
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 71
    apples_agent-0_mean: 11.24
    apples_agent-0_min: 0
    apples_agent-1_max: 115
    apples_agent-1_mean: 26.52
    apples_agent-1_min: 0
    apples_agent-2_max: 397
    apples_agent-2_mean: 12.97
    apples_agent-2_min: 0
    apples_agent-3_max: 183
    apples_agent-3_mean: 95.53
    apples_agent-3_min: 19
    apples_agent-4_max: 79
    apples_agent-4_mean: 4.71
    apples_agent-4_min: 0
    apples_agent-5_max: 123
    apples_agent-5_mean: 73.52
    apples_agent-5_min: 16
    cleaning_beam_agent-0_max: 461
    cleaning_beam_agent-0_mean: 252.85
    cleaning_beam_agent-0_min: 94
    cleaning_beam_agent-1_max: 500
    cleaning_beam_agent-1_mean: 271.62
    cleaning_beam_agent-1_min: 126
    cleaning_beam_agent-2_max: 655
    cleaning_beam_agent-2_mean: 385.55
    cleaning_beam_agent-2_min: 96
    cleaning_beam_agent-3_max: 205
    cleaning_beam_agent-3_mean: 51.84
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 592
    cleaning_beam_agent-4_mean: 422.17
    cleaning_beam_agent-4_min: 92
    cleaning_beam_agent-5_max: 195
    cleaning_beam_agent-5_mean: 61.28
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-01-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 833.9999999999868
  episode_reward_mean: 585.6899999999987
  episode_reward_min: 160.99999999999957
  episodes_this_iter: 96
  episodes_total: 8352
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20049.049
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007448255782946944
        entropy: 1.2654441595077515
        entropy_coeff: 0.0017600000137463212
        kl: 0.011116355657577515
        model: {}
        policy_loss: -0.027225906029343605
        total_loss: -0.025767797604203224
        vf_explained_var: 0.07251164317131042
        vf_loss: 14.620223999023438
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007448255782946944
        entropy: 1.1706002950668335
        entropy_coeff: 0.0017600000137463212
        kl: 0.0156230004504323
        model: {}
        policy_loss: -0.03542349487543106
        total_loss: -0.03285745158791542
        vf_explained_var: 0.047426655888557434
        vf_loss: 15.016992568969727
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007448255782946944
        entropy: 1.1650418043136597
        entropy_coeff: 0.0017600000137463212
        kl: 0.012790737673640251
        model: {}
        policy_loss: -0.03073994256556034
        total_loss: -0.028808237984776497
        vf_explained_var: 0.09640929102897644
        vf_loss: 14.240297317504883
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007448255782946944
        entropy: 0.8642695546150208
        entropy_coeff: 0.0017600000137463212
        kl: 0.010391062125563622
        model: {}
        policy_loss: -0.023654181510210037
        total_loss: -0.02186065912246704
        vf_explained_var: 0.21565993130207062
        vf_loss: 12.364273071289062
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007448255782946944
        entropy: 1.0865215063095093
        entropy_coeff: 0.0017600000137463212
        kl: 0.014154983684420586
        model: {}
        policy_loss: -0.03603578731417656
        total_loss: -0.03371923044323921
        vf_explained_var: 0.11330978572368622
        vf_loss: 13.978434562683105
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007448255782946944
        entropy: 1.0479631423950195
        entropy_coeff: 0.0017600000137463212
        kl: 0.013572176918387413
        model: {}
        policy_loss: -0.036480389535427094
        total_loss: -0.034326836466789246
        vf_explained_var: 0.18560287356376648
        vf_loss: 12.835318565368652
    load_time_ms: 14592.747
    num_steps_sampled: 8352000
    num_steps_trained: 8352000
    sample_time_ms: 91264.965
    update_time_ms: 27.725
  iterations_since_restore: 27
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.87514124293785
    ram_util_percent: 16.217514124293785
  pid: 24061
  policy_reward_max:
    agent-0: 139.00000000000014
    agent-1: 139.00000000000014
    agent-2: 139.00000000000014
    agent-3: 139.00000000000014
    agent-4: 139.00000000000014
    agent-5: 139.00000000000014
  policy_reward_mean:
    agent-0: 97.61500000000025
    agent-1: 97.61500000000025
    agent-2: 97.61500000000025
    agent-3: 97.61500000000025
    agent-4: 97.61500000000025
    agent-5: 97.61500000000025
  policy_reward_min:
    agent-0: 26.833333333333393
    agent-1: 26.833333333333393
    agent-2: 26.833333333333393
    agent-3: 26.833333333333393
    agent-4: 26.833333333333393
    agent-5: 26.833333333333393
  sampler_perf:
    mean_env_wait_ms: 24.388855603439453
    mean_inference_ms: 12.40748305785657
    mean_processing_ms: 51.21507343386217
  time_since_restore: 4100.90708732605
  time_this_iter_s: 124.60244941711426
  time_total_s: 13226.918901205063
  timestamp: 1637028092
  timesteps_since_restore: 2592000
  timesteps_this_iter: 96000
  timesteps_total: 8352000
  training_iteration: 87
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |     87 |          13226.9 | 8352000 |   585.69 |
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 54
    apples_agent-0_mean: 8.21
    apples_agent-0_min: 0
    apples_agent-1_max: 91
    apples_agent-1_mean: 28.14
    apples_agent-1_min: 0
    apples_agent-2_max: 223
    apples_agent-2_mean: 9.8
    apples_agent-2_min: 0
    apples_agent-3_max: 171
    apples_agent-3_mean: 93.26
    apples_agent-3_min: 18
    apples_agent-4_max: 75
    apples_agent-4_mean: 5.92
    apples_agent-4_min: 0
    apples_agent-5_max: 111
    apples_agent-5_mean: 70.01
    apples_agent-5_min: 10
    cleaning_beam_agent-0_max: 559
    cleaning_beam_agent-0_mean: 269.71
    cleaning_beam_agent-0_min: 145
    cleaning_beam_agent-1_max: 355
    cleaning_beam_agent-1_mean: 232.11
    cleaning_beam_agent-1_min: 86
    cleaning_beam_agent-2_max: 600
    cleaning_beam_agent-2_mean: 379.88
    cleaning_beam_agent-2_min: 113
    cleaning_beam_agent-3_max: 205
    cleaning_beam_agent-3_mean: 54.59
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 581
    cleaning_beam_agent-4_mean: 419.0
    cleaning_beam_agent-4_min: 142
    cleaning_beam_agent-5_max: 197
    cleaning_beam_agent-5_mean: 63.93
    cleaning_beam_agent-5_min: 16
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-03-35
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 804.9999999999733
  episode_reward_mean: 572.5099999999995
  episode_reward_min: 172.9999999999991
  episodes_this_iter: 96
  episodes_total: 8448
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20039.668
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007388351950794458
        entropy: 1.2699830532073975
        entropy_coeff: 0.0017600000137463212
        kl: 0.010618251748383045
        model: {}
        policy_loss: -0.027715617790818214
        total_loss: -0.02636377140879631
        vf_explained_var: 0.0895727127790451
        vf_loss: 14.633676528930664
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007388351950794458
        entropy: 1.1723135709762573
        entropy_coeff: 0.0017600000137463212
        kl: 0.01485401950776577
        model: {}
        policy_loss: -0.03552882745862007
        total_loss: -0.03303871676325798
        vf_explained_var: 0.015740275382995605
        vf_loss: 15.82577896118164
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007388351950794458
        entropy: 1.1560355424880981
        entropy_coeff: 0.0017600000137463212
        kl: 0.012489043176174164
        model: {}
        policy_loss: -0.030386915430426598
        total_loss: -0.02844301238656044
        vf_explained_var: 0.07828634977340698
        vf_loss: 14.807183265686035
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007388351950794458
        entropy: 0.8611223101615906
        entropy_coeff: 0.0017600000137463212
        kl: 0.010125197470188141
        model: {}
        policy_loss: -0.023459868505597115
        total_loss: -0.021688390523195267
        vf_explained_var: 0.21494939923286438
        vf_loss: 12.62014389038086
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007388351950794458
        entropy: 1.0952171087265015
        entropy_coeff: 0.0017600000137463212
        kl: 0.013784604147076607
        model: {}
        policy_loss: -0.037567123770713806
        total_loss: -0.03539540618658066
        vf_explained_var: 0.16494010388851166
        vf_loss: 13.423791885375977
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007388351950794458
        entropy: 1.0369105339050293
        entropy_coeff: 0.0017600000137463212
        kl: 0.013442778028547764
        model: {}
        policy_loss: -0.037512753158807755
        total_loss: -0.03531164303421974
        vf_explained_var: 0.16792090237140656
        vf_loss: 13.375236511230469
    load_time_ms: 14586.132
    num_steps_sampled: 8448000
    num_steps_trained: 8448000
    sample_time_ms: 91189.66
    update_time_ms: 28.365
  iterations_since_restore: 28
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.464204545454544
    ram_util_percent: 16.15284090909091
  pid: 24061
  policy_reward_max:
    agent-0: 134.16666666666714
    agent-1: 134.16666666666714
    agent-2: 134.16666666666714
    agent-3: 134.16666666666714
    agent-4: 134.16666666666714
    agent-5: 134.16666666666714
  policy_reward_mean:
    agent-0: 95.41833333333358
    agent-1: 95.41833333333358
    agent-2: 95.41833333333358
    agent-3: 95.41833333333358
    agent-4: 95.41833333333358
    agent-5: 95.41833333333358
  policy_reward_min:
    agent-0: 28.833333333333375
    agent-1: 28.833333333333375
    agent-2: 28.833333333333375
    agent-3: 28.833333333333375
    agent-4: 28.833333333333375
    agent-5: 28.833333333333375
  sampler_perf:
    mean_env_wait_ms: 24.37219184903191
    mean_inference_ms: 12.396396616596496
    mean_processing_ms: 51.17498307945169
  time_since_restore: 4224.5982048511505
  time_this_iter_s: 123.69111752510071
  time_total_s: 13350.610018730164
  timestamp: 1637028215
  timesteps_since_restore: 2688000
  timesteps_this_iter: 96000
  timesteps_total: 8448000
  training_iteration: 88
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |     88 |          13350.6 | 8448000 |   572.51 |
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 66
    apples_agent-0_mean: 6.15
    apples_agent-0_min: 0
    apples_agent-1_max: 98
    apples_agent-1_mean: 22.65
    apples_agent-1_min: 0
    apples_agent-2_max: 223
    apples_agent-2_mean: 12.09
    apples_agent-2_min: 0
    apples_agent-3_max: 172
    apples_agent-3_mean: 98.44
    apples_agent-3_min: 29
    apples_agent-4_max: 76
    apples_agent-4_mean: 7.02
    apples_agent-4_min: 0
    apples_agent-5_max: 137
    apples_agent-5_mean: 77.4
    apples_agent-5_min: 20
    cleaning_beam_agent-0_max: 585
    cleaning_beam_agent-0_mean: 287.42
    cleaning_beam_agent-0_min: 163
    cleaning_beam_agent-1_max: 416
    cleaning_beam_agent-1_mean: 265.65
    cleaning_beam_agent-1_min: 112
    cleaning_beam_agent-2_max: 622
    cleaning_beam_agent-2_mean: 380.44
    cleaning_beam_agent-2_min: 189
    cleaning_beam_agent-3_max: 151
    cleaning_beam_agent-3_mean: 50.05
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 631
    cleaning_beam_agent-4_mean: 439.29
    cleaning_beam_agent-4_min: 62
    cleaning_beam_agent-5_max: 150
    cleaning_beam_agent-5_mean: 53.87
    cleaning_beam_agent-5_min: 13
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 4
    fire_beam_agent-5_mean: 0.08
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-05-39
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 829.9999999999912
  episode_reward_mean: 611.1999999999985
  episode_reward_min: 202.99999999999676
  episodes_this_iter: 96
  episodes_total: 8544
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20021.425
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007328448118641973
        entropy: 1.2654855251312256
        entropy_coeff: 0.0017600000137463212
        kl: 0.010530219413340092
        model: {}
        policy_loss: -0.026346607133746147
        total_loss: -0.025082379579544067
        vf_explained_var: 0.10866168141365051
        vf_loss: 13.854400634765625
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007328448118641973
        entropy: 1.1770038604736328
        entropy_coeff: 0.0017600000137463212
        kl: 0.015651272609829903
        model: {}
        policy_loss: -0.03516070544719696
        total_loss: -0.03259510174393654
        vf_explained_var: 0.03165818750858307
        vf_loss: 15.068760871887207
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007328448118641973
        entropy: 1.1547718048095703
        entropy_coeff: 0.0017600000137463212
        kl: 0.014075842685997486
        model: {}
        policy_loss: -0.03031376749277115
        total_loss: -0.028095338493585587
        vf_explained_var: 0.07735992968082428
        vf_loss: 14.356572151184082
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007328448118641973
        entropy: 0.8341324925422668
        entropy_coeff: 0.0017600000137463212
        kl: 0.009162240661680698
        model: {}
        policy_loss: -0.02278239279985428
        total_loss: -0.021192321553826332
        vf_explained_var: 0.21155332028865814
        vf_loss: 12.256961822509766
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007328448118641973
        entropy: 1.071120262145996
        entropy_coeff: 0.0017600000137463212
        kl: 0.014337917789816856
        model: {}
        policy_loss: -0.037212539464235306
        total_loss: -0.03485060855746269
        vf_explained_var: 0.1130932867527008
        vf_loss: 13.795145988464355
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007328448118641973
        entropy: 1.017256736755371
        entropy_coeff: 0.0017600000137463212
        kl: 0.013774015009403229
        model: {}
        policy_loss: -0.03588653355836868
        total_loss: -0.033665839582681656
        vf_explained_var: 0.1926354169845581
        vf_loss: 12.562628746032715
    load_time_ms: 14155.103
    num_steps_sampled: 8544000
    num_steps_trained: 8544000
    sample_time_ms: 91037.771
    update_time_ms: 29.295
  iterations_since_restore: 29
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 17.276271186440677
    ram_util_percent: 16.30564971751412
  pid: 24061
  policy_reward_max:
    agent-0: 138.3333333333335
    agent-1: 138.3333333333335
    agent-2: 138.3333333333335
    agent-3: 138.3333333333335
    agent-4: 138.3333333333335
    agent-5: 138.3333333333335
  policy_reward_mean:
    agent-0: 101.86666666666696
    agent-1: 101.86666666666696
    agent-2: 101.86666666666696
    agent-3: 101.86666666666696
    agent-4: 101.86666666666696
    agent-5: 101.86666666666696
  policy_reward_min:
    agent-0: 33.83333333333339
    agent-1: 33.83333333333339
    agent-2: 33.83333333333339
    agent-3: 33.83333333333339
    agent-4: 33.83333333333339
    agent-5: 33.83333333333339
  sampler_perf:
    mean_env_wait_ms: 24.374471584418984
    mean_inference_ms: 12.389071055940931
    mean_processing_ms: 51.148551276606526
  time_since_restore: 4348.41579413414
  time_this_iter_s: 123.8175892829895
  time_total_s: 13474.427608013153
  timestamp: 1637028339
  timesteps_since_restore: 2784000
  timesteps_this_iter: 96000
  timesteps_total: 8544000
  training_iteration: 89
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |     89 |          13474.4 | 8544000 |    611.2 |
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 64
    apples_agent-0_mean: 8.73
    apples_agent-0_min: 0
    apples_agent-1_max: 126
    apples_agent-1_mean: 28.56
    apples_agent-1_min: 0
    apples_agent-2_max: 211
    apples_agent-2_mean: 11.11
    apples_agent-2_min: 0
    apples_agent-3_max: 178
    apples_agent-3_mean: 95.35
    apples_agent-3_min: 23
    apples_agent-4_max: 57
    apples_agent-4_mean: 5.2
    apples_agent-4_min: 0
    apples_agent-5_max: 205
    apples_agent-5_mean: 77.24
    apples_agent-5_min: 23
    cleaning_beam_agent-0_max: 548
    cleaning_beam_agent-0_mean: 283.86
    cleaning_beam_agent-0_min: 124
    cleaning_beam_agent-1_max: 419
    cleaning_beam_agent-1_mean: 243.35
    cleaning_beam_agent-1_min: 99
    cleaning_beam_agent-2_max: 622
    cleaning_beam_agent-2_mean: 428.8
    cleaning_beam_agent-2_min: 140
    cleaning_beam_agent-3_max: 257
    cleaning_beam_agent-3_mean: 57.51
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 633
    cleaning_beam_agent-4_mean: 455.89
    cleaning_beam_agent-4_min: 149
    cleaning_beam_agent-5_max: 150
    cleaning_beam_agent-5_mean: 60.22
    cleaning_beam_agent-5_min: 12
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.06
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-07-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 932.9999999999797
  episode_reward_mean: 605.6799999999982
  episode_reward_min: 274.9999999999972
  episodes_this_iter: 96
  episodes_total: 8640
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19968.675
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007268544286489487
        entropy: 1.2850168943405151
        entropy_coeff: 0.0017600000137463212
        kl: 0.010785362683236599
        model: {}
        policy_loss: -0.027267757803201675
        total_loss: -0.025889409705996513
        vf_explained_var: 0.1075604259967804
        vf_loss: 14.82902717590332
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007268544286489487
        entropy: 1.1715892553329468
        entropy_coeff: 0.0017600000137463212
        kl: 0.015456080436706543
        model: {}
        policy_loss: -0.03572812303900719
        total_loss: -0.03309702128171921
        vf_explained_var: 0.03533118963241577
        vf_loss: 16.018808364868164
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007268544286489487
        entropy: 1.125699520111084
        entropy_coeff: 0.0017600000137463212
        kl: 0.013476564548909664
        model: {}
        policy_loss: -0.0326618030667305
        total_loss: -0.0304473377764225
        vf_explained_var: 0.0969487875699997
        vf_loss: 15.003811836242676
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007268544286489487
        entropy: 0.8674545884132385
        entropy_coeff: 0.0017600000137463212
        kl: 0.009686172939836979
        model: {}
        policy_loss: -0.02447449415922165
        total_loss: -0.022726932540535927
        vf_explained_var: 0.19404499232769012
        vf_loss: 13.370464324951172
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007268544286489487
        entropy: 1.072972297668457
        entropy_coeff: 0.0017600000137463212
        kl: 0.01376679353415966
        model: {}
        policy_loss: -0.036353543400764465
        total_loss: -0.03395400196313858
        vf_explained_var: 0.07607634365558624
        vf_loss: 15.346064567565918
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007268544286489487
        entropy: 1.0160844326019287
        entropy_coeff: 0.0017600000137463212
        kl: 0.013449995778501034
        model: {}
        policy_loss: -0.036837637424468994
        total_loss: -0.03456754982471466
        vf_explained_var: 0.17568299174308777
        vf_loss: 13.684006690979004
    load_time_ms: 13792.162
    num_steps_sampled: 8640000
    num_steps_trained: 8640000
    sample_time_ms: 91376.205
    update_time_ms: 29.122
  iterations_since_restore: 30
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 19.151666666666667
    ram_util_percent: 16.599444444444444
  pid: 24061
  policy_reward_max:
    agent-0: 155.50000000000009
    agent-1: 155.50000000000009
    agent-2: 155.50000000000009
    agent-3: 155.50000000000009
    agent-4: 155.50000000000009
    agent-5: 155.50000000000009
  policy_reward_mean:
    agent-0: 100.9466666666669
    agent-1: 100.9466666666669
    agent-2: 100.9466666666669
    agent-3: 100.9466666666669
    agent-4: 100.9466666666669
    agent-5: 100.9466666666669
  policy_reward_min:
    agent-0: 45.83333333333322
    agent-1: 45.83333333333322
    agent-2: 45.83333333333322
    agent-3: 45.83333333333322
    agent-4: 45.83333333333322
    agent-5: 45.83333333333322
  sampler_perf:
    mean_env_wait_ms: 24.391869214107142
    mean_inference_ms: 12.388130793761404
    mean_processing_ms: 51.15067772642539
  time_since_restore: 4474.332715988159
  time_this_iter_s: 125.91692185401917
  time_total_s: 13600.344529867172
  timestamp: 1637028465
  timesteps_since_restore: 2880000
  timesteps_this_iter: 96000
  timesteps_total: 8640000
  training_iteration: 90
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 30.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |     90 |          13600.3 | 8640000 |   605.68 |
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 50
    apples_agent-0_mean: 7.06
    apples_agent-0_min: 0
    apples_agent-1_max: 146
    apples_agent-1_mean: 24.82
    apples_agent-1_min: 0
    apples_agent-2_max: 88
    apples_agent-2_mean: 10.61
    apples_agent-2_min: 0
    apples_agent-3_max: 172
    apples_agent-3_mean: 100.69
    apples_agent-3_min: 40
    apples_agent-4_max: 70
    apples_agent-4_mean: 3.55
    apples_agent-4_min: 0
    apples_agent-5_max: 172
    apples_agent-5_mean: 80.63
    apples_agent-5_min: 24
    cleaning_beam_agent-0_max: 546
    cleaning_beam_agent-0_mean: 263.92
    cleaning_beam_agent-0_min: 91
    cleaning_beam_agent-1_max: 391
    cleaning_beam_agent-1_mean: 260.31
    cleaning_beam_agent-1_min: 111
    cleaning_beam_agent-2_max: 679
    cleaning_beam_agent-2_mean: 430.28
    cleaning_beam_agent-2_min: 148
    cleaning_beam_agent-3_max: 207
    cleaning_beam_agent-3_mean: 53.48
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 662
    cleaning_beam_agent-4_mean: 447.45
    cleaning_beam_agent-4_min: 212
    cleaning_beam_agent-5_max: 213
    cleaning_beam_agent-5_mean: 61.43
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-09-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 864.9999999999751
  episode_reward_mean: 607.1899999999983
  episode_reward_min: 277.99999999999966
  episodes_this_iter: 96
  episodes_total: 8736
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19946.146
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007208639872260392
        entropy: 1.3215160369873047
        entropy_coeff: 0.0017600000137463212
        kl: 0.011090127751231194
        model: {}
        policy_loss: -0.02905387245118618
        total_loss: -0.027825884521007538
        vf_explained_var: 0.06183737516403198
        vf_loss: 13.358309745788574
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007208639872260392
        entropy: 1.16896653175354
        entropy_coeff: 0.0017600000137463212
        kl: 0.015361253172159195
        model: {}
        policy_loss: -0.03678528591990471
        total_loss: -0.03439893200993538
        vf_explained_var: 0.03678523004055023
        vf_loss: 13.71484375
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007208639872260392
        entropy: 1.1148438453674316
        entropy_coeff: 0.0017600000137463212
        kl: 0.01444464735686779
        model: {}
        policy_loss: -0.031146522611379623
        total_loss: -0.02892213687300682
        vf_explained_var: 0.08843645453453064
        vf_loss: 12.975810050964355
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007208639872260392
        entropy: 0.8711346387863159
        entropy_coeff: 0.0017600000137463212
        kl: 0.012969331815838814
        model: {}
        policy_loss: -0.022423334419727325
        total_loss: -0.020232506096363068
        vf_explained_var: 0.2056645303964615
        vf_loss: 11.301557540893555
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007208639872260392
        entropy: 1.0896011590957642
        entropy_coeff: 0.0017600000137463212
        kl: 0.014817693270742893
        model: {}
        policy_loss: -0.035695530474185944
        total_loss: -0.033391840755939484
        vf_explained_var: 0.1162429004907608
        vf_loss: 12.578519821166992
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007208639872260392
        entropy: 1.0135008096694946
        entropy_coeff: 0.0017600000137463212
        kl: 0.013788975775241852
        model: {}
        policy_loss: -0.037747833877801895
        total_loss: -0.035515278577804565
        vf_explained_var: 0.11602559685707092
        vf_loss: 12.585199356079102
    load_time_ms: 13779.529
    num_steps_sampled: 8736000
    num_steps_trained: 8736000
    sample_time_ms: 91599.296
    update_time_ms: 28.711
  iterations_since_restore: 31
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 19.230898876404495
    ram_util_percent: 16.652808988764043
  pid: 24061
  policy_reward_max:
    agent-0: 144.16666666666706
    agent-1: 144.16666666666706
    agent-2: 144.16666666666706
    agent-3: 144.16666666666706
    agent-4: 144.16666666666706
    agent-5: 144.16666666666706
  policy_reward_mean:
    agent-0: 101.19833333333364
    agent-1: 101.19833333333364
    agent-2: 101.19833333333364
    agent-3: 101.19833333333364
    agent-4: 101.19833333333364
    agent-5: 101.19833333333364
  policy_reward_min:
    agent-0: 46.33333333333322
    agent-1: 46.33333333333322
    agent-2: 46.33333333333322
    agent-3: 46.33333333333322
    agent-4: 46.33333333333322
    agent-5: 46.33333333333322
  sampler_perf:
    mean_env_wait_ms: 24.40933457416343
    mean_inference_ms: 12.387229817320033
    mean_processing_ms: 51.15246553357947
  time_since_restore: 4599.743844509125
  time_this_iter_s: 125.41112852096558
  time_total_s: 13725.755658388138
  timestamp: 1637028591
  timesteps_since_restore: 2976000
  timesteps_this_iter: 96000
  timesteps_total: 8736000
  training_iteration: 91
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |     91 |          13725.8 | 8736000 |   607.19 |
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 44
    apples_agent-0_mean: 7.3
    apples_agent-0_min: 0
    apples_agent-1_max: 94
    apples_agent-1_mean: 25.89
    apples_agent-1_min: 0
    apples_agent-2_max: 59
    apples_agent-2_mean: 6.46
    apples_agent-2_min: 0
    apples_agent-3_max: 188
    apples_agent-3_mean: 99.35
    apples_agent-3_min: 26
    apples_agent-4_max: 70
    apples_agent-4_mean: 3.53
    apples_agent-4_min: 0
    apples_agent-5_max: 146
    apples_agent-5_mean: 81.35
    apples_agent-5_min: 10
    cleaning_beam_agent-0_max: 453
    cleaning_beam_agent-0_mean: 293.43
    cleaning_beam_agent-0_min: 119
    cleaning_beam_agent-1_max: 504
    cleaning_beam_agent-1_mean: 265.04
    cleaning_beam_agent-1_min: 149
    cleaning_beam_agent-2_max: 697
    cleaning_beam_agent-2_mean: 451.04
    cleaning_beam_agent-2_min: 179
    cleaning_beam_agent-3_max: 182
    cleaning_beam_agent-3_mean: 49.32
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 631
    cleaning_beam_agent-4_mean: 446.22
    cleaning_beam_agent-4_min: 162
    cleaning_beam_agent-5_max: 152
    cleaning_beam_agent-5_mean: 54.78
    cleaning_beam_agent-5_min: 16
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-11-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 845.9999999999922
  episode_reward_mean: 619.6599999999976
  episode_reward_min: 240.99999999999667
  episodes_this_iter: 96
  episodes_total: 8832
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19897.745
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007148736040107906
        entropy: 1.294999122619629
        entropy_coeff: 0.0017600000137463212
        kl: 0.01163944136351347
        model: {}
        policy_loss: -0.02804652415215969
        total_loss: -0.02657218649983406
        vf_explained_var: 0.09160400927066803
        vf_loss: 14.256462097167969
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007148736040107906
        entropy: 1.1609442234039307
        entropy_coeff: 0.0017600000137463212
        kl: 0.016924668103456497
        model: {}
        policy_loss: -0.03636835888028145
        total_loss: -0.033499836921691895
        vf_explained_var: 0.026393011212348938
        vf_loss: 15.268508911132812
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007148736040107906
        entropy: 1.0970805883407593
        entropy_coeff: 0.0017600000137463212
        kl: 0.013586646877229214
        model: {}
        policy_loss: -0.030783677473664284
        total_loss: -0.028492901474237442
        vf_explained_var: 0.04111291468143463
        vf_loss: 15.043058395385742
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007148736040107906
        entropy: 0.8410055041313171
        entropy_coeff: 0.0017600000137463212
        kl: 0.01030697114765644
        model: {}
        policy_loss: -0.02450665459036827
        total_loss: -0.022719260305166245
        vf_explained_var: 0.23104259371757507
        vf_loss: 12.06167221069336
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007148736040107906
        entropy: 1.0769634246826172
        entropy_coeff: 0.0017600000137463212
        kl: 0.013741211965680122
        model: {}
        policy_loss: -0.03619561716914177
        total_loss: -0.03392639011144638
        vf_explained_var: 0.09801779687404633
        vf_loss: 14.164437294006348
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007148736040107906
        entropy: 0.9877161383628845
        entropy_coeff: 0.0017600000137463212
        kl: 0.014384277164936066
        model: {}
        policy_loss: -0.03600659221410751
        total_loss: -0.033607691526412964
        vf_explained_var: 0.1975422203540802
        vf_loss: 12.604276657104492
    load_time_ms: 13626.543
    num_steps_sampled: 8832000
    num_steps_trained: 8832000
    sample_time_ms: 91764.818
    update_time_ms: 28.628
  iterations_since_restore: 32
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 19.2
    ram_util_percent: 16.67709497206704
  pid: 24061
  policy_reward_max:
    agent-0: 141.00000000000014
    agent-1: 141.00000000000014
    agent-2: 141.00000000000014
    agent-3: 141.00000000000014
    agent-4: 141.00000000000014
    agent-5: 141.00000000000014
  policy_reward_mean:
    agent-0: 103.27666666666693
    agent-1: 103.27666666666693
    agent-2: 103.27666666666693
    agent-3: 103.27666666666693
    agent-4: 103.27666666666693
    agent-5: 103.27666666666693
  policy_reward_min:
    agent-0: 40.166666666666664
    agent-1: 40.166666666666664
    agent-2: 40.166666666666664
    agent-3: 40.166666666666664
    agent-4: 40.166666666666664
    agent-5: 40.166666666666664
  sampler_perf:
    mean_env_wait_ms: 24.4286599656653
    mean_inference_ms: 12.385081950812094
    mean_processing_ms: 51.15501938946938
  time_since_restore: 4725.240163803101
  time_this_iter_s: 125.49631929397583
  time_total_s: 13851.251977682114
  timestamp: 1637028717
  timesteps_since_restore: 3072000
  timesteps_this_iter: 96000
  timesteps_total: 8832000
  training_iteration: 92
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |     92 |          13851.3 | 8832000 |   619.66 |
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 83
    apples_agent-0_mean: 7.62
    apples_agent-0_min: 0
    apples_agent-1_max: 94
    apples_agent-1_mean: 28.27
    apples_agent-1_min: 0
    apples_agent-2_max: 87
    apples_agent-2_mean: 4.99
    apples_agent-2_min: 0
    apples_agent-3_max: 163
    apples_agent-3_mean: 93.39
    apples_agent-3_min: 13
    apples_agent-4_max: 81
    apples_agent-4_mean: 4.11
    apples_agent-4_min: 0
    apples_agent-5_max: 133
    apples_agent-5_mean: 80.44
    apples_agent-5_min: 29
    cleaning_beam_agent-0_max: 446
    cleaning_beam_agent-0_mean: 276.34
    cleaning_beam_agent-0_min: 94
    cleaning_beam_agent-1_max: 447
    cleaning_beam_agent-1_mean: 240.97
    cleaning_beam_agent-1_min: 137
    cleaning_beam_agent-2_max: 643
    cleaning_beam_agent-2_mean: 449.74
    cleaning_beam_agent-2_min: 179
    cleaning_beam_agent-3_max: 198
    cleaning_beam_agent-3_mean: 49.8
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 605
    cleaning_beam_agent-4_mean: 451.67
    cleaning_beam_agent-4_min: 169
    cleaning_beam_agent-5_max: 165
    cleaning_beam_agent-5_mean: 62.87
    cleaning_beam_agent-5_min: 19
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.08
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-14-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 851.9999999999948
  episode_reward_mean: 595.2599999999993
  episode_reward_min: 132.00000000000077
  episodes_this_iter: 96
  episodes_total: 8928
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19891.802
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000708883220795542
        entropy: 1.3103069067001343
        entropy_coeff: 0.0017600000137463212
        kl: 0.01082280371338129
        model: {}
        policy_loss: -0.028520021587610245
        total_loss: -0.027174681425094604
        vf_explained_var: 0.08245791494846344
        vf_loss: 14.869237899780273
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000708883220795542
        entropy: 1.1683509349822998
        entropy_coeff: 0.0017600000137463212
        kl: 0.01504112221300602
        model: {}
        policy_loss: -0.03810986876487732
        total_loss: -0.03555796295404434
        vf_explained_var: 0.012347251176834106
        vf_loss: 15.99972152709961
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000708883220795542
        entropy: 1.101739525794983
        entropy_coeff: 0.0017600000137463212
        kl: 0.01280893012881279
        model: {}
        policy_loss: -0.03084607981145382
        total_loss: -0.02868122234940529
        vf_explained_var: 0.04797528684139252
        vf_loss: 15.421354293823242
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000708883220795542
        entropy: 0.8648668527603149
        entropy_coeff: 0.0017600000137463212
        kl: 0.010980229824781418
        model: {}
        policy_loss: -0.02598022297024727
        total_loss: -0.02404354140162468
        vf_explained_var: 0.22045844793319702
        vf_loss: 12.628011703491211
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000708883220795542
        entropy: 1.0632667541503906
        entropy_coeff: 0.0017600000137463212
        kl: 0.014534232206642628
        model: {}
        policy_loss: -0.03718862310051918
        total_loss: -0.0347462072968483
        vf_explained_var: 0.13019004464149475
        vf_loss: 14.069201469421387
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000708883220795542
        entropy: 1.0050485134124756
        entropy_coeff: 0.0017600000137463212
        kl: 0.013680290430784225
        model: {}
        policy_loss: -0.037480294704437256
        total_loss: -0.03519071266055107
        vf_explained_var: 0.1831805258989334
        vf_loss: 13.224115371704102
    load_time_ms: 13792.404
    num_steps_sampled: 8928000
    num_steps_trained: 8928000
    sample_time_ms: 97373.168
    update_time_ms: 27.225
  iterations_since_restore: 33
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 18.975842696629215
    ram_util_percent: 16.706179775280898
  pid: 24061
  policy_reward_max:
    agent-0: 142.0000000000003
    agent-1: 142.0000000000003
    agent-2: 142.0000000000003
    agent-3: 142.0000000000003
    agent-4: 142.0000000000003
    agent-5: 142.0000000000003
  policy_reward_mean:
    agent-0: 99.21000000000028
    agent-1: 99.21000000000028
    agent-2: 99.21000000000028
    agent-3: 99.21000000000028
    agent-4: 99.21000000000028
    agent-5: 99.21000000000028
  policy_reward_min:
    agent-0: 22.00000000000003
    agent-1: 22.00000000000003
    agent-2: 22.00000000000003
    agent-3: 22.00000000000003
    agent-4: 22.00000000000003
    agent-5: 22.00000000000003
  sampler_perf:
    mean_env_wait_ms: 24.445286241586327
    mean_inference_ms: 12.385174271092488
    mean_processing_ms: 51.16037170090529
  time_since_restore: 4906.595091819763
  time_this_iter_s: 181.3549280166626
  time_total_s: 14032.606905698776
  timestamp: 1637028898
  timesteps_since_restore: 3168000
  timesteps_this_iter: 96000
  timesteps_total: 8928000
  training_iteration: 93
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 30.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |     93 |          14032.6 | 8928000 |   595.26 |
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 63
    apples_agent-0_mean: 8.35
    apples_agent-0_min: 0
    apples_agent-1_max: 88
    apples_agent-1_mean: 31.84
    apples_agent-1_min: 0
    apples_agent-2_max: 167
    apples_agent-2_mean: 10.14
    apples_agent-2_min: 0
    apples_agent-3_max: 193
    apples_agent-3_mean: 100.28
    apples_agent-3_min: 39
    apples_agent-4_max: 57
    apples_agent-4_mean: 3.4
    apples_agent-4_min: 0
    apples_agent-5_max: 156
    apples_agent-5_mean: 80.44
    apples_agent-5_min: 18
    cleaning_beam_agent-0_max: 432
    cleaning_beam_agent-0_mean: 266.88
    cleaning_beam_agent-0_min: 107
    cleaning_beam_agent-1_max: 495
    cleaning_beam_agent-1_mean: 224.5
    cleaning_beam_agent-1_min: 73
    cleaning_beam_agent-2_max: 666
    cleaning_beam_agent-2_mean: 436.5
    cleaning_beam_agent-2_min: 175
    cleaning_beam_agent-3_max: 150
    cleaning_beam_agent-3_mean: 48.85
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 636
    cleaning_beam_agent-4_mean: 453.98
    cleaning_beam_agent-4_min: 194
    cleaning_beam_agent-5_max: 207
    cleaning_beam_agent-5_mean: 62.16
    cleaning_beam_agent-5_min: 14
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 19
    fire_beam_agent-5_mean: 0.2
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-17-05
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 878.9999999999875
  episode_reward_mean: 599.2899999999988
  episode_reward_min: 183.9999999999967
  episodes_this_iter: 96
  episodes_total: 9024
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19914.378
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007028927793726325
        entropy: 1.3056507110595703
        entropy_coeff: 0.0017600000137463212
        kl: 0.011199478060007095
        model: {}
        policy_loss: -0.02492441236972809
        total_loss: -0.0232885479927063
        vf_explained_var: 0.04504173994064331
        vf_loss: 16.939159393310547
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007028927793726325
        entropy: 1.151401162147522
        entropy_coeff: 0.0017600000137463212
        kl: 0.014403820037841797
        model: {}
        policy_loss: -0.03431675583124161
        total_loss: -0.03173278272151947
        vf_explained_var: 0.025586366653442383
        vf_loss: 17.296810150146484
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007028927793726325
        entropy: 1.1060137748718262
        entropy_coeff: 0.0017600000137463212
        kl: 0.01347849890589714
        model: {}
        policy_loss: -0.028674528002738953
        total_loss: -0.026252372190356255
        vf_explained_var: 0.05724121630191803
        vf_loss: 16.730424880981445
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007028927793726325
        entropy: 0.8731931447982788
        entropy_coeff: 0.0017600000137463212
        kl: 0.009970898739993572
        model: {}
        policy_loss: -0.023429173976182938
        total_loss: -0.02148646116256714
        vf_explained_var: 0.16504758596420288
        vf_loss: 14.853529930114746
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007028927793726325
        entropy: 1.0481956005096436
        entropy_coeff: 0.0017600000137463212
        kl: 0.01316016260534525
        model: {}
        policy_loss: -0.03350433334708214
        total_loss: -0.03107459843158722
        vf_explained_var: 0.07354049384593964
        vf_loss: 16.42526626586914
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007028927793726325
        entropy: 1.0221593379974365
        entropy_coeff: 0.0017600000137463212
        kl: 0.014123069122433662
        model: {}
        policy_loss: -0.037126798182725906
        total_loss: -0.03461940959095955
        vf_explained_var: 0.16282784938812256
        vf_loss: 14.817768096923828
    load_time_ms: 13768.005
    num_steps_sampled: 9024000
    num_steps_trained: 9024000
    sample_time_ms: 96975.819
    update_time_ms: 28.445
  iterations_since_restore: 34
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 19.047777777777778
    ram_util_percent: 16.683888888888887
  pid: 24061
  policy_reward_max:
    agent-0: 146.49999999999994
    agent-1: 146.49999999999994
    agent-2: 146.49999999999994
    agent-3: 146.49999999999994
    agent-4: 146.49999999999994
    agent-5: 146.49999999999994
  policy_reward_mean:
    agent-0: 99.88166666666693
    agent-1: 99.88166666666693
    agent-2: 99.88166666666693
    agent-3: 99.88166666666693
    agent-4: 99.88166666666693
    agent-5: 99.88166666666693
  policy_reward_min:
    agent-0: 30.66666666666677
    agent-1: 30.66666666666677
    agent-2: 30.66666666666677
    agent-3: 30.66666666666677
    agent-4: 30.66666666666677
    agent-5: 30.66666666666677
  sampler_perf:
    mean_env_wait_ms: 24.455420940062695
    mean_inference_ms: 12.384257742106957
    mean_processing_ms: 51.16323060938671
  time_since_restore: 5033.596231222153
  time_this_iter_s: 127.00113940238953
  time_total_s: 14159.608045101166
  timestamp: 1637029025
  timesteps_since_restore: 3264000
  timesteps_this_iter: 96000
  timesteps_total: 9024000
  training_iteration: 94
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |     94 |          14159.6 | 9024000 |   599.29 |
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 89
    apples_agent-0_mean: 7.67
    apples_agent-0_min: 0
    apples_agent-1_max: 89
    apples_agent-1_mean: 31.72
    apples_agent-1_min: 0
    apples_agent-2_max: 86
    apples_agent-2_mean: 6.33
    apples_agent-2_min: 0
    apples_agent-3_max: 171
    apples_agent-3_mean: 99.26
    apples_agent-3_min: 23
    apples_agent-4_max: 60
    apples_agent-4_mean: 4.8
    apples_agent-4_min: 0
    apples_agent-5_max: 156
    apples_agent-5_mean: 77.83
    apples_agent-5_min: 20
    cleaning_beam_agent-0_max: 488
    cleaning_beam_agent-0_mean: 246.43
    cleaning_beam_agent-0_min: 90
    cleaning_beam_agent-1_max: 435
    cleaning_beam_agent-1_mean: 229.04
    cleaning_beam_agent-1_min: 126
    cleaning_beam_agent-2_max: 716
    cleaning_beam_agent-2_mean: 440.33
    cleaning_beam_agent-2_min: 181
    cleaning_beam_agent-3_max: 202
    cleaning_beam_agent-3_mean: 51.43
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 649
    cleaning_beam_agent-4_mean: 459.52
    cleaning_beam_agent-4_min: 105
    cleaning_beam_agent-5_max: 215
    cleaning_beam_agent-5_mean: 65.29
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-19-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 842.9999999999886
  episode_reward_mean: 583.9199999999987
  episode_reward_min: 161.0000000000001
  episodes_this_iter: 96
  episodes_total: 9120
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19924.762
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006969023961573839
        entropy: 1.3201441764831543
        entropy_coeff: 0.0017600000137463212
        kl: 0.011804771609604359
        model: {}
        policy_loss: -0.030361179262399673
        total_loss: -0.02883465215563774
        vf_explained_var: 0.06719362735748291
        vf_loss: 14.89030647277832
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006969023961573839
        entropy: 1.152120590209961
        entropy_coeff: 0.0017600000137463212
        kl: 0.015227017924189568
        model: {}
        policy_loss: -0.03793879598379135
        total_loss: -0.03538581356406212
        vf_explained_var: 0.038043245673179626
        vf_loss: 15.353156089782715
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006969023961573839
        entropy: 1.0945767164230347
        entropy_coeff: 0.0017600000137463212
        kl: 0.014075253158807755
        model: {}
        policy_loss: -0.03221976011991501
        total_loss: -0.029850300401449203
        vf_explained_var: 0.07208496332168579
        vf_loss: 14.808660507202148
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006969023961573839
        entropy: 0.8913694620132446
        entropy_coeff: 0.0017600000137463212
        kl: 0.010746736079454422
        model: {}
        policy_loss: -0.026102351024746895
        total_loss: -0.024291403591632843
        vf_explained_var: 0.22922898828983307
        vf_loss: 12.304121971130371
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006969023961573839
        entropy: 1.038469910621643
        entropy_coeff: 0.0017600000137463212
        kl: 0.01454286091029644
        model: {}
        policy_loss: -0.03675750270485878
        total_loss: -0.034292448312044144
        vf_explained_var: 0.13259395956993103
        vf_loss: 13.84189510345459
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006969023961573839
        entropy: 1.015386700630188
        entropy_coeff: 0.0017600000137463212
        kl: 0.013882435858249664
        model: {}
        policy_loss: -0.03893047571182251
        total_loss: -0.03667517751455307
        vf_explained_var: 0.20705446600914001
        vf_loss: 12.658905982971191
    load_time_ms: 13589.113
    num_steps_sampled: 9120000
    num_steps_trained: 9120000
    sample_time_ms: 97160.655
    update_time_ms: 26.186
  iterations_since_restore: 35
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 19.101117318435755
    ram_util_percent: 16.649162011173182
  pid: 24061
  policy_reward_max:
    agent-0: 140.50000000000034
    agent-1: 140.50000000000034
    agent-2: 140.50000000000034
    agent-3: 140.50000000000034
    agent-4: 140.50000000000034
    agent-5: 140.50000000000034
  policy_reward_mean:
    agent-0: 97.32000000000025
    agent-1: 97.32000000000025
    agent-2: 97.32000000000025
    agent-3: 97.32000000000025
    agent-4: 97.32000000000025
    agent-5: 97.32000000000025
  policy_reward_min:
    agent-0: 26.833333333333343
    agent-1: 26.833333333333343
    agent-2: 26.833333333333343
    agent-3: 26.833333333333343
    agent-4: 26.833333333333343
    agent-5: 26.833333333333343
  sampler_perf:
    mean_env_wait_ms: 24.46345766522771
    mean_inference_ms: 12.38352243662208
    mean_processing_ms: 51.163785591517104
  time_since_restore: 5159.603656053543
  time_this_iter_s: 126.00742483139038
  time_total_s: 14285.615469932556
  timestamp: 1637029152
  timesteps_since_restore: 3360000
  timesteps_this_iter: 96000
  timesteps_total: 9120000
  training_iteration: 95
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |     95 |          14285.6 | 9120000 |   583.92 |
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 72
    apples_agent-0_mean: 8.19
    apples_agent-0_min: 0
    apples_agent-1_max: 128
    apples_agent-1_mean: 29.41
    apples_agent-1_min: 0
    apples_agent-2_max: 162
    apples_agent-2_mean: 7.96
    apples_agent-2_min: 0
    apples_agent-3_max: 197
    apples_agent-3_mean: 99.71
    apples_agent-3_min: 24
    apples_agent-4_max: 58
    apples_agent-4_mean: 4.07
    apples_agent-4_min: 0
    apples_agent-5_max: 134
    apples_agent-5_mean: 72.37
    apples_agent-5_min: 16
    cleaning_beam_agent-0_max: 488
    cleaning_beam_agent-0_mean: 276.08
    cleaning_beam_agent-0_min: 96
    cleaning_beam_agent-1_max: 374
    cleaning_beam_agent-1_mean: 218.34
    cleaning_beam_agent-1_min: 69
    cleaning_beam_agent-2_max: 631
    cleaning_beam_agent-2_mean: 426.79
    cleaning_beam_agent-2_min: 107
    cleaning_beam_agent-3_max: 158
    cleaning_beam_agent-3_mean: 45.87
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 609
    cleaning_beam_agent-4_mean: 451.88
    cleaning_beam_agent-4_min: 100
    cleaning_beam_agent-5_max: 237
    cleaning_beam_agent-5_mean: 67.6
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.05
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.12
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 4
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-21-17
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 796.9999999999798
  episode_reward_mean: 592.919999999998
  episode_reward_min: 127.00000000000132
  episodes_this_iter: 96
  episodes_total: 9216
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19896.917
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006909120129421353
        entropy: 1.2767293453216553
        entropy_coeff: 0.0017600000137463212
        kl: 0.01286185160279274
        model: {}
        policy_loss: -0.0295986607670784
        total_loss: -0.02765822783112526
        vf_explained_var: 0.06685036420822144
        vf_loss: 16.151052474975586
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006909120129421353
        entropy: 1.1265449523925781
        entropy_coeff: 0.0017600000137463212
        kl: 0.015497000887989998
        model: {}
        policy_loss: -0.038933273404836655
        total_loss: -0.036088164895772934
        vf_explained_var: 0.0024551749229431152
        vf_loss: 17.2842960357666
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006909120129421353
        entropy: 1.1059458255767822
        entropy_coeff: 0.0017600000137463212
        kl: 0.013057433068752289
        model: {}
        policy_loss: -0.031142305582761765
        total_loss: -0.02889448031783104
        vf_explained_var: 0.08577847480773926
        vf_loss: 15.827999114990234
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006909120129421353
        entropy: 0.8912369608879089
        entropy_coeff: 0.0017600000137463212
        kl: 0.01117473840713501
        model: {}
        policy_loss: -0.027315059676766396
        total_loss: -0.02537066675722599
        vf_explained_var: 0.26170629262924194
        vf_loss: 12.780203819274902
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006909120129421353
        entropy: 1.0413196086883545
        entropy_coeff: 0.0017600000137463212
        kl: 0.014724766835570335
        model: {}
        policy_loss: -0.037513114511966705
        total_loss: -0.034896448254585266
        vf_explained_var: 0.13154014945030212
        vf_loss: 15.04435920715332
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006909120129421353
        entropy: 0.9984531402587891
        entropy_coeff: 0.0017600000137463212
        kl: 0.014029969461262226
        model: {}
        policy_loss: -0.03794608265161514
        total_loss: -0.035502877086400986
        vf_explained_var: 0.19428986310958862
        vf_loss: 13.944879531860352
    load_time_ms: 13499.99
    num_steps_sampled: 9216000
    num_steps_trained: 9216000
    sample_time_ms: 97350.206
    update_time_ms: 25.871
  iterations_since_restore: 36
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 19.184916201117314
    ram_util_percent: 16.650837988826815
  pid: 24061
  policy_reward_max:
    agent-0: 132.83333333333388
    agent-1: 132.83333333333388
    agent-2: 132.83333333333388
    agent-3: 132.83333333333388
    agent-4: 132.83333333333388
    agent-5: 132.83333333333388
  policy_reward_mean:
    agent-0: 98.82000000000029
    agent-1: 98.82000000000029
    agent-2: 98.82000000000029
    agent-3: 98.82000000000029
    agent-4: 98.82000000000029
    agent-5: 98.82000000000029
  policy_reward_min:
    agent-0: 21.166666666666682
    agent-1: 21.166666666666682
    agent-2: 21.166666666666682
    agent-3: 21.166666666666682
    agent-4: 21.166666666666682
    agent-5: 21.166666666666682
  sampler_perf:
    mean_env_wait_ms: 24.476546440311903
    mean_inference_ms: 12.38435870567022
    mean_processing_ms: 51.175418115867785
  time_since_restore: 5285.095815420151
  time_this_iter_s: 125.49215936660767
  time_total_s: 14411.107629299164
  timestamp: 1637029277
  timesteps_since_restore: 3456000
  timesteps_this_iter: 96000
  timesteps_total: 9216000
  training_iteration: 96
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 30.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |     96 |          14411.1 | 9216000 |   592.92 |
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 70
    apples_agent-0_mean: 8.51
    apples_agent-0_min: 0
    apples_agent-1_max: 92
    apples_agent-1_mean: 27.05
    apples_agent-1_min: 0
    apples_agent-2_max: 120
    apples_agent-2_mean: 8.24
    apples_agent-2_min: 0
    apples_agent-3_max: 187
    apples_agent-3_mean: 111.16
    apples_agent-3_min: 34
    apples_agent-4_max: 58
    apples_agent-4_mean: 2.72
    apples_agent-4_min: 0
    apples_agent-5_max: 137
    apples_agent-5_mean: 76.08
    apples_agent-5_min: 18
    cleaning_beam_agent-0_max: 427
    cleaning_beam_agent-0_mean: 272.64
    cleaning_beam_agent-0_min: 117
    cleaning_beam_agent-1_max: 410
    cleaning_beam_agent-1_mean: 244.27
    cleaning_beam_agent-1_min: 83
    cleaning_beam_agent-2_max: 669
    cleaning_beam_agent-2_mean: 436.75
    cleaning_beam_agent-2_min: 167
    cleaning_beam_agent-3_max: 142
    cleaning_beam_agent-3_mean: 46.28
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 631
    cleaning_beam_agent-4_mean: 473.01
    cleaning_beam_agent-4_min: 190
    cleaning_beam_agent-5_max: 150
    cleaning_beam_agent-5_mean: 65.34
    cleaning_beam_agent-5_min: 16
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.09
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-23-22
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 852.9999999999908
  episode_reward_mean: 610.0999999999983
  episode_reward_min: 254.99999999999585
  episodes_this_iter: 96
  episodes_total: 9312
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19907.953
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006849215715192258
        entropy: 1.288848876953125
        entropy_coeff: 0.0017600000137463212
        kl: 0.011684665456414223
        model: {}
        policy_loss: -0.029552115127444267
        total_loss: -0.027988897636532784
        vf_explained_var: 0.07274721562862396
        vf_loss: 14.94660758972168
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006849215715192258
        entropy: 1.1291316747665405
        entropy_coeff: 0.0017600000137463212
        kl: 0.014897680841386318
        model: {}
        policy_loss: -0.037554096430540085
        total_loss: -0.03499744459986687
        vf_explained_var: 0.02945461869239807
        vf_loss: 15.643832206726074
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006849215715192258
        entropy: 1.090815782546997
        entropy_coeff: 0.0017600000137463212
        kl: 0.014132006093859673
        model: {}
        policy_loss: -0.029363790526986122
        total_loss: -0.02696295827627182
        vf_explained_var: 0.07251213490962982
        vf_loss: 14.942689895629883
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006849215715192258
        entropy: 0.8736138343811035
        entropy_coeff: 0.0017600000137463212
        kl: 0.01006912998855114
        model: {}
        policy_loss: -0.025936966761946678
        total_loss: -0.02416623756289482
        vf_explained_var: 0.1970462054014206
        vf_loss: 12.94462776184082
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006849215715192258
        entropy: 1.0194435119628906
        entropy_coeff: 0.0017600000137463212
        kl: 0.014848417602479458
        model: {}
        policy_loss: -0.036055393517017365
        total_loss: -0.03341052681207657
        vf_explained_var: 0.08794067800045013
        vf_loss: 14.694052696228027
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006849215715192258
        entropy: 1.0011725425720215
        entropy_coeff: 0.0017600000137463212
        kl: 0.014505143277347088
        model: {}
        policy_loss: -0.03665424883365631
        total_loss: -0.03418569266796112
        vf_explained_var: 0.17489756643772125
        vf_loss: 13.295857429504395
    load_time_ms: 13438.946
    num_steps_sampled: 9312000
    num_steps_trained: 9312000
    sample_time_ms: 97417.617
    update_time_ms: 25.872
  iterations_since_restore: 37
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 19.120224719101124
    ram_util_percent: 16.684831460674154
  pid: 24061
  policy_reward_max:
    agent-0: 142.16666666666694
    agent-1: 142.16666666666694
    agent-2: 142.16666666666694
    agent-3: 142.16666666666694
    agent-4: 142.16666666666694
    agent-5: 142.16666666666694
  policy_reward_mean:
    agent-0: 101.6833333333336
    agent-1: 101.6833333333336
    agent-2: 101.6833333333336
    agent-3: 101.6833333333336
    agent-4: 101.6833333333336
    agent-5: 101.6833333333336
  policy_reward_min:
    agent-0: 42.50000000000001
    agent-1: 42.50000000000001
    agent-2: 42.50000000000001
    agent-3: 42.50000000000001
    agent-4: 42.50000000000001
    agent-5: 42.50000000000001
  sampler_perf:
    mean_env_wait_ms: 24.48845156575248
    mean_inference_ms: 12.382771387587434
    mean_processing_ms: 51.16803530189659
  time_since_restore: 5409.9737911224365
  time_this_iter_s: 124.87797570228577
  time_total_s: 14535.98560500145
  timestamp: 1637029402
  timesteps_since_restore: 3552000
  timesteps_this_iter: 96000
  timesteps_total: 9312000
  training_iteration: 97
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |     97 |            14536 | 9312000 |    610.1 |
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 67
    apples_agent-0_mean: 8.07
    apples_agent-0_min: 0
    apples_agent-1_max: 138
    apples_agent-1_mean: 30.05
    apples_agent-1_min: 0
    apples_agent-2_max: 175
    apples_agent-2_mean: 10.13
    apples_agent-2_min: 0
    apples_agent-3_max: 181
    apples_agent-3_mean: 111.77
    apples_agent-3_min: 34
    apples_agent-4_max: 81
    apples_agent-4_mean: 2.44
    apples_agent-4_min: 0
    apples_agent-5_max: 169
    apples_agent-5_mean: 84.37
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 495
    cleaning_beam_agent-0_mean: 256.41
    cleaning_beam_agent-0_min: 135
    cleaning_beam_agent-1_max: 470
    cleaning_beam_agent-1_mean: 237.51
    cleaning_beam_agent-1_min: 72
    cleaning_beam_agent-2_max: 619
    cleaning_beam_agent-2_mean: 449.59
    cleaning_beam_agent-2_min: 203
    cleaning_beam_agent-3_max: 155
    cleaning_beam_agent-3_mean: 48.12
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 604
    cleaning_beam_agent-4_mean: 464.42
    cleaning_beam_agent-4_min: 145
    cleaning_beam_agent-5_max: 143
    cleaning_beam_agent-5_mean: 63.98
    cleaning_beam_agent-5_min: 14
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-25-28
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 839.9999999999874
  episode_reward_mean: 613.5499999999981
  episode_reward_min: 280.99999999999704
  episodes_this_iter: 96
  episodes_total: 9408
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19890.224
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006789311883039773
        entropy: 1.3208911418914795
        entropy_coeff: 0.0017600000137463212
        kl: 0.011962220072746277
        model: {}
        policy_loss: -0.0284181647002697
        total_loss: -0.027047976851463318
        vf_explained_var: 0.0660395622253418
        vf_loss: 13.025068283081055
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006789311883039773
        entropy: 1.1500914096832275
        entropy_coeff: 0.0017600000137463212
        kl: 0.015522769652307034
        model: {}
        policy_loss: -0.03793042153120041
        total_loss: -0.03549298271536827
        vf_explained_var: 0.028049036860466003
        vf_loss: 13.570399284362793
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006789311883039773
        entropy: 1.0713802576065063
        entropy_coeff: 0.0017600000137463212
        kl: 0.014024769887328148
        model: {}
        policy_loss: -0.0329289473593235
        total_loss: -0.030694976449012756
        vf_explained_var: 0.056897714734077454
        vf_loss: 13.146434783935547
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006789311883039773
        entropy: 0.8824077844619751
        entropy_coeff: 0.0017600000137463212
        kl: 0.012326369062066078
        model: {}
        policy_loss: -0.022670304402709007
        total_loss: -0.020586378872394562
        vf_explained_var: 0.15973691642284393
        vf_loss: 11.716904640197754
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006789311883039773
        entropy: 1.0125603675842285
        entropy_coeff: 0.0017600000137463212
        kl: 0.014080408029258251
        model: {}
        policy_loss: -0.035588204860687256
        total_loss: -0.03331223875284195
        vf_explained_var: 0.1092253178358078
        vf_loss: 12.419954299926758
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006789311883039773
        entropy: 1.0061702728271484
        entropy_coeff: 0.0017600000137463212
        kl: 0.014851273037493229
        model: {}
        policy_loss: -0.03786202520132065
        total_loss: -0.03544045239686966
        vf_explained_var: 0.12321436405181885
        vf_loss: 12.22176742553711
    load_time_ms: 13505.594
    num_steps_sampled: 9408000
    num_steps_trained: 9408000
    sample_time_ms: 97541.667
    update_time_ms: 37.839
  iterations_since_restore: 38
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 19.08715083798883
    ram_util_percent: 16.60558659217877
  pid: 24061
  policy_reward_max:
    agent-0: 140.00000000000043
    agent-1: 140.00000000000043
    agent-2: 140.00000000000043
    agent-3: 140.00000000000043
    agent-4: 140.00000000000043
    agent-5: 140.00000000000043
  policy_reward_mean:
    agent-0: 102.25833333333365
    agent-1: 102.25833333333365
    agent-2: 102.25833333333365
    agent-3: 102.25833333333365
    agent-4: 102.25833333333365
    agent-5: 102.25833333333365
  policy_reward_min:
    agent-0: 46.83333333333326
    agent-1: 46.83333333333326
    agent-2: 46.83333333333326
    agent-3: 46.83333333333326
    agent-4: 46.83333333333326
    agent-5: 46.83333333333326
  sampler_perf:
    mean_env_wait_ms: 24.49604062405585
    mean_inference_ms: 12.38116242985476
    mean_processing_ms: 51.16333248141045
  time_since_restore: 5535.49983549118
  time_this_iter_s: 125.5260443687439
  time_total_s: 14661.511649370193
  timestamp: 1637029528
  timesteps_since_restore: 3648000
  timesteps_this_iter: 96000
  timesteps_total: 9408000
  training_iteration: 98
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |     98 |          14661.5 | 9408000 |   613.55 |
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 247
    apples_agent-0_mean: 10.13
    apples_agent-0_min: 0
    apples_agent-1_max: 105
    apples_agent-1_mean: 27.39
    apples_agent-1_min: 0
    apples_agent-2_max: 147
    apples_agent-2_mean: 7.39
    apples_agent-2_min: 0
    apples_agent-3_max: 183
    apples_agent-3_mean: 113.46
    apples_agent-3_min: 54
    apples_agent-4_max: 41
    apples_agent-4_mean: 1.97
    apples_agent-4_min: 0
    apples_agent-5_max: 145
    apples_agent-5_mean: 79.74
    apples_agent-5_min: 24
    cleaning_beam_agent-0_max: 497
    cleaning_beam_agent-0_mean: 258.37
    cleaning_beam_agent-0_min: 129
    cleaning_beam_agent-1_max: 487
    cleaning_beam_agent-1_mean: 249.07
    cleaning_beam_agent-1_min: 94
    cleaning_beam_agent-2_max: 682
    cleaning_beam_agent-2_mean: 464.91
    cleaning_beam_agent-2_min: 142
    cleaning_beam_agent-3_max: 119
    cleaning_beam_agent-3_mean: 46.69
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 604
    cleaning_beam_agent-4_mean: 460.67
    cleaning_beam_agent-4_min: 213
    cleaning_beam_agent-5_max: 147
    cleaning_beam_agent-5_mean: 67.11
    cleaning_beam_agent-5_min: 14
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-27-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 832.9999999999895
  episode_reward_mean: 630.3599999999974
  episode_reward_min: 278.9999999999976
  episodes_this_iter: 96
  episodes_total: 9504
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19872.386
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006729408050887287
        entropy: 1.2983973026275635
        entropy_coeff: 0.0017600000137463212
        kl: 0.012326359748840332
        model: {}
        policy_loss: -0.03076080046594143
        total_loss: -0.029235757887363434
        vf_explained_var: 0.03998635709285736
        vf_loss: 13.449501991271973
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006729408050887287
        entropy: 1.1301145553588867
        entropy_coeff: 0.0017600000137463212
        kl: 0.015519934706389904
        model: {}
        policy_loss: -0.03724339231848717
        total_loss: -0.03476234897971153
        vf_explained_var: 0.023311004042625427
        vf_loss: 13.660584449768066
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006729408050887287
        entropy: 1.0545333623886108
        entropy_coeff: 0.0017600000137463212
        kl: 0.013179592788219452
        model: {}
        policy_loss: -0.03060276433825493
        total_loss: -0.028517285361886024
        vf_explained_var: 0.06560812890529633
        vf_loss: 13.055327415466309
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006729408050887287
        entropy: 0.8826251029968262
        entropy_coeff: 0.0017600000137463212
        kl: 0.012369494885206223
        model: {}
        policy_loss: -0.02499529719352722
        total_loss: -0.02292689122259617
        vf_explained_var: 0.17977274954319
        vf_loss: 11.479267120361328
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006729408050887287
        entropy: 1.035980224609375
        entropy_coeff: 0.0017600000137463212
        kl: 0.015538433566689491
        model: {}
        policy_loss: -0.035365428775548935
        total_loss: -0.03284056484699249
        vf_explained_var: 0.11314204335212708
        vf_loss: 12.405019760131836
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006729408050887287
        entropy: 0.99082350730896
        entropy_coeff: 0.0017600000137463212
        kl: 0.013503091409802437
        model: {}
        policy_loss: -0.03616414591670036
        total_loss: -0.034008998423814774
        vf_explained_var: 0.1431857943534851
        vf_loss: 11.983774185180664
    load_time_ms: 13529.262
    num_steps_sampled: 9504000
    num_steps_trained: 9504000
    sample_time_ms: 97680.361
    update_time_ms: 42.565
  iterations_since_restore: 39
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 19.298882681564248
    ram_util_percent: 16.665363128491617
  pid: 24061
  policy_reward_max:
    agent-0: 138.83333333333366
    agent-1: 138.83333333333366
    agent-2: 138.83333333333366
    agent-3: 138.83333333333366
    agent-4: 138.83333333333366
    agent-5: 138.83333333333366
  policy_reward_mean:
    agent-0: 105.06000000000034
    agent-1: 105.06000000000034
    agent-2: 105.06000000000034
    agent-3: 105.06000000000034
    agent-4: 105.06000000000034
    agent-5: 105.06000000000034
  policy_reward_min:
    agent-0: 46.4999999999999
    agent-1: 46.4999999999999
    agent-2: 46.4999999999999
    agent-3: 46.4999999999999
    agent-4: 46.4999999999999
    agent-5: 46.4999999999999
  sampler_perf:
    mean_env_wait_ms: 24.509909179029528
    mean_inference_ms: 12.381864715728382
    mean_processing_ms: 51.16378584429985
  time_since_restore: 5661.082515478134
  time_this_iter_s: 125.58267998695374
  time_total_s: 14787.094329357147
  timestamp: 1637029654
  timesteps_since_restore: 3744000
  timesteps_this_iter: 96000
  timesteps_total: 9504000
  training_iteration: 99
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |     99 |          14787.1 | 9504000 |   630.36 |
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 247
    apples_agent-0_mean: 9.17
    apples_agent-0_min: 0
    apples_agent-1_max: 166
    apples_agent-1_mean: 32.92
    apples_agent-1_min: 0
    apples_agent-2_max: 158
    apples_agent-2_mean: 7.16
    apples_agent-2_min: 0
    apples_agent-3_max: 177
    apples_agent-3_mean: 105.61
    apples_agent-3_min: 28
    apples_agent-4_max: 41
    apples_agent-4_mean: 1.72
    apples_agent-4_min: 0
    apples_agent-5_max: 144
    apples_agent-5_mean: 80.66
    apples_agent-5_min: 25
    cleaning_beam_agent-0_max: 460
    cleaning_beam_agent-0_mean: 278.89
    cleaning_beam_agent-0_min: 90
    cleaning_beam_agent-1_max: 397
    cleaning_beam_agent-1_mean: 234.39
    cleaning_beam_agent-1_min: 83
    cleaning_beam_agent-2_max: 713
    cleaning_beam_agent-2_mean: 479.96
    cleaning_beam_agent-2_min: 82
    cleaning_beam_agent-3_max: 202
    cleaning_beam_agent-3_mean: 41.12
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 590
    cleaning_beam_agent-4_mean: 470.47
    cleaning_beam_agent-4_min: 289
    cleaning_beam_agent-5_max: 144
    cleaning_beam_agent-5_mean: 62.75
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 19
    fire_beam_agent-5_mean: 0.23
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-29-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 866.9999999999751
  episode_reward_mean: 620.0899999999986
  episode_reward_min: 243.99999999999693
  episodes_this_iter: 96
  episodes_total: 9600
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19906.168
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006669504218734801
        entropy: 1.3037545680999756
        entropy_coeff: 0.0017600000137463212
        kl: 0.012159893289208412
        model: {}
        policy_loss: -0.031237449496984482
        total_loss: -0.02966279536485672
        vf_explained_var: 0.028660938143730164
        vf_loss: 14.372875213623047
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006669504218734801
        entropy: 1.1494749784469604
        entropy_coeff: 0.0017600000137463212
        kl: 0.015262464992702007
        model: {}
        policy_loss: -0.037766046822071075
        total_loss: -0.03530725836753845
        vf_explained_var: 0.03379061818122864
        vf_loss: 14.293697357177734
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006669504218734801
        entropy: 1.0170060396194458
        entropy_coeff: 0.0017600000137463212
        kl: 0.013572799041867256
        model: {}
        policy_loss: -0.03009505197405815
        total_loss: -0.027775902301073074
        vf_explained_var: 0.05851040780544281
        vf_loss: 13.945228576660156
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006669504218734801
        entropy: 0.8664543628692627
        entropy_coeff: 0.0017600000137463212
        kl: 0.010008193552494049
        model: {}
        policy_loss: -0.0257413350045681
        total_loss: -0.02400907129049301
        vf_explained_var: 0.15087555348873138
        vf_loss: 12.555815696716309
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006669504218734801
        entropy: 1.0104269981384277
        entropy_coeff: 0.0017600000137463212
        kl: 0.014521182514727116
        model: {}
        policy_loss: -0.03732521831989288
        total_loss: -0.03488454595208168
        vf_explained_var: 0.11091090738773346
        vf_loss: 13.147915840148926
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006669504218734801
        entropy: 0.9854962825775146
        entropy_coeff: 0.0017600000137463212
        kl: 0.013677665032446384
        model: {}
        policy_loss: -0.03635580092668533
        total_loss: -0.03412730619311333
        vf_explained_var: 0.17014186084270477
        vf_loss: 12.27437973022461
    load_time_ms: 13520.055
    num_steps_sampled: 9600000
    num_steps_trained: 9600000
    sample_time_ms: 97644.884
    update_time_ms: 42.266
  iterations_since_restore: 40
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 19.25921787709497
    ram_util_percent: 16.65195530726257
  pid: 24061
  policy_reward_max:
    agent-0: 144.5000000000002
    agent-1: 144.5000000000002
    agent-2: 144.5000000000002
    agent-3: 144.5000000000002
    agent-4: 144.5000000000002
    agent-5: 144.5000000000002
  policy_reward_mean:
    agent-0: 103.34833333333361
    agent-1: 103.34833333333361
    agent-2: 103.34833333333361
    agent-3: 103.34833333333361
    agent-4: 103.34833333333361
    agent-5: 103.34833333333361
  policy_reward_min:
    agent-0: 40.66666666666664
    agent-1: 40.66666666666664
    agent-2: 40.66666666666664
    agent-3: 40.66666666666664
    agent-4: 40.66666666666664
    agent-5: 40.66666666666664
  sampler_perf:
    mean_env_wait_ms: 24.527309075320574
    mean_inference_ms: 12.380461483790718
    mean_processing_ms: 51.17273307788906
  time_since_restore: 5786.860854148865
  time_this_iter_s: 125.77833867073059
  time_total_s: 14912.872668027878
  timestamp: 1637029780
  timesteps_since_restore: 3840000
  timesteps_this_iter: 96000
  timesteps_total: 9600000
  training_iteration: 100
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    100 |          14912.9 | 9600000 |   620.09 |
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 81
    apples_agent-0_mean: 6.12
    apples_agent-0_min: 0
    apples_agent-1_max: 93
    apples_agent-1_mean: 30.51
    apples_agent-1_min: 0
    apples_agent-2_max: 218
    apples_agent-2_mean: 9.77
    apples_agent-2_min: 0
    apples_agent-3_max: 195
    apples_agent-3_mean: 107.96
    apples_agent-3_min: 28
    apples_agent-4_max: 54
    apples_agent-4_mean: 2.49
    apples_agent-4_min: 0
    apples_agent-5_max: 140
    apples_agent-5_mean: 74.99
    apples_agent-5_min: 9
    cleaning_beam_agent-0_max: 430
    cleaning_beam_agent-0_mean: 259.2
    cleaning_beam_agent-0_min: 119
    cleaning_beam_agent-1_max: 402
    cleaning_beam_agent-1_mean: 239.86
    cleaning_beam_agent-1_min: 94
    cleaning_beam_agent-2_max: 681
    cleaning_beam_agent-2_mean: 479.44
    cleaning_beam_agent-2_min: 195
    cleaning_beam_agent-3_max: 174
    cleaning_beam_agent-3_mean: 43.92
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 602
    cleaning_beam_agent-4_mean: 458.72
    cleaning_beam_agent-4_min: 184
    cleaning_beam_agent-5_max: 224
    cleaning_beam_agent-5_mean: 66.51
    cleaning_beam_agent-5_min: 18
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-31-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 829.9999999999799
  episode_reward_mean: 612.3699999999975
  episode_reward_min: 249.99999999999832
  episodes_this_iter: 96
  episodes_total: 9696
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19862.255
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006609599804505706
        entropy: 1.3288465738296509
        entropy_coeff: 0.0017600000137463212
        kl: 0.011994936503469944
        model: {}
        policy_loss: -0.03145704045891762
        total_loss: -0.02993154153227806
        vf_explained_var: 0.07845263183116913
        vf_loss: 14.652822494506836
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006609599804505706
        entropy: 1.1271374225616455
        entropy_coeff: 0.0017600000137463212
        kl: 0.016080304980278015
        model: {}
        policy_loss: -0.03913392871618271
        total_loss: -0.036308955401182175
        vf_explained_var: -0.001622900366783142
        vf_loss: 15.926735877990723
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006609599804505706
        entropy: 1.018707275390625
        entropy_coeff: 0.0017600000137463212
        kl: 0.014052232727408409
        model: {}
        policy_loss: -0.029878046363592148
        total_loss: -0.02741183340549469
        vf_explained_var: 0.08870546519756317
        vf_loss: 14.486883163452148
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006609599804505706
        entropy: 0.8606197834014893
        entropy_coeff: 0.0017600000137463212
        kl: 0.009901297278702259
        model: {}
        policy_loss: -0.026235433295369148
        total_loss: -0.02448182739317417
        vf_explained_var: 0.18959841132164001
        vf_loss: 12.880300521850586
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006609599804505706
        entropy: 1.0211988687515259
        entropy_coeff: 0.0017600000137463212
        kl: 0.01480819471180439
        model: {}
        policy_loss: -0.036529604345560074
        total_loss: -0.03394379839301109
        vf_explained_var: 0.10592304170131683
        vf_loss: 14.214753150939941
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006609599804505706
        entropy: 0.961499035358429
        entropy_coeff: 0.0017600000137463212
        kl: 0.013584215193986893
        model: {}
        policy_loss: -0.03657755255699158
        total_loss: -0.034271448850631714
        vf_explained_var: 0.1935468316078186
        vf_loss: 12.81497573852539
    load_time_ms: 13523.63
    num_steps_sampled: 9696000
    num_steps_trained: 9696000
    sample_time_ms: 97779.762
    update_time_ms: 42.272
  iterations_since_restore: 41
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 19.58166666666667
    ram_util_percent: 16.696666666666665
  pid: 24061
  policy_reward_max:
    agent-0: 138.33333333333348
    agent-1: 138.33333333333348
    agent-2: 138.33333333333348
    agent-3: 138.33333333333348
    agent-4: 138.33333333333348
    agent-5: 138.33333333333348
  policy_reward_mean:
    agent-0: 102.06166666666694
    agent-1: 102.06166666666694
    agent-2: 102.06166666666694
    agent-3: 102.06166666666694
    agent-4: 102.06166666666694
    agent-5: 102.06166666666694
  policy_reward_min:
    agent-0: 41.666666666666615
    agent-1: 41.666666666666615
    agent-2: 41.666666666666615
    agent-3: 41.666666666666615
    agent-4: 41.666666666666615
    agent-5: 41.666666666666615
  sampler_perf:
    mean_env_wait_ms: 24.54561843959917
    mean_inference_ms: 12.38307187939283
    mean_processing_ms: 51.185599276281145
  time_since_restore: 5913.267784833908
  time_this_iter_s: 126.40693068504333
  time_total_s: 15039.279598712921
  timestamp: 1637029907
  timesteps_since_restore: 3936000
  timesteps_this_iter: 96000
  timesteps_total: 9696000
  training_iteration: 101
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    101 |          15039.3 | 9696000 |   612.37 |
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 112
    apples_agent-0_mean: 8.08
    apples_agent-0_min: 0
    apples_agent-1_max: 110
    apples_agent-1_mean: 29.58
    apples_agent-1_min: 0
    apples_agent-2_max: 63
    apples_agent-2_mean: 4.05
    apples_agent-2_min: 0
    apples_agent-3_max: 200
    apples_agent-3_mean: 108.66
    apples_agent-3_min: 44
    apples_agent-4_max: 112
    apples_agent-4_mean: 4.95
    apples_agent-4_min: 0
    apples_agent-5_max: 137
    apples_agent-5_mean: 72.1
    apples_agent-5_min: 19
    cleaning_beam_agent-0_max: 513
    cleaning_beam_agent-0_mean: 261.16
    cleaning_beam_agent-0_min: 69
    cleaning_beam_agent-1_max: 370
    cleaning_beam_agent-1_mean: 237.65
    cleaning_beam_agent-1_min: 137
    cleaning_beam_agent-2_max: 739
    cleaning_beam_agent-2_mean: 472.44
    cleaning_beam_agent-2_min: 215
    cleaning_beam_agent-3_max: 164
    cleaning_beam_agent-3_mean: 39.91
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 633
    cleaning_beam_agent-4_mean: 451.13
    cleaning_beam_agent-4_min: 178
    cleaning_beam_agent-5_max: 214
    cleaning_beam_agent-5_mean: 69.14
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-33-52
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 800.9999999999837
  episode_reward_mean: 601.6899999999988
  episode_reward_min: 297.9999999999986
  episodes_this_iter: 96
  episodes_total: 9792
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19839.012
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000654969597235322
        entropy: 1.3190040588378906
        entropy_coeff: 0.0017600000137463212
        kl: 0.012105739675462246
        model: {}
        policy_loss: -0.03143216669559479
        total_loss: -0.029894333332777023
        vf_explained_var: 0.05495542287826538
        vf_loss: 14.38132095336914
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000654969597235322
        entropy: 1.1493680477142334
        entropy_coeff: 0.0017600000137463212
        kl: 0.0156321469694376
        model: {}
        policy_loss: -0.03900608792901039
        total_loss: -0.03642607480287552
        vf_explained_var: 0.030059814453125
        vf_loss: 14.764716148376465
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000654969597235322
        entropy: 1.023248314857483
        entropy_coeff: 0.0017600000137463212
        kl: 0.013162950053811073
        model: {}
        policy_loss: -0.02942127361893654
        total_loss: -0.027231678366661072
        vf_explained_var: 0.1077570915222168
        vf_loss: 13.579242706298828
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000654969597235322
        entropy: 0.8678622245788574
        entropy_coeff: 0.0017600000137463212
        kl: 0.01114292349666357
        model: {}
        policy_loss: -0.02630648761987686
        total_loss: -0.02437867969274521
        vf_explained_var: 0.1947755217552185
        vf_loss: 12.266609191894531
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000654969597235322
        entropy: 1.0222495794296265
        entropy_coeff: 0.0017600000137463212
        kl: 0.014451914466917515
        model: {}
        policy_loss: -0.036359600722789764
        total_loss: -0.03391736373305321
        vf_explained_var: 0.11238227784633636
        vf_loss: 13.510122299194336
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000654969597235322
        entropy: 0.9750761985778809
        entropy_coeff: 0.0017600000137463212
        kl: 0.01363346353173256
        model: {}
        policy_loss: -0.036327797919511795
        total_loss: -0.03406374156475067
        vf_explained_var: 0.1761254072189331
        vf_loss: 12.53493881225586
    load_time_ms: 13492.896
    num_steps_sampled: 9792000
    num_steps_trained: 9792000
    sample_time_ms: 97823.655
    update_time_ms: 41.875
  iterations_since_restore: 42
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 19.092178770949722
    ram_util_percent: 16.720111731843573
  pid: 24061
  policy_reward_max:
    agent-0: 133.50000000000037
    agent-1: 133.50000000000037
    agent-2: 133.50000000000037
    agent-3: 133.50000000000037
    agent-4: 133.50000000000037
    agent-5: 133.50000000000037
  policy_reward_mean:
    agent-0: 100.28166666666695
    agent-1: 100.28166666666695
    agent-2: 100.28166666666695
    agent-3: 100.28166666666695
    agent-4: 100.28166666666695
    agent-5: 100.28166666666695
  policy_reward_min:
    agent-0: 49.66666666666658
    agent-1: 49.66666666666658
    agent-2: 49.66666666666658
    agent-3: 49.66666666666658
    agent-4: 49.66666666666658
    agent-5: 49.66666666666658
  sampler_perf:
    mean_env_wait_ms: 24.556131301556032
    mean_inference_ms: 12.382248148992685
    mean_processing_ms: 51.18150019163542
  time_since_restore: 6038.640292167664
  time_this_iter_s: 125.3725073337555
  time_total_s: 15164.652106046677
  timestamp: 1637030032
  timesteps_since_restore: 4032000
  timesteps_this_iter: 96000
  timesteps_total: 9792000
  training_iteration: 102
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    102 |          15164.7 | 9792000 |   601.69 |
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 49
    apples_agent-0_mean: 6.74
    apples_agent-0_min: 0
    apples_agent-1_max: 90
    apples_agent-1_mean: 32.74
    apples_agent-1_min: 0
    apples_agent-2_max: 149
    apples_agent-2_mean: 6.25
    apples_agent-2_min: 0
    apples_agent-3_max: 187
    apples_agent-3_mean: 104.69
    apples_agent-3_min: 29
    apples_agent-4_max: 90
    apples_agent-4_mean: 3.1
    apples_agent-4_min: 0
    apples_agent-5_max: 148
    apples_agent-5_mean: 76.95
    apples_agent-5_min: 20
    cleaning_beam_agent-0_max: 478
    cleaning_beam_agent-0_mean: 260.77
    cleaning_beam_agent-0_min: 107
    cleaning_beam_agent-1_max: 370
    cleaning_beam_agent-1_mean: 232.23
    cleaning_beam_agent-1_min: 87
    cleaning_beam_agent-2_max: 734
    cleaning_beam_agent-2_mean: 468.76
    cleaning_beam_agent-2_min: 184
    cleaning_beam_agent-3_max: 111
    cleaning_beam_agent-3_mean: 38.85
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 661
    cleaning_beam_agent-4_mean: 466.4
    cleaning_beam_agent-4_min: 150
    cleaning_beam_agent-5_max: 243
    cleaning_beam_agent-5_mean: 65.58
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-35-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 793.999999999981
  episode_reward_mean: 612.9799999999972
  episode_reward_min: 242.9999999999961
  episodes_this_iter: 96
  episodes_total: 9888
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19784.947
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006489792140200734
        entropy: 1.316978096961975
        entropy_coeff: 0.0017600000137463212
        kl: 0.012427074834704399
        model: {}
        policy_loss: -0.029875045642256737
        total_loss: -0.02824636548757553
        vf_explained_var: 0.05872257053852081
        vf_loss: 14.611488342285156
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006489792140200734
        entropy: 1.1278358697891235
        entropy_coeff: 0.0017600000137463212
        kl: 0.015358025208115578
        model: {}
        policy_loss: -0.03939082846045494
        total_loss: -0.03681086003780365
        vf_explained_var: 0.03725375235080719
        vf_loss: 14.933499336242676
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006489792140200734
        entropy: 1.0119738578796387
        entropy_coeff: 0.0017600000137463212
        kl: 0.012058662250638008
        model: {}
        policy_loss: -0.030011646449565887
        total_loss: -0.027996046468615532
        vf_explained_var: 0.1074099987745285
        vf_loss: 13.849431037902832
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006489792140200734
        entropy: 0.8502017855644226
        entropy_coeff: 0.0017600000137463212
        kl: 0.01030717883259058
        model: {}
        policy_loss: -0.025925830006599426
        total_loss: -0.024078959599137306
        vf_explained_var: 0.17432448267936707
        vf_loss: 12.817853927612305
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006489792140200734
        entropy: 1.0070462226867676
        entropy_coeff: 0.0017600000137463212
        kl: 0.013474015519022942
        model: {}
        policy_loss: -0.036041196435689926
        total_loss: -0.033747125416994095
        vf_explained_var: 0.11620509624481201
        vf_loss: 13.716678619384766
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006489792140200734
        entropy: 0.9550913572311401
        entropy_coeff: 0.0017600000137463212
        kl: 0.01299362163990736
        model: {}
        policy_loss: -0.03536483645439148
        total_loss: -0.03318808600306511
        vf_explained_var: 0.18818272650241852
        vf_loss: 12.589860916137695
    load_time_ms: 13319.207
    num_steps_sampled: 9888000
    num_steps_trained: 9888000
    sample_time_ms: 92341.221
    update_time_ms: 42.034
  iterations_since_restore: 43
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 19.251977401129942
    ram_util_percent: 16.666666666666668
  pid: 24061
  policy_reward_max:
    agent-0: 132.33333333333377
    agent-1: 132.33333333333377
    agent-2: 132.33333333333377
    agent-3: 132.33333333333377
    agent-4: 132.33333333333377
    agent-5: 132.33333333333377
  policy_reward_mean:
    agent-0: 102.16333333333365
    agent-1: 102.16333333333365
    agent-2: 102.16333333333365
    agent-3: 102.16333333333365
    agent-4: 102.16333333333365
    agent-5: 102.16333333333365
  policy_reward_min:
    agent-0: 40.50000000000001
    agent-1: 40.50000000000001
    agent-2: 40.50000000000001
    agent-3: 40.50000000000001
    agent-4: 40.50000000000001
    agent-5: 40.50000000000001
  sampler_perf:
    mean_env_wait_ms: 24.566942982889728
    mean_inference_ms: 12.382393931260797
    mean_processing_ms: 51.181909920732444
  time_since_restore: 6162.905193090439
  time_this_iter_s: 124.26490092277527
  time_total_s: 15288.917006969452
  timestamp: 1637030157
  timesteps_since_restore: 4128000
  timesteps_this_iter: 96000
  timesteps_total: 9888000
  training_iteration: 103
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    103 |          15288.9 | 9888000 |   612.98 |
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 49
    apples_agent-0_mean: 5.61
    apples_agent-0_min: 0
    apples_agent-1_max: 108
    apples_agent-1_mean: 31.51
    apples_agent-1_min: 0
    apples_agent-2_max: 147
    apples_agent-2_mean: 6.42
    apples_agent-2_min: 0
    apples_agent-3_max: 174
    apples_agent-3_mean: 108.58
    apples_agent-3_min: 46
    apples_agent-4_max: 43
    apples_agent-4_mean: 1.41
    apples_agent-4_min: 0
    apples_agent-5_max: 136
    apples_agent-5_mean: 80.98
    apples_agent-5_min: 33
    cleaning_beam_agent-0_max: 484
    cleaning_beam_agent-0_mean: 271.03
    cleaning_beam_agent-0_min: 118
    cleaning_beam_agent-1_max: 365
    cleaning_beam_agent-1_mean: 220.83
    cleaning_beam_agent-1_min: 23
    cleaning_beam_agent-2_max: 662
    cleaning_beam_agent-2_mean: 473.28
    cleaning_beam_agent-2_min: 163
    cleaning_beam_agent-3_max: 96
    cleaning_beam_agent-3_mean: 34.54
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 611
    cleaning_beam_agent-4_mean: 474.62
    cleaning_beam_agent-4_min: 227
    cleaning_beam_agent-5_max: 161
    cleaning_beam_agent-5_mean: 68.73
    cleaning_beam_agent-5_min: 17
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-38-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 892.9999999999801
  episode_reward_mean: 652.8399999999954
  episode_reward_min: 323.9999999999992
  episodes_this_iter: 96
  episodes_total: 9984
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19762.157
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006429887725971639
        entropy: 1.3059914112091064
        entropy_coeff: 0.0017600000137463212
        kl: 0.012452778406441212
        model: {}
        policy_loss: -0.0300297774374485
        total_loss: -0.02844661846756935
        vf_explained_var: 0.0747305303812027
        vf_loss: 13.9115571975708
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006429887725971639
        entropy: 1.130648136138916
        entropy_coeff: 0.0017600000137463212
        kl: 0.01641605794429779
        model: {}
        policy_loss: -0.03843138366937637
        total_loss: -0.035611592233181
        vf_explained_var: -0.016982778906822205
        vf_loss: 15.26524543762207
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006429887725971639
        entropy: 1.0213106870651245
        entropy_coeff: 0.0017600000137463212
        kl: 0.012103395536541939
        model: {}
        policy_loss: -0.029716722667217255
        total_loss: -0.027713190764188766
        vf_explained_var: 0.08165179193019867
        vf_loss: 13.80362319946289
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006429887725971639
        entropy: 0.8355424404144287
        entropy_coeff: 0.0017600000137463212
        kl: 0.009716912172734737
        model: {}
        policy_loss: -0.02662324532866478
        total_loss: -0.02490268647670746
        vf_explained_var: 0.16826677322387695
        vf_loss: 12.477300643920898
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006429887725971639
        entropy: 0.9989647269248962
        entropy_coeff: 0.0017600000137463212
        kl: 0.018622232601046562
        model: {}
        policy_loss: -0.033566754311323166
        total_loss: -0.030251236632466316
        vf_explained_var: 0.10079160332679749
        vf_loss: 13.492486953735352
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006429887725971639
        entropy: 0.94655442237854
        entropy_coeff: 0.0017600000137463212
        kl: 0.01345029752701521
        model: {}
        policy_loss: -0.034890979528427124
        total_loss: -0.03264405205845833
        vf_explained_var: 0.185958132147789
        vf_loss: 12.228041648864746
    load_time_ms: 13183.463
    num_steps_sampled: 9984000
    num_steps_trained: 9984000
    sample_time_ms: 92386.054
    update_time_ms: 40.844
  iterations_since_restore: 44
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 19.078888888888887
    ram_util_percent: 16.72388888888889
  pid: 24061
  policy_reward_max:
    agent-0: 148.83333333333317
    agent-1: 148.83333333333317
    agent-2: 148.83333333333317
    agent-3: 148.83333333333317
    agent-4: 148.83333333333317
    agent-5: 148.83333333333317
  policy_reward_mean:
    agent-0: 108.80666666666701
    agent-1: 108.80666666666701
    agent-2: 108.80666666666701
    agent-3: 108.80666666666701
    agent-4: 108.80666666666701
    agent-5: 108.80666666666701
  policy_reward_min:
    agent-0: 53.99999999999991
    agent-1: 53.99999999999991
    agent-2: 53.99999999999991
    agent-3: 53.99999999999991
    agent-4: 53.99999999999991
    agent-5: 53.99999999999991
  sampler_perf:
    mean_env_wait_ms: 24.57857504586721
    mean_inference_ms: 12.382431204515576
    mean_processing_ms: 51.185075462820194
  time_since_restore: 6288.760961294174
  time_this_iter_s: 125.85576820373535
  time_total_s: 15414.772775173187
  timestamp: 1637030283
  timesteps_since_restore: 4224000
  timesteps_this_iter: 96000
  timesteps_total: 9984000
  training_iteration: 104
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 30.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    104 |          15414.8 | 9984000 |   652.84 |
+--------------------------------------+----------+--------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 63
    apples_agent-0_mean: 9.68
    apples_agent-0_min: 0
    apples_agent-1_max: 90
    apples_agent-1_mean: 26.8
    apples_agent-1_min: 0
    apples_agent-2_max: 88
    apples_agent-2_mean: 5.27
    apples_agent-2_min: 0
    apples_agent-3_max: 170
    apples_agent-3_mean: 97.34
    apples_agent-3_min: 18
    apples_agent-4_max: 56
    apples_agent-4_mean: 2.04
    apples_agent-4_min: 0
    apples_agent-5_max: 168
    apples_agent-5_mean: 75.47
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 440
    cleaning_beam_agent-0_mean: 256.31
    cleaning_beam_agent-0_min: 83
    cleaning_beam_agent-1_max: 380
    cleaning_beam_agent-1_mean: 232.05
    cleaning_beam_agent-1_min: 23
    cleaning_beam_agent-2_max: 676
    cleaning_beam_agent-2_mean: 487.07
    cleaning_beam_agent-2_min: 163
    cleaning_beam_agent-3_max: 166
    cleaning_beam_agent-3_mean: 38.58
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 599
    cleaning_beam_agent-4_mean: 444.95
    cleaning_beam_agent-4_min: 173
    cleaning_beam_agent-5_max: 152
    cleaning_beam_agent-5_mean: 68.07
    cleaning_beam_agent-5_min: 20
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 5
    fire_beam_agent-3_mean: 0.07
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.06
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-40-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 836.999999999985
  episode_reward_mean: 614.4799999999982
  episode_reward_min: 195.9999999999991
  episodes_this_iter: 96
  episodes_total: 10080
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19739.678
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006369983893819153
        entropy: 1.3029799461364746
        entropy_coeff: 0.0017600000137463212
        kl: 0.013097868300974369
        model: {}
        policy_loss: -0.03200440853834152
        total_loss: -0.030234139412641525
        vf_explained_var: 0.07427172362804413
        vf_loss: 14.439423561096191
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006369983893819153
        entropy: 1.1394422054290771
        entropy_coeff: 0.0017600000137463212
        kl: 0.015410229563713074
        model: {}
        policy_loss: -0.03882499039173126
        total_loss: -0.036215513944625854
        vf_explained_var: 0.018962696194648743
        vf_loss: 15.32845687866211
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006369983893819153
        entropy: 1.0045840740203857
        entropy_coeff: 0.0017600000137463212
        kl: 0.012222226709127426
        model: {}
        policy_loss: -0.029791222885251045
        total_loss: -0.027668412774801254
        vf_explained_var: 0.07465563714504242
        vf_loss: 14.46430778503418
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006369983893819153
        entropy: 0.8576607704162598
        entropy_coeff: 0.0017600000137463212
        kl: 0.010614363476634026
        model: {}
        policy_loss: -0.025119639933109283
        total_loss: -0.023223809897899628
        vf_explained_var: 0.17843417823314667
        vf_loss: 12.824405670166016
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006369983893819153
        entropy: 1.03744375705719
        entropy_coeff: 0.0017600000137463212
        kl: 0.014058290049433708
        model: {}
        policy_loss: -0.0369449183344841
        total_loss: -0.034590139985084534
        vf_explained_var: 0.12395636737346649
        vf_loss: 13.690268516540527
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006369983893819153
        entropy: 0.9547539949417114
        entropy_coeff: 0.0017600000137463212
        kl: 0.013384334743022919
        model: {}
        policy_loss: -0.03616593778133392
        total_loss: -0.03394429385662079
        vf_explained_var: 0.21516157686710358
        vf_loss: 12.251385688781738
    load_time_ms: 13187.607
    num_steps_sampled: 10080000
    num_steps_trained: 10080000
    sample_time_ms: 92324.655
    update_time_ms: 40.913
  iterations_since_restore: 45
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 19.214044943820223
    ram_util_percent: 16.746629213483143
  pid: 24061
  policy_reward_max:
    agent-0: 139.50000000000014
    agent-1: 139.50000000000014
    agent-2: 139.50000000000014
    agent-3: 139.50000000000014
    agent-4: 139.50000000000014
    agent-5: 139.50000000000014
  policy_reward_mean:
    agent-0: 102.4133333333336
    agent-1: 102.4133333333336
    agent-2: 102.4133333333336
    agent-3: 102.4133333333336
    agent-4: 102.4133333333336
    agent-5: 102.4133333333336
  policy_reward_min:
    agent-0: 32.666666666666714
    agent-1: 32.666666666666714
    agent-2: 32.666666666666714
    agent-3: 32.666666666666714
    agent-4: 32.666666666666714
    agent-5: 32.666666666666714
  sampler_perf:
    mean_env_wait_ms: 24.59056750162894
    mean_inference_ms: 12.382851268942726
    mean_processing_ms: 51.184908905462514
  time_since_restore: 6413.971216678619
  time_this_iter_s: 125.21025538444519
  time_total_s: 15539.983030557632
  timestamp: 1637030408
  timesteps_since_restore: 4320000
  timesteps_this_iter: 96000
  timesteps_total: 10080000
  training_iteration: 105
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 30.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    105 |            15540 | 10080000 |   614.48 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 82
    apples_agent-0_mean: 7.4
    apples_agent-0_min: 0
    apples_agent-1_max: 94
    apples_agent-1_mean: 31.57
    apples_agent-1_min: 0
    apples_agent-2_max: 145
    apples_agent-2_mean: 6.5
    apples_agent-2_min: 0
    apples_agent-3_max: 158
    apples_agent-3_mean: 90.26
    apples_agent-3_min: 18
    apples_agent-4_max: 25
    apples_agent-4_mean: 1.78
    apples_agent-4_min: 0
    apples_agent-5_max: 168
    apples_agent-5_mean: 75.14
    apples_agent-5_min: 29
    cleaning_beam_agent-0_max: 497
    cleaning_beam_agent-0_mean: 258.23
    cleaning_beam_agent-0_min: 84
    cleaning_beam_agent-1_max: 397
    cleaning_beam_agent-1_mean: 232.06
    cleaning_beam_agent-1_min: 109
    cleaning_beam_agent-2_max: 687
    cleaning_beam_agent-2_mean: 445.19
    cleaning_beam_agent-2_min: 190
    cleaning_beam_agent-3_max: 121
    cleaning_beam_agent-3_mean: 36.29
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 592
    cleaning_beam_agent-4_mean: 441.35
    cleaning_beam_agent-4_min: 163
    cleaning_beam_agent-5_max: 178
    cleaning_beam_agent-5_mean: 71.61
    cleaning_beam_agent-5_min: 18
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 4
    fire_beam_agent-1_mean: 0.05
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 17
    fire_beam_agent-5_mean: 0.32
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-42-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 856.9999999999872
  episode_reward_mean: 604.3799999999997
  episode_reward_min: 231.99999999999775
  episodes_this_iter: 96
  episodes_total: 10176
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19749.352
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006310080061666667
        entropy: 1.2869207859039307
        entropy_coeff: 0.0017600000137463212
        kl: 0.013387513346970081
        model: {}
        policy_loss: -0.03101298213005066
        total_loss: -0.029060812667012215
        vf_explained_var: 0.09029039740562439
        vf_loss: 15.396442413330078
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006310080061666667
        entropy: 1.1414785385131836
        entropy_coeff: 0.0017600000137463212
        kl: 0.015039309859275818
        model: {}
        policy_loss: -0.03856528177857399
        total_loss: -0.03597017377614975
        vf_explained_var: 0.05645054578781128
        vf_loss: 15.962499618530273
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006310080061666667
        entropy: 1.0518054962158203
        entropy_coeff: 0.0017600000137463212
        kl: 0.012903889641165733
        model: {}
        policy_loss: -0.030932720750570297
        total_loss: -0.02861998975276947
        vf_explained_var: 0.06423990428447723
        vf_loss: 15.83133602142334
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006310080061666667
        entropy: 0.8519318699836731
        entropy_coeff: 0.0017600000137463212
        kl: 0.01058105193078518
        model: {}
        policy_loss: -0.026676945388317108
        total_loss: -0.024773981422185898
        vf_explained_var: 0.2383340746164322
        vf_loss: 12.861542701721191
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006310080061666667
        entropy: 1.0317962169647217
        entropy_coeff: 0.0017600000137463212
        kl: 0.014177328906953335
        model: {}
        policy_loss: -0.0376402884721756
        total_loss: -0.035080719739198685
        vf_explained_var: 0.09035767614841461
        vf_loss: 15.400613784790039
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006310080061666667
        entropy: 0.9509143829345703
        entropy_coeff: 0.0017600000137463212
        kl: 0.013181290589272976
        model: {}
        policy_loss: -0.03488769754767418
        total_loss: -0.03262803331017494
        vf_explained_var: 0.23359480500221252
        vf_loss: 12.970199584960938
    load_time_ms: 13206.602
    num_steps_sampled: 10176000
    num_steps_trained: 10176000
    sample_time_ms: 92472.389
    update_time_ms: 40.651
  iterations_since_restore: 46
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 19.065193370165748
    ram_util_percent: 16.67348066298343
  pid: 24061
  policy_reward_max:
    agent-0: 142.83333333333348
    agent-1: 142.83333333333348
    agent-2: 142.83333333333348
    agent-3: 142.83333333333348
    agent-4: 142.83333333333348
    agent-5: 142.83333333333348
  policy_reward_mean:
    agent-0: 100.73000000000023
    agent-1: 100.73000000000023
    agent-2: 100.73000000000023
    agent-3: 100.73000000000023
    agent-4: 100.73000000000023
    agent-5: 100.73000000000023
  policy_reward_min:
    agent-0: 38.66666666666664
    agent-1: 38.66666666666664
    agent-2: 38.66666666666664
    agent-3: 38.66666666666664
    agent-4: 38.66666666666664
    agent-5: 38.66666666666664
  sampler_perf:
    mean_env_wait_ms: 24.598157677148397
    mean_inference_ms: 12.38316197219058
    mean_processing_ms: 51.19544397507681
  time_since_restore: 6541.132019281387
  time_this_iter_s: 127.16080260276794
  time_total_s: 15667.1438331604
  timestamp: 1637030536
  timesteps_since_restore: 4416000
  timesteps_this_iter: 96000
  timesteps_total: 10176000
  training_iteration: 106
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 30.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    106 |          15667.1 | 10176000 |   604.38 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 36
    apples_agent-0_mean: 4.6
    apples_agent-0_min: 0
    apples_agent-1_max: 99
    apples_agent-1_mean: 32.15
    apples_agent-1_min: 0
    apples_agent-2_max: 113
    apples_agent-2_mean: 6.45
    apples_agent-2_min: 0
    apples_agent-3_max: 170
    apples_agent-3_mean: 101.04
    apples_agent-3_min: 22
    apples_agent-4_max: 61
    apples_agent-4_mean: 1.61
    apples_agent-4_min: 0
    apples_agent-5_max: 163
    apples_agent-5_mean: 72.46
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 518
    cleaning_beam_agent-0_mean: 255.68
    cleaning_beam_agent-0_min: 114
    cleaning_beam_agent-1_max: 386
    cleaning_beam_agent-1_mean: 227.4
    cleaning_beam_agent-1_min: 115
    cleaning_beam_agent-2_max: 614
    cleaning_beam_agent-2_mean: 431.22
    cleaning_beam_agent-2_min: 177
    cleaning_beam_agent-3_max: 114
    cleaning_beam_agent-3_mean: 41.41
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 587
    cleaning_beam_agent-4_mean: 445.61
    cleaning_beam_agent-4_min: 252
    cleaning_beam_agent-5_max: 196
    cleaning_beam_agent-5_mean: 72.79
    cleaning_beam_agent-5_min: 19
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 4
    fire_beam_agent-4_mean: 0.07
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-44-21
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 932.9999999999764
  episode_reward_mean: 606.8799999999985
  episode_reward_min: 220.99999999999764
  episodes_this_iter: 96
  episodes_total: 10272
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19738.588
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006250176229514182
        entropy: 1.2953120470046997
        entropy_coeff: 0.0017600000137463212
        kl: 0.012676257640123367
        model: {}
        policy_loss: -0.03271143138408661
        total_loss: -0.031123168766498566
        vf_explained_var: 0.11392630636692047
        vf_loss: 13.327625274658203
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006250176229514182
        entropy: 1.1392884254455566
        entropy_coeff: 0.0017600000137463212
        kl: 0.01418229565024376
        model: {}
        policy_loss: -0.03805897384881973
        total_loss: -0.03578989580273628
        vf_explained_var: 0.04447661340236664
        vf_loss: 14.377671241760254
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006250176229514182
        entropy: 1.0635380744934082
        entropy_coeff: 0.0017600000137463212
        kl: 0.01310637779533863
        model: {}
        policy_loss: -0.03156609460711479
        total_loss: -0.02945755422115326
        vf_explained_var: 0.09680655598640442
        vf_loss: 13.590953826904297
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006250176229514182
        entropy: 0.8712741136550903
        entropy_coeff: 0.0017600000137463212
        kl: 0.010518789291381836
        model: {}
        policy_loss: -0.027740830555558205
        total_loss: -0.02593556046485901
        vf_explained_var: 0.1792241632938385
        vf_loss: 12.349529266357422
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006250176229514182
        entropy: 1.0308740139007568
        entropy_coeff: 0.0017600000137463212
        kl: 0.014533182606101036
        model: {}
        policy_loss: -0.03608643636107445
        total_loss: -0.033579882234334946
        vf_explained_var: 0.060419365763664246
        vf_loss: 14.142566680908203
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006250176229514182
        entropy: 0.9482836723327637
        entropy_coeff: 0.0017600000137463212
        kl: 0.013721080496907234
        model: {}
        policy_loss: -0.03545922040939331
        total_loss: -0.03314527869224548
        vf_explained_var: 0.17716829478740692
        vf_loss: 12.387017250061035
    load_time_ms: 13207.641
    num_steps_sampled: 10272000
    num_steps_trained: 10272000
    sample_time_ms: 92532.462
    update_time_ms: 42.268
  iterations_since_restore: 47
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 19.14860335195531
    ram_util_percent: 16.750837988826813
  pid: 24061
  policy_reward_max:
    agent-0: 155.4999999999998
    agent-1: 155.4999999999998
    agent-2: 155.4999999999998
    agent-3: 155.4999999999998
    agent-4: 155.4999999999998
    agent-5: 155.4999999999998
  policy_reward_mean:
    agent-0: 101.14666666666693
    agent-1: 101.14666666666693
    agent-2: 101.14666666666693
    agent-3: 101.14666666666693
    agent-4: 101.14666666666693
    agent-5: 101.14666666666693
  policy_reward_min:
    agent-0: 36.83333333333336
    agent-1: 36.83333333333336
    agent-2: 36.83333333333336
    agent-3: 36.83333333333336
    agent-4: 36.83333333333336
    agent-5: 36.83333333333336
  sampler_perf:
    mean_env_wait_ms: 24.602192133801463
    mean_inference_ms: 12.383855243496189
    mean_processing_ms: 51.20182077384993
  time_since_restore: 6666.393948316574
  time_this_iter_s: 125.26192903518677
  time_total_s: 15792.405762195587
  timestamp: 1637030661
  timesteps_since_restore: 4512000
  timesteps_this_iter: 96000
  timesteps_total: 10272000
  training_iteration: 107
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    107 |          15792.4 | 10272000 |   606.88 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 52
    apples_agent-0_mean: 8.08
    apples_agent-0_min: 0
    apples_agent-1_max: 143
    apples_agent-1_mean: 29.76
    apples_agent-1_min: 0
    apples_agent-2_max: 106
    apples_agent-2_mean: 4.28
    apples_agent-2_min: 0
    apples_agent-3_max: 182
    apples_agent-3_mean: 104.68
    apples_agent-3_min: 36
    apples_agent-4_max: 41
    apples_agent-4_mean: 1.11
    apples_agent-4_min: 0
    apples_agent-5_max: 123
    apples_agent-5_mean: 72.65
    apples_agent-5_min: 26
    cleaning_beam_agent-0_max: 463
    cleaning_beam_agent-0_mean: 249.59
    cleaning_beam_agent-0_min: 104
    cleaning_beam_agent-1_max: 406
    cleaning_beam_agent-1_mean: 233.75
    cleaning_beam_agent-1_min: 98
    cleaning_beam_agent-2_max: 677
    cleaning_beam_agent-2_mean: 426.67
    cleaning_beam_agent-2_min: 33
    cleaning_beam_agent-3_max: 121
    cleaning_beam_agent-3_mean: 37.89
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 596
    cleaning_beam_agent-4_mean: 443.94
    cleaning_beam_agent-4_min: 161
    cleaning_beam_agent-5_max: 249
    cleaning_beam_agent-5_mean: 71.37
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.11
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-46-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 818.9999999999765
  episode_reward_mean: 616.569999999998
  episode_reward_min: 246.99999999999912
  episodes_this_iter: 96
  episodes_total: 10368
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19736.723
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006190271815285087
        entropy: 1.2908518314361572
        entropy_coeff: 0.0017600000137463212
        kl: 0.014046434313058853
        model: {}
        policy_loss: -0.031070977449417114
        total_loss: -0.029143236577510834
        vf_explained_var: 0.1049877405166626
        vf_loss: 13.903507232666016
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006190271815285087
        entropy: 1.1369175910949707
        entropy_coeff: 0.0017600000137463212
        kl: 0.014836378395557404
        model: {}
        policy_loss: -0.03685174137353897
        total_loss: -0.034402165561914444
        vf_explained_var: 0.04405848681926727
        vf_loss: 14.832745552062988
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006190271815285087
        entropy: 1.0765011310577393
        entropy_coeff: 0.0017600000137463212
        kl: 0.012528691440820694
        model: {}
        policy_loss: -0.030967896804213524
        total_loss: -0.02894601970911026
        vf_explained_var: 0.0911935418844223
        vf_loss: 14.107841491699219
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006190271815285087
        entropy: 0.8713918328285217
        entropy_coeff: 0.0017600000137463212
        kl: 0.010763287544250488
        model: {}
        policy_loss: -0.028578463941812515
        total_loss: -0.02666058950126171
        vf_explained_var: 0.16182488203048706
        vf_loss: 12.988678932189941
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006190271815285087
        entropy: 1.0419894456863403
        entropy_coeff: 0.0017600000137463212
        kl: 0.012818146497011185
        model: {}
        policy_loss: -0.03541075065732002
        total_loss: -0.03324763476848602
        vf_explained_var: 0.0755528062582016
        vf_loss: 14.333900451660156
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006190271815285087
        entropy: 0.9404623508453369
        entropy_coeff: 0.0017600000137463212
        kl: 0.013521512970328331
        model: {}
        policy_loss: -0.035892099142074585
        total_loss: -0.03357060253620148
        vf_explained_var: 0.1794053167104721
        vf_loss: 12.72408676147461
    load_time_ms: 13104.227
    num_steps_sampled: 10368000
    num_steps_trained: 10368000
    sample_time_ms: 92600.484
    update_time_ms: 43.847
  iterations_since_restore: 48
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 19.13631284916201
    ram_util_percent: 16.755307262569833
  pid: 24061
  policy_reward_max:
    agent-0: 136.50000000000009
    agent-1: 136.50000000000009
    agent-2: 136.50000000000009
    agent-3: 136.50000000000009
    agent-4: 136.50000000000009
    agent-5: 136.50000000000009
  policy_reward_mean:
    agent-0: 102.76166666666698
    agent-1: 102.76166666666698
    agent-2: 102.76166666666698
    agent-3: 102.76166666666698
    agent-4: 102.76166666666698
    agent-5: 102.76166666666698
  policy_reward_min:
    agent-0: 41.16666666666666
    agent-1: 41.16666666666666
    agent-2: 41.16666666666666
    agent-3: 41.16666666666666
    agent-4: 41.16666666666666
    agent-5: 41.16666666666666
  sampler_perf:
    mean_env_wait_ms: 24.60681830351065
    mean_inference_ms: 12.384063478870091
    mean_processing_ms: 51.2019457248523
  time_since_restore: 6791.573197841644
  time_this_iter_s: 125.17924952507019
  time_total_s: 15917.585011720657
  timestamp: 1637030786
  timesteps_since_restore: 4608000
  timesteps_this_iter: 96000
  timesteps_total: 10368000
  training_iteration: 108
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    108 |          15917.6 | 10368000 |   616.57 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 42
    apples_agent-0_mean: 4.54
    apples_agent-0_min: 0
    apples_agent-1_max: 90
    apples_agent-1_mean: 29.95
    apples_agent-1_min: 0
    apples_agent-2_max: 92
    apples_agent-2_mean: 5.15
    apples_agent-2_min: 0
    apples_agent-3_max: 163
    apples_agent-3_mean: 104.08
    apples_agent-3_min: 38
    apples_agent-4_max: 25
    apples_agent-4_mean: 1.22
    apples_agent-4_min: 0
    apples_agent-5_max: 155
    apples_agent-5_mean: 75.59
    apples_agent-5_min: 10
    cleaning_beam_agent-0_max: 424
    cleaning_beam_agent-0_mean: 267.35
    cleaning_beam_agent-0_min: 131
    cleaning_beam_agent-1_max: 411
    cleaning_beam_agent-1_mean: 237.16
    cleaning_beam_agent-1_min: 96
    cleaning_beam_agent-2_max: 596
    cleaning_beam_agent-2_mean: 406.47
    cleaning_beam_agent-2_min: 153
    cleaning_beam_agent-3_max: 89
    cleaning_beam_agent-3_mean: 33.58
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 587
    cleaning_beam_agent-4_mean: 440.03
    cleaning_beam_agent-4_min: 233
    cleaning_beam_agent-5_max: 220
    cleaning_beam_agent-5_mean: 70.06
    cleaning_beam_agent-5_min: 14
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 5
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-48-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 817.9999999999771
  episode_reward_mean: 622.869999999998
  episode_reward_min: 254.99999999999588
  episodes_this_iter: 96
  episodes_total: 10464
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19728.831
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006130367983132601
        entropy: 1.31265127658844
        entropy_coeff: 0.0017600000137463212
        kl: 0.013819634914398193
        model: {}
        policy_loss: -0.031098278239369392
        total_loss: -0.02920999377965927
        vf_explained_var: 0.0692203938961029
        vf_loss: 14.346248626708984
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006130367983132601
        entropy: 1.150849461555481
        entropy_coeff: 0.0017600000137463212
        kl: 0.015135521069169044
        model: {}
        policy_loss: -0.037284765392541885
        total_loss: -0.034796953201293945
        vf_explained_var: 0.03531409800052643
        vf_loss: 14.86197280883789
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006130367983132601
        entropy: 1.0779566764831543
        entropy_coeff: 0.0017600000137463212
        kl: 0.012835772708058357
        model: {}
        policy_loss: -0.031978704035282135
        total_loss: -0.02986777573823929
        vf_explained_var: 0.06517791748046875
        vf_loss: 14.409759521484375
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006130367983132601
        entropy: 0.8547836542129517
        entropy_coeff: 0.0017600000137463212
        kl: 0.010436784476041794
        model: {}
        policy_loss: -0.027843162417411804
        total_loss: -0.025968380272388458
        vf_explained_var: 0.16201284527778625
        vf_loss: 12.918436050415039
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006130367983132601
        entropy: 1.032551884651184
        entropy_coeff: 0.0017600000137463212
        kl: 0.01368385087698698
        model: {}
        policy_loss: -0.03717328608036041
        total_loss: -0.034889813512563705
        vf_explained_var: 0.11473158001899719
        vf_loss: 13.639978408813477
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006130367983132601
        entropy: 0.9457007646560669
        entropy_coeff: 0.0017600000137463212
        kl: 0.014520542696118355
        model: {}
        policy_loss: -0.03537808358669281
        total_loss: -0.03288794681429863
        vf_explained_var: 0.18797609210014343
        vf_loss: 12.504619598388672
    load_time_ms: 13111.962
    num_steps_sampled: 10464000
    num_steps_trained: 10464000
    sample_time_ms: 92607.211
    update_time_ms: 41.273
  iterations_since_restore: 49
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 19.321787709497205
    ram_util_percent: 16.73128491620112
  pid: 24061
  policy_reward_max:
    agent-0: 136.33333333333388
    agent-1: 136.33333333333388
    agent-2: 136.33333333333388
    agent-3: 136.33333333333388
    agent-4: 136.33333333333388
    agent-5: 136.33333333333388
  policy_reward_mean:
    agent-0: 103.81166666666697
    agent-1: 103.81166666666697
    agent-2: 103.81166666666697
    agent-3: 103.81166666666697
    agent-4: 103.81166666666697
    agent-5: 103.81166666666697
  policy_reward_min:
    agent-0: 42.499999999999915
    agent-1: 42.499999999999915
    agent-2: 42.499999999999915
    agent-3: 42.499999999999915
    agent-4: 42.499999999999915
    agent-5: 42.499999999999915
  sampler_perf:
    mean_env_wait_ms: 24.61105347242111
    mean_inference_ms: 12.383915397915239
    mean_processing_ms: 51.20320172724537
  time_since_restore: 6916.946314573288
  time_this_iter_s: 125.37311673164368
  time_total_s: 16042.958128452301
  timestamp: 1637030912
  timesteps_since_restore: 4704000
  timesteps_this_iter: 96000
  timesteps_total: 10464000
  training_iteration: 109
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    109 |            16043 | 10464000 |   622.87 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 67
    apples_agent-0_mean: 6.76
    apples_agent-0_min: 0
    apples_agent-1_max: 76
    apples_agent-1_mean: 25.39
    apples_agent-1_min: 0
    apples_agent-2_max: 66
    apples_agent-2_mean: 4.31
    apples_agent-2_min: 0
    apples_agent-3_max: 167
    apples_agent-3_mean: 102.03
    apples_agent-3_min: 24
    apples_agent-4_max: 34
    apples_agent-4_mean: 1.41
    apples_agent-4_min: 0
    apples_agent-5_max: 118
    apples_agent-5_mean: 71.26
    apples_agent-5_min: 31
    cleaning_beam_agent-0_max: 451
    cleaning_beam_agent-0_mean: 242.28
    cleaning_beam_agent-0_min: 85
    cleaning_beam_agent-1_max: 373
    cleaning_beam_agent-1_mean: 234.2
    cleaning_beam_agent-1_min: 96
    cleaning_beam_agent-2_max: 549
    cleaning_beam_agent-2_mean: 406.44
    cleaning_beam_agent-2_min: 197
    cleaning_beam_agent-3_max: 161
    cleaning_beam_agent-3_mean: 42.43
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 593
    cleaning_beam_agent-4_mean: 443.6
    cleaning_beam_agent-4_min: 228
    cleaning_beam_agent-5_max: 165
    cleaning_beam_agent-5_mean: 69.36
    cleaning_beam_agent-5_min: 16
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.09
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-50-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 851.9999999999783
  episode_reward_mean: 621.7999999999981
  episode_reward_min: 262.999999999996
  episodes_this_iter: 96
  episodes_total: 10560
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19717.544
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006070464150980115
        entropy: 1.3098198175430298
        entropy_coeff: 0.0017600000137463212
        kl: 0.012547800317406654
        model: {}
        policy_loss: -0.03156952187418938
        total_loss: -0.029993345960974693
        vf_explained_var: 0.09929433465003967
        vf_loss: 13.718997955322266
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006070464150980115
        entropy: 1.1404105424880981
        entropy_coeff: 0.0017600000137463212
        kl: 0.01416032575070858
        model: {}
        policy_loss: -0.036572106182575226
        total_loss: -0.03430773317813873
        vf_explained_var: 0.05511780083179474
        vf_loss: 14.394285202026367
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006070464150980115
        entropy: 1.1068589687347412
        entropy_coeff: 0.0017600000137463212
        kl: 0.013394386507570744
        model: {}
        policy_loss: -0.03237311542034149
        total_loss: -0.030215280130505562
        vf_explained_var: 0.06256169080734253
        vf_loss: 14.270277976989746
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006070464150980115
        entropy: 0.8872695565223694
        entropy_coeff: 0.0017600000137463212
        kl: 0.011050057597458363
        model: {}
        policy_loss: -0.027440249919891357
        total_loss: -0.02554978057742119
        vf_explained_var: 0.18431952595710754
        vf_loss: 12.420462608337402
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006070464150980115
        entropy: 1.034044623374939
        entropy_coeff: 0.0017600000137463212
        kl: 0.013833425007760525
        model: {}
        policy_loss: -0.03761014714837074
        total_loss: -0.03529994562268257
        vf_explained_var: 0.10322146117687225
        vf_loss: 13.634352684020996
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006070464150980115
        entropy: 0.8910379409790039
        entropy_coeff: 0.0017600000137463212
        kl: 0.012544522061944008
        model: {}
        policy_loss: -0.03454000502824783
        total_loss: -0.032324135303497314
        vf_explained_var: 0.16205255687236786
        vf_loss: 12.751938819885254
    load_time_ms: 13105.018
    num_steps_sampled: 10560000
    num_steps_trained: 10560000
    sample_time_ms: 92446.114
    update_time_ms: 41.515
  iterations_since_restore: 50
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 19.23220338983051
    ram_util_percent: 16.8271186440678
  pid: 24061
  policy_reward_max:
    agent-0: 142.00000000000023
    agent-1: 142.00000000000023
    agent-2: 142.00000000000023
    agent-3: 142.00000000000023
    agent-4: 142.00000000000023
    agent-5: 142.00000000000023
  policy_reward_mean:
    agent-0: 103.63333333333361
    agent-1: 103.63333333333361
    agent-2: 103.63333333333361
    agent-3: 103.63333333333361
    agent-4: 103.63333333333361
    agent-5: 103.63333333333361
  policy_reward_min:
    agent-0: 43.833333333333314
    agent-1: 43.833333333333314
    agent-2: 43.833333333333314
    agent-3: 43.833333333333314
    agent-4: 43.833333333333314
    agent-5: 43.833333333333314
  sampler_perf:
    mean_env_wait_ms: 24.606703938968188
    mean_inference_ms: 12.383505306365048
    mean_processing_ms: 51.19903924123223
  time_since_restore: 7040.930062532425
  time_this_iter_s: 123.98374795913696
  time_total_s: 16166.941876411438
  timestamp: 1637031036
  timesteps_since_restore: 4800000
  timesteps_this_iter: 96000
  timesteps_total: 10560000
  training_iteration: 110
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    110 |          16166.9 | 10560000 |    621.8 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 71
    apples_agent-0_mean: 7.29
    apples_agent-0_min: 0
    apples_agent-1_max: 88
    apples_agent-1_mean: 28.45
    apples_agent-1_min: 0
    apples_agent-2_max: 159
    apples_agent-2_mean: 7.32
    apples_agent-2_min: 0
    apples_agent-3_max: 188
    apples_agent-3_mean: 105.48
    apples_agent-3_min: 52
    apples_agent-4_max: 14
    apples_agent-4_mean: 0.5
    apples_agent-4_min: 0
    apples_agent-5_max: 121
    apples_agent-5_mean: 70.51
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 467
    cleaning_beam_agent-0_mean: 253.83
    cleaning_beam_agent-0_min: 120
    cleaning_beam_agent-1_max: 369
    cleaning_beam_agent-1_mean: 230.38
    cleaning_beam_agent-1_min: 86
    cleaning_beam_agent-2_max: 641
    cleaning_beam_agent-2_mean: 391.91
    cleaning_beam_agent-2_min: 113
    cleaning_beam_agent-3_max: 161
    cleaning_beam_agent-3_mean: 47.82
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 605
    cleaning_beam_agent-4_mean: 441.17
    cleaning_beam_agent-4_min: 269
    cleaning_beam_agent-5_max: 193
    cleaning_beam_agent-5_mean: 68.67
    cleaning_beam_agent-5_min: 21
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 4
    fire_beam_agent-1_mean: 0.05
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-52-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 843.99999999997
  episode_reward_mean: 628.8799999999965
  episode_reward_min: 217.99999999999594
  episodes_this_iter: 96
  episodes_total: 10656
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19763.493
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000601055973675102
        entropy: 1.3139714002609253
        entropy_coeff: 0.0017600000137463212
        kl: 0.011879146099090576
        model: {}
        policy_loss: -0.031532756984233856
        total_loss: -0.029993362724781036
        vf_explained_var: 0.09725169837474823
        vf_loss: 14.76159381866455
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000601055973675102
        entropy: 1.134903907775879
        entropy_coeff: 0.0017600000137463212
        kl: 0.01384337805211544
        model: {}
        policy_loss: -0.03663896024227142
        total_loss: -0.03430536016821861
        vf_explained_var: 0.04394716024398804
        vf_loss: 15.623567581176758
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000601055973675102
        entropy: 1.087348222732544
        entropy_coeff: 0.0017600000137463212
        kl: 0.014963542111217976
        model: {}
        policy_loss: -0.0291670560836792
        total_loss: -0.02661050669848919
        vf_explained_var: 0.09583105146884918
        vf_loss: 14.775768280029297
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000601055973675102
        entropy: 0.8483986258506775
        entropy_coeff: 0.0017600000137463212
        kl: 0.010095613077282906
        model: {}
        policy_loss: -0.02660641074180603
        total_loss: -0.024708734825253487
        vf_explained_var: 0.1609354317188263
        vf_loss: 13.717326164245605
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000601055973675102
        entropy: 1.0239520072937012
        entropy_coeff: 0.0017600000137463212
        kl: 0.013510163873434067
        model: {}
        policy_loss: -0.036565978080034256
        total_loss: -0.034139152616262436
        vf_explained_var: 0.06576406955718994
        vf_loss: 15.269462585449219
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000601055973675102
        entropy: 0.8975425958633423
        entropy_coeff: 0.0017600000137463212
        kl: 0.012855751439929008
        model: {}
        policy_loss: -0.03426169231534004
        total_loss: -0.031919147819280624
        vf_explained_var: 0.17325738072395325
        vf_loss: 13.510680198669434
    load_time_ms: 13091.801
    num_steps_sampled: 10656000
    num_steps_trained: 10656000
    sample_time_ms: 92275.856
    update_time_ms: 41.331
  iterations_since_restore: 51
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 19.20112359550562
    ram_util_percent: 16.685955056179775
  pid: 24061
  policy_reward_max:
    agent-0: 140.6666666666668
    agent-1: 140.6666666666668
    agent-2: 140.6666666666668
    agent-3: 140.6666666666668
    agent-4: 140.6666666666668
    agent-5: 140.6666666666668
  policy_reward_mean:
    agent-0: 104.81333333333367
    agent-1: 104.81333333333367
    agent-2: 104.81333333333367
    agent-3: 104.81333333333367
    agent-4: 104.81333333333367
    agent-5: 104.81333333333367
  policy_reward_min:
    agent-0: 36.33333333333336
    agent-1: 36.33333333333336
    agent-2: 36.33333333333336
    agent-3: 36.33333333333336
    agent-4: 36.33333333333336
    agent-5: 36.33333333333336
  sampler_perf:
    mean_env_wait_ms: 24.607384872734148
    mean_inference_ms: 12.383076920507504
    mean_processing_ms: 51.20008388286493
  time_since_restore: 7166.203378915787
  time_this_iter_s: 125.27331638336182
  time_total_s: 16292.2151927948
  timestamp: 1637031162
  timesteps_since_restore: 4896000
  timesteps_this_iter: 96000
  timesteps_total: 10656000
  training_iteration: 111
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    111 |          16292.2 | 10656000 |   628.88 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 47
    apples_agent-0_mean: 7.4
    apples_agent-0_min: 0
    apples_agent-1_max: 88
    apples_agent-1_mean: 31.98
    apples_agent-1_min: 0
    apples_agent-2_max: 54
    apples_agent-2_mean: 4.27
    apples_agent-2_min: 0
    apples_agent-3_max: 193
    apples_agent-3_mean: 101.39
    apples_agent-3_min: 6
    apples_agent-4_max: 39
    apples_agent-4_mean: 1.81
    apples_agent-4_min: 0
    apples_agent-5_max: 139
    apples_agent-5_mean: 69.69
    apples_agent-5_min: 12
    cleaning_beam_agent-0_max: 413
    cleaning_beam_agent-0_mean: 237.07
    cleaning_beam_agent-0_min: 132
    cleaning_beam_agent-1_max: 424
    cleaning_beam_agent-1_mean: 219.76
    cleaning_beam_agent-1_min: 98
    cleaning_beam_agent-2_max: 545
    cleaning_beam_agent-2_mean: 388.95
    cleaning_beam_agent-2_min: 172
    cleaning_beam_agent-3_max: 172
    cleaning_beam_agent-3_mean: 48.62
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 583
    cleaning_beam_agent-4_mean: 440.3
    cleaning_beam_agent-4_min: 147
    cleaning_beam_agent-5_max: 398
    cleaning_beam_agent-5_mean: 75.51
    cleaning_beam_agent-5_min: 14
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-54-48
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 839.9999999999834
  episode_reward_mean: 611.6599999999981
  episode_reward_min: 126.00000000000088
  episodes_this_iter: 96
  episodes_total: 10752
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19765.102
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005950655904598534
        entropy: 1.3229072093963623
        entropy_coeff: 0.0017600000137463212
        kl: 0.012529909610748291
        model: {}
        policy_loss: -0.032321181148290634
        total_loss: -0.030727341771125793
        vf_explained_var: 0.09298960864543915
        vf_loss: 14.161722183227539
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005950655904598534
        entropy: 1.1390341520309448
        entropy_coeff: 0.0017600000137463212
        kl: 0.01487857848405838
        model: {}
        policy_loss: -0.03766251355409622
        total_loss: -0.035153161734342575
        vf_explained_var: 0.014978080987930298
        vf_loss: 15.383411407470703
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005950655904598534
        entropy: 1.1014658212661743
        entropy_coeff: 0.0017600000137463212
        kl: 0.014452321454882622
        model: {}
        policy_loss: -0.030156375840306282
        total_loss: -0.027768930420279503
        vf_explained_var: 0.0801272988319397
        vf_loss: 14.355640411376953
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005950655904598534
        entropy: 0.8685169219970703
        entropy_coeff: 0.0017600000137463212
        kl: 0.010168122127652168
        model: {}
        policy_loss: -0.027453051880002022
        total_loss: -0.025687657296657562
        vf_explained_var: 0.19239138066768646
        vf_loss: 12.603594779968262
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005950655904598534
        entropy: 1.029482364654541
        entropy_coeff: 0.0017600000137463212
        kl: 0.013601213693618774
        model: {}
        policy_loss: -0.03656167909502983
        total_loss: -0.03424696996808052
        vf_explained_var: 0.09937843680381775
        vf_loss: 14.063499450683594
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005950655904598534
        entropy: 0.930397093296051
        entropy_coeff: 0.0017600000137463212
        kl: 0.013332799077033997
        model: {}
        policy_loss: -0.035472601652145386
        total_loss: -0.033152785152196884
        vf_explained_var: 0.1737448126077652
        vf_loss: 12.907608032226562
    load_time_ms: 13096.368
    num_steps_sampled: 10752000
    num_steps_trained: 10752000
    sample_time_ms: 92351.374
    update_time_ms: 41.289
  iterations_since_restore: 52
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 19.12277777777778
    ram_util_percent: 16.812777777777782
  pid: 24061
  policy_reward_max:
    agent-0: 140.00000000000028
    agent-1: 140.00000000000028
    agent-2: 140.00000000000028
    agent-3: 140.00000000000028
    agent-4: 140.00000000000028
    agent-5: 140.00000000000028
  policy_reward_mean:
    agent-0: 101.94333333333363
    agent-1: 101.94333333333363
    agent-2: 101.94333333333363
    agent-3: 101.94333333333363
    agent-4: 101.94333333333363
    agent-5: 101.94333333333363
  policy_reward_min:
    agent-0: 21.00000000000001
    agent-1: 21.00000000000001
    agent-2: 21.00000000000001
    agent-3: 21.00000000000001
    agent-4: 21.00000000000001
    agent-5: 21.00000000000001
  sampler_perf:
    mean_env_wait_ms: 24.61104203557558
    mean_inference_ms: 12.383305278764256
    mean_processing_ms: 51.21221849834972
  time_since_restore: 7292.418622016907
  time_this_iter_s: 126.21524310112
  time_total_s: 16418.43043589592
  timestamp: 1637031288
  timesteps_since_restore: 4992000
  timesteps_this_iter: 96000
  timesteps_total: 10752000
  training_iteration: 112
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 30.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    112 |          16418.4 | 10752000 |   611.66 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 81
    apples_agent-0_mean: 6.64
    apples_agent-0_min: 0
    apples_agent-1_max: 113
    apples_agent-1_mean: 28.06
    apples_agent-1_min: 0
    apples_agent-2_max: 139
    apples_agent-2_mean: 6.85
    apples_agent-2_min: 0
    apples_agent-3_max: 270
    apples_agent-3_mean: 107.1
    apples_agent-3_min: 44
    apples_agent-4_max: 23
    apples_agent-4_mean: 1.05
    apples_agent-4_min: 0
    apples_agent-5_max: 125
    apples_agent-5_mean: 63.98
    apples_agent-5_min: 3
    cleaning_beam_agent-0_max: 442
    cleaning_beam_agent-0_mean: 280.04
    cleaning_beam_agent-0_min: 118
    cleaning_beam_agent-1_max: 444
    cleaning_beam_agent-1_mean: 229.84
    cleaning_beam_agent-1_min: 85
    cleaning_beam_agent-2_max: 654
    cleaning_beam_agent-2_mean: 368.16
    cleaning_beam_agent-2_min: 149
    cleaning_beam_agent-3_max: 117
    cleaning_beam_agent-3_mean: 43.85
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 594
    cleaning_beam_agent-4_mean: 454.37
    cleaning_beam_agent-4_min: 296
    cleaning_beam_agent-5_max: 274
    cleaning_beam_agent-5_mean: 80.31
    cleaning_beam_agent-5_min: 21
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.07
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-56-54
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 934.999999999985
  episode_reward_mean: 628.2499999999973
  episode_reward_min: 276.9999999999974
  episodes_this_iter: 96
  episodes_total: 10848
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19770.35
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005890752072446048
        entropy: 1.2800145149230957
        entropy_coeff: 0.0017600000137463212
        kl: 0.012004729360342026
        model: {}
        policy_loss: -0.031331948935985565
        total_loss: -0.029655877500772476
        vf_explained_var: 0.07688289880752563
        vf_loss: 15.279548645019531
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005890752072446048
        entropy: 1.1302082538604736
        entropy_coeff: 0.0017600000137463212
        kl: 0.015060041099786758
        model: {}
        policy_loss: -0.03652703016996384
        total_loss: -0.03391972929239273
        vf_explained_var: 0.042262002825737
        vf_loss: 15.844619750976562
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005890752072446048
        entropy: 1.1279298067092896
        entropy_coeff: 0.0017600000137463212
        kl: 0.013052724301815033
        model: {}
        policy_loss: -0.03270410746335983
        total_loss: -0.030608100816607475
        vf_explained_var: 0.11077940464019775
        vf_loss: 14.706235885620117
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005890752072446048
        entropy: 0.8281114101409912
        entropy_coeff: 0.0017600000137463212
        kl: 0.01077093742787838
        model: {}
        policy_loss: -0.025459980592131615
        total_loss: -0.023483799770474434
        vf_explained_var: 0.2271067202091217
        vf_loss: 12.794692993164062
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005890752072446048
        entropy: 1.0111041069030762
        entropy_coeff: 0.0017600000137463212
        kl: 0.01491369679570198
        model: {}
        policy_loss: -0.037188541144132614
        total_loss: -0.03448359668254852
        vf_explained_var: 0.09291066229343414
        vf_loss: 15.017492294311523
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005890752072446048
        entropy: 0.9061858654022217
        entropy_coeff: 0.0017600000137463212
        kl: 0.012952612712979317
        model: {}
        policy_loss: -0.03338615968823433
        total_loss: -0.031074194237589836
        vf_explained_var: 0.20405027270317078
        vf_loss: 13.163305282592773
    load_time_ms: 13113.402
    num_steps_sampled: 10848000
    num_steps_trained: 10848000
    sample_time_ms: 92409.666
    update_time_ms: 41.411
  iterations_since_restore: 53
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 19.197752808988767
    ram_util_percent: 16.764606741573036
  pid: 24061
  policy_reward_max:
    agent-0: 155.83333333333317
    agent-1: 155.83333333333317
    agent-2: 155.83333333333317
    agent-3: 155.83333333333317
    agent-4: 155.83333333333317
    agent-5: 155.83333333333317
  policy_reward_mean:
    agent-0: 104.70833333333361
    agent-1: 104.70833333333361
    agent-2: 104.70833333333361
    agent-3: 104.70833333333361
    agent-4: 104.70833333333361
    agent-5: 104.70833333333361
  policy_reward_min:
    agent-0: 46.16666666666667
    agent-1: 46.16666666666667
    agent-2: 46.16666666666667
    agent-3: 46.16666666666667
    agent-4: 46.16666666666667
    agent-5: 46.16666666666667
  sampler_perf:
    mean_env_wait_ms: 24.61462553474748
    mean_inference_ms: 12.38293535835345
    mean_processing_ms: 51.21540270469857
  time_since_restore: 7417.499964475632
  time_this_iter_s: 125.08134245872498
  time_total_s: 16543.511778354645
  timestamp: 1637031414
  timesteps_since_restore: 5088000
  timesteps_this_iter: 96000
  timesteps_total: 10848000
  training_iteration: 113
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    113 |          16543.5 | 10848000 |   628.25 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 61
    apples_agent-0_mean: 7.05
    apples_agent-0_min: 0
    apples_agent-1_max: 152
    apples_agent-1_mean: 28.69
    apples_agent-1_min: 0
    apples_agent-2_max: 111
    apples_agent-2_mean: 7.27
    apples_agent-2_min: 0
    apples_agent-3_max: 270
    apples_agent-3_mean: 107.62
    apples_agent-3_min: 30
    apples_agent-4_max: 37
    apples_agent-4_mean: 1.95
    apples_agent-4_min: 0
    apples_agent-5_max: 131
    apples_agent-5_mean: 66.52
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 442
    cleaning_beam_agent-0_mean: 262.04
    cleaning_beam_agent-0_min: 109
    cleaning_beam_agent-1_max: 375
    cleaning_beam_agent-1_mean: 232.41
    cleaning_beam_agent-1_min: 127
    cleaning_beam_agent-2_max: 570
    cleaning_beam_agent-2_mean: 382.39
    cleaning_beam_agent-2_min: 118
    cleaning_beam_agent-3_max: 110
    cleaning_beam_agent-3_mean: 45.26
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 633
    cleaning_beam_agent-4_mean: 429.76
    cleaning_beam_agent-4_min: 234
    cleaning_beam_agent-5_max: 216
    cleaning_beam_agent-5_mean: 72.57
    cleaning_beam_agent-5_min: 22
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-58-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 897.9999999999726
  episode_reward_mean: 612.5399999999972
  episode_reward_min: 230.9999999999959
  episodes_this_iter: 96
  episodes_total: 10944
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19803.939
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005830848240293562
        entropy: 1.3037934303283691
        entropy_coeff: 0.0017600000137463212
        kl: 0.01236528530716896
        model: {}
        policy_loss: -0.03271733224391937
        total_loss: -0.03095058910548687
        vf_explained_var: 0.07131241261959076
        vf_loss: 15.88364028930664
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005830848240293562
        entropy: 1.1269724369049072
        entropy_coeff: 0.0017600000137463212
        kl: 0.015095190145075321
        model: {}
        policy_loss: -0.038542330265045166
        total_loss: -0.035833537578582764
        vf_explained_var: 0.022401243448257446
        vf_loss: 16.732275009155273
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005830848240293562
        entropy: 1.0966534614562988
        entropy_coeff: 0.0017600000137463212
        kl: 0.013563500717282295
        model: {}
        policy_loss: -0.03187131509184837
        total_loss: -0.02953757531940937
        vf_explained_var: 0.09267432987689972
        vf_loss: 15.51152229309082
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005830848240293562
        entropy: 0.8662609457969666
        entropy_coeff: 0.0017600000137463212
        kl: 0.010961378924548626
        model: {}
        policy_loss: -0.02793329581618309
        total_loss: -0.025889802724123
        vf_explained_var: 0.19560910761356354
        vf_loss: 13.758346557617188
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005830848240293562
        entropy: 1.0431814193725586
        entropy_coeff: 0.0017600000137463212
        kl: 0.015086756087839603
        model: {}
        policy_loss: -0.0374910868704319
        total_loss: -0.03475796431303024
        vf_explained_var: 0.09208711981773376
        vf_loss: 15.517718315124512
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005830848240293562
        entropy: 0.9131515026092529
        entropy_coeff: 0.0017600000137463212
        kl: 0.013037951663136482
        model: {}
        policy_loss: -0.03481419011950493
        total_loss: -0.03242263197898865
        vf_explained_var: 0.18682698905467987
        vf_loss: 13.911111831665039
    load_time_ms: 13100.206
    num_steps_sampled: 10944000
    num_steps_trained: 10944000
    sample_time_ms: 92230.097
    update_time_ms: 40.961
  iterations_since_restore: 54
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 19.254802259887004
    ram_util_percent: 16.771186440677965
  pid: 24061
  policy_reward_max:
    agent-0: 149.6666666666668
    agent-1: 149.6666666666668
    agent-2: 149.6666666666668
    agent-3: 149.6666666666668
    agent-4: 149.6666666666668
    agent-5: 149.6666666666668
  policy_reward_mean:
    agent-0: 102.09000000000027
    agent-1: 102.09000000000027
    agent-2: 102.09000000000027
    agent-3: 102.09000000000027
    agent-4: 102.09000000000027
    agent-5: 102.09000000000027
  policy_reward_min:
    agent-0: 38.499999999999986
    agent-1: 38.499999999999986
    agent-2: 38.499999999999986
    agent-3: 38.499999999999986
    agent-4: 38.499999999999986
    agent-5: 38.499999999999986
  sampler_perf:
    mean_env_wait_ms: 24.61490194475396
    mean_inference_ms: 12.383058605908989
    mean_processing_ms: 51.21715401533999
  time_since_restore: 7541.827633142471
  time_this_iter_s: 124.3276686668396
  time_total_s: 16667.839447021484
  timestamp: 1637031538
  timesteps_since_restore: 5184000
  timesteps_this_iter: 96000
  timesteps_total: 10944000
  training_iteration: 114
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    114 |          16667.8 | 10944000 |   612.54 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 43
    apples_agent-0_mean: 6.12
    apples_agent-0_min: 0
    apples_agent-1_max: 78
    apples_agent-1_mean: 28.25
    apples_agent-1_min: 0
    apples_agent-2_max: 273
    apples_agent-2_mean: 5.86
    apples_agent-2_min: 0
    apples_agent-3_max: 169
    apples_agent-3_mean: 102.93
    apples_agent-3_min: 27
    apples_agent-4_max: 56
    apples_agent-4_mean: 2.8
    apples_agent-4_min: 0
    apples_agent-5_max: 124
    apples_agent-5_mean: 65.73
    apples_agent-5_min: 15
    cleaning_beam_agent-0_max: 485
    cleaning_beam_agent-0_mean: 278.39
    cleaning_beam_agent-0_min: 77
    cleaning_beam_agent-1_max: 380
    cleaning_beam_agent-1_mean: 223.23
    cleaning_beam_agent-1_min: 93
    cleaning_beam_agent-2_max: 558
    cleaning_beam_agent-2_mean: 367.47
    cleaning_beam_agent-2_min: 137
    cleaning_beam_agent-3_max: 156
    cleaning_beam_agent-3_mean: 51.28
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 547
    cleaning_beam_agent-4_mean: 396.58
    cleaning_beam_agent-4_min: 201
    cleaning_beam_agent-5_max: 182
    cleaning_beam_agent-5_mean: 78.39
    cleaning_beam_agent-5_min: 22
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-01-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 888.9999999999877
  episode_reward_mean: 597.0099999999984
  episode_reward_min: 239.9999999999975
  episodes_this_iter: 96
  episodes_total: 11040
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19809.413
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005770943826064467
        entropy: 1.2878940105438232
        entropy_coeff: 0.0017600000137463212
        kl: 0.013578031212091446
        model: {}
        policy_loss: -0.031338274478912354
        total_loss: -0.029253238812088966
        vf_explained_var: 0.07682777941226959
        vf_loss: 16.361183166503906
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005770943826064467
        entropy: 1.124747395515442
        entropy_coeff: 0.0017600000137463212
        kl: 0.014976900070905685
        model: {}
        policy_loss: -0.0367610938847065
        total_loss: -0.03404492139816284
        vf_explained_var: 0.04103231430053711
        vf_loss: 17.00347900390625
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005770943826064467
        entropy: 1.114234209060669
        entropy_coeff: 0.0017600000137463212
        kl: 0.012728599831461906
        model: {}
        policy_loss: -0.03176885470747948
        total_loss: -0.029590779915452003
        vf_explained_var: 0.10091626644134521
        vf_loss: 15.934081077575684
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005770943826064467
        entropy: 0.8797840476036072
        entropy_coeff: 0.0017600000137463212
        kl: 0.01063015591353178
        model: {}
        policy_loss: -0.027660805732011795
        total_loss: -0.025727245956659317
        vf_explained_var: 0.23503634333610535
        vf_loss: 13.559469223022461
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005770943826064467
        entropy: 1.048098087310791
        entropy_coeff: 0.0017600000137463212
        kl: 0.01454236265271902
        model: {}
        policy_loss: -0.03800664097070694
        total_loss: -0.035374078899621964
        vf_explained_var: 0.11487449705600739
        vf_loss: 15.687399864196777
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005770943826064467
        entropy: 0.9258385300636292
        entropy_coeff: 0.0017600000137463212
        kl: 0.017464254051446915
        model: {}
        policy_loss: -0.030973264947533607
        total_loss: -0.027758775278925896
        vf_explained_var: 0.23733727633953094
        vf_loss: 13.511124610900879
    load_time_ms: 13114.86
    num_steps_sampled: 11040000
    num_steps_trained: 11040000
    sample_time_ms: 92279.308
    update_time_ms: 41.529
  iterations_since_restore: 55
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 19.544134078212288
    ram_util_percent: 16.87206703910615
  pid: 24061
  policy_reward_max:
    agent-0: 148.16666666666688
    agent-1: 148.16666666666688
    agent-2: 148.16666666666688
    agent-3: 148.16666666666688
    agent-4: 148.16666666666688
    agent-5: 148.16666666666688
  policy_reward_mean:
    agent-0: 99.50166666666692
    agent-1: 99.50166666666692
    agent-2: 99.50166666666692
    agent-3: 99.50166666666692
    agent-4: 99.50166666666692
    agent-5: 99.50166666666692
  policy_reward_min:
    agent-0: 39.99999999999996
    agent-1: 39.99999999999996
    agent-2: 39.99999999999996
    agent-3: 39.99999999999996
    agent-4: 39.99999999999996
    agent-5: 39.99999999999996
  sampler_perf:
    mean_env_wait_ms: 24.61271176146915
    mean_inference_ms: 12.383718147292974
    mean_processing_ms: 51.23009040880196
  time_since_restore: 7667.80359172821
  time_this_iter_s: 125.97595858573914
  time_total_s: 16793.815405607224
  timestamp: 1637031664
  timesteps_since_restore: 5280000
  timesteps_this_iter: 96000
  timesteps_total: 11040000
  training_iteration: 115
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    115 |          16793.8 | 11040000 |   597.01 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 173
    apples_agent-0_mean: 8.68
    apples_agent-0_min: 0
    apples_agent-1_max: 125
    apples_agent-1_mean: 27.25
    apples_agent-1_min: 0
    apples_agent-2_max: 56
    apples_agent-2_mean: 4.27
    apples_agent-2_min: 0
    apples_agent-3_max: 178
    apples_agent-3_mean: 104.62
    apples_agent-3_min: 21
    apples_agent-4_max: 53
    apples_agent-4_mean: 2.98
    apples_agent-4_min: 0
    apples_agent-5_max: 104
    apples_agent-5_mean: 62.42
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 446
    cleaning_beam_agent-0_mean: 269.93
    cleaning_beam_agent-0_min: 126
    cleaning_beam_agent-1_max: 380
    cleaning_beam_agent-1_mean: 216.64
    cleaning_beam_agent-1_min: 51
    cleaning_beam_agent-2_max: 591
    cleaning_beam_agent-2_mean: 376.84
    cleaning_beam_agent-2_min: 206
    cleaning_beam_agent-3_max: 155
    cleaning_beam_agent-3_mean: 49.15
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 610
    cleaning_beam_agent-4_mean: 416.48
    cleaning_beam_agent-4_min: 144
    cleaning_beam_agent-5_max: 292
    cleaning_beam_agent-5_mean: 87.53
    cleaning_beam_agent-5_min: 17
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 3
    fire_beam_agent-4_mean: 0.06
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-03-10
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 815.9999999999933
  episode_reward_mean: 599.1099999999993
  episode_reward_min: 225.99999999999682
  episodes_this_iter: 96
  episodes_total: 11136
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19850.535
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005711039993911982
        entropy: 1.278398036956787
        entropy_coeff: 0.0017600000137463212
        kl: 0.012728835456073284
        model: {}
        policy_loss: -0.032844219356775284
        total_loss: -0.03105093166232109
        vf_explained_var: 0.09411247074604034
        vf_loss: 14.975006103515625
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005711039993911982
        entropy: 1.1265032291412354
        entropy_coeff: 0.0017600000137463212
        kl: 0.014407448470592499
        model: {}
        policy_loss: -0.03662827983498573
        total_loss: -0.0341838076710701
        vf_explained_var: 0.06508064270019531
        vf_loss: 15.456296920776367
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005711039993911982
        entropy: 1.1381754875183105
        entropy_coeff: 0.0017600000137463212
        kl: 0.014144079759716988
        model: {}
        policy_loss: -0.03244990482926369
        total_loss: -0.030137978494167328
        vf_explained_var: 0.1015980988740921
        vf_loss: 14.863001823425293
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005711039993911982
        entropy: 0.8472900390625
        entropy_coeff: 0.0017600000137463212
        kl: 0.009918575175106525
        model: {}
        policy_loss: -0.02718084678053856
        total_loss: -0.025396138429641724
        vf_explained_var: 0.21852415800094604
        vf_loss: 12.922219276428223
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005711039993911982
        entropy: 1.0502026081085205
        entropy_coeff: 0.0017600000137463212
        kl: 0.014125599525868893
        model: {}
        policy_loss: -0.03875312581658363
        total_loss: -0.03630928695201874
        vf_explained_var: 0.11297301948070526
        vf_loss: 14.670766830444336
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005711039993911982
        entropy: 0.9255204796791077
        entropy_coeff: 0.0017600000137463212
        kl: 0.012748011387884617
        model: {}
        policy_loss: -0.03453961759805679
        total_loss: -0.03229977935552597
        vf_explained_var: 0.20203593373298645
        vf_loss: 13.19145393371582
    load_time_ms: 13091.626
    num_steps_sampled: 11136000
    num_steps_trained: 11136000
    sample_time_ms: 92079.85
    update_time_ms: 41.257
  iterations_since_restore: 56
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 19.433519553072625
    ram_util_percent: 16.94022346368715
  pid: 24061
  policy_reward_max:
    agent-0: 136.00000000000017
    agent-1: 136.00000000000017
    agent-2: 136.00000000000017
    agent-3: 136.00000000000017
    agent-4: 136.00000000000017
    agent-5: 136.00000000000017
  policy_reward_mean:
    agent-0: 99.85166666666696
    agent-1: 99.85166666666696
    agent-2: 99.85166666666696
    agent-3: 99.85166666666696
    agent-4: 99.85166666666696
    agent-5: 99.85166666666696
  policy_reward_min:
    agent-0: 37.66666666666673
    agent-1: 37.66666666666673
    agent-2: 37.66666666666673
    agent-3: 37.66666666666673
    agent-4: 37.66666666666673
    agent-5: 37.66666666666673
  sampler_perf:
    mean_env_wait_ms: 24.613704896575122
    mean_inference_ms: 12.383654580657929
    mean_processing_ms: 51.23176892582456
  time_since_restore: 7793.246218442917
  time_this_iter_s: 125.44262671470642
  time_total_s: 16919.25803232193
  timestamp: 1637031790
  timesteps_since_restore: 5376000
  timesteps_this_iter: 96000
  timesteps_total: 11136000
  training_iteration: 116
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    116 |          16919.3 | 11136000 |   599.11 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 77
    apples_agent-0_mean: 7.46
    apples_agent-0_min: 0
    apples_agent-1_max: 92
    apples_agent-1_mean: 29.86
    apples_agent-1_min: 0
    apples_agent-2_max: 61
    apples_agent-2_mean: 5.67
    apples_agent-2_min: 0
    apples_agent-3_max: 154
    apples_agent-3_mean: 104.96
    apples_agent-3_min: 22
    apples_agent-4_max: 59
    apples_agent-4_mean: 4.18
    apples_agent-4_min: 0
    apples_agent-5_max: 115
    apples_agent-5_mean: 61.47
    apples_agent-5_min: 16
    cleaning_beam_agent-0_max: 432
    cleaning_beam_agent-0_mean: 269.4
    cleaning_beam_agent-0_min: 100
    cleaning_beam_agent-1_max: 355
    cleaning_beam_agent-1_mean: 209.88
    cleaning_beam_agent-1_min: 87
    cleaning_beam_agent-2_max: 624
    cleaning_beam_agent-2_mean: 335.72
    cleaning_beam_agent-2_min: 182
    cleaning_beam_agent-3_max: 191
    cleaning_beam_agent-3_mean: 52.72
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 632
    cleaning_beam_agent-4_mean: 404.34
    cleaning_beam_agent-4_min: 143
    cleaning_beam_agent-5_max: 236
    cleaning_beam_agent-5_mean: 101.0
    cleaning_beam_agent-5_min: 17
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-05-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 824.9999999999782
  episode_reward_mean: 580.82
  episode_reward_min: 139.00000000000088
  episodes_this_iter: 96
  episodes_total: 11232
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19908.377
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005651136161759496
        entropy: 1.2638893127441406
        entropy_coeff: 0.0017600000137463212
        kl: 0.01228648517280817
        model: {}
        policy_loss: -0.032183874398469925
        total_loss: -0.030381986871361732
        vf_explained_var: 0.05919528007507324
        vf_loss: 15.69035816192627
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005651136161759496
        entropy: 1.1105549335479736
        entropy_coeff: 0.0017600000137463212
        kl: 0.014170603826642036
        model: {}
        policy_loss: -0.036317624151706696
        total_loss: -0.033860400319099426
        vf_explained_var: 0.05411316454410553
        vf_loss: 15.776803970336914
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005651136161759496
        entropy: 1.1604113578796387
        entropy_coeff: 0.0017600000137463212
        kl: 0.013986123725771904
        model: {}
        policy_loss: -0.03204083815217018
        total_loss: -0.029766030609607697
        vf_explained_var: 0.08856487274169922
        vf_loss: 15.19908332824707
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005651136161759496
        entropy: 0.8986404538154602
        entropy_coeff: 0.0017600000137463212
        kl: 0.010844685137271881
        model: {}
        policy_loss: -0.027463335543870926
        total_loss: -0.025597434490919113
        vf_explained_var: 0.23205682635307312
        vf_loss: 12.785659790039062
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005651136161759496
        entropy: 1.0633509159088135
        entropy_coeff: 0.0017600000137463212
        kl: 0.014212490059435368
        model: {}
        policy_loss: -0.03689110279083252
        total_loss: -0.034455448389053345
        vf_explained_var: 0.12212002277374268
        vf_loss: 14.646535873413086
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005651136161759496
        entropy: 0.936558187007904
        entropy_coeff: 0.0017600000137463212
        kl: 0.01372861210256815
        model: {}
        policy_loss: -0.03461252525448799
        total_loss: -0.032175213098526
        vf_explained_var: 0.19733059406280518
        vf_loss: 13.399283409118652
    load_time_ms: 15134.953
    num_steps_sampled: 11232000
    num_steps_trained: 11232000
    sample_time_ms: 92122.144
    update_time_ms: 41.449
  iterations_since_restore: 57
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.64714285714286
    ram_util_percent: 18.602857142857143
  pid: 24061
  policy_reward_max:
    agent-0: 137.50000000000003
    agent-1: 137.50000000000003
    agent-2: 137.50000000000003
    agent-3: 137.50000000000003
    agent-4: 137.50000000000003
    agent-5: 137.50000000000003
  policy_reward_mean:
    agent-0: 96.80333333333355
    agent-1: 96.80333333333355
    agent-2: 96.80333333333355
    agent-3: 96.80333333333355
    agent-4: 96.80333333333355
    agent-5: 96.80333333333355
  policy_reward_min:
    agent-0: 23.16666666666668
    agent-1: 23.16666666666668
    agent-2: 23.16666666666668
    agent-3: 23.16666666666668
    agent-4: 23.16666666666668
    agent-5: 23.16666666666668
  sampler_perf:
    mean_env_wait_ms: 24.617058846064293
    mean_inference_ms: 12.387627395323358
    mean_processing_ms: 51.2574386114235
  time_since_restore: 7939.983361244202
  time_this_iter_s: 146.7371428012848
  time_total_s: 17065.995175123215
  timestamp: 1637031937
  timesteps_since_restore: 5472000
  timesteps_this_iter: 96000
  timesteps_total: 11232000
  training_iteration: 117
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 33.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    117 |            17066 | 11232000 |   580.82 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 50
    apples_agent-0_mean: 7.47
    apples_agent-0_min: 0
    apples_agent-1_max: 100
    apples_agent-1_mean: 34.91
    apples_agent-1_min: 0
    apples_agent-2_max: 157
    apples_agent-2_mean: 5.05
    apples_agent-2_min: 0
    apples_agent-3_max: 206
    apples_agent-3_mean: 106.24
    apples_agent-3_min: 22
    apples_agent-4_max: 87
    apples_agent-4_mean: 3.7
    apples_agent-4_min: 0
    apples_agent-5_max: 142
    apples_agent-5_mean: 63.86
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 431
    cleaning_beam_agent-0_mean: 276.84
    cleaning_beam_agent-0_min: 109
    cleaning_beam_agent-1_max: 405
    cleaning_beam_agent-1_mean: 209.18
    cleaning_beam_agent-1_min: 89
    cleaning_beam_agent-2_max: 496
    cleaning_beam_agent-2_mean: 344.78
    cleaning_beam_agent-2_min: 157
    cleaning_beam_agent-3_max: 191
    cleaning_beam_agent-3_mean: 41.16
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 616
    cleaning_beam_agent-4_mean: 418.67
    cleaning_beam_agent-4_min: 190
    cleaning_beam_agent-5_max: 275
    cleaning_beam_agent-5_mean: 101.32
    cleaning_beam_agent-5_min: 13
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 7
    fire_beam_agent-4_mean: 0.08
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-07-44
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 849.9999999999961
  episode_reward_mean: 607.3899999999983
  episode_reward_min: 139.00000000000088
  episodes_this_iter: 96
  episodes_total: 11328
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19883.99
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005591231747530401
        entropy: 1.2512558698654175
        entropy_coeff: 0.0017600000137463212
        kl: 0.012821434997022152
        model: {}
        policy_loss: -0.03280160576105118
        total_loss: -0.030976708978414536
        vf_explained_var: 0.07546579837799072
        vf_loss: 14.628241539001465
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005591231747530401
        entropy: 1.1015565395355225
        entropy_coeff: 0.0017600000137463212
        kl: 0.015087795443832874
        model: {}
        policy_loss: -0.03748590871691704
        total_loss: -0.034893203526735306
        vf_explained_var: 0.042782530188560486
        vf_loss: 15.13882827758789
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005591231747530401
        entropy: 1.1465165615081787
        entropy_coeff: 0.0017600000137463212
        kl: 0.014293944463133812
        model: {}
        policy_loss: -0.030647121369838715
        total_loss: -0.02828781120479107
        vf_explained_var: 0.0400528609752655
        vf_loss: 15.183931350708008
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005591231747530401
        entropy: 0.8391397595405579
        entropy_coeff: 0.0017600000137463212
        kl: 0.010247736237943172
        model: {}
        policy_loss: -0.026058493182063103
        total_loss: -0.024184226989746094
        vf_explained_var: 0.17678315937519073
        vf_loss: 13.016011238098145
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005591231747530401
        entropy: 1.0447168350219727
        entropy_coeff: 0.0017600000137463212
        kl: 0.013902176171541214
        model: {}
        policy_loss: -0.03628351911902428
        total_loss: -0.03390652686357498
        vf_explained_var: 0.09256923198699951
        vf_loss: 14.352624893188477
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005591231747530401
        entropy: 0.9281125664710999
        entropy_coeff: 0.0017600000137463212
        kl: 0.012856882065534592
        model: {}
        policy_loss: -0.032952845096588135
        total_loss: -0.030708979815244675
        vf_explained_var: 0.17399431765079498
        vf_loss: 13.059670448303223
    load_time_ms: 15342.405
    num_steps_sampled: 11328000
    num_steps_trained: 11328000
    sample_time_ms: 92143.848
    update_time_ms: 27.673
  iterations_since_restore: 58
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.501104972375696
    ram_util_percent: 14.129834254143647
  pid: 24061
  policy_reward_max:
    agent-0: 141.6666666666668
    agent-1: 141.6666666666668
    agent-2: 141.6666666666668
    agent-3: 141.6666666666668
    agent-4: 141.6666666666668
    agent-5: 141.6666666666668
  policy_reward_mean:
    agent-0: 101.23166666666697
    agent-1: 101.23166666666697
    agent-2: 101.23166666666697
    agent-3: 101.23166666666697
    agent-4: 101.23166666666697
    agent-5: 101.23166666666697
  policy_reward_min:
    agent-0: 23.16666666666668
    agent-1: 23.16666666666668
    agent-2: 23.16666666666668
    agent-3: 23.16666666666668
    agent-4: 23.16666666666668
    agent-5: 23.16666666666668
  sampler_perf:
    mean_env_wait_ms: 24.618403704536508
    mean_inference_ms: 12.389906321471512
    mean_processing_ms: 51.27031671271794
  time_since_restore: 8067.121592760086
  time_this_iter_s: 127.1382315158844
  time_total_s: 17193.1334066391
  timestamp: 1637032064
  timesteps_since_restore: 5568000
  timesteps_this_iter: 96000
  timesteps_total: 11328000
  training_iteration: 118
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 26.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    118 |          17193.1 | 11328000 |   607.39 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 67
    apples_agent-0_mean: 8.45
    apples_agent-0_min: 0
    apples_agent-1_max: 94
    apples_agent-1_mean: 25.49
    apples_agent-1_min: 0
    apples_agent-2_max: 103
    apples_agent-2_mean: 7.14
    apples_agent-2_min: 0
    apples_agent-3_max: 194
    apples_agent-3_mean: 103.86
    apples_agent-3_min: 29
    apples_agent-4_max: 82
    apples_agent-4_mean: 3.12
    apples_agent-4_min: 0
    apples_agent-5_max: 132
    apples_agent-5_mean: 65.04
    apples_agent-5_min: 2
    cleaning_beam_agent-0_max: 468
    cleaning_beam_agent-0_mean: 270.23
    cleaning_beam_agent-0_min: 83
    cleaning_beam_agent-1_max: 394
    cleaning_beam_agent-1_mean: 211.6
    cleaning_beam_agent-1_min: 77
    cleaning_beam_agent-2_max: 570
    cleaning_beam_agent-2_mean: 332.67
    cleaning_beam_agent-2_min: 118
    cleaning_beam_agent-3_max: 127
    cleaning_beam_agent-3_mean: 46.54
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 567
    cleaning_beam_agent-4_mean: 410.35
    cleaning_beam_agent-4_min: 169
    cleaning_beam_agent-5_max: 295
    cleaning_beam_agent-5_mean: 94.8
    cleaning_beam_agent-5_min: 23
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.09
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 5
    fire_beam_agent-5_mean: 0.09
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-09-52
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 822.9999999999746
  episode_reward_mean: 587.5699999999988
  episode_reward_min: 180.9999999999981
  episodes_this_iter: 96
  episodes_total: 11424
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19900.326
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005531327915377915
        entropy: 1.2572896480560303
        entropy_coeff: 0.0017600000137463212
        kl: 0.01282187458127737
        model: {}
        policy_loss: -0.03323450684547424
        total_loss: -0.03128843009471893
        vf_explained_var: 0.0853174477815628
        vf_loss: 15.945342063903809
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005531327915377915
        entropy: 1.1219103336334229
        entropy_coeff: 0.0017600000137463212
        kl: 0.014709549956023693
        model: {}
        policy_loss: -0.036917053163051605
        total_loss: -0.03428816795349121
        vf_explained_var: 0.04705303907394409
        vf_loss: 16.615388870239258
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005531327915377915
        entropy: 1.1365752220153809
        entropy_coeff: 0.0017600000137463212
        kl: 0.012944837100803852
        model: {}
        policy_loss: -0.0336160808801651
        total_loss: -0.031509798020124435
        vf_explained_var: 0.12840619683265686
        vf_loss: 15.176870346069336
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005531327915377915
        entropy: 0.8750373125076294
        entropy_coeff: 0.0017600000137463212
        kl: 0.010792220942676067
        model: {}
        policy_loss: -0.0293994452804327
        total_loss: -0.027444951236248016
        vf_explained_var: 0.23291230201721191
        vf_loss: 13.361154556274414
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005531327915377915
        entropy: 1.04862380027771
        entropy_coeff: 0.0017600000137463212
        kl: 0.014895757660269737
        model: {}
        policy_loss: -0.0374075323343277
        total_loss: -0.03466078266501427
        vf_explained_var: 0.07377459108829498
        vf_loss: 16.131750106811523
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005531327915377915
        entropy: 0.9259377121925354
        entropy_coeff: 0.0017600000137463212
        kl: 0.013106727972626686
        model: {}
        policy_loss: -0.034227803349494934
        total_loss: -0.031899236142635345
        vf_explained_var: 0.23288768529891968
        vf_loss: 13.368692398071289
    load_time_ms: 15357.21
    num_steps_sampled: 11424000
    num_steps_trained: 11424000
    sample_time_ms: 92286.57
    update_time_ms: 26.97
  iterations_since_restore: 59
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.952486187845302
    ram_util_percent: 14.455801104972373
  pid: 24061
  policy_reward_max:
    agent-0: 137.16666666666708
    agent-1: 137.16666666666708
    agent-2: 137.16666666666708
    agent-3: 137.16666666666708
    agent-4: 137.16666666666708
    agent-5: 137.16666666666708
  policy_reward_mean:
    agent-0: 97.9283333333336
    agent-1: 97.9283333333336
    agent-2: 97.9283333333336
    agent-3: 97.9283333333336
    agent-4: 97.9283333333336
    agent-5: 97.9283333333336
  policy_reward_min:
    agent-0: 30.16666666666673
    agent-1: 30.16666666666673
    agent-2: 30.16666666666673
    agent-3: 30.16666666666673
    agent-4: 30.16666666666673
    agent-5: 30.16666666666673
  sampler_perf:
    mean_env_wait_ms: 24.62137459291679
    mean_inference_ms: 12.392818249234033
    mean_processing_ms: 51.28677961520968
  time_since_restore: 8194.206762552261
  time_this_iter_s: 127.0851697921753
  time_total_s: 17320.218576431274
  timestamp: 1637032192
  timesteps_since_restore: 5664000
  timesteps_this_iter: 96000
  timesteps_total: 11424000
  training_iteration: 119
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 26.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    119 |          17320.2 | 11424000 |   587.57 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 98
    apples_agent-0_mean: 6.66
    apples_agent-0_min: 0
    apples_agent-1_max: 88
    apples_agent-1_mean: 25.43
    apples_agent-1_min: 0
    apples_agent-2_max: 67
    apples_agent-2_mean: 7.22
    apples_agent-2_min: 0
    apples_agent-3_max: 153
    apples_agent-3_mean: 98.47
    apples_agent-3_min: 33
    apples_agent-4_max: 74
    apples_agent-4_mean: 3.35
    apples_agent-4_min: 0
    apples_agent-5_max: 118
    apples_agent-5_mean: 62.47
    apples_agent-5_min: 20
    cleaning_beam_agent-0_max: 520
    cleaning_beam_agent-0_mean: 283.76
    cleaning_beam_agent-0_min: 156
    cleaning_beam_agent-1_max: 390
    cleaning_beam_agent-1_mean: 234.74
    cleaning_beam_agent-1_min: 112
    cleaning_beam_agent-2_max: 489
    cleaning_beam_agent-2_mean: 338.35
    cleaning_beam_agent-2_min: 71
    cleaning_beam_agent-3_max: 187
    cleaning_beam_agent-3_mean: 46.97
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 556
    cleaning_beam_agent-4_mean: 411.31
    cleaning_beam_agent-4_min: 177
    cleaning_beam_agent-5_max: 274
    cleaning_beam_agent-5_mean: 96.23
    cleaning_beam_agent-5_min: 16
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.04
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-11-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 803.9999999999817
  episode_reward_mean: 570.5099999999995
  episode_reward_min: 155.00000000000082
  episodes_this_iter: 96
  episodes_total: 11520
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19895.501
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005471424083225429
        entropy: 1.2250491380691528
        entropy_coeff: 0.0017600000137463212
        kl: 0.012315783649682999
        model: {}
        policy_loss: -0.033530354499816895
        total_loss: -0.031672343611717224
        vf_explained_var: 0.04003836214542389
        vf_loss: 15.509404182434082
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005471424083225429
        entropy: 1.1208475828170776
        entropy_coeff: 0.0017600000137463212
        kl: 0.014077840372920036
        model: {}
        policy_loss: -0.0368482731282711
        total_loss: -0.0344705656170845
        vf_explained_var: 0.050521597266197205
        vf_loss: 15.34835147857666
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005471424083225429
        entropy: 1.1262071132659912
        entropy_coeff: 0.0017600000137463212
        kl: 0.013609427958726883
        model: {}
        policy_loss: -0.03232987970113754
        total_loss: -0.030184468254446983
        vf_explained_var: 0.1297163963317871
        vf_loss: 14.056496620178223
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005471424083225429
        entropy: 0.8825381994247437
        entropy_coeff: 0.0017600000137463212
        kl: 0.011263608932495117
        model: {}
        policy_loss: -0.02979341521859169
        total_loss: -0.027811024338006973
        vf_explained_var: 0.20549224317073822
        vf_loss: 12.829337120056152
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005471424083225429
        entropy: 1.054485559463501
        entropy_coeff: 0.0017600000137463212
        kl: 0.013632629066705704
        model: {}
        policy_loss: -0.035790786147117615
        total_loss: -0.03348592668771744
        vf_explained_var: 0.11247624456882477
        vf_loss: 14.342310905456543
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005471424083225429
        entropy: 0.9298661351203918
        entropy_coeff: 0.0017600000137463212
        kl: 0.01259175967425108
        model: {}
        policy_loss: -0.034574463963508606
        total_loss: -0.032441604882478714
        vf_explained_var: 0.22548474371433258
        vf_loss: 12.510763168334961
    load_time_ms: 15385.925
    num_steps_sampled: 11520000
    num_steps_trained: 11520000
    sample_time_ms: 92435.55
    update_time_ms: 26.893
  iterations_since_restore: 60
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.09333333333333
    ram_util_percent: 14.456666666666665
  pid: 24061
  policy_reward_max:
    agent-0: 134.00000000000057
    agent-1: 134.00000000000057
    agent-2: 134.00000000000057
    agent-3: 134.00000000000057
    agent-4: 134.00000000000057
    agent-5: 134.00000000000057
  policy_reward_mean:
    agent-0: 95.08500000000025
    agent-1: 95.08500000000025
    agent-2: 95.08500000000025
    agent-3: 95.08500000000025
    agent-4: 95.08500000000025
    agent-5: 95.08500000000025
  policy_reward_min:
    agent-0: 25.83333333333336
    agent-1: 25.83333333333336
    agent-2: 25.83333333333336
    agent-3: 25.83333333333336
    agent-4: 25.83333333333336
    agent-5: 25.83333333333336
  sampler_perf:
    mean_env_wait_ms: 24.627636115480705
    mean_inference_ms: 12.395595718350778
    mean_processing_ms: 51.302859924163556
  time_since_restore: 8320.03487610817
  time_this_iter_s: 125.8281135559082
  time_total_s: 17446.046689987183
  timestamp: 1637032318
  timesteps_since_restore: 5760000
  timesteps_this_iter: 96000
  timesteps_total: 11520000
  training_iteration: 120
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 26.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    120 |            17446 | 11520000 |   570.51 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 93
    apples_agent-0_mean: 11.76
    apples_agent-0_min: 0
    apples_agent-1_max: 73
    apples_agent-1_mean: 27.39
    apples_agent-1_min: 0
    apples_agent-2_max: 384
    apples_agent-2_mean: 10.07
    apples_agent-2_min: 0
    apples_agent-3_max: 185
    apples_agent-3_mean: 100.93
    apples_agent-3_min: 44
    apples_agent-4_max: 168
    apples_agent-4_mean: 2.2
    apples_agent-4_min: 0
    apples_agent-5_max: 127
    apples_agent-5_mean: 59.87
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 488
    cleaning_beam_agent-0_mean: 277.41
    cleaning_beam_agent-0_min: 119
    cleaning_beam_agent-1_max: 432
    cleaning_beam_agent-1_mean: 234.93
    cleaning_beam_agent-1_min: 99
    cleaning_beam_agent-2_max: 547
    cleaning_beam_agent-2_mean: 331.49
    cleaning_beam_agent-2_min: 48
    cleaning_beam_agent-3_max: 144
    cleaning_beam_agent-3_mean: 41.35
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 594
    cleaning_beam_agent-4_mean: 414.83
    cleaning_beam_agent-4_min: 245
    cleaning_beam_agent-5_max: 339
    cleaning_beam_agent-5_mean: 101.07
    cleaning_beam_agent-5_min: 19
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-14-07
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 848.9999999999815
  episode_reward_mean: 589.9799999999991
  episode_reward_min: 260.9999999999969
  episodes_this_iter: 96
  episodes_total: 11616
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19899.409
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005411520251072943
        entropy: 1.2273311614990234
        entropy_coeff: 0.0017600000137463212
        kl: 0.012829111889004707
        model: {}
        policy_loss: -0.03341219574213028
        total_loss: -0.03150991350412369
        vf_explained_var: 0.07465170323848724
        vf_loss: 14.965715408325195
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005411520251072943
        entropy: 1.1110596656799316
        entropy_coeff: 0.0017600000137463212
        kl: 0.014896705746650696
        model: {}
        policy_loss: -0.036409977823495865
        total_loss: -0.0338621623814106
        vf_explained_var: 0.057094886898994446
        vf_loss: 15.239396095275879
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005411520251072943
        entropy: 1.1195673942565918
        entropy_coeff: 0.0017600000137463212
        kl: 0.012884233146905899
        model: {}
        policy_loss: -0.03248370438814163
        total_loss: -0.030384954065084457
        vf_explained_var: 0.07693709433078766
        vf_loss: 14.923458099365234
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005411520251072943
        entropy: 0.8645620346069336
        entropy_coeff: 0.0017600000137463212
        kl: 0.010935250669717789
        model: {}
        policy_loss: -0.028384916484355927
        total_loss: -0.02647285722196102
        vf_explained_var: 0.22857621312141418
        vf_loss: 12.466350555419922
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005411520251072943
        entropy: 1.0433213710784912
        entropy_coeff: 0.0017600000137463212
        kl: 0.01363417599350214
        model: {}
        policy_loss: -0.036021433770656586
        total_loss: -0.03361761197447777
        vf_explained_var: 0.06397745013237
        vf_loss: 15.132359504699707
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005411520251072943
        entropy: 0.9109989404678345
        entropy_coeff: 0.0017600000137463212
        kl: 0.012252057902514935
        model: {}
        policy_loss: -0.03312310948967934
        total_loss: -0.030921466648578644
        vf_explained_var: 0.16224104166030884
        vf_loss: 13.54585075378418
    load_time_ms: 15393.455
    num_steps_sampled: 11616000
    num_steps_trained: 11616000
    sample_time_ms: 92804.359
    update_time_ms: 27.056
  iterations_since_restore: 61
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.67663043478261
    ram_util_percent: 14.447282608695653
  pid: 24061
  policy_reward_max:
    agent-0: 141.50000000000003
    agent-1: 141.50000000000003
    agent-2: 141.50000000000003
    agent-3: 141.50000000000003
    agent-4: 141.50000000000003
    agent-5: 141.50000000000003
  policy_reward_mean:
    agent-0: 98.33000000000025
    agent-1: 98.33000000000025
    agent-2: 98.33000000000025
    agent-3: 98.33000000000025
    agent-4: 98.33000000000025
    agent-5: 98.33000000000025
  policy_reward_min:
    agent-0: 43.50000000000004
    agent-1: 43.50000000000004
    agent-2: 43.50000000000004
    agent-3: 43.50000000000004
    agent-4: 43.50000000000004
    agent-5: 43.50000000000004
  sampler_perf:
    mean_env_wait_ms: 24.631185602470588
    mean_inference_ms: 12.396736941129229
    mean_processing_ms: 51.30987323974771
  time_since_restore: 8448.843910217285
  time_this_iter_s: 128.8090341091156
  time_total_s: 17574.8557240963
  timestamp: 1637032447
  timesteps_since_restore: 5856000
  timesteps_this_iter: 96000
  timesteps_total: 11616000
  training_iteration: 121
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 26.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    121 |          17574.9 | 11616000 |   589.98 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 121
    apples_agent-0_mean: 9.72
    apples_agent-0_min: 0
    apples_agent-1_max: 118
    apples_agent-1_mean: 28.88
    apples_agent-1_min: 0
    apples_agent-2_max: 99
    apples_agent-2_mean: 6.56
    apples_agent-2_min: 0
    apples_agent-3_max: 198
    apples_agent-3_mean: 109.35
    apples_agent-3_min: 39
    apples_agent-4_max: 61
    apples_agent-4_mean: 2.81
    apples_agent-4_min: 0
    apples_agent-5_max: 146
    apples_agent-5_mean: 64.23
    apples_agent-5_min: 6
    cleaning_beam_agent-0_max: 422
    cleaning_beam_agent-0_mean: 269.57
    cleaning_beam_agent-0_min: 124
    cleaning_beam_agent-1_max: 429
    cleaning_beam_agent-1_mean: 235.26
    cleaning_beam_agent-1_min: 96
    cleaning_beam_agent-2_max: 503
    cleaning_beam_agent-2_mean: 340.72
    cleaning_beam_agent-2_min: 106
    cleaning_beam_agent-3_max: 181
    cleaning_beam_agent-3_mean: 43.5
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 594
    cleaning_beam_agent-4_mean: 410.72
    cleaning_beam_agent-4_min: 137
    cleaning_beam_agent-5_max: 339
    cleaning_beam_agent-5_mean: 95.49
    cleaning_beam_agent-5_min: 16
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-16-15
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 888.9999999999695
  episode_reward_mean: 600.0099999999983
  episode_reward_min: 277.9999999999989
  episodes_this_iter: 96
  episodes_total: 11712
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19917.195
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005351615836843848
        entropy: 1.2382913827896118
        entropy_coeff: 0.0017600000137463212
        kl: 0.012196600437164307
        model: {}
        policy_loss: -0.033554837107658386
        total_loss: -0.031777963042259216
        vf_explained_var: 0.06190057098865509
        vf_loss: 15.169452667236328
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005351615836843848
        entropy: 1.1145522594451904
        entropy_coeff: 0.0017600000137463212
        kl: 0.014057984575629234
        model: {}
        policy_loss: -0.03556956350803375
        total_loss: -0.03319978713989258
        vf_explained_var: 0.05884075164794922
        vf_loss: 15.19797134399414
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005351615836843848
        entropy: 1.124405860900879
        entropy_coeff: 0.0017600000137463212
        kl: 0.012549815699458122
        model: {}
        policy_loss: -0.032837413251399994
        total_loss: -0.030781032517552376
        vf_explained_var: 0.05661168694496155
        vf_loss: 15.253705978393555
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005351615836843848
        entropy: 0.8664873838424683
        entropy_coeff: 0.0017600000137463212
        kl: 0.011110806837677956
        model: {}
        policy_loss: -0.027889572083950043
        total_loss: -0.025902993977069855
        vf_explained_var: 0.20300306379795074
        vf_loss: 12.894351959228516
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005351615836843848
        entropy: 1.0488831996917725
        entropy_coeff: 0.0017600000137463212
        kl: 0.014053437858819962
        model: {}
        policy_loss: -0.03521514683961868
        total_loss: -0.03279288858175278
        vf_explained_var: 0.09877921640872955
        vf_loss: 14.576043128967285
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005351615836843848
        entropy: 0.9066632390022278
        entropy_coeff: 0.0017600000137463212
        kl: 0.012314331717789173
        model: {}
        policy_loss: -0.032961390912532806
        total_loss: -0.030789121985435486
        vf_explained_var: 0.19227467477321625
        vf_loss: 13.051291465759277
    load_time_ms: 15407.594
    num_steps_sampled: 11712000
    num_steps_trained: 11712000
    sample_time_ms: 92900.57
    update_time_ms: 28.005
  iterations_since_restore: 62
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.071978021978023
    ram_util_percent: 14.432967032967033
  pid: 24061
  policy_reward_max:
    agent-0: 148.1666666666664
    agent-1: 148.1666666666664
    agent-2: 148.1666666666664
    agent-3: 148.1666666666664
    agent-4: 148.1666666666664
    agent-5: 148.1666666666664
  policy_reward_mean:
    agent-0: 100.00166666666694
    agent-1: 100.00166666666694
    agent-2: 100.00166666666694
    agent-3: 100.00166666666694
    agent-4: 100.00166666666694
    agent-5: 100.00166666666694
  policy_reward_min:
    agent-0: 46.33333333333322
    agent-1: 46.33333333333322
    agent-2: 46.33333333333322
    agent-3: 46.33333333333322
    agent-4: 46.33333333333322
    agent-5: 46.33333333333322
  sampler_perf:
    mean_env_wait_ms: 24.63848900745539
    mean_inference_ms: 12.400454702330881
    mean_processing_ms: 51.330697412987256
  time_since_restore: 8576.360065937042
  time_this_iter_s: 127.51615571975708
  time_total_s: 17702.371879816055
  timestamp: 1637032575
  timesteps_since_restore: 5952000
  timesteps_this_iter: 96000
  timesteps_total: 11712000
  training_iteration: 122
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 26.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    122 |          17702.4 | 11712000 |   600.01 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 82
    apples_agent-0_mean: 8.65
    apples_agent-0_min: 0
    apples_agent-1_max: 118
    apples_agent-1_mean: 26.97
    apples_agent-1_min: 0
    apples_agent-2_max: 153
    apples_agent-2_mean: 6.67
    apples_agent-2_min: 0
    apples_agent-3_max: 168
    apples_agent-3_mean: 102.79
    apples_agent-3_min: 41
    apples_agent-4_max: 33
    apples_agent-4_mean: 0.63
    apples_agent-4_min: 0
    apples_agent-5_max: 167
    apples_agent-5_mean: 65.41
    apples_agent-5_min: 1
    cleaning_beam_agent-0_max: 410
    cleaning_beam_agent-0_mean: 269.66
    cleaning_beam_agent-0_min: 151
    cleaning_beam_agent-1_max: 401
    cleaning_beam_agent-1_mean: 237.58
    cleaning_beam_agent-1_min: 111
    cleaning_beam_agent-2_max: 509
    cleaning_beam_agent-2_mean: 344.83
    cleaning_beam_agent-2_min: 164
    cleaning_beam_agent-3_max: 150
    cleaning_beam_agent-3_mean: 45.65
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 591
    cleaning_beam_agent-4_mean: 421.19
    cleaning_beam_agent-4_min: 244
    cleaning_beam_agent-5_max: 308
    cleaning_beam_agent-5_mean: 88.14
    cleaning_beam_agent-5_min: 21
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-18-21
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 835.9999999999882
  episode_reward_mean: 603.9699999999995
  episode_reward_min: 245.99999999999642
  episodes_this_iter: 96
  episodes_total: 11808
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19964.08
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005291712004691362
        entropy: 1.230743169784546
        entropy_coeff: 0.0017600000137463212
        kl: 0.012575549073517323
        model: {}
        policy_loss: -0.03427991643548012
        total_loss: -0.03253600001335144
        vf_explained_var: 0.055103808641433716
        vf_loss: 13.949151992797852
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005291712004691362
        entropy: 1.1138172149658203
        entropy_coeff: 0.0017600000137463212
        kl: 0.014391405507922173
        model: {}
        policy_loss: -0.03624719753861427
        total_loss: -0.033881958574056625
        vf_explained_var: 0.02008315920829773
        vf_loss: 14.472766876220703
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005291712004691362
        entropy: 1.1301283836364746
        entropy_coeff: 0.0017600000137463212
        kl: 0.013015888631343842
        model: {}
        policy_loss: -0.03071805275976658
        total_loss: -0.028792476281523705
        vf_explained_var: 0.11138531565666199
        vf_loss: 13.114280700683594
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005291712004691362
        entropy: 0.8478379845619202
        entropy_coeff: 0.0017600000137463212
        kl: 0.010423519648611546
        model: {}
        policy_loss: -0.027564333751797676
        total_loss: -0.0257014911621809
        vf_explained_var: 0.14008674025535583
        vf_loss: 12.703368186950684
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005291712004691362
        entropy: 1.0333378314971924
        entropy_coeff: 0.0017600000137463212
        kl: 0.013606781139969826
        model: {}
        policy_loss: -0.034044794738292694
        total_loss: -0.0317528136074543
        vf_explained_var: 0.05871441960334778
        vf_loss: 13.892993927001953
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005291712004691362
        entropy: 0.8995128273963928
        entropy_coeff: 0.0017600000137463212
        kl: 0.012543848715722561
        model: {}
        policy_loss: -0.03282715380191803
        total_loss: -0.03067104145884514
        vf_explained_var: 0.16673028469085693
        vf_loss: 12.304813385009766
    load_time_ms: 15438.681
    num_steps_sampled: 11808000
    num_steps_trained: 11808000
    sample_time_ms: 92904.53
    update_time_ms: 28.712
  iterations_since_restore: 63
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.073743016759774
    ram_util_percent: 14.391061452513966
  pid: 24061
  policy_reward_max:
    agent-0: 139.33333333333368
    agent-1: 139.33333333333368
    agent-2: 139.33333333333368
    agent-3: 139.33333333333368
    agent-4: 139.33333333333368
    agent-5: 139.33333333333368
  policy_reward_mean:
    agent-0: 100.66166666666695
    agent-1: 100.66166666666695
    agent-2: 100.66166666666695
    agent-3: 100.66166666666695
    agent-4: 100.66166666666695
    agent-5: 100.66166666666695
  policy_reward_min:
    agent-0: 40.99999999999998
    agent-1: 40.99999999999998
    agent-2: 40.99999999999998
    agent-3: 40.99999999999998
    agent-4: 40.99999999999998
    agent-5: 40.99999999999998
  sampler_perf:
    mean_env_wait_ms: 24.640361189627175
    mean_inference_ms: 12.401668189208056
    mean_processing_ms: 51.34114501757238
  time_since_restore: 8702.249173164368
  time_this_iter_s: 125.88910722732544
  time_total_s: 17828.26098704338
  timestamp: 1637032701
  timesteps_since_restore: 6048000
  timesteps_this_iter: 96000
  timesteps_total: 11808000
  training_iteration: 123
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 26.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    123 |          17828.3 | 11808000 |   603.97 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 114
    apples_agent-0_mean: 10.4
    apples_agent-0_min: 0
    apples_agent-1_max: 99
    apples_agent-1_mean: 27.12
    apples_agent-1_min: 0
    apples_agent-2_max: 230
    apples_agent-2_mean: 9.95
    apples_agent-2_min: 0
    apples_agent-3_max: 164
    apples_agent-3_mean: 96.97
    apples_agent-3_min: 27
    apples_agent-4_max: 26
    apples_agent-4_mean: 0.75
    apples_agent-4_min: 0
    apples_agent-5_max: 137
    apples_agent-5_mean: 66.41
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 397
    cleaning_beam_agent-0_mean: 268.23
    cleaning_beam_agent-0_min: 114
    cleaning_beam_agent-1_max: 369
    cleaning_beam_agent-1_mean: 235.08
    cleaning_beam_agent-1_min: 92
    cleaning_beam_agent-2_max: 529
    cleaning_beam_agent-2_mean: 348.79
    cleaning_beam_agent-2_min: 133
    cleaning_beam_agent-3_max: 121
    cleaning_beam_agent-3_mean: 45.54
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 565
    cleaning_beam_agent-4_mean: 403.26
    cleaning_beam_agent-4_min: 201
    cleaning_beam_agent-5_max: 317
    cleaning_beam_agent-5_mean: 91.46
    cleaning_beam_agent-5_min: 24
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-20-28
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 877.9999999999939
  episode_reward_mean: 601.6599999999987
  episode_reward_min: 158.0000000000004
  episodes_this_iter: 96
  episodes_total: 11904
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19924.24
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005231808172538877
        entropy: 1.2263295650482178
        entropy_coeff: 0.0017600000137463212
        kl: 0.012448694556951523
        model: {}
        policy_loss: -0.033966805785894394
        total_loss: -0.03215299919247627
        vf_explained_var: 0.07313135266304016
        vf_loss: 14.82414722442627
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005231808172538877
        entropy: 1.1224384307861328
        entropy_coeff: 0.0017600000137463212
        kl: 0.01435103826224804
        model: {}
        policy_loss: -0.03600332885980606
        total_loss: -0.03358732908964157
        vf_explained_var: 0.04814215004444122
        vf_loss: 15.212831497192383
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005231808172538877
        entropy: 1.1037676334381104
        entropy_coeff: 0.0017600000137463212
        kl: 0.013806484639644623
        model: {}
        policy_loss: -0.032239172607660294
        total_loss: -0.029984842985868454
        vf_explained_var: 0.1018291562795639
        vf_loss: 14.356640815734863
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005231808172538877
        entropy: 0.8569600582122803
        entropy_coeff: 0.0017600000137463212
        kl: 0.011838359758257866
        model: {}
        policy_loss: -0.028744786977767944
        total_loss: -0.026602664962410927
        vf_explained_var: 0.1971072405576706
        vf_loss: 12.827016830444336
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005231808172538877
        entropy: 1.0578469038009644
        entropy_coeff: 0.0017600000137463212
        kl: 0.01379755511879921
        model: {}
        policy_loss: -0.033638570457696915
        total_loss: -0.031289417296648026
        vf_explained_var: 0.09173232316970825
        vf_loss: 14.514511108398438
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005231808172538877
        entropy: 0.9083747267723083
        entropy_coeff: 0.0017600000137463212
        kl: 0.01224469393491745
        model: {}
        policy_loss: -0.03343493491411209
        total_loss: -0.031268686056137085
        vf_explained_var: 0.17667777836322784
        vf_loss: 13.160520553588867
    load_time_ms: 15470.68
    num_steps_sampled: 11904000
    num_steps_trained: 11904000
    sample_time_ms: 93245.982
    update_time_ms: 28.497
  iterations_since_restore: 64
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.77307692307692
    ram_util_percent: 14.51373626373626
  pid: 24061
  policy_reward_max:
    agent-0: 146.3333333333334
    agent-1: 146.3333333333334
    agent-2: 146.3333333333334
    agent-3: 146.3333333333334
    agent-4: 146.3333333333334
    agent-5: 146.3333333333334
  policy_reward_mean:
    agent-0: 100.27666666666691
    agent-1: 100.27666666666691
    agent-2: 100.27666666666691
    agent-3: 100.27666666666691
    agent-4: 100.27666666666691
    agent-5: 100.27666666666691
  policy_reward_min:
    agent-0: 26.33333333333334
    agent-1: 26.33333333333334
    agent-2: 26.33333333333334
    agent-3: 26.33333333333334
    agent-4: 26.33333333333334
    agent-5: 26.33333333333334
  sampler_perf:
    mean_env_wait_ms: 24.644433817047844
    mean_inference_ms: 12.402656404631761
    mean_processing_ms: 51.35192519696718
  time_since_restore: 8829.842675924301
  time_this_iter_s: 127.59350275993347
  time_total_s: 17955.854489803314
  timestamp: 1637032828
  timesteps_since_restore: 6144000
  timesteps_this_iter: 96000
  timesteps_total: 11904000
  training_iteration: 124
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 26.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    124 |          17955.9 | 11904000 |   601.66 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 96
    apples_agent-0_mean: 10.04
    apples_agent-0_min: 0
    apples_agent-1_max: 97
    apples_agent-1_mean: 27.25
    apples_agent-1_min: 0
    apples_agent-2_max: 62
    apples_agent-2_mean: 3.68
    apples_agent-2_min: 0
    apples_agent-3_max: 189
    apples_agent-3_mean: 104.45
    apples_agent-3_min: 28
    apples_agent-4_max: 38
    apples_agent-4_mean: 2.07
    apples_agent-4_min: 0
    apples_agent-5_max: 116
    apples_agent-5_mean: 66.3
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 431
    cleaning_beam_agent-0_mean: 270.52
    cleaning_beam_agent-0_min: 90
    cleaning_beam_agent-1_max: 390
    cleaning_beam_agent-1_mean: 223.11
    cleaning_beam_agent-1_min: 96
    cleaning_beam_agent-2_max: 598
    cleaning_beam_agent-2_mean: 358.13
    cleaning_beam_agent-2_min: 131
    cleaning_beam_agent-3_max: 186
    cleaning_beam_agent-3_mean: 49.26
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 617
    cleaning_beam_agent-4_mean: 419.12
    cleaning_beam_agent-4_min: 201
    cleaning_beam_agent-5_max: 338
    cleaning_beam_agent-5_mean: 94.81
    cleaning_beam_agent-5_min: 30
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-22-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 845.999999999981
  episode_reward_mean: 610.329999999998
  episode_reward_min: 178.99999999999864
  episodes_this_iter: 96
  episodes_total: 12000
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19894.241
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005171903758309782
        entropy: 1.2234703302383423
        entropy_coeff: 0.0017600000137463212
        kl: 0.012718993239104748
        model: {}
        policy_loss: -0.032901883125305176
        total_loss: -0.031025651842355728
        vf_explained_var: 0.0869251936674118
        vf_loss: 14.85744857788086
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005171903758309782
        entropy: 1.1379872560501099
        entropy_coeff: 0.0017600000137463212
        kl: 0.014199594035744667
        model: {}
        policy_loss: -0.03627127408981323
        total_loss: -0.03385671228170395
        vf_explained_var: 0.03095938265323639
        vf_loss: 15.775012016296387
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005171903758309782
        entropy: 1.123724341392517
        entropy_coeff: 0.0017600000137463212
        kl: 0.013382021337747574
        model: {}
        policy_loss: -0.032242078334093094
        total_loss: -0.030041005462408066
        vf_explained_var: 0.07705293595790863
        vf_loss: 15.024240493774414
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005171903758309782
        entropy: 0.8560800552368164
        entropy_coeff: 0.0017600000137463212
        kl: 0.010665103793144226
        model: {}
        policy_loss: -0.02871062606573105
        total_loss: -0.026831787079572678
        vf_explained_var: 0.23038341104984283
        vf_loss: 12.525228500366211
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005171903758309782
        entropy: 1.0577869415283203
        entropy_coeff: 0.0017600000137463212
        kl: 0.013680722564458847
        model: {}
        policy_loss: -0.035086240619421005
        total_loss: -0.032809436321258545
        vf_explained_var: 0.13800577819347382
        vf_loss: 14.023629188537598
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005171903758309782
        entropy: 0.8902000188827515
        entropy_coeff: 0.0017600000137463212
        kl: 0.012554271146655083
        model: {}
        policy_loss: -0.03431107476353645
        total_loss: -0.03204250708222389
        vf_explained_var: 0.1862751692533493
        vf_loss: 13.244659423828125
    load_time_ms: 15500.183
    num_steps_sampled: 12000000
    num_steps_trained: 12000000
    sample_time_ms: 93416.493
    update_time_ms: 28.077
  iterations_since_restore: 65
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.92513661202186
    ram_util_percent: 14.447540983606556
  pid: 24061
  policy_reward_max:
    agent-0: 141.00000000000023
    agent-1: 141.00000000000023
    agent-2: 141.00000000000023
    agent-3: 141.00000000000023
    agent-4: 141.00000000000023
    agent-5: 141.00000000000023
  policy_reward_mean:
    agent-0: 101.72166666666698
    agent-1: 101.72166666666698
    agent-2: 101.72166666666698
    agent-3: 101.72166666666698
    agent-4: 101.72166666666698
    agent-5: 101.72166666666698
  policy_reward_min:
    agent-0: 29.8333333333334
    agent-1: 29.8333333333334
    agent-2: 29.8333333333334
    agent-3: 29.8333333333334
    agent-4: 29.8333333333334
    agent-5: 29.8333333333334
  sampler_perf:
    mean_env_wait_ms: 24.64907612538527
    mean_inference_ms: 12.40462881661867
    mean_processing_ms: 51.36790724506877
  time_since_restore: 8957.486932992935
  time_this_iter_s: 127.64425706863403
  time_total_s: 18083.49874687195
  timestamp: 1637032956
  timesteps_since_restore: 6240000
  timesteps_this_iter: 96000
  timesteps_total: 12000000
  training_iteration: 125
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 26.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    125 |          18083.5 | 12000000 |   610.33 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 90
    apples_agent-0_mean: 9.8
    apples_agent-0_min: 0
    apples_agent-1_max: 78
    apples_agent-1_mean: 23.88
    apples_agent-1_min: 0
    apples_agent-2_max: 118
    apples_agent-2_mean: 10.68
    apples_agent-2_min: 0
    apples_agent-3_max: 155
    apples_agent-3_mean: 96.08
    apples_agent-3_min: 36
    apples_agent-4_max: 17
    apples_agent-4_mean: 0.81
    apples_agent-4_min: 0
    apples_agent-5_max: 141
    apples_agent-5_mean: 66.2
    apples_agent-5_min: 15
    cleaning_beam_agent-0_max: 411
    cleaning_beam_agent-0_mean: 278.56
    cleaning_beam_agent-0_min: 133
    cleaning_beam_agent-1_max: 364
    cleaning_beam_agent-1_mean: 212.91
    cleaning_beam_agent-1_min: 91
    cleaning_beam_agent-2_max: 542
    cleaning_beam_agent-2_mean: 336.21
    cleaning_beam_agent-2_min: 127
    cleaning_beam_agent-3_max: 159
    cleaning_beam_agent-3_mean: 51.94
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 547
    cleaning_beam_agent-4_mean: 404.45
    cleaning_beam_agent-4_min: 259
    cleaning_beam_agent-5_max: 231
    cleaning_beam_agent-5_mean: 97.26
    cleaning_beam_agent-5_min: 27
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-24-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 891.9999999999856
  episode_reward_mean: 591.3800000000015
  episode_reward_min: 277.99999999999875
  episodes_this_iter: 96
  episodes_total: 12096
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19844.348
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005111999926157296
        entropy: 1.2210112810134888
        entropy_coeff: 0.0017600000137463212
        kl: 0.012842012569308281
        model: {}
        policy_loss: -0.03236647695302963
        total_loss: -0.030606577172875404
        vf_explained_var: 0.046588122844696045
        vf_loss: 13.40473747253418
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005111999926157296
        entropy: 1.1174647808074951
        entropy_coeff: 0.0017600000137463212
        kl: 0.01429179310798645
        model: {}
        policy_loss: -0.03749268501996994
        total_loss: -0.035194918513298035
        vf_explained_var: 0.0006834864616394043
        vf_loss: 14.061505317687988
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005111999926157296
        entropy: 1.101166844367981
        entropy_coeff: 0.0017600000137463212
        kl: 0.012473677285015583
        model: {}
        policy_loss: -0.03147595375776291
        total_loss: -0.029630014672875404
        vf_explained_var: 0.08275260031223297
        vf_loss: 12.892534255981445
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005111999926157296
        entropy: 0.8511285185813904
        entropy_coeff: 0.0017600000137463212
        kl: 0.011303018778562546
        model: {}
        policy_loss: -0.028208797797560692
        total_loss: -0.026288462802767754
        vf_explained_var: 0.17649303376674652
        vf_loss: 11.577141761779785
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005111999926157296
        entropy: 1.0542985200881958
        entropy_coeff: 0.0017600000137463212
        kl: 0.013457775115966797
        model: {}
        policy_loss: -0.03396283835172653
        total_loss: -0.03185206279158592
        vf_explained_var: 0.09274816513061523
        vf_loss: 12.747902870178223
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005111999926157296
        entropy: 0.9115516543388367
        entropy_coeff: 0.0017600000137463212
        kl: 0.012259602546691895
        model: {}
        policy_loss: -0.033781155943870544
        total_loss: -0.03174547851085663
        vf_explained_var: 0.15433986485004425
        vf_loss: 11.88088607788086
    load_time_ms: 15685.881
    num_steps_sampled: 12096000
    num_steps_trained: 12096000
    sample_time_ms: 93593.113
    update_time_ms: 28.418
  iterations_since_restore: 66
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.827868852459016
    ram_util_percent: 14.477049180327867
  pid: 24061
  policy_reward_max:
    agent-0: 148.6666666666667
    agent-1: 148.6666666666667
    agent-2: 148.6666666666667
    agent-3: 148.6666666666667
    agent-4: 148.6666666666667
    agent-5: 148.6666666666667
  policy_reward_mean:
    agent-0: 98.56333333333359
    agent-1: 98.56333333333359
    agent-2: 98.56333333333359
    agent-3: 98.56333333333359
    agent-4: 98.56333333333359
    agent-5: 98.56333333333359
  policy_reward_min:
    agent-0: 46.33333333333325
    agent-1: 46.33333333333325
    agent-2: 46.33333333333325
    agent-3: 46.33333333333325
    agent-4: 46.33333333333325
    agent-5: 46.33333333333325
  sampler_perf:
    mean_env_wait_ms: 24.653122064920048
    mean_inference_ms: 12.407389150370559
    mean_processing_ms: 51.37865176300851
  time_since_restore: 9085.909081697464
  time_this_iter_s: 128.4221487045288
  time_total_s: 18211.920895576477
  timestamp: 1637033085
  timesteps_since_restore: 6336000
  timesteps_this_iter: 96000
  timesteps_total: 12096000
  training_iteration: 126
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 26.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    126 |          18211.9 | 12096000 |   591.38 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 243
    apples_agent-0_mean: 11.07
    apples_agent-0_min: 0
    apples_agent-1_max: 79
    apples_agent-1_mean: 24.66
    apples_agent-1_min: 0
    apples_agent-2_max: 100
    apples_agent-2_mean: 6.32
    apples_agent-2_min: 0
    apples_agent-3_max: 247
    apples_agent-3_mean: 102.95
    apples_agent-3_min: 35
    apples_agent-4_max: 33
    apples_agent-4_mean: 1.95
    apples_agent-4_min: 0
    apples_agent-5_max: 236
    apples_agent-5_mean: 69.17
    apples_agent-5_min: 18
    cleaning_beam_agent-0_max: 458
    cleaning_beam_agent-0_mean: 297.37
    cleaning_beam_agent-0_min: 123
    cleaning_beam_agent-1_max: 374
    cleaning_beam_agent-1_mean: 220.32
    cleaning_beam_agent-1_min: 105
    cleaning_beam_agent-2_max: 542
    cleaning_beam_agent-2_mean: 330.55
    cleaning_beam_agent-2_min: 112
    cleaning_beam_agent-3_max: 125
    cleaning_beam_agent-3_mean: 46.83
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 572
    cleaning_beam_agent-4_mean: 415.23
    cleaning_beam_agent-4_min: 236
    cleaning_beam_agent-5_max: 322
    cleaning_beam_agent-5_mean: 97.36
    cleaning_beam_agent-5_min: 36
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-26-56
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 816.9999999999694
  episode_reward_mean: 604.6699999999989
  episode_reward_min: 312.9999999999986
  episodes_this_iter: 96
  episodes_total: 12192
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19813.45
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000505209609400481
        entropy: 1.209061622619629
        entropy_coeff: 0.0017600000137463212
        kl: 0.013020381331443787
        model: {}
        policy_loss: -0.03160088509321213
        total_loss: -0.02955351397395134
        vf_explained_var: 0.02297312021255493
        vf_loss: 15.712471008300781
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000505209609400481
        entropy: 1.1299265623092651
        entropy_coeff: 0.0017600000137463212
        kl: 0.014360721223056316
        model: {}
        policy_loss: -0.0365072563290596
        total_loss: -0.034095779061317444
        vf_explained_var: 0.04977284371852875
        vf_loss: 15.280109405517578
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000505209609400481
        entropy: 1.121556043624878
        entropy_coeff: 0.0017600000137463212
        kl: 0.012881077826023102
        model: {}
        policy_loss: -0.03191368281841278
        total_loss: -0.02982952818274498
        vf_explained_var: 0.07766430079936981
        vf_loss: 14.81875228881836
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000505209609400481
        entropy: 0.8321763873100281
        entropy_coeff: 0.0017600000137463212
        kl: 0.01093490794301033
        model: {}
        policy_loss: -0.026331964880228043
        total_loss: -0.024344302713871002
        vf_explained_var: 0.21169878542423248
        vf_loss: 12.653095245361328
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000505209609400481
        entropy: 1.03609037399292
        entropy_coeff: 0.0017600000137463212
        kl: 0.012939682230353355
        model: {}
        policy_loss: -0.03475889563560486
        total_loss: -0.03251580148935318
        vf_explained_var: 0.07992395758628845
        vf_loss: 14.786779403686523
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000505209609400481
        entropy: 0.9030628800392151
        entropy_coeff: 0.0017600000137463212
        kl: 0.012939120642840862
        model: {}
        policy_loss: -0.034331995993852615
        total_loss: -0.03199691325426102
        vf_explained_var: 0.16833049058914185
        vf_loss: 13.366476058959961
    load_time_ms: 14040.231
    num_steps_sampled: 12192000
    num_steps_trained: 12192000
    sample_time_ms: 93692.208
    update_time_ms: 26.285
  iterations_since_restore: 67
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.485026737967917
    ram_util_percent: 14.575935828877004
  pid: 24061
  policy_reward_max:
    agent-0: 136.16666666666683
    agent-1: 136.16666666666683
    agent-2: 136.16666666666683
    agent-3: 136.16666666666683
    agent-4: 136.16666666666683
    agent-5: 136.16666666666683
  policy_reward_mean:
    agent-0: 100.77833333333363
    agent-1: 100.77833333333363
    agent-2: 100.77833333333363
    agent-3: 100.77833333333363
    agent-4: 100.77833333333363
    agent-5: 100.77833333333363
  policy_reward_min:
    agent-0: 52.16666666666654
    agent-1: 52.16666666666654
    agent-2: 52.16666666666654
    agent-3: 52.16666666666654
    agent-4: 52.16666666666654
    agent-5: 52.16666666666654
  sampler_perf:
    mean_env_wait_ms: 24.65534771266222
    mean_inference_ms: 12.408973726003685
    mean_processing_ms: 51.38549269071214
  time_since_restore: 9216.858130693436
  time_this_iter_s: 130.94904899597168
  time_total_s: 18342.86994457245
  timestamp: 1637033216
  timesteps_since_restore: 6432000
  timesteps_this_iter: 96000
  timesteps_total: 12192000
  training_iteration: 127
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 26.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    127 |          18342.9 | 12192000 |   604.67 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 273
    apples_agent-0_mean: 10.25
    apples_agent-0_min: 0
    apples_agent-1_max: 88
    apples_agent-1_mean: 25.4
    apples_agent-1_min: 0
    apples_agent-2_max: 60
    apples_agent-2_mean: 5.69
    apples_agent-2_min: 0
    apples_agent-3_max: 186
    apples_agent-3_mean: 100.31
    apples_agent-3_min: 33
    apples_agent-4_max: 58
    apples_agent-4_mean: 2.23
    apples_agent-4_min: 0
    apples_agent-5_max: 125
    apples_agent-5_mean: 64.65
    apples_agent-5_min: 23
    cleaning_beam_agent-0_max: 458
    cleaning_beam_agent-0_mean: 285.94
    cleaning_beam_agent-0_min: 112
    cleaning_beam_agent-1_max: 451
    cleaning_beam_agent-1_mean: 236.81
    cleaning_beam_agent-1_min: 122
    cleaning_beam_agent-2_max: 585
    cleaning_beam_agent-2_mean: 326.77
    cleaning_beam_agent-2_min: 123
    cleaning_beam_agent-3_max: 163
    cleaning_beam_agent-3_mean: 47.9
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 579
    cleaning_beam_agent-4_mean: 400.64
    cleaning_beam_agent-4_min: 155
    cleaning_beam_agent-5_max: 362
    cleaning_beam_agent-5_mean: 104.65
    cleaning_beam_agent-5_min: 27
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-29-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 870.9999999999822
  episode_reward_mean: 595.0499999999988
  episode_reward_min: 206.99999999999847
  episodes_this_iter: 96
  episodes_total: 12288
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19834.562
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004992192261852324
        entropy: 1.2039204835891724
        entropy_coeff: 0.0017600000137463212
        kl: 0.012590620666742325
        model: {}
        policy_loss: -0.03221303969621658
        total_loss: -0.03022196888923645
        vf_explained_var: 0.051426440477371216
        vf_loss: 15.918452262878418
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004992192261852324
        entropy: 1.1246535778045654
        entropy_coeff: 0.0017600000137463212
        kl: 0.014428247697651386
        model: {}
        policy_loss: -0.035585131496191025
        total_loss: -0.03305903449654579
        vf_explained_var: 0.03414203226566315
        vf_loss: 16.19839859008789
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004992192261852324
        entropy: 1.1199175119400024
        entropy_coeff: 0.0017600000137463212
        kl: 0.013223382644355297
        model: {}
        policy_loss: -0.031695231795310974
        total_loss: -0.029482290148735046
        vf_explained_var: 0.08151957392692566
        vf_loss: 15.39320182800293
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004992192261852324
        entropy: 0.8551112413406372
        entropy_coeff: 0.0017600000137463212
        kl: 0.010694470256567001
        model: {}
        policy_loss: -0.028367534279823303
        total_loss: -0.02641771361231804
        vf_explained_var: 0.21528927981853485
        vf_loss: 13.159248352050781
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004992192261852324
        entropy: 1.0609023571014404
        entropy_coeff: 0.0017600000137463212
        kl: 0.01296982727944851
        model: {}
        policy_loss: -0.0346626378595829
        total_loss: -0.032465655356645584
        vf_explained_var: 0.12389814853668213
        vf_loss: 14.701996803283691
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004992192261852324
        entropy: 0.9066274166107178
        entropy_coeff: 0.0017600000137463212
        kl: 0.013723630458116531
        model: {}
        policy_loss: -0.03458971530199051
        total_loss: -0.032138530164957047
        vf_explained_var: 0.22369113564491272
        vf_loss: 13.021164894104004
    load_time_ms: 14125.085
    num_steps_sampled: 12288000
    num_steps_trained: 12288000
    sample_time_ms: 93525.643
    update_time_ms: 26.455
  iterations_since_restore: 68
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.753333333333334
    ram_util_percent: 14.494444444444444
  pid: 24061
  policy_reward_max:
    agent-0: 145.16666666666714
    agent-1: 145.16666666666714
    agent-2: 145.16666666666714
    agent-3: 145.16666666666714
    agent-4: 145.16666666666714
    agent-5: 145.16666666666714
  policy_reward_mean:
    agent-0: 99.17500000000025
    agent-1: 99.17500000000025
    agent-2: 99.17500000000025
    agent-3: 99.17500000000025
    agent-4: 99.17500000000025
    agent-5: 99.17500000000025
  policy_reward_min:
    agent-0: 34.50000000000003
    agent-1: 34.50000000000003
    agent-2: 34.50000000000003
    agent-3: 34.50000000000003
    agent-4: 34.50000000000003
    agent-5: 34.50000000000003
  sampler_perf:
    mean_env_wait_ms: 24.65114572681279
    mean_inference_ms: 12.407891594100413
    mean_processing_ms: 51.37795144339592
  time_since_restore: 9343.33983373642
  time_this_iter_s: 126.48170304298401
  time_total_s: 18469.351647615433
  timestamp: 1637033343
  timesteps_since_restore: 6528000
  timesteps_this_iter: 96000
  timesteps_total: 12288000
  training_iteration: 128
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 26.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    128 |          18469.4 | 12288000 |   595.05 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 165
    apples_agent-0_mean: 8.78
    apples_agent-0_min: 0
    apples_agent-1_max: 78
    apples_agent-1_mean: 29.88
    apples_agent-1_min: 0
    apples_agent-2_max: 58
    apples_agent-2_mean: 3.64
    apples_agent-2_min: 0
    apples_agent-3_max: 163
    apples_agent-3_mean: 102.64
    apples_agent-3_min: 34
    apples_agent-4_max: 39
    apples_agent-4_mean: 1.3
    apples_agent-4_min: 0
    apples_agent-5_max: 142
    apples_agent-5_mean: 67.92
    apples_agent-5_min: 13
    cleaning_beam_agent-0_max: 468
    cleaning_beam_agent-0_mean: 291.65
    cleaning_beam_agent-0_min: 104
    cleaning_beam_agent-1_max: 369
    cleaning_beam_agent-1_mean: 224.48
    cleaning_beam_agent-1_min: 56
    cleaning_beam_agent-2_max: 584
    cleaning_beam_agent-2_mean: 340.7
    cleaning_beam_agent-2_min: 147
    cleaning_beam_agent-3_max: 130
    cleaning_beam_agent-3_mean: 38.74
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 579
    cleaning_beam_agent-4_mean: 395.89
    cleaning_beam_agent-4_min: 198
    cleaning_beam_agent-5_max: 298
    cleaning_beam_agent-5_mean: 100.3
    cleaning_beam_agent-5_min: 34
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-31-11
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 869.9999999999784
  episode_reward_mean: 618.3699999999982
  episode_reward_min: 274.9999999999981
  episodes_this_iter: 96
  episodes_total: 12384
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19832.994
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004932287847623229
        entropy: 1.2085775136947632
        entropy_coeff: 0.0017600000137463212
        kl: 0.011370422318577766
        model: {}
        policy_loss: -0.030587559565901756
        total_loss: -0.029022999107837677
        vf_explained_var: 0.059846431016922
        vf_loss: 14.175727844238281
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004932287847623229
        entropy: 1.1301772594451904
        entropy_coeff: 0.0017600000137463212
        kl: 0.013830707408487797
        model: {}
        policy_loss: -0.0361640602350235
        total_loss: -0.033889736980199814
        vf_explained_var: 0.007229894399642944
        vf_loss: 14.972915649414062
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004932287847623229
        entropy: 1.1211198568344116
        entropy_coeff: 0.0017600000137463212
        kl: 0.012178832665085793
        model: {}
        policy_loss: -0.03046420030295849
        total_loss: -0.028627805411815643
        vf_explained_var: 0.08894231915473938
        vf_loss: 13.737987518310547
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004932287847623229
        entropy: 0.8187975883483887
        entropy_coeff: 0.0017600000137463212
        kl: 0.01024828851222992
        model: {}
        policy_loss: -0.026755690574645996
        total_loss: -0.024878596886992455
        vf_explained_var: 0.1585375964641571
        vf_loss: 12.68519115447998
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004932287847623229
        entropy: 1.0449845790863037
        entropy_coeff: 0.0017600000137463212
        kl: 0.014071669429540634
        model: {}
        policy_loss: -0.033885594457387924
        total_loss: -0.03152601420879364
        vf_explained_var: 0.08187317848205566
        vf_loss: 13.844188690185547
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004932287847623229
        entropy: 0.9163725972175598
        entropy_coeff: 0.0017600000137463212
        kl: 0.012212253175675869
        model: {}
        policy_loss: -0.033346161246299744
        total_loss: -0.031244978308677673
        vf_explained_var: 0.15632176399230957
        vf_loss: 12.715514183044434
    load_time_ms: 14375.578
    num_steps_sampled: 12384000
    num_steps_trained: 12384000
    sample_time_ms: 93414.424
    update_time_ms: 22.551
  iterations_since_restore: 69
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.76502732240437
    ram_util_percent: 14.48524590163934
  pid: 24061
  policy_reward_max:
    agent-0: 145.00000000000034
    agent-1: 145.00000000000034
    agent-2: 145.00000000000034
    agent-3: 145.00000000000034
    agent-4: 145.00000000000034
    agent-5: 145.00000000000034
  policy_reward_mean:
    agent-0: 103.06166666666697
    agent-1: 103.06166666666697
    agent-2: 103.06166666666697
    agent-3: 103.06166666666697
    agent-4: 103.06166666666697
    agent-5: 103.06166666666697
  policy_reward_min:
    agent-0: 45.83333333333322
    agent-1: 45.83333333333322
    agent-2: 45.83333333333322
    agent-3: 45.83333333333322
    agent-4: 45.83333333333322
    agent-5: 45.83333333333322
  sampler_perf:
    mean_env_wait_ms: 24.65308112883339
    mean_inference_ms: 12.407949641373705
    mean_processing_ms: 51.3815811766445
  time_since_restore: 9471.791492938995
  time_this_iter_s: 128.45165920257568
  time_total_s: 18597.80330681801
  timestamp: 1637033471
  timesteps_since_restore: 6624000
  timesteps_this_iter: 96000
  timesteps_total: 12384000
  training_iteration: 129
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 26.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    129 |          18597.8 | 12384000 |   618.37 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 100
    apples_agent-0_mean: 10.5
    apples_agent-0_min: 0
    apples_agent-1_max: 75
    apples_agent-1_mean: 22.43
    apples_agent-1_min: 0
    apples_agent-2_max: 33
    apples_agent-2_mean: 2.53
    apples_agent-2_min: 0
    apples_agent-3_max: 186
    apples_agent-3_mean: 100.09
    apples_agent-3_min: 5
    apples_agent-4_max: 65
    apples_agent-4_mean: 2.06
    apples_agent-4_min: 0
    apples_agent-5_max: 141
    apples_agent-5_mean: 67.61
    apples_agent-5_min: 3
    cleaning_beam_agent-0_max: 415
    cleaning_beam_agent-0_mean: 276.12
    cleaning_beam_agent-0_min: 123
    cleaning_beam_agent-1_max: 421
    cleaning_beam_agent-1_mean: 222.51
    cleaning_beam_agent-1_min: 113
    cleaning_beam_agent-2_max: 537
    cleaning_beam_agent-2_mean: 346.07
    cleaning_beam_agent-2_min: 171
    cleaning_beam_agent-3_max: 136
    cleaning_beam_agent-3_mean: 47.18
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 579
    cleaning_beam_agent-4_mean: 395.16
    cleaning_beam_agent-4_min: 192
    cleaning_beam_agent-5_max: 424
    cleaning_beam_agent-5_mean: 98.97
    cleaning_beam_agent-5_min: 29
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-33-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 860.99999999998
  episode_reward_mean: 597.8499999999983
  episode_reward_min: 99.0000000000002
  episodes_this_iter: 96
  episodes_total: 12480
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19828.548
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004872384015470743
        entropy: 1.221949577331543
        entropy_coeff: 0.0017600000137463212
        kl: 0.012816272675991058
        model: {}
        policy_loss: -0.0315280556678772
        total_loss: -0.029504625126719475
        vf_explained_var: 0.055299222469329834
        vf_loss: 16.108108520507812
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004872384015470743
        entropy: 1.122870922088623
        entropy_coeff: 0.0017600000137463212
        kl: 0.013320492580533028
        model: {}
        policy_loss: -0.03473702073097229
        total_loss: -0.032423682510852814
        vf_explained_var: 0.04714028537273407
        vf_loss: 16.254884719848633
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004872384015470743
        entropy: 1.1181745529174805
        entropy_coeff: 0.0017600000137463212
        kl: 0.012797552160918713
        model: {}
        policy_loss: -0.030375752598047256
        total_loss: -0.028212793171405792
        vf_explained_var: 0.07896332442760468
        vf_loss: 15.714347839355469
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004872384015470743
        entropy: 0.852410614490509
        entropy_coeff: 0.0017600000137463212
        kl: 0.010467324405908585
        model: {}
        policy_loss: -0.02835754118859768
        total_loss: -0.026445474475622177
        vf_explained_var: 0.22627495229244232
        vf_loss: 13.188461303710938
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004872384015470743
        entropy: 1.0467979907989502
        entropy_coeff: 0.0017600000137463212
        kl: 0.013006463646888733
        model: {}
        policy_loss: -0.03583351895213127
        total_loss: -0.033534884452819824
        vf_explained_var: 0.09754414856433868
        vf_loss: 15.397056579589844
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004872384015470743
        entropy: 0.9099952578544617
        entropy_coeff: 0.0017600000137463212
        kl: 0.01341702789068222
        model: {}
        policy_loss: -0.033449821174144745
        total_loss: -0.031037647277116776
        vf_explained_var: 0.22018422186374664
        vf_loss: 13.30359935760498
    load_time_ms: 14474.217
    num_steps_sampled: 12480000
    num_steps_trained: 12480000
    sample_time_ms: 93487.983
    update_time_ms: 22.32
  iterations_since_restore: 70
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.92967032967033
    ram_util_percent: 14.465934065934063
  pid: 24061
  policy_reward_max:
    agent-0: 143.5000000000003
    agent-1: 143.5000000000003
    agent-2: 143.5000000000003
    agent-3: 143.5000000000003
    agent-4: 143.5000000000003
    agent-5: 143.5000000000003
  policy_reward_mean:
    agent-0: 99.64166666666695
    agent-1: 99.64166666666695
    agent-2: 99.64166666666695
    agent-3: 99.64166666666695
    agent-4: 99.64166666666695
    agent-5: 99.64166666666695
  policy_reward_min:
    agent-0: 16.499999999999986
    agent-1: 16.499999999999986
    agent-2: 16.499999999999986
    agent-3: 16.499999999999986
    agent-4: 16.499999999999986
    agent-5: 16.499999999999986
  sampler_perf:
    mean_env_wait_ms: 24.652325738376295
    mean_inference_ms: 12.410332320398688
    mean_processing_ms: 51.395389183340285
  time_since_restore: 9599.21655011177
  time_this_iter_s: 127.42505717277527
  time_total_s: 18725.228363990784
  timestamp: 1637033599
  timesteps_since_restore: 6720000
  timesteps_this_iter: 96000
  timesteps_total: 12480000
  training_iteration: 130
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 26.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    130 |          18725.2 | 12480000 |   597.85 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 57
    apples_agent-0_mean: 7.44
    apples_agent-0_min: 0
    apples_agent-1_max: 82
    apples_agent-1_mean: 25.49
    apples_agent-1_min: 0
    apples_agent-2_max: 59
    apples_agent-2_mean: 5.76
    apples_agent-2_min: 0
    apples_agent-3_max: 160
    apples_agent-3_mean: 102.8
    apples_agent-3_min: 39
    apples_agent-4_max: 35
    apples_agent-4_mean: 0.82
    apples_agent-4_min: 0
    apples_agent-5_max: 136
    apples_agent-5_mean: 67.1
    apples_agent-5_min: 7
    cleaning_beam_agent-0_max: 402
    cleaning_beam_agent-0_mean: 273.59
    cleaning_beam_agent-0_min: 98
    cleaning_beam_agent-1_max: 410
    cleaning_beam_agent-1_mean: 223.38
    cleaning_beam_agent-1_min: 96
    cleaning_beam_agent-2_max: 502
    cleaning_beam_agent-2_mean: 319.0
    cleaning_beam_agent-2_min: 49
    cleaning_beam_agent-3_max: 115
    cleaning_beam_agent-3_mean: 44.61
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 555
    cleaning_beam_agent-4_mean: 420.59
    cleaning_beam_agent-4_min: 293
    cleaning_beam_agent-5_max: 246
    cleaning_beam_agent-5_mean: 103.28
    cleaning_beam_agent-5_min: 32
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-35-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 866.9999999999866
  episode_reward_mean: 614.0399999999994
  episode_reward_min: 302.99999999999915
  episodes_this_iter: 96
  episodes_total: 12576
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19769.599
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004812479892279953
        entropy: 1.227437138557434
        entropy_coeff: 0.0017600000137463212
        kl: 0.01200931891798973
        model: {}
        policy_loss: -0.03148554638028145
        total_loss: -0.029789015650749207
        vf_explained_var: 0.033248573541641235
        vf_loss: 14.549568176269531
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004812479892279953
        entropy: 1.134458065032959
        entropy_coeff: 0.0017600000137463212
        kl: 0.013671990483999252
        model: {}
        policy_loss: -0.03445347398519516
        total_loss: -0.03226972371339798
        vf_explained_var: 0.03894403576850891
        vf_loss: 14.459992408752441
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004812479892279953
        entropy: 1.1094650030136108
        entropy_coeff: 0.0017600000137463212
        kl: 0.012862693518400192
        model: {}
        policy_loss: -0.031415075063705444
        total_loss: -0.02944822423160076
        vf_explained_var: 0.10460975766181946
        vf_loss: 13.469695091247559
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004812479892279953
        entropy: 0.8309503197669983
        entropy_coeff: 0.0017600000137463212
        kl: 0.00991907436400652
        model: {}
        policy_loss: -0.026653485372662544
        total_loss: -0.024884125217795372
        vf_explained_var: 0.17162208259105682
        vf_loss: 12.4801664352417
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004812479892279953
        entropy: 1.0370391607284546
        entropy_coeff: 0.0017600000137463212
        kl: 0.013089546002447605
        model: {}
        policy_loss: -0.033165592700242996
        total_loss: -0.030993320047855377
        vf_explained_var: 0.08270725607872009
        vf_loss: 13.795552253723145
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004812479892279953
        entropy: 0.9113138914108276
        entropy_coeff: 0.0017600000137463212
        kl: 0.012234826572239399
        model: {}
        policy_loss: -0.03358117863535881
        total_loss: -0.03145429119467735
        vf_explained_var: 0.14775221049785614
        vf_loss: 12.8383207321167
    load_time_ms: 14502.291
    num_steps_sampled: 12576000
    num_steps_trained: 12576000
    sample_time_ms: 93129.525
    update_time_ms: 23.113
  iterations_since_restore: 71
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.857303370786514
    ram_util_percent: 14.456179775280898
  pid: 24061
  policy_reward_max:
    agent-0: 144.5
    agent-1: 144.5
    agent-2: 144.5
    agent-3: 144.5
    agent-4: 144.5
    agent-5: 144.5
  policy_reward_mean:
    agent-0: 102.34000000000029
    agent-1: 102.34000000000029
    agent-2: 102.34000000000029
    agent-3: 102.34000000000029
    agent-4: 102.34000000000029
    agent-5: 102.34000000000029
  policy_reward_min:
    agent-0: 50.49999999999994
    agent-1: 50.49999999999994
    agent-2: 50.49999999999994
    agent-3: 50.49999999999994
    agent-4: 50.49999999999994
    agent-5: 50.49999999999994
  sampler_perf:
    mean_env_wait_ms: 24.648467135744554
    mean_inference_ms: 12.408798664791757
    mean_processing_ms: 51.39211329825492
  time_since_restore: 9724.12266755104
  time_this_iter_s: 124.90611743927002
  time_total_s: 18850.134481430054
  timestamp: 1637033724
  timesteps_since_restore: 6816000
  timesteps_this_iter: 96000
  timesteps_total: 12576000
  training_iteration: 131
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 26.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    131 |          18850.1 | 12576000 |   614.04 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 83
    apples_agent-0_mean: 6.88
    apples_agent-0_min: 0
    apples_agent-1_max: 169
    apples_agent-1_mean: 27.93
    apples_agent-1_min: 0
    apples_agent-2_max: 170
    apples_agent-2_mean: 8.55
    apples_agent-2_min: 0
    apples_agent-3_max: 193
    apples_agent-3_mean: 101.98
    apples_agent-3_min: 21
    apples_agent-4_max: 39
    apples_agent-4_mean: 1.85
    apples_agent-4_min: 0
    apples_agent-5_max: 132
    apples_agent-5_mean: 69.42
    apples_agent-5_min: 18
    cleaning_beam_agent-0_max: 471
    cleaning_beam_agent-0_mean: 273.27
    cleaning_beam_agent-0_min: 131
    cleaning_beam_agent-1_max: 356
    cleaning_beam_agent-1_mean: 208.38
    cleaning_beam_agent-1_min: 79
    cleaning_beam_agent-2_max: 459
    cleaning_beam_agent-2_mean: 322.85
    cleaning_beam_agent-2_min: 63
    cleaning_beam_agent-3_max: 152
    cleaning_beam_agent-3_mean: 55.33
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 553
    cleaning_beam_agent-4_mean: 404.24
    cleaning_beam_agent-4_min: 184
    cleaning_beam_agent-5_max: 313
    cleaning_beam_agent-5_mean: 101.44
    cleaning_beam_agent-5_min: 25
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-37-28
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 863.9999999999924
  episode_reward_mean: 593.7799999999997
  episode_reward_min: 113.00000000000067
  episodes_this_iter: 96
  episodes_total: 12672
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19755.44
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004752576060127467
        entropy: 1.2215886116027832
        entropy_coeff: 0.0017600000137463212
        kl: 0.011975329369306564
        model: {}
        policy_loss: -0.03269600123167038
        total_loss: -0.030930280685424805
        vf_explained_var: 0.09962400794029236
        vf_loss: 15.206474304199219
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004752576060127467
        entropy: 1.1300104856491089
        entropy_coeff: 0.0017600000137463212
        kl: 0.013412542641162872
        model: {}
        policy_loss: -0.03598563373088837
        total_loss: -0.03369615972042084
        vf_explained_var: 0.055919915437698364
        vf_loss: 15.957832336425781
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004752576060127467
        entropy: 1.118342399597168
        entropy_coeff: 0.0017600000137463212
        kl: 0.012150345370173454
        model: {}
        policy_loss: -0.03103725053369999
        total_loss: -0.029099538922309875
        vf_explained_var: 0.12616442143917084
        vf_loss: 14.759239196777344
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004752576060127467
        entropy: 0.862687349319458
        entropy_coeff: 0.0017600000137463212
        kl: 0.010186483152210712
        model: {}
        policy_loss: -0.02816954255104065
        total_loss: -0.02633010968565941
        vf_explained_var: 0.21767503023147583
        vf_loss: 13.204675674438477
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004752576060127467
        entropy: 1.0374469757080078
        entropy_coeff: 0.0017600000137463212
        kl: 0.013788927346467972
        model: {}
        policy_loss: -0.03566240146756172
        total_loss: -0.03317706659436226
        vf_explained_var: 0.08065581321716309
        vf_loss: 15.534562110900879
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004752576060127467
        entropy: 0.9152616262435913
        entropy_coeff: 0.0017600000137463212
        kl: 0.013742645271122456
        model: {}
        policy_loss: -0.033860333263874054
        total_loss: -0.031357645988464355
        vf_explained_var: 0.19143208861351013
        vf_loss: 13.650182723999023
    load_time_ms: 14482.223
    num_steps_sampled: 12672000
    num_steps_trained: 12672000
    sample_time_ms: 92844.66
    update_time_ms: 22.369
  iterations_since_restore: 72
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 19.766666666666666
    ram_util_percent: 12.608474576271185
  pid: 24061
  policy_reward_max:
    agent-0: 144.00000000000017
    agent-1: 144.00000000000017
    agent-2: 144.00000000000017
    agent-3: 144.00000000000017
    agent-4: 144.00000000000017
    agent-5: 144.00000000000017
  policy_reward_mean:
    agent-0: 98.96333333333358
    agent-1: 98.96333333333358
    agent-2: 98.96333333333358
    agent-3: 98.96333333333358
    agent-4: 98.96333333333358
    agent-5: 98.96333333333358
  policy_reward_min:
    agent-0: 18.833333333333325
    agent-1: 18.833333333333325
    agent-2: 18.833333333333325
    agent-3: 18.833333333333325
    agent-4: 18.833333333333325
    agent-5: 18.833333333333325
  sampler_perf:
    mean_env_wait_ms: 24.64447028947261
    mean_inference_ms: 12.408433116187334
    mean_processing_ms: 51.387704658503374
  time_since_restore: 9848.433750629425
  time_this_iter_s: 124.3110830783844
  time_total_s: 18974.445564508438
  timestamp: 1637033848
  timesteps_since_restore: 6912000
  timesteps_this_iter: 96000
  timesteps_total: 12672000
  training_iteration: 132
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    132 |          18974.4 | 12672000 |   593.78 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 70
    apples_agent-0_mean: 8.3
    apples_agent-0_min: 0
    apples_agent-1_max: 145
    apples_agent-1_mean: 31.12
    apples_agent-1_min: 0
    apples_agent-2_max: 170
    apples_agent-2_mean: 9.35
    apples_agent-2_min: 0
    apples_agent-3_max: 165
    apples_agent-3_mean: 103.36
    apples_agent-3_min: 43
    apples_agent-4_max: 51
    apples_agent-4_mean: 1.05
    apples_agent-4_min: 0
    apples_agent-5_max: 140
    apples_agent-5_mean: 66.92
    apples_agent-5_min: 8
    cleaning_beam_agent-0_max: 493
    cleaning_beam_agent-0_mean: 277.66
    cleaning_beam_agent-0_min: 105
    cleaning_beam_agent-1_max: 342
    cleaning_beam_agent-1_mean: 209.37
    cleaning_beam_agent-1_min: 81
    cleaning_beam_agent-2_max: 526
    cleaning_beam_agent-2_mean: 320.42
    cleaning_beam_agent-2_min: 118
    cleaning_beam_agent-3_max: 147
    cleaning_beam_agent-3_mean: 54.05
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 514
    cleaning_beam_agent-4_mean: 399.9
    cleaning_beam_agent-4_min: 165
    cleaning_beam_agent-5_max: 306
    cleaning_beam_agent-5_mean: 107.72
    cleaning_beam_agent-5_min: 27
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-39-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 853.9999999999797
  episode_reward_mean: 604.0999999999976
  episode_reward_min: 169.99999999999883
  episodes_this_iter: 96
  episodes_total: 12768
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19732.013
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00046926719369366765
        entropy: 1.2150205373764038
        entropy_coeff: 0.0017600000137463212
        kl: 0.012699265964329243
        model: {}
        policy_loss: -0.033989790827035904
        total_loss: -0.03217120096087456
        vf_explained_var: 0.1244061291217804
        vf_loss: 14.171738624572754
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00046926719369366765
        entropy: 1.130614995956421
        entropy_coeff: 0.0017600000137463212
        kl: 0.013430433347821236
        model: {}
        policy_loss: -0.0362824946641922
        total_loss: -0.034012507647275925
        vf_explained_var: 0.028142496943473816
        vf_loss: 15.737833023071289
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00046926719369366765
        entropy: 1.107692837715149
        entropy_coeff: 0.0017600000137463212
        kl: 0.013632550835609436
        model: {}
        policy_loss: -0.031082594767212868
        total_loss: -0.028838157653808594
        vf_explained_var: 0.09398345649242401
        vf_loss: 14.674715042114258
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00046926719369366765
        entropy: 0.8584123849868774
        entropy_coeff: 0.0017600000137463212
        kl: 0.010607745498418808
        model: {}
        policy_loss: -0.02817394956946373
        total_loss: -0.026294169947504997
        vf_explained_var: 0.21599601209163666
        vf_loss: 12.690323829650879
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00046926719369366765
        entropy: 1.0406107902526855
        entropy_coeff: 0.0017600000137463212
        kl: 0.012645397335290909
        model: {}
        policy_loss: -0.03436994180083275
        total_loss: -0.03218657150864601
        vf_explained_var: 0.08187051117420197
        vf_loss: 14.857706069946289
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00046926719369366765
        entropy: 0.9003853797912598
        entropy_coeff: 0.0017600000137463212
        kl: 0.012263940647244453
        model: {}
        policy_loss: -0.0332370363175869
        total_loss: -0.030984660610556602
        vf_explained_var: 0.14467290043830872
        vf_loss: 13.842668533325195
    load_time_ms: 14438.084
    num_steps_sampled: 12768000
    num_steps_trained: 12768000
    sample_time_ms: 92544.163
    update_time_ms: 21.654
  iterations_since_restore: 73
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.398850574712647
    ram_util_percent: 10.09655172413793
  pid: 24061
  policy_reward_max:
    agent-0: 142.33333333333363
    agent-1: 142.33333333333363
    agent-2: 142.33333333333363
    agent-3: 142.33333333333363
    agent-4: 142.33333333333363
    agent-5: 142.33333333333363
  policy_reward_mean:
    agent-0: 100.68333333333356
    agent-1: 100.68333333333356
    agent-2: 100.68333333333356
    agent-3: 100.68333333333356
    agent-4: 100.68333333333356
    agent-5: 100.68333333333356
  policy_reward_min:
    agent-0: 28.333333333333396
    agent-1: 28.333333333333396
    agent-2: 28.333333333333396
    agent-3: 28.333333333333396
    agent-4: 28.333333333333396
    agent-5: 28.333333333333396
  sampler_perf:
    mean_env_wait_ms: 24.634022346213747
    mean_inference_ms: 12.404423955268312
    mean_processing_ms: 51.3705702740157
  time_since_restore: 9970.636319160461
  time_this_iter_s: 122.20256853103638
  time_total_s: 19096.648133039474
  timestamp: 1637033971
  timesteps_since_restore: 7008000
  timesteps_this_iter: 96000
  timesteps_total: 12768000
  training_iteration: 133
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    133 |          19096.6 | 12768000 |    604.1 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 144
    apples_agent-0_mean: 7.79
    apples_agent-0_min: 0
    apples_agent-1_max: 97
    apples_agent-1_mean: 27.87
    apples_agent-1_min: 0
    apples_agent-2_max: 76
    apples_agent-2_mean: 5.61
    apples_agent-2_min: 0
    apples_agent-3_max: 156
    apples_agent-3_mean: 99.82
    apples_agent-3_min: 23
    apples_agent-4_max: 48
    apples_agent-4_mean: 1.53
    apples_agent-4_min: 0
    apples_agent-5_max: 120
    apples_agent-5_mean: 67.36
    apples_agent-5_min: 15
    cleaning_beam_agent-0_max: 422
    cleaning_beam_agent-0_mean: 275.14
    cleaning_beam_agent-0_min: 108
    cleaning_beam_agent-1_max: 351
    cleaning_beam_agent-1_mean: 208.24
    cleaning_beam_agent-1_min: 76
    cleaning_beam_agent-2_max: 531
    cleaning_beam_agent-2_mean: 347.49
    cleaning_beam_agent-2_min: 116
    cleaning_beam_agent-3_max: 116
    cleaning_beam_agent-3_mean: 49.1
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 479
    cleaning_beam_agent-4_mean: 384.98
    cleaning_beam_agent-4_min: 190
    cleaning_beam_agent-5_max: 298
    cleaning_beam_agent-5_mean: 107.96
    cleaning_beam_agent-5_min: 33
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-41-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 803.9999999999858
  episode_reward_mean: 600.2899999999983
  episode_reward_min: 166.9999999999993
  episodes_this_iter: 96
  episodes_total: 12864
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19754.901
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00046327681047841907
        entropy: 1.2122430801391602
        entropy_coeff: 0.0017600000137463212
        kl: 0.012008610181510448
        model: {}
        policy_loss: -0.03106905147433281
        total_loss: -0.029325293377041817
        vf_explained_var: 0.026696741580963135
        vf_loss: 14.755844116210938
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00046327681047841907
        entropy: 1.141689419746399
        entropy_coeff: 0.0017600000137463212
        kl: 0.013834703713655472
        model: {}
        policy_loss: -0.03535623475909233
        total_loss: -0.0331251285970211
        vf_explained_var: 0.027018845081329346
        vf_loss: 14.735380172729492
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00046327681047841907
        entropy: 1.0920157432556152
        entropy_coeff: 0.0017600000137463212
        kl: 0.012285304255783558
        model: {}
        policy_loss: -0.03141675144433975
        total_loss: -0.02951488085091114
        vf_explained_var: 0.09717519581317902
        vf_loss: 13.667596817016602
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00046327681047841907
        entropy: 0.8618365526199341
        entropy_coeff: 0.0017600000137463212
        kl: 0.01113123819231987
        model: {}
        policy_loss: -0.026712672784924507
        total_loss: -0.02474343590438366
        vf_explained_var: 0.16777142882347107
        vf_loss: 12.598176002502441
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00046327681047841907
        entropy: 1.0440257787704468
        entropy_coeff: 0.0017600000137463212
        kl: 0.013078817166388035
        model: {}
        policy_loss: -0.034872639924287796
        total_loss: -0.03275226056575775
        vf_explained_var: 0.11382490396499634
        vf_loss: 13.421042442321777
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00046327681047841907
        entropy: 0.9014318585395813
        entropy_coeff: 0.0017600000137463212
        kl: 0.013015427626669407
        model: {}
        policy_loss: -0.033410895615816116
        total_loss: -0.03117377683520317
        vf_explained_var: 0.19394361972808838
        vf_loss: 12.205496788024902
    load_time_ms: 14855.839
    num_steps_sampled: 12864000
    num_steps_trained: 12864000
    sample_time_ms: 91937.2
    update_time_ms: 21.85
  iterations_since_restore: 74
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 17.377777777777776
    ram_util_percent: 11.588888888888892
  pid: 24061
  policy_reward_max:
    agent-0: 134.0000000000001
    agent-1: 134.0000000000001
    agent-2: 134.0000000000001
    agent-3: 134.0000000000001
    agent-4: 134.0000000000001
    agent-5: 134.0000000000001
  policy_reward_mean:
    agent-0: 100.04833333333362
    agent-1: 100.04833333333362
    agent-2: 100.04833333333362
    agent-3: 100.04833333333362
    agent-4: 100.04833333333362
    agent-5: 100.04833333333362
  policy_reward_min:
    agent-0: 27.833333333333382
    agent-1: 27.833333333333382
    agent-2: 27.833333333333382
    agent-3: 27.833333333333382
    agent-4: 27.833333333333382
    agent-5: 27.833333333333382
  sampler_perf:
    mean_env_wait_ms: 24.62342005858756
    mean_inference_ms: 12.399934953396755
    mean_processing_ms: 51.35351064470216
  time_since_restore: 10096.577758789062
  time_this_iter_s: 125.94143962860107
  time_total_s: 19222.589572668076
  timestamp: 1637034097
  timesteps_since_restore: 7104000
  timesteps_this_iter: 96000
  timesteps_total: 12864000
  training_iteration: 134
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 26.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    134 |          19222.6 | 12864000 |   600.29 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 216
    apples_agent-0_mean: 11.74
    apples_agent-0_min: 0
    apples_agent-1_max: 310
    apples_agent-1_mean: 31.28
    apples_agent-1_min: 0
    apples_agent-2_max: 320
    apples_agent-2_mean: 10.17
    apples_agent-2_min: 0
    apples_agent-3_max: 167
    apples_agent-3_mean: 97.31
    apples_agent-3_min: 16
    apples_agent-4_max: 39
    apples_agent-4_mean: 1.34
    apples_agent-4_min: 0
    apples_agent-5_max: 167
    apples_agent-5_mean: 66.16
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 403
    cleaning_beam_agent-0_mean: 247.09
    cleaning_beam_agent-0_min: 72
    cleaning_beam_agent-1_max: 394
    cleaning_beam_agent-1_mean: 213.76
    cleaning_beam_agent-1_min: 85
    cleaning_beam_agent-2_max: 499
    cleaning_beam_agent-2_mean: 329.92
    cleaning_beam_agent-2_min: 59
    cleaning_beam_agent-3_max: 140
    cleaning_beam_agent-3_mean: 48.42
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 521
    cleaning_beam_agent-4_mean: 382.06
    cleaning_beam_agent-4_min: 100
    cleaning_beam_agent-5_max: 274
    cleaning_beam_agent-5_mean: 103.82
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-43-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 853.9999999999861
  episode_reward_mean: 572.0200000000004
  episode_reward_min: 163.99999999999994
  episodes_this_iter: 96
  episodes_total: 12960
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19754.901
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00045728639815934
        entropy: 1.2111917734146118
        entropy_coeff: 0.0017600000137463212
        kl: 0.012410642579197884
        model: {}
        policy_loss: -0.03161049634218216
        total_loss: -0.029807817190885544
        vf_explained_var: 0.07767881453037262
        vf_loss: 14.522481918334961
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00045728639815934
        entropy: 1.1285423040390015
        entropy_coeff: 0.0017600000137463212
        kl: 0.014132456853985786
        model: {}
        policy_loss: -0.037132538855075836
        total_loss: -0.034765858203172684
        vf_explained_var: 0.030516192317008972
        vf_loss: 15.264240264892578
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00045728639815934
        entropy: 1.078102707862854
        entropy_coeff: 0.0017600000137463212
        kl: 0.012477722018957138
        model: {}
        policy_loss: -0.03101474605500698
        total_loss: -0.028973232954740524
        vf_explained_var: 0.08267061412334442
        vf_loss: 14.434305191040039
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00045728639815934
        entropy: 0.8590237498283386
        entropy_coeff: 0.0017600000137463212
        kl: 0.011353681795299053
        model: {}
        policy_loss: -0.02611539140343666
        total_loss: -0.02412625588476658
        vf_explained_var: 0.21819046139717102
        vf_loss: 12.302827835083008
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00045728639815934
        entropy: 1.0490360260009766
        entropy_coeff: 0.0017600000137463212
        kl: 0.013622111640870571
        model: {}
        policy_loss: -0.0332716703414917
        total_loss: -0.030969111248850822
        vf_explained_var: 0.09485629200935364
        vf_loss: 14.244388580322266
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00045728639815934
        entropy: 0.9140340089797974
        entropy_coeff: 0.0017600000137463212
        kl: 0.013163017109036446
        model: {}
        policy_loss: -0.0346236377954483
        total_loss: -0.03229304030537605
        vf_explained_var: 0.16962577402591705
        vf_loss: 13.066954612731934
    load_time_ms: 15135.196
    num_steps_sampled: 12960000
    num_steps_trained: 12960000
    sample_time_ms: 91426.021
    update_time_ms: 22.058
  iterations_since_restore: 75
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.818539325842696
    ram_util_percent: 14.888202247191014
  pid: 24061
  policy_reward_max:
    agent-0: 142.3333333333337
    agent-1: 142.3333333333337
    agent-2: 142.3333333333337
    agent-3: 142.3333333333337
    agent-4: 142.3333333333337
    agent-5: 142.3333333333337
  policy_reward_mean:
    agent-0: 95.33666666666687
    agent-1: 95.33666666666687
    agent-2: 95.33666666666687
    agent-3: 95.33666666666687
    agent-4: 95.33666666666687
    agent-5: 95.33666666666687
  policy_reward_min:
    agent-0: 27.333333333333382
    agent-1: 27.333333333333382
    agent-2: 27.333333333333382
    agent-3: 27.333333333333382
    agent-4: 27.333333333333382
    agent-5: 27.333333333333382
  sampler_perf:
    mean_env_wait_ms: 24.614826877978913
    mean_inference_ms: 12.397689880639934
    mean_processing_ms: 51.34113698189369
  time_since_restore: 10221.87080693245
  time_this_iter_s: 125.29304814338684
  time_total_s: 19347.882620811462
  timestamp: 1637034222
  timesteps_since_restore: 7200000
  timesteps_this_iter: 96000
  timesteps_total: 12960000
  training_iteration: 135
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    135 |          19347.9 | 12960000 |   572.02 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 77
    apples_agent-0_mean: 9.53
    apples_agent-0_min: 0
    apples_agent-1_max: 85
    apples_agent-1_mean: 28.77
    apples_agent-1_min: 0
    apples_agent-2_max: 126
    apples_agent-2_mean: 6.65
    apples_agent-2_min: 0
    apples_agent-3_max: 188
    apples_agent-3_mean: 101.4
    apples_agent-3_min: 37
    apples_agent-4_max: 163
    apples_agent-4_mean: 3.33
    apples_agent-4_min: 0
    apples_agent-5_max: 122
    apples_agent-5_mean: 65.25
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 412
    cleaning_beam_agent-0_mean: 256.88
    cleaning_beam_agent-0_min: 105
    cleaning_beam_agent-1_max: 340
    cleaning_beam_agent-1_mean: 207.62
    cleaning_beam_agent-1_min: 76
    cleaning_beam_agent-2_max: 498
    cleaning_beam_agent-2_mean: 341.38
    cleaning_beam_agent-2_min: 126
    cleaning_beam_agent-3_max: 112
    cleaning_beam_agent-3_mean: 48.98
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 510
    cleaning_beam_agent-4_mean: 383.65
    cleaning_beam_agent-4_min: 181
    cleaning_beam_agent-5_max: 274
    cleaning_beam_agent-5_mean: 97.82
    cleaning_beam_agent-5_min: 30
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-45-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 887.9999999999827
  episode_reward_mean: 600.0699999999995
  episode_reward_min: 260.99999999999966
  episodes_this_iter: 96
  episodes_total: 13056
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19732.113
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000451295985840261
        entropy: 1.1928961277008057
        entropy_coeff: 0.0017600000137463212
        kl: 0.011973208747804165
        model: {}
        policy_loss: -0.033185724169015884
        total_loss: -0.031373899430036545
        vf_explained_var: 0.005731850862503052
        vf_loss: 15.166797637939453
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000451295985840261
        entropy: 1.1345887184143066
        entropy_coeff: 0.0017600000137463212
        kl: 0.013609791174530983
        model: {}
        policy_loss: -0.03607507795095444
        total_loss: -0.033849406987428665
        vf_explained_var: 0.015833809971809387
        vf_loss: 15.005845069885254
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000451295985840261
        entropy: 1.106917142868042
        entropy_coeff: 0.0017600000137463212
        kl: 0.01270219311118126
        model: {}
        policy_loss: -0.03086249716579914
        total_loss: -0.028912149369716644
        vf_explained_var: 0.10869646072387695
        vf_loss: 13.58080005645752
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000451295985840261
        entropy: 0.8390252590179443
        entropy_coeff: 0.0017600000137463212
        kl: 0.010406086221337318
        model: {}
        policy_loss: -0.02632465586066246
        total_loss: -0.024501964449882507
        vf_explained_var: 0.20055656135082245
        vf_loss: 12.181601524353027
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000451295985840261
        entropy: 1.0325093269348145
        entropy_coeff: 0.0017600000137463212
        kl: 0.012825115583837032
        model: {}
        policy_loss: -0.03410317748785019
        total_loss: -0.03200570121407509
        vf_explained_var: 0.1147090345621109
        vf_loss: 13.496700286865234
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000451295985840261
        entropy: 0.913592517375946
        entropy_coeff: 0.0017600000137463212
        kl: 0.012629160657525063
        model: {}
        policy_loss: -0.033951371908187866
        total_loss: -0.03175738826394081
        vf_explained_var: 0.16307629644870758
        vf_loss: 12.760734558105469
    load_time_ms: 15443.219
    num_steps_sampled: 13056000
    num_steps_trained: 13056000
    sample_time_ms: 91115.779
    update_time_ms: 21.767
  iterations_since_restore: 76
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.601092896174862
    ram_util_percent: 14.88852459016394
  pid: 24061
  policy_reward_max:
    agent-0: 148.00000000000009
    agent-1: 148.00000000000009
    agent-2: 148.00000000000009
    agent-3: 148.00000000000009
    agent-4: 148.00000000000009
    agent-5: 148.00000000000009
  policy_reward_mean:
    agent-0: 100.01166666666691
    agent-1: 100.01166666666691
    agent-2: 100.01166666666691
    agent-3: 100.01166666666691
    agent-4: 100.01166666666691
    agent-5: 100.01166666666691
  policy_reward_min:
    agent-0: 43.50000000000005
    agent-1: 43.50000000000005
    agent-2: 43.50000000000005
    agent-3: 43.50000000000005
    agent-4: 43.50000000000005
    agent-5: 43.50000000000005
  sampler_perf:
    mean_env_wait_ms: 24.608328228807736
    mean_inference_ms: 12.396087171573031
    mean_processing_ms: 51.339137738185876
  time_since_restore: 10350.12867641449
  time_this_iter_s: 128.2578694820404
  time_total_s: 19476.140490293503
  timestamp: 1637034351
  timesteps_since_restore: 7296000
  timesteps_this_iter: 96000
  timesteps_total: 13056000
  training_iteration: 136
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    136 |          19476.1 | 13056000 |   600.07 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 166
    apples_agent-0_mean: 8.32
    apples_agent-0_min: 0
    apples_agent-1_max: 81
    apples_agent-1_mean: 26.82
    apples_agent-1_min: 0
    apples_agent-2_max: 285
    apples_agent-2_mean: 9.7
    apples_agent-2_min: 0
    apples_agent-3_max: 172
    apples_agent-3_mean: 96.72
    apples_agent-3_min: 30
    apples_agent-4_max: 33
    apples_agent-4_mean: 1.96
    apples_agent-4_min: 0
    apples_agent-5_max: 128
    apples_agent-5_mean: 66.1
    apples_agent-5_min: 7
    cleaning_beam_agent-0_max: 369
    cleaning_beam_agent-0_mean: 242.92
    cleaning_beam_agent-0_min: 116
    cleaning_beam_agent-1_max: 373
    cleaning_beam_agent-1_mean: 212.24
    cleaning_beam_agent-1_min: 71
    cleaning_beam_agent-2_max: 462
    cleaning_beam_agent-2_mean: 303.89
    cleaning_beam_agent-2_min: 111
    cleaning_beam_agent-3_max: 145
    cleaning_beam_agent-3_mean: 50.67
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 496
    cleaning_beam_agent-4_mean: 381.64
    cleaning_beam_agent-4_min: 140
    cleaning_beam_agent-5_max: 366
    cleaning_beam_agent-5_mean: 90.97
    cleaning_beam_agent-5_min: 14
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-47-56
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 865.9999999999837
  episode_reward_mean: 593.2599999999999
  episode_reward_min: 193.99999999999852
  episodes_this_iter: 96
  episodes_total: 13152
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19730.576
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004453056026250124
        entropy: 1.1918705701828003
        entropy_coeff: 0.0017600000137463212
        kl: 0.012020752765238285
        model: {}
        policy_loss: -0.03283623978495598
        total_loss: -0.030887940898537636
        vf_explained_var: 0.06211280822753906
        vf_loss: 16.418373107910156
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004453056026250124
        entropy: 1.140013575553894
        entropy_coeff: 0.0017600000137463212
        kl: 0.012584280222654343
        model: {}
        policy_loss: -0.03553701564669609
        total_loss: -0.0333414226770401
        vf_explained_var: 0.03740786015987396
        vf_loss: 16.85159683227539
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004453056026250124
        entropy: 1.103824257850647
        entropy_coeff: 0.0017600000137463212
        kl: 0.012597811408340931
        model: {}
        policy_loss: -0.030720820650458336
        total_loss: -0.02851482480764389
        vf_explained_var: 0.07007607817649841
        vf_loss: 16.2916259765625
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004453056026250124
        entropy: 0.8660622239112854
        entropy_coeff: 0.0017600000137463212
        kl: 0.010885164141654968
        model: {}
        policy_loss: -0.026807578280568123
        total_loss: -0.024782828986644745
        vf_explained_var: 0.21630218625068665
        vf_loss: 13.719894409179688
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004453056026250124
        entropy: 1.0450420379638672
        entropy_coeff: 0.0017600000137463212
        kl: 0.012612106278538704
        model: {}
        policy_loss: -0.03470394015312195
        total_loss: -0.03243684768676758
        vf_explained_var: 0.09541139006614685
        vf_loss: 15.839441299438477
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004453056026250124
        entropy: 0.8773462772369385
        entropy_coeff: 0.0017600000137463212
        kl: 0.012177260592579842
        model: {}
        policy_loss: -0.03247549757361412
        total_loss: -0.030142823234200478
        vf_explained_var: 0.17687159776687622
        vf_loss: 14.413568496704102
    load_time_ms: 15038.618
    num_steps_sampled: 13152000
    num_steps_trained: 13152000
    sample_time_ms: 90964.472
    update_time_ms: 21.91
  iterations_since_restore: 77
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.78715083798883
    ram_util_percent: 14.773743016759777
  pid: 24061
  policy_reward_max:
    agent-0: 144.33333333333348
    agent-1: 144.33333333333348
    agent-2: 144.33333333333348
    agent-3: 144.33333333333348
    agent-4: 144.33333333333348
    agent-5: 144.33333333333348
  policy_reward_mean:
    agent-0: 98.87666666666689
    agent-1: 98.87666666666689
    agent-2: 98.87666666666689
    agent-3: 98.87666666666689
    agent-4: 98.87666666666689
    agent-5: 98.87666666666689
  policy_reward_min:
    agent-0: 32.33333333333339
    agent-1: 32.33333333333339
    agent-2: 32.33333333333339
    agent-3: 32.33333333333339
    agent-4: 32.33333333333339
    agent-5: 32.33333333333339
  sampler_perf:
    mean_env_wait_ms: 24.602214900844633
    mean_inference_ms: 12.39537342634012
    mean_processing_ms: 51.335162644185466
  time_since_restore: 10475.496683359146
  time_this_iter_s: 125.36800694465637
  time_total_s: 19601.50849723816
  timestamp: 1637034476
  timesteps_since_restore: 7392000
  timesteps_this_iter: 96000
  timesteps_total: 13152000
  training_iteration: 137
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    137 |          19601.5 | 13152000 |   593.26 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 91
    apples_agent-0_mean: 8.42
    apples_agent-0_min: 0
    apples_agent-1_max: 121
    apples_agent-1_mean: 28.36
    apples_agent-1_min: 0
    apples_agent-2_max: 244
    apples_agent-2_mean: 8.29
    apples_agent-2_min: 0
    apples_agent-3_max: 165
    apples_agent-3_mean: 103.62
    apples_agent-3_min: 25
    apples_agent-4_max: 30
    apples_agent-4_mean: 0.95
    apples_agent-4_min: 0
    apples_agent-5_max: 129
    apples_agent-5_mean: 69.05
    apples_agent-5_min: 11
    cleaning_beam_agent-0_max: 458
    cleaning_beam_agent-0_mean: 259.3
    cleaning_beam_agent-0_min: 133
    cleaning_beam_agent-1_max: 379
    cleaning_beam_agent-1_mean: 216.59
    cleaning_beam_agent-1_min: 71
    cleaning_beam_agent-2_max: 478
    cleaning_beam_agent-2_mean: 322.96
    cleaning_beam_agent-2_min: 107
    cleaning_beam_agent-3_max: 209
    cleaning_beam_agent-3_mean: 54.21
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 508
    cleaning_beam_agent-4_mean: 378.92
    cleaning_beam_agent-4_min: 187
    cleaning_beam_agent-5_max: 257
    cleaning_beam_agent-5_mean: 85.46
    cleaning_beam_agent-5_min: 23
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-50-02
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 929.9999999999812
  episode_reward_mean: 623.2699999999966
  episode_reward_min: 154.99999999999906
  episodes_this_iter: 96
  episodes_total: 13248
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19722.849
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00043931519030593336
        entropy: 1.1739578247070312
        entropy_coeff: 0.0017600000137463212
        kl: 0.011729560792446136
        model: {}
        policy_loss: -0.031650133430957794
        total_loss: -0.029835829511284828
        vf_explained_var: 0.10524991154670715
        vf_loss: 15.345602989196777
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00043931519030593336
        entropy: 1.125905156135559
        entropy_coeff: 0.0017600000137463212
        kl: 0.013655804097652435
        model: {}
        policy_loss: -0.03457379341125488
        total_loss: -0.03212130442261696
        vf_explained_var: 0.00706067681312561
        vf_loss: 17.029224395751953
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00043931519030593336
        entropy: 1.0963557958602905
        entropy_coeff: 0.0017600000137463212
        kl: 0.012341550551354885
        model: {}
        policy_loss: -0.03152408078312874
        total_loss: -0.029503345489501953
        vf_explained_var: 0.13561829924583435
        vf_loss: 14.820108413696289
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00043931519030593336
        entropy: 0.8239694237709045
        entropy_coeff: 0.0017600000137463212
        kl: 0.009766394272446632
        model: {}
        policy_loss: -0.027107832953333855
        total_loss: -0.025268232449889183
        vf_explained_var: 0.2202950268983841
        vf_loss: 13.365055084228516
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00043931519030593336
        entropy: 1.0443496704101562
        entropy_coeff: 0.0017600000137463212
        kl: 0.01264004223048687
        model: {}
        policy_loss: -0.03434509411454201
        total_loss: -0.03207291290163994
        vf_explained_var: 0.07697522640228271
        vf_loss: 15.822264671325684
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00043931519030593336
        entropy: 0.8671431541442871
        entropy_coeff: 0.0017600000137463212
        kl: 0.011645183898508549
        model: {}
        policy_loss: -0.0329107791185379
        total_loss: -0.030707666650414467
        vf_explained_var: 0.18282738327980042
        vf_loss: 14.002460479736328
    load_time_ms: 14760.309
    num_steps_sampled: 13248000
    num_steps_trained: 13248000
    sample_time_ms: 91160.621
    update_time_ms: 21.377
  iterations_since_restore: 78
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.898870056497174
    ram_util_percent: 14.833898305084748
  pid: 24061
  policy_reward_max:
    agent-0: 154.99999999999997
    agent-1: 154.99999999999997
    agent-2: 154.99999999999997
    agent-3: 154.99999999999997
    agent-4: 154.99999999999997
    agent-5: 154.99999999999997
  policy_reward_mean:
    agent-0: 103.87833333333361
    agent-1: 103.87833333333361
    agent-2: 103.87833333333361
    agent-3: 103.87833333333361
    agent-4: 103.87833333333361
    agent-5: 103.87833333333361
  policy_reward_min:
    agent-0: 25.833333333333382
    agent-1: 25.833333333333382
    agent-2: 25.833333333333382
    agent-3: 25.833333333333382
    agent-4: 25.833333333333382
    agent-5: 25.833333333333382
  sampler_perf:
    mean_env_wait_ms: 24.598960781981408
    mean_inference_ms: 12.395552641309255
    mean_processing_ms: 51.337329552787104
  time_since_restore: 10601.097887992859
  time_this_iter_s: 125.60120463371277
  time_total_s: 19727.109701871872
  timestamp: 1637034602
  timesteps_since_restore: 7488000
  timesteps_this_iter: 96000
  timesteps_total: 13248000
  training_iteration: 138
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    138 |          19727.1 | 13248000 |   623.27 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 64
    apples_agent-0_mean: 7.1
    apples_agent-0_min: 0
    apples_agent-1_max: 103
    apples_agent-1_mean: 28.07
    apples_agent-1_min: 0
    apples_agent-2_max: 57
    apples_agent-2_mean: 4.55
    apples_agent-2_min: 0
    apples_agent-3_max: 179
    apples_agent-3_mean: 99.73
    apples_agent-3_min: 27
    apples_agent-4_max: 40
    apples_agent-4_mean: 1.64
    apples_agent-4_min: 0
    apples_agent-5_max: 119
    apples_agent-5_mean: 65.24
    apples_agent-5_min: 6
    cleaning_beam_agent-0_max: 477
    cleaning_beam_agent-0_mean: 275.1
    cleaning_beam_agent-0_min: 131
    cleaning_beam_agent-1_max: 351
    cleaning_beam_agent-1_mean: 214.05
    cleaning_beam_agent-1_min: 80
    cleaning_beam_agent-2_max: 562
    cleaning_beam_agent-2_mean: 325.55
    cleaning_beam_agent-2_min: 148
    cleaning_beam_agent-3_max: 210
    cleaning_beam_agent-3_mean: 54.53
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 473
    cleaning_beam_agent-4_mean: 367.45
    cleaning_beam_agent-4_min: 149
    cleaning_beam_agent-5_max: 320
    cleaning_beam_agent-5_mean: 84.5
    cleaning_beam_agent-5_min: 20
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-52-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 922.999999999984
  episode_reward_mean: 618.3399999999968
  episode_reward_min: 137.00000000000108
  episodes_this_iter: 96
  episodes_total: 13344
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19713.676
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00043332480709068477
        entropy: 1.186018943786621
        entropy_coeff: 0.0017600000137463212
        kl: 0.01162794604897499
        model: {}
        policy_loss: -0.032214175909757614
        total_loss: -0.03033820167183876
        vf_explained_var: 0.10883931815624237
        vf_loss: 16.377769470214844
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00043332480709068477
        entropy: 1.1290674209594727
        entropy_coeff: 0.0017600000137463212
        kl: 0.013759773224592209
        model: {}
        policy_loss: -0.03481099382042885
        total_loss: -0.032225776463747025
        vf_explained_var: 0.009497120976448059
        vf_loss: 18.204238891601562
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00043332480709068477
        entropy: 1.0887181758880615
        entropy_coeff: 0.0017600000137463212
        kl: 0.01215377077460289
        model: {}
        policy_loss: -0.030703816562891006
        total_loss: -0.02858351171016693
        vf_explained_var: 0.12592841684818268
        vf_loss: 16.056957244873047
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00043332480709068477
        entropy: 0.8373134136199951
        entropy_coeff: 0.0017600000137463212
        kl: 0.010076710022985935
        model: {}
        policy_loss: -0.0280148945748806
        total_loss: -0.026044689118862152
        vf_explained_var: 0.22248399257659912
        vf_loss: 14.285360336303711
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00043332480709068477
        entropy: 1.040889024734497
        entropy_coeff: 0.0017600000137463212
        kl: 0.013198667205870152
        model: {}
        policy_loss: -0.03445591777563095
        total_loss: -0.03200281783938408
        vf_explained_var: 0.10482029616832733
        vf_loss: 16.453304290771484
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00043332480709068477
        entropy: 0.8750952482223511
        entropy_coeff: 0.0017600000137463212
        kl: 0.011649834923446178
        model: {}
        policy_loss: -0.03282327950000763
        total_loss: -0.030596520751714706
        vf_explained_var: 0.21873843669891357
        vf_loss: 14.36961555480957
    load_time_ms: 14475.955
    num_steps_sampled: 13344000
    num_steps_trained: 13344000
    sample_time_ms: 91303.109
    update_time_ms: 20.996
  iterations_since_restore: 79
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.851381215469615
    ram_util_percent: 14.827071823204424
  pid: 24061
  policy_reward_max:
    agent-0: 153.83333333333303
    agent-1: 153.83333333333303
    agent-2: 153.83333333333303
    agent-3: 153.83333333333303
    agent-4: 153.83333333333303
    agent-5: 153.83333333333303
  policy_reward_mean:
    agent-0: 103.05666666666693
    agent-1: 103.05666666666693
    agent-2: 103.05666666666693
    agent-3: 103.05666666666693
    agent-4: 103.05666666666693
    agent-5: 103.05666666666693
  policy_reward_min:
    agent-0: 22.83333333333336
    agent-1: 22.83333333333336
    agent-2: 22.83333333333336
    agent-3: 22.83333333333336
    agent-4: 22.83333333333336
    agent-5: 22.83333333333336
  sampler_perf:
    mean_env_wait_ms: 24.596995558481904
    mean_inference_ms: 12.396288002134328
    mean_processing_ms: 51.337682710744495
  time_since_restore: 10728.032837867737
  time_this_iter_s: 126.93494987487793
  time_total_s: 19854.04465174675
  timestamp: 1637034729
  timesteps_since_restore: 7584000
  timesteps_this_iter: 96000
  timesteps_total: 13344000
  training_iteration: 139
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    139 |            19854 | 13344000 |   618.34 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 56
    apples_agent-0_mean: 7.62
    apples_agent-0_min: 0
    apples_agent-1_max: 78
    apples_agent-1_mean: 23.89
    apples_agent-1_min: 0
    apples_agent-2_max: 183
    apples_agent-2_mean: 9.13
    apples_agent-2_min: 0
    apples_agent-3_max: 187
    apples_agent-3_mean: 101.62
    apples_agent-3_min: 22
    apples_agent-4_max: 49
    apples_agent-4_mean: 1.52
    apples_agent-4_min: 0
    apples_agent-5_max: 138
    apples_agent-5_mean: 66.76
    apples_agent-5_min: 6
    cleaning_beam_agent-0_max: 488
    cleaning_beam_agent-0_mean: 276.13
    cleaning_beam_agent-0_min: 107
    cleaning_beam_agent-1_max: 402
    cleaning_beam_agent-1_mean: 225.88
    cleaning_beam_agent-1_min: 93
    cleaning_beam_agent-2_max: 527
    cleaning_beam_agent-2_mean: 313.58
    cleaning_beam_agent-2_min: 118
    cleaning_beam_agent-3_max: 210
    cleaning_beam_agent-3_mean: 51.65
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 483
    cleaning_beam_agent-4_mean: 366.05
    cleaning_beam_agent-4_min: 149
    cleaning_beam_agent-5_max: 373
    cleaning_beam_agent-5_mean: 102.43
    cleaning_beam_agent-5_min: 15
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-54-15
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 844.9999999999832
  episode_reward_mean: 609.7199999999974
  episode_reward_min: 181.0000000000011
  episodes_this_iter: 96
  episodes_total: 13440
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19717.848
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00042733439477160573
        entropy: 1.187943458557129
        entropy_coeff: 0.0017600000137463212
        kl: 0.011402564123272896
        model: {}
        policy_loss: -0.0315466970205307
        total_loss: -0.029770681634545326
        vf_explained_var: 0.06846305727958679
        vf_loss: 15.86285400390625
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00042733439477160573
        entropy: 1.1355650424957275
        entropy_coeff: 0.0017600000137463212
        kl: 0.013501828536391258
        model: {}
        policy_loss: -0.03351062163710594
        total_loss: -0.031190672889351845
        vf_explained_var: 0.04860769212245941
        vf_loss: 16.181795120239258
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00042733439477160573
        entropy: 1.1009913682937622
        entropy_coeff: 0.0017600000137463212
        kl: 0.012363819405436516
        model: {}
        policy_loss: -0.03039170429110527
        total_loss: -0.0284024216234684
        vf_explained_var: 0.14504213631153107
        vf_loss: 14.542646408081055
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00042733439477160573
        entropy: 0.8228009939193726
        entropy_coeff: 0.0017600000137463212
        kl: 0.010956558398902416
        model: {}
        policy_loss: -0.024840382859110832
        total_loss: -0.02271736040711403
        vf_explained_var: 0.18841975927352905
        vf_loss: 13.798412322998047
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00042733439477160573
        entropy: 1.0429301261901855
        entropy_coeff: 0.0017600000137463212
        kl: 0.013412922620773315
        model: {}
        policy_loss: -0.035392872989177704
        total_loss: -0.03294548764824867
        vf_explained_var: 0.05881574749946594
        vf_loss: 16.00353240966797
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00042733439477160573
        entropy: 0.8925291895866394
        entropy_coeff: 0.0017600000137463212
        kl: 0.011814938858151436
        model: {}
        policy_loss: -0.034002866595983505
        total_loss: -0.03184795379638672
        vf_explained_var: 0.19853341579437256
        vf_loss: 13.62773609161377
    load_time_ms: 14385.619
    num_steps_sampled: 13440000
    num_steps_trained: 13440000
    sample_time_ms: 91169.485
    update_time_ms: 21.876
  iterations_since_restore: 80
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.074157303370786
    ram_util_percent: 14.835955056179776
  pid: 24061
  policy_reward_max:
    agent-0: 140.8333333333334
    agent-1: 140.8333333333334
    agent-2: 140.8333333333334
    agent-3: 140.8333333333334
    agent-4: 140.8333333333334
    agent-5: 140.8333333333334
  policy_reward_mean:
    agent-0: 101.62000000000027
    agent-1: 101.62000000000027
    agent-2: 101.62000000000027
    agent-3: 101.62000000000027
    agent-4: 101.62000000000027
    agent-5: 101.62000000000027
  policy_reward_min:
    agent-0: 30.166666666666657
    agent-1: 30.166666666666657
    agent-2: 30.166666666666657
    agent-3: 30.166666666666657
    agent-4: 30.166666666666657
    agent-5: 30.166666666666657
  sampler_perf:
    mean_env_wait_ms: 24.59428419743705
    mean_inference_ms: 12.396387694529796
    mean_processing_ms: 51.3396933135597
  time_since_restore: 10853.236768722534
  time_this_iter_s: 125.20393085479736
  time_total_s: 19979.248582601547
  timestamp: 1637034855
  timesteps_since_restore: 7680000
  timesteps_this_iter: 96000
  timesteps_total: 13440000
  training_iteration: 140
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    140 |          19979.2 | 13440000 |   609.72 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 89
    apples_agent-0_mean: 5.99
    apples_agent-0_min: 0
    apples_agent-1_max: 104
    apples_agent-1_mean: 25.19
    apples_agent-1_min: 0
    apples_agent-2_max: 87
    apples_agent-2_mean: 7.38
    apples_agent-2_min: 0
    apples_agent-3_max: 161
    apples_agent-3_mean: 100.78
    apples_agent-3_min: 19
    apples_agent-4_max: 33
    apples_agent-4_mean: 1.71
    apples_agent-4_min: 0
    apples_agent-5_max: 122
    apples_agent-5_mean: 70.78
    apples_agent-5_min: 22
    cleaning_beam_agent-0_max: 485
    cleaning_beam_agent-0_mean: 296.57
    cleaning_beam_agent-0_min: 176
    cleaning_beam_agent-1_max: 356
    cleaning_beam_agent-1_mean: 227.96
    cleaning_beam_agent-1_min: 96
    cleaning_beam_agent-2_max: 438
    cleaning_beam_agent-2_mean: 309.9
    cleaning_beam_agent-2_min: 103
    cleaning_beam_agent-3_max: 160
    cleaning_beam_agent-3_mean: 55.58
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 513
    cleaning_beam_agent-4_mean: 372.78
    cleaning_beam_agent-4_min: 224
    cleaning_beam_agent-5_max: 257
    cleaning_beam_agent-5_mean: 87.3
    cleaning_beam_agent-5_min: 28
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-56-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 961.9999999999798
  episode_reward_mean: 639.169999999995
  episode_reward_min: 207.99999999999807
  episodes_this_iter: 96
  episodes_total: 13536
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19760.941
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00042134401155635715
        entropy: 1.1908626556396484
        entropy_coeff: 0.0017600000137463212
        kl: 0.01111851166933775
        model: {}
        policy_loss: -0.03244591876864433
        total_loss: -0.030657729133963585
        vf_explained_var: 0.0764908492565155
        vf_loss: 16.604036331176758
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00042134401155635715
        entropy: 1.123581051826477
        entropy_coeff: 0.0017600000137463212
        kl: 0.012643185444176197
        model: {}
        policy_loss: -0.032910969108343124
        total_loss: -0.030620263889431953
        vf_explained_var: 0.033705249428749084
        vf_loss: 17.395719528198242
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00042134401155635715
        entropy: 1.0987775325775146
        entropy_coeff: 0.0017600000137463212
        kl: 0.012057946063578129
        model: {}
        policy_loss: -0.028964180499315262
        total_loss: -0.026861807331442833
        vf_explained_var: 0.09713323414325714
        vf_loss: 16.24633026123047
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00042134401155635715
        entropy: 0.8055247664451599
        entropy_coeff: 0.0017600000137463212
        kl: 0.009570063091814518
        model: {}
        policy_loss: -0.026284432038664818
        total_loss: -0.024380864575505257
        vf_explained_var: 0.21829120814800262
        vf_loss: 14.07280445098877
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00042134401155635715
        entropy: 1.0423468351364136
        entropy_coeff: 0.0017600000137463212
        kl: 0.012698834761977196
        model: {}
        policy_loss: -0.03457573801279068
        total_loss: -0.032165735960006714
        vf_explained_var: 0.05203787982463837
        vf_loss: 17.047672271728516
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00042134401155635715
        entropy: 0.8796003460884094
        entropy_coeff: 0.0017600000137463212
        kl: 0.011277319863438606
        model: {}
        policy_loss: -0.031416695564985275
        total_loss: -0.029250647872686386
        vf_explained_var: 0.18926513195037842
        vf_loss: 14.586787223815918
    load_time_ms: 14446.009
    num_steps_sampled: 13536000
    num_steps_trained: 13536000
    sample_time_ms: 91082.911
    update_time_ms: 21.159
  iterations_since_restore: 81
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.974301675977653
    ram_util_percent: 14.861452513966485
  pid: 24061
  policy_reward_max:
    agent-0: 160.33333333333306
    agent-1: 160.33333333333306
    agent-2: 160.33333333333306
    agent-3: 160.33333333333306
    agent-4: 160.33333333333306
    agent-5: 160.33333333333306
  policy_reward_mean:
    agent-0: 106.52833333333363
    agent-1: 106.52833333333363
    agent-2: 106.52833333333363
    agent-3: 106.52833333333363
    agent-4: 106.52833333333363
    agent-5: 106.52833333333363
  policy_reward_min:
    agent-0: 34.666666666666714
    agent-1: 34.666666666666714
    agent-2: 34.666666666666714
    agent-3: 34.666666666666714
    agent-4: 34.666666666666714
    agent-5: 34.666666666666714
  sampler_perf:
    mean_env_wait_ms: 24.592681997384922
    mean_inference_ms: 12.397988499070756
    mean_processing_ms: 51.34300915763921
  time_since_restore: 10978.304630041122
  time_this_iter_s: 125.06786131858826
  time_total_s: 20104.316443920135
  timestamp: 1637034980
  timesteps_since_restore: 7776000
  timesteps_this_iter: 96000
  timesteps_total: 13536000
  training_iteration: 141
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    141 |          20104.3 | 13536000 |   639.17 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 43
    apples_agent-0_mean: 5.86
    apples_agent-0_min: 0
    apples_agent-1_max: 136
    apples_agent-1_mean: 26.02
    apples_agent-1_min: 0
    apples_agent-2_max: 181
    apples_agent-2_mean: 6.57
    apples_agent-2_min: 0
    apples_agent-3_max: 176
    apples_agent-3_mean: 102.88
    apples_agent-3_min: 33
    apples_agent-4_max: 26
    apples_agent-4_mean: 1.54
    apples_agent-4_min: 0
    apples_agent-5_max: 116
    apples_agent-5_mean: 68.79
    apples_agent-5_min: 18
    cleaning_beam_agent-0_max: 461
    cleaning_beam_agent-0_mean: 290.72
    cleaning_beam_agent-0_min: 124
    cleaning_beam_agent-1_max: 413
    cleaning_beam_agent-1_mean: 238.18
    cleaning_beam_agent-1_min: 119
    cleaning_beam_agent-2_max: 484
    cleaning_beam_agent-2_mean: 328.46
    cleaning_beam_agent-2_min: 67
    cleaning_beam_agent-3_max: 168
    cleaning_beam_agent-3_mean: 53.45
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 505
    cleaning_beam_agent-4_mean: 378.55
    cleaning_beam_agent-4_min: 270
    cleaning_beam_agent-5_max: 362
    cleaning_beam_agent-5_mean: 84.38
    cleaning_beam_agent-5_min: 17
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-58-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 865.9999999999789
  episode_reward_mean: 650.9599999999956
  episode_reward_min: 317.999999999999
  episodes_this_iter: 96
  episodes_total: 13632
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19782.454
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004153535992372781
        entropy: 1.1834709644317627
        entropy_coeff: 0.0017600000137463212
        kl: 0.011424991302192211
        model: {}
        policy_loss: -0.03115752339363098
        total_loss: -0.02953147515654564
        vf_explained_var: 0.05078965425491333
        vf_loss: 14.239596366882324
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004153535992372781
        entropy: 1.1322400569915771
        entropy_coeff: 0.0017600000137463212
        kl: 0.012234640307724476
        model: {}
        policy_loss: -0.033796653151512146
        total_loss: -0.03188346326351166
        vf_explained_var: 0.02781890332698822
        vf_loss: 14.590058326721191
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004153535992372781
        entropy: 1.104198932647705
        entropy_coeff: 0.0017600000137463212
        kl: 0.012056336738169193
        model: {}
        policy_loss: -0.029922131448984146
        total_loss: -0.02804774045944214
        vf_explained_var: 0.061986565589904785
        vf_loss: 14.06512451171875
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004153535992372781
        entropy: 0.800020694732666
        entropy_coeff: 0.0017600000137463212
        kl: 0.009668936021625996
        model: {}
        policy_loss: -0.023745808750391006
        total_loss: -0.021949417889118195
        vf_explained_var: 0.15403787791728973
        vf_loss: 12.7063627243042
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004153535992372781
        entropy: 1.042222499847412
        entropy_coeff: 0.0017600000137463212
        kl: 0.012399114668369293
        model: {}
        policy_loss: -0.034421250224113464
        total_loss: -0.03237386792898178
        vf_explained_var: 0.06555995345115662
        vf_loss: 14.018685340881348
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004153535992372781
        entropy: 0.8497611284255981
        entropy_coeff: 0.0017600000137463212
        kl: 0.0115297706797719
        model: {}
        policy_loss: -0.030424123629927635
        total_loss: -0.02828277461230755
        vf_explained_var: 0.1135723888874054
        vf_loss: 13.309707641601562
    load_time_ms: 14466.038
    num_steps_sampled: 13632000
    num_steps_trained: 13632000
    sample_time_ms: 91165.789
    update_time_ms: 20.719
  iterations_since_restore: 82
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.95195530726257
    ram_util_percent: 14.834078212290503
  pid: 24061
  policy_reward_max:
    agent-0: 144.3333333333334
    agent-1: 144.3333333333334
    agent-2: 144.3333333333334
    agent-3: 144.3333333333334
    agent-4: 144.3333333333334
    agent-5: 144.3333333333334
  policy_reward_mean:
    agent-0: 108.49333333333365
    agent-1: 108.49333333333365
    agent-2: 108.49333333333365
    agent-3: 108.49333333333365
    agent-4: 108.49333333333365
    agent-5: 108.49333333333365
  policy_reward_min:
    agent-0: 52.999999999999865
    agent-1: 52.999999999999865
    agent-2: 52.999999999999865
    agent-3: 52.999999999999865
    agent-4: 52.999999999999865
    agent-5: 52.999999999999865
  sampler_perf:
    mean_env_wait_ms: 24.59229020616044
    mean_inference_ms: 12.397898677111723
    mean_processing_ms: 51.343333680732115
  time_since_restore: 11103.857537984848
  time_this_iter_s: 125.55290794372559
  time_total_s: 20229.86935186386
  timestamp: 1637035106
  timesteps_since_restore: 7872000
  timesteps_this_iter: 96000
  timesteps_total: 13632000
  training_iteration: 142
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    142 |          20229.9 | 13632000 |   650.96 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 79
    apples_agent-0_mean: 9.25
    apples_agent-0_min: 0
    apples_agent-1_max: 82
    apples_agent-1_mean: 23.25
    apples_agent-1_min: 0
    apples_agent-2_max: 340
    apples_agent-2_mean: 10.08
    apples_agent-2_min: 0
    apples_agent-3_max: 227
    apples_agent-3_mean: 97.14
    apples_agent-3_min: 36
    apples_agent-4_max: 38
    apples_agent-4_mean: 2.39
    apples_agent-4_min: 0
    apples_agent-5_max: 131
    apples_agent-5_mean: 66.51
    apples_agent-5_min: 10
    cleaning_beam_agent-0_max: 431
    cleaning_beam_agent-0_mean: 290.47
    cleaning_beam_agent-0_min: 108
    cleaning_beam_agent-1_max: 380
    cleaning_beam_agent-1_mean: 232.03
    cleaning_beam_agent-1_min: 91
    cleaning_beam_agent-2_max: 471
    cleaning_beam_agent-2_mean: 308.97
    cleaning_beam_agent-2_min: 37
    cleaning_beam_agent-3_max: 171
    cleaning_beam_agent-3_mean: 60.08
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 467
    cleaning_beam_agent-4_mean: 372.97
    cleaning_beam_agent-4_min: 177
    cleaning_beam_agent-5_max: 172
    cleaning_beam_agent-5_mean: 80.12
    cleaning_beam_agent-5_min: 18
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-00-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 868.9999999999926
  episode_reward_mean: 623.839999999997
  episode_reward_min: 335.99999999999926
  episodes_this_iter: 96
  episodes_total: 13728
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19791.24
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00040936318691819906
        entropy: 1.1847587823867798
        entropy_coeff: 0.0017600000137463212
        kl: 0.012005513533949852
        model: {}
        policy_loss: -0.030702106654644012
        total_loss: -0.02881527692079544
        vf_explained_var: 0.05932788550853729
        vf_loss: 15.708993911743164
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00040936318691819906
        entropy: 1.1480861902236938
        entropy_coeff: 0.0017600000137463212
        kl: 0.012599648907780647
        model: {}
        policy_loss: -0.03412041440606117
        total_loss: -0.03200877085328102
        vf_explained_var: 0.03286270797252655
        vf_loss: 16.123403549194336
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00040936318691819906
        entropy: 1.1004934310913086
        entropy_coeff: 0.0017600000137463212
        kl: 0.012229179963469505
        model: {}
        policy_loss: -0.029933124780654907
        total_loss: -0.027886582538485527
        vf_explained_var: 0.0789370983839035
        vf_loss: 15.375712394714355
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00040936318691819906
        entropy: 0.8371767997741699
        entropy_coeff: 0.0017600000137463212
        kl: 0.009464379400014877
        model: {}
        policy_loss: -0.024908408522605896
        total_loss: -0.0231655053794384
        vf_explained_var: 0.20657335221767426
        vf_loss: 13.234580993652344
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00040936318691819906
        entropy: 1.021183729171753
        entropy_coeff: 0.0017600000137463212
        kl: 0.012500361539423466
        model: {}
        policy_loss: -0.034211400896310806
        total_loss: -0.03199775516986847
        vf_explained_var: 0.0945921242237091
        vf_loss: 15.108515739440918
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00040936318691819906
        entropy: 0.8862351775169373
        entropy_coeff: 0.0017600000137463212
        kl: 0.011557011865079403
        model: {}
        policy_loss: -0.03144105151295662
        total_loss: -0.02934974431991577
        vf_explained_var: 0.1971837878227234
        vf_loss: 13.396775245666504
    load_time_ms: 14697.953
    num_steps_sampled: 13728000
    num_steps_trained: 13728000
    sample_time_ms: 91324.453
    update_time_ms: 20.47
  iterations_since_restore: 83
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.860555555555553
    ram_util_percent: 14.886666666666668
  pid: 24061
  policy_reward_max:
    agent-0: 144.83333333333331
    agent-1: 144.83333333333331
    agent-2: 144.83333333333331
    agent-3: 144.83333333333331
    agent-4: 144.83333333333331
    agent-5: 144.83333333333331
  policy_reward_mean:
    agent-0: 103.97333333333357
    agent-1: 103.97333333333357
    agent-2: 103.97333333333357
    agent-3: 103.97333333333357
    agent-4: 103.97333333333357
    agent-5: 103.97333333333357
  policy_reward_min:
    agent-0: 55.999999999999865
    agent-1: 55.999999999999865
    agent-2: 55.999999999999865
    agent-3: 55.999999999999865
    agent-4: 55.999999999999865
    agent-5: 55.999999999999865
  sampler_perf:
    mean_env_wait_ms: 24.58782396750845
    mean_inference_ms: 12.397813323960019
    mean_processing_ms: 51.34035951557452
  time_since_restore: 11230.115588188171
  time_this_iter_s: 126.25805020332336
  time_total_s: 20356.127402067184
  timestamp: 1637035232
  timesteps_since_restore: 7968000
  timesteps_this_iter: 96000
  timesteps_total: 13728000
  training_iteration: 143
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    143 |          20356.1 | 13728000 |   623.84 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 66
    apples_agent-0_mean: 8.13
    apples_agent-0_min: 0
    apples_agent-1_max: 81
    apples_agent-1_mean: 22.09
    apples_agent-1_min: 0
    apples_agent-2_max: 170
    apples_agent-2_mean: 8.57
    apples_agent-2_min: 0
    apples_agent-3_max: 174
    apples_agent-3_mean: 97.84
    apples_agent-3_min: 43
    apples_agent-4_max: 48
    apples_agent-4_mean: 2.67
    apples_agent-4_min: 0
    apples_agent-5_max: 124
    apples_agent-5_mean: 69.71
    apples_agent-5_min: 17
    cleaning_beam_agent-0_max: 420
    cleaning_beam_agent-0_mean: 269.37
    cleaning_beam_agent-0_min: 113
    cleaning_beam_agent-1_max: 443
    cleaning_beam_agent-1_mean: 228.59
    cleaning_beam_agent-1_min: 105
    cleaning_beam_agent-2_max: 413
    cleaning_beam_agent-2_mean: 311.51
    cleaning_beam_agent-2_min: 102
    cleaning_beam_agent-3_max: 176
    cleaning_beam_agent-3_mean: 58.97
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 493
    cleaning_beam_agent-4_mean: 364.15
    cleaning_beam_agent-4_min: 157
    cleaning_beam_agent-5_max: 354
    cleaning_beam_agent-5_mean: 95.13
    cleaning_beam_agent-5_min: 22
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-02-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 929.9999999999711
  episode_reward_mean: 627.3599999999968
  episode_reward_min: 288.999999999997
  episodes_this_iter: 96
  episodes_total: 13824
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19841.448
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004033728037029505
        entropy: 1.1902153491973877
        entropy_coeff: 0.0017600000137463212
        kl: 0.011596864089369774
        model: {}
        policy_loss: -0.031178314238786697
        total_loss: -0.029432173818349838
        vf_explained_var: 0.08584156632423401
        vf_loss: 15.215471267700195
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004033728037029505
        entropy: 1.1211812496185303
        entropy_coeff: 0.0017600000137463212
        kl: 0.012908236123621464
        model: {}
        policy_loss: -0.0333743542432785
        total_loss: -0.031149689108133316
        vf_explained_var: 0.02875550091266632
        vf_loss: 16.162952423095703
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004033728037029505
        entropy: 1.092413306236267
        entropy_coeff: 0.0017600000137463212
        kl: 0.012009349651634693
        model: {}
        policy_loss: -0.03171280771493912
        total_loss: -0.029690595343708992
        vf_explained_var: 0.07214626669883728
        vf_loss: 15.429915428161621
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004033728037029505
        entropy: 0.8371931910514832
        entropy_coeff: 0.0017600000137463212
        kl: 0.010419312864542007
        model: {}
        policy_loss: -0.026687953621149063
        total_loss: -0.024767521768808365
        vf_explained_var: 0.21202446520328522
        vf_loss: 13.10031795501709
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004033728037029505
        entropy: 1.0319234132766724
        entropy_coeff: 0.0017600000137463212
        kl: 0.012754990719258785
        model: {}
        policy_loss: -0.03492838516831398
        total_loss: -0.03270234167575836
        vf_explained_var: 0.10328987240791321
        vf_loss: 14.91229248046875
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004033728037029505
        entropy: 0.8768635988235474
        entropy_coeff: 0.0017600000137463212
        kl: 0.01170838437974453
        model: {}
        policy_loss: -0.03187553957104683
        total_loss: -0.029703836888074875
        vf_explained_var: 0.17453037202358246
        vf_loss: 13.733010292053223
    load_time_ms: 14277.042
    num_steps_sampled: 13824000
    num_steps_trained: 13824000
    sample_time_ms: 91628.668
    update_time_ms: 20.693
  iterations_since_restore: 84
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.988826815642458
    ram_util_percent: 14.875977653631287
  pid: 24061
  policy_reward_max:
    agent-0: 154.99999999999977
    agent-1: 154.99999999999977
    agent-2: 154.99999999999977
    agent-3: 154.99999999999977
    agent-4: 154.99999999999977
    agent-5: 154.99999999999977
  policy_reward_mean:
    agent-0: 104.56000000000027
    agent-1: 104.56000000000027
    agent-2: 104.56000000000027
    agent-3: 104.56000000000027
    agent-4: 104.56000000000027
    agent-5: 104.56000000000027
  policy_reward_min:
    agent-0: 48.16666666666656
    agent-1: 48.16666666666656
    agent-2: 48.16666666666656
    agent-3: 48.16666666666656
    agent-4: 48.16666666666656
    agent-5: 48.16666666666656
  sampler_perf:
    mean_env_wait_ms: 24.58398284844684
    mean_inference_ms: 12.39748314888947
    mean_processing_ms: 51.34209412218135
  time_since_restore: 11355.364911794662
  time_this_iter_s: 125.24932360649109
  time_total_s: 20481.376725673676
  timestamp: 1637035358
  timesteps_since_restore: 8064000
  timesteps_this_iter: 96000
  timesteps_total: 13824000
  training_iteration: 144
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    144 |          20481.4 | 13824000 |   627.36 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 106
    apples_agent-0_mean: 9.6
    apples_agent-0_min: 0
    apples_agent-1_max: 79
    apples_agent-1_mean: 25.23
    apples_agent-1_min: 0
    apples_agent-2_max: 78
    apples_agent-2_mean: 6.8
    apples_agent-2_min: 0
    apples_agent-3_max: 161
    apples_agent-3_mean: 99.31
    apples_agent-3_min: 42
    apples_agent-4_max: 24
    apples_agent-4_mean: 1.12
    apples_agent-4_min: 0
    apples_agent-5_max: 136
    apples_agent-5_mean: 75.24
    apples_agent-5_min: 13
    cleaning_beam_agent-0_max: 396
    cleaning_beam_agent-0_mean: 280.35
    cleaning_beam_agent-0_min: 149
    cleaning_beam_agent-1_max: 487
    cleaning_beam_agent-1_mean: 223.07
    cleaning_beam_agent-1_min: 89
    cleaning_beam_agent-2_max: 420
    cleaning_beam_agent-2_mean: 306.95
    cleaning_beam_agent-2_min: 101
    cleaning_beam_agent-3_max: 146
    cleaning_beam_agent-3_mean: 60.43
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 546
    cleaning_beam_agent-4_mean: 362.22
    cleaning_beam_agent-4_min: 235
    cleaning_beam_agent-5_max: 318
    cleaning_beam_agent-5_mean: 97.41
    cleaning_beam_agent-5_min: 22
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-05-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 872.9999999999725
  episode_reward_mean: 642.3899999999968
  episode_reward_min: 389.00000000000847
  episodes_this_iter: 96
  episodes_total: 13920
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19853.992
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00039738239138387144
        entropy: 1.1860454082489014
        entropy_coeff: 0.0017600000137463212
        kl: 0.011281358078122139
        model: {}
        policy_loss: -0.03062511794269085
        total_loss: -0.028924843296408653
        vf_explained_var: 0.07018885016441345
        vf_loss: 15.314434051513672
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00039738239138387144
        entropy: 1.1305428743362427
        entropy_coeff: 0.0017600000137463212
        kl: 0.011774145998060703
        model: {}
        policy_loss: -0.032031916081905365
        total_loss: -0.030057676136493683
        vf_explained_var: 0.022468775510787964
        vf_loss: 16.091697692871094
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00039738239138387144
        entropy: 1.0968821048736572
        entropy_coeff: 0.0017600000137463212
        kl: 0.011872483417391777
        model: {}
        policy_loss: -0.030551567673683167
        total_loss: -0.028610751032829285
        vf_explained_var: 0.09135791659355164
        vf_loss: 14.968313217163086
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00039738239138387144
        entropy: 0.8286212086677551
        entropy_coeff: 0.0017600000137463212
        kl: 0.010053306818008423
        model: {}
        policy_loss: -0.026432223618030548
        total_loss: -0.024572106078267097
        vf_explained_var: 0.2044549584388733
        vf_loss: 13.078332901000977
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00039738239138387144
        entropy: 1.0376514196395874
        entropy_coeff: 0.0017600000137463212
        kl: 0.01201539859175682
        model: {}
        policy_loss: -0.03415737673640251
        total_loss: -0.03202146291732788
        vf_explained_var: 0.05299709737300873
        vf_loss: 15.591044425964355
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00039738239138387144
        entropy: 0.8770809173583984
        entropy_coeff: 0.0017600000137463212
        kl: 0.01237774919718504
        model: {}
        policy_loss: -0.03198636323213577
        total_loss: -0.0297147948294878
        vf_explained_var: 0.1867060661315918
        vf_loss: 13.396862983703613
    load_time_ms: 16147.558
    num_steps_sampled: 13920000
    num_steps_trained: 13920000
    sample_time_ms: 91728.376
    update_time_ms: 20.123
  iterations_since_restore: 85
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.216908212560384
    ram_util_percent: 15.063768115942032
  pid: 24061
  policy_reward_max:
    agent-0: 145.50000000000006
    agent-1: 145.50000000000006
    agent-2: 145.50000000000006
    agent-3: 145.50000000000006
    agent-4: 145.50000000000006
    agent-5: 145.50000000000006
  policy_reward_mean:
    agent-0: 107.06500000000031
    agent-1: 107.06500000000031
    agent-2: 107.06500000000031
    agent-3: 107.06500000000031
    agent-4: 107.06500000000031
    agent-5: 107.06500000000031
  policy_reward_min:
    agent-0: 64.8333333333332
    agent-1: 64.8333333333332
    agent-2: 64.8333333333332
    agent-3: 64.8333333333332
    agent-4: 64.8333333333332
    agent-5: 64.8333333333332
  sampler_perf:
    mean_env_wait_ms: 24.578618103300077
    mean_inference_ms: 12.39582644112095
    mean_processing_ms: 51.3392948725884
  time_since_restore: 11500.534997701645
  time_this_iter_s: 145.17008590698242
  time_total_s: 20626.546811580658
  timestamp: 1637035503
  timesteps_since_restore: 8160000
  timesteps_this_iter: 96000
  timesteps_total: 13920000
  training_iteration: 145
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    145 |          20626.5 | 13920000 |   642.39 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 65
    apples_agent-0_mean: 7.14
    apples_agent-0_min: 0
    apples_agent-1_max: 76
    apples_agent-1_mean: 24.3
    apples_agent-1_min: 0
    apples_agent-2_max: 76
    apples_agent-2_mean: 5.11
    apples_agent-2_min: 0
    apples_agent-3_max: 226
    apples_agent-3_mean: 96.44
    apples_agent-3_min: 34
    apples_agent-4_max: 70
    apples_agent-4_mean: 1.64
    apples_agent-4_min: 0
    apples_agent-5_max: 163
    apples_agent-5_mean: 77.98
    apples_agent-5_min: 33
    cleaning_beam_agent-0_max: 442
    cleaning_beam_agent-0_mean: 268.22
    cleaning_beam_agent-0_min: 119
    cleaning_beam_agent-1_max: 360
    cleaning_beam_agent-1_mean: 215.47
    cleaning_beam_agent-1_min: 65
    cleaning_beam_agent-2_max: 462
    cleaning_beam_agent-2_mean: 314.37
    cleaning_beam_agent-2_min: 155
    cleaning_beam_agent-3_max: 146
    cleaning_beam_agent-3_mean: 58.65
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 518
    cleaning_beam_agent-4_mean: 369.19
    cleaning_beam_agent-4_min: 151
    cleaning_beam_agent-5_max: 156
    cleaning_beam_agent-5_mean: 78.92
    cleaning_beam_agent-5_min: 23
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-07-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 863.9999999999859
  episode_reward_mean: 644.1099999999965
  episode_reward_min: 342.00000000000074
  episodes_this_iter: 96
  episodes_total: 14016
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19853.095
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00039139200816862285
        entropy: 1.1730892658233643
        entropy_coeff: 0.0017600000137463212
        kl: 0.011099404655396938
        model: {}
        policy_loss: -0.031324878334999084
        total_loss: -0.02969018742442131
        vf_explained_var: 0.0481087863445282
        vf_loss: 14.794492721557617
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00039139200816862285
        entropy: 1.1280399560928345
        entropy_coeff: 0.0017600000137463212
        kl: 0.012314984574913979
        model: {}
        policy_loss: -0.032845333218574524
        total_loss: -0.030862294137477875
        vf_explained_var: 0.03124883770942688
        vf_loss: 15.053933143615723
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00039139200816862285
        entropy: 1.1060571670532227
        entropy_coeff: 0.0017600000137463212
        kl: 0.012351619079709053
        model: {}
        policy_loss: -0.03008178621530533
        total_loss: -0.02808614820241928
        vf_explained_var: 0.0530058890581131
        vf_loss: 14.719724655151367
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00039139200816862285
        entropy: 0.8313028812408447
        entropy_coeff: 0.0017600000137463212
        kl: 0.009875568561255932
        model: {}
        policy_loss: -0.025445278733968735
        total_loss: -0.023637430742383003
        vf_explained_var: 0.1669084131717682
        vf_loss: 12.958288192749023
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00039139200816862285
        entropy: 1.035965085029602
        entropy_coeff: 0.0017600000137463212
        kl: 0.012493111193180084
        model: {}
        policy_loss: -0.03421253710985184
        total_loss: -0.03206551820039749
        vf_explained_var: 0.05290050804615021
        vf_loss: 14.71693229675293
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00039139200816862285
        entropy: 0.8694698810577393
        entropy_coeff: 0.0017600000137463212
        kl: 0.011387285776436329
        model: {}
        policy_loss: -0.0314377136528492
        total_loss: -0.029349692165851593
        vf_explained_var: 0.1381502002477646
        vf_loss: 13.40831470489502
    load_time_ms: 15656.21
    num_steps_sampled: 14016000
    num_steps_trained: 14016000
    sample_time_ms: 91882.47
    update_time_ms: 20.886
  iterations_since_restore: 86
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.05842696629213
    ram_util_percent: 14.776404494382021
  pid: 24061
  policy_reward_max:
    agent-0: 143.99999999999991
    agent-1: 143.99999999999991
    agent-2: 143.99999999999991
    agent-3: 143.99999999999991
    agent-4: 143.99999999999991
    agent-5: 143.99999999999991
  policy_reward_mean:
    agent-0: 107.35166666666693
    agent-1: 107.35166666666693
    agent-2: 107.35166666666693
    agent-3: 107.35166666666693
    agent-4: 107.35166666666693
    agent-5: 107.35166666666693
  policy_reward_min:
    agent-0: 56.999999999999865
    agent-1: 56.999999999999865
    agent-2: 56.999999999999865
    agent-3: 56.999999999999865
    agent-4: 56.999999999999865
    agent-5: 56.999999999999865
  sampler_perf:
    mean_env_wait_ms: 24.575723740095864
    mean_inference_ms: 12.3964643709977
    mean_processing_ms: 51.345743029241895
  time_since_restore: 11625.338923215866
  time_this_iter_s: 124.80392551422119
  time_total_s: 20751.35073709488
  timestamp: 1637035628
  timesteps_since_restore: 8256000
  timesteps_this_iter: 96000
  timesteps_total: 14016000
  training_iteration: 146
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    146 |          20751.4 | 14016000 |   644.11 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 40
    apples_agent-0_mean: 5.4
    apples_agent-0_min: 0
    apples_agent-1_max: 91
    apples_agent-1_mean: 24.27
    apples_agent-1_min: 0
    apples_agent-2_max: 108
    apples_agent-2_mean: 7.39
    apples_agent-2_min: 0
    apples_agent-3_max: 167
    apples_agent-3_mean: 96.78
    apples_agent-3_min: 48
    apples_agent-4_max: 70
    apples_agent-4_mean: 1.42
    apples_agent-4_min: 0
    apples_agent-5_max: 166
    apples_agent-5_mean: 75.3
    apples_agent-5_min: 30
    cleaning_beam_agent-0_max: 419
    cleaning_beam_agent-0_mean: 281.92
    cleaning_beam_agent-0_min: 121
    cleaning_beam_agent-1_max: 412
    cleaning_beam_agent-1_mean: 238.02
    cleaning_beam_agent-1_min: 92
    cleaning_beam_agent-2_max: 479
    cleaning_beam_agent-2_mean: 303.65
    cleaning_beam_agent-2_min: 108
    cleaning_beam_agent-3_max: 156
    cleaning_beam_agent-3_mean: 61.87
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 462
    cleaning_beam_agent-4_mean: 362.44
    cleaning_beam_agent-4_min: 200
    cleaning_beam_agent-5_max: 247
    cleaning_beam_agent-5_mean: 83.9
    cleaning_beam_agent-5_min: 16
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-09-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 865.9999999999809
  episode_reward_mean: 641.4599999999969
  episode_reward_min: 312.0000000000008
  episodes_this_iter: 96
  episodes_total: 14112
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19812.614
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003854015958495438
        entropy: 1.1662142276763916
        entropy_coeff: 0.0017600000137463212
        kl: 0.010508622042834759
        model: {}
        policy_loss: -0.029168108478188515
        total_loss: -0.027581607922911644
        vf_explained_var: 0.08984382450580597
        vf_loss: 15.373169898986816
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003854015958495438
        entropy: 1.1157020330429077
        entropy_coeff: 0.0017600000137463212
        kl: 0.012391708791255951
        model: {}
        policy_loss: -0.033559661358594894
        total_loss: -0.031438007950782776
        vf_explained_var: 0.048373639583587646
        vf_loss: 16.069414138793945
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003854015958495438
        entropy: 1.1082497835159302
        entropy_coeff: 0.0017600000137463212
        kl: 0.01235571876168251
        model: {}
        policy_loss: -0.03157545253634453
        total_loss: -0.029526878148317337
        vf_explained_var: 0.09512503445148468
        vf_loss: 15.279480934143066
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003854015958495438
        entropy: 0.8299895524978638
        entropy_coeff: 0.0017600000137463212
        kl: 0.009425999596714973
        model: {}
        policy_loss: -0.025438867509365082
        total_loss: -0.02361510507762432
        vf_explained_var: 0.17142407596111298
        vf_loss: 13.993473052978516
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003854015958495438
        entropy: 1.0353671312332153
        entropy_coeff: 0.0017600000137463212
        kl: 0.012337366119027138
        model: {}
        policy_loss: -0.03438820689916611
        total_loss: -0.03215525299310684
        vf_explained_var: 0.06051993370056152
        vf_loss: 15.87724781036377
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003854015958495438
        entropy: 0.8666080236434937
        entropy_coeff: 0.0017600000137463212
        kl: 0.011883028782904148
        model: {}
        policy_loss: -0.03193753957748413
        total_loss: -0.02970714308321476
        vf_explained_var: 0.18341179192066193
        vf_loss: 13.79018497467041
    load_time_ms: 15799.148
    num_steps_sampled: 14112000
    num_steps_trained: 14112000
    sample_time_ms: 91810.118
    update_time_ms: 20.777
  iterations_since_restore: 87
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.9536312849162
    ram_util_percent: 14.835754189944137
  pid: 24061
  policy_reward_max:
    agent-0: 144.33333333333354
    agent-1: 144.33333333333354
    agent-2: 144.33333333333354
    agent-3: 144.33333333333354
    agent-4: 144.33333333333354
    agent-5: 144.33333333333354
  policy_reward_mean:
    agent-0: 106.91000000000031
    agent-1: 106.91000000000031
    agent-2: 106.91000000000031
    agent-3: 106.91000000000031
    agent-4: 106.91000000000031
    agent-5: 106.91000000000031
  policy_reward_min:
    agent-0: 52.00000000000002
    agent-1: 52.00000000000002
    agent-2: 52.00000000000002
    agent-3: 52.00000000000002
    agent-4: 52.00000000000002
    agent-5: 52.00000000000002
  sampler_perf:
    mean_env_wait_ms: 24.575423681405404
    mean_inference_ms: 12.397413794712651
    mean_processing_ms: 51.34998548728188
  time_since_restore: 11750.998063087463
  time_this_iter_s: 125.65913987159729
  time_total_s: 20877.009876966476
  timestamp: 1637035754
  timesteps_since_restore: 8352000
  timesteps_this_iter: 96000
  timesteps_total: 14112000
  training_iteration: 147
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    147 |            20877 | 14112000 |   641.46 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 55
    apples_agent-0_mean: 4.74
    apples_agent-0_min: 0
    apples_agent-1_max: 91
    apples_agent-1_mean: 23.97
    apples_agent-1_min: 0
    apples_agent-2_max: 76
    apples_agent-2_mean: 7.01
    apples_agent-2_min: 0
    apples_agent-3_max: 168
    apples_agent-3_mean: 93.67
    apples_agent-3_min: 13
    apples_agent-4_max: 40
    apples_agent-4_mean: 1.43
    apples_agent-4_min: 0
    apples_agent-5_max: 130
    apples_agent-5_mean: 76.57
    apples_agent-5_min: 34
    cleaning_beam_agent-0_max: 434
    cleaning_beam_agent-0_mean: 285.52
    cleaning_beam_agent-0_min: 149
    cleaning_beam_agent-1_max: 457
    cleaning_beam_agent-1_mean: 242.1
    cleaning_beam_agent-1_min: 112
    cleaning_beam_agent-2_max: 426
    cleaning_beam_agent-2_mean: 304.11
    cleaning_beam_agent-2_min: 66
    cleaning_beam_agent-3_max: 199
    cleaning_beam_agent-3_mean: 63.41
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 498
    cleaning_beam_agent-4_mean: 352.57
    cleaning_beam_agent-4_min: 152
    cleaning_beam_agent-5_max: 189
    cleaning_beam_agent-5_mean: 76.72
    cleaning_beam_agent-5_min: 16
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-11-28
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 893.999999999975
  episode_reward_mean: 656.6599999999955
  episode_reward_min: 267.99999999999693
  episodes_this_iter: 96
  episodes_total: 14208
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19840.815
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003794112126342952
        entropy: 1.1668171882629395
        entropy_coeff: 0.0017600000137463212
        kl: 0.010446453467011452
        model: {}
        policy_loss: -0.029313519597053528
        total_loss: -0.02758219465613365
        vf_explained_var: 0.026547998189926147
        vf_loss: 16.9563045501709
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003794112126342952
        entropy: 1.126704454421997
        entropy_coeff: 0.0017600000137463212
        kl: 0.012457568198442459
        model: {}
        policy_loss: -0.03156828507781029
        total_loss: -0.029371213167905807
        vf_explained_var: 0.030828431248664856
        vf_loss: 16.885574340820312
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003794112126342952
        entropy: 1.0885653495788574
        entropy_coeff: 0.0017600000137463212
        kl: 0.010996934957802296
        model: {}
        policy_loss: -0.03093675523996353
        total_loss: -0.02909683808684349
        vf_explained_var: 0.10649794340133667
        vf_loss: 15.564023971557617
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003794112126342952
        entropy: 0.8214660882949829
        entropy_coeff: 0.0017600000137463212
        kl: 0.009791841730475426
        model: {}
        policy_loss: -0.026314709335565567
        total_loss: -0.024361364543437958
        vf_explained_var: 0.17336289584636688
        vf_loss: 14.407546997070312
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003794112126342952
        entropy: 1.0446234941482544
        entropy_coeff: 0.0017600000137463212
        kl: 0.012734871357679367
        model: {}
        policy_loss: -0.03504273295402527
        total_loss: -0.03274538740515709
        vf_explained_var: 0.08782002329826355
        vf_loss: 15.889076232910156
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003794112126342952
        entropy: 0.8559921979904175
        entropy_coeff: 0.0017600000137463212
        kl: 0.010691854171454906
        model: {}
        policy_loss: -0.03080429509282112
        total_loss: -0.028761982917785645
        vf_explained_var: 0.19045352935791016
        vf_loss: 14.104846000671387
    load_time_ms: 16803.138
    num_steps_sampled: 14208000
    num_steps_trained: 14208000
    sample_time_ms: 91684.275
    update_time_ms: 20.789
  iterations_since_restore: 88
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.081347150259067
    ram_util_percent: 14.980310880829016
  pid: 24061
  policy_reward_max:
    agent-0: 148.9999999999998
    agent-1: 148.9999999999998
    agent-2: 148.9999999999998
    agent-3: 148.9999999999998
    agent-4: 148.9999999999998
    agent-5: 148.9999999999998
  policy_reward_mean:
    agent-0: 109.44333333333361
    agent-1: 109.44333333333361
    agent-2: 109.44333333333361
    agent-3: 109.44333333333361
    agent-4: 109.44333333333361
    agent-5: 109.44333333333361
  policy_reward_min:
    agent-0: 44.66666666666661
    agent-1: 44.66666666666661
    agent-2: 44.66666666666661
    agent-3: 44.66666666666661
    agent-4: 44.66666666666661
    agent-5: 44.66666666666661
  sampler_perf:
    mean_env_wait_ms: 24.572328215211872
    mean_inference_ms: 12.397701366854134
    mean_processing_ms: 51.34854466566303
  time_since_restore: 11885.63638663292
  time_this_iter_s: 134.63832354545593
  time_total_s: 21011.648200511932
  timestamp: 1637035888
  timesteps_since_restore: 8448000
  timesteps_this_iter: 96000
  timesteps_total: 14208000
  training_iteration: 148
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    148 |          21011.6 | 14208000 |   656.66 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 82
    apples_agent-0_mean: 6.61
    apples_agent-0_min: 0
    apples_agent-1_max: 87
    apples_agent-1_mean: 24.6
    apples_agent-1_min: 0
    apples_agent-2_max: 93
    apples_agent-2_mean: 7.01
    apples_agent-2_min: 0
    apples_agent-3_max: 159
    apples_agent-3_mean: 101.23
    apples_agent-3_min: 31
    apples_agent-4_max: 61
    apples_agent-4_mean: 1.8
    apples_agent-4_min: 0
    apples_agent-5_max: 153
    apples_agent-5_mean: 75.11
    apples_agent-5_min: 5
    cleaning_beam_agent-0_max: 437
    cleaning_beam_agent-0_mean: 287.4
    cleaning_beam_agent-0_min: 125
    cleaning_beam_agent-1_max: 458
    cleaning_beam_agent-1_mean: 245.88
    cleaning_beam_agent-1_min: 95
    cleaning_beam_agent-2_max: 459
    cleaning_beam_agent-2_mean: 306.99
    cleaning_beam_agent-2_min: 113
    cleaning_beam_agent-3_max: 168
    cleaning_beam_agent-3_mean: 56.3
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 499
    cleaning_beam_agent-4_mean: 360.74
    cleaning_beam_agent-4_min: 180
    cleaning_beam_agent-5_max: 239
    cleaning_beam_agent-5_mean: 83.84
    cleaning_beam_agent-5_min: 29
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-13-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 863.9999999999725
  episode_reward_mean: 650.7199999999948
  episode_reward_min: 203.99999999999784
  episodes_this_iter: 96
  episodes_total: 14304
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19847.253
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003734208003152162
        entropy: 1.166499137878418
        entropy_coeff: 0.0017600000137463212
        kl: 0.010163824073970318
        model: {}
        policy_loss: -0.028212476521730423
        total_loss: -0.026692423969507217
        vf_explained_var: 0.05175928771495819
        vf_loss: 15.403264999389648
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003734208003152162
        entropy: 1.1193296909332275
        entropy_coeff: 0.0017600000137463212
        kl: 0.01138545572757721
        model: {}
        policy_loss: -0.03197938948869705
        total_loss: -0.030061254277825356
        vf_explained_var: 0.007956922054290771
        vf_loss: 16.110666275024414
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003734208003152162
        entropy: 1.09404718875885
        entropy_coeff: 0.0017600000137463212
        kl: 0.011224830523133278
        model: {}
        policy_loss: -0.030401693657040596
        total_loss: -0.028641164302825928
        vf_explained_var: 0.11257560551166534
        vf_loss: 14.410848617553711
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003734208003152162
        entropy: 0.8111231327056885
        entropy_coeff: 0.0017600000137463212
        kl: 0.009689593687653542
        model: {}
        policy_loss: -0.02430543303489685
        total_loss: -0.022413339465856552
        vf_explained_var: 0.1492784172296524
        vf_loss: 13.817500114440918
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003734208003152162
        entropy: 1.032041072845459
        entropy_coeff: 0.0017600000137463212
        kl: 0.011645711958408356
        model: {}
        policy_loss: -0.03318360447883606
        total_loss: -0.031102459877729416
        vf_explained_var: 0.03496810793876648
        vf_loss: 15.683897972106934
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003734208003152162
        entropy: 0.86991286277771
        entropy_coeff: 0.0017600000137463212
        kl: 0.01056409627199173
        model: {}
        policy_loss: -0.03076137788593769
        total_loss: -0.028828533366322517
        vf_explained_var: 0.1680573672056198
        vf_loss: 13.510699272155762
    load_time_ms: 16809.401
    num_steps_sampled: 14304000
    num_steps_trained: 14304000
    sample_time_ms: 91347.564
    update_time_ms: 20.874
  iterations_since_restore: 89
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.06931818181818
    ram_util_percent: 14.840909090909092
  pid: 24061
  policy_reward_max:
    agent-0: 144.0000000000001
    agent-1: 144.0000000000001
    agent-2: 144.0000000000001
    agent-3: 144.0000000000001
    agent-4: 144.0000000000001
    agent-5: 144.0000000000001
  policy_reward_mean:
    agent-0: 108.45333333333369
    agent-1: 108.45333333333369
    agent-2: 108.45333333333369
    agent-3: 108.45333333333369
    agent-4: 108.45333333333369
    agent-5: 108.45333333333369
  policy_reward_min:
    agent-0: 34.00000000000004
    agent-1: 34.00000000000004
    agent-2: 34.00000000000004
    agent-3: 34.00000000000004
    agent-4: 34.00000000000004
    agent-5: 34.00000000000004
  sampler_perf:
    mean_env_wait_ms: 24.571170384069184
    mean_inference_ms: 12.39799632855148
    mean_processing_ms: 51.34737121716866
  time_since_restore: 12009.323376893997
  time_this_iter_s: 123.68699026107788
  time_total_s: 21135.33519077301
  timestamp: 1637036012
  timesteps_since_restore: 8544000
  timesteps_this_iter: 96000
  timesteps_total: 14304000
  training_iteration: 149
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    149 |          21135.3 | 14304000 |   650.72 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 64
    apples_agent-0_mean: 5.66
    apples_agent-0_min: 0
    apples_agent-1_max: 113
    apples_agent-1_mean: 25.43
    apples_agent-1_min: 0
    apples_agent-2_max: 160
    apples_agent-2_mean: 10.41
    apples_agent-2_min: 0
    apples_agent-3_max: 150
    apples_agent-3_mean: 97.87
    apples_agent-3_min: 26
    apples_agent-4_max: 40
    apples_agent-4_mean: 0.94
    apples_agent-4_min: 0
    apples_agent-5_max: 168
    apples_agent-5_mean: 81.87
    apples_agent-5_min: 5
    cleaning_beam_agent-0_max: 476
    cleaning_beam_agent-0_mean: 294.75
    cleaning_beam_agent-0_min: 160
    cleaning_beam_agent-1_max: 398
    cleaning_beam_agent-1_mean: 238.44
    cleaning_beam_agent-1_min: 67
    cleaning_beam_agent-2_max: 474
    cleaning_beam_agent-2_mean: 305.57
    cleaning_beam_agent-2_min: 100
    cleaning_beam_agent-3_max: 175
    cleaning_beam_agent-3_mean: 60.57
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 476
    cleaning_beam_agent-4_mean: 354.79
    cleaning_beam_agent-4_min: 134
    cleaning_beam_agent-5_max: 239
    cleaning_beam_agent-5_mean: 80.57
    cleaning_beam_agent-5_min: 22
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-15-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 846.9999999999784
  episode_reward_mean: 658.6299999999957
  episode_reward_min: 196.99999999999878
  episodes_this_iter: 96
  episodes_total: 14400
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19840.938
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00036743038799613714
        entropy: 1.1698839664459229
        entropy_coeff: 0.0017600000137463212
        kl: 0.010491330176591873
        model: {}
        policy_loss: -0.028861146420240402
        total_loss: -0.02729082852602005
        vf_explained_var: 0.051049068570137024
        vf_loss: 15.310476303100586
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00036743038799613714
        entropy: 1.1367837190628052
        entropy_coeff: 0.0017600000137463212
        kl: 0.012349081225693226
        model: {}
        policy_loss: -0.03241565078496933
        total_loss: -0.030395979061722755
        vf_explained_var: 0.038695573806762695
        vf_loss: 15.505966186523438
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00036743038799613714
        entropy: 1.0871038436889648
        entropy_coeff: 0.0017600000137463212
        kl: 0.011459597386419773
        model: {}
        policy_loss: -0.030217409133911133
        total_loss: -0.02831554040312767
        vf_explained_var: 0.056210532784461975
        vf_loss: 15.232512474060059
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00036743038799613714
        entropy: 0.8024851679801941
        entropy_coeff: 0.0017600000137463212
        kl: 0.009457201696932316
        model: {}
        policy_loss: -0.02482185699045658
        total_loss: -0.023015322163701057
        vf_explained_var: 0.17679880559444427
        vf_loss: 13.274654388427734
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00036743038799613714
        entropy: 1.0440077781677246
        entropy_coeff: 0.0017600000137463212
        kl: 0.012563315220177174
        model: {}
        policy_loss: -0.034063324332237244
        total_loss: -0.031904928386211395
        vf_explained_var: 0.0800451934337616
        vf_loss: 14.83188247680664
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00036743038799613714
        entropy: 0.8698498010635376
        entropy_coeff: 0.0017600000137463212
        kl: 0.011360350996255875
        model: {}
        policy_loss: -0.03271292522549629
        total_loss: -0.030606694519519806
        vf_explained_var: 0.1536616086959839
        vf_loss: 13.650938034057617
    load_time_ms: 16796.242
    num_steps_sampled: 14400000
    num_steps_trained: 14400000
    sample_time_ms: 91233.907
    update_time_ms: 19.848
  iterations_since_restore: 90
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.98418079096045
    ram_util_percent: 14.86045197740113
  pid: 24061
  policy_reward_max:
    agent-0: 141.1666666666669
    agent-1: 141.1666666666669
    agent-2: 141.1666666666669
    agent-3: 141.1666666666669
    agent-4: 141.1666666666669
    agent-5: 141.1666666666669
  policy_reward_mean:
    agent-0: 109.77166666666703
    agent-1: 109.77166666666703
    agent-2: 109.77166666666703
    agent-3: 109.77166666666703
    agent-4: 109.77166666666703
    agent-5: 109.77166666666703
  policy_reward_min:
    agent-0: 32.833333333333385
    agent-1: 32.833333333333385
    agent-2: 32.833333333333385
    agent-3: 32.833333333333385
    agent-4: 32.833333333333385
    agent-5: 32.833333333333385
  sampler_perf:
    mean_env_wait_ms: 24.56889806093052
    mean_inference_ms: 12.396791273047855
    mean_processing_ms: 51.34693882822601
  time_since_restore: 12133.25688457489
  time_this_iter_s: 123.93350768089294
  time_total_s: 21259.268698453903
  timestamp: 1637036136
  timesteps_since_restore: 8640000
  timesteps_this_iter: 96000
  timesteps_total: 14400000
  training_iteration: 150
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    150 |          21259.3 | 14400000 |   658.63 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 38
    apples_agent-0_mean: 6.78
    apples_agent-0_min: 0
    apples_agent-1_max: 94
    apples_agent-1_mean: 23.98
    apples_agent-1_min: 0
    apples_agent-2_max: 107
    apples_agent-2_mean: 7.99
    apples_agent-2_min: 0
    apples_agent-3_max: 174
    apples_agent-3_mean: 100.33
    apples_agent-3_min: 40
    apples_agent-4_max: 24
    apples_agent-4_mean: 1.15
    apples_agent-4_min: 0
    apples_agent-5_max: 152
    apples_agent-5_mean: 83.57
    apples_agent-5_min: 23
    cleaning_beam_agent-0_max: 466
    cleaning_beam_agent-0_mean: 294.61
    cleaning_beam_agent-0_min: 123
    cleaning_beam_agent-1_max: 462
    cleaning_beam_agent-1_mean: 251.85
    cleaning_beam_agent-1_min: 82
    cleaning_beam_agent-2_max: 479
    cleaning_beam_agent-2_mean: 299.75
    cleaning_beam_agent-2_min: 145
    cleaning_beam_agent-3_max: 147
    cleaning_beam_agent-3_mean: 57.34
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 498
    cleaning_beam_agent-4_mean: 360.15
    cleaning_beam_agent-4_min: 190
    cleaning_beam_agent-5_max: 215
    cleaning_beam_agent-5_mean: 77.28
    cleaning_beam_agent-5_min: 26
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-17-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 916.9999999999803
  episode_reward_mean: 657.4499999999956
  episode_reward_min: 410.0000000000074
  episodes_this_iter: 96
  episodes_total: 14496
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19801.132
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00036144000478088856
        entropy: 1.1780309677124023
        entropy_coeff: 0.0017600000137463212
        kl: 0.010187812149524689
        model: {}
        policy_loss: -0.02901572734117508
        total_loss: -0.027519449591636658
        vf_explained_var: 0.0647234320640564
        vf_loss: 15.320484161376953
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00036144000478088856
        entropy: 1.122765302658081
        entropy_coeff: 0.0017600000137463212
        kl: 0.012075573205947876
        model: {}
        policy_loss: -0.03193304315209389
        total_loss: -0.02991008199751377
        vf_explained_var: 0.03303635120391846
        vf_loss: 15.839132308959961
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00036144000478088856
        entropy: 1.10184645652771
        entropy_coeff: 0.0017600000137463212
        kl: 0.012449067085981369
        model: {}
        policy_loss: -0.02886662818491459
        total_loss: -0.026770971715450287
        vf_explained_var: 0.057550132274627686
        vf_loss: 15.450949668884277
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00036144000478088856
        entropy: 0.814753532409668
        entropy_coeff: 0.0017600000137463212
        kl: 0.009008249267935753
        model: {}
        policy_loss: -0.024889327585697174
        total_loss: -0.023111723363399506
        vf_explained_var: 0.1389952003955841
        vf_loss: 14.09919548034668
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00036144000478088856
        entropy: 1.0330196619033813
        entropy_coeff: 0.0017600000137463212
        kl: 0.012226602993905544
        model: {}
        policy_loss: -0.03424076735973358
        total_loss: -0.03205493837594986
        vf_explained_var: 0.04891987144947052
        vf_loss: 15.586235046386719
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00036144000478088856
        entropy: 0.8755501508712769
        entropy_coeff: 0.0017600000137463212
        kl: 0.010647783987224102
        model: {}
        policy_loss: -0.030748698860406876
        total_loss: -0.028783254325389862
        vf_explained_var: 0.15912392735481262
        vf_loss: 13.768562316894531
    load_time_ms: 16734.781
    num_steps_sampled: 14496000
    num_steps_trained: 14496000
    sample_time_ms: 91337.079
    update_time_ms: 19.824
  iterations_since_restore: 91
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.870224719101127
    ram_util_percent: 14.832584269662924
  pid: 24061
  policy_reward_max:
    agent-0: 152.83333333333323
    agent-1: 152.83333333333323
    agent-2: 152.83333333333323
    agent-3: 152.83333333333323
    agent-4: 152.83333333333323
    agent-5: 152.83333333333323
  policy_reward_mean:
    agent-0: 109.57500000000034
    agent-1: 109.57500000000034
    agent-2: 109.57500000000034
    agent-3: 109.57500000000034
    agent-4: 109.57500000000034
    agent-5: 109.57500000000034
  policy_reward_min:
    agent-0: 68.33333333333316
    agent-1: 68.33333333333316
    agent-2: 68.33333333333316
    agent-3: 68.33333333333316
    agent-4: 68.33333333333316
    agent-5: 68.33333333333316
  sampler_perf:
    mean_env_wait_ms: 24.56629876922959
    mean_inference_ms: 12.396083312086159
    mean_processing_ms: 51.34954149940286
  time_since_restore: 12258.343956708908
  time_this_iter_s: 125.08707213401794
  time_total_s: 21384.35577058792
  timestamp: 1637036262
  timesteps_since_restore: 8736000
  timesteps_this_iter: 96000
  timesteps_total: 14496000
  training_iteration: 151
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    151 |          21384.4 | 14496000 |   657.45 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 45
    apples_agent-0_mean: 4.54
    apples_agent-0_min: 0
    apples_agent-1_max: 77
    apples_agent-1_mean: 21.84
    apples_agent-1_min: 0
    apples_agent-2_max: 226
    apples_agent-2_mean: 11.99
    apples_agent-2_min: 0
    apples_agent-3_max: 174
    apples_agent-3_mean: 97.95
    apples_agent-3_min: 27
    apples_agent-4_max: 40
    apples_agent-4_mean: 1.29
    apples_agent-4_min: 0
    apples_agent-5_max: 139
    apples_agent-5_mean: 76.13
    apples_agent-5_min: 5
    cleaning_beam_agent-0_max: 441
    cleaning_beam_agent-0_mean: 297.66
    cleaning_beam_agent-0_min: 139
    cleaning_beam_agent-1_max: 392
    cleaning_beam_agent-1_mean: 245.17
    cleaning_beam_agent-1_min: 88
    cleaning_beam_agent-2_max: 463
    cleaning_beam_agent-2_mean: 293.64
    cleaning_beam_agent-2_min: 64
    cleaning_beam_agent-3_max: 146
    cleaning_beam_agent-3_mean: 56.02
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 491
    cleaning_beam_agent-4_mean: 346.99
    cleaning_beam_agent-4_min: 108
    cleaning_beam_agent-5_max: 210
    cleaning_beam_agent-5_mean: 81.37
    cleaning_beam_agent-5_min: 19
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.05
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-19-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 906.9999999999696
  episode_reward_mean: 640.3499999999958
  episode_reward_min: 221.99999999999812
  episodes_this_iter: 96
  episodes_total: 14592
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19805.876
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003554495924618095
        entropy: 1.1735090017318726
        entropy_coeff: 0.0017600000137463212
        kl: 0.010616499930620193
        model: {}
        policy_loss: -0.02804735116660595
        total_loss: -0.0262872576713562
        vf_explained_var: 0.08035682141780853
        vf_loss: 17.021669387817383
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003554495924618095
        entropy: 1.1251015663146973
        entropy_coeff: 0.0017600000137463212
        kl: 0.012187634594738483
        model: {}
        policy_loss: -0.03166010603308678
        total_loss: -0.029380805790424347
        vf_explained_var: 0.0144491046667099
        vf_loss: 18.21951675415039
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003554495924618095
        entropy: 1.0962973833084106
        entropy_coeff: 0.0017600000137463212
        kl: 0.011848507449030876
        model: {}
        policy_loss: -0.029909562319517136
        total_loss: -0.02783150225877762
        vf_explained_var: 0.1146935373544693
        vf_loss: 16.37842559814453
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003554495924618095
        entropy: 0.8378790020942688
        entropy_coeff: 0.0017600000137463212
        kl: 0.009067554026842117
        model: {}
        policy_loss: -0.02569524198770523
        total_loss: -0.023850537836551666
        vf_explained_var: 0.1853974610567093
        vf_loss: 15.058597564697266
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003554495924618095
        entropy: 1.0370298624038696
        entropy_coeff: 0.0017600000137463212
        kl: 0.011899909004569054
        model: {}
        policy_loss: -0.034282948821783066
        total_loss: -0.032053470611572266
        vf_explained_var: 0.0939662754535675
        vf_loss: 16.746692657470703
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003554495924618095
        entropy: 0.8684813380241394
        entropy_coeff: 0.0017600000137463212
        kl: 0.011614983901381493
        model: {}
        policy_loss: -0.030977735295891762
        total_loss: -0.028742806985974312
        vf_explained_var: 0.2210465967655182
        vf_loss: 14.40457534790039
    load_time_ms: 17372.656
    num_steps_sampled: 14592000
    num_steps_trained: 14592000
    sample_time_ms: 91228.419
    update_time_ms: 20.27
  iterations_since_restore: 92
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.34705882352941
    ram_util_percent: 14.933155080213906
  pid: 24061
  policy_reward_max:
    agent-0: 151.1666666666665
    agent-1: 151.1666666666665
    agent-2: 151.1666666666665
    agent-3: 151.1666666666665
    agent-4: 151.1666666666665
    agent-5: 151.1666666666665
  policy_reward_mean:
    agent-0: 106.7250000000003
    agent-1: 106.7250000000003
    agent-2: 106.7250000000003
    agent-3: 106.7250000000003
    agent-4: 106.7250000000003
    agent-5: 106.7250000000003
  policy_reward_min:
    agent-0: 36.999999999999986
    agent-1: 36.999999999999986
    agent-2: 36.999999999999986
    agent-3: 36.999999999999986
    agent-4: 36.999999999999986
    agent-5: 36.999999999999986
  sampler_perf:
    mean_env_wait_ms: 24.56270545719907
    mean_inference_ms: 12.39579341514789
    mean_processing_ms: 51.34752023242006
  time_since_restore: 12389.236950874329
  time_this_iter_s: 130.89299416542053
  time_total_s: 21515.24876475334
  timestamp: 1637036393
  timesteps_since_restore: 8832000
  timesteps_this_iter: 96000
  timesteps_total: 14592000
  training_iteration: 152
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    152 |          21515.2 | 14592000 |   640.35 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 113
    apples_agent-0_mean: 8.06
    apples_agent-0_min: 0
    apples_agent-1_max: 74
    apples_agent-1_mean: 24.48
    apples_agent-1_min: 0
    apples_agent-2_max: 140
    apples_agent-2_mean: 5.81
    apples_agent-2_min: 0
    apples_agent-3_max: 164
    apples_agent-3_mean: 95.53
    apples_agent-3_min: 31
    apples_agent-4_max: 39
    apples_agent-4_mean: 1.14
    apples_agent-4_min: 0
    apples_agent-5_max: 176
    apples_agent-5_mean: 82.59
    apples_agent-5_min: 9
    cleaning_beam_agent-0_max: 507
    cleaning_beam_agent-0_mean: 280.96
    cleaning_beam_agent-0_min: 140
    cleaning_beam_agent-1_max: 409
    cleaning_beam_agent-1_mean: 244.3
    cleaning_beam_agent-1_min: 118
    cleaning_beam_agent-2_max: 431
    cleaning_beam_agent-2_mean: 306.7
    cleaning_beam_agent-2_min: 43
    cleaning_beam_agent-3_max: 157
    cleaning_beam_agent-3_mean: 58.63
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 503
    cleaning_beam_agent-4_mean: 357.62
    cleaning_beam_agent-4_min: 199
    cleaning_beam_agent-5_max: 362
    cleaning_beam_agent-5_mean: 89.78
    cleaning_beam_agent-5_min: 16
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-21-56
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 927.9999999999753
  episode_reward_mean: 648.5599999999962
  episode_reward_min: 231.99999999999662
  episodes_this_iter: 96
  episodes_total: 14688
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19773.997
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00034945920924656093
        entropy: 1.1698192358016968
        entropy_coeff: 0.0017600000137463212
        kl: 0.010749096982181072
        model: {}
        policy_loss: -0.029447555541992188
        total_loss: -0.02785298600792885
        vf_explained_var: 0.0470770001411438
        vf_loss: 15.03628158569336
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00034945920924656093
        entropy: 1.1241886615753174
        entropy_coeff: 0.0017600000137463212
        kl: 0.011395437642931938
        model: {}
        policy_loss: -0.03054969385266304
        total_loss: -0.02873196452856064
        vf_explained_var: 0.0378970205783844
        vf_loss: 15.172160148620605
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00034945920924656093
        entropy: 1.1005821228027344
        entropy_coeff: 0.0017600000137463212
        kl: 0.010783982463181019
        model: {}
        policy_loss: -0.029102956876158714
        total_loss: -0.027356328442692757
        vf_explained_var: 0.03151416778564453
        vf_loss: 15.268577575683594
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00034945920924656093
        entropy: 0.8240507245063782
        entropy_coeff: 0.0017600000137463212
        kl: 0.009088778868317604
        model: {}
        policy_loss: -0.02490360289812088
        total_loss: -0.02317960001528263
        vf_explained_var: 0.1401665061712265
        vf_loss: 13.565764427185059
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00034945920924656093
        entropy: 1.022689938545227
        entropy_coeff: 0.0017600000137463212
        kl: 0.012653466314077377
        model: {}
        policy_loss: -0.03351055085659027
        total_loss: -0.03126215562224388
        vf_explained_var: 0.038056209683418274
        vf_loss: 15.176403045654297
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00034945920924656093
        entropy: 0.8742369413375854
        entropy_coeff: 0.0017600000137463212
        kl: 0.010841669514775276
        model: {}
        policy_loss: -0.03257240355014801
        total_loss: -0.030613481998443604
        vf_explained_var: 0.1575525552034378
        vf_loss: 13.292475700378418
    load_time_ms: 17126.032
    num_steps_sampled: 14688000
    num_steps_trained: 14688000
    sample_time_ms: 91207.849
    update_time_ms: 20.774
  iterations_since_restore: 93
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.156000000000002
    ram_util_percent: 12.997142857142856
  pid: 24061
  policy_reward_max:
    agent-0: 154.66666666666677
    agent-1: 154.66666666666677
    agent-2: 154.66666666666677
    agent-3: 154.66666666666677
    agent-4: 154.66666666666677
    agent-5: 154.66666666666677
  policy_reward_mean:
    agent-0: 108.09333333333365
    agent-1: 108.09333333333365
    agent-2: 108.09333333333365
    agent-3: 108.09333333333365
    agent-4: 108.09333333333365
    agent-5: 108.09333333333365
  policy_reward_min:
    agent-0: 38.666666666666686
    agent-1: 38.666666666666686
    agent-2: 38.666666666666686
    agent-3: 38.666666666666686
    agent-4: 38.666666666666686
    agent-5: 38.666666666666686
  sampler_perf:
    mean_env_wait_ms: 24.558257233441893
    mean_inference_ms: 12.395466680766585
    mean_processing_ms: 51.34361346750798
  time_since_restore: 12512.427284479141
  time_this_iter_s: 123.19033360481262
  time_total_s: 21638.439098358154
  timestamp: 1637036516
  timesteps_since_restore: 8928000
  timesteps_this_iter: 96000
  timesteps_total: 14688000
  training_iteration: 153
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    153 |          21638.4 | 14688000 |   648.56 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 58
    apples_agent-0_mean: 5.26
    apples_agent-0_min: 0
    apples_agent-1_max: 86
    apples_agent-1_mean: 27.76
    apples_agent-1_min: 0
    apples_agent-2_max: 266
    apples_agent-2_mean: 11.19
    apples_agent-2_min: 0
    apples_agent-3_max: 186
    apples_agent-3_mean: 96.86
    apples_agent-3_min: 7
    apples_agent-4_max: 64
    apples_agent-4_mean: 1.76
    apples_agent-4_min: 0
    apples_agent-5_max: 151
    apples_agent-5_mean: 80.7
    apples_agent-5_min: 31
    cleaning_beam_agent-0_max: 452
    cleaning_beam_agent-0_mean: 300.48
    cleaning_beam_agent-0_min: 127
    cleaning_beam_agent-1_max: 372
    cleaning_beam_agent-1_mean: 221.03
    cleaning_beam_agent-1_min: 98
    cleaning_beam_agent-2_max: 437
    cleaning_beam_agent-2_mean: 308.14
    cleaning_beam_agent-2_min: 43
    cleaning_beam_agent-3_max: 157
    cleaning_beam_agent-3_mean: 58.18
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 450
    cleaning_beam_agent-4_mean: 343.32
    cleaning_beam_agent-4_min: 159
    cleaning_beam_agent-5_max: 194
    cleaning_beam_agent-5_mean: 79.91
    cleaning_beam_agent-5_min: 19
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-23-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 910.9999999999789
  episode_reward_mean: 649.3999999999953
  episode_reward_min: 273.99999999999727
  episodes_this_iter: 96
  episodes_total: 14784
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19688.051
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003434687969274819
        entropy: 1.1715315580368042
        entropy_coeff: 0.0017600000137463212
        kl: 0.009921570308506489
        model: {}
        policy_loss: -0.02906256914138794
        total_loss: -0.027511868625879288
        vf_explained_var: 0.06751729547977448
        vf_loss: 16.282812118530273
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003434687969274819
        entropy: 1.1203172206878662
        entropy_coeff: 0.0017600000137463212
        kl: 0.011264996603131294
        model: {}
        policy_loss: -0.031585320830345154
        total_loss: -0.029594741761684418
        vf_explained_var: 0.01984301209449768
        vf_loss: 17.093385696411133
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003434687969274819
        entropy: 1.0933070182800293
        entropy_coeff: 0.0017600000137463212
        kl: 0.011434320360422134
        model: {}
        policy_loss: -0.030211037024855614
        total_loss: -0.028260070830583572
        vf_explained_var: 0.0896565318107605
        vf_loss: 15.883199691772461
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003434687969274819
        entropy: 0.8243908882141113
        entropy_coeff: 0.0017600000137463212
        kl: 0.008685835637152195
        model: {}
        policy_loss: -0.024858370423316956
        total_loss: -0.023200154304504395
        vf_explained_var: 0.21349725127220154
        vf_loss: 13.719761848449707
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003434687969274819
        entropy: 1.0268268585205078
        entropy_coeff: 0.0017600000137463212
        kl: 0.011639619246125221
        model: {}
        policy_loss: -0.032508015632629395
        total_loss: -0.03036714717745781
        vf_explained_var: 0.07099883258342743
        vf_loss: 16.201635360717773
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003434687969274819
        entropy: 0.8715567588806152
        entropy_coeff: 0.0017600000137463212
        kl: 0.011007187888026237
        model: {}
        policy_loss: -0.03220604732632637
        total_loss: -0.030098451301455498
        vf_explained_var: 0.1747344583272934
        vf_loss: 14.400941848754883
    load_time_ms: 17082.184
    num_steps_sampled: 14784000
    num_steps_trained: 14784000
    sample_time_ms: 91012.421
    update_time_ms: 21.225
  iterations_since_restore: 94
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.356896551724136
    ram_util_percent: 10.149999999999997
  pid: 24061
  policy_reward_max:
    agent-0: 151.83333333333368
    agent-1: 151.83333333333368
    agent-2: 151.83333333333368
    agent-3: 151.83333333333368
    agent-4: 151.83333333333368
    agent-5: 151.83333333333368
  policy_reward_mean:
    agent-0: 108.23333333333369
    agent-1: 108.23333333333369
    agent-2: 108.23333333333369
    agent-3: 108.23333333333369
    agent-4: 108.23333333333369
    agent-5: 108.23333333333369
  policy_reward_min:
    agent-0: 45.66666666666664
    agent-1: 45.66666666666664
    agent-2: 45.66666666666664
    agent-3: 45.66666666666664
    agent-4: 45.66666666666664
    agent-5: 45.66666666666664
  sampler_perf:
    mean_env_wait_ms: 24.550565081538743
    mean_inference_ms: 12.392574498581709
    mean_processing_ms: 51.33101957884078
  time_since_restore: 12634.454087495804
  time_this_iter_s: 122.0268030166626
  time_total_s: 21760.465901374817
  timestamp: 1637036638
  timesteps_since_restore: 9024000
  timesteps_this_iter: 96000
  timesteps_total: 14784000
  training_iteration: 154
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    154 |          21760.5 | 14784000 |    649.4 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 99
    apples_agent-0_mean: 7.46
    apples_agent-0_min: 0
    apples_agent-1_max: 98
    apples_agent-1_mean: 24.96
    apples_agent-1_min: 0
    apples_agent-2_max: 115
    apples_agent-2_mean: 4.98
    apples_agent-2_min: 0
    apples_agent-3_max: 161
    apples_agent-3_mean: 104.83
    apples_agent-3_min: 43
    apples_agent-4_max: 65
    apples_agent-4_mean: 1.59
    apples_agent-4_min: 0
    apples_agent-5_max: 153
    apples_agent-5_mean: 82.84
    apples_agent-5_min: 13
    cleaning_beam_agent-0_max: 567
    cleaning_beam_agent-0_mean: 307.99
    cleaning_beam_agent-0_min: 111
    cleaning_beam_agent-1_max: 467
    cleaning_beam_agent-1_mean: 234.07
    cleaning_beam_agent-1_min: 113
    cleaning_beam_agent-2_max: 492
    cleaning_beam_agent-2_mean: 327.83
    cleaning_beam_agent-2_min: 167
    cleaning_beam_agent-3_max: 151
    cleaning_beam_agent-3_mean: 52.03
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 487
    cleaning_beam_agent-4_mean: 350.87
    cleaning_beam_agent-4_min: 168
    cleaning_beam_agent-5_max: 283
    cleaning_beam_agent-5_mean: 86.0
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-26-01
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 918.999999999985
  episode_reward_mean: 690.7399999999933
  episode_reward_min: 386.00000000000716
  episodes_this_iter: 96
  episodes_total: 14880
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19646.14
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003374784137122333
        entropy: 1.1606593132019043
        entropy_coeff: 0.0017600000137463212
        kl: 0.00989767350256443
        model: {}
        policy_loss: -0.028051378205418587
        total_loss: -0.02664276584982872
        vf_explained_var: 0.087861567735672
        vf_loss: 14.718381881713867
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003374784137122333
        entropy: 1.1172205209732056
        entropy_coeff: 0.0017600000137463212
        kl: 0.011195818893611431
        model: {}
        policy_loss: -0.03038283996284008
        total_loss: -0.02851278707385063
        vf_explained_var: 0.010024294257164001
        vf_loss: 15.971955299377441
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003374784137122333
        entropy: 1.0824317932128906
        entropy_coeff: 0.0017600000137463212
        kl: 0.011023142375051975
        model: {}
        policy_loss: -0.028779350221157074
        total_loss: -0.026999130845069885
        vf_explained_var: 0.08102439343929291
        vf_loss: 14.806717872619629
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003374784137122333
        entropy: 0.7990597486495972
        entropy_coeff: 0.0017600000137463212
        kl: 0.008303762413561344
        model: {}
        policy_loss: -0.023393532261252403
        total_loss: -0.021775994449853897
        vf_explained_var: 0.15508590638637543
        vf_loss: 13.631298065185547
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003374784137122333
        entropy: 1.0367765426635742
        entropy_coeff: 0.0017600000137463212
        kl: 0.011465124785900116
        model: {}
        policy_loss: -0.03266654163599014
        total_loss: -0.03067060559988022
        vf_explained_var: 0.05327504873275757
        vf_loss: 15.276376724243164
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003374784137122333
        entropy: 0.8683979511260986
        entropy_coeff: 0.0017600000137463212
        kl: 0.01073588989675045
        model: {}
        policy_loss: -0.030185189098119736
        total_loss: -0.02817896008491516
        vf_explained_var: 0.14078201353549957
        vf_loss: 13.874298095703125
    load_time_ms: 14890.21
    num_steps_sampled: 14880000
    num_steps_trained: 14880000
    sample_time_ms: 90947.514
    update_time_ms: 21.068
  iterations_since_restore: 95
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.382183908045977
    ram_util_percent: 10.135632183908047
  pid: 24061
  policy_reward_max:
    agent-0: 153.16666666666657
    agent-1: 153.16666666666657
    agent-2: 153.16666666666657
    agent-3: 153.16666666666657
    agent-4: 153.16666666666657
    agent-5: 153.16666666666657
  policy_reward_mean:
    agent-0: 115.12333333333366
    agent-1: 115.12333333333366
    agent-2: 115.12333333333366
    agent-3: 115.12333333333366
    agent-4: 115.12333333333366
    agent-5: 115.12333333333366
  policy_reward_min:
    agent-0: 64.33333333333316
    agent-1: 64.33333333333316
    agent-2: 64.33333333333316
    agent-3: 64.33333333333316
    agent-4: 64.33333333333316
    agent-5: 64.33333333333316
  sampler_perf:
    mean_env_wait_ms: 24.543571458925086
    mean_inference_ms: 12.389307250734102
    mean_processing_ms: 51.31997226616483
  time_since_restore: 12756.60869026184
  time_this_iter_s: 122.15460276603699
  time_total_s: 21882.620504140854
  timestamp: 1637036761
  timesteps_since_restore: 9120000
  timesteps_this_iter: 96000
  timesteps_total: 14880000
  training_iteration: 155
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    155 |          21882.6 | 14880000 |   690.74 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 60
    apples_agent-0_mean: 5.92
    apples_agent-0_min: 0
    apples_agent-1_max: 90
    apples_agent-1_mean: 27.71
    apples_agent-1_min: 0
    apples_agent-2_max: 257
    apples_agent-2_mean: 13.38
    apples_agent-2_min: 0
    apples_agent-3_max: 166
    apples_agent-3_mean: 97.55
    apples_agent-3_min: 28
    apples_agent-4_max: 65
    apples_agent-4_mean: 1.33
    apples_agent-4_min: 0
    apples_agent-5_max: 155
    apples_agent-5_mean: 82.95
    apples_agent-5_min: 16
    cleaning_beam_agent-0_max: 440
    cleaning_beam_agent-0_mean: 315.41
    cleaning_beam_agent-0_min: 152
    cleaning_beam_agent-1_max: 483
    cleaning_beam_agent-1_mean: 232.87
    cleaning_beam_agent-1_min: 89
    cleaning_beam_agent-2_max: 460
    cleaning_beam_agent-2_mean: 308.91
    cleaning_beam_agent-2_min: 102
    cleaning_beam_agent-3_max: 113
    cleaning_beam_agent-3_mean: 52.23
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 443
    cleaning_beam_agent-4_mean: 333.45
    cleaning_beam_agent-4_min: 168
    cleaning_beam_agent-5_max: 239
    cleaning_beam_agent-5_mean: 86.03
    cleaning_beam_agent-5_min: 26
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 4
    fire_beam_agent-3_mean: 0.05
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-28-02
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 862.9999999999835
  episode_reward_mean: 644.8599999999958
  episode_reward_min: 240.99999999999596
  episodes_this_iter: 96
  episodes_total: 14976
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19636.545
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00033148800139315426
        entropy: 1.1646339893341064
        entropy_coeff: 0.0017600000137463212
        kl: 0.00946272723376751
        model: {}
        policy_loss: -0.027471356093883514
        total_loss: -0.02604556269943714
        vf_explained_var: 0.02069053053855896
        vf_loss: 15.830004692077637
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00033148800139315426
        entropy: 1.1053550243377686
        entropy_coeff: 0.0017600000137463212
        kl: 0.010959524661302567
        model: {}
        policy_loss: -0.03061651811003685
        total_loss: -0.028778623789548874
        vf_explained_var: 0.012929245829582214
        vf_loss: 15.914108276367188
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00033148800139315426
        entropy: 1.0750470161437988
        entropy_coeff: 0.0017600000137463212
        kl: 0.011351409368216991
        model: {}
        policy_loss: -0.027855180203914642
        total_loss: -0.02594829350709915
        vf_explained_var: 0.05200055241584778
        vf_loss: 15.28687572479248
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00033148800139315426
        entropy: 0.814475417137146
        entropy_coeff: 0.0017600000137463212
        kl: 0.008844212628901005
        model: {}
        policy_loss: -0.02434537187218666
        total_loss: -0.02267662063241005
        vf_explained_var: 0.17224614322185516
        vf_loss: 13.333842277526855
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00033148800139315426
        entropy: 1.0304274559020996
        entropy_coeff: 0.0017600000137463212
        kl: 0.011111130006611347
        model: {}
        policy_loss: -0.032750360667705536
        total_loss: -0.03081779181957245
        vf_explained_var: 0.05336838960647583
        vf_loss: 15.238974571228027
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00033148800139315426
        entropy: 0.8928247690200806
        entropy_coeff: 0.0017600000137463212
        kl: 0.011133932508528233
        model: {}
        policy_loss: -0.031692828983068466
        total_loss: -0.02968388982117176
        vf_explained_var: 0.1598990112543106
        vf_loss: 13.5352783203125
    load_time_ms: 14879.241
    num_steps_sampled: 14976000
    num_steps_trained: 14976000
    sample_time_ms: 90625.689
    update_time_ms: 20.625
  iterations_since_restore: 96
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.50115606936416
    ram_util_percent: 10.150867052023118
  pid: 24061
  policy_reward_max:
    agent-0: 143.83333333333374
    agent-1: 143.83333333333374
    agent-2: 143.83333333333374
    agent-3: 143.83333333333374
    agent-4: 143.83333333333374
    agent-5: 143.83333333333374
  policy_reward_mean:
    agent-0: 107.47666666666697
    agent-1: 107.47666666666697
    agent-2: 107.47666666666697
    agent-3: 107.47666666666697
    agent-4: 107.47666666666697
    agent-5: 107.47666666666697
  policy_reward_min:
    agent-0: 40.16666666666662
    agent-1: 40.16666666666662
    agent-2: 40.16666666666662
    agent-3: 40.16666666666662
    agent-4: 40.16666666666662
    agent-5: 40.16666666666662
  sampler_perf:
    mean_env_wait_ms: 24.536382535324528
    mean_inference_ms: 12.38624995224226
    mean_processing_ms: 51.308531096302744
  time_since_restore: 12877.969210386276
  time_this_iter_s: 121.36052012443542
  time_total_s: 22003.98102426529
  timestamp: 1637036882
  timesteps_since_restore: 9216000
  timesteps_this_iter: 96000
  timesteps_total: 14976000
  training_iteration: 156
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    156 |            22004 | 14976000 |   644.86 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 83
    apples_agent-0_mean: 4.62
    apples_agent-0_min: 0
    apples_agent-1_max: 104
    apples_agent-1_mean: 29.45
    apples_agent-1_min: 0
    apples_agent-2_max: 226
    apples_agent-2_mean: 10.04
    apples_agent-2_min: 0
    apples_agent-3_max: 166
    apples_agent-3_mean: 93.8
    apples_agent-3_min: 29
    apples_agent-4_max: 94
    apples_agent-4_mean: 2.65
    apples_agent-4_min: 0
    apples_agent-5_max: 179
    apples_agent-5_mean: 85.92
    apples_agent-5_min: 37
    cleaning_beam_agent-0_max: 468
    cleaning_beam_agent-0_mean: 326.03
    cleaning_beam_agent-0_min: 155
    cleaning_beam_agent-1_max: 467
    cleaning_beam_agent-1_mean: 228.95
    cleaning_beam_agent-1_min: 76
    cleaning_beam_agent-2_max: 446
    cleaning_beam_agent-2_mean: 307.24
    cleaning_beam_agent-2_min: 45
    cleaning_beam_agent-3_max: 205
    cleaning_beam_agent-3_mean: 57.48
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 468
    cleaning_beam_agent-4_mean: 327.9
    cleaning_beam_agent-4_min: 177
    cleaning_beam_agent-5_max: 195
    cleaning_beam_agent-5_mean: 81.09
    cleaning_beam_agent-5_min: 15
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-30-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 909.9999999999889
  episode_reward_mean: 645.7999999999955
  episode_reward_min: 287.99999999999704
  episodes_this_iter: 96
  episodes_total: 15072
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19642.351
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003254975890740752
        entropy: 1.1618072986602783
        entropy_coeff: 0.0017600000137463212
        kl: 0.010161519050598145
        model: {}
        policy_loss: -0.02847069688141346
        total_loss: -0.02685554139316082
        vf_explained_var: 0.06388057768344879
        vf_loss: 16.276329040527344
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003254975890740752
        entropy: 1.1087782382965088
        entropy_coeff: 0.0017600000137463212
        kl: 0.010915035381913185
        model: {}
        policy_loss: -0.031259190291166306
        total_loss: -0.0293133482336998
        vf_explained_var: 0.01425391435623169
        vf_loss: 17.142850875854492
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003254975890740752
        entropy: 1.0707218647003174
        entropy_coeff: 0.0017600000137463212
        kl: 0.010455062612891197
        model: {}
        policy_loss: -0.02891823835670948
        total_loss: -0.0271453820168972
        vf_explained_var: 0.09934490919113159
        vf_loss: 15.663158416748047
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003254975890740752
        entropy: 0.8207702040672302
        entropy_coeff: 0.0017600000137463212
        kl: 0.009250096045434475
        model: {}
        policy_loss: -0.0244038924574852
        total_loss: -0.022640671581029892
        vf_explained_var: 0.21878136694431305
        vf_loss: 13.577548027038574
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003254975890740752
        entropy: 1.0456109046936035
        entropy_coeff: 0.0017600000137463212
        kl: 0.011728672310709953
        model: {}
        policy_loss: -0.03323482722043991
        total_loss: -0.031146792694926262
        vf_explained_var: 0.09003880620002747
        vf_loss: 15.82569694519043
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003254975890740752
        entropy: 0.8883308172225952
        entropy_coeff: 0.0017600000137463212
        kl: 0.011038942262530327
        model: {}
        policy_loss: -0.03015395998954773
        total_loss: -0.028111331164836884
        vf_explained_var: 0.19582784175872803
        vf_loss: 13.983036994934082
    load_time_ms: 14741.867
    num_steps_sampled: 15072000
    num_steps_trained: 15072000
    sample_time_ms: 90345.057
    update_time_ms: 20.774
  iterations_since_restore: 97
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.490751445086703
    ram_util_percent: 10.153757225433523
  pid: 24061
  policy_reward_max:
    agent-0: 151.66666666666634
    agent-1: 151.66666666666634
    agent-2: 151.66666666666634
    agent-3: 151.66666666666634
    agent-4: 151.66666666666634
    agent-5: 151.66666666666634
  policy_reward_mean:
    agent-0: 107.6333333333336
    agent-1: 107.6333333333336
    agent-2: 107.6333333333336
    agent-3: 107.6333333333336
    agent-4: 107.6333333333336
    agent-5: 107.6333333333336
  policy_reward_min:
    agent-0: 47.999999999999915
    agent-1: 47.999999999999915
    agent-2: 47.999999999999915
    agent-3: 47.999999999999915
    agent-4: 47.999999999999915
    agent-5: 47.999999999999915
  sampler_perf:
    mean_env_wait_ms: 24.528868797276978
    mean_inference_ms: 12.383689053518138
    mean_processing_ms: 51.29670733577116
  time_since_restore: 12999.50655221939
  time_this_iter_s: 121.53734183311462
  time_total_s: 22125.518366098404
  timestamp: 1637037004
  timesteps_since_restore: 9312000
  timesteps_this_iter: 96000
  timesteps_total: 15072000
  training_iteration: 157
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    157 |          22125.5 | 15072000 |    645.8 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 135
    apples_agent-0_mean: 5.84
    apples_agent-0_min: 0
    apples_agent-1_max: 83
    apples_agent-1_mean: 26.16
    apples_agent-1_min: 0
    apples_agent-2_max: 292
    apples_agent-2_mean: 12.92
    apples_agent-2_min: 0
    apples_agent-3_max: 186
    apples_agent-3_mean: 93.07
    apples_agent-3_min: 21
    apples_agent-4_max: 54
    apples_agent-4_mean: 1.47
    apples_agent-4_min: 0
    apples_agent-5_max: 151
    apples_agent-5_mean: 84.33
    apples_agent-5_min: 23
    cleaning_beam_agent-0_max: 518
    cleaning_beam_agent-0_mean: 323.61
    cleaning_beam_agent-0_min: 143
    cleaning_beam_agent-1_max: 469
    cleaning_beam_agent-1_mean: 237.53
    cleaning_beam_agent-1_min: 132
    cleaning_beam_agent-2_max: 426
    cleaning_beam_agent-2_mean: 296.64
    cleaning_beam_agent-2_min: 45
    cleaning_beam_agent-3_max: 186
    cleaning_beam_agent-3_mean: 58.58
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 465
    cleaning_beam_agent-4_mean: 337.64
    cleaning_beam_agent-4_min: 109
    cleaning_beam_agent-5_max: 279
    cleaning_beam_agent-5_mean: 88.43
    cleaning_beam_agent-5_min: 21
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-32-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 897.9999999999809
  episode_reward_mean: 643.3399999999957
  episode_reward_min: 224.99999999999676
  episodes_this_iter: 96
  episodes_total: 15168
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19587.185
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00031950720585882664
        entropy: 1.153425693511963
        entropy_coeff: 0.0017600000137463212
        kl: 0.009510578587651253
        model: {}
        policy_loss: -0.026374759152531624
        total_loss: -0.024779947474598885
        vf_explained_var: 0.05581127107143402
        vf_loss: 17.22719383239746
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00031950720585882664
        entropy: 1.1298000812530518
        entropy_coeff: 0.0017600000137463212
        kl: 0.010532023385167122
        model: {}
        policy_loss: -0.02940087392926216
        total_loss: -0.02747979387640953
        vf_explained_var: 0.012098059058189392
        vf_loss: 18.03123664855957
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00031950720585882664
        entropy: 1.0818331241607666
        entropy_coeff: 0.0017600000137463212
        kl: 0.010299565270543098
        model: {}
        policy_loss: -0.02887888066470623
        total_loss: -0.02711537852883339
        vf_explained_var: 0.11910764873027802
        vf_loss: 16.076160430908203
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00031950720585882664
        entropy: 0.8287340998649597
        entropy_coeff: 0.0017600000137463212
        kl: 0.008558244444429874
        model: {}
        policy_loss: -0.025093279778957367
        total_loss: -0.023376677185297012
        vf_explained_var: 0.19759994745254517
        vf_loss: 14.635250091552734
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00031950720585882664
        entropy: 1.027219295501709
        entropy_coeff: 0.0017600000137463212
        kl: 0.0111561119556427
        model: {}
        policy_loss: -0.03250771760940552
        total_loss: -0.030418209731578827
        vf_explained_var: 0.08653761446475983
        vf_loss: 16.661880493164062
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00031950720585882664
        entropy: 0.8857100009918213
        entropy_coeff: 0.0017600000137463212
        kl: 0.010867406614124775
        model: {}
        policy_loss: -0.030985429883003235
        total_loss: -0.028935760259628296
        vf_explained_var: 0.21311645209789276
        vf_loss: 14.35036563873291
    load_time_ms: 13739.301
    num_steps_sampled: 15168000
    num_steps_trained: 15168000
    sample_time_ms: 90304.872
    update_time_ms: 21.123
  iterations_since_restore: 98
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.888636363636362
    ram_util_percent: 10.235795454545453
  pid: 24061
  policy_reward_max:
    agent-0: 149.6666666666666
    agent-1: 149.6666666666666
    agent-2: 149.6666666666666
    agent-3: 149.6666666666666
    agent-4: 149.6666666666666
    agent-5: 149.6666666666666
  policy_reward_mean:
    agent-0: 107.22333333333361
    agent-1: 107.22333333333361
    agent-2: 107.22333333333361
    agent-3: 107.22333333333361
    agent-4: 107.22333333333361
    agent-5: 107.22333333333361
  policy_reward_min:
    agent-0: 37.49999999999999
    agent-1: 37.49999999999999
    agent-2: 37.49999999999999
    agent-3: 37.49999999999999
    agent-4: 37.49999999999999
    agent-5: 37.49999999999999
  sampler_perf:
    mean_env_wait_ms: 24.524379283775712
    mean_inference_ms: 12.381686709036336
    mean_processing_ms: 51.28914097727013
  time_since_restore: 13123.162930727005
  time_this_iter_s: 123.65637850761414
  time_total_s: 22249.174744606018
  timestamp: 1637037128
  timesteps_since_restore: 9408000
  timesteps_this_iter: 96000
  timesteps_total: 15168000
  training_iteration: 158
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    158 |          22249.2 | 15168000 |   643.34 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 201
    apples_agent-0_mean: 8.27
    apples_agent-0_min: 0
    apples_agent-1_max: 83
    apples_agent-1_mean: 25.71
    apples_agent-1_min: 0
    apples_agent-2_max: 81
    apples_agent-2_mean: 7.15
    apples_agent-2_min: 0
    apples_agent-3_max: 160
    apples_agent-3_mean: 92.23
    apples_agent-3_min: 23
    apples_agent-4_max: 31
    apples_agent-4_mean: 1.57
    apples_agent-4_min: 0
    apples_agent-5_max: 169
    apples_agent-5_mean: 79.92
    apples_agent-5_min: 23
    cleaning_beam_agent-0_max: 497
    cleaning_beam_agent-0_mean: 325.28
    cleaning_beam_agent-0_min: 141
    cleaning_beam_agent-1_max: 434
    cleaning_beam_agent-1_mean: 236.63
    cleaning_beam_agent-1_min: 79
    cleaning_beam_agent-2_max: 431
    cleaning_beam_agent-2_mean: 303.8
    cleaning_beam_agent-2_min: 132
    cleaning_beam_agent-3_max: 175
    cleaning_beam_agent-3_mean: 52.75
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 441
    cleaning_beam_agent-4_mean: 334.36
    cleaning_beam_agent-4_min: 142
    cleaning_beam_agent-5_max: 284
    cleaning_beam_agent-5_mean: 92.26
    cleaning_beam_agent-5_min: 24
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-34-10
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 897.9999999999889
  episode_reward_mean: 641.3399999999959
  episode_reward_min: 239.99999999999596
  episodes_this_iter: 96
  episodes_total: 15264
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19520.656
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003135167935397476
        entropy: 1.1429691314697266
        entropy_coeff: 0.0017600000137463212
        kl: 0.009664695709943771
        model: {}
        policy_loss: -0.02711400017142296
        total_loss: -0.0256042443215847
        vf_explained_var: 0.11729250848293304
        vf_loss: 15.884420394897461
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003135167935397476
        entropy: 1.1138461828231812
        entropy_coeff: 0.0017600000137463212
        kl: 0.010529241524636745
        model: {}
        policy_loss: -0.02965935319662094
        total_loss: -0.027743082493543625
        vf_explained_var: 0.016003698110580444
        vf_loss: 17.70792007446289
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003135167935397476
        entropy: 1.087550163269043
        entropy_coeff: 0.0017600000137463212
        kl: 0.010444139130413532
        model: {}
        policy_loss: -0.028977934271097183
        total_loss: -0.027118109166622162
        vf_explained_var: 0.06412479281425476
        vf_loss: 16.85088348388672
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003135167935397476
        entropy: 0.8225802183151245
        entropy_coeff: 0.0017600000137463212
        kl: 0.00876082293689251
        model: {}
        policy_loss: -0.02437749318778515
        total_loss: -0.022657189518213272
        vf_explained_var: 0.213069885969162
        vf_loss: 14.158822059631348
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003135167935397476
        entropy: 1.0426838397979736
        entropy_coeff: 0.0017600000137463212
        kl: 0.01120353676378727
        model: {}
        policy_loss: -0.03267509490251541
        total_loss: -0.030585557222366333
        vf_explained_var: 0.06436735391616821
        vf_loss: 16.839557647705078
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003135167935397476
        entropy: 0.8855629563331604
        entropy_coeff: 0.0017600000137463212
        kl: 0.010821818374097347
        model: {}
        policy_loss: -0.030255688354372978
        total_loss: -0.028168488293886185
        vf_explained_var: 0.1762937605381012
        vf_loss: 14.814250946044922
    load_time_ms: 13735.749
    num_steps_sampled: 15264000
    num_steps_trained: 15264000
    sample_time_ms: 90226.178
    update_time_ms: 21.225
  iterations_since_restore: 99
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.376000000000001
    ram_util_percent: 10.098285714285714
  pid: 24061
  policy_reward_max:
    agent-0: 149.66666666666686
    agent-1: 149.66666666666686
    agent-2: 149.66666666666686
    agent-3: 149.66666666666686
    agent-4: 149.66666666666686
    agent-5: 149.66666666666686
  policy_reward_mean:
    agent-0: 106.89000000000026
    agent-1: 106.89000000000026
    agent-2: 106.89000000000026
    agent-3: 106.89000000000026
    agent-4: 106.89000000000026
    agent-5: 106.89000000000026
  policy_reward_min:
    agent-0: 39.99999999999999
    agent-1: 39.99999999999999
    agent-2: 39.99999999999999
    agent-3: 39.99999999999999
    agent-4: 39.99999999999999
    agent-5: 39.99999999999999
  sampler_perf:
    mean_env_wait_ms: 24.51863521904054
    mean_inference_ms: 12.378714031297523
    mean_processing_ms: 51.27976336383289
  time_since_restore: 13245.366224050522
  time_this_iter_s: 122.20329332351685
  time_total_s: 22371.378037929535
  timestamp: 1637037250
  timesteps_since_restore: 9504000
  timesteps_this_iter: 96000
  timesteps_total: 15264000
  training_iteration: 159
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    159 |          22371.4 | 15264000 |   641.34 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 37
    apples_agent-0_mean: 5.01
    apples_agent-0_min: 0
    apples_agent-1_max: 106
    apples_agent-1_mean: 28.42
    apples_agent-1_min: 0
    apples_agent-2_max: 70
    apples_agent-2_mean: 5.94
    apples_agent-2_min: 0
    apples_agent-3_max: 166
    apples_agent-3_mean: 98.64
    apples_agent-3_min: 25
    apples_agent-4_max: 53
    apples_agent-4_mean: 2.31
    apples_agent-4_min: 0
    apples_agent-5_max: 182
    apples_agent-5_mean: 82.18
    apples_agent-5_min: 16
    cleaning_beam_agent-0_max: 514
    cleaning_beam_agent-0_mean: 330.49
    cleaning_beam_agent-0_min: 145
    cleaning_beam_agent-1_max: 517
    cleaning_beam_agent-1_mean: 232.41
    cleaning_beam_agent-1_min: 78
    cleaning_beam_agent-2_max: 433
    cleaning_beam_agent-2_mean: 303.21
    cleaning_beam_agent-2_min: 169
    cleaning_beam_agent-3_max: 172
    cleaning_beam_agent-3_mean: 51.46
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 443
    cleaning_beam_agent-4_mean: 329.99
    cleaning_beam_agent-4_min: 167
    cleaning_beam_agent-5_max: 286
    cleaning_beam_agent-5_mean: 93.14
    cleaning_beam_agent-5_min: 20
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-36-13
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 994.9999999999792
  episode_reward_mean: 652.8499999999946
  episode_reward_min: 158.99999999999974
  episodes_this_iter: 96
  episodes_total: 15360
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19531.624
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000307526410324499
        entropy: 1.1535879373550415
        entropy_coeff: 0.0017600000137463212
        kl: 0.009580392390489578
        model: {}
        policy_loss: -0.02684769779443741
        total_loss: -0.02529313787817955
        vf_explained_var: 0.06922869384288788
        vf_loss: 16.687959671020508
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000307526410324499
        entropy: 1.1152763366699219
        entropy_coeff: 0.0017600000137463212
        kl: 0.010744048282504082
        model: {}
        policy_loss: -0.029564402997493744
        total_loss: -0.027601182460784912
        vf_explained_var: 0.008411407470703125
        vf_loss: 17.772968292236328
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000307526410324499
        entropy: 1.1000595092773438
        entropy_coeff: 0.0017600000137463212
        kl: 0.010820058174431324
        model: {}
        policy_loss: -0.028538644313812256
        total_loss: -0.026667535305023193
        vf_explained_var: 0.08444568514823914
        vf_loss: 16.431991577148438
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000307526410324499
        entropy: 0.8219042420387268
        entropy_coeff: 0.0017600000137463212
        kl: 0.00907911453396082
        model: {}
        policy_loss: -0.02490762248635292
        total_loss: -0.02310570515692234
        vf_explained_var: 0.2006850242614746
        vf_loss: 14.32645320892334
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000307526410324499
        entropy: 1.040325403213501
        entropy_coeff: 0.0017600000137463212
        kl: 0.011547960340976715
        model: {}
        policy_loss: -0.03273778781294823
        total_loss: -0.03057268261909485
        vf_explained_var: 0.059591323137283325
        vf_loss: 16.86485481262207
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000307526410324499
        entropy: 0.8975352048873901
        entropy_coeff: 0.0017600000137463212
        kl: 0.010409975424408913
        model: {}
        policy_loss: -0.03156024217605591
        total_loss: -0.02968078851699829
        vf_explained_var: 0.2319740206003189
        vf_loss: 13.771228790283203
    load_time_ms: 13718.578
    num_steps_sampled: 15360000
    num_steps_trained: 15360000
    sample_time_ms: 90149.458
    update_time_ms: 21.883
  iterations_since_restore: 100
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.386857142857142
    ram_util_percent: 10.136000000000001
  pid: 24061
  policy_reward_max:
    agent-0: 165.83333333333275
    agent-1: 165.83333333333275
    agent-2: 165.83333333333275
    agent-3: 165.83333333333275
    agent-4: 165.83333333333275
    agent-5: 165.83333333333275
  policy_reward_mean:
    agent-0: 108.8083333333336
    agent-1: 108.8083333333336
    agent-2: 108.8083333333336
    agent-3: 108.8083333333336
    agent-4: 108.8083333333336
    agent-5: 108.8083333333336
  policy_reward_min:
    agent-0: 26.500000000000032
    agent-1: 26.500000000000032
    agent-2: 26.500000000000032
    agent-3: 26.500000000000032
    agent-4: 26.500000000000032
    agent-5: 26.500000000000032
  sampler_perf:
    mean_env_wait_ms: 24.51377649517037
    mean_inference_ms: 12.37615055253327
    mean_processing_ms: 51.26980767858906
  time_since_restore: 13368.410777330399
  time_this_iter_s: 123.04455327987671
  time_total_s: 22494.42259120941
  timestamp: 1637037373
  timesteps_since_restore: 9600000
  timesteps_this_iter: 96000
  timesteps_total: 15360000
  training_iteration: 160
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    160 |          22494.4 | 15360000 |   652.85 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 59
    apples_agent-0_mean: 2.63
    apples_agent-0_min: 0
    apples_agent-1_max: 91
    apples_agent-1_mean: 23.3
    apples_agent-1_min: 0
    apples_agent-2_max: 111
    apples_agent-2_mean: 8.16
    apples_agent-2_min: 0
    apples_agent-3_max: 170
    apples_agent-3_mean: 98.47
    apples_agent-3_min: 15
    apples_agent-4_max: 105
    apples_agent-4_mean: 3.76
    apples_agent-4_min: 0
    apples_agent-5_max: 148
    apples_agent-5_mean: 84.49
    apples_agent-5_min: 9
    cleaning_beam_agent-0_max: 481
    cleaning_beam_agent-0_mean: 338.27
    cleaning_beam_agent-0_min: 136
    cleaning_beam_agent-1_max: 417
    cleaning_beam_agent-1_mean: 237.86
    cleaning_beam_agent-1_min: 79
    cleaning_beam_agent-2_max: 393
    cleaning_beam_agent-2_mean: 298.0
    cleaning_beam_agent-2_min: 84
    cleaning_beam_agent-3_max: 152
    cleaning_beam_agent-3_mean: 57.8
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 460
    cleaning_beam_agent-4_mean: 327.77
    cleaning_beam_agent-4_min: 173
    cleaning_beam_agent-5_max: 252
    cleaning_beam_agent-5_mean: 94.43
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-38-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 877.9999999999811
  episode_reward_mean: 649.8399999999963
  episode_reward_min: 236.99999999999693
  episodes_this_iter: 96
  episodes_total: 15456
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19608.466
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00030153599800541997
        entropy: 1.1516380310058594
        entropy_coeff: 0.0017600000137463212
        kl: 0.009861612692475319
        model: {}
        policy_loss: -0.027430016547441483
        total_loss: -0.025853492319583893
        vf_explained_var: 0.022481098771095276
        vf_loss: 16.310829162597656
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00030153599800541997
        entropy: 1.108646035194397
        entropy_coeff: 0.0017600000137463212
        kl: 0.010633810423314571
        model: {}
        policy_loss: -0.02829143777489662
        total_loss: -0.026469968259334564
        vf_explained_var: 0.013917967677116394
        vf_loss: 16.45931053161621
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00030153599800541997
        entropy: 1.08658766746521
        entropy_coeff: 0.0017600000137463212
        kl: 0.009994886815547943
        model: {}
        policy_loss: -0.02846485748887062
        total_loss: -0.026848431676626205
        vf_explained_var: 0.08341623842716217
        vf_loss: 15.298402786254883
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00030153599800541997
        entropy: 0.8199334144592285
        entropy_coeff: 0.0017600000137463212
        kl: 0.008818324655294418
        model: {}
        policy_loss: -0.024463893845677376
        total_loss: -0.022710664197802544
        vf_explained_var: 0.14258180558681488
        vf_loss: 14.326499938964844
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00030153599800541997
        entropy: 1.0274170637130737
        entropy_coeff: 0.0017600000137463212
        kl: 0.010798987001180649
        model: {}
        policy_loss: -0.031009970232844353
        total_loss: -0.029105771332979202
        vf_explained_var: 0.06882035732269287
        vf_loss: 15.526591300964355
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00030153599800541997
        entropy: 0.8815679550170898
        entropy_coeff: 0.0017600000137463212
        kl: 0.010417904704809189
        model: {}
        policy_loss: -0.030131826177239418
        total_loss: -0.028211699798703194
        vf_explained_var: 0.16812294721603394
        vf_loss: 13.881115913391113
    load_time_ms: 13673.83
    num_steps_sampled: 15456000
    num_steps_trained: 15456000
    sample_time_ms: 89883.535
    update_time_ms: 21.94
  iterations_since_restore: 101
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 16.496000000000002
    ram_util_percent: 10.54342857142857
  pid: 24061
  policy_reward_max:
    agent-0: 146.3333333333333
    agent-1: 146.3333333333333
    agent-2: 146.3333333333333
    agent-3: 146.3333333333333
    agent-4: 146.3333333333333
    agent-5: 146.3333333333333
  policy_reward_mean:
    agent-0: 108.30666666666697
    agent-1: 108.30666666666697
    agent-2: 108.30666666666697
    agent-3: 108.30666666666697
    agent-4: 108.30666666666697
    agent-5: 108.30666666666697
  policy_reward_min:
    agent-0: 39.499999999999986
    agent-1: 39.499999999999986
    agent-2: 39.499999999999986
    agent-3: 39.499999999999986
    agent-4: 39.499999999999986
    agent-5: 39.499999999999986
  sampler_perf:
    mean_env_wait_ms: 24.506904591884204
    mean_inference_ms: 12.373057608144498
    mean_processing_ms: 51.25864913939321
  time_since_restore: 13491.179636716843
  time_this_iter_s: 122.76885938644409
  time_total_s: 22617.191450595856
  timestamp: 1637037496
  timesteps_since_restore: 9696000
  timesteps_this_iter: 96000
  timesteps_total: 15456000
  training_iteration: 161
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    161 |          22617.2 | 15456000 |   649.84 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 79
    apples_agent-0_mean: 5.03
    apples_agent-0_min: 0
    apples_agent-1_max: 93
    apples_agent-1_mean: 26.6
    apples_agent-1_min: 0
    apples_agent-2_max: 74
    apples_agent-2_mean: 5.12
    apples_agent-2_min: 0
    apples_agent-3_max: 164
    apples_agent-3_mean: 100.36
    apples_agent-3_min: 15
    apples_agent-4_max: 48
    apples_agent-4_mean: 1.95
    apples_agent-4_min: 0
    apples_agent-5_max: 145
    apples_agent-5_mean: 86.18
    apples_agent-5_min: 9
    cleaning_beam_agent-0_max: 517
    cleaning_beam_agent-0_mean: 327.96
    cleaning_beam_agent-0_min: 158
    cleaning_beam_agent-1_max: 381
    cleaning_beam_agent-1_mean: 238.4
    cleaning_beam_agent-1_min: 94
    cleaning_beam_agent-2_max: 417
    cleaning_beam_agent-2_mean: 306.52
    cleaning_beam_agent-2_min: 84
    cleaning_beam_agent-3_max: 142
    cleaning_beam_agent-3_mean: 49.36
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 468
    cleaning_beam_agent-4_mean: 339.23
    cleaning_beam_agent-4_min: 199
    cleaning_beam_agent-5_max: 423
    cleaning_beam_agent-5_mean: 93.43
    cleaning_beam_agent-5_min: 30
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-40-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 881.9999999999816
  episode_reward_mean: 671.3899999999949
  episode_reward_min: 399.0000000000098
  episodes_this_iter: 96
  episodes_total: 15552
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19626.004
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00029554558568634093
        entropy: 1.1441832780838013
        entropy_coeff: 0.0017600000137463212
        kl: 0.009581508114933968
        model: {}
        policy_loss: -0.025948654860258102
        total_loss: -0.024494227021932602
        vf_explained_var: 0.05085413157939911
        vf_loss: 15.51891040802002
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00029554558568634093
        entropy: 1.1217243671417236
        entropy_coeff: 0.0017600000137463212
        kl: 0.010549996048212051
        model: {}
        policy_loss: -0.028425266966223717
        total_loss: -0.02665259689092636
        vf_explained_var: -0.0006076246500015259
        vf_loss: 16.369049072265625
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00029554558568634093
        entropy: 1.0844740867614746
        entropy_coeff: 0.0017600000137463212
        kl: 0.009650246240198612
        model: {}
        policy_loss: -0.027771800756454468
        total_loss: -0.026243098080158234
        vf_explained_var: 0.07800082862377167
        vf_loss: 15.073298454284668
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00029554558568634093
        entropy: 0.7985496520996094
        entropy_coeff: 0.0017600000137463212
        kl: 0.008222445845603943
        model: {}
        policy_loss: -0.022838853299617767
        total_loss: -0.021286683157086372
        vf_explained_var: 0.1971856653690338
        vf_loss: 13.13125991821289
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00029554558568634093
        entropy: 1.0306071043014526
        entropy_coeff: 0.0017600000137463212
        kl: 0.010884666815400124
        model: {}
        policy_loss: -0.03141096979379654
        total_loss: -0.029510220512747765
        vf_explained_var: 0.05997660756111145
        vf_loss: 15.376869201660156
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00029554558568634093
        entropy: 0.8699338436126709
        entropy_coeff: 0.0017600000137463212
        kl: 0.01021377183496952
        model: {}
        policy_loss: -0.029580533504486084
        total_loss: -0.02767188660800457
        vf_explained_var: 0.14628887176513672
        vf_loss: 13.969781875610352
    load_time_ms: 13023.686
    num_steps_sampled: 15552000
    num_steps_trained: 15552000
    sample_time_ms: 89824.233
    update_time_ms: 22.634
  iterations_since_restore: 102
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.080681818181816
    ram_util_percent: 13.935227272727275
  pid: 24061
  policy_reward_max:
    agent-0: 146.9999999999999
    agent-1: 146.9999999999999
    agent-2: 146.9999999999999
    agent-3: 146.9999999999999
    agent-4: 146.9999999999999
    agent-5: 146.9999999999999
  policy_reward_mean:
    agent-0: 111.89833333333362
    agent-1: 111.89833333333362
    agent-2: 111.89833333333362
    agent-3: 111.89833333333362
    agent-4: 111.89833333333362
    agent-5: 111.89833333333362
  policy_reward_min:
    agent-0: 66.49999999999982
    agent-1: 66.49999999999982
    agent-2: 66.49999999999982
    agent-3: 66.49999999999982
    agent-4: 66.49999999999982
    agent-5: 66.49999999999982
  sampler_perf:
    mean_env_wait_ms: 24.50453755321348
    mean_inference_ms: 12.372647223848467
    mean_processing_ms: 51.25611773930284
  time_since_restore: 13615.159425497055
  time_this_iter_s: 123.9797887802124
  time_total_s: 22741.171239376068
  timestamp: 1637037620
  timesteps_since_restore: 9792000
  timesteps_this_iter: 96000
  timesteps_total: 15552000
  training_iteration: 162
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    162 |          22741.2 | 15552000 |   671.39 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 52
    apples_agent-0_mean: 3.48
    apples_agent-0_min: 0
    apples_agent-1_max: 105
    apples_agent-1_mean: 28.65
    apples_agent-1_min: 0
    apples_agent-2_max: 86
    apples_agent-2_mean: 5.86
    apples_agent-2_min: 0
    apples_agent-3_max: 187
    apples_agent-3_mean: 97.06
    apples_agent-3_min: 28
    apples_agent-4_max: 93
    apples_agent-4_mean: 2.57
    apples_agent-4_min: 0
    apples_agent-5_max: 146
    apples_agent-5_mean: 84.64
    apples_agent-5_min: 13
    cleaning_beam_agent-0_max: 482
    cleaning_beam_agent-0_mean: 327.09
    cleaning_beam_agent-0_min: 136
    cleaning_beam_agent-1_max: 574
    cleaning_beam_agent-1_mean: 239.98
    cleaning_beam_agent-1_min: 98
    cleaning_beam_agent-2_max: 458
    cleaning_beam_agent-2_mean: 305.76
    cleaning_beam_agent-2_min: 149
    cleaning_beam_agent-3_max: 165
    cleaning_beam_agent-3_mean: 55.96
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 473
    cleaning_beam_agent-4_mean: 328.04
    cleaning_beam_agent-4_min: 166
    cleaning_beam_agent-5_max: 308
    cleaning_beam_agent-5_mean: 95.51
    cleaning_beam_agent-5_min: 17
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-42-25
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 904.9999999999808
  episode_reward_mean: 651.9199999999969
  episode_reward_min: 171.99999999999872
  episodes_this_iter: 96
  episodes_total: 15648
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19678.599
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00028955520247109234
        entropy: 1.150544285774231
        entropy_coeff: 0.0017600000137463212
        kl: 0.00980435498058796
        model: {}
        policy_loss: -0.026904167607426643
        total_loss: -0.02543344721198082
        vf_explained_var: 0.10890601575374603
        vf_loss: 15.348044395446777
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00028955520247109234
        entropy: 1.1097068786621094
        entropy_coeff: 0.0017600000137463212
        kl: 0.010186808183789253
        model: {}
        policy_loss: -0.028476379811763763
        total_loss: -0.026715347543358803
        vf_explained_var: 0.025767624378204346
        vf_loss: 16.767559051513672
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00028955520247109234
        entropy: 1.091064453125
        entropy_coeff: 0.0017600000137463212
        kl: 0.009883316233754158
        model: {}
        policy_loss: -0.02783985435962677
        total_loss: -0.026147091761231422
        vf_explained_var: 0.04938262701034546
        vf_loss: 16.363758087158203
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00028955520247109234
        entropy: 0.8176844716072083
        entropy_coeff: 0.0017600000137463212
        kl: 0.008658839389681816
        model: {}
        policy_loss: -0.024364106357097626
        total_loss: -0.022705594077706337
        vf_explained_var: 0.20672263205051422
        vf_loss: 13.658689498901367
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00028955520247109234
        entropy: 1.0376367568969727
        entropy_coeff: 0.0017600000137463212
        kl: 0.011202845722436905
        model: {}
        policy_loss: -0.0322037935256958
        total_loss: -0.03020717389881611
        vf_explained_var: 0.08184699714183807
        vf_loss: 15.822903633117676
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00028955520247109234
        entropy: 0.8930490612983704
        entropy_coeff: 0.0017600000137463212
        kl: 0.01036893017590046
        model: {}
        policy_loss: -0.029559355229139328
        total_loss: -0.02765917032957077
        vf_explained_var: 0.18743111193180084
        vf_loss: 13.981656074523926
    load_time_ms: 13042.181
    num_steps_sampled: 15648000
    num_steps_trained: 15648000
    sample_time_ms: 89849.193
    update_time_ms: 22.05
  iterations_since_restore: 103
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.848314606741575
    ram_util_percent: 13.957865168539326
  pid: 24061
  policy_reward_max:
    agent-0: 150.83333333333317
    agent-1: 150.83333333333317
    agent-2: 150.83333333333317
    agent-3: 150.83333333333317
    agent-4: 150.83333333333317
    agent-5: 150.83333333333317
  policy_reward_mean:
    agent-0: 108.6533333333336
    agent-1: 108.6533333333336
    agent-2: 108.6533333333336
    agent-3: 108.6533333333336
    agent-4: 108.6533333333336
    agent-5: 108.6533333333336
  policy_reward_min:
    agent-0: 28.666666666666707
    agent-1: 28.666666666666707
    agent-2: 28.666666666666707
    agent-3: 28.666666666666707
    agent-4: 28.666666666666707
    agent-5: 28.666666666666707
  sampler_perf:
    mean_env_wait_ms: 24.50112113696182
    mean_inference_ms: 12.371722454508253
    mean_processing_ms: 51.25088655419665
  time_since_restore: 13739.309870004654
  time_this_iter_s: 124.15044450759888
  time_total_s: 22865.321683883667
  timestamp: 1637037745
  timesteps_since_restore: 9888000
  timesteps_this_iter: 96000
  timesteps_total: 15648000
  training_iteration: 163
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    163 |          22865.3 | 15648000 |   651.92 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 105
    apples_agent-0_mean: 7.6
    apples_agent-0_min: 0
    apples_agent-1_max: 77
    apples_agent-1_mean: 26.16
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 5.29
    apples_agent-2_min: 0
    apples_agent-3_max: 166
    apples_agent-3_mean: 98.42
    apples_agent-3_min: 22
    apples_agent-4_max: 65
    apples_agent-4_mean: 2.06
    apples_agent-4_min: 0
    apples_agent-5_max: 148
    apples_agent-5_mean: 80.94
    apples_agent-5_min: 23
    cleaning_beam_agent-0_max: 505
    cleaning_beam_agent-0_mean: 331.45
    cleaning_beam_agent-0_min: 128
    cleaning_beam_agent-1_max: 393
    cleaning_beam_agent-1_mean: 220.02
    cleaning_beam_agent-1_min: 78
    cleaning_beam_agent-2_max: 450
    cleaning_beam_agent-2_mean: 293.27
    cleaning_beam_agent-2_min: 189
    cleaning_beam_agent-3_max: 166
    cleaning_beam_agent-3_mean: 63.19
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 460
    cleaning_beam_agent-4_mean: 330.15
    cleaning_beam_agent-4_min: 146
    cleaning_beam_agent-5_max: 306
    cleaning_beam_agent-5_mean: 99.21
    cleaning_beam_agent-5_min: 34
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-44-28
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 881.9999999999811
  episode_reward_mean: 633.3999999999967
  episode_reward_min: 292.99999999999915
  episodes_this_iter: 96
  episodes_total: 15744
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19755.158
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002835647901520133
        entropy: 1.1442408561706543
        entropy_coeff: 0.0017600000137463212
        kl: 0.010026223957538605
        model: {}
        policy_loss: -0.025294391438364983
        total_loss: -0.023771686479449272
        vf_explained_var: 0.08694705367088318
        vf_loss: 15.31322956085205
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002835647901520133
        entropy: 1.1059818267822266
        entropy_coeff: 0.0017600000137463212
        kl: 0.010322300717234612
        model: {}
        policy_loss: -0.0290692001581192
        total_loss: -0.027354639023542404
        vf_explained_var: 0.04769377410411835
        vf_loss: 15.96631908416748
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002835647901520133
        entropy: 1.0962703227996826
        entropy_coeff: 0.0017600000137463212
        kl: 0.010658729821443558
        model: {}
        policy_loss: -0.027657024562358856
        total_loss: -0.025849195197224617
        vf_explained_var: 0.04219399392604828
        vf_loss: 16.05518913269043
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002835647901520133
        entropy: 0.8470210433006287
        entropy_coeff: 0.0017600000137463212
        kl: 0.008723462000489235
        model: {}
        policy_loss: -0.023731883615255356
        total_loss: -0.022110184654593468
        vf_explained_var: 0.18425866961479187
        vf_loss: 13.677663803100586
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002835647901520133
        entropy: 1.0285133123397827
        entropy_coeff: 0.0017600000137463212
        kl: 0.011242630891501904
        model: {}
        policy_loss: -0.03148699924349785
        total_loss: -0.029445918276906013
        vf_explained_var: 0.04462754726409912
        vf_loss: 16.027328491210938
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002835647901520133
        entropy: 0.8861324787139893
        entropy_coeff: 0.0017600000137463212
        kl: 0.01034536026418209
        model: {}
        policy_loss: -0.029691390693187714
        total_loss: -0.027786603197455406
        vf_explained_var: 0.16777753829956055
        vf_loss: 13.953094482421875
    load_time_ms: 13065.524
    num_steps_sampled: 15744000
    num_steps_trained: 15744000
    sample_time_ms: 89898.968
    update_time_ms: 21.426
  iterations_since_restore: 104
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.08457142857143
    ram_util_percent: 13.961142857142857
  pid: 24061
  policy_reward_max:
    agent-0: 147.00000000000026
    agent-1: 147.00000000000026
    agent-2: 147.00000000000026
    agent-3: 147.00000000000026
    agent-4: 147.00000000000026
    agent-5: 147.00000000000026
  policy_reward_mean:
    agent-0: 105.56666666666698
    agent-1: 105.56666666666698
    agent-2: 105.56666666666698
    agent-3: 105.56666666666698
    agent-4: 105.56666666666698
    agent-5: 105.56666666666698
  policy_reward_min:
    agent-0: 48.83333333333322
    agent-1: 48.83333333333322
    agent-2: 48.83333333333322
    agent-3: 48.83333333333322
    agent-4: 48.83333333333322
    agent-5: 48.83333333333322
  sampler_perf:
    mean_env_wait_ms: 24.498142292300912
    mean_inference_ms: 12.371617156984735
    mean_processing_ms: 51.24812410536618
  time_since_restore: 13862.823451042175
  time_this_iter_s: 123.51358103752136
  time_total_s: 22988.83526492119
  timestamp: 1637037868
  timesteps_since_restore: 9984000
  timesteps_this_iter: 96000
  timesteps_total: 15744000
  training_iteration: 164
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    164 |          22988.8 | 15744000 |    633.4 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 85
    apples_agent-0_mean: 7.57
    apples_agent-0_min: 0
    apples_agent-1_max: 100
    apples_agent-1_mean: 24.6
    apples_agent-1_min: 0
    apples_agent-2_max: 57
    apples_agent-2_mean: 3.38
    apples_agent-2_min: 0
    apples_agent-3_max: 195
    apples_agent-3_mean: 108.05
    apples_agent-3_min: 30
    apples_agent-4_max: 47
    apples_agent-4_mean: 2.37
    apples_agent-4_min: 0
    apples_agent-5_max: 159
    apples_agent-5_mean: 84.48
    apples_agent-5_min: 23
    cleaning_beam_agent-0_max: 469
    cleaning_beam_agent-0_mean: 334.18
    cleaning_beam_agent-0_min: 180
    cleaning_beam_agent-1_max: 502
    cleaning_beam_agent-1_mean: 259.31
    cleaning_beam_agent-1_min: 106
    cleaning_beam_agent-2_max: 466
    cleaning_beam_agent-2_mean: 304.77
    cleaning_beam_agent-2_min: 172
    cleaning_beam_agent-3_max: 205
    cleaning_beam_agent-3_mean: 56.15
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 473
    cleaning_beam_agent-4_mean: 337.1
    cleaning_beam_agent-4_min: 116
    cleaning_beam_agent-5_max: 443
    cleaning_beam_agent-5_mean: 102.66
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-46-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 898.9999999999841
  episode_reward_mean: 667.7899999999936
  episode_reward_min: 214.99999999999753
  episodes_this_iter: 96
  episodes_total: 15840
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19815.861
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002775744069367647
        entropy: 1.1498740911483765
        entropy_coeff: 0.0017600000137463212
        kl: 0.009375747293233871
        model: {}
        policy_loss: -0.026419755071401596
        total_loss: -0.025034774094820023
        vf_explained_var: 0.0623774528503418
        vf_loss: 15.33608627319336
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002775744069367647
        entropy: 1.1034667491912842
        entropy_coeff: 0.0017600000137463212
        kl: 0.010139202699065208
        model: {}
        policy_loss: -0.028055766597390175
        total_loss: -0.026352018117904663
        vf_explained_var: 0.011290222406387329
        vf_loss: 16.180103302001953
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002775744069367647
        entropy: 1.0966007709503174
        entropy_coeff: 0.0017600000137463212
        kl: 0.010139322839677334
        model: {}
        policy_loss: -0.027391329407691956
        total_loss: -0.025668345391750336
        vf_explained_var: 0.006193295121192932
        vf_loss: 16.25136375427246
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002775744069367647
        entropy: 0.8093152642250061
        entropy_coeff: 0.0017600000137463212
        kl: 0.008057036437094212
        model: {}
        policy_loss: -0.02318311110138893
        total_loss: -0.021602345630526543
        vf_explained_var: 0.14768067002296448
        vf_loss: 13.937528610229492
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002775744069367647
        entropy: 1.025907278060913
        entropy_coeff: 0.0017600000137463212
        kl: 0.010876432061195374
        model: {}
        policy_loss: -0.030136043205857277
        total_loss: -0.028264256194233894
        vf_explained_var: 0.0817384123802185
        vf_loss: 15.020964622497559
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002775744069367647
        entropy: 0.8761770129203796
        entropy_coeff: 0.0017600000137463212
        kl: 0.009997120127081871
        model: {}
        policy_loss: -0.029160715639591217
        total_loss: -0.027315951883792877
        vf_explained_var: 0.1516284942626953
        vf_loss: 13.87411880493164
    load_time_ms: 13063.768
    num_steps_sampled: 15840000
    num_steps_trained: 15840000
    sample_time_ms: 89983.861
    update_time_ms: 22.045
  iterations_since_restore: 105
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.946892655367233
    ram_util_percent: 13.970621468926554
  pid: 24061
  policy_reward_max:
    agent-0: 149.83333333333303
    agent-1: 149.83333333333303
    agent-2: 149.83333333333303
    agent-3: 149.83333333333303
    agent-4: 149.83333333333303
    agent-5: 149.83333333333303
  policy_reward_mean:
    agent-0: 111.29833333333364
    agent-1: 111.29833333333364
    agent-2: 111.29833333333364
    agent-3: 111.29833333333364
    agent-4: 111.29833333333364
    agent-5: 111.29833333333364
  policy_reward_min:
    agent-0: 35.83333333333338
    agent-1: 35.83333333333338
    agent-2: 35.83333333333338
    agent-3: 35.83333333333338
    agent-4: 35.83333333333338
    agent-5: 35.83333333333338
  sampler_perf:
    mean_env_wait_ms: 24.496822279465977
    mean_inference_ms: 12.371030971991459
    mean_processing_ms: 51.24317970515459
  time_since_restore: 13986.481750011444
  time_this_iter_s: 123.6582989692688
  time_total_s: 23112.493563890457
  timestamp: 1637037992
  timesteps_since_restore: 10080000
  timesteps_this_iter: 96000
  timesteps_total: 15840000
  training_iteration: 165
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    165 |          23112.5 | 15840000 |   667.79 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 57
    apples_agent-0_mean: 4.47
    apples_agent-0_min: 0
    apples_agent-1_max: 181
    apples_agent-1_mean: 25.35
    apples_agent-1_min: 0
    apples_agent-2_max: 225
    apples_agent-2_mean: 6.58
    apples_agent-2_min: 0
    apples_agent-3_max: 195
    apples_agent-3_mean: 107.49
    apples_agent-3_min: 45
    apples_agent-4_max: 29
    apples_agent-4_mean: 1.28
    apples_agent-4_min: 0
    apples_agent-5_max: 159
    apples_agent-5_mean: 88.68
    apples_agent-5_min: 27
    cleaning_beam_agent-0_max: 490
    cleaning_beam_agent-0_mean: 320.81
    cleaning_beam_agent-0_min: 162
    cleaning_beam_agent-1_max: 502
    cleaning_beam_agent-1_mean: 243.28
    cleaning_beam_agent-1_min: 82
    cleaning_beam_agent-2_max: 413
    cleaning_beam_agent-2_mean: 298.03
    cleaning_beam_agent-2_min: 137
    cleaning_beam_agent-3_max: 134
    cleaning_beam_agent-3_mean: 52.76
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 458
    cleaning_beam_agent-4_mean: 338.26
    cleaning_beam_agent-4_min: 261
    cleaning_beam_agent-5_max: 217
    cleaning_beam_agent-5_mean: 85.97
    cleaning_beam_agent-5_min: 12
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-48-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 923.9999999999764
  episode_reward_mean: 687.479999999994
  episode_reward_min: 425.00000000000205
  episodes_this_iter: 96
  episodes_total: 15936
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19847.145
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002715839946176857
        entropy: 1.1468933820724487
        entropy_coeff: 0.0017600000137463212
        kl: 0.009946681559085846
        model: {}
        policy_loss: -0.025850191712379456
        total_loss: -0.024336079135537148
        vf_explained_var: 0.04311460256576538
        vf_loss: 15.433073043823242
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002715839946176857
        entropy: 1.1065239906311035
        entropy_coeff: 0.0017600000137463212
        kl: 0.00980126578360796
        model: {}
        policy_loss: -0.02745121717453003
        total_loss: -0.025861166417598724
        vf_explained_var: 0.021113023161888123
        vf_loss: 15.772806167602539
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002715839946176857
        entropy: 1.0936261415481567
        entropy_coeff: 0.0017600000137463212
        kl: 0.009885845705866814
        model: {}
        policy_loss: -0.026730969548225403
        total_loss: -0.02514338679611683
        vf_explained_var: 0.0480068176984787
        vf_loss: 15.351971626281738
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002715839946176857
        entropy: 0.8086285591125488
        entropy_coeff: 0.0017600000137463212
        kl: 0.007650990970432758
        model: {}
        policy_loss: -0.021936796605587006
        total_loss: -0.0204518623650074
        vf_explained_var: 0.14520347118377686
        vf_loss: 13.779212951660156
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002715839946176857
        entropy: 1.0238728523254395
        entropy_coeff: 0.0017600000137463212
        kl: 0.010534781962633133
        model: {}
        policy_loss: -0.030726958066225052
        total_loss: -0.028854714706540108
        vf_explained_var: 0.02829456329345703
        vf_loss: 15.673070907592773
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002715839946176857
        entropy: 0.8682945966720581
        entropy_coeff: 0.0017600000137463212
        kl: 0.010033131577074528
        model: {}
        policy_loss: -0.02813885547220707
        total_loss: -0.026354743167757988
        vf_explained_var: 0.18957461416721344
        vf_loss: 13.056861877441406
    load_time_ms: 13071.94
    num_steps_sampled: 15936000
    num_steps_trained: 15936000
    sample_time_ms: 90194.908
    update_time_ms: 21.892
  iterations_since_restore: 106
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.142372881355932
    ram_util_percent: 13.968926553672317
  pid: 24061
  policy_reward_max:
    agent-0: 153.99999999999994
    agent-1: 153.99999999999994
    agent-2: 153.99999999999994
    agent-3: 153.99999999999994
    agent-4: 153.99999999999994
    agent-5: 153.99999999999994
  policy_reward_mean:
    agent-0: 114.5800000000003
    agent-1: 114.5800000000003
    agent-2: 114.5800000000003
    agent-3: 114.5800000000003
    agent-4: 114.5800000000003
    agent-5: 114.5800000000003
  policy_reward_min:
    agent-0: 70.83333333333309
    agent-1: 70.83333333333309
    agent-2: 70.83333333333309
    agent-3: 70.83333333333309
    agent-4: 70.83333333333309
    agent-5: 70.83333333333309
  sampler_perf:
    mean_env_wait_ms: 24.495871286843222
    mean_inference_ms: 12.371503335803057
    mean_processing_ms: 51.24460615012198
  time_since_restore: 14110.354563236237
  time_this_iter_s: 123.87281322479248
  time_total_s: 23236.36637711525
  timestamp: 1637038116
  timesteps_since_restore: 10176000
  timesteps_this_iter: 96000
  timesteps_total: 15936000
  training_iteration: 166
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    166 |          23236.4 | 15936000 |   687.48 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 36
    apples_agent-0_mean: 3.43
    apples_agent-0_min: 0
    apples_agent-1_max: 100
    apples_agent-1_mean: 30.06
    apples_agent-1_min: 0
    apples_agent-2_max: 150
    apples_agent-2_mean: 7.99
    apples_agent-2_min: 0
    apples_agent-3_max: 178
    apples_agent-3_mean: 104.08
    apples_agent-3_min: 20
    apples_agent-4_max: 104
    apples_agent-4_mean: 4.42
    apples_agent-4_min: 0
    apples_agent-5_max: 159
    apples_agent-5_mean: 82.62
    apples_agent-5_min: 9
    cleaning_beam_agent-0_max: 440
    cleaning_beam_agent-0_mean: 321.39
    cleaning_beam_agent-0_min: 186
    cleaning_beam_agent-1_max: 400
    cleaning_beam_agent-1_mean: 240.86
    cleaning_beam_agent-1_min: 82
    cleaning_beam_agent-2_max: 480
    cleaning_beam_agent-2_mean: 299.66
    cleaning_beam_agent-2_min: 158
    cleaning_beam_agent-3_max: 128
    cleaning_beam_agent-3_mean: 53.36
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 441
    cleaning_beam_agent-4_mean: 330.96
    cleaning_beam_agent-4_min: 137
    cleaning_beam_agent-5_max: 307
    cleaning_beam_agent-5_mean: 88.97
    cleaning_beam_agent-5_min: 22
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-51-02
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 870.9999999999884
  episode_reward_mean: 660.6699999999942
  episode_reward_min: 183.9999999999988
  episodes_this_iter: 96
  episodes_total: 16032
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19908.574
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002655936114024371
        entropy: 1.1747820377349854
        entropy_coeff: 0.0017600000137463212
        kl: 0.008954392746090889
        model: {}
        policy_loss: -0.026236489415168762
        total_loss: -0.024832209572196007
        vf_explained_var: 0.03551971912384033
        vf_loss: 16.810203552246094
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002655936114024371
        entropy: 1.102639079093933
        entropy_coeff: 0.0017600000137463212
        kl: 0.010188877582550049
        model: {}
        policy_loss: -0.027924442663788795
        total_loss: -0.026073971763253212
        vf_explained_var: -0.005509465932846069
        vf_loss: 17.53340721130371
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002655936114024371
        entropy: 1.0891575813293457
        entropy_coeff: 0.0017600000137463212
        kl: 0.009415713138878345
        model: {}
        policy_loss: -0.027023185044527054
        total_loss: -0.025473108515143394
        vf_explained_var: 0.09123976528644562
        vf_loss: 15.838528633117676
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002655936114024371
        entropy: 0.8126784563064575
        entropy_coeff: 0.0017600000137463212
        kl: 0.007926961407065392
        model: {}
        policy_loss: -0.02280149981379509
        total_loss: -0.02125813439488411
        vf_explained_var: 0.2047523558139801
        vf_loss: 13.88290786743164
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002655936114024371
        entropy: 1.0217585563659668
        entropy_coeff: 0.0017600000137463212
        kl: 0.010938621126115322
        model: {}
        policy_loss: -0.03119809553027153
        total_loss: -0.029215382412075996
        vf_explained_var: 0.08619837462902069
        vf_loss: 15.932825088500977
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002655936114024371
        entropy: 0.8791173696517944
        entropy_coeff: 0.0017600000137463212
        kl: 0.009328420273959637
        model: {}
        policy_loss: -0.027418795973062515
        total_loss: -0.025690395385026932
        vf_explained_var: 0.19162945449352264
        vf_loss: 14.099630355834961
    load_time_ms: 13651.853
    num_steps_sampled: 16032000
    num_steps_trained: 16032000
    sample_time_ms: 90421.869
    update_time_ms: 21.801
  iterations_since_restore: 107
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.322222222222226
    ram_util_percent: 14.012560386473428
  pid: 24061
  policy_reward_max:
    agent-0: 145.16666666666686
    agent-1: 145.16666666666686
    agent-2: 145.16666666666686
    agent-3: 145.16666666666686
    agent-4: 145.16666666666686
    agent-5: 145.16666666666686
  policy_reward_mean:
    agent-0: 110.111666666667
    agent-1: 110.111666666667
    agent-2: 110.111666666667
    agent-3: 110.111666666667
    agent-4: 110.111666666667
    agent-5: 110.111666666667
  policy_reward_min:
    agent-0: 30.66666666666672
    agent-1: 30.66666666666672
    agent-2: 30.66666666666672
    agent-3: 30.66666666666672
    agent-4: 30.66666666666672
    agent-5: 30.66666666666672
  sampler_perf:
    mean_env_wait_ms: 24.49505380793712
    mean_inference_ms: 12.371390253675537
    mean_processing_ms: 51.246251997247654
  time_since_restore: 14240.578482627869
  time_this_iter_s: 130.22391939163208
  time_total_s: 23366.59029650688
  timestamp: 1637038262
  timesteps_since_restore: 10272000
  timesteps_this_iter: 96000
  timesteps_total: 16032000
  training_iteration: 167
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    167 |          23366.6 | 16032000 |   660.67 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 162
    apples_agent-0_mean: 6.29
    apples_agent-0_min: 0
    apples_agent-1_max: 108
    apples_agent-1_mean: 29.75
    apples_agent-1_min: 0
    apples_agent-2_max: 67
    apples_agent-2_mean: 5.85
    apples_agent-2_min: 0
    apples_agent-3_max: 187
    apples_agent-3_mean: 104.78
    apples_agent-3_min: 37
    apples_agent-4_max: 21
    apples_agent-4_mean: 0.63
    apples_agent-4_min: 0
    apples_agent-5_max: 137
    apples_agent-5_mean: 86.12
    apples_agent-5_min: 22
    cleaning_beam_agent-0_max: 495
    cleaning_beam_agent-0_mean: 316.38
    cleaning_beam_agent-0_min: 112
    cleaning_beam_agent-1_max: 431
    cleaning_beam_agent-1_mean: 233.2
    cleaning_beam_agent-1_min: 77
    cleaning_beam_agent-2_max: 546
    cleaning_beam_agent-2_mean: 307.57
    cleaning_beam_agent-2_min: 119
    cleaning_beam_agent-3_max: 142
    cleaning_beam_agent-3_mean: 50.37
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 419
    cleaning_beam_agent-4_mean: 337.2
    cleaning_beam_agent-4_min: 171
    cleaning_beam_agent-5_max: 181
    cleaning_beam_agent-5_mean: 76.67
    cleaning_beam_agent-5_min: 30
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-53-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 918.9999999999774
  episode_reward_mean: 676.8699999999939
  episode_reward_min: 327.00000000000205
  episodes_this_iter: 96
  episodes_total: 16128
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19967.051
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00025960319908335805
        entropy: 1.1446126699447632
        entropy_coeff: 0.0017600000137463212
        kl: 0.008452639915049076
        model: {}
        policy_loss: -0.024997500702738762
        total_loss: -0.02376710996031761
        vf_explained_var: 0.06954245269298553
        vf_loss: 15.543787956237793
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00025960319908335805
        entropy: 1.1101107597351074
        entropy_coeff: 0.0017600000137463212
        kl: 0.009424564428627491
        model: {}
        policy_loss: -0.027516480535268784
        total_loss: -0.025939352810382843
        vf_explained_var: 0.014354720711708069
        vf_loss: 16.46010971069336
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00025960319908335805
        entropy: 1.0761364698410034
        entropy_coeff: 0.0017600000137463212
        kl: 0.009222184307873249
        model: {}
        policy_loss: -0.02681264653801918
        total_loss: -0.025345446541905403
        vf_explained_var: 0.09201429784297943
        vf_loss: 15.167664527893066
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00025960319908335805
        entropy: 0.8203393816947937
        entropy_coeff: 0.0017600000137463212
        kl: 0.007683652453124523
        model: {}
        policy_loss: -0.021889181807637215
        total_loss: -0.02044336311519146
        vf_explained_var: 0.1903635412454605
        vf_loss: 13.528857231140137
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00025960319908335805
        entropy: 1.0255435705184937
        entropy_coeff: 0.0017600000137463212
        kl: 0.01029164157807827
        model: {}
        policy_loss: -0.02964802458882332
        total_loss: -0.02775339037179947
        vf_explained_var: 0.017785996198654175
        vf_loss: 16.4125919342041
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00025960319908335805
        entropy: 0.8732000589370728
        entropy_coeff: 0.0017600000137463212
        kl: 0.009739250876009464
        model: {}
        policy_loss: -0.02839839830994606
        total_loss: -0.02665313519537449
        vf_explained_var: 0.20229984819889069
        vf_loss: 13.342442512512207
    load_time_ms: 13936.181
    num_steps_sampled: 16128000
    num_steps_trained: 16128000
    sample_time_ms: 90439.552
    update_time_ms: 23.133
  iterations_since_restore: 108
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.745054945054942
    ram_util_percent: 14.079120879120877
  pid: 24061
  policy_reward_max:
    agent-0: 153.16666666666674
    agent-1: 153.16666666666674
    agent-2: 153.16666666666674
    agent-3: 153.16666666666674
    agent-4: 153.16666666666674
    agent-5: 153.16666666666674
  policy_reward_mean:
    agent-0: 112.811666666667
    agent-1: 112.811666666667
    agent-2: 112.811666666667
    agent-3: 112.811666666667
    agent-4: 112.811666666667
    agent-5: 112.811666666667
  policy_reward_min:
    agent-0: 54.49999999999981
    agent-1: 54.49999999999981
    agent-2: 54.49999999999981
    agent-3: 54.49999999999981
    agent-4: 54.49999999999981
    agent-5: 54.49999999999981
  sampler_perf:
    mean_env_wait_ms: 24.49256052469158
    mean_inference_ms: 12.370996290185108
    mean_processing_ms: 51.24568506849959
  time_since_restore: 14367.872237920761
  time_this_iter_s: 127.29375529289246
  time_total_s: 23493.884051799774
  timestamp: 1637038389
  timesteps_since_restore: 10368000
  timesteps_this_iter: 96000
  timesteps_total: 16128000
  training_iteration: 168
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 25.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    168 |          23493.9 | 16128000 |   676.87 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 52
    apples_agent-0_mean: 5.71
    apples_agent-0_min: 0
    apples_agent-1_max: 116
    apples_agent-1_mean: 28.76
    apples_agent-1_min: 0
    apples_agent-2_max: 105
    apples_agent-2_mean: 6.05
    apples_agent-2_min: 0
    apples_agent-3_max: 170
    apples_agent-3_mean: 101.29
    apples_agent-3_min: 30
    apples_agent-4_max: 42
    apples_agent-4_mean: 2.2
    apples_agent-4_min: 0
    apples_agent-5_max: 136
    apples_agent-5_mean: 83.72
    apples_agent-5_min: 10
    cleaning_beam_agent-0_max: 468
    cleaning_beam_agent-0_mean: 328.26
    cleaning_beam_agent-0_min: 149
    cleaning_beam_agent-1_max: 482
    cleaning_beam_agent-1_mean: 223.58
    cleaning_beam_agent-1_min: 77
    cleaning_beam_agent-2_max: 439
    cleaning_beam_agent-2_mean: 310.63
    cleaning_beam_agent-2_min: 182
    cleaning_beam_agent-3_max: 171
    cleaning_beam_agent-3_mean: 52.16
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 484
    cleaning_beam_agent-4_mean: 338.49
    cleaning_beam_agent-4_min: 223
    cleaning_beam_agent-5_max: 213
    cleaning_beam_agent-5_mean: 80.82
    cleaning_beam_agent-5_min: 18
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-55-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 913.9999999999804
  episode_reward_mean: 668.0099999999949
  episode_reward_min: 248.9999999999983
  episodes_this_iter: 96
  episodes_total: 16224
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20019.719
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000253612786764279
        entropy: 1.142094612121582
        entropy_coeff: 0.0017600000137463212
        kl: 0.008074812591075897
        model: {}
        policy_loss: -0.024688394740223885
        total_loss: -0.023692205548286438
        vf_explained_var: 0.0752377063035965
        vf_loss: 13.913125038146973
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000253612786764279
        entropy: 1.1024630069732666
        entropy_coeff: 0.0017600000137463212
        kl: 0.009792976081371307
        model: {}
        policy_loss: -0.02782856486737728
        total_loss: -0.026309583336114883
        vf_explained_var: 0.00204525887966156
        vf_loss: 15.007209777832031
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000253612786764279
        entropy: 1.0852270126342773
        entropy_coeff: 0.0017600000137463212
        kl: 0.009120065718889236
        model: {}
        policy_loss: -0.025172026827931404
        total_loss: -0.02382008172571659
        vf_explained_var: 0.04349257051944733
        vf_loss: 14.379301071166992
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000253612786764279
        entropy: 0.809674859046936
        entropy_coeff: 0.0017600000137463212
        kl: 0.007021081168204546
        model: {}
        policy_loss: -0.02034963294863701
        total_loss: -0.019097933545708656
        vf_explained_var: 0.15426959097385406
        vf_loss: 12.725116729736328
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000253612786764279
        entropy: 1.0183063745498657
        entropy_coeff: 0.0017600000137463212
        kl: 0.010353917255997658
        model: {}
        policy_loss: -0.02978072501718998
        total_loss: -0.02803932875394821
        vf_explained_var: 0.027191326022148132
        vf_loss: 14.628304481506348
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000253612786764279
        entropy: 0.8877303600311279
        entropy_coeff: 0.0017600000137463212
        kl: 0.009318524040281773
        model: {}
        policy_loss: -0.027426332235336304
        total_loss: -0.025833765044808388
        vf_explained_var: 0.14089323580265045
        vf_loss: 12.912656784057617
    load_time_ms: 14367.429
    num_steps_sampled: 16224000
    num_steps_trained: 16224000
    sample_time_ms: 90646.995
    update_time_ms: 24.135
  iterations_since_restore: 109
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.697282608695648
    ram_util_percent: 14.045652173913044
  pid: 24061
  policy_reward_max:
    agent-0: 152.3333333333332
    agent-1: 152.3333333333332
    agent-2: 152.3333333333332
    agent-3: 152.3333333333332
    agent-4: 152.3333333333332
    agent-5: 152.3333333333332
  policy_reward_mean:
    agent-0: 111.33500000000035
    agent-1: 111.33500000000035
    agent-2: 111.33500000000035
    agent-3: 111.33500000000035
    agent-4: 111.33500000000035
    agent-5: 111.33500000000035
  policy_reward_min:
    agent-0: 41.49999999999998
    agent-1: 41.49999999999998
    agent-2: 41.49999999999998
    agent-3: 41.49999999999998
    agent-4: 41.49999999999998
    agent-5: 41.49999999999998
  sampler_perf:
    mean_env_wait_ms: 24.49333162915335
    mean_inference_ms: 12.371731873608457
    mean_processing_ms: 51.25078707114329
  time_since_restore: 14496.994533061981
  time_this_iter_s: 129.1222951412201
  time_total_s: 23623.006346940994
  timestamp: 1637038519
  timesteps_since_restore: 10464000
  timesteps_this_iter: 96000
  timesteps_total: 16224000
  training_iteration: 169
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    169 |            23623 | 16224000 |   668.01 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 91
    apples_agent-0_mean: 7.74
    apples_agent-0_min: 0
    apples_agent-1_max: 102
    apples_agent-1_mean: 28.23
    apples_agent-1_min: 0
    apples_agent-2_max: 133
    apples_agent-2_mean: 6.15
    apples_agent-2_min: 0
    apples_agent-3_max: 157
    apples_agent-3_mean: 103.26
    apples_agent-3_min: 46
    apples_agent-4_max: 23
    apples_agent-4_mean: 1.39
    apples_agent-4_min: 0
    apples_agent-5_max: 137
    apples_agent-5_mean: 84.86
    apples_agent-5_min: 43
    cleaning_beam_agent-0_max: 450
    cleaning_beam_agent-0_mean: 315.82
    cleaning_beam_agent-0_min: 150
    cleaning_beam_agent-1_max: 422
    cleaning_beam_agent-1_mean: 230.02
    cleaning_beam_agent-1_min: 86
    cleaning_beam_agent-2_max: 450
    cleaning_beam_agent-2_mean: 316.15
    cleaning_beam_agent-2_min: 109
    cleaning_beam_agent-3_max: 184
    cleaning_beam_agent-3_mean: 53.52
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 439
    cleaning_beam_agent-4_mean: 330.7
    cleaning_beam_agent-4_min: 170
    cleaning_beam_agent-5_max: 205
    cleaning_beam_agent-5_mean: 79.77
    cleaning_beam_agent-5_min: 24
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-57-28
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 907.9999999999729
  episode_reward_mean: 685.3999999999941
  episode_reward_min: 281.99999999999835
  episodes_this_iter: 96
  episodes_total: 16320
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20020.191
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002476224035490304
        entropy: 1.1659574508666992
        entropy_coeff: 0.0017600000137463212
        kl: 0.008540008217096329
        model: {}
        policy_loss: -0.025239573791623116
        total_loss: -0.023976275697350502
        vf_explained_var: 0.05108186602592468
        vf_loss: 16.073814392089844
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002476224035490304
        entropy: 1.1045316457748413
        entropy_coeff: 0.0017600000137463212
        kl: 0.009147549979388714
        model: {}
        policy_loss: -0.027152350172400475
        total_loss: -0.02556779608130455
        vf_explained_var: -0.0033782124519348145
        vf_loss: 16.990163803100586
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002476224035490304
        entropy: 1.0735057592391968
        entropy_coeff: 0.0017600000137463212
        kl: 0.008972817100584507
        model: {}
        policy_loss: -0.025807462632656097
        total_loss: -0.024292465299367905
        vf_explained_var: 0.04918473958969116
        vf_loss: 16.098045349121094
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002476224035490304
        entropy: 0.7990638017654419
        entropy_coeff: 0.0017600000137463212
        kl: 0.00790205504745245
        model: {}
        policy_loss: -0.021733300760388374
        total_loss: -0.020172256976366043
        vf_explained_var: 0.18018481135368347
        vf_loss: 13.869829177856445
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002476224035490304
        entropy: 1.0288792848587036
        entropy_coeff: 0.0017600000137463212
        kl: 0.010128006339073181
        model: {}
        policy_loss: -0.029813677072525024
        total_loss: -0.02798902802169323
        vf_explained_var: 0.049809783697128296
        vf_loss: 16.098773956298828
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002476224035490304
        entropy: 0.8804510831832886
        entropy_coeff: 0.0017600000137463212
        kl: 0.009289820678532124
        model: {}
        policy_loss: -0.027854830026626587
        total_loss: -0.02608482725918293
        vf_explained_var: 0.13750368356704712
        vf_loss: 14.616308212280273
    load_time_ms: 14717.916
    num_steps_sampled: 16320000
    num_steps_trained: 16320000
    sample_time_ms: 90902.126
    update_time_ms: 23.969
  iterations_since_restore: 110
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.72717391304348
    ram_util_percent: 13.06739130434783
  pid: 24061
  policy_reward_max:
    agent-0: 151.33333333333292
    agent-1: 151.33333333333292
    agent-2: 151.33333333333292
    agent-3: 151.33333333333292
    agent-4: 151.33333333333292
    agent-5: 151.33333333333292
  policy_reward_mean:
    agent-0: 114.23333333333363
    agent-1: 114.23333333333363
    agent-2: 114.23333333333363
    agent-3: 114.23333333333363
    agent-4: 114.23333333333363
    agent-5: 114.23333333333363
  policy_reward_min:
    agent-0: 46.999999999999964
    agent-1: 46.999999999999964
    agent-2: 46.999999999999964
    agent-3: 46.999999999999964
    agent-4: 46.999999999999964
    agent-5: 46.999999999999964
  sampler_perf:
    mean_env_wait_ms: 24.491756074567043
    mean_inference_ms: 12.371797051448434
    mean_processing_ms: 51.251416499746135
  time_since_restore: 14626.119288921356
  time_this_iter_s: 129.124755859375
  time_total_s: 23752.13110280037
  timestamp: 1637038648
  timesteps_since_restore: 10560000
  timesteps_this_iter: 96000
  timesteps_total: 16320000
  training_iteration: 170
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    170 |          23752.1 | 16320000 |    685.4 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 62
    apples_agent-0_mean: 4.02
    apples_agent-0_min: 0
    apples_agent-1_max: 106
    apples_agent-1_mean: 23.08
    apples_agent-1_min: 0
    apples_agent-2_max: 137
    apples_agent-2_mean: 5.24
    apples_agent-2_min: 0
    apples_agent-3_max: 170
    apples_agent-3_mean: 107.24
    apples_agent-3_min: 25
    apples_agent-4_max: 82
    apples_agent-4_mean: 2.23
    apples_agent-4_min: 0
    apples_agent-5_max: 192
    apples_agent-5_mean: 89.06
    apples_agent-5_min: 30
    cleaning_beam_agent-0_max: 486
    cleaning_beam_agent-0_mean: 338.02
    cleaning_beam_agent-0_min: 182
    cleaning_beam_agent-1_max: 416
    cleaning_beam_agent-1_mean: 246.63
    cleaning_beam_agent-1_min: 95
    cleaning_beam_agent-2_max: 447
    cleaning_beam_agent-2_mean: 314.34
    cleaning_beam_agent-2_min: 183
    cleaning_beam_agent-3_max: 288
    cleaning_beam_agent-3_mean: 50.6
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 435
    cleaning_beam_agent-4_mean: 333.39
    cleaning_beam_agent-4_min: 151
    cleaning_beam_agent-5_max: 189
    cleaning_beam_agent-5_mean: 76.26
    cleaning_beam_agent-5_min: 17
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-59-30
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 892.9999999999804
  episode_reward_mean: 703.6399999999933
  episode_reward_min: 382.0000000000082
  episodes_this_iter: 96
  episodes_total: 16416
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19956.184
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002416320057818666
        entropy: 1.1447126865386963
        entropy_coeff: 0.0017600000137463212
        kl: 0.008360357955098152
        model: {}
        policy_loss: -0.023760639131069183
        total_loss: -0.022523585706949234
        vf_explained_var: 0.07011154294013977
        vf_loss: 15.796747207641602
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002416320057818666
        entropy: 1.113646388053894
        entropy_coeff: 0.0017600000137463212
        kl: 0.009615299291908741
        model: {}
        policy_loss: -0.027580343186855316
        total_loss: -0.025972068309783936
        vf_explained_var: 0.03143753111362457
        vf_loss: 16.452335357666016
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002416320057818666
        entropy: 1.0724506378173828
        entropy_coeff: 0.0017600000137463212
        kl: 0.009055634960532188
        model: {}
        policy_loss: -0.0253708828240633
        total_loss: -0.02385001629590988
        vf_explained_var: 0.0608377605676651
        vf_loss: 15.972511291503906
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002416320057818666
        entropy: 0.7763400077819824
        entropy_coeff: 0.0017600000137463212
        kl: 0.007308598142117262
        model: {}
        policy_loss: -0.020021649077534676
        total_loss: -0.0185621939599514
        vf_explained_var: 0.19678989052772522
        vf_loss: 13.640950202941895
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002416320057818666
        entropy: 1.0188274383544922
        entropy_coeff: 0.0017600000137463212
        kl: 0.009686261415481567
        model: {}
        policy_loss: -0.02926984801888466
        total_loss: -0.02758461982011795
        vf_explained_var: 0.09292550384998322
        vf_loss: 15.411099433898926
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002416320057818666
        entropy: 0.8838474750518799
        entropy_coeff: 0.0017600000137463212
        kl: 0.009080212563276291
        model: {}
        policy_loss: -0.02701694332063198
        total_loss: -0.025325998663902283
        vf_explained_var: 0.15824556350708008
        vf_loss: 14.304790496826172
    load_time_ms: 14714.309
    num_steps_sampled: 16416000
    num_steps_trained: 16416000
    sample_time_ms: 90905.841
    update_time_ms: 23.964
  iterations_since_restore: 111
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.487931034482756
    ram_util_percent: 10.163218390804596
  pid: 24061
  policy_reward_max:
    agent-0: 148.8333333333332
    agent-1: 148.8333333333332
    agent-2: 148.8333333333332
    agent-3: 148.8333333333332
    agent-4: 148.8333333333332
    agent-5: 148.8333333333332
  policy_reward_mean:
    agent-0: 117.27333333333365
    agent-1: 117.27333333333365
    agent-2: 117.27333333333365
    agent-3: 117.27333333333365
    agent-4: 117.27333333333365
    agent-5: 117.27333333333365
  policy_reward_min:
    agent-0: 63.666666666666494
    agent-1: 63.666666666666494
    agent-2: 63.666666666666494
    agent-3: 63.666666666666494
    agent-4: 63.666666666666494
    agent-5: 63.666666666666494
  sampler_perf:
    mean_env_wait_ms: 24.486226813067383
    mean_inference_ms: 12.370019879052562
    mean_processing_ms: 51.24332309119391
  time_since_restore: 14748.21999168396
  time_this_iter_s: 122.10070276260376
  time_total_s: 23874.231805562973
  timestamp: 1637038770
  timesteps_since_restore: 10656000
  timesteps_this_iter: 96000
  timesteps_total: 16416000
  training_iteration: 171
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    171 |          23874.2 | 16416000 |   703.64 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 99
    apples_agent-0_mean: 6.84
    apples_agent-0_min: 0
    apples_agent-1_max: 91
    apples_agent-1_mean: 26.82
    apples_agent-1_min: 0
    apples_agent-2_max: 137
    apples_agent-2_mean: 7.47
    apples_agent-2_min: 0
    apples_agent-3_max: 176
    apples_agent-3_mean: 103.06
    apples_agent-3_min: 24
    apples_agent-4_max: 49
    apples_agent-4_mean: 1.69
    apples_agent-4_min: 0
    apples_agent-5_max: 137
    apples_agent-5_mean: 85.7
    apples_agent-5_min: 37
    cleaning_beam_agent-0_max: 471
    cleaning_beam_agent-0_mean: 313.41
    cleaning_beam_agent-0_min: 161
    cleaning_beam_agent-1_max: 480
    cleaning_beam_agent-1_mean: 246.93
    cleaning_beam_agent-1_min: 105
    cleaning_beam_agent-2_max: 505
    cleaning_beam_agent-2_mean: 298.59
    cleaning_beam_agent-2_min: 134
    cleaning_beam_agent-3_max: 152
    cleaning_beam_agent-3_mean: 54.96
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 442
    cleaning_beam_agent-4_mean: 331.22
    cleaning_beam_agent-4_min: 227
    cleaning_beam_agent-5_max: 206
    cleaning_beam_agent-5_mean: 73.17
    cleaning_beam_agent-5_min: 23
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-01-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 910.9999999999839
  episode_reward_mean: 667.0999999999943
  episode_reward_min: 272.9999999999971
  episodes_this_iter: 96
  episodes_total: 16512
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19896.478
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00023564159346278757
        entropy: 1.1502411365509033
        entropy_coeff: 0.0017600000137463212
        kl: 0.008144169114530087
        model: {}
        policy_loss: -0.024222426116466522
        total_loss: -0.02306152507662773
        vf_explained_var: 0.06804479658603668
        vf_loss: 15.564956665039062
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00023564159346278757
        entropy: 1.108462929725647
        entropy_coeff: 0.0017600000137463212
        kl: 0.010188892483711243
        model: {}
        policy_loss: -0.02764776721596718
        total_loss: -0.025890681892633438
        vf_explained_var: -0.0002253800630569458
        vf_loss: 16.701969146728516
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00023564159346278757
        entropy: 1.0936745405197144
        entropy_coeff: 0.0017600000137463212
        kl: 0.008838053792715073
        model: {}
        policy_loss: -0.02508934959769249
        total_loss: -0.023681415244936943
        vf_explained_var: 0.06347896158695221
        vf_loss: 15.651908874511719
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00023564159346278757
        entropy: 0.817354679107666
        entropy_coeff: 0.0017600000137463212
        kl: 0.007412471808493137
        model: {}
        policy_loss: -0.021297795698046684
        total_loss: -0.01987142488360405
        vf_explained_var: 0.17320221662521362
        vf_loss: 13.82421875
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00023564159346278757
        entropy: 1.0239766836166382
        entropy_coeff: 0.0017600000137463212
        kl: 0.009530153125524521
        model: {}
        policy_loss: -0.028989151120185852
        total_loss: -0.027277864515781403
        vf_explained_var: 0.03784690797328949
        vf_loss: 16.07455825805664
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00023564159346278757
        entropy: 0.9078286290168762
        entropy_coeff: 0.0017600000137463212
        kl: 0.009379863739013672
        model: {}
        policy_loss: -0.027898751199245453
        total_loss: -0.02623436599969864
        vf_explained_var: 0.17003752291202545
        vf_loss: 13.861919403076172
    load_time_ms: 14941.677
    num_steps_sampled: 16512000
    num_steps_trained: 16512000
    sample_time_ms: 90883.893
    update_time_ms: 22.782
  iterations_since_restore: 112
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.780446927374301
    ram_util_percent: 10.25195530726257
  pid: 24061
  policy_reward_max:
    agent-0: 151.83333333333292
    agent-1: 151.83333333333292
    agent-2: 151.83333333333292
    agent-3: 151.83333333333292
    agent-4: 151.83333333333292
    agent-5: 151.83333333333292
  policy_reward_mean:
    agent-0: 111.18333333333364
    agent-1: 111.18333333333364
    agent-2: 111.18333333333364
    agent-3: 111.18333333333364
    agent-4: 111.18333333333364
    agent-5: 111.18333333333364
  policy_reward_min:
    agent-0: 45.499999999999936
    agent-1: 45.499999999999936
    agent-2: 45.499999999999936
    agent-3: 45.499999999999936
    agent-4: 45.499999999999936
    agent-5: 45.499999999999936
  sampler_perf:
    mean_env_wait_ms: 24.48305356582711
    mean_inference_ms: 12.369333011994604
    mean_processing_ms: 51.24006446107842
  time_since_restore: 14873.676589250565
  time_this_iter_s: 125.45659756660461
  time_total_s: 23999.688403129578
  timestamp: 1637038896
  timesteps_since_restore: 10752000
  timesteps_this_iter: 96000
  timesteps_total: 16512000
  training_iteration: 172
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    172 |          23999.7 | 16512000 |    667.1 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 92
    apples_agent-0_mean: 4.37
    apples_agent-0_min: 0
    apples_agent-1_max: 91
    apples_agent-1_mean: 24.89
    apples_agent-1_min: 0
    apples_agent-2_max: 132
    apples_agent-2_mean: 4.65
    apples_agent-2_min: 0
    apples_agent-3_max: 392
    apples_agent-3_mean: 114.06
    apples_agent-3_min: 32
    apples_agent-4_max: 45
    apples_agent-4_mean: 1.2
    apples_agent-4_min: 0
    apples_agent-5_max: 397
    apples_agent-5_mean: 92.46
    apples_agent-5_min: 35
    cleaning_beam_agent-0_max: 458
    cleaning_beam_agent-0_mean: 317.2
    cleaning_beam_agent-0_min: 152
    cleaning_beam_agent-1_max: 479
    cleaning_beam_agent-1_mean: 242.7
    cleaning_beam_agent-1_min: 98
    cleaning_beam_agent-2_max: 505
    cleaning_beam_agent-2_mean: 313.19
    cleaning_beam_agent-2_min: 96
    cleaning_beam_agent-3_max: 135
    cleaning_beam_agent-3_mean: 44.55
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 456
    cleaning_beam_agent-4_mean: 332.42
    cleaning_beam_agent-4_min: 229
    cleaning_beam_agent-5_max: 190
    cleaning_beam_agent-5_mean: 71.97
    cleaning_beam_agent-5_min: 19
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-03-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 940.9999999999816
  episode_reward_mean: 698.2899999999921
  episode_reward_min: 234.99999999999667
  episodes_this_iter: 96
  episodes_total: 16608
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19856.458
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00022965119569562376
        entropy: 1.1569148302078247
        entropy_coeff: 0.0017600000137463212
        kl: 0.007956420071423054
        model: {}
        policy_loss: -0.023032426834106445
        total_loss: -0.021918885409832
        vf_explained_var: 0.05745704472064972
        vf_loss: 15.584272384643555
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00022965119569562376
        entropy: 1.1018253564834595
        entropy_coeff: 0.0017600000137463212
        kl: 0.009934840723872185
        model: {}
        policy_loss: -0.026539182290434837
        total_loss: -0.024864083155989647
        vf_explained_var: 0.016141057014465332
        vf_loss: 16.2734317779541
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00022965119569562376
        entropy: 1.0749776363372803
        entropy_coeff: 0.0017600000137463212
        kl: 0.008612008765339851
        model: {}
        policy_loss: -0.024956880137324333
        total_loss: -0.0235490370541811
        vf_explained_var: 0.0464491993188858
        vf_loss: 15.773996353149414
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00022965119569562376
        entropy: 0.7708057761192322
        entropy_coeff: 0.0017600000137463212
        kl: 0.007117370143532753
        model: {}
        policy_loss: -0.02030814252793789
        total_loss: -0.018857963383197784
        vf_explained_var: 0.16315491497516632
        vf_loss: 13.833218574523926
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00022965119569562376
        entropy: 1.0275520086288452
        entropy_coeff: 0.0017600000137463212
        kl: 0.010214069858193398
        model: {}
        policy_loss: -0.028724558651447296
        total_loss: -0.026825379580259323
        vf_explained_var: -0.005226552486419678
        vf_loss: 16.648540496826172
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00022965119569562376
        entropy: 0.8778222799301147
        entropy_coeff: 0.0017600000137463212
        kl: 0.008492134511470795
        model: {}
        policy_loss: -0.02569342590868473
        total_loss: -0.024096637964248657
        vf_explained_var: 0.12876513600349426
        vf_loss: 14.433298110961914
    load_time_ms: 14931.257
    num_steps_sampled: 16608000
    num_steps_trained: 16608000
    sample_time_ms: 90723.23
    update_time_ms: 23.258
  iterations_since_restore: 113
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.43757225433526
    ram_util_percent: 10.154335260115605
  pid: 24061
  policy_reward_max:
    agent-0: 156.8333333333335
    agent-1: 156.8333333333335
    agent-2: 156.8333333333335
    agent-3: 156.8333333333335
    agent-4: 156.8333333333335
    agent-5: 156.8333333333335
  policy_reward_mean:
    agent-0: 116.38166666666703
    agent-1: 116.38166666666703
    agent-2: 116.38166666666703
    agent-3: 116.38166666666703
    agent-4: 116.38166666666703
    agent-5: 116.38166666666703
  policy_reward_min:
    agent-0: 39.16666666666666
    agent-1: 39.16666666666666
    agent-2: 39.16666666666666
    agent-3: 39.16666666666666
    agent-4: 39.16666666666666
    agent-5: 39.16666666666666
  sampler_perf:
    mean_env_wait_ms: 24.477512407096775
    mean_inference_ms: 12.366736820692443
    mean_processing_ms: 51.23057186930637
  time_since_restore: 14995.72327208519
  time_this_iter_s: 122.04668283462524
  time_total_s: 24121.735085964203
  timestamp: 1637039018
  timesteps_since_restore: 10848000
  timesteps_this_iter: 96000
  timesteps_total: 16608000
  training_iteration: 173
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    173 |          24121.7 | 16608000 |   698.29 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 28
    apples_agent-0_mean: 2.42
    apples_agent-0_min: 0
    apples_agent-1_max: 90
    apples_agent-1_mean: 29.91
    apples_agent-1_min: 0
    apples_agent-2_max: 74
    apples_agent-2_mean: 7.52
    apples_agent-2_min: 0
    apples_agent-3_max: 168
    apples_agent-3_mean: 104.73
    apples_agent-3_min: 32
    apples_agent-4_max: 47
    apples_agent-4_mean: 2.09
    apples_agent-4_min: 0
    apples_agent-5_max: 134
    apples_agent-5_mean: 86.72
    apples_agent-5_min: 34
    cleaning_beam_agent-0_max: 477
    cleaning_beam_agent-0_mean: 315.19
    cleaning_beam_agent-0_min: 134
    cleaning_beam_agent-1_max: 478
    cleaning_beam_agent-1_mean: 242.52
    cleaning_beam_agent-1_min: 108
    cleaning_beam_agent-2_max: 498
    cleaning_beam_agent-2_mean: 306.26
    cleaning_beam_agent-2_min: 96
    cleaning_beam_agent-3_max: 159
    cleaning_beam_agent-3_mean: 49.59
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 444
    cleaning_beam_agent-4_mean: 324.53
    cleaning_beam_agent-4_min: 134
    cleaning_beam_agent-5_max: 279
    cleaning_beam_agent-5_mean: 82.14
    cleaning_beam_agent-5_min: 17
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-05-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 954.9999999999741
  episode_reward_mean: 683.0799999999938
  episode_reward_min: 234.99999999999667
  episodes_this_iter: 96
  episodes_total: 16704
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19808.261
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00022366079792845994
        entropy: 1.1596280336380005
        entropy_coeff: 0.0017600000137463212
        kl: 0.008401110768318176
        model: {}
        policy_loss: -0.023981887847185135
        total_loss: -0.02279345504939556
        vf_explained_var: 0.0834493339061737
        vf_loss: 15.491536140441895
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00022366079792845994
        entropy: 1.1038907766342163
        entropy_coeff: 0.0017600000137463212
        kl: 0.009055690839886665
        model: {}
        policy_loss: -0.02621275745332241
        total_loss: -0.024697396904230118
        vf_explained_var: 0.02566593885421753
        vf_loss: 16.470706939697266
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00022366079792845994
        entropy: 1.06362783908844
        entropy_coeff: 0.0017600000137463212
        kl: 0.008316274732351303
        model: {}
        policy_loss: -0.024446532130241394
        total_loss: -0.023111488670110703
        vf_explained_var: 0.08662837743759155
        vf_loss: 15.43770980834961
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00022366079792845994
        entropy: 0.779961347579956
        entropy_coeff: 0.0017600000137463212
        kl: 0.007722684182226658
        model: {}
        policy_loss: -0.020833801478147507
        total_loss: -0.019312076270580292
        vf_explained_var: 0.20187629759311676
        vf_loss: 13.499190330505371
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00022366079792845994
        entropy: 1.0388249158859253
        entropy_coeff: 0.0017600000137463212
        kl: 0.009019268676638603
        model: {}
        policy_loss: -0.028424866497516632
        total_loss: -0.026834027841687202
        vf_explained_var: 0.04635171592235565
        vf_loss: 16.153141021728516
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00022366079792845994
        entropy: 0.8970774412155151
        entropy_coeff: 0.0017600000137463212
        kl: 0.0089657511562109
        model: {}
        policy_loss: -0.0270022414624691
        total_loss: -0.025362728163599968
        vf_explained_var: 0.15708503127098083
        vf_loss: 14.252201080322266
    load_time_ms: 14899.165
    num_steps_sampled: 16704000
    num_steps_trained: 16704000
    sample_time_ms: 90618.423
    update_time_ms: 22.98
  iterations_since_restore: 114
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.40689655172414
    ram_util_percent: 10.159195402298847
  pid: 24061
  policy_reward_max:
    agent-0: 159.16666666666669
    agent-1: 159.16666666666669
    agent-2: 159.16666666666669
    agent-3: 159.16666666666669
    agent-4: 159.16666666666669
    agent-5: 159.16666666666669
  policy_reward_mean:
    agent-0: 113.84666666666699
    agent-1: 113.84666666666699
    agent-2: 113.84666666666699
    agent-3: 113.84666666666699
    agent-4: 113.84666666666699
    agent-5: 113.84666666666699
  policy_reward_min:
    agent-0: 39.16666666666666
    agent-1: 39.16666666666666
    agent-2: 39.16666666666666
    agent-3: 39.16666666666666
    agent-4: 39.16666666666666
    agent-5: 39.16666666666666
  sampler_perf:
    mean_env_wait_ms: 24.47215474224389
    mean_inference_ms: 12.364441504697954
    mean_processing_ms: 51.22267049461361
  time_since_restore: 15117.383969783783
  time_this_iter_s: 121.66069769859314
  time_total_s: 24243.395783662796
  timestamp: 1637039140
  timesteps_since_restore: 10944000
  timesteps_this_iter: 96000
  timesteps_total: 16704000
  training_iteration: 174
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    174 |          24243.4 | 16704000 |   683.08 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 49
    apples_agent-0_mean: 3.77
    apples_agent-0_min: 0
    apples_agent-1_max: 128
    apples_agent-1_mean: 26.95
    apples_agent-1_min: 0
    apples_agent-2_max: 74
    apples_agent-2_mean: 5.55
    apples_agent-2_min: 0
    apples_agent-3_max: 195
    apples_agent-3_mean: 105.35
    apples_agent-3_min: 26
    apples_agent-4_max: 35
    apples_agent-4_mean: 1.76
    apples_agent-4_min: 0
    apples_agent-5_max: 144
    apples_agent-5_mean: 83.26
    apples_agent-5_min: 32
    cleaning_beam_agent-0_max: 470
    cleaning_beam_agent-0_mean: 310.78
    cleaning_beam_agent-0_min: 178
    cleaning_beam_agent-1_max: 563
    cleaning_beam_agent-1_mean: 254.46
    cleaning_beam_agent-1_min: 106
    cleaning_beam_agent-2_max: 489
    cleaning_beam_agent-2_mean: 298.92
    cleaning_beam_agent-2_min: 162
    cleaning_beam_agent-3_max: 161
    cleaning_beam_agent-3_mean: 46.4
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 461
    cleaning_beam_agent-4_mean: 334.46
    cleaning_beam_agent-4_min: 233
    cleaning_beam_agent-5_max: 204
    cleaning_beam_agent-5_mean: 83.87
    cleaning_beam_agent-5_min: 24
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-07-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 889.9999999999808
  episode_reward_mean: 684.7599999999923
  episode_reward_min: 263.99999999999545
  episodes_this_iter: 96
  episodes_total: 16800
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19769.754
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00021767040016129613
        entropy: 1.152445912361145
        entropy_coeff: 0.0017600000137463212
        kl: 0.008380023762583733
        model: {}
        policy_loss: -0.023500539362430573
        total_loss: -0.0223221592605114
        vf_explained_var: 0.03336445987224579
        vf_loss: 15.306800842285156
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00021767040016129613
        entropy: 1.1038568019866943
        entropy_coeff: 0.0017600000137463212
        kl: 0.008864371106028557
        model: {}
        policy_loss: -0.025751512497663498
        total_loss: -0.02442272938787937
        vf_explained_var: 0.0537741482257843
        vf_loss: 14.986959457397461
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00021767040016129613
        entropy: 1.0701696872711182
        entropy_coeff: 0.0017600000137463212
        kl: 0.00849307980388403
        model: {}
        policy_loss: -0.02466508559882641
        total_loss: -0.023392239585518837
        vf_explained_var: 0.07886715233325958
        vf_loss: 14.577280044555664
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00021767040016129613
        entropy: 0.7852473258972168
        entropy_coeff: 0.0017600000137463212
        kl: 0.00729535473510623
        model: {}
        policy_loss: -0.020410174503922462
        total_loss: -0.019027354195713997
        vf_explained_var: 0.17543555796146393
        vf_loss: 13.057859420776367
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00021767040016129613
        entropy: 1.0311534404754639
        entropy_coeff: 0.0017600000137463212
        kl: 0.009211746975779533
        model: {}
        policy_loss: -0.028382131829857826
        total_loss: -0.02681881934404373
        vf_explained_var: 0.030678212642669678
        vf_loss: 15.357928276062012
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00021767040016129613
        entropy: 0.9041752815246582
        entropy_coeff: 0.0017600000137463212
        kl: 0.009124936535954475
        model: {}
        policy_loss: -0.026904473081231117
        total_loss: -0.02532794699072838
        vf_explained_var: 0.1523742973804474
        vf_loss: 13.428862571716309
    load_time_ms: 14902.774
    num_steps_sampled: 16800000
    num_steps_trained: 16800000
    sample_time_ms: 90583.533
    update_time_ms: 23.352
  iterations_since_restore: 115
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.318857142857144
    ram_util_percent: 10.143999999999998
  pid: 24061
  policy_reward_max:
    agent-0: 148.33333333333366
    agent-1: 148.33333333333366
    agent-2: 148.33333333333366
    agent-3: 148.33333333333366
    agent-4: 148.33333333333366
    agent-5: 148.33333333333366
  policy_reward_mean:
    agent-0: 114.12666666666705
    agent-1: 114.12666666666705
    agent-2: 114.12666666666705
    agent-3: 114.12666666666705
    agent-4: 114.12666666666705
    agent-5: 114.12666666666705
  policy_reward_min:
    agent-0: 43.99999999999997
    agent-1: 43.99999999999997
    agent-2: 43.99999999999997
    agent-3: 43.99999999999997
    agent-4: 43.99999999999997
    agent-5: 43.99999999999997
  sampler_perf:
    mean_env_wait_ms: 24.466017230714478
    mean_inference_ms: 12.361754607264656
    mean_processing_ms: 51.213682420522474
  time_since_restore: 15240.275500059128
  time_this_iter_s: 122.89153027534485
  time_total_s: 24366.28731393814
  timestamp: 1637039263
  timesteps_since_restore: 11040000
  timesteps_this_iter: 96000
  timesteps_total: 16800000
  training_iteration: 175
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    175 |          24366.3 | 16800000 |   684.76 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 85
    apples_agent-0_mean: 6.16
    apples_agent-0_min: 0
    apples_agent-1_max: 106
    apples_agent-1_mean: 26.96
    apples_agent-1_min: 0
    apples_agent-2_max: 69
    apples_agent-2_mean: 4.97
    apples_agent-2_min: 0
    apples_agent-3_max: 173
    apples_agent-3_mean: 101.86
    apples_agent-3_min: 26
    apples_agent-4_max: 85
    apples_agent-4_mean: 2.86
    apples_agent-4_min: 0
    apples_agent-5_max: 150
    apples_agent-5_mean: 86.6
    apples_agent-5_min: 13
    cleaning_beam_agent-0_max: 477
    cleaning_beam_agent-0_mean: 306.36
    cleaning_beam_agent-0_min: 167
    cleaning_beam_agent-1_max: 500
    cleaning_beam_agent-1_mean: 252.7
    cleaning_beam_agent-1_min: 116
    cleaning_beam_agent-2_max: 476
    cleaning_beam_agent-2_mean: 310.17
    cleaning_beam_agent-2_min: 82
    cleaning_beam_agent-3_max: 132
    cleaning_beam_agent-3_mean: 47.9
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 457
    cleaning_beam_agent-4_mean: 331.83
    cleaning_beam_agent-4_min: 215
    cleaning_beam_agent-5_max: 339
    cleaning_beam_agent-5_mean: 79.89
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-09-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 917.9999999999654
  episode_reward_mean: 693.9599999999928
  episode_reward_min: 374.0000000000045
  episodes_this_iter: 96
  episodes_total: 16896
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19712.556
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00021168000239413232
        entropy: 1.141047477722168
        entropy_coeff: 0.0017600000137463212
        kl: 0.007869349792599678
        model: {}
        policy_loss: -0.022894080728292465
        total_loss: -0.02170603908598423
        vf_explained_var: 0.03265887498855591
        vf_loss: 16.22414779663086
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00021168000239413232
        entropy: 1.1015901565551758
        entropy_coeff: 0.0017600000137463212
        kl: 0.008690308779478073
        model: {}
        policy_loss: -0.025262245908379555
        total_loss: -0.023784978315234184
        vf_explained_var: -0.001406356692314148
        vf_loss: 16.78003692626953
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00021168000239413232
        entropy: 1.0553826093673706
        entropy_coeff: 0.0017600000137463212
        kl: 0.008605355396866798
        model: {}
        policy_loss: -0.024325057864189148
        total_loss: -0.022880438715219498
        vf_explained_var: 0.05634358525276184
        vf_loss: 15.810263633728027
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00021168000239413232
        entropy: 0.7773813009262085
        entropy_coeff: 0.0017600000137463212
        kl: 0.006928637623786926
        model: {}
        policy_loss: -0.020748842507600784
        total_loss: -0.01930985599756241
        vf_explained_var: 0.15205782651901245
        vf_loss: 14.214485168457031
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00021168000239413232
        entropy: 1.0388118028640747
        entropy_coeff: 0.0017600000137463212
        kl: 0.009088288992643356
        model: {}
        policy_loss: -0.028374813497066498
        total_loss: -0.0268025491386652
        vf_explained_var: 0.05599556863307953
        vf_loss: 15.829132080078125
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00021168000239413232
        entropy: 0.8940572142601013
        entropy_coeff: 0.0017600000137463212
        kl: 0.008853934705257416
        model: {}
        policy_loss: -0.026331929489970207
        total_loss: -0.024781689047813416
        vf_explained_var: 0.19229756295681
        vf_loss: 13.529962539672852
    load_time_ms: 14877.134
    num_steps_sampled: 16896000
    num_steps_trained: 16896000
    sample_time_ms: 90447.739
    update_time_ms: 23.446
  iterations_since_restore: 116
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.406358381502889
    ram_util_percent: 10.084971098265894
  pid: 24061
  policy_reward_max:
    agent-0: 152.99999999999983
    agent-1: 152.99999999999983
    agent-2: 152.99999999999983
    agent-3: 152.99999999999983
    agent-4: 152.99999999999983
    agent-5: 152.99999999999983
  policy_reward_mean:
    agent-0: 115.66000000000038
    agent-1: 115.66000000000038
    agent-2: 115.66000000000038
    agent-3: 115.66000000000038
    agent-4: 115.66000000000038
    agent-5: 115.66000000000038
  policy_reward_min:
    agent-0: 62.33333333333311
    agent-1: 62.33333333333311
    agent-2: 62.33333333333311
    agent-3: 62.33333333333311
    agent-4: 62.33333333333311
    agent-5: 62.33333333333311
  sampler_perf:
    mean_env_wait_ms: 24.459877879294673
    mean_inference_ms: 12.359309015315562
    mean_processing_ms: 51.20465813732741
  time_since_restore: 15361.953632831573
  time_this_iter_s: 121.67813277244568
  time_total_s: 24487.965446710587
  timestamp: 1637039385
  timesteps_since_restore: 11136000
  timesteps_this_iter: 96000
  timesteps_total: 16896000
  training_iteration: 176
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    176 |            24488 | 16896000 |   693.96 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 43
    apples_agent-0_mean: 3.12
    apples_agent-0_min: 0
    apples_agent-1_max: 106
    apples_agent-1_mean: 27.92
    apples_agent-1_min: 0
    apples_agent-2_max: 111
    apples_agent-2_mean: 4.66
    apples_agent-2_min: 0
    apples_agent-3_max: 178
    apples_agent-3_mean: 101.67
    apples_agent-3_min: 43
    apples_agent-4_max: 38
    apples_agent-4_mean: 1.49
    apples_agent-4_min: 0
    apples_agent-5_max: 153
    apples_agent-5_mean: 86.45
    apples_agent-5_min: 26
    cleaning_beam_agent-0_max: 427
    cleaning_beam_agent-0_mean: 308.65
    cleaning_beam_agent-0_min: 170
    cleaning_beam_agent-1_max: 458
    cleaning_beam_agent-1_mean: 253.75
    cleaning_beam_agent-1_min: 105
    cleaning_beam_agent-2_max: 421
    cleaning_beam_agent-2_mean: 307.91
    cleaning_beam_agent-2_min: 168
    cleaning_beam_agent-3_max: 144
    cleaning_beam_agent-3_mean: 53.39
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 434
    cleaning_beam_agent-4_mean: 328.02
    cleaning_beam_agent-4_min: 221
    cleaning_beam_agent-5_max: 360
    cleaning_beam_agent-5_mean: 70.66
    cleaning_beam_agent-5_min: 15
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-11-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 937.9999999999919
  episode_reward_mean: 694.8299999999924
  episode_reward_min: 304.0000000000029
  episodes_this_iter: 96
  episodes_total: 16992
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19646.464
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002056896046269685
        entropy: 1.1404693126678467
        entropy_coeff: 0.0017600000137463212
        kl: 0.008230975829064846
        model: {}
        policy_loss: -0.02331743761897087
        total_loss: -0.022077718749642372
        vf_explained_var: 0.013236522674560547
        vf_loss: 16.007497787475586
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002056896046269685
        entropy: 1.1022875308990479
        entropy_coeff: 0.0017600000137463212
        kl: 0.008399948477745056
        model: {}
        policy_loss: -0.02423674799501896
        total_loss: -0.022893086075782776
        vf_explained_var: 0.01207558810710907
        vf_loss: 16.037012100219727
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002056896046269685
        entropy: 1.0695797204971313
        entropy_coeff: 0.0017600000137463212
        kl: 0.008769444189965725
        model: {}
        policy_loss: -0.023961033672094345
        total_loss: -0.02255208045244217
        vf_explained_var: 0.05298185348510742
        vf_loss: 15.37519645690918
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002056896046269685
        entropy: 0.7981192469596863
        entropy_coeff: 0.0017600000137463212
        kl: 0.007142964284867048
        model: {}
        policy_loss: -0.018815098330378532
        total_loss: -0.017407327890396118
        vf_explained_var: 0.14702479541301727
        vf_loss: 13.838650703430176
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002056896046269685
        entropy: 1.0417155027389526
        entropy_coeff: 0.0017600000137463212
        kl: 0.009044026024639606
        model: {}
        policy_loss: -0.02697448432445526
        total_loss: -0.02541428431868553
        vf_explained_var: 0.02391780912876129
        vf_loss: 15.848154067993164
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002056896046269685
        entropy: 0.8584146499633789
        entropy_coeff: 0.0017600000137463212
        kl: 0.008851416409015656
        model: {}
        policy_loss: -0.026112375780940056
        total_loss: -0.024514641612768173
        vf_explained_var: 0.17507010698318481
        vf_loss: 13.382619857788086
    load_time_ms: 14281.927
    num_steps_sampled: 16992000
    num_steps_trained: 16992000
    sample_time_ms: 90206.748
    update_time_ms: 23.776
  iterations_since_restore: 117
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.459883720930234
    ram_util_percent: 10.144767441860466
  pid: 24061
  policy_reward_max:
    agent-0: 156.33333333333343
    agent-1: 156.33333333333343
    agent-2: 156.33333333333343
    agent-3: 156.33333333333343
    agent-4: 156.33333333333343
    agent-5: 156.33333333333343
  policy_reward_mean:
    agent-0: 115.80500000000036
    agent-1: 115.80500000000036
    agent-2: 115.80500000000036
    agent-3: 115.80500000000036
    agent-4: 115.80500000000036
    agent-5: 115.80500000000036
  policy_reward_min:
    agent-0: 50.666666666666565
    agent-1: 50.666666666666565
    agent-2: 50.666666666666565
    agent-3: 50.666666666666565
    agent-4: 50.666666666666565
    agent-5: 50.666666666666565
  sampler_perf:
    mean_env_wait_ms: 24.45329381900713
    mean_inference_ms: 12.357039174640816
    mean_processing_ms: 51.19290896626054
  time_since_restore: 15483.151148080826
  time_this_iter_s: 121.19751524925232
  time_total_s: 24609.16296195984
  timestamp: 1637039506
  timesteps_since_restore: 11232000
  timesteps_this_iter: 96000
  timesteps_total: 16992000
  training_iteration: 177
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    177 |          24609.2 | 16992000 |   694.83 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 32
    apples_agent-0_mean: 3.77
    apples_agent-0_min: 0
    apples_agent-1_max: 79
    apples_agent-1_mean: 31.6
    apples_agent-1_min: 0
    apples_agent-2_max: 81
    apples_agent-2_mean: 6.85
    apples_agent-2_min: 0
    apples_agent-3_max: 162
    apples_agent-3_mean: 99.4
    apples_agent-3_min: 43
    apples_agent-4_max: 44
    apples_agent-4_mean: 2.13
    apples_agent-4_min: 0
    apples_agent-5_max: 165
    apples_agent-5_mean: 85.71
    apples_agent-5_min: 30
    cleaning_beam_agent-0_max: 428
    cleaning_beam_agent-0_mean: 302.71
    cleaning_beam_agent-0_min: 191
    cleaning_beam_agent-1_max: 512
    cleaning_beam_agent-1_mean: 242.3
    cleaning_beam_agent-1_min: 99
    cleaning_beam_agent-2_max: 515
    cleaning_beam_agent-2_mean: 307.43
    cleaning_beam_agent-2_min: 153
    cleaning_beam_agent-3_max: 122
    cleaning_beam_agent-3_mean: 57.24
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 417
    cleaning_beam_agent-4_mean: 319.2
    cleaning_beam_agent-4_min: 208
    cleaning_beam_agent-5_max: 287
    cleaning_beam_agent-5_mean: 73.54
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-13-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 934.9999999999775
  episode_reward_mean: 673.5399999999936
  episode_reward_min: 304.0000000000029
  episodes_this_iter: 96
  episodes_total: 17088
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19616.369
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001996992068598047
        entropy: 1.1575995683670044
        entropy_coeff: 0.0017600000137463212
        kl: 0.008482726290822029
        model: {}
        policy_loss: -0.023448288440704346
        total_loss: -0.022353682667016983
        vf_explained_var: 0.02476583421230316
        vf_loss: 14.354354858398438
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001996992068598047
        entropy: 1.1012431383132935
        entropy_coeff: 0.0017600000137463212
        kl: 0.00835123285651207
        model: {}
        policy_loss: -0.02405543252825737
        total_loss: -0.022884272038936615
        vf_explained_var: 0.02193915843963623
        vf_loss: 14.391075134277344
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001996992068598047
        entropy: 1.0580847263336182
        entropy_coeff: 0.0017600000137463212
        kl: 0.009123045951128006
        model: {}
        policy_loss: -0.02402021922171116
        total_loss: -0.02260827273130417
        vf_explained_var: 0.014683246612548828
        vf_loss: 14.495705604553223
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001996992068598047
        entropy: 0.8228980302810669
        entropy_coeff: 0.0017600000137463212
        kl: 0.0067437272518873215
        model: {}
        policy_loss: -0.019439371302723885
        total_loss: -0.018249845132231712
        vf_explained_var: 0.12452597916126251
        vf_loss: 12.890840530395508
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001996992068598047
        entropy: 1.0397695302963257
        entropy_coeff: 0.0017600000137463212
        kl: 0.008595868945121765
        model: {}
        policy_loss: -0.026843871921300888
        total_loss: -0.02555386908352375
        vf_explained_var: 0.04834634065628052
        vf_loss: 14.00828742980957
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001996992068598047
        entropy: 0.8889428377151489
        entropy_coeff: 0.0017600000137463212
        kl: 0.008266067132353783
        model: {}
        policy_loss: -0.02535771019756794
        total_loss: -0.023934882134199142
        vf_explained_var: 0.09416505694389343
        vf_loss: 13.341529846191406
    load_time_ms: 14155.096
    num_steps_sampled: 17088000
    num_steps_trained: 17088000
    sample_time_ms: 90084.817
    update_time_ms: 22.139
  iterations_since_restore: 118
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.165168539325844
    ram_util_percent: 10.129213483146067
  pid: 24061
  policy_reward_max:
    agent-0: 155.83333333333331
    agent-1: 155.83333333333331
    agent-2: 155.83333333333331
    agent-3: 155.83333333333331
    agent-4: 155.83333333333331
    agent-5: 155.83333333333331
  policy_reward_mean:
    agent-0: 112.25666666666703
    agent-1: 112.25666666666703
    agent-2: 112.25666666666703
    agent-3: 112.25666666666703
    agent-4: 112.25666666666703
    agent-5: 112.25666666666703
  policy_reward_min:
    agent-0: 50.666666666666565
    agent-1: 50.666666666666565
    agent-2: 50.666666666666565
    agent-3: 50.666666666666565
    agent-4: 50.666666666666565
    agent-5: 50.666666666666565
  sampler_perf:
    mean_env_wait_ms: 24.4466545673012
    mean_inference_ms: 12.355233190717845
    mean_processing_ms: 51.18363304914329
  time_since_restore: 15607.64966917038
  time_this_iter_s: 124.49852108955383
  time_total_s: 24733.661483049393
  timestamp: 1637039631
  timesteps_since_restore: 11328000
  timesteps_this_iter: 96000
  timesteps_total: 17088000
  training_iteration: 178
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    178 |          24733.7 | 17088000 |   673.54 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 278
    apples_agent-0_mean: 6.5
    apples_agent-0_min: 0
    apples_agent-1_max: 70
    apples_agent-1_mean: 24.29
    apples_agent-1_min: 0
    apples_agent-2_max: 59
    apples_agent-2_mean: 6.6
    apples_agent-2_min: 0
    apples_agent-3_max: 164
    apples_agent-3_mean: 102.01
    apples_agent-3_min: 45
    apples_agent-4_max: 79
    apples_agent-4_mean: 3.01
    apples_agent-4_min: 0
    apples_agent-5_max: 162
    apples_agent-5_mean: 84.36
    apples_agent-5_min: 36
    cleaning_beam_agent-0_max: 404
    cleaning_beam_agent-0_mean: 303.41
    cleaning_beam_agent-0_min: 146
    cleaning_beam_agent-1_max: 477
    cleaning_beam_agent-1_mean: 237.86
    cleaning_beam_agent-1_min: 76
    cleaning_beam_agent-2_max: 455
    cleaning_beam_agent-2_mean: 309.24
    cleaning_beam_agent-2_min: 139
    cleaning_beam_agent-3_max: 135
    cleaning_beam_agent-3_mean: 50.67
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 426
    cleaning_beam_agent-4_mean: 323.84
    cleaning_beam_agent-4_min: 133
    cleaning_beam_agent-5_max: 158
    cleaning_beam_agent-5_mean: 60.83
    cleaning_beam_agent-5_min: 12
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-15-52
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 941.9999999999923
  episode_reward_mean: 705.9899999999924
  episode_reward_min: 382.00000000000256
  episodes_this_iter: 96
  episodes_total: 17184
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19610.951
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00019370879454072565
        entropy: 1.13258957862854
        entropy_coeff: 0.0017600000137463212
        kl: 0.007400034926831722
        model: {}
        policy_loss: -0.02239927276968956
        total_loss: -0.021210886538028717
        vf_explained_var: 0.03470563888549805
        vf_loss: 17.017364501953125
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00019370879454072565
        entropy: 1.1090396642684937
        entropy_coeff: 0.0017600000137463212
        kl: 0.008282744325697422
        model: {}
        policy_loss: -0.02391696721315384
        total_loss: -0.02250686101615429
        vf_explained_var: 0.03242482244968414
        vf_loss: 17.05464744567871
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00019370879454072565
        entropy: 1.0588388442993164
        entropy_coeff: 0.0017600000137463212
        kl: 0.008543708361685276
        model: {}
        policy_loss: -0.023616157472133636
        total_loss: -0.022127404808998108
        vf_explained_var: 0.06853446364402771
        vf_loss: 16.435665130615234
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00019370879454072565
        entropy: 0.7935937643051147
        entropy_coeff: 0.0017600000137463212
        kl: 0.006653424818068743
        model: {}
        policy_loss: -0.01957058161497116
        total_loss: -0.018195929005742073
        vf_explained_var: 0.18244147300720215
        vf_loss: 14.406933784484863
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00019370879454072565
        entropy: 1.032468318939209
        entropy_coeff: 0.0017600000137463212
        kl: 0.008505970239639282
        model: {}
        policy_loss: -0.026077069342136383
        total_loss: -0.024565942585468292
        vf_explained_var: 0.07829399406909943
        vf_loss: 16.270811080932617
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00019370879454072565
        entropy: 0.862758219242096
        entropy_coeff: 0.0017600000137463212
        kl: 0.008067401126027107
        model: {}
        policy_loss: -0.024857306852936745
        total_loss: -0.023295000195503235
        vf_explained_var: 0.1681973785161972
        vf_loss: 14.672880172729492
    load_time_ms: 13701.842
    num_steps_sampled: 17184000
    num_steps_trained: 17184000
    sample_time_ms: 89794.782
    update_time_ms: 21.219
  iterations_since_restore: 119
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.3867816091954
    ram_util_percent: 10.182758620689658
  pid: 24061
  policy_reward_max:
    agent-0: 157.00000000000014
    agent-1: 157.00000000000014
    agent-2: 157.00000000000014
    agent-3: 157.00000000000014
    agent-4: 157.00000000000014
    agent-5: 157.00000000000014
  policy_reward_mean:
    agent-0: 117.66500000000033
    agent-1: 117.66500000000033
    agent-2: 117.66500000000033
    agent-3: 117.66500000000033
    agent-4: 117.66500000000033
    agent-5: 117.66500000000033
  policy_reward_min:
    agent-0: 63.666666666666465
    agent-1: 63.666666666666465
    agent-2: 63.666666666666465
    agent-3: 63.666666666666465
    agent-4: 63.666666666666465
    agent-5: 63.666666666666465
  sampler_perf:
    mean_env_wait_ms: 24.440759389814815
    mean_inference_ms: 12.353210446881493
    mean_processing_ms: 51.17558750234311
  time_since_restore: 15729.27683210373
  time_this_iter_s: 121.62716293334961
  time_total_s: 24855.288645982742
  timestamp: 1637039752
  timesteps_since_restore: 11424000
  timesteps_this_iter: 96000
  timesteps_total: 17184000
  training_iteration: 179
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    179 |          24855.3 | 17184000 |   705.99 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 82
    apples_agent-0_mean: 5.29
    apples_agent-0_min: 0
    apples_agent-1_max: 106
    apples_agent-1_mean: 32.87
    apples_agent-1_min: 0
    apples_agent-2_max: 79
    apples_agent-2_mean: 6.03
    apples_agent-2_min: 0
    apples_agent-3_max: 149
    apples_agent-3_mean: 96.46
    apples_agent-3_min: 41
    apples_agent-4_max: 42
    apples_agent-4_mean: 1.39
    apples_agent-4_min: 0
    apples_agent-5_max: 130
    apples_agent-5_mean: 86.38
    apples_agent-5_min: 36
    cleaning_beam_agent-0_max: 470
    cleaning_beam_agent-0_mean: 300.58
    cleaning_beam_agent-0_min: 167
    cleaning_beam_agent-1_max: 446
    cleaning_beam_agent-1_mean: 236.94
    cleaning_beam_agent-1_min: 112
    cleaning_beam_agent-2_max: 509
    cleaning_beam_agent-2_mean: 300.41
    cleaning_beam_agent-2_min: 137
    cleaning_beam_agent-3_max: 131
    cleaning_beam_agent-3_mean: 53.38
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 426
    cleaning_beam_agent-4_mean: 323.83
    cleaning_beam_agent-4_min: 208
    cleaning_beam_agent-5_max: 244
    cleaning_beam_agent-5_mean: 73.09
    cleaning_beam_agent-5_min: 12
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-17-54
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 902.999999999981
  episode_reward_mean: 685.909999999993
  episode_reward_min: 395.00000000000966
  episodes_this_iter: 96
  episodes_total: 17280
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19594.296
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00018771839677356184
        entropy: 1.1545706987380981
        entropy_coeff: 0.0017600000137463212
        kl: 0.00816424936056137
        model: {}
        policy_loss: -0.023069357499480247
        total_loss: -0.021920904517173767
        vf_explained_var: 0.04718451201915741
        vf_loss: 15.476478576660156
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00018771839677356184
        entropy: 1.0973222255706787
        entropy_coeff: 0.0017600000137463212
        kl: 0.007745538372546434
        model: {}
        policy_loss: -0.02343629114329815
        total_loss: -0.02217596024274826
        vf_explained_var: -0.011233478784561157
        vf_loss: 16.425119400024414
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00018771839677356184
        entropy: 1.0625883340835571
        entropy_coeff: 0.0017600000137463212
        kl: 0.008314118720591068
        model: {}
        policy_loss: -0.024001004174351692
        total_loss: -0.02266319841146469
        vf_explained_var: 0.04872588813304901
        vf_loss: 15.451380729675293
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00018771839677356184
        entropy: 0.7965168952941895
        entropy_coeff: 0.0017600000137463212
        kl: 0.006733222398906946
        model: {}
        policy_loss: -0.019618581980466843
        total_loss: -0.018308542668819427
        vf_explained_var: 0.15929144620895386
        vf_loss: 13.6526460647583
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00018771839677356184
        entropy: 1.036373496055603
        entropy_coeff: 0.0017600000137463212
        kl: 0.008231479674577713
        model: {}
        policy_loss: -0.025601517409086227
        total_loss: -0.024247441440820694
        vf_explained_var: 0.05625124275684357
        vf_loss: 15.317962646484375
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00018771839677356184
        entropy: 0.890559196472168
        entropy_coeff: 0.0017600000137463212
        kl: 0.008142855018377304
        model: {}
        policy_loss: -0.02451196312904358
        total_loss: -0.023016829043626785
        vf_explained_var: 0.11703316867351532
        vf_loss: 14.339482307434082
    load_time_ms: 13328.394
    num_steps_sampled: 17280000
    num_steps_trained: 17280000
    sample_time_ms: 89404.03
    update_time_ms: 20.923
  iterations_since_restore: 120
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.476744186046512
    ram_util_percent: 10.075581395348838
  pid: 24061
  policy_reward_max:
    agent-0: 150.49999999999994
    agent-1: 150.49999999999994
    agent-2: 150.49999999999994
    agent-3: 150.49999999999994
    agent-4: 150.49999999999994
    agent-5: 150.49999999999994
  policy_reward_mean:
    agent-0: 114.31833333333367
    agent-1: 114.31833333333367
    agent-2: 114.31833333333367
    agent-3: 114.31833333333367
    agent-4: 114.31833333333367
    agent-5: 114.31833333333367
  policy_reward_min:
    agent-0: 65.83333333333313
    agent-1: 65.83333333333313
    agent-2: 65.83333333333313
    agent-3: 65.83333333333313
    agent-4: 65.83333333333313
    agent-5: 65.83333333333313
  sampler_perf:
    mean_env_wait_ms: 24.434495455179732
    mean_inference_ms: 12.350980807503511
    mean_processing_ms: 51.16697460103702
  time_since_restore: 15850.566705226898
  time_this_iter_s: 121.28987312316895
  time_total_s: 24976.57851910591
  timestamp: 1637039874
  timesteps_since_restore: 11520000
  timesteps_this_iter: 96000
  timesteps_total: 17280000
  training_iteration: 180
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    180 |          24976.6 | 17280000 |   685.91 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 66
    apples_agent-0_mean: 4.47
    apples_agent-0_min: 0
    apples_agent-1_max: 83
    apples_agent-1_mean: 25.33
    apples_agent-1_min: 0
    apples_agent-2_max: 57
    apples_agent-2_mean: 4.47
    apples_agent-2_min: 0
    apples_agent-3_max: 176
    apples_agent-3_mean: 99.32
    apples_agent-3_min: 41
    apples_agent-4_max: 30
    apples_agent-4_mean: 1.6
    apples_agent-4_min: 0
    apples_agent-5_max: 150
    apples_agent-5_mean: 84.55
    apples_agent-5_min: 34
    cleaning_beam_agent-0_max: 427
    cleaning_beam_agent-0_mean: 290.77
    cleaning_beam_agent-0_min: 151
    cleaning_beam_agent-1_max: 500
    cleaning_beam_agent-1_mean: 255.26
    cleaning_beam_agent-1_min: 96
    cleaning_beam_agent-2_max: 502
    cleaning_beam_agent-2_mean: 310.25
    cleaning_beam_agent-2_min: 94
    cleaning_beam_agent-3_max: 160
    cleaning_beam_agent-3_mean: 54.4
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 412
    cleaning_beam_agent-4_mean: 315.18
    cleaning_beam_agent-4_min: 174
    cleaning_beam_agent-5_max: 204
    cleaning_beam_agent-5_mean: 60.69
    cleaning_beam_agent-5_min: 13
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-19-56
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 890.9999999999792
  episode_reward_mean: 682.6899999999933
  episode_reward_min: 405.0000000000042
  episodes_this_iter: 96
  episodes_total: 17376
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19622.354
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00018172799900639802
        entropy: 1.1384336948394775
        entropy_coeff: 0.0017600000137463212
        kl: 0.007598372176289558
        model: {}
        policy_loss: -0.022152401506900787
        total_loss: -0.02111726626753807
        vf_explained_var: 0.027714058756828308
        vf_loss: 15.191048622131348
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00018172799900639802
        entropy: 1.1089409589767456
        entropy_coeff: 0.0017600000137463212
        kl: 0.008241193369030952
        model: {}
        policy_loss: -0.023641742765903473
        total_loss: -0.02241472527384758
        vf_explained_var: 0.020548492670059204
        vf_loss: 15.305143356323242
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00018172799900639802
        entropy: 1.062798261642456
        entropy_coeff: 0.0017600000137463212
        kl: 0.007733787409961224
        model: {}
        policy_loss: -0.02240006998181343
        total_loss: -0.021265942603349686
        vf_explained_var: 0.06633388996124268
        vf_loss: 14.57890510559082
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00018172799900639802
        entropy: 0.8024933338165283
        entropy_coeff: 0.0017600000137463212
        kl: 0.0064896224066615105
        model: {}
        policy_loss: -0.01889640837907791
        total_loss: -0.01765662431716919
        vf_explained_var: 0.13322296738624573
        vf_loss: 13.542466163635254
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00018172799900639802
        entropy: 1.0317180156707764
        entropy_coeff: 0.0017600000137463212
        kl: 0.00811596680432558
        model: {}
        policy_loss: -0.025775529444217682
        total_loss: -0.024505864828824997
        vf_explained_var: 0.06465277075767517
        vf_loss: 14.622916221618652
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00018172799900639802
        entropy: 0.8701291084289551
        entropy_coeff: 0.0017600000137463212
        kl: 0.007499761879444122
        model: {}
        policy_loss: -0.02380296029150486
        total_loss: -0.02250353991985321
        vf_explained_var: 0.14805342257022858
        vf_loss: 13.308956146240234
    load_time_ms: 13341.471
    num_steps_sampled: 17376000
    num_steps_trained: 17376000
    sample_time_ms: 89295.355
    update_time_ms: 21.007
  iterations_since_restore: 121
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.37471264367816
    ram_util_percent: 10.140229885057472
  pid: 24061
  policy_reward_max:
    agent-0: 148.5000000000003
    agent-1: 148.5000000000003
    agent-2: 148.5000000000003
    agent-3: 148.5000000000003
    agent-4: 148.5000000000003
    agent-5: 148.5000000000003
  policy_reward_mean:
    agent-0: 113.78166666666702
    agent-1: 113.78166666666702
    agent-2: 113.78166666666702
    agent-3: 113.78166666666702
    agent-4: 113.78166666666702
    agent-5: 113.78166666666702
  policy_reward_min:
    agent-0: 67.49999999999977
    agent-1: 67.49999999999977
    agent-2: 67.49999999999977
    agent-3: 67.49999999999977
    agent-4: 67.49999999999977
    agent-5: 67.49999999999977
  sampler_perf:
    mean_env_wait_ms: 24.427975686626315
    mean_inference_ms: 12.348682659544439
    mean_processing_ms: 51.157682207238956
  time_since_restore: 15972.013162374496
  time_this_iter_s: 121.44645714759827
  time_total_s: 25098.02497625351
  timestamp: 1637039996
  timesteps_since_restore: 11616000
  timesteps_this_iter: 96000
  timesteps_total: 17376000
  training_iteration: 181
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    181 |            25098 | 17376000 |   682.69 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 61
    apples_agent-0_mean: 5.89
    apples_agent-0_min: 0
    apples_agent-1_max: 81
    apples_agent-1_mean: 26.37
    apples_agent-1_min: 0
    apples_agent-2_max: 76
    apples_agent-2_mean: 6.7
    apples_agent-2_min: 0
    apples_agent-3_max: 153
    apples_agent-3_mean: 97.49
    apples_agent-3_min: 51
    apples_agent-4_max: 26
    apples_agent-4_mean: 1.15
    apples_agent-4_min: 0
    apples_agent-5_max: 145
    apples_agent-5_mean: 87.68
    apples_agent-5_min: 9
    cleaning_beam_agent-0_max: 478
    cleaning_beam_agent-0_mean: 294.11
    cleaning_beam_agent-0_min: 158
    cleaning_beam_agent-1_max: 469
    cleaning_beam_agent-1_mean: 265.78
    cleaning_beam_agent-1_min: 106
    cleaning_beam_agent-2_max: 483
    cleaning_beam_agent-2_mean: 304.95
    cleaning_beam_agent-2_min: 173
    cleaning_beam_agent-3_max: 171
    cleaning_beam_agent-3_mean: 55.21
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 412
    cleaning_beam_agent-4_mean: 318.91
    cleaning_beam_agent-4_min: 218
    cleaning_beam_agent-5_max: 316
    cleaning_beam_agent-5_mean: 58.73
    cleaning_beam_agent-5_min: 16
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-21-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 900.9999999999853
  episode_reward_mean: 692.7699999999928
  episode_reward_min: 316.00000000000125
  episodes_this_iter: 96
  episodes_total: 17472
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19622.102
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001757376012392342
        entropy: 1.1397970914840698
        entropy_coeff: 0.0017600000137463212
        kl: 0.007413147017359734
        model: {}
        policy_loss: -0.022323764860630035
        total_loss: -0.021357960999011993
        vf_explained_var: 0.0564005970954895
        vf_loss: 14.892192840576172
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001757376012392342
        entropy: 1.109769582748413
        entropy_coeff: 0.0017600000137463212
        kl: 0.007861616089940071
        model: {}
        policy_loss: -0.02253277599811554
        total_loss: -0.021356990560889244
        vf_explained_var: 0.012888744473457336
        vf_loss: 15.566587448120117
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001757376012392342
        entropy: 1.058638572692871
        entropy_coeff: 0.0017600000137463212
        kl: 0.007227200549095869
        model: {}
        policy_loss: -0.02177618443965912
        total_loss: -0.02075061947107315
        vf_explained_var: 0.08565478026866913
        vf_loss: 14.433300018310547
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001757376012392342
        entropy: 0.7963770627975464
        entropy_coeff: 0.0017600000137463212
        kl: 0.006426707841455936
        model: {}
        policy_loss: -0.019007405266165733
        total_loss: -0.01777602918446064
        vf_explained_var: 0.14492662250995636
        vf_loss: 13.476578712463379
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001757376012392342
        entropy: 1.0336791276931763
        entropy_coeff: 0.0017600000137463212
        kl: 0.00822580698877573
        model: {}
        policy_loss: -0.025331486016511917
        total_loss: -0.02398703247308731
        vf_explained_var: 0.03799481689929962
        vf_loss: 15.185678482055664
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001757376012392342
        entropy: 0.8630262613296509
        entropy_coeff: 0.0017600000137463212
        kl: 0.007362930569797754
        model: {}
        policy_loss: -0.023127369582653046
        total_loss: -0.021832257509231567
        vf_explained_var: 0.14888758957386017
        vf_loss: 13.414531707763672
    load_time_ms: 13103.604
    num_steps_sampled: 17472000
    num_steps_trained: 17472000
    sample_time_ms: 89309.645
    update_time_ms: 20.929
  iterations_since_restore: 122
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.386931818181818
    ram_util_percent: 10.156818181818181
  pid: 24061
  policy_reward_max:
    agent-0: 150.1666666666668
    agent-1: 150.1666666666668
    agent-2: 150.1666666666668
    agent-3: 150.1666666666668
    agent-4: 150.1666666666668
    agent-5: 150.1666666666668
  policy_reward_mean:
    agent-0: 115.461666666667
    agent-1: 115.461666666667
    agent-2: 115.461666666667
    agent-3: 115.461666666667
    agent-4: 115.461666666667
    agent-5: 115.461666666667
  policy_reward_min:
    agent-0: 52.66666666666648
    agent-1: 52.66666666666648
    agent-2: 52.66666666666648
    agent-3: 52.66666666666648
    agent-4: 52.66666666666648
    agent-5: 52.66666666666648
  sampler_perf:
    mean_env_wait_ms: 24.423074276397074
    mean_inference_ms: 12.346950219146336
    mean_processing_ms: 51.152517616939306
  time_since_restore: 16095.165011405945
  time_this_iter_s: 123.15184903144836
  time_total_s: 25221.176825284958
  timestamp: 1637040119
  timesteps_since_restore: 11712000
  timesteps_this_iter: 96000
  timesteps_total: 17472000
  training_iteration: 182
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    182 |          25221.2 | 17472000 |   692.77 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 107
    apples_agent-0_mean: 6.04
    apples_agent-0_min: 0
    apples_agent-1_max: 138
    apples_agent-1_mean: 28.23
    apples_agent-1_min: 0
    apples_agent-2_max: 44
    apples_agent-2_mean: 3.55
    apples_agent-2_min: 0
    apples_agent-3_max: 143
    apples_agent-3_mean: 96.6
    apples_agent-3_min: 33
    apples_agent-4_max: 38
    apples_agent-4_mean: 2.48
    apples_agent-4_min: 0
    apples_agent-5_max: 149
    apples_agent-5_mean: 88.35
    apples_agent-5_min: 25
    cleaning_beam_agent-0_max: 419
    cleaning_beam_agent-0_mean: 286.74
    cleaning_beam_agent-0_min: 158
    cleaning_beam_agent-1_max: 391
    cleaning_beam_agent-1_mean: 249.14
    cleaning_beam_agent-1_min: 104
    cleaning_beam_agent-2_max: 471
    cleaning_beam_agent-2_mean: 318.24
    cleaning_beam_agent-2_min: 178
    cleaning_beam_agent-3_max: 149
    cleaning_beam_agent-3_mean: 52.26
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 404
    cleaning_beam_agent-4_mean: 316.6
    cleaning_beam_agent-4_min: 155
    cleaning_beam_agent-5_max: 163
    cleaning_beam_agent-5_mean: 58.08
    cleaning_beam_agent-5_min: 12
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-24-01
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 940.9999999999825
  episode_reward_mean: 697.0299999999945
  episode_reward_min: 228.99999999999687
  episodes_this_iter: 96
  episodes_total: 17568
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19637.559
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001697472034720704
        entropy: 1.1328260898590088
        entropy_coeff: 0.0017600000137463212
        kl: 0.0075506712310016155
        model: {}
        policy_loss: -0.02151534892618656
        total_loss: -0.020483434200286865
        vf_explained_var: 0.03663043677806854
        vf_loss: 15.155540466308594
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001697472034720704
        entropy: 1.1069386005401611
        entropy_coeff: 0.0017600000137463212
        kl: 0.007913175970315933
        model: {}
        policy_loss: -0.022682350128889084
        total_loss: -0.021492809057235718
        vf_explained_var: 0.01191394031047821
        vf_loss: 15.551196098327637
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001697472034720704
        entropy: 1.0557152032852173
        entropy_coeff: 0.0017600000137463212
        kl: 0.007563022896647453
        model: {}
        policy_loss: -0.021745972335338593
        total_loss: -0.020595608279109
        vf_explained_var: 0.04988300800323486
        vf_loss: 14.958168983459473
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001697472034720704
        entropy: 0.7915496230125427
        entropy_coeff: 0.0017600000137463212
        kl: 0.0063661839812994
        model: {}
        policy_loss: -0.018391896039247513
        total_loss: -0.017154362052679062
        vf_explained_var: 0.1377047598361969
        vf_loss: 13.574275016784668
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001697472034720704
        entropy: 1.0350013971328735
        entropy_coeff: 0.0017600000137463212
        kl: 0.007885858416557312
        model: {}
        policy_loss: -0.024877401068806648
        total_loss: -0.02364923804998398
        vf_explained_var: 0.06481865048408508
        vf_loss: 14.725932121276855
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001697472034720704
        entropy: 0.8766576051712036
        entropy_coeff: 0.0017600000137463212
        kl: 0.0077462634071707726
        model: {}
        policy_loss: -0.023540079593658447
        total_loss: -0.022165972739458084
        vf_explained_var: 0.13121679425239563
        vf_loss: 13.677719116210938
    load_time_ms: 13124.828
    num_steps_sampled: 17568000
    num_steps_trained: 17568000
    sample_time_ms: 89253.573
    update_time_ms: 20.905
  iterations_since_restore: 123
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.506358381502887
    ram_util_percent: 10.163005780346817
  pid: 24061
  policy_reward_max:
    agent-0: 156.83333333333306
    agent-1: 156.83333333333306
    agent-2: 156.83333333333306
    agent-3: 156.83333333333306
    agent-4: 156.83333333333306
    agent-5: 156.83333333333306
  policy_reward_mean:
    agent-0: 116.171666666667
    agent-1: 116.171666666667
    agent-2: 116.171666666667
    agent-3: 116.171666666667
    agent-4: 116.171666666667
    agent-5: 116.171666666667
  policy_reward_min:
    agent-0: 38.16666666666664
    agent-1: 38.16666666666664
    agent-2: 38.16666666666664
    agent-3: 38.16666666666664
    agent-4: 38.16666666666664
    agent-5: 38.16666666666664
  sampler_perf:
    mean_env_wait_ms: 24.418058970826383
    mean_inference_ms: 12.345394392541065
    mean_processing_ms: 51.14671961408466
  time_since_restore: 16217.045022010803
  time_this_iter_s: 121.8800106048584
  time_total_s: 25343.056835889816
  timestamp: 1637040241
  timesteps_since_restore: 11808000
  timesteps_this_iter: 96000
  timesteps_total: 17568000
  training_iteration: 183
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    183 |          25343.1 | 17568000 |   697.03 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 51
    apples_agent-0_mean: 5.63
    apples_agent-0_min: 0
    apples_agent-1_max: 89
    apples_agent-1_mean: 27.82
    apples_agent-1_min: 0
    apples_agent-2_max: 65
    apples_agent-2_mean: 4.48
    apples_agent-2_min: 0
    apples_agent-3_max: 158
    apples_agent-3_mean: 97.9
    apples_agent-3_min: 44
    apples_agent-4_max: 46
    apples_agent-4_mean: 2.1
    apples_agent-4_min: 0
    apples_agent-5_max: 149
    apples_agent-5_mean: 86.09
    apples_agent-5_min: 21
    cleaning_beam_agent-0_max: 413
    cleaning_beam_agent-0_mean: 288.02
    cleaning_beam_agent-0_min: 150
    cleaning_beam_agent-1_max: 441
    cleaning_beam_agent-1_mean: 261.1
    cleaning_beam_agent-1_min: 126
    cleaning_beam_agent-2_max: 470
    cleaning_beam_agent-2_mean: 302.7
    cleaning_beam_agent-2_min: 132
    cleaning_beam_agent-3_max: 168
    cleaning_beam_agent-3_mean: 57.59
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 439
    cleaning_beam_agent-4_mean: 333.22
    cleaning_beam_agent-4_min: 232
    cleaning_beam_agent-5_max: 415
    cleaning_beam_agent-5_mean: 64.82
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-26-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 940.9999999999789
  episode_reward_mean: 709.7299999999923
  episode_reward_min: 341.0000000000027
  episodes_this_iter: 96
  episodes_total: 17664
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19648.693
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00016375680570490658
        entropy: 1.152048945426941
        entropy_coeff: 0.0017600000137463212
        kl: 0.007038586772978306
        model: {}
        policy_loss: -0.02100992761552334
        total_loss: -0.020080994814634323
        vf_explained_var: 0.0343867689371109
        vf_loss: 15.488202095031738
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00016375680570490658
        entropy: 1.112696647644043
        entropy_coeff: 0.0017600000137463212
        kl: 0.007643275894224644
        model: {}
        policy_loss: -0.022743400186300278
        total_loss: -0.021574219688773155
        vf_explained_var: 0.0030076205730438232
        vf_loss: 15.988730430603027
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00016375680570490658
        entropy: 1.0502405166625977
        entropy_coeff: 0.0017600000137463212
        kl: 0.007486913353204727
        model: {}
        policy_loss: -0.02100531756877899
        total_loss: -0.019841957837343216
        vf_explained_var: 0.05605362355709076
        vf_loss: 15.144035339355469
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00016375680570490658
        entropy: 0.7844206094741821
        entropy_coeff: 0.0017600000137463212
        kl: 0.005817784927785397
        model: {}
        policy_loss: -0.01793758198618889
        total_loss: -0.016806337982416153
        vf_explained_var: 0.15933047235012054
        vf_loss: 13.482647895812988
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00016375680570490658
        entropy: 1.0279669761657715
        entropy_coeff: 0.0017600000137463212
        kl: 0.008203886449337006
        model: {}
        policy_loss: -0.024619437754154205
        total_loss: -0.023243654519319534
        vf_explained_var: 0.036314696073532104
        vf_loss: 15.44230842590332
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00016375680570490658
        entropy: 0.8602731227874756
        entropy_coeff: 0.0017600000137463212
        kl: 0.007242645602673292
        model: {}
        policy_loss: -0.02276083454489708
        total_loss: -0.021386153995990753
        vf_explained_var: 0.10159561038017273
        vf_loss: 14.402329444885254
    load_time_ms: 13129.587
    num_steps_sampled: 17664000
    num_steps_trained: 17664000
    sample_time_ms: 89357.01
    update_time_ms: 21.071
  iterations_since_restore: 124
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.339999999999998
    ram_util_percent: 10.156571428571427
  pid: 24061
  policy_reward_max:
    agent-0: 156.83333333333314
    agent-1: 156.83333333333314
    agent-2: 156.83333333333314
    agent-3: 156.83333333333314
    agent-4: 156.83333333333314
    agent-5: 156.83333333333314
  policy_reward_mean:
    agent-0: 118.28833333333368
    agent-1: 118.28833333333368
    agent-2: 118.28833333333368
    agent-3: 118.28833333333368
    agent-4: 118.28833333333368
    agent-5: 118.28833333333368
  policy_reward_min:
    agent-0: 56.83333333333312
    agent-1: 56.83333333333312
    agent-2: 56.83333333333312
    agent-3: 56.83333333333312
    agent-4: 56.83333333333312
    agent-5: 56.83333333333312
  sampler_perf:
    mean_env_wait_ms: 24.412522704562015
    mean_inference_ms: 12.343606737182903
    mean_processing_ms: 51.13903870090162
  time_since_restore: 16339.89601612091
  time_this_iter_s: 122.85099411010742
  time_total_s: 25465.907829999924
  timestamp: 1637040364
  timesteps_since_restore: 11904000
  timesteps_this_iter: 96000
  timesteps_total: 17664000
  training_iteration: 184
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    184 |          25465.9 | 17664000 |   709.73 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 96
    apples_agent-0_mean: 5.8
    apples_agent-0_min: 0
    apples_agent-1_max: 96
    apples_agent-1_mean: 32.05
    apples_agent-1_min: 0
    apples_agent-2_max: 198
    apples_agent-2_mean: 5.38
    apples_agent-2_min: 0
    apples_agent-3_max: 191
    apples_agent-3_mean: 100.45
    apples_agent-3_min: 47
    apples_agent-4_max: 43
    apples_agent-4_mean: 1.33
    apples_agent-4_min: 0
    apples_agent-5_max: 150
    apples_agent-5_mean: 90.13
    apples_agent-5_min: 24
    cleaning_beam_agent-0_max: 435
    cleaning_beam_agent-0_mean: 298.74
    cleaning_beam_agent-0_min: 156
    cleaning_beam_agent-1_max: 459
    cleaning_beam_agent-1_mean: 240.65
    cleaning_beam_agent-1_min: 92
    cleaning_beam_agent-2_max: 461
    cleaning_beam_agent-2_mean: 321.81
    cleaning_beam_agent-2_min: 161
    cleaning_beam_agent-3_max: 142
    cleaning_beam_agent-3_mean: 50.63
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 443
    cleaning_beam_agent-4_mean: 334.96
    cleaning_beam_agent-4_min: 210
    cleaning_beam_agent-5_max: 415
    cleaning_beam_agent-5_mean: 61.09
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-28-07
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 926.9999999999746
  episode_reward_mean: 735.9999999999906
  episode_reward_min: 456.0000000000121
  episodes_this_iter: 96
  episodes_total: 17760
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19669.025
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00015776639338582754
        entropy: 1.1468191146850586
        entropy_coeff: 0.0017600000137463212
        kl: 0.006698540411889553
        model: {}
        policy_loss: -0.02024872787296772
        total_loss: -0.019403710961341858
        vf_explained_var: 0.05775149166584015
        vf_loss: 15.237110137939453
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00015776639338582754
        entropy: 1.1194849014282227
        entropy_coeff: 0.0017600000137463212
        kl: 0.007659361697733402
        model: {}
        policy_loss: -0.022097522392868996
        total_loss: -0.020954720675945282
        vf_explained_var: 0.023141995072364807
        vf_loss: 15.812215805053711
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00015776639338582754
        entropy: 1.0538140535354614
        entropy_coeff: 0.0017600000137463212
        kl: 0.006919406354427338
        model: {}
        policy_loss: -0.021579120308160782
        total_loss: -0.020470382645726204
        vf_explained_var: 0.024077728390693665
        vf_loss: 15.795703887939453
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00015776639338582754
        entropy: 0.7694669365882874
        entropy_coeff: 0.0017600000137463212
        kl: 0.00602702284231782
        model: {}
        policy_loss: -0.017663732171058655
        total_loss: -0.016412293538451195
        vf_explained_var: 0.13466967642307281
        vf_loss: 14.002981185913086
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00015776639338582754
        entropy: 1.0167808532714844
        entropy_coeff: 0.0017600000137463212
        kl: 0.007566781714558601
        model: {}
        policy_loss: -0.023533668369054794
        total_loss: -0.022224657237529755
        vf_explained_var: 0.02179919183254242
        vf_loss: 15.851922035217285
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00015776639338582754
        entropy: 0.850523829460144
        entropy_coeff: 0.0017600000137463212
        kl: 0.007514300290495157
        model: {}
        policy_loss: -0.021680787205696106
        total_loss: -0.020222878083586693
        vf_explained_var: 0.10222353041172028
        vf_loss: 14.519672393798828
    load_time_ms: 13130.526
    num_steps_sampled: 17760000
    num_steps_trained: 17760000
    sample_time_ms: 89273.371
    update_time_ms: 20.632
  iterations_since_restore: 125
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.363793103448277
    ram_util_percent: 10.162068965517237
  pid: 24061
  policy_reward_max:
    agent-0: 154.4999999999999
    agent-1: 154.4999999999999
    agent-2: 154.4999999999999
    agent-3: 154.4999999999999
    agent-4: 154.4999999999999
    agent-5: 154.4999999999999
  policy_reward_mean:
    agent-0: 122.66666666666698
    agent-1: 122.66666666666698
    agent-2: 122.66666666666698
    agent-3: 122.66666666666698
    agent-4: 122.66666666666698
    agent-5: 122.66666666666698
  policy_reward_min:
    agent-0: 76.00000000000021
    agent-1: 76.00000000000021
    agent-2: 76.00000000000021
    agent-3: 76.00000000000021
    agent-4: 76.00000000000021
    agent-5: 76.00000000000021
  sampler_perf:
    mean_env_wait_ms: 24.40670599151133
    mean_inference_ms: 12.341581274929302
    mean_processing_ms: 51.13145530856518
  time_since_restore: 16462.139330148697
  time_this_iter_s: 122.24331402778625
  time_total_s: 25588.15114402771
  timestamp: 1637040487
  timesteps_since_restore: 12000000
  timesteps_this_iter: 96000
  timesteps_total: 17760000
  training_iteration: 185
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    185 |          25588.2 | 17760000 |      736 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 96
    apples_agent-0_mean: 5.55
    apples_agent-0_min: 0
    apples_agent-1_max: 92
    apples_agent-1_mean: 27.64
    apples_agent-1_min: 0
    apples_agent-2_max: 143
    apples_agent-2_mean: 6.02
    apples_agent-2_min: 0
    apples_agent-3_max: 143
    apples_agent-3_mean: 93.39
    apples_agent-3_min: 26
    apples_agent-4_max: 64
    apples_agent-4_mean: 2.86
    apples_agent-4_min: 0
    apples_agent-5_max: 132
    apples_agent-5_mean: 85.52
    apples_agent-5_min: 33
    cleaning_beam_agent-0_max: 430
    cleaning_beam_agent-0_mean: 297.99
    cleaning_beam_agent-0_min: 157
    cleaning_beam_agent-1_max: 535
    cleaning_beam_agent-1_mean: 240.66
    cleaning_beam_agent-1_min: 123
    cleaning_beam_agent-2_max: 458
    cleaning_beam_agent-2_mean: 301.95
    cleaning_beam_agent-2_min: 108
    cleaning_beam_agent-3_max: 155
    cleaning_beam_agent-3_mean: 52.23
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 427
    cleaning_beam_agent-4_mean: 330.32
    cleaning_beam_agent-4_min: 122
    cleaning_beam_agent-5_max: 192
    cleaning_beam_agent-5_mean: 63.09
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-30-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 986.9999999999723
  episode_reward_mean: 686.7999999999926
  episode_reward_min: 202.99999999999747
  episodes_this_iter: 96
  episodes_total: 17856
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19711.023
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00015177599561866373
        entropy: 1.1446783542633057
        entropy_coeff: 0.0017600000137463212
        kl: 0.007291554939001799
        model: {}
        policy_loss: -0.02044558897614479
        total_loss: -0.019237371161580086
        vf_explained_var: 0.06349456310272217
        vf_loss: 17.645383834838867
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00015177599561866373
        entropy: 1.1021337509155273
        entropy_coeff: 0.0017600000137463212
        kl: 0.00726099219173193
        model: {}
        policy_loss: -0.021132444962859154
        total_loss: -0.01975727453827858
        vf_explained_var: 0.010591104626655579
        vf_loss: 18.62729263305664
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00015177599561866373
        entropy: 1.0469684600830078
        entropy_coeff: 0.0017600000137463212
        kl: 0.007055091205984354
        model: {}
        policy_loss: -0.02064596116542816
        total_loss: -0.019339270889759064
        vf_explained_var: 0.07615451514720917
        vf_loss: 17.383310317993164
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00015177599561866373
        entropy: 0.7918199896812439
        entropy_coeff: 0.0017600000137463212
        kl: 0.005964862648397684
        model: {}
        policy_loss: -0.018312910571694374
        total_loss: -0.01695316471159458
        vf_explained_var: 0.16983915865421295
        vf_loss: 15.603760719299316
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00015177599561866373
        entropy: 1.0228240489959717
        entropy_coeff: 0.0017600000137463212
        kl: 0.007321844808757305
        model: {}
        policy_loss: -0.023656899109482765
        total_loss: -0.022272074595093727
        vf_explained_var: 0.08642759919166565
        vf_loss: 17.206214904785156
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00015177599561866373
        entropy: 0.8564784526824951
        entropy_coeff: 0.0017600000137463212
        kl: 0.00734332948923111
        model: {}
        policy_loss: -0.021546054631471634
        total_loss: -0.020064927637577057
        vf_explained_var: 0.1925685852766037
        vf_loss: 15.198613166809082
    load_time_ms: 13258.344
    num_steps_sampled: 17856000
    num_steps_trained: 17856000
    sample_time_ms: 89188.154
    update_time_ms: 20.418
  iterations_since_restore: 126
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.263218390804596
    ram_util_percent: 10.17758620689655
  pid: 24061
  policy_reward_max:
    agent-0: 164.49999999999991
    agent-1: 164.49999999999991
    agent-2: 164.49999999999991
    agent-3: 164.49999999999991
    agent-4: 164.49999999999991
    agent-5: 164.49999999999991
  policy_reward_mean:
    agent-0: 114.46666666666697
    agent-1: 114.46666666666697
    agent-2: 114.46666666666697
    agent-3: 114.46666666666697
    agent-4: 114.46666666666697
    agent-5: 114.46666666666697
  policy_reward_min:
    agent-0: 33.8333333333334
    agent-1: 33.8333333333334
    agent-2: 33.8333333333334
    agent-3: 33.8333333333334
    agent-4: 33.8333333333334
    agent-5: 33.8333333333334
  sampler_perf:
    mean_env_wait_ms: 24.40106560838106
    mean_inference_ms: 12.339452954745235
    mean_processing_ms: 51.12347784451297
  time_since_restore: 16584.6945104599
  time_this_iter_s: 122.555180311203
  time_total_s: 25710.706324338913
  timestamp: 1637040609
  timesteps_since_restore: 12096000
  timesteps_this_iter: 96000
  timesteps_total: 17856000
  training_iteration: 186
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    186 |          25710.7 | 17856000 |    686.8 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 121
    apples_agent-0_mean: 6.77
    apples_agent-0_min: 0
    apples_agent-1_max: 98
    apples_agent-1_mean: 32.82
    apples_agent-1_min: 0
    apples_agent-2_max: 46
    apples_agent-2_mean: 3.5
    apples_agent-2_min: 0
    apples_agent-3_max: 168
    apples_agent-3_mean: 98.13
    apples_agent-3_min: 26
    apples_agent-4_max: 34
    apples_agent-4_mean: 1.14
    apples_agent-4_min: 0
    apples_agent-5_max: 150
    apples_agent-5_mean: 90.43
    apples_agent-5_min: 45
    cleaning_beam_agent-0_max: 472
    cleaning_beam_agent-0_mean: 300.9
    cleaning_beam_agent-0_min: 132
    cleaning_beam_agent-1_max: 402
    cleaning_beam_agent-1_mean: 245.36
    cleaning_beam_agent-1_min: 114
    cleaning_beam_agent-2_max: 438
    cleaning_beam_agent-2_mean: 325.69
    cleaning_beam_agent-2_min: 162
    cleaning_beam_agent-3_max: 140
    cleaning_beam_agent-3_mean: 53.89
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 464
    cleaning_beam_agent-4_mean: 347.88
    cleaning_beam_agent-4_min: 225
    cleaning_beam_agent-5_max: 178
    cleaning_beam_agent-5_mean: 60.53
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-32-13
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 938.9999999999809
  episode_reward_mean: 718.919999999991
  episode_reward_min: 346.0000000000033
  episodes_this_iter: 96
  episodes_total: 17952
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19729.891
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00014578559785149992
        entropy: 1.134765386581421
        entropy_coeff: 0.0017600000137463212
        kl: 0.006690487265586853
        model: {}
        policy_loss: -0.01977606490254402
        total_loss: -0.018864067271351814
        vf_explained_var: 0.003922775387763977
        vf_loss: 15.710918426513672
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00014578559785149992
        entropy: 1.112261176109314
        entropy_coeff: 0.0017600000137463212
        kl: 0.0074621038511395454
        model: {}
        policy_loss: -0.021896399557590485
        total_loss: -0.020773399621248245
        vf_explained_var: -0.008470073342323303
        vf_loss: 15.881569862365723
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00014578559785149992
        entropy: 1.0509053468704224
        entropy_coeff: 0.0017600000137463212
        kl: 0.006862415932118893
        model: {}
        policy_loss: -0.01989186555147171
        total_loss: -0.018850846216082573
        vf_explained_var: 0.035369500517845154
        vf_loss: 15.181312561035156
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00014578559785149992
        entropy: 0.7775798439979553
        entropy_coeff: 0.0017600000137463212
        kl: 0.005827680695801973
        model: {}
        policy_loss: -0.016636036336421967
        total_loss: -0.015440164133906364
        vf_explained_var: 0.10885517299175262
        vf_loss: 13.988801002502441
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00014578559785149992
        entropy: 1.0103569030761719
        entropy_coeff: 0.0017600000137463212
        kl: 0.00706175621598959
        model: {}
        policy_loss: -0.02176632359623909
        total_loss: -0.02065064013004303
        vf_explained_var: 0.05888204276561737
        vf_loss: 14.815600395202637
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00014578559785149992
        entropy: 0.8584076762199402
        entropy_coeff: 0.0017600000137463212
        kl: 0.0063866544514894485
        model: {}
        policy_loss: -0.01980631798505783
        total_loss: -0.01857595145702362
        vf_explained_var: 0.07034489512443542
        vf_loss: 14.638309478759766
    load_time_ms: 13261.065
    num_steps_sampled: 17952000
    num_steps_trained: 17952000
    sample_time_ms: 89378.121
    update_time_ms: 20.461
  iterations_since_restore: 127
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.967613636363637
    ram_util_percent: 10.209659090909089
  pid: 24061
  policy_reward_max:
    agent-0: 156.4999999999999
    agent-1: 156.4999999999999
    agent-2: 156.4999999999999
    agent-3: 156.4999999999999
    agent-4: 156.4999999999999
    agent-5: 156.4999999999999
  policy_reward_mean:
    agent-0: 119.82000000000035
    agent-1: 119.82000000000035
    agent-2: 119.82000000000035
    agent-3: 119.82000000000035
    agent-4: 119.82000000000035
    agent-5: 119.82000000000035
  policy_reward_min:
    agent-0: 57.66666666666654
    agent-1: 57.66666666666654
    agent-2: 57.66666666666654
    agent-3: 57.66666666666654
    agent-4: 57.66666666666654
    agent-5: 57.66666666666654
  sampler_perf:
    mean_env_wait_ms: 24.39857394667863
    mean_inference_ms: 12.338337569167274
    mean_processing_ms: 51.12044936835243
  time_since_restore: 16707.99333024025
  time_this_iter_s: 123.29881978034973
  time_total_s: 25834.005144119263
  timestamp: 1637040733
  timesteps_since_restore: 12192000
  timesteps_this_iter: 96000
  timesteps_total: 17952000
  training_iteration: 187
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    187 |            25834 | 17952000 |   718.92 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 97
    apples_agent-0_mean: 6.67
    apples_agent-0_min: 0
    apples_agent-1_max: 80
    apples_agent-1_mean: 27.71
    apples_agent-1_min: 0
    apples_agent-2_max: 107
    apples_agent-2_mean: 5.4
    apples_agent-2_min: 0
    apples_agent-3_max: 161
    apples_agent-3_mean: 100.06
    apples_agent-3_min: 43
    apples_agent-4_max: 53
    apples_agent-4_mean: 2.35
    apples_agent-4_min: 0
    apples_agent-5_max: 156
    apples_agent-5_mean: 86.94
    apples_agent-5_min: 38
    cleaning_beam_agent-0_max: 408
    cleaning_beam_agent-0_mean: 298.17
    cleaning_beam_agent-0_min: 161
    cleaning_beam_agent-1_max: 413
    cleaning_beam_agent-1_mean: 238.88
    cleaning_beam_agent-1_min: 97
    cleaning_beam_agent-2_max: 471
    cleaning_beam_agent-2_mean: 318.35
    cleaning_beam_agent-2_min: 124
    cleaning_beam_agent-3_max: 154
    cleaning_beam_agent-3_mean: 50.71
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 461
    cleaning_beam_agent-4_mean: 352.25
    cleaning_beam_agent-4_min: 190
    cleaning_beam_agent-5_max: 266
    cleaning_beam_agent-5_mean: 56.62
    cleaning_beam_agent-5_min: 13
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-34-15
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1011.9999999999708
  episode_reward_mean: 730.78999999999
  episode_reward_min: 346.0000000000033
  episodes_this_iter: 96
  episodes_total: 18048
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19708.881
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001397952000843361
        entropy: 1.1363641023635864
        entropy_coeff: 0.0017600000137463212
        kl: 0.006567133590579033
        model: {}
        policy_loss: -0.019276143983006477
        total_loss: -0.018406299874186516
        vf_explained_var: 0.05831809341907501
        vf_loss: 15.564191818237305
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001397952000843361
        entropy: 1.107616662979126
        entropy_coeff: 0.0017600000137463212
        kl: 0.006907017435878515
        model: {}
        policy_loss: -0.020961113274097443
        total_loss: -0.019894851371645927
        vf_explained_var: 0.011246055364608765
        vf_loss: 16.342607498168945
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001397952000843361
        entropy: 1.053647518157959
        entropy_coeff: 0.0017600000137463212
        kl: 0.007040317170321941
        model: {}
        policy_loss: -0.020278165116906166
        total_loss: -0.019161904230713844
        vf_explained_var: 0.05538439750671387
        vf_loss: 15.626171112060547
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001397952000843361
        entropy: 0.7704724073410034
        entropy_coeff: 0.0017600000137463212
        kl: 0.004968428052961826
        model: {}
        policy_loss: -0.0156780444085598
        total_loss: -0.014599455520510674
        vf_explained_var: 0.12754374742507935
        vf_loss: 14.409332275390625
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001397952000843361
        entropy: 1.0073542594909668
        entropy_coeff: 0.0017600000137463212
        kl: 0.0072126626037061214
        model: {}
        policy_loss: -0.022162826731801033
        total_loss: -0.020909354090690613
        vf_explained_var: 0.041382670402526855
        vf_loss: 15.8388671875
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001397952000843361
        entropy: 0.8313236832618713
        entropy_coeff: 0.0017600000137463212
        kl: 0.006637263111770153
        model: {}
        policy_loss: -0.02018616534769535
        total_loss: -0.018917927518486977
        vf_explained_var: 0.14945918321609497
        vf_loss: 14.039137840270996
    load_time_ms: 13108.314
    num_steps_sampled: 18048000
    num_steps_trained: 18048000
    sample_time_ms: 89307.45
    update_time_ms: 20.481
  iterations_since_restore: 128
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.471098265895954
    ram_util_percent: 10.149710982658958
  pid: 24061
  policy_reward_max:
    agent-0: 168.66666666666654
    agent-1: 168.66666666666654
    agent-2: 168.66666666666654
    agent-3: 168.66666666666654
    agent-4: 168.66666666666654
    agent-5: 168.66666666666654
  policy_reward_mean:
    agent-0: 121.79833333333367
    agent-1: 121.79833333333367
    agent-2: 121.79833333333367
    agent-3: 121.79833333333367
    agent-4: 121.79833333333367
    agent-5: 121.79833333333367
  policy_reward_min:
    agent-0: 57.66666666666654
    agent-1: 57.66666666666654
    agent-2: 57.66666666666654
    agent-3: 57.66666666666654
    agent-4: 57.66666666666654
    agent-5: 57.66666666666654
  sampler_perf:
    mean_env_wait_ms: 24.39392504289572
    mean_inference_ms: 12.336616438929259
    mean_processing_ms: 51.11355299407126
  time_since_restore: 16830.04127550125
  time_this_iter_s: 122.04794526100159
  time_total_s: 25956.053089380264
  timestamp: 1637040855
  timesteps_since_restore: 12288000
  timesteps_this_iter: 96000
  timesteps_total: 18048000
  training_iteration: 188
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    188 |          25956.1 | 18048000 |   730.79 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 90
    apples_agent-0_mean: 5.58
    apples_agent-0_min: 0
    apples_agent-1_max: 106
    apples_agent-1_mean: 31.2
    apples_agent-1_min: 0
    apples_agent-2_max: 52
    apples_agent-2_mean: 3.52
    apples_agent-2_min: 0
    apples_agent-3_max: 169
    apples_agent-3_mean: 97.34
    apples_agent-3_min: 32
    apples_agent-4_max: 30
    apples_agent-4_mean: 1.49
    apples_agent-4_min: 0
    apples_agent-5_max: 156
    apples_agent-5_mean: 89.0
    apples_agent-5_min: 19
    cleaning_beam_agent-0_max: 437
    cleaning_beam_agent-0_mean: 316.41
    cleaning_beam_agent-0_min: 169
    cleaning_beam_agent-1_max: 384
    cleaning_beam_agent-1_mean: 235.38
    cleaning_beam_agent-1_min: 112
    cleaning_beam_agent-2_max: 468
    cleaning_beam_agent-2_mean: 316.08
    cleaning_beam_agent-2_min: 167
    cleaning_beam_agent-3_max: 130
    cleaning_beam_agent-3_mean: 47.71
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 436
    cleaning_beam_agent-4_mean: 353.74
    cleaning_beam_agent-4_min: 220
    cleaning_beam_agent-5_max: 188
    cleaning_beam_agent-5_mean: 57.69
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-36-17
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 971.9999999999742
  episode_reward_mean: 725.6599999999903
  episode_reward_min: 284.99999999999966
  episodes_this_iter: 96
  episodes_total: 18144
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19722.514
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001338048023171723
        entropy: 1.1367719173431396
        entropy_coeff: 0.0017600000137463212
        kl: 0.006578825414180756
        model: {}
        policy_loss: -0.019206974655389786
        total_loss: -0.01819125935435295
        vf_explained_var: 0.03997175395488739
        vf_loss: 17.00667953491211
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001338048023171723
        entropy: 1.1181682348251343
        entropy_coeff: 0.0017600000137463212
        kl: 0.006893654353916645
        model: {}
        policy_loss: -0.02064771205186844
        total_loss: -0.019472401589155197
        vf_explained_var: 0.004520818591117859
        vf_loss: 17.645530700683594
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001338048023171723
        entropy: 1.0539746284484863
        entropy_coeff: 0.0017600000137463212
        kl: 0.00640891445800662
        model: {}
        policy_loss: -0.01952698454260826
        total_loss: -0.018425192683935165
        vf_explained_var: 0.055059075355529785
        vf_loss: 16.750022888183594
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001338048023171723
        entropy: 0.7741975784301758
        entropy_coeff: 0.0017600000137463212
        kl: 0.0059786392375826836
        model: {}
        policy_loss: -0.01594015769660473
        total_loss: -0.015218179672956467
        vf_explained_var: 0.1609000861644745
        vf_loss: 14.867033004760742
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001338048023171723
        entropy: 1.009936809539795
        entropy_coeff: 0.0017600000137463212
        kl: 0.006908471696078777
        model: {}
        policy_loss: -0.021958965808153152
        total_loss: -0.020664207637310028
        vf_explained_var: 0.046080440282821655
        vf_loss: 16.905540466308594
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001338048023171723
        entropy: 0.845014214515686
        entropy_coeff: 0.0017600000137463212
        kl: 0.006267187185585499
        model: {}
        policy_loss: -0.019988644868135452
        total_loss: -0.018685366958379745
        vf_explained_var: 0.1337043195962906
        vf_loss: 15.370664596557617
    load_time_ms: 13121.641
    num_steps_sampled: 18144000
    num_steps_trained: 18144000
    sample_time_ms: 89318.869
    update_time_ms: 20.361
  iterations_since_restore: 129
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.326285714285714
    ram_util_percent: 10.177714285714282
  pid: 24061
  policy_reward_max:
    agent-0: 161.99999999999974
    agent-1: 161.99999999999974
    agent-2: 161.99999999999974
    agent-3: 161.99999999999974
    agent-4: 161.99999999999974
    agent-5: 161.99999999999974
  policy_reward_mean:
    agent-0: 120.94333333333363
    agent-1: 120.94333333333363
    agent-2: 120.94333333333363
    agent-3: 120.94333333333363
    agent-4: 120.94333333333363
    agent-5: 120.94333333333363
  policy_reward_min:
    agent-0: 47.49999999999993
    agent-1: 47.49999999999993
    agent-2: 47.49999999999993
    agent-3: 47.49999999999993
    agent-4: 47.49999999999993
    agent-5: 47.49999999999993
  sampler_perf:
    mean_env_wait_ms: 24.389468435975676
    mean_inference_ms: 12.33452829766858
    mean_processing_ms: 51.1065528494717
  time_since_restore: 16952.02302789688
  time_this_iter_s: 121.98175239562988
  time_total_s: 26078.034841775894
  timestamp: 1637040977
  timesteps_since_restore: 12384000
  timesteps_this_iter: 96000
  timesteps_total: 18144000
  training_iteration: 189
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    189 |            26078 | 18144000 |   725.66 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 52
    apples_agent-0_mean: 4.25
    apples_agent-0_min: 0
    apples_agent-1_max: 81
    apples_agent-1_mean: 31.15
    apples_agent-1_min: 0
    apples_agent-2_max: 54
    apples_agent-2_mean: 2.77
    apples_agent-2_min: 0
    apples_agent-3_max: 160
    apples_agent-3_mean: 102.05
    apples_agent-3_min: 45
    apples_agent-4_max: 58
    apples_agent-4_mean: 1.64
    apples_agent-4_min: 0
    apples_agent-5_max: 128
    apples_agent-5_mean: 84.65
    apples_agent-5_min: 29
    cleaning_beam_agent-0_max: 423
    cleaning_beam_agent-0_mean: 312.21
    cleaning_beam_agent-0_min: 122
    cleaning_beam_agent-1_max: 522
    cleaning_beam_agent-1_mean: 244.3
    cleaning_beam_agent-1_min: 111
    cleaning_beam_agent-2_max: 443
    cleaning_beam_agent-2_mean: 320.47
    cleaning_beam_agent-2_min: 175
    cleaning_beam_agent-3_max: 107
    cleaning_beam_agent-3_mean: 46.84
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 473
    cleaning_beam_agent-4_mean: 340.48
    cleaning_beam_agent-4_min: 253
    cleaning_beam_agent-5_max: 252
    cleaning_beam_agent-5_mean: 57.84
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-38-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 927.999999999979
  episode_reward_mean: 734.2699999999892
  episode_reward_min: 453.0000000000108
  episodes_this_iter: 96
  episodes_total: 18240
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19697.707
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00012781440455000848
        entropy: 1.148158311843872
        entropy_coeff: 0.0017600000137463212
        kl: 0.006515904329717159
        model: {}
        policy_loss: -0.018504831939935684
        total_loss: -0.017693253234028816
        vf_explained_var: 0.049855008721351624
        vf_loss: 15.291565895080566
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00012781440455000848
        entropy: 1.1036932468414307
        entropy_coeff: 0.0017600000137463212
        kl: 0.006336911581456661
        model: {}
        policy_loss: -0.020486120134592056
        total_loss: -0.019557109102606773
        vf_explained_var: 0.004150271415710449
        vf_loss: 16.041290283203125
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00012781440455000848
        entropy: 1.0501688718795776
        entropy_coeff: 0.0017600000137463212
        kl: 0.0060699712485075
        model: {}
        policy_loss: -0.018522268161177635
        total_loss: -0.017561325803399086
        vf_explained_var: 0.010198041796684265
        vf_loss: 15.952444076538086
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00012781440455000848
        entropy: 0.7551815509796143
        entropy_coeff: 0.0017600000137463212
        kl: 0.005995132494717836
        model: {}
        policy_loss: -0.014924533665180206
        total_loss: -0.014233702793717384
        vf_explained_var: 0.11699935793876648
        vf_loss: 14.204357147216797
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00012781440455000848
        entropy: 1.0085158348083496
        entropy_coeff: 0.0017600000137463212
        kl: 0.006835896987468004
        model: {}
        policy_loss: -0.02109133079648018
        total_loss: -0.019936639815568924
        vf_explained_var: 0.02987763285636902
        vf_loss: 15.624963760375977
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00012781440455000848
        entropy: 0.8450781106948853
        entropy_coeff: 0.0017600000137463212
        kl: 0.006045821122825146
        model: {}
        policy_loss: -0.019036319106817245
        total_loss: -0.01786755956709385
        vf_explained_var: 0.10243414342403412
        vf_loss: 14.46931266784668
    load_time_ms: 13152.408
    num_steps_sampled: 18240000
    num_steps_trained: 18240000
    sample_time_ms: 89385.893
    update_time_ms: 20.216
  iterations_since_restore: 130
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.382658959537574
    ram_util_percent: 10.143930635838151
  pid: 24061
  policy_reward_max:
    agent-0: 154.66666666666637
    agent-1: 154.66666666666637
    agent-2: 154.66666666666637
    agent-3: 154.66666666666637
    agent-4: 154.66666666666637
    agent-5: 154.66666666666637
  policy_reward_mean:
    agent-0: 122.37833333333364
    agent-1: 122.37833333333364
    agent-2: 122.37833333333364
    agent-3: 122.37833333333364
    agent-4: 122.37833333333364
    agent-5: 122.37833333333364
  policy_reward_min:
    agent-0: 75.49999999999993
    agent-1: 75.49999999999993
    agent-2: 75.49999999999993
    agent-3: 75.49999999999993
    agent-4: 75.49999999999993
    agent-5: 75.49999999999993
  sampler_perf:
    mean_env_wait_ms: 24.384779465875322
    mean_inference_ms: 12.332659117417451
    mean_processing_ms: 51.09889468953335
  time_since_restore: 17074.02994132042
  time_this_iter_s: 122.00691342353821
  time_total_s: 26200.041755199432
  timestamp: 1637041099
  timesteps_since_restore: 12480000
  timesteps_this_iter: 96000
  timesteps_total: 18240000
  training_iteration: 190
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    190 |            26200 | 18240000 |   734.27 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 67
    apples_agent-0_mean: 4.56
    apples_agent-0_min: 0
    apples_agent-1_max: 142
    apples_agent-1_mean: 31.85
    apples_agent-1_min: 0
    apples_agent-2_max: 93
    apples_agent-2_mean: 2.94
    apples_agent-2_min: 0
    apples_agent-3_max: 141
    apples_agent-3_mean: 96.85
    apples_agent-3_min: 35
    apples_agent-4_max: 64
    apples_agent-4_mean: 2.92
    apples_agent-4_min: 0
    apples_agent-5_max: 142
    apples_agent-5_mean: 85.75
    apples_agent-5_min: 15
    cleaning_beam_agent-0_max: 444
    cleaning_beam_agent-0_mean: 307.45
    cleaning_beam_agent-0_min: 157
    cleaning_beam_agent-1_max: 424
    cleaning_beam_agent-1_mean: 229.94
    cleaning_beam_agent-1_min: 90
    cleaning_beam_agent-2_max: 485
    cleaning_beam_agent-2_mean: 317.64
    cleaning_beam_agent-2_min: 164
    cleaning_beam_agent-3_max: 195
    cleaning_beam_agent-3_mean: 52.78
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 442
    cleaning_beam_agent-4_mean: 334.05
    cleaning_beam_agent-4_min: 146
    cleaning_beam_agent-5_max: 195
    cleaning_beam_agent-5_mean: 59.87
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-40-22
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 984.9999999999692
  episode_reward_mean: 699.889999999992
  episode_reward_min: 326.0000000000023
  episodes_this_iter: 96
  episodes_total: 18336
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19702.653
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00012182399950688705
        entropy: 1.1499409675598145
        entropy_coeff: 0.0017600000137463212
        kl: 0.006109760608524084
        model: {}
        policy_loss: -0.01792246848344803
        total_loss: -0.0171563271433115
        vf_explained_var: 0.033067867159843445
        vf_loss: 15.680839538574219
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00012182399950688705
        entropy: 1.1120083332061768
        entropy_coeff: 0.0017600000137463212
        kl: 0.007046306971460581
        model: {}
        policy_loss: -0.020675282925367355
        total_loss: -0.01960153877735138
        vf_explained_var: 0.0007850229740142822
        vf_loss: 16.216163635253906
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00012182399950688705
        entropy: 1.0345845222473145
        entropy_coeff: 0.0017600000137463212
        kl: 0.006293431855738163
        model: {}
        policy_loss: -0.017681505531072617
        total_loss: -0.016647137701511383
        vf_explained_var: 0.015774086117744446
        vf_loss: 15.965469360351562
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00012182399950688705
        entropy: 0.7577266097068787
        entropy_coeff: 0.0017600000137463212
        kl: 0.006084369961172342
        model: {}
        policy_loss: -0.015162093564867973
        total_loss: -0.014537091366946697
        vf_explained_var: 0.16753144562244415
        vf_loss: 13.501648902893066
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00012182399950688705
        entropy: 1.0156394243240356
        entropy_coeff: 0.0017600000137463212
        kl: 0.006565110757946968
        model: {}
        policy_loss: -0.02114091068506241
        total_loss: -0.020062992349267006
        vf_explained_var: 0.04322044551372528
        vf_loss: 15.524171829223633
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00012182399950688705
        entropy: 0.8619775176048279
        entropy_coeff: 0.0017600000137463212
        kl: 0.0060708411037921906
        model: {}
        policy_loss: -0.019377639517188072
        total_loss: -0.0182369202375412
        vf_explained_var: 0.10947470366954803
        vf_loss: 14.436295509338379
    load_time_ms: 13167.496
    num_steps_sampled: 18336000
    num_steps_trained: 18336000
    sample_time_ms: 89441.671
    update_time_ms: 20.501
  iterations_since_restore: 131
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.34
    ram_util_percent: 10.149142857142857
  pid: 24061
  policy_reward_max:
    agent-0: 164.16666666666623
    agent-1: 164.16666666666623
    agent-2: 164.16666666666623
    agent-3: 164.16666666666623
    agent-4: 164.16666666666623
    agent-5: 164.16666666666623
  policy_reward_mean:
    agent-0: 116.64833333333365
    agent-1: 116.64833333333365
    agent-2: 116.64833333333365
    agent-3: 116.64833333333365
    agent-4: 116.64833333333365
    agent-5: 116.64833333333365
  policy_reward_min:
    agent-0: 54.33333333333312
    agent-1: 54.33333333333312
    agent-2: 54.33333333333312
    agent-3: 54.33333333333312
    agent-4: 54.33333333333312
    agent-5: 54.33333333333312
  sampler_perf:
    mean_env_wait_ms: 24.380300931034856
    mean_inference_ms: 12.330408447926068
    mean_processing_ms: 51.09349996136371
  time_since_restore: 17196.283580303192
  time_this_iter_s: 122.25363898277283
  time_total_s: 26322.295394182205
  timestamp: 1637041222
  timesteps_since_restore: 12576000
  timesteps_this_iter: 96000
  timesteps_total: 18336000
  training_iteration: 191
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    191 |          26322.3 | 18336000 |   699.89 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 55
    apples_agent-0_mean: 6.96
    apples_agent-0_min: 0
    apples_agent-1_max: 93
    apples_agent-1_mean: 27.9
    apples_agent-1_min: 0
    apples_agent-2_max: 55
    apples_agent-2_mean: 3.12
    apples_agent-2_min: 0
    apples_agent-3_max: 210
    apples_agent-3_mean: 98.74
    apples_agent-3_min: 38
    apples_agent-4_max: 23
    apples_agent-4_mean: 0.94
    apples_agent-4_min: 0
    apples_agent-5_max: 151
    apples_agent-5_mean: 88.05
    apples_agent-5_min: 17
    cleaning_beam_agent-0_max: 430
    cleaning_beam_agent-0_mean: 292.65
    cleaning_beam_agent-0_min: 135
    cleaning_beam_agent-1_max: 455
    cleaning_beam_agent-1_mean: 236.41
    cleaning_beam_agent-1_min: 116
    cleaning_beam_agent-2_max: 465
    cleaning_beam_agent-2_mean: 327.08
    cleaning_beam_agent-2_min: 160
    cleaning_beam_agent-3_max: 165
    cleaning_beam_agent-3_mean: 42.94
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 478
    cleaning_beam_agent-4_mean: 346.79
    cleaning_beam_agent-4_min: 194
    cleaning_beam_agent-5_max: 202
    cleaning_beam_agent-5_mean: 53.73
    cleaning_beam_agent-5_min: 18
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-42-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 963.9999999999854
  episode_reward_mean: 734.7299999999904
  episode_reward_min: 412.00000000000665
  episodes_this_iter: 96
  episodes_total: 18432
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19724.919
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00011583360173972324
        entropy: 1.1460996866226196
        entropy_coeff: 0.0017600000137463212
        kl: 0.005824091844260693
        model: {}
        policy_loss: -0.017312966287136078
        total_loss: -0.01647193916141987
        vf_explained_var: 0.0012314170598983765
        vf_loss: 16.933422088623047
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00011583360173972324
        entropy: 1.113781452178955
        entropy_coeff: 0.0017600000137463212
        kl: 0.0066710226237773895
        model: {}
        policy_loss: -0.02000250667333603
        total_loss: -0.018967796117067337
        vf_explained_var: 0.01535235345363617
        vf_loss: 16.607635498046875
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00011583360173972324
        entropy: 1.0352458953857422
        entropy_coeff: 0.0017600000137463212
        kl: 0.0059326523914933205
        model: {}
        policy_loss: -0.017452111467719078
        total_loss: -0.016452234238386154
        vf_explained_var: 0.028609320521354675
        vf_loss: 16.35382080078125
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00011583360173972324
        entropy: 0.7273135781288147
        entropy_coeff: 0.0017600000137463212
        kl: 0.005829364061355591
        model: {}
        policy_loss: -0.014497598633170128
        total_loss: -0.013813693076372147
        vf_explained_var: 0.1766088604927063
        vf_loss: 13.810388565063477
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00011583360173972324
        entropy: 1.0155436992645264
        entropy_coeff: 0.0017600000137463212
        kl: 0.0061393012292683125
        model: {}
        policy_loss: -0.02006928250193596
        total_loss: -0.019066620618104935
        vf_explained_var: 0.07135811448097229
        vf_loss: 15.621627807617188
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00011583360173972324
        entropy: 0.8533083200454712
        entropy_coeff: 0.0017600000137463212
        kl: 0.005510847084224224
        model: {}
        policy_loss: -0.01878305897116661
        total_loss: -0.017707964405417442
        vf_explained_var: 0.12407812476158142
        vf_loss: 14.747491836547852
    load_time_ms: 13167.116
    num_steps_sampled: 18432000
    num_steps_trained: 18432000
    sample_time_ms: 89320.019
    update_time_ms: 21.05
  iterations_since_restore: 132
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.398850574712647
    ram_util_percent: 10.099999999999998
  pid: 24061
  policy_reward_max:
    agent-0: 160.66666666666654
    agent-1: 160.66666666666654
    agent-2: 160.66666666666654
    agent-3: 160.66666666666654
    agent-4: 160.66666666666654
    agent-5: 160.66666666666654
  policy_reward_mean:
    agent-0: 122.45500000000033
    agent-1: 122.45500000000033
    agent-2: 122.45500000000033
    agent-3: 122.45500000000033
    agent-4: 122.45500000000033
    agent-5: 122.45500000000033
  policy_reward_min:
    agent-0: 68.66666666666652
    agent-1: 68.66666666666652
    agent-2: 68.66666666666652
    agent-3: 68.66666666666652
    agent-4: 68.66666666666652
    agent-5: 68.66666666666652
  sampler_perf:
    mean_env_wait_ms: 24.376171418500597
    mean_inference_ms: 12.328625680975554
    mean_processing_ms: 51.08655978386835
  time_since_restore: 17318.43969631195
  time_this_iter_s: 122.15611600875854
  time_total_s: 26444.451510190964
  timestamp: 1637041344
  timesteps_since_restore: 12672000
  timesteps_this_iter: 96000
  timesteps_total: 18432000
  training_iteration: 192
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    192 |          26444.5 | 18432000 |   734.73 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 69
    apples_agent-0_mean: 6.21
    apples_agent-0_min: 0
    apples_agent-1_max: 126
    apples_agent-1_mean: 26.25
    apples_agent-1_min: 0
    apples_agent-2_max: 45
    apples_agent-2_mean: 3.97
    apples_agent-2_min: 0
    apples_agent-3_max: 147
    apples_agent-3_mean: 95.23
    apples_agent-3_min: 14
    apples_agent-4_max: 32
    apples_agent-4_mean: 1.34
    apples_agent-4_min: 0
    apples_agent-5_max: 155
    apples_agent-5_mean: 90.51
    apples_agent-5_min: 27
    cleaning_beam_agent-0_max: 465
    cleaning_beam_agent-0_mean: 294.36
    cleaning_beam_agent-0_min: 153
    cleaning_beam_agent-1_max: 419
    cleaning_beam_agent-1_mean: 218.59
    cleaning_beam_agent-1_min: 99
    cleaning_beam_agent-2_max: 496
    cleaning_beam_agent-2_mean: 322.97
    cleaning_beam_agent-2_min: 150
    cleaning_beam_agent-3_max: 157
    cleaning_beam_agent-3_mean: 47.73
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 504
    cleaning_beam_agent-4_mean: 332.98
    cleaning_beam_agent-4_min: 206
    cleaning_beam_agent-5_max: 177
    cleaning_beam_agent-5_mean: 61.32
    cleaning_beam_agent-5_min: 12
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-44-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 991.9999999999711
  episode_reward_mean: 710.9799999999929
  episode_reward_min: 277.9999999999962
  episodes_this_iter: 96
  episodes_total: 18528
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19742.211
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00010984319669660181
        entropy: 1.1551060676574707
        entropy_coeff: 0.0017600000137463212
        kl: 0.00608114805072546
        model: {}
        policy_loss: -0.017962850630283356
        total_loss: -0.016972392797470093
        vf_explained_var: 0.03520902991294861
        vf_loss: 18.072154998779297
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00010984319669660181
        entropy: 1.117703914642334
        entropy_coeff: 0.0017600000137463212
        kl: 0.005829362664371729
        model: {}
        policy_loss: -0.018352819606661797
        total_loss: -0.01735532470047474
        vf_explained_var: 0.03994166851043701
        vf_loss: 17.987823486328125
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00010984319669660181
        entropy: 1.0359560251235962
        entropy_coeff: 0.0017600000137463212
        kl: 0.0057494002394378185
        model: {}
        policy_loss: -0.0172076728194952
        total_loss: -0.01615682803094387
        vf_explained_var: 0.07936863601207733
        vf_loss: 17.24246597290039
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00010984319669660181
        entropy: 0.7613043189048767
        entropy_coeff: 0.0017600000137463212
        kl: 0.0055463602766394615
        model: {}
        policy_loss: -0.0148292426019907
        total_loss: -0.014093078672885895
        vf_explained_var: 0.18738433718681335
        vf_loss: 15.214237213134766
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00010984319669660181
        entropy: 1.0233131647109985
        entropy_coeff: 0.0017600000137463212
        kl: 0.006040544714778662
        model: {}
        policy_loss: -0.019775938242673874
        total_loss: -0.01863105781376362
        vf_explained_var: 0.0720314234495163
        vf_loss: 17.37804412841797
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00010984319669660181
        entropy: 0.8434746265411377
        entropy_coeff: 0.0017600000137463212
        kl: 0.005647761281579733
        model: {}
        policy_loss: -0.01823205128312111
        total_loss: -0.017071710899472237
        vf_explained_var: 0.19107747077941895
        vf_loss: 15.153030395507812
    load_time_ms: 13153.313
    num_steps_sampled: 18528000
    num_steps_trained: 18528000
    sample_time_ms: 89290.86
    update_time_ms: 20.754
  iterations_since_restore: 133
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.435838150289017
    ram_util_percent: 10.145086705202313
  pid: 24061
  policy_reward_max:
    agent-0: 165.33333333333354
    agent-1: 165.33333333333354
    agent-2: 165.33333333333354
    agent-3: 165.33333333333354
    agent-4: 165.33333333333354
    agent-5: 165.33333333333354
  policy_reward_mean:
    agent-0: 118.49666666666698
    agent-1: 118.49666666666698
    agent-2: 118.49666666666698
    agent-3: 118.49666666666698
    agent-4: 118.49666666666698
    agent-5: 118.49666666666698
  policy_reward_min:
    agent-0: 46.33333333333325
    agent-1: 46.33333333333325
    agent-2: 46.33333333333325
    agent-3: 46.33333333333325
    agent-4: 46.33333333333325
    agent-5: 46.33333333333325
  sampler_perf:
    mean_env_wait_ms: 24.37102762580798
    mean_inference_ms: 12.3264482481619
    mean_processing_ms: 51.07860022844254
  time_since_restore: 17440.069581508636
  time_this_iter_s: 121.62988519668579
  time_total_s: 26566.08139538765
  timestamp: 1637041466
  timesteps_since_restore: 12768000
  timesteps_this_iter: 96000
  timesteps_total: 18528000
  training_iteration: 193
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    193 |          26566.1 | 18528000 |   710.98 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 68
    apples_agent-0_mean: 5.15
    apples_agent-0_min: 0
    apples_agent-1_max: 86
    apples_agent-1_mean: 24.99
    apples_agent-1_min: 0
    apples_agent-2_max: 56
    apples_agent-2_mean: 3.65
    apples_agent-2_min: 0
    apples_agent-3_max: 162
    apples_agent-3_mean: 95.1
    apples_agent-3_min: 27
    apples_agent-4_max: 28
    apples_agent-4_mean: 0.79
    apples_agent-4_min: 0
    apples_agent-5_max: 152
    apples_agent-5_mean: 87.22
    apples_agent-5_min: 33
    cleaning_beam_agent-0_max: 417
    cleaning_beam_agent-0_mean: 292.01
    cleaning_beam_agent-0_min: 147
    cleaning_beam_agent-1_max: 467
    cleaning_beam_agent-1_mean: 227.3
    cleaning_beam_agent-1_min: 84
    cleaning_beam_agent-2_max: 561
    cleaning_beam_agent-2_mean: 341.63
    cleaning_beam_agent-2_min: 117
    cleaning_beam_agent-3_max: 94
    cleaning_beam_agent-3_mean: 41.94
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 450
    cleaning_beam_agent-4_mean: 332.95
    cleaning_beam_agent-4_min: 213
    cleaning_beam_agent-5_max: 329
    cleaning_beam_agent-5_mean: 64.77
    cleaning_beam_agent-5_min: 14
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-46-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 994.9999999999791
  episode_reward_mean: 718.5299999999912
  episode_reward_min: 239.99999999999642
  episodes_this_iter: 96
  episodes_total: 18624
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19724.854
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000103852798929438
        entropy: 1.1608505249023438
        entropy_coeff: 0.0017600000137463212
        kl: 0.005612905137240887
        model: {}
        policy_loss: -0.01669541746377945
        total_loss: -0.01589380018413067
        vf_explained_var: 0.028646394610404968
        vf_loss: 17.221332550048828
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000103852798929438
        entropy: 1.1232225894927979
        entropy_coeff: 0.0017600000137463212
        kl: 0.006160759832710028
        model: {}
        policy_loss: -0.01832297444343567
        total_loss: -0.017376068979501724
        vf_explained_var: 0.045710086822509766
        vf_loss: 16.916257858276367
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000103852798929438
        entropy: 1.0219992399215698
        entropy_coeff: 0.0017600000137463212
        kl: 0.005050684791058302
        model: {}
        policy_loss: -0.01530533842742443
        total_loss: -0.014435894787311554
        vf_explained_var: 0.06408093869686127
        vf_loss: 16.58025550842285
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000103852798929438
        entropy: 0.7491093277931213
        entropy_coeff: 0.0017600000137463212
        kl: 0.005509828217327595
        model: {}
        policy_loss: -0.013952618464827538
        total_loss: -0.013283965177834034
        vf_explained_var: 0.18907347321510315
        vf_loss: 14.361019134521484
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000103852798929438
        entropy: 1.017680287361145
        entropy_coeff: 0.0017600000137463212
        kl: 0.005842515267431736
        model: {}
        policy_loss: -0.019088245928287506
        total_loss: -0.01806878112256527
        vf_explained_var: 0.07447445392608643
        vf_loss: 16.420778274536133
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000103852798929438
        entropy: 0.8499593138694763
        entropy_coeff: 0.0017600000137463212
        kl: 0.005282485857605934
        model: {}
        policy_loss: -0.01715818978846073
        total_loss: -0.016055816784501076
        vf_explained_var: 0.12925302982330322
        vf_loss: 15.418066024780273
    load_time_ms: 13158.167
    num_steps_sampled: 18624000
    num_steps_trained: 18624000
    sample_time_ms: 89301.134
    update_time_ms: 20.885
  iterations_since_restore: 134
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.334857142857144
    ram_util_percent: 10.151428571428575
  pid: 24061
  policy_reward_max:
    agent-0: 165.83333333333306
    agent-1: 165.83333333333306
    agent-2: 165.83333333333306
    agent-3: 165.83333333333306
    agent-4: 165.83333333333306
    agent-5: 165.83333333333306
  policy_reward_mean:
    agent-0: 119.7550000000003
    agent-1: 119.7550000000003
    agent-2: 119.7550000000003
    agent-3: 119.7550000000003
    agent-4: 119.7550000000003
    agent-5: 119.7550000000003
  policy_reward_min:
    agent-0: 40.00000000000002
    agent-1: 40.00000000000002
    agent-2: 40.00000000000002
    agent-3: 40.00000000000002
    agent-4: 40.00000000000002
    agent-5: 40.00000000000002
  sampler_perf:
    mean_env_wait_ms: 24.36699887805238
    mean_inference_ms: 12.324806920100528
    mean_processing_ms: 51.072861000964416
  time_since_restore: 17562.90579676628
  time_this_iter_s: 122.83621525764465
  time_total_s: 26688.917610645294
  timestamp: 1637041589
  timesteps_since_restore: 12864000
  timesteps_this_iter: 96000
  timesteps_total: 18624000
  training_iteration: 194
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    194 |          26688.9 | 18624000 |   718.53 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 95
    apples_agent-0_mean: 5.49
    apples_agent-0_min: 0
    apples_agent-1_max: 208
    apples_agent-1_mean: 31.66
    apples_agent-1_min: 0
    apples_agent-2_max: 76
    apples_agent-2_mean: 6.05
    apples_agent-2_min: 0
    apples_agent-3_max: 199
    apples_agent-3_mean: 98.33
    apples_agent-3_min: 29
    apples_agent-4_max: 34
    apples_agent-4_mean: 1.42
    apples_agent-4_min: 0
    apples_agent-5_max: 189
    apples_agent-5_mean: 91.05
    apples_agent-5_min: 18
    cleaning_beam_agent-0_max: 434
    cleaning_beam_agent-0_mean: 316.83
    cleaning_beam_agent-0_min: 108
    cleaning_beam_agent-1_max: 573
    cleaning_beam_agent-1_mean: 221.16
    cleaning_beam_agent-1_min: 103
    cleaning_beam_agent-2_max: 479
    cleaning_beam_agent-2_mean: 336.06
    cleaning_beam_agent-2_min: 140
    cleaning_beam_agent-3_max: 129
    cleaning_beam_agent-3_mean: 46.65
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 425
    cleaning_beam_agent-4_mean: 338.89
    cleaning_beam_agent-4_min: 216
    cleaning_beam_agent-5_max: 172
    cleaning_beam_agent-5_mean: 52.65
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-48-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 963.9999999999693
  episode_reward_mean: 748.9499999999891
  episode_reward_min: 356.00000000000426
  episodes_this_iter: 96
  episodes_total: 18720
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19715.074
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 9.786240116227418e-05
        entropy: 1.1574594974517822
        entropy_coeff: 0.0017600000137463212
        kl: 0.005616649519652128
        model: {}
        policy_loss: -0.01613530144095421
        total_loss: -0.01529490016400814
        vf_explained_var: 0.025063306093215942
        vf_loss: 17.54199981689453
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 9.786240116227418e-05
        entropy: 1.1129200458526611
        entropy_coeff: 0.0017600000137463212
        kl: 0.0056497338227927685
        model: {}
        policy_loss: -0.017917867749929428
        total_loss: -0.01699060946702957
        vf_explained_var: 0.028115347027778625
        vf_loss: 17.560508728027344
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 9.786240116227418e-05
        entropy: 1.032073736190796
        entropy_coeff: 0.0017600000137463212
        kl: 0.005510449875146151
        model: {}
        policy_loss: -0.015922384336590767
        total_loss: -0.014995676465332508
        vf_explained_var: 0.09093970060348511
        vf_loss: 16.410688400268555
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 9.786240116227418e-05
        entropy: 0.7336215376853943
        entropy_coeff: 0.0017600000137463212
        kl: 0.0049979668110609055
        model: {}
        policy_loss: -0.012774961069226265
        total_loss: -0.01206187717616558
        vf_explained_var: 0.16523075103759766
        vf_loss: 15.044585227966309
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 9.786240116227418e-05
        entropy: 1.016129970550537
        entropy_coeff: 0.0017600000137463212
        kl: 0.005873244255781174
        model: {}
        policy_loss: -0.018241748213768005
        total_loss: -0.01713297888636589
        vf_explained_var: 0.04484117031097412
        vf_loss: 17.225109100341797
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 9.786240116227418e-05
        entropy: 0.8395674824714661
        entropy_coeff: 0.0017600000137463212
        kl: 0.0052423011511564255
        model: {}
        policy_loss: -0.017508387565612793
        total_loss: -0.016413666307926178
        vf_explained_var: 0.15363101661205292
        vf_loss: 15.23901081085205
    load_time_ms: 13141.641
    num_steps_sampled: 18720000
    num_steps_trained: 18720000
    sample_time_ms: 89317.32
    update_time_ms: 21.043
  iterations_since_restore: 135
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.374285714285714
    ram_util_percent: 10.169142857142853
  pid: 24061
  policy_reward_max:
    agent-0: 160.6666666666664
    agent-1: 160.6666666666664
    agent-2: 160.6666666666664
    agent-3: 160.6666666666664
    agent-4: 160.6666666666664
    agent-5: 160.6666666666664
  policy_reward_mean:
    agent-0: 124.82500000000034
    agent-1: 124.82500000000034
    agent-2: 124.82500000000034
    agent-3: 124.82500000000034
    agent-4: 124.82500000000034
    agent-5: 124.82500000000034
  policy_reward_min:
    agent-0: 59.33333333333318
    agent-1: 59.33333333333318
    agent-2: 59.33333333333318
    agent-3: 59.33333333333318
    agent-4: 59.33333333333318
    agent-5: 59.33333333333318
  sampler_perf:
    mean_env_wait_ms: 24.363682454717782
    mean_inference_ms: 12.323284494951249
    mean_processing_ms: 51.06623191299764
  time_since_restore: 17685.043800115585
  time_this_iter_s: 122.1380033493042
  time_total_s: 26811.0556139946
  timestamp: 1637041711
  timesteps_since_restore: 12960000
  timesteps_this_iter: 96000
  timesteps_total: 18720000
  training_iteration: 195
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    195 |          26811.1 | 18720000 |   748.95 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 80
    apples_agent-0_mean: 5.96
    apples_agent-0_min: 0
    apples_agent-1_max: 82
    apples_agent-1_mean: 27.35
    apples_agent-1_min: 0
    apples_agent-2_max: 69
    apples_agent-2_mean: 4.1
    apples_agent-2_min: 0
    apples_agent-3_max: 163
    apples_agent-3_mean: 101.49
    apples_agent-3_min: 48
    apples_agent-4_max: 24
    apples_agent-4_mean: 0.87
    apples_agent-4_min: 0
    apples_agent-5_max: 155
    apples_agent-5_mean: 92.14
    apples_agent-5_min: 24
    cleaning_beam_agent-0_max: 443
    cleaning_beam_agent-0_mean: 303.53
    cleaning_beam_agent-0_min: 167
    cleaning_beam_agent-1_max: 448
    cleaning_beam_agent-1_mean: 225.1
    cleaning_beam_agent-1_min: 107
    cleaning_beam_agent-2_max: 522
    cleaning_beam_agent-2_mean: 337.77
    cleaning_beam_agent-2_min: 176
    cleaning_beam_agent-3_max: 143
    cleaning_beam_agent-3_mean: 46.75
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 451
    cleaning_beam_agent-4_mean: 345.89
    cleaning_beam_agent-4_min: 194
    cleaning_beam_agent-5_max: 188
    cleaning_beam_agent-5_mean: 57.52
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-50-33
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 966.9999999999901
  episode_reward_mean: 747.8799999999891
  episode_reward_min: 357.00000000000006
  episodes_this_iter: 96
  episodes_total: 18816
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19699.761
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 9.187200339511037e-05
        entropy: 1.165369987487793
        entropy_coeff: 0.0017600000137463212
        kl: 0.0048452881164848804
        model: {}
        policy_loss: -0.014459249563515186
        total_loss: -0.014004578813910484
        vf_explained_var: 0.04520304501056671
        vf_loss: 15.366643905639648
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 9.187200339511037e-05
        entropy: 1.1290168762207031
        entropy_coeff: 0.0017600000137463212
        kl: 0.0054369959980249405
        model: {}
        policy_loss: -0.01741388998925686
        total_loss: -0.016701461747288704
        vf_explained_var: -0.0016721934080123901
        vf_loss: 16.12098503112793
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 9.187200339511037e-05
        entropy: 1.0273548364639282
        entropy_coeff: 0.0017600000137463212
        kl: 0.004569191951304674
        model: {}
        policy_loss: -0.014543144032359123
        total_loss: -0.013820760883390903
        vf_explained_var: -0.005220919847488403
        vf_loss: 16.166873931884766
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 9.187200339511037e-05
        entropy: 0.7270869016647339
        entropy_coeff: 0.0017600000137463212
        kl: 0.0048965951427817345
        model: {}
        policy_loss: -0.011990305967628956
        total_loss: -0.011606799438595772
        vf_explained_var: 0.11823539435863495
        vf_loss: 14.183511734008789
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 9.187200339511037e-05
        entropy: 1.0099269151687622
        entropy_coeff: 0.0017600000137463212
        kl: 0.005228463560342789
        model: {}
        policy_loss: -0.016875872388482094
        total_loss: -0.016067305579781532
        vf_explained_var: 0.04426635801792145
        vf_loss: 15.403459548950195
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 9.187200339511037e-05
        entropy: 0.8493202924728394
        entropy_coeff: 0.0017600000137463212
        kl: 0.005120804533362389
        model: {}
        policy_loss: -0.015919402241706848
        total_loss: -0.014926357194781303
        vf_explained_var: 0.09146545827388763
        vf_loss: 14.636887550354004
    load_time_ms: 13043.292
    num_steps_sampled: 18816000
    num_steps_trained: 18816000
    sample_time_ms: 89338.427
    update_time_ms: 21.065
  iterations_since_restore: 136
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.416763005780343
    ram_util_percent: 10.185549132947974
  pid: 24061
  policy_reward_max:
    agent-0: 161.16666666666669
    agent-1: 161.16666666666669
    agent-2: 161.16666666666669
    agent-3: 161.16666666666669
    agent-4: 161.16666666666669
    agent-5: 161.16666666666669
  policy_reward_mean:
    agent-0: 124.646666666667
    agent-1: 124.646666666667
    agent-2: 124.646666666667
    agent-3: 124.646666666667
    agent-4: 124.646666666667
    agent-5: 124.646666666667
  policy_reward_min:
    agent-0: 59.49999999999993
    agent-1: 59.49999999999993
    agent-2: 59.49999999999993
    agent-3: 59.49999999999993
    agent-4: 59.49999999999993
    agent-5: 59.49999999999993
  sampler_perf:
    mean_env_wait_ms: 24.360315341763158
    mean_inference_ms: 12.321457572302233
    mean_processing_ms: 51.05928982684302
  time_since_restore: 17806.66307926178
  time_this_iter_s: 121.61927914619446
  time_total_s: 26932.674893140793
  timestamp: 1637041833
  timesteps_since_restore: 13056000
  timesteps_this_iter: 96000
  timesteps_total: 18816000
  training_iteration: 196
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    196 |          26932.7 | 18816000 |   747.88 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 53
    apples_agent-0_mean: 5.2
    apples_agent-0_min: 0
    apples_agent-1_max: 92
    apples_agent-1_mean: 28.28
    apples_agent-1_min: 0
    apples_agent-2_max: 66
    apples_agent-2_mean: 6.61
    apples_agent-2_min: 0
    apples_agent-3_max: 139
    apples_agent-3_mean: 98.35
    apples_agent-3_min: 49
    apples_agent-4_max: 53
    apples_agent-4_mean: 2.42
    apples_agent-4_min: 0
    apples_agent-5_max: 195
    apples_agent-5_mean: 91.97
    apples_agent-5_min: 25
    cleaning_beam_agent-0_max: 466
    cleaning_beam_agent-0_mean: 309.7
    cleaning_beam_agent-0_min: 147
    cleaning_beam_agent-1_max: 428
    cleaning_beam_agent-1_mean: 226.56
    cleaning_beam_agent-1_min: 123
    cleaning_beam_agent-2_max: 507
    cleaning_beam_agent-2_mean: 349.31
    cleaning_beam_agent-2_min: 115
    cleaning_beam_agent-3_max: 107
    cleaning_beam_agent-3_mean: 43.24
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 461
    cleaning_beam_agent-4_mean: 341.78
    cleaning_beam_agent-4_min: 220
    cleaning_beam_agent-5_max: 142
    cleaning_beam_agent-5_mean: 52.24
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-52-35
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 984.999999999976
  episode_reward_mean: 753.7499999999887
  episode_reward_min: 408.0000000000054
  episodes_this_iter: 96
  episodes_total: 18912
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19696.759
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 8.588159835198894e-05
        entropy: 1.1483092308044434
        entropy_coeff: 0.0017600000137463212
        kl: 0.00572233647108078
        model: {}
        policy_loss: -0.014555169269442558
        total_loss: -0.014386561699211597
        vf_explained_var: 0.05672819912433624
        vf_loss: 16.173995971679688
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 8.588159835198894e-05
        entropy: 1.119979977607727
        entropy_coeff: 0.0017600000137463212
        kl: 0.005505451001226902
        model: {}
        policy_loss: -0.017106246203184128
        total_loss: -0.01628948375582695
        vf_explained_var: 0.017084062099456787
        vf_loss: 16.868362426757812
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 8.588159835198894e-05
        entropy: 1.018874168395996
        entropy_coeff: 0.0017600000137463212
        kl: 0.005415081046521664
        model: {}
        policy_loss: -0.014643652364611626
        total_loss: -0.014214380644261837
        vf_explained_var: 0.02295982837677002
        vf_loss: 16.8098201751709
      agent-3:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 8.588159835198894e-05
        entropy: 0.7040523290634155
        entropy_coeff: 0.0017600000137463212
        kl: 0.00497399689629674
        model: {}
        policy_loss: -0.011592257767915726
        total_loss: -0.011244174093008041
        vf_explained_var: 0.14768941700458527
        vf_loss: 14.628665924072266
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 8.588159835198894e-05
        entropy: 1.022748351097107
        entropy_coeff: 0.0017600000137463212
        kl: 0.004937102552503347
        model: {}
        policy_loss: -0.016415473073720932
        total_loss: -0.015612892806529999
        vf_explained_var: 0.05990822613239288
        vf_loss: 16.15199089050293
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 8.588159835198894e-05
        entropy: 0.845946729183197
        entropy_coeff: 0.0017600000137463212
        kl: 0.004829375538975
        model: {}
        policy_loss: -0.015602878294885159
        total_loss: -0.0146596385166049
        vf_explained_var: 0.1477348357439041
        vf_loss: 14.662343978881836
    load_time_ms: 13061.537
    num_steps_sampled: 18912000
    num_steps_trained: 18912000
    sample_time_ms: 89234.352
    update_time_ms: 21.042
  iterations_since_restore: 137
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.314285714285715
    ram_util_percent: 10.161142857142854
  pid: 24061
  policy_reward_max:
    agent-0: 164.16666666666612
    agent-1: 164.16666666666612
    agent-2: 164.16666666666612
    agent-3: 164.16666666666612
    agent-4: 164.16666666666612
    agent-5: 164.16666666666612
  policy_reward_mean:
    agent-0: 125.62500000000033
    agent-1: 125.62500000000033
    agent-2: 125.62500000000033
    agent-3: 125.62500000000033
    agent-4: 125.62500000000033
    agent-5: 125.62500000000033
  policy_reward_min:
    agent-0: 67.99999999999999
    agent-1: 67.99999999999999
    agent-2: 67.99999999999999
    agent-3: 67.99999999999999
    agent-4: 67.99999999999999
    agent-5: 67.99999999999999
  sampler_perf:
    mean_env_wait_ms: 24.35654272217007
    mean_inference_ms: 12.319454806980566
    mean_processing_ms: 51.05226797284007
  time_since_restore: 17929.09720516205
  time_this_iter_s: 122.43412590026855
  time_total_s: 27055.10901904106
  timestamp: 1637041955
  timesteps_since_restore: 13152000
  timesteps_this_iter: 96000
  timesteps_total: 18912000
  training_iteration: 197
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    197 |          27055.1 | 18912000 |   753.75 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 53
    apples_agent-0_mean: 4.39
    apples_agent-0_min: 0
    apples_agent-1_max: 109
    apples_agent-1_mean: 29.05
    apples_agent-1_min: 0
    apples_agent-2_max: 59
    apples_agent-2_mean: 5.12
    apples_agent-2_min: 0
    apples_agent-3_max: 165
    apples_agent-3_mean: 97.36
    apples_agent-3_min: 47
    apples_agent-4_max: 51
    apples_agent-4_mean: 1.17
    apples_agent-4_min: 0
    apples_agent-5_max: 143
    apples_agent-5_mean: 87.54
    apples_agent-5_min: 34
    cleaning_beam_agent-0_max: 432
    cleaning_beam_agent-0_mean: 304.29
    cleaning_beam_agent-0_min: 159
    cleaning_beam_agent-1_max: 403
    cleaning_beam_agent-1_mean: 223.76
    cleaning_beam_agent-1_min: 94
    cleaning_beam_agent-2_max: 479
    cleaning_beam_agent-2_mean: 332.53
    cleaning_beam_agent-2_min: 101
    cleaning_beam_agent-3_max: 134
    cleaning_beam_agent-3_mean: 43.41
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 443
    cleaning_beam_agent-4_mean: 337.28
    cleaning_beam_agent-4_min: 199
    cleaning_beam_agent-5_max: 214
    cleaning_beam_agent-5_mean: 61.49
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-54-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 955.9999999999785
  episode_reward_mean: 735.509999999991
  episode_reward_min: 438.00000000001
  episodes_this_iter: 96
  episodes_total: 19008
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19696.12
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 7.989120058482513e-05
        entropy: 1.1644437313079834
        entropy_coeff: 0.0017600000137463212
        kl: 0.005805386230349541
        model: {}
        policy_loss: -0.014315363019704819
        total_loss: -0.014217203482985497
        vf_explained_var: 0.042423248291015625
        vf_loss: 15.670417785644531
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 7.989120058482513e-05
        entropy: 1.1044141054153442
        entropy_coeff: 0.0017600000137463212
        kl: 0.004853166174143553
        model: {}
        policy_loss: -0.015812112018465996
        total_loss: -0.015144526958465576
        vf_explained_var: -0.002001330256462097
        vf_loss: 16.407161712646484
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 7.989120058482513e-05
        entropy: 1.0257062911987305
        entropy_coeff: 0.0017600000137463212
        kl: 0.005939492490142584
        model: {}
        policy_loss: -0.014452050440013409
        total_loss: -0.0140644870698452
        vf_explained_var: 0.02359415590763092
        vf_loss: 15.988534927368164
      agent-3:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 7.989120058482513e-05
        entropy: 0.731887936592102
        entropy_coeff: 0.0017600000137463212
        kl: 0.004895749967545271
        model: {}
        policy_loss: -0.011867436580359936
        total_loss: -0.011715451255440712
        vf_explained_var: 0.15805548429489136
        vf_loss: 13.789117813110352
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 7.989120058482513e-05
        entropy: 1.007891058921814
        entropy_coeff: 0.0017600000137463212
        kl: 0.005694007966667414
        model: {}
        policy_loss: -0.016461394727230072
        total_loss: -0.016091041266918182
        vf_explained_var: 0.03754331171512604
        vf_loss: 15.748392105102539
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 7.989120058482513e-05
        entropy: 0.848939836025238
        entropy_coeff: 0.0017600000137463212
        kl: 0.005033161491155624
        model: {}
        policy_loss: -0.0151478610932827
        total_loss: -0.01465597003698349
        vf_explained_var: 0.093315988779068
        vf_loss: 14.827062606811523
    load_time_ms: 13065.46
    num_steps_sampled: 19008000
    num_steps_trained: 19008000
    sample_time_ms: 89235.737
    update_time_ms: 21.287
  iterations_since_restore: 138
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.332758620689656
    ram_util_percent: 10.076436781609198
  pid: 24061
  policy_reward_max:
    agent-0: 159.3333333333331
    agent-1: 159.3333333333331
    agent-2: 159.3333333333331
    agent-3: 159.3333333333331
    agent-4: 159.3333333333331
    agent-5: 159.3333333333331
  policy_reward_mean:
    agent-0: 122.58500000000035
    agent-1: 122.58500000000035
    agent-2: 122.58500000000035
    agent-3: 122.58500000000035
    agent-4: 122.58500000000035
    agent-5: 122.58500000000035
  policy_reward_min:
    agent-0: 73.00000000000006
    agent-1: 73.00000000000006
    agent-2: 73.00000000000006
    agent-3: 73.00000000000006
    agent-4: 73.00000000000006
    agent-5: 73.00000000000006
  sampler_perf:
    mean_env_wait_ms: 24.352056022050114
    mean_inference_ms: 12.317503905419665
    mean_processing_ms: 51.0441779450079
  time_since_restore: 18051.177082061768
  time_this_iter_s: 122.07987689971924
  time_total_s: 27177.18889594078
  timestamp: 1637042078
  timesteps_since_restore: 13248000
  timesteps_this_iter: 96000
  timesteps_total: 19008000
  training_iteration: 198
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    198 |          27177.2 | 19008000 |   735.51 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 55
    apples_agent-0_mean: 4.03
    apples_agent-0_min: 0
    apples_agent-1_max: 104
    apples_agent-1_mean: 28.66
    apples_agent-1_min: 0
    apples_agent-2_max: 80
    apples_agent-2_mean: 4.25
    apples_agent-2_min: 0
    apples_agent-3_max: 174
    apples_agent-3_mean: 100.48
    apples_agent-3_min: 39
    apples_agent-4_max: 55
    apples_agent-4_mean: 1.89
    apples_agent-4_min: 0
    apples_agent-5_max: 138
    apples_agent-5_mean: 88.02
    apples_agent-5_min: 8
    cleaning_beam_agent-0_max: 447
    cleaning_beam_agent-0_mean: 310.88
    cleaning_beam_agent-0_min: 181
    cleaning_beam_agent-1_max: 413
    cleaning_beam_agent-1_mean: 231.26
    cleaning_beam_agent-1_min: 83
    cleaning_beam_agent-2_max: 546
    cleaning_beam_agent-2_mean: 333.47
    cleaning_beam_agent-2_min: 141
    cleaning_beam_agent-3_max: 113
    cleaning_beam_agent-3_mean: 37.15
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 448
    cleaning_beam_agent-4_mean: 340.6
    cleaning_beam_agent-4_min: 174
    cleaning_beam_agent-5_max: 182
    cleaning_beam_agent-5_mean: 60.4
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-56-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 996.9999999999857
  episode_reward_mean: 747.8299999999898
  episode_reward_min: 303.9999999999997
  episodes_this_iter: 96
  episodes_total: 19104
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19669.595
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 7.390080281766132e-05
        entropy: 1.1567723751068115
        entropy_coeff: 0.0017600000137463212
        kl: 0.0050346325151622295
        model: {}
        policy_loss: -0.013772930018603802
        total_loss: -0.013727683573961258
        vf_explained_var: 0.054203927516937256
        vf_loss: 15.777058601379395
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 7.390080281766132e-05
        entropy: 1.1279878616333008
        entropy_coeff: 0.0017600000137463212
        kl: 0.005667337216436863
        model: {}
        policy_loss: -0.015544619411230087
        total_loss: -0.01535988412797451
        vf_explained_var: 0.04026445746421814
        vf_loss: 16.0325927734375
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 7.390080281766132e-05
        entropy: 1.031855821609497
        entropy_coeff: 0.0017600000137463212
        kl: 0.004940364509820938
        model: {}
        policy_loss: -0.01397712156176567
        total_loss: -0.013661457225680351
        vf_explained_var: 0.021854162216186523
        vf_loss: 16.376953125
      agent-3:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 7.390080281766132e-05
        entropy: 0.6981165409088135
        entropy_coeff: 0.0017600000137463212
        kl: 0.005079053342342377
        model: {}
        policy_loss: -0.011263025924563408
        total_loss: -0.011053064838051796
        vf_explained_var: 0.15607643127441406
        vf_loss: 14.068997383117676
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 7.390080281766132e-05
        entropy: 1.0063278675079346
        entropy_coeff: 0.0017600000137463212
        kl: 0.005618511699140072
        model: {}
        policy_loss: -0.01562204398214817
        total_loss: -0.015209253877401352
        vf_explained_var: 0.029050558805465698
        vf_loss: 16.22077751159668
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 7.390080281766132e-05
        entropy: 0.8509237170219421
        entropy_coeff: 0.0017600000137463212
        kl: 0.0051257857121527195
        model: {}
        policy_loss: -0.014838812872767448
        total_loss: -0.01431557722389698
        vf_explained_var: 0.09779514372348785
        vf_loss: 15.08285140991211
    load_time_ms: 13074.3
    num_steps_sampled: 19104000
    num_steps_trained: 19104000
    sample_time_ms: 89281.696
    update_time_ms: 21.251
  iterations_since_restore: 139
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.367816091954023
    ram_util_percent: 10.1683908045977
  pid: 24061
  policy_reward_max:
    agent-0: 166.1666666666668
    agent-1: 166.1666666666668
    agent-2: 166.1666666666668
    agent-3: 166.1666666666668
    agent-4: 166.1666666666668
    agent-5: 166.1666666666668
  policy_reward_mean:
    agent-0: 124.63833333333366
    agent-1: 124.63833333333366
    agent-2: 124.63833333333366
    agent-3: 124.63833333333366
    agent-4: 124.63833333333366
    agent-5: 124.63833333333366
  policy_reward_min:
    agent-0: 50.66666666666649
    agent-1: 50.66666666666649
    agent-2: 50.66666666666649
    agent-3: 50.66666666666649
    agent-4: 50.66666666666649
    agent-5: 50.66666666666649
  sampler_perf:
    mean_env_wait_ms: 24.34737145006602
    mean_inference_ms: 12.316330703029823
    mean_processing_ms: 51.03842729632333
  time_since_restore: 18173.47080397606
  time_this_iter_s: 122.29372191429138
  time_total_s: 27299.482617855072
  timestamp: 1637042200
  timesteps_since_restore: 13344000
  timesteps_this_iter: 96000
  timesteps_total: 19104000
  training_iteration: 199
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    199 |          27299.5 | 19104000 |   747.83 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 219
    apples_agent-0_mean: 6.76
    apples_agent-0_min: 0
    apples_agent-1_max: 119
    apples_agent-1_mean: 28.0
    apples_agent-1_min: 0
    apples_agent-2_max: 33
    apples_agent-2_mean: 2.28
    apples_agent-2_min: 0
    apples_agent-3_max: 144
    apples_agent-3_mean: 98.29
    apples_agent-3_min: 38
    apples_agent-4_max: 31
    apples_agent-4_mean: 0.7
    apples_agent-4_min: 0
    apples_agent-5_max: 142
    apples_agent-5_mean: 89.2
    apples_agent-5_min: 24
    cleaning_beam_agent-0_max: 475
    cleaning_beam_agent-0_mean: 300.88
    cleaning_beam_agent-0_min: 131
    cleaning_beam_agent-1_max: 409
    cleaning_beam_agent-1_mean: 218.58
    cleaning_beam_agent-1_min: 110
    cleaning_beam_agent-2_max: 507
    cleaning_beam_agent-2_mean: 345.6
    cleaning_beam_agent-2_min: 175
    cleaning_beam_agent-3_max: 211
    cleaning_beam_agent-3_mean: 40.24
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 448
    cleaning_beam_agent-4_mean: 346.44
    cleaning_beam_agent-4_min: 238
    cleaning_beam_agent-5_max: 200
    cleaning_beam_agent-5_mean: 52.21
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-58-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 953.9999999999727
  episode_reward_mean: 752.8199999999887
  episode_reward_min: 359.00000000000074
  episodes_this_iter: 96
  episodes_total: 19200
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19709.718
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 6.791039777453989e-05
        entropy: 1.149107813835144
        entropy_coeff: 0.0017600000137463212
        kl: 0.005213249009102583
        model: {}
        policy_loss: -0.01340274978429079
        total_loss: -0.013256359845399857
        vf_explained_var: 0.04181253910064697
        vf_loss: 16.474945068359375
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 6.791039777453989e-05
        entropy: 1.114717721939087
        entropy_coeff: 0.0017600000137463212
        kl: 0.005848056171089411
        model: {}
        policy_loss: -0.015814276412129402
        total_loss: -0.015494697727262974
        vf_explained_var: 0.014439791440963745
        vf_loss: 16.96678924560547
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 6.791039777453989e-05
        entropy: 1.035048007965088
        entropy_coeff: 0.0017600000137463212
        kl: 0.005287863779813051
        model: {}
        policy_loss: -0.013820435851812363
        total_loss: -0.013752685859799385
        vf_explained_var: 0.05468423664569855
        vf_loss: 16.250446319580078
      agent-3:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 6.791039777453989e-05
        entropy: 0.7198004722595215
        entropy_coeff: 0.0017600000137463212
        kl: 0.004870874807238579
        model: {}
        policy_loss: -0.010875880718231201
        total_loss: -0.010635536164045334
        vf_explained_var: 0.14215540885925293
        vf_loss: 14.767515182495117
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 6.791039777453989e-05
        entropy: 1.0109763145446777
        entropy_coeff: 0.0017600000137463212
        kl: 0.005154788959771395
        model: {}
        policy_loss: -0.014518829993903637
        total_loss: -0.014123301021754742
        vf_explained_var: 0.03504045307636261
        vf_loss: 16.593734741210938
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 6.791039777453989e-05
        entropy: 0.8358171582221985
        entropy_coeff: 0.0017600000137463212
        kl: 0.00436590239405632
        model: {}
        policy_loss: -0.013163439929485321
        total_loss: -0.012750482186675072
        vf_explained_var: 0.1578771024942398
        vf_loss: 14.474077224731445
    load_time_ms: 13084.251
    num_steps_sampled: 19200000
    num_steps_trained: 19200000
    sample_time_ms: 89264.454
    update_time_ms: 21.523
  iterations_since_restore: 140
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.285714285714286
    ram_util_percent: 10.155428571428573
  pid: 24061
  policy_reward_max:
    agent-0: 158.99999999999966
    agent-1: 158.99999999999966
    agent-2: 158.99999999999966
    agent-3: 158.99999999999966
    agent-4: 158.99999999999966
    agent-5: 158.99999999999966
  policy_reward_mean:
    agent-0: 125.47000000000031
    agent-1: 125.47000000000031
    agent-2: 125.47000000000031
    agent-3: 125.47000000000031
    agent-4: 125.47000000000031
    agent-5: 125.47000000000031
  policy_reward_min:
    agent-0: 59.83333333333318
    agent-1: 59.83333333333318
    agent-2: 59.83333333333318
    agent-3: 59.83333333333318
    agent-4: 59.83333333333318
    agent-5: 59.83333333333318
  sampler_perf:
    mean_env_wait_ms: 24.342753591157745
    mean_inference_ms: 12.314490391445222
    mean_processing_ms: 51.031609463850124
  time_since_restore: 18295.834898471832
  time_this_iter_s: 122.36409449577332
  time_total_s: 27421.846712350845
  timestamp: 1637042323
  timesteps_since_restore: 13440000
  timesteps_this_iter: 96000
  timesteps_total: 19200000
  training_iteration: 200
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    200 |          27421.8 | 19200000 |   752.82 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 81
    apples_agent-0_mean: 6.33
    apples_agent-0_min: 0
    apples_agent-1_max: 110
    apples_agent-1_mean: 27.53
    apples_agent-1_min: 0
    apples_agent-2_max: 35
    apples_agent-2_mean: 3.25
    apples_agent-2_min: 0
    apples_agent-3_max: 172
    apples_agent-3_mean: 94.31
    apples_agent-3_min: 27
    apples_agent-4_max: 37
    apples_agent-4_mean: 1.89
    apples_agent-4_min: 0
    apples_agent-5_max: 142
    apples_agent-5_mean: 89.46
    apples_agent-5_min: 20
    cleaning_beam_agent-0_max: 450
    cleaning_beam_agent-0_mean: 299.28
    cleaning_beam_agent-0_min: 148
    cleaning_beam_agent-1_max: 386
    cleaning_beam_agent-1_mean: 219.35
    cleaning_beam_agent-1_min: 60
    cleaning_beam_agent-2_max: 494
    cleaning_beam_agent-2_mean: 357.08
    cleaning_beam_agent-2_min: 172
    cleaning_beam_agent-3_max: 142
    cleaning_beam_agent-3_mean: 40.37
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 464
    cleaning_beam_agent-4_mean: 341.37
    cleaning_beam_agent-4_min: 216
    cleaning_beam_agent-5_max: 225
    cleaning_beam_agent-5_mean: 50.29
    cleaning_beam_agent-5_min: 12
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-00-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 967.9999999999801
  episode_reward_mean: 763.8499999999888
  episode_reward_min: 200.99999999999685
  episodes_this_iter: 96
  episodes_total: 19296
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19662.861
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 6.192000000737607e-05
        entropy: 1.1505014896392822
        entropy_coeff: 0.0017600000137463212
        kl: 0.004674585070461035
        model: {}
        policy_loss: -0.01197932381182909
        total_loss: -0.011653438210487366
        vf_explained_var: 0.006883889436721802
        vf_loss: 18.83309555053711
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 6.192000000737607e-05
        entropy: 1.1192295551300049
        entropy_coeff: 0.0017600000137463212
        kl: 0.004776519723236561
        model: {}
        policy_loss: -0.013897566124796867
        total_loss: -0.013457471504807472
        vf_explained_var: -0.021135151386260986
        vf_loss: 19.3228816986084
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 6.192000000737607e-05
        entropy: 1.0176531076431274
        entropy_coeff: 0.0017600000137463212
        kl: 0.0049341535195708275
        model: {}
        policy_loss: -0.011646907776594162
        total_loss: -0.011385258287191391
        vf_explained_var: 0.04504367709159851
        vf_loss: 18.06007194519043
      agent-3:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 6.192000000737607e-05
        entropy: 0.7122522592544556
        entropy_coeff: 0.0017600000137463212
        kl: 0.003981541842222214
        model: {}
        policy_loss: -0.009516259655356407
        total_loss: -0.009196460247039795
        vf_explained_var: 0.17270810902118683
        vf_loss: 15.609223365783691
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 6.192000000737607e-05
        entropy: 1.001155138015747
        entropy_coeff: 0.0017600000137463212
        kl: 0.004544451367110014
        model: {}
        policy_loss: -0.01351239439100027
        total_loss: -0.01306244544684887
        vf_explained_var: 0.07059818506240845
        vf_loss: 17.575408935546875
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 6.192000000737607e-05
        entropy: 0.8377869129180908
        entropy_coeff: 0.0017600000137463212
        kl: 0.004886310081928968
        model: {}
        policy_loss: -0.013351141475141048
        total_loss: -0.012927308678627014
        vf_explained_var: 0.12544666230678558
        vf_loss: 16.540231704711914
    load_time_ms: 13061.062
    num_steps_sampled: 19296000
    num_steps_trained: 19296000
    sample_time_ms: 89356.532
    update_time_ms: 20.876
  iterations_since_restore: 141
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.842285714285712
    ram_util_percent: 10.234285714285711
  pid: 24061
  policy_reward_max:
    agent-0: 161.33333333333294
    agent-1: 161.33333333333294
    agent-2: 161.33333333333294
    agent-3: 161.33333333333294
    agent-4: 161.33333333333294
    agent-5: 161.33333333333294
  policy_reward_mean:
    agent-0: 127.30833333333358
    agent-1: 127.30833333333358
    agent-2: 127.30833333333358
    agent-3: 127.30833333333358
    agent-4: 127.30833333333358
    agent-5: 127.30833333333358
  policy_reward_min:
    agent-0: 33.50000000000006
    agent-1: 33.50000000000006
    agent-2: 33.50000000000006
    agent-3: 33.50000000000006
    agent-4: 33.50000000000006
    agent-5: 33.50000000000006
  sampler_perf:
    mean_env_wait_ms: 24.339154398121764
    mean_inference_ms: 12.312853562485925
    mean_processing_ms: 51.025869158263404
  time_since_restore: 18418.245426654816
  time_this_iter_s: 122.4105281829834
  time_total_s: 27544.25724053383
  timestamp: 1637042446
  timesteps_since_restore: 13536000
  timesteps_this_iter: 96000
  timesteps_total: 19296000
  training_iteration: 201
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    201 |          27544.3 | 19296000 |   763.85 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 39
    apples_agent-0_mean: 4.35
    apples_agent-0_min: 0
    apples_agent-1_max: 122
    apples_agent-1_mean: 32.44
    apples_agent-1_min: 0
    apples_agent-2_max: 86
    apples_agent-2_mean: 4.13
    apples_agent-2_min: 0
    apples_agent-3_max: 146
    apples_agent-3_mean: 96.52
    apples_agent-3_min: 46
    apples_agent-4_max: 51
    apples_agent-4_mean: 1.45
    apples_agent-4_min: 0
    apples_agent-5_max: 135
    apples_agent-5_mean: 87.1
    apples_agent-5_min: 10
    cleaning_beam_agent-0_max: 448
    cleaning_beam_agent-0_mean: 312.65
    cleaning_beam_agent-0_min: 162
    cleaning_beam_agent-1_max: 467
    cleaning_beam_agent-1_mean: 227.03
    cleaning_beam_agent-1_min: 124
    cleaning_beam_agent-2_max: 522
    cleaning_beam_agent-2_mean: 350.35
    cleaning_beam_agent-2_min: 175
    cleaning_beam_agent-3_max: 140
    cleaning_beam_agent-3_mean: 40.74
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 465
    cleaning_beam_agent-4_mean: 352.01
    cleaning_beam_agent-4_min: 189
    cleaning_beam_agent-5_max: 391
    cleaning_beam_agent-5_mean: 51.79
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-02-48
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 979.9999999999865
  episode_reward_mean: 772.3899999999888
  episode_reward_min: 294.99999999999943
  episodes_this_iter: 96
  episodes_total: 19392
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19662.969
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 5.5929598602233455e-05
        entropy: 1.1409107446670532
        entropy_coeff: 0.0017600000137463212
        kl: 0.00471842847764492
        model: {}
        policy_loss: -0.011262880638241768
        total_loss: -0.011431762017309666
        vf_explained_var: 0.040053561329841614
        vf_loss: 16.03201675415039
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 5.5929598602233455e-05
        entropy: 1.1139583587646484
        entropy_coeff: 0.0017600000137463212
        kl: 0.004915648140013218
        model: {}
        policy_loss: -0.013090632855892181
        total_loss: -0.01307196356356144
        vf_explained_var: -0.03661850094795227
        vf_loss: 17.334495544433594
      agent-2:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 5.5929598602233455e-05
        entropy: 1.026056170463562
        entropy_coeff: 0.0017600000137463212
        kl: 0.005024104379117489
        model: {}
        policy_loss: -0.011331195011734962
        total_loss: -0.011445684358477592
        vf_explained_var: 0.060970962047576904
        vf_loss: 15.657699584960938
      agent-3:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 5.5929598602233455e-05
        entropy: 0.7072316408157349
        entropy_coeff: 0.0017600000137463212
        kl: 0.004441030789166689
        model: {}
        policy_loss: -0.009176314808428288
        total_loss: -0.008938427083194256
        vf_explained_var: 0.11393828690052032
        vf_loss: 14.756752967834473
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 5.5929598602233455e-05
        entropy: 0.9981061816215515
        entropy_coeff: 0.0017600000137463212
        kl: 0.005315684247761965
        model: {}
        policy_loss: -0.012598982080817223
        total_loss: -0.012462135404348373
        vf_explained_var: 0.02452176809310913
        vf_loss: 16.27731704711914
      agent-5:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 5.5929598602233455e-05
        entropy: 0.8321801424026489
        entropy_coeff: 0.0017600000137463212
        kl: 0.004828665405511856
        model: {}
        policy_loss: -0.012413311749696732
        total_loss: -0.012253073044121265
        vf_explained_var: 0.09855854511260986
        vf_loss: 15.04158878326416
    load_time_ms: 13082.836
    num_steps_sampled: 19392000
    num_steps_trained: 19392000
    sample_time_ms: 89376.01
    update_time_ms: 20.593
  iterations_since_restore: 142
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.357714285714282
    ram_util_percent: 10.10342857142857
  pid: 24061
  policy_reward_max:
    agent-0: 163.33333333333294
    agent-1: 163.33333333333294
    agent-2: 163.33333333333294
    agent-3: 163.33333333333294
    agent-4: 163.33333333333294
    agent-5: 163.33333333333294
  policy_reward_mean:
    agent-0: 128.7316666666669
    agent-1: 128.7316666666669
    agent-2: 128.7316666666669
    agent-3: 128.7316666666669
    agent-4: 128.7316666666669
    agent-5: 128.7316666666669
  policy_reward_min:
    agent-0: 49.166666666666536
    agent-1: 49.166666666666536
    agent-2: 49.166666666666536
    agent-3: 49.166666666666536
    agent-4: 49.166666666666536
    agent-5: 49.166666666666536
  sampler_perf:
    mean_env_wait_ms: 24.336701919962987
    mean_inference_ms: 12.311153889274872
    mean_processing_ms: 51.02053810734572
  time_since_restore: 18540.85526752472
  time_this_iter_s: 122.60984086990356
  time_total_s: 27666.867081403732
  timestamp: 1637042568
  timesteps_since_restore: 13632000
  timesteps_this_iter: 96000
  timesteps_total: 19392000
  training_iteration: 202
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    202 |          27666.9 | 19392000 |   772.39 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 120
    apples_agent-0_mean: 5.64
    apples_agent-0_min: 0
    apples_agent-1_max: 102
    apples_agent-1_mean: 26.85
    apples_agent-1_min: 0
    apples_agent-2_max: 81
    apples_agent-2_mean: 4.09
    apples_agent-2_min: 0
    apples_agent-3_max: 160
    apples_agent-3_mean: 101.06
    apples_agent-3_min: 21
    apples_agent-4_max: 36
    apples_agent-4_mean: 1.8
    apples_agent-4_min: 0
    apples_agent-5_max: 147
    apples_agent-5_mean: 91.02
    apples_agent-5_min: 33
    cleaning_beam_agent-0_max: 435
    cleaning_beam_agent-0_mean: 310.55
    cleaning_beam_agent-0_min: 146
    cleaning_beam_agent-1_max: 434
    cleaning_beam_agent-1_mean: 220.22
    cleaning_beam_agent-1_min: 98
    cleaning_beam_agent-2_max: 558
    cleaning_beam_agent-2_mean: 354.44
    cleaning_beam_agent-2_min: 90
    cleaning_beam_agent-3_max: 228
    cleaning_beam_agent-3_mean: 43.5
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 478
    cleaning_beam_agent-4_mean: 346.44
    cleaning_beam_agent-4_min: 179
    cleaning_beam_agent-5_max: 156
    cleaning_beam_agent-5_mean: 45.62
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-04-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 975.9999999999839
  episode_reward_mean: 759.0099999999879
  episode_reward_min: 352.00000000000574
  episodes_this_iter: 96
  episodes_total: 19488
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19626.504
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 4.993920083506964e-05
        entropy: 1.1360409259796143
        entropy_coeff: 0.0017600000137463212
        kl: 0.004665137734264135
        model: {}
        policy_loss: -0.010976752266287804
        total_loss: -0.011140672490000725
        vf_explained_var: 0.03426623344421387
        vf_loss: 17.188819885253906
      agent-1:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 4.993920083506964e-05
        entropy: 1.1058127880096436
        entropy_coeff: 0.0017600000137463212
        kl: 0.005126353353261948
        model: {}
        policy_loss: -0.0123191699385643
        total_loss: -0.012304768897593021
        vf_explained_var: -0.029817402362823486
        vf_loss: 18.324710845947266
      agent-2:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 4.993920083506964e-05
        entropy: 1.0052008628845215
        entropy_coeff: 0.0017600000137463212
        kl: 0.004319594241678715
        model: {}
        policy_loss: -0.010457961820065975
        total_loss: -0.010463272221386433
        vf_explained_var: 0.06909316778182983
        vf_loss: 16.558517456054688
      agent-3:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 4.993920083506964e-05
        entropy: 0.711728036403656
        entropy_coeff: 0.0017600000137463212
        kl: 0.0040847910568118095
        model: {}
        policy_loss: -0.009028062224388123
        total_loss: -0.008840184658765793
        vf_explained_var: 0.19249732792377472
        vf_loss: 14.373289108276367
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 4.993920083506964e-05
        entropy: 0.9973156452178955
        entropy_coeff: 0.0017600000137463212
        kl: 0.004598027095198631
        model: {}
        policy_loss: -0.011450071819126606
        total_loss: -0.011300644837319851
        vf_explained_var: 0.05993473529815674
        vf_loss: 16.747997283935547
      agent-5:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 4.993920083506964e-05
        entropy: 0.8485748171806335
        entropy_coeff: 0.0017600000137463212
        kl: 0.00449150986969471
        model: {}
        policy_loss: -0.011514793150126934
        total_loss: -0.011355225928127766
        vf_explained_var: 0.10263073444366455
        vf_loss: 15.969121932983398
    load_time_ms: 13065.003
    num_steps_sampled: 19488000
    num_steps_trained: 19488000
    sample_time_ms: 89531.07
    update_time_ms: 21.31
  iterations_since_restore: 143
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.390857142857145
    ram_util_percent: 10.206857142857144
  pid: 24061
  policy_reward_max:
    agent-0: 162.66666666666686
    agent-1: 162.66666666666686
    agent-2: 162.66666666666686
    agent-3: 162.66666666666686
    agent-4: 162.66666666666686
    agent-5: 162.66666666666686
  policy_reward_mean:
    agent-0: 126.50166666666699
    agent-1: 126.50166666666699
    agent-2: 126.50166666666699
    agent-3: 126.50166666666699
    agent-4: 126.50166666666699
    agent-5: 126.50166666666699
  policy_reward_min:
    agent-0: 58.66666666666643
    agent-1: 58.66666666666643
    agent-2: 58.66666666666643
    agent-3: 58.66666666666643
    agent-4: 58.66666666666643
    agent-5: 58.66666666666643
  sampler_perf:
    mean_env_wait_ms: 24.334316316627763
    mean_inference_ms: 12.309697730234843
    mean_processing_ms: 51.016111872215916
  time_since_restore: 18663.48234796524
  time_this_iter_s: 122.62708044052124
  time_total_s: 27789.494161844254
  timestamp: 1637042691
  timesteps_since_restore: 13728000
  timesteps_this_iter: 96000
  timesteps_total: 19488000
  training_iteration: 203
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    203 |          27789.5 | 19488000 |   759.01 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 47
    apples_agent-0_mean: 5.31
    apples_agent-0_min: 0
    apples_agent-1_max: 92
    apples_agent-1_mean: 32.38
    apples_agent-1_min: 0
    apples_agent-2_max: 173
    apples_agent-2_mean: 4.91
    apples_agent-2_min: 0
    apples_agent-3_max: 145
    apples_agent-3_mean: 98.33
    apples_agent-3_min: 35
    apples_agent-4_max: 45
    apples_agent-4_mean: 1.26
    apples_agent-4_min: 0
    apples_agent-5_max: 134
    apples_agent-5_mean: 89.64
    apples_agent-5_min: 37
    cleaning_beam_agent-0_max: 438
    cleaning_beam_agent-0_mean: 313.09
    cleaning_beam_agent-0_min: 168
    cleaning_beam_agent-1_max: 366
    cleaning_beam_agent-1_mean: 225.07
    cleaning_beam_agent-1_min: 88
    cleaning_beam_agent-2_max: 509
    cleaning_beam_agent-2_mean: 360.6
    cleaning_beam_agent-2_min: 125
    cleaning_beam_agent-3_max: 128
    cleaning_beam_agent-3_mean: 38.54
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 453
    cleaning_beam_agent-4_mean: 368.17
    cleaning_beam_agent-4_min: 256
    cleaning_beam_agent-5_max: 122
    cleaning_beam_agent-5_mean: 44.33
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-06-54
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 994.9999999999698
  episode_reward_mean: 775.1799999999873
  episode_reward_min: 422.0000000000065
  episodes_this_iter: 96
  episodes_total: 19584
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19604.718
    learner:
      agent-0:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 4.394879942992702e-05
        entropy: 1.136124610900879
        entropy_coeff: 0.0017600000137463212
        kl: 0.004469789564609528
        model: {}
        policy_loss: -0.009720085188746452
        total_loss: -0.010095389559864998
        vf_explained_var: 0.0071983784437179565
        vf_loss: 15.68403148651123
      agent-1:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 4.394879942992702e-05
        entropy: 1.110916018486023
        entropy_coeff: 0.0017600000137463212
        kl: 0.004396346863359213
        model: {}
        policy_loss: -0.011620279401540756
        total_loss: -0.011852206662297249
        vf_explained_var: -0.017115771770477295
        vf_loss: 16.133760452270508
      agent-2:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 4.394879942992702e-05
        entropy: 1.0166767835617065
        entropy_coeff: 0.0017600000137463212
        kl: 0.005419242195785046
        model: {}
        policy_loss: -0.010475805029273033
        total_loss: -0.010702449828386307
        vf_explained_var: 0.05235591530799866
        vf_loss: 14.94963550567627
      agent-3:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 4.394879942992702e-05
        entropy: 0.6826490759849548
        entropy_coeff: 0.0017600000137463212
        kl: 0.0036430980544537306
        model: {}
        policy_loss: -0.007630982436239719
        total_loss: -0.0074058230966329575
        vf_explained_var: 0.09655016660690308
        vf_loss: 14.251995086669922
      agent-4:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 4.394879942992702e-05
        entropy: 0.9862912893295288
        entropy_coeff: 0.0017600000137463212
        kl: 0.003941315226256847
        model: {}
        policy_loss: -0.010655651800334454
        total_loss: -0.010750512592494488
        vf_explained_var: 0.022033527493476868
        vf_loss: 15.424779891967773
      agent-5:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 4.394879942992702e-05
        entropy: 0.8512277007102966
        entropy_coeff: 0.0017600000137463212
        kl: 0.003769727423787117
        model: {}
        policy_loss: -0.010470107197761536
        total_loss: -0.010449781082570553
        vf_explained_var: 0.05578252673149109
        vf_loss: 14.949250221252441
    load_time_ms: 13040.923
    num_steps_sampled: 19584000
    num_steps_trained: 19584000
    sample_time_ms: 89525.713
    update_time_ms: 21.343
  iterations_since_restore: 144
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.39080459770115
    ram_util_percent: 10.07758620689655
  pid: 24061
  policy_reward_max:
    agent-0: 165.83333333333312
    agent-1: 165.83333333333312
    agent-2: 165.83333333333312
    agent-3: 165.83333333333312
    agent-4: 165.83333333333312
    agent-5: 165.83333333333312
  policy_reward_mean:
    agent-0: 129.19666666666697
    agent-1: 129.19666666666697
    agent-2: 129.19666666666697
    agent-3: 129.19666666666697
    agent-4: 129.19666666666697
    agent-5: 129.19666666666697
  policy_reward_min:
    agent-0: 70.3333333333333
    agent-1: 70.3333333333333
    agent-2: 70.3333333333333
    agent-3: 70.3333333333333
    agent-4: 70.3333333333333
    agent-5: 70.3333333333333
  sampler_perf:
    mean_env_wait_ms: 24.33211611510701
    mean_inference_ms: 12.308389401157363
    mean_processing_ms: 51.01001140831826
  time_since_restore: 18785.769917488098
  time_this_iter_s: 122.28756952285767
  time_total_s: 27911.78173136711
  timestamp: 1637042814
  timesteps_since_restore: 13824000
  timesteps_this_iter: 96000
  timesteps_total: 19584000
  training_iteration: 204
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    204 |          27911.8 | 19584000 |   775.18 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 137
    apples_agent-0_mean: 5.76
    apples_agent-0_min: 0
    apples_agent-1_max: 102
    apples_agent-1_mean: 31.17
    apples_agent-1_min: 0
    apples_agent-2_max: 71
    apples_agent-2_mean: 4.37
    apples_agent-2_min: 0
    apples_agent-3_max: 160
    apples_agent-3_mean: 96.87
    apples_agent-3_min: 45
    apples_agent-4_max: 43
    apples_agent-4_mean: 1.52
    apples_agent-4_min: 0
    apples_agent-5_max: 141
    apples_agent-5_mean: 93.12
    apples_agent-5_min: 32
    cleaning_beam_agent-0_max: 498
    cleaning_beam_agent-0_mean: 326.37
    cleaning_beam_agent-0_min: 129
    cleaning_beam_agent-1_max: 394
    cleaning_beam_agent-1_mean: 238.27
    cleaning_beam_agent-1_min: 96
    cleaning_beam_agent-2_max: 495
    cleaning_beam_agent-2_mean: 362.04
    cleaning_beam_agent-2_min: 193
    cleaning_beam_agent-3_max: 92
    cleaning_beam_agent-3_mean: 36.26
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 496
    cleaning_beam_agent-4_mean: 377.66
    cleaning_beam_agent-4_min: 264
    cleaning_beam_agent-5_max: 146
    cleaning_beam_agent-5_mean: 44.36
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-08-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 995.9999999999811
  episode_reward_mean: 785.7599999999865
  episode_reward_min: 473.000000000011
  episodes_this_iter: 96
  episodes_total: 19680
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19596.674
    learner:
      agent-0:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 3.795840166276321e-05
        entropy: 1.1288338899612427
        entropy_coeff: 0.0017600000137463212
        kl: 0.004285803996026516
        model: {}
        policy_loss: -0.008876108564436436
        total_loss: -0.009147865697741508
        vf_explained_var: 0.03403542935848236
        vf_loss: 16.881999969482422
      agent-1:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 3.795840166276321e-05
        entropy: 1.1056225299835205
        entropy_coeff: 0.0017600000137463212
        kl: 0.004201309289783239
        model: {}
        policy_loss: -0.010716116987168789
        total_loss: -0.010896033607423306
        vf_explained_var: 0.02233603596687317
        vf_loss: 17.134632110595703
      agent-2:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 3.795840166276321e-05
        entropy: 1.016914963722229
        entropy_coeff: 0.0017600000137463212
        kl: 0.003960927948355675
        model: {}
        policy_loss: -0.009194008074700832
        total_loss: -0.009208094328641891
        vf_explained_var: 0.016217559576034546
        vf_loss: 17.261749267578125
      agent-3:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 3.795840166276321e-05
        entropy: 0.6836883425712585
        entropy_coeff: 0.0017600000137463212
        kl: 0.0028184105176478624
        model: {}
        policy_loss: -0.006482293829321861
        total_loss: -0.0061762649565935135
        vf_explained_var: 0.13551382720470428
        vf_loss: 15.087675094604492
      agent-4:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 3.795840166276321e-05
        entropy: 0.9786313772201538
        entropy_coeff: 0.0017600000137463212
        kl: 0.0042359549552202225
        model: {}
        policy_loss: -0.009759529493749142
        total_loss: -0.009718002751469612
        vf_explained_var: 0.02216498553752899
        vf_loss: 17.10969352722168
      agent-5:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 3.795840166276321e-05
        entropy: 0.828977108001709
        entropy_coeff: 0.0017600000137463212
        kl: 0.003951136022806168
        model: {}
        policy_loss: -0.009866819716989994
        total_loss: -0.009730474092066288
        vf_explained_var: 0.09243233501911163
        vf_loss: 15.829959869384766
    load_time_ms: 13034.54
    num_steps_sampled: 19680000
    num_steps_trained: 19680000
    sample_time_ms: 89503.397
    update_time_ms: 21.081
  iterations_since_restore: 145
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.39137931034483
    ram_util_percent: 10.148275862068964
  pid: 24061
  policy_reward_max:
    agent-0: 166.00000000000003
    agent-1: 166.00000000000003
    agent-2: 166.00000000000003
    agent-3: 166.00000000000003
    agent-4: 166.00000000000003
    agent-5: 166.00000000000003
  policy_reward_mean:
    agent-0: 130.96000000000026
    agent-1: 130.96000000000026
    agent-2: 130.96000000000026
    agent-3: 130.96000000000026
    agent-4: 130.96000000000026
    agent-5: 130.96000000000026
  policy_reward_min:
    agent-0: 78.83333333333351
    agent-1: 78.83333333333351
    agent-2: 78.83333333333351
    agent-3: 78.83333333333351
    agent-4: 78.83333333333351
    agent-5: 78.83333333333351
  sampler_perf:
    mean_env_wait_ms: 24.33123772148257
    mean_inference_ms: 12.307269485746362
    mean_processing_ms: 51.00379008201158
  time_since_restore: 18907.54764533043
  time_this_iter_s: 121.77772784233093
  time_total_s: 28033.559459209442
  timestamp: 1637042935
  timesteps_since_restore: 13920000
  timesteps_this_iter: 96000
  timesteps_total: 19680000
  training_iteration: 205
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    205 |          28033.6 | 19680000 |   785.76 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 103
    apples_agent-0_mean: 4.96
    apples_agent-0_min: 0
    apples_agent-1_max: 95
    apples_agent-1_mean: 27.25
    apples_agent-1_min: 0
    apples_agent-2_max: 174
    apples_agent-2_mean: 4.98
    apples_agent-2_min: 0
    apples_agent-3_max: 201
    apples_agent-3_mean: 100.9
    apples_agent-3_min: 24
    apples_agent-4_max: 73
    apples_agent-4_mean: 3.47
    apples_agent-4_min: 0
    apples_agent-5_max: 130
    apples_agent-5_mean: 89.48
    apples_agent-5_min: 20
    cleaning_beam_agent-0_max: 444
    cleaning_beam_agent-0_mean: 319.73
    cleaning_beam_agent-0_min: 147
    cleaning_beam_agent-1_max: 383
    cleaning_beam_agent-1_mean: 242.07
    cleaning_beam_agent-1_min: 111
    cleaning_beam_agent-2_max: 515
    cleaning_beam_agent-2_mean: 358.56
    cleaning_beam_agent-2_min: 117
    cleaning_beam_agent-3_max: 130
    cleaning_beam_agent-3_mean: 37.18
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 467
    cleaning_beam_agent-4_mean: 375.41
    cleaning_beam_agent-4_min: 170
    cleaning_beam_agent-5_max: 150
    cleaning_beam_agent-5_mean: 44.65
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-10-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 967.9999999999818
  episode_reward_mean: 782.3799999999878
  episode_reward_min: 216.9999999999976
  episodes_this_iter: 96
  episodes_total: 19776
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19584.131
    learner:
      agent-0:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 3.196800025762059e-05
        entropy: 1.1207301616668701
        entropy_coeff: 0.0017600000137463212
        kl: 0.0033512283116579056
        model: {}
        policy_loss: -0.007534551899880171
        total_loss: -0.007731019519269466
        vf_explained_var: -0.01109263300895691
        vf_loss: 17.65544319152832
      agent-1:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 3.196800025762059e-05
        entropy: 1.1111465692520142
        entropy_coeff: 0.0017600000137463212
        kl: 0.0038409882690757513
        model: {}
        policy_loss: -0.009288820438086987
        total_loss: -0.009478923864662647
        vf_explained_var: 0.004599720239639282
        vf_loss: 17.415084838867188
      agent-2:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 3.196800025762059e-05
        entropy: 1.0213052034378052
        entropy_coeff: 0.0017600000137463212
        kl: 0.003212242852896452
        model: {}
        policy_loss: -0.008125468157231808
        total_loss: -0.008166924118995667
        vf_explained_var: 0.0051107704639434814
        vf_loss: 17.35968780517578
      agent-3:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 3.196800025762059e-05
        entropy: 0.6795224547386169
        entropy_coeff: 0.0017600000137463212
        kl: 0.0028106754180043936
        model: {}
        policy_loss: -0.006226596888154745
        total_loss: -0.00595768541097641
        vf_explained_var: 0.16079705953598022
        vf_loss: 14.645964622497559
      agent-4:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 3.196800025762059e-05
        entropy: 0.9880119562149048
        entropy_coeff: 0.0017600000137463212
        kl: 0.003559831064194441
        model: {}
        policy_loss: -0.00869283452630043
        total_loss: -0.008782003074884415
        vf_explained_var: 0.07110165059566498
        vf_loss: 16.274818420410156
      agent-5:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 3.196800025762059e-05
        entropy: 0.826138973236084
        entropy_coeff: 0.0017600000137463212
        kl: 0.002936451230198145
        model: {}
        policy_loss: -0.008459161967039108
        total_loss: -0.008327962830662727
        vf_explained_var: 0.09240967035293579
        vf_loss: 15.806147575378418
    load_time_ms: 13059.74
    num_steps_sampled: 19776000
    num_steps_trained: 19776000
    sample_time_ms: 89617.499
    update_time_ms: 21.649
  iterations_since_restore: 146
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.342285714285715
    ram_util_percent: 10.132000000000001
  pid: 24061
  policy_reward_max:
    agent-0: 161.33333333333314
    agent-1: 161.33333333333314
    agent-2: 161.33333333333314
    agent-3: 161.33333333333314
    agent-4: 161.33333333333314
    agent-5: 161.33333333333314
  policy_reward_mean:
    agent-0: 130.3966666666669
    agent-1: 130.3966666666669
    agent-2: 130.3966666666669
    agent-3: 130.3966666666669
    agent-4: 130.3966666666669
    agent-5: 130.3966666666669
  policy_reward_min:
    agent-0: 36.166666666666664
    agent-1: 36.166666666666664
    agent-2: 36.166666666666664
    agent-3: 36.166666666666664
    agent-4: 36.166666666666664
    agent-5: 36.166666666666664
  sampler_perf:
    mean_env_wait_ms: 24.329119141702744
    mean_inference_ms: 12.30600301013287
    mean_processing_ms: 50.99721934318372
  time_since_restore: 19030.508896112442
  time_this_iter_s: 122.96125078201294
  time_total_s: 28156.520709991455
  timestamp: 1637043059
  timesteps_since_restore: 14016000
  timesteps_this_iter: 96000
  timesteps_total: 19776000
  training_iteration: 206
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    206 |          28156.5 | 19776000 |   782.38 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 89
    apples_agent-0_mean: 6.2
    apples_agent-0_min: 0
    apples_agent-1_max: 120
    apples_agent-1_mean: 33.42
    apples_agent-1_min: 0
    apples_agent-2_max: 74
    apples_agent-2_mean: 2.05
    apples_agent-2_min: 0
    apples_agent-3_max: 168
    apples_agent-3_mean: 102.53
    apples_agent-3_min: 48
    apples_agent-4_max: 70
    apples_agent-4_mean: 1.72
    apples_agent-4_min: 0
    apples_agent-5_max: 157
    apples_agent-5_mean: 89.92
    apples_agent-5_min: 42
    cleaning_beam_agent-0_max: 456
    cleaning_beam_agent-0_mean: 323.95
    cleaning_beam_agent-0_min: 142
    cleaning_beam_agent-1_max: 424
    cleaning_beam_agent-1_mean: 239.07
    cleaning_beam_agent-1_min: 118
    cleaning_beam_agent-2_max: 495
    cleaning_beam_agent-2_mean: 365.94
    cleaning_beam_agent-2_min: 210
    cleaning_beam_agent-3_max: 92
    cleaning_beam_agent-3_mean: 35.47
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 490
    cleaning_beam_agent-4_mean: 387.52
    cleaning_beam_agent-4_min: 212
    cleaning_beam_agent-5_max: 211
    cleaning_beam_agent-5_mean: 42.36
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-13-02
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1027.9999999999832
  episode_reward_mean: 806.2799999999863
  episode_reward_min: 496.0000000000108
  episodes_this_iter: 96
  episodes_total: 19872
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19588.745
    learner:
      agent-0:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 2.597760067146737e-05
        entropy: 1.1292554140090942
        entropy_coeff: 0.0017600000137463212
        kl: 0.0036604427732527256
        model: {}
        policy_loss: -0.007129560690373182
        total_loss: -0.007387137971818447
        vf_explained_var: 0.04446101188659668
        vf_loss: 17.24192237854004
      agent-1:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 2.597760067146737e-05
        entropy: 1.120207667350769
        entropy_coeff: 0.0017600000137463212
        kl: 0.0025779486168175936
        model: {}
        policy_loss: -0.007803587708622217
        total_loss: -0.007988558150827885
        vf_explained_var: 0.017798617482185364
        vf_loss: 17.785354614257812
      agent-2:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 2.597760067146737e-05
        entropy: 1.020865559577942
        entropy_coeff: 0.0017600000137463212
        kl: 0.003185704816132784
        model: {}
        policy_loss: -0.007252953946590424
        total_loss: -0.007305597886443138
        vf_explained_var: 0.03634047508239746
        vf_loss: 17.34123992919922
      agent-3:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 2.597760067146737e-05
        entropy: 0.668332040309906
        entropy_coeff: 0.0017600000137463212
        kl: 0.001998953288421035
        model: {}
        policy_loss: -0.005163555033504963
        total_loss: -0.004767861217260361
        vf_explained_var: 0.12776561081409454
        vf_loss: 15.71865177154541
      agent-4:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 2.597760067146737e-05
        entropy: 0.9809180498123169
        entropy_coeff: 0.0017600000137463212
        kl: 0.0027817245572805405
        model: {}
        policy_loss: -0.007501533720642328
        total_loss: -0.007474725134670734
        vf_explained_var: 0.032586053013801575
        vf_loss: 17.44530487060547
      agent-5:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 2.597760067146737e-05
        entropy: 0.8121203184127808
        entropy_coeff: 0.0017600000137463212
        kl: 0.0030494045931845903
        model: {}
        policy_loss: -0.00733679486438632
        total_loss: -0.007103622891008854
        vf_explained_var: 0.08190034329891205
        vf_loss: 16.6012020111084
    load_time_ms: 13051.629
    num_steps_sampled: 19872000
    num_steps_trained: 19872000
    sample_time_ms: 89660.325
    update_time_ms: 21.679
  iterations_since_restore: 147
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.334285714285715
    ram_util_percent: 10.169142857142853
  pid: 24061
  policy_reward_max:
    agent-0: 171.3333333333334
    agent-1: 171.3333333333334
    agent-2: 171.3333333333334
    agent-3: 171.3333333333334
    agent-4: 171.3333333333334
    agent-5: 171.3333333333334
  policy_reward_mean:
    agent-0: 134.38000000000025
    agent-1: 134.38000000000025
    agent-2: 134.38000000000025
    agent-3: 134.38000000000025
    agent-4: 134.38000000000025
    agent-5: 134.38000000000025
  policy_reward_min:
    agent-0: 82.66666666666688
    agent-1: 82.66666666666688
    agent-2: 82.66666666666688
    agent-3: 82.66666666666688
    agent-4: 82.66666666666688
    agent-5: 82.66666666666688
  sampler_perf:
    mean_env_wait_ms: 24.32786785668962
    mean_inference_ms: 12.304739268207825
    mean_processing_ms: 50.990929498915335
  time_since_restore: 19153.30503845215
  time_this_iter_s: 122.79614233970642
  time_total_s: 28279.31685233116
  timestamp: 1637043182
  timesteps_since_restore: 14112000
  timesteps_this_iter: 96000
  timesteps_total: 19872000
  training_iteration: 207
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    207 |          28279.3 | 19872000 |   806.28 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 51
    apples_agent-0_mean: 4.66
    apples_agent-0_min: 0
    apples_agent-1_max: 205
    apples_agent-1_mean: 32.5
    apples_agent-1_min: 0
    apples_agent-2_max: 60
    apples_agent-2_mean: 3.74
    apples_agent-2_min: 0
    apples_agent-3_max: 174
    apples_agent-3_mean: 103.38
    apples_agent-3_min: 43
    apples_agent-4_max: 57
    apples_agent-4_mean: 1.28
    apples_agent-4_min: 0
    apples_agent-5_max: 146
    apples_agent-5_mean: 90.52
    apples_agent-5_min: 29
    cleaning_beam_agent-0_max: 467
    cleaning_beam_agent-0_mean: 325.27
    cleaning_beam_agent-0_min: 174
    cleaning_beam_agent-1_max: 453
    cleaning_beam_agent-1_mean: 234.35
    cleaning_beam_agent-1_min: 103
    cleaning_beam_agent-2_max: 499
    cleaning_beam_agent-2_mean: 359.62
    cleaning_beam_agent-2_min: 137
    cleaning_beam_agent-3_max: 126
    cleaning_beam_agent-3_mean: 36.2
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 507
    cleaning_beam_agent-4_mean: 393.31
    cleaning_beam_agent-4_min: 154
    cleaning_beam_agent-5_max: 134
    cleaning_beam_agent-5_mean: 49.19
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-15-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1005.9999999999873
  episode_reward_mean: 793.2799999999879
  episode_reward_min: 292.99999999999943
  episodes_this_iter: 96
  episodes_total: 19968
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19566.885
    learner:
      agent-0:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 1.998719926632475e-05
        entropy: 1.1439745426177979
        entropy_coeff: 0.0017600000137463212
        kl: 0.0029912618920207024
        model: {}
        policy_loss: -0.005555993411689997
        total_loss: -0.005798380821943283
        vf_explained_var: 0.0437597930431366
        vf_loss: 17.686704635620117
      agent-1:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 1.998719926632475e-05
        entropy: 1.1011836528778076
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024892587680369616
        model: {}
        policy_loss: -0.006599139887839556
        total_loss: -0.0066975075751543045
        vf_explained_var: 0.0075354427099227905
        vf_loss: 18.358285903930664
      agent-2:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 1.998719926632475e-05
        entropy: 1.0058804750442505
        entropy_coeff: 0.0017600000137463212
        kl: 0.003415734274312854
        model: {}
        policy_loss: -0.005812937393784523
        total_loss: -0.0058151488192379475
        vf_explained_var: 0.04817187786102295
        vf_loss: 17.628026962280273
      agent-3:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.998719926632475e-05
        entropy: 0.6811872720718384
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018479828722774982
        model: {}
        policy_loss: -0.00436576409265399
        total_loss: -0.004013105295598507
        vf_explained_var: 0.16029226779937744
        vf_loss: 15.515083312988281
      agent-4:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 1.998719926632475e-05
        entropy: 0.9798110723495483
        entropy_coeff: 0.0017600000137463212
        kl: 0.002915068995207548
        model: {}
        policy_loss: -0.006581904366612434
        total_loss: -0.006521078757941723
        vf_explained_var: 0.03802752494812012
        vf_loss: 17.807376861572266
      agent-5:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 1.998719926632475e-05
        entropy: 0.8157960772514343
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019843371119350195
        model: {}
        policy_loss: -0.005725347436964512
        total_loss: -0.005567324347794056
        vf_explained_var: 0.14250054955482483
        vf_loss: 15.930526733398438
    load_time_ms: 13040.073
    num_steps_sampled: 19968000
    num_steps_trained: 19968000
    sample_time_ms: 89659.566
    update_time_ms: 21.133
  iterations_since_restore: 148
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.400578034682079
    ram_util_percent: 10.134682080924856
  pid: 24061
  policy_reward_max:
    agent-0: 167.66666666666686
    agent-1: 167.66666666666686
    agent-2: 167.66666666666686
    agent-3: 167.66666666666686
    agent-4: 167.66666666666686
    agent-5: 167.66666666666686
  policy_reward_mean:
    agent-0: 132.21333333333357
    agent-1: 132.21333333333357
    agent-2: 132.21333333333357
    agent-3: 132.21333333333357
    agent-4: 132.21333333333357
    agent-5: 132.21333333333357
  policy_reward_min:
    agent-0: 48.83333333333323
    agent-1: 48.83333333333323
    agent-2: 48.83333333333323
    agent-3: 48.83333333333323
    agent-4: 48.83333333333323
    agent-5: 48.83333333333323
  sampler_perf:
    mean_env_wait_ms: 24.326161956534975
    mean_inference_ms: 12.303118390516076
    mean_processing_ms: 50.983755429706235
  time_since_restore: 19275.061775922775
  time_this_iter_s: 121.75673747062683
  time_total_s: 28401.07358980179
  timestamp: 1637043303
  timesteps_since_restore: 14208000
  timesteps_this_iter: 96000
  timesteps_total: 19968000
  training_iteration: 208
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    208 |          28401.1 | 19968000 |   793.28 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 70
    apples_agent-0_mean: 7.34
    apples_agent-0_min: 0
    apples_agent-1_max: 127
    apples_agent-1_mean: 29.54
    apples_agent-1_min: 0
    apples_agent-2_max: 78
    apples_agent-2_mean: 3.49
    apples_agent-2_min: 0
    apples_agent-3_max: 168
    apples_agent-3_mean: 105.32
    apples_agent-3_min: 53
    apples_agent-4_max: 45
    apples_agent-4_mean: 1.25
    apples_agent-4_min: 0
    apples_agent-5_max: 133
    apples_agent-5_mean: 90.31
    apples_agent-5_min: 34
    cleaning_beam_agent-0_max: 436
    cleaning_beam_agent-0_mean: 315.1
    cleaning_beam_agent-0_min: 110
    cleaning_beam_agent-1_max: 434
    cleaning_beam_agent-1_mean: 244.39
    cleaning_beam_agent-1_min: 95
    cleaning_beam_agent-2_max: 544
    cleaning_beam_agent-2_mean: 384.16
    cleaning_beam_agent-2_min: 265
    cleaning_beam_agent-3_max: 100
    cleaning_beam_agent-3_mean: 31.73
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 554
    cleaning_beam_agent-4_mean: 394.38
    cleaning_beam_agent-4_min: 284
    cleaning_beam_agent-5_max: 173
    cleaning_beam_agent-5_mean: 49.08
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-17-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 999.9999999999859
  episode_reward_mean: 816.779999999986
  episode_reward_min: 519.0000000000077
  episodes_this_iter: 96
  episodes_total: 20064
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19606.41
    learner:
      agent-0:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 1.3996799680171534e-05
        entropy: 1.1450257301330566
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019354956457391381
        model: {}
        policy_loss: -0.004238037392497063
        total_loss: -0.004592638462781906
        vf_explained_var: 0.0308399498462677
        vf_loss: 16.598880767822266
      agent-1:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 1.3996799680171534e-05
        entropy: 1.1094313859939575
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016213118797168136
        model: {}
        policy_loss: -0.004975262563675642
        total_loss: -0.005211976356804371
        vf_explained_var: -0.005594536662101746
        vf_loss: 17.14623260498047
      agent-2:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 1.3996799680171534e-05
        entropy: 0.9970853328704834
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020352620631456375
        model: {}
        policy_loss: -0.004363628104329109
        total_loss: -0.004370519891381264
        vf_explained_var: -0.024869143962860107
        vf_loss: 17.46392059326172
      agent-3:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.3996799680171534e-05
        entropy: 0.6670225858688354
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017342779319733381
        model: {}
        policy_loss: -0.0033208185341209173
        total_loss: -0.003040559822693467
        vf_explained_var: 0.1399378627538681
        vf_loss: 14.542048454284668
      agent-4:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 1.3996799680171534e-05
        entropy: 0.9846627712249756
        entropy_coeff: 0.0017600000137463212
        kl: 0.002095791744068265
        model: {}
        policy_loss: -0.005014872178435326
        total_loss: -0.005072262138128281
        vf_explained_var: 0.025782138109207153
        vf_loss: 16.7397518157959
      agent-5:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.3996799680171534e-05
        entropy: 0.8043665289878845
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018038181588053703
        model: {}
        policy_loss: -0.004785848781466484
        total_loss: -0.004670029040426016
        vf_explained_var: 0.10134164988994598
        vf_loss: 15.311549186706543
    load_time_ms: 13032.713
    num_steps_sampled: 20064000
    num_steps_trained: 20064000
    sample_time_ms: 89647.332
    update_time_ms: 21.071
  iterations_since_restore: 149
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.366285714285711
    ram_util_percent: 10.167999999999997
  pid: 24061
  policy_reward_max:
    agent-0: 166.66666666666615
    agent-1: 166.66666666666615
    agent-2: 166.66666666666615
    agent-3: 166.66666666666615
    agent-4: 166.66666666666615
    agent-5: 166.66666666666615
  policy_reward_mean:
    agent-0: 136.13000000000022
    agent-1: 136.13000000000022
    agent-2: 136.13000000000022
    agent-3: 136.13000000000022
    agent-4: 136.13000000000022
    agent-5: 136.13000000000022
  policy_reward_min:
    agent-0: 86.50000000000018
    agent-1: 86.50000000000018
    agent-2: 86.50000000000018
    agent-3: 86.50000000000018
    agent-4: 86.50000000000018
    agent-5: 86.50000000000018
  sampler_perf:
    mean_env_wait_ms: 24.32523159247791
    mean_inference_ms: 12.301548547339241
    mean_processing_ms: 50.978107416231644
  time_since_restore: 19397.56849384308
  time_this_iter_s: 122.50671792030334
  time_total_s: 28523.58030772209
  timestamp: 1637043426
  timesteps_since_restore: 14304000
  timesteps_this_iter: 96000
  timesteps_total: 20064000
  training_iteration: 209
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    209 |          28523.6 | 20064000 |   816.78 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 79
    apples_agent-0_mean: 4.36
    apples_agent-0_min: 0
    apples_agent-1_max: 93
    apples_agent-1_mean: 29.61
    apples_agent-1_min: 0
    apples_agent-2_max: 72
    apples_agent-2_mean: 5.99
    apples_agent-2_min: 0
    apples_agent-3_max: 160
    apples_agent-3_mean: 105.32
    apples_agent-3_min: 30
    apples_agent-4_max: 48
    apples_agent-4_mean: 1.68
    apples_agent-4_min: 0
    apples_agent-5_max: 155
    apples_agent-5_mean: 89.63
    apples_agent-5_min: 8
    cleaning_beam_agent-0_max: 471
    cleaning_beam_agent-0_mean: 321.45
    cleaning_beam_agent-0_min: 157
    cleaning_beam_agent-1_max: 403
    cleaning_beam_agent-1_mean: 240.02
    cleaning_beam_agent-1_min: 113
    cleaning_beam_agent-2_max: 530
    cleaning_beam_agent-2_mean: 369.39
    cleaning_beam_agent-2_min: 147
    cleaning_beam_agent-3_max: 109
    cleaning_beam_agent-3_mean: 35.99
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 504
    cleaning_beam_agent-4_mean: 389.75
    cleaning_beam_agent-4_min: 249
    cleaning_beam_agent-5_max: 328
    cleaning_beam_agent-5_mean: 51.73
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-19-10
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1009.9999999999907
  episode_reward_mean: 804.1699999999854
  episode_reward_min: 276.9999999999995
  episodes_this_iter: 96
  episodes_total: 20160
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19604.183
    learner:
      agent-0:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1321988105773926
        entropy_coeff: 0.0017600000137463212
        kl: 0.0025674859061837196
        model: {}
        policy_loss: -0.003947022836655378
        total_loss: -0.004186868201941252
        vf_explained_var: 0.04208499193191528
        vf_loss: 17.52325439453125
      agent-1:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1080265045166016
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014697711449116468
        model: {}
        policy_loss: -0.004194260109215975
        total_loss: -0.004283948335796595
        vf_explained_var: -0.012184590101242065
        vf_loss: 18.598661422729492
      agent-2:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0116403102874756
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015895722899585962
        model: {}
        policy_loss: -0.004050277639180422
        total_loss: -0.004097274038940668
        vf_explained_var: 0.05684155225753784
        vf_loss: 17.328712463378906
      agent-3:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6931447982788086
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013290538918226957
        model: {}
        policy_loss: -0.002888008253648877
        total_loss: -0.002543540671467781
        vf_explained_var: 0.1448729783296585
        vf_loss: 15.643982887268066
      agent-4:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9846515655517578
        entropy_coeff: 0.0017600000137463212
        kl: 0.001547555672004819
        model: {}
        policy_loss: -0.0044999923557043076
        total_loss: -0.0044378554448485374
        vf_explained_var: 0.022102296352386475
        vf_loss: 17.94522476196289
      agent-5:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8023461699485779
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016389882657676935
        model: {}
        policy_loss: -0.004106161184608936
        total_loss: -0.0038536451756954193
        vf_explained_var: 0.08935068547725677
        vf_loss: 16.64490509033203
    load_time_ms: 13004.948
    num_steps_sampled: 20160000
    num_steps_trained: 20160000
    sample_time_ms: 89769.242
    update_time_ms: 21.234
  iterations_since_restore: 150
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.336931818181817
    ram_util_percent: 10.155681818181817
  pid: 24061
  policy_reward_max:
    agent-0: 168.33333333333348
    agent-1: 168.33333333333348
    agent-2: 168.33333333333348
    agent-3: 168.33333333333348
    agent-4: 168.33333333333348
    agent-5: 168.33333333333348
  policy_reward_mean:
    agent-0: 134.02833333333356
    agent-1: 134.02833333333356
    agent-2: 134.02833333333356
    agent-3: 134.02833333333356
    agent-4: 134.02833333333356
    agent-5: 134.02833333333356
  policy_reward_min:
    agent-0: 46.16666666666661
    agent-1: 46.16666666666661
    agent-2: 46.16666666666661
    agent-3: 46.16666666666661
    agent-4: 46.16666666666661
    agent-5: 46.16666666666661
  sampler_perf:
    mean_env_wait_ms: 24.325072415719845
    mean_inference_ms: 12.300214077932674
    mean_processing_ms: 50.972546509015444
  time_since_restore: 19520.835671663284
  time_this_iter_s: 123.26717782020569
  time_total_s: 28646.847485542297
  timestamp: 1637043550
  timesteps_since_restore: 14400000
  timesteps_this_iter: 96000
  timesteps_total: 20160000
  training_iteration: 210
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    210 |          28646.8 | 20160000 |   804.17 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 73
    apples_agent-0_mean: 6.15
    apples_agent-0_min: 0
    apples_agent-1_max: 112
    apples_agent-1_mean: 32.56
    apples_agent-1_min: 0
    apples_agent-2_max: 196
    apples_agent-2_mean: 5.64
    apples_agent-2_min: 0
    apples_agent-3_max: 157
    apples_agent-3_mean: 103.44
    apples_agent-3_min: 32
    apples_agent-4_max: 65
    apples_agent-4_mean: 2.98
    apples_agent-4_min: 0
    apples_agent-5_max: 140
    apples_agent-5_mean: 87.58
    apples_agent-5_min: 18
    cleaning_beam_agent-0_max: 470
    cleaning_beam_agent-0_mean: 322.93
    cleaning_beam_agent-0_min: 92
    cleaning_beam_agent-1_max: 392
    cleaning_beam_agent-1_mean: 228.02
    cleaning_beam_agent-1_min: 102
    cleaning_beam_agent-2_max: 518
    cleaning_beam_agent-2_mean: 385.23
    cleaning_beam_agent-2_min: 187
    cleaning_beam_agent-3_max: 131
    cleaning_beam_agent-3_mean: 34.41
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 484
    cleaning_beam_agent-4_mean: 380.7
    cleaning_beam_agent-4_min: 166
    cleaning_beam_agent-5_max: 247
    cleaning_beam_agent-5_mean: 50.66
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-21-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1057.9999999999695
  episode_reward_mean: 796.5399999999873
  episode_reward_min: 257.9999999999962
  episodes_this_iter: 96
  episodes_total: 20256
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19621.583
    learner:
      agent-0:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 1.146079182624817
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016201803227886558
        model: {}
        policy_loss: -0.003757237922400236
        total_loss: -0.003924843855202198
        vf_explained_var: 0.05161154270172119
        vf_loss: 18.4934024810791
      agent-1:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1006258726119995
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019563448149710894
        model: {}
        policy_loss: -0.004027741029858589
        total_loss: -0.00403561582788825
        vf_explained_var: 0.013006433844566345
        vf_loss: 19.28845977783203
      agent-2:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9923186302185059
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012896549887955189
        model: {}
        policy_loss: -0.0035694597754627466
        total_loss: -0.0034071700647473335
        vf_explained_var: 0.020647317171096802
        vf_loss: 19.085233688354492
      agent-3:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7017672657966614
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013247827300801873
        model: {}
        policy_loss: -0.0032355687581002712
        total_loss: -0.0028698230162262917
        vf_explained_var: 0.18018992245197296
        vf_loss: 16.008516311645508
      agent-4:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9825564622879028
        entropy_coeff: 0.0017600000137463212
        kl: 0.001576932379975915
        model: {}
        policy_loss: -0.004580502398312092
        total_loss: -0.004549045115709305
        vf_explained_var: 0.09979069232940674
        vf_loss: 17.604509353637695
      agent-5:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.812812328338623
        entropy_coeff: 0.0017600000137463212
        kl: 0.001273329253308475
        model: {}
        policy_loss: -0.004388472996652126
        total_loss: -0.004121085628867149
        vf_explained_var: 0.13088655471801758
        vf_loss: 16.978759765625
    load_time_ms: 13021.799
    num_steps_sampled: 20256000
    num_steps_trained: 20256000
    sample_time_ms: 89708.027
    update_time_ms: 21.271
  iterations_since_restore: 151
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.556069364161848
    ram_util_percent: 10.150867052023118
  pid: 24061
  policy_reward_max:
    agent-0: 176.33333333333314
    agent-1: 176.33333333333314
    agent-2: 176.33333333333314
    agent-3: 176.33333333333314
    agent-4: 176.33333333333314
    agent-5: 176.33333333333314
  policy_reward_mean:
    agent-0: 132.7566666666669
    agent-1: 132.7566666666669
    agent-2: 132.7566666666669
    agent-3: 132.7566666666669
    agent-4: 132.7566666666669
    agent-5: 132.7566666666669
  policy_reward_min:
    agent-0: 42.99999999999995
    agent-1: 42.99999999999995
    agent-2: 42.99999999999995
    agent-3: 42.99999999999995
    agent-4: 42.99999999999995
    agent-5: 42.99999999999995
  sampler_perf:
    mean_env_wait_ms: 24.324972360955115
    mean_inference_ms: 12.299339356215844
    mean_processing_ms: 50.96976689538968
  time_since_restore: 19643.000502586365
  time_this_iter_s: 122.16483092308044
  time_total_s: 28769.012316465378
  timestamp: 1637043672
  timesteps_since_restore: 14496000
  timesteps_this_iter: 96000
  timesteps_total: 20256000
  training_iteration: 211
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    211 |            28769 | 20256000 |   796.54 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 99
    apples_agent-0_mean: 6.95
    apples_agent-0_min: 0
    apples_agent-1_max: 91
    apples_agent-1_mean: 30.04
    apples_agent-1_min: 0
    apples_agent-2_max: 32
    apples_agent-2_mean: 1.6
    apples_agent-2_min: 0
    apples_agent-3_max: 157
    apples_agent-3_mean: 104.49
    apples_agent-3_min: 51
    apples_agent-4_max: 65
    apples_agent-4_mean: 1.71
    apples_agent-4_min: 0
    apples_agent-5_max: 152
    apples_agent-5_mean: 89.61
    apples_agent-5_min: 33
    cleaning_beam_agent-0_max: 438
    cleaning_beam_agent-0_mean: 310.63
    cleaning_beam_agent-0_min: 174
    cleaning_beam_agent-1_max: 430
    cleaning_beam_agent-1_mean: 246.16
    cleaning_beam_agent-1_min: 99
    cleaning_beam_agent-2_max: 540
    cleaning_beam_agent-2_mean: 394.71
    cleaning_beam_agent-2_min: 127
    cleaning_beam_agent-3_max: 115
    cleaning_beam_agent-3_mean: 34.82
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 480
    cleaning_beam_agent-4_mean: 393.18
    cleaning_beam_agent-4_min: 255
    cleaning_beam_agent-5_max: 165
    cleaning_beam_agent-5_mean: 48.01
    cleaning_beam_agent-5_min: 14
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-23-17
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1000.9999999999849
  episode_reward_mean: 822.3599999999869
  episode_reward_min: 496.00000000001097
  episodes_this_iter: 96
  episodes_total: 20352
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19614.126
    learner:
      agent-0:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1443694829940796
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013964931713417172
        model: {}
        policy_loss: -0.0034453459084033966
        total_loss: -0.0037172501906752586
        vf_explained_var: 0.05413876473903656
        vf_loss: 17.421245574951172
      agent-1:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1002601385116577
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014051664620637894
        model: {}
        policy_loss: -0.004262846894562244
        total_loss: -0.0043514384888112545
        vf_explained_var: -0.0016427934169769287
        vf_loss: 18.477325439453125
      agent-2:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9960482120513916
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016990902367979288
        model: {}
        policy_loss: -0.0038634520024061203
        total_loss: -0.0038663512095808983
        vf_explained_var: 0.03785838186740875
        vf_loss: 17.499839782714844
      agent-3:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6843608617782593
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011707645608112216
        model: {}
        policy_loss: -0.002903246320784092
        total_loss: -0.0025460151955485344
        vf_explained_var: 0.1459393948316574
        vf_loss: 15.617019653320312
      agent-4:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9738041162490845
        entropy_coeff: 0.0017600000137463212
        kl: 0.002895844168961048
        model: {}
        policy_loss: -0.004750605672597885
        total_loss: -0.004662043880671263
        vf_explained_var: 0.01714235544204712
        vf_loss: 18.021738052368164
      agent-5:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7838112115859985
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013892323477193713
        model: {}
        policy_loss: -0.003771168412640691
        total_loss: -0.003494719974696636
        vf_explained_var: 0.09318436682224274
        vf_loss: 16.559261322021484
    load_time_ms: 13110.964
    num_steps_sampled: 20352000
    num_steps_trained: 20352000
    sample_time_ms: 89798.477
    update_time_ms: 21.597
  iterations_since_restore: 152
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.255367231638417
    ram_util_percent: 10.171186440677964
  pid: 24061
  policy_reward_max:
    agent-0: 166.83333333333331
    agent-1: 166.83333333333331
    agent-2: 166.83333333333331
    agent-3: 166.83333333333331
    agent-4: 166.83333333333331
    agent-5: 166.83333333333331
  policy_reward_mean:
    agent-0: 137.06000000000017
    agent-1: 137.06000000000017
    agent-2: 137.06000000000017
    agent-3: 137.06000000000017
    agent-4: 137.06000000000017
    agent-5: 137.06000000000017
  policy_reward_min:
    agent-0: 82.6666666666668
    agent-1: 82.6666666666668
    agent-2: 82.6666666666668
    agent-3: 82.6666666666668
    agent-4: 82.6666666666668
    agent-5: 82.6666666666668
  sampler_perf:
    mean_env_wait_ms: 24.324951897784263
    mean_inference_ms: 12.297754107624208
    mean_processing_ms: 50.96393705271408
  time_since_restore: 19767.352898836136
  time_this_iter_s: 124.35239624977112
  time_total_s: 28893.36471271515
  timestamp: 1637043797
  timesteps_since_restore: 14592000
  timesteps_this_iter: 96000
  timesteps_total: 20352000
  training_iteration: 212
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    212 |          28893.4 | 20352000 |   822.36 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 63
    apples_agent-0_mean: 3.99
    apples_agent-0_min: 0
    apples_agent-1_max: 249
    apples_agent-1_mean: 36.03
    apples_agent-1_min: 0
    apples_agent-2_max: 201
    apples_agent-2_mean: 4.88
    apples_agent-2_min: 0
    apples_agent-3_max: 167
    apples_agent-3_mean: 105.27
    apples_agent-3_min: 38
    apples_agent-4_max: 70
    apples_agent-4_mean: 1.05
    apples_agent-4_min: 0
    apples_agent-5_max: 139
    apples_agent-5_mean: 89.91
    apples_agent-5_min: 33
    cleaning_beam_agent-0_max: 440
    cleaning_beam_agent-0_mean: 325.48
    cleaning_beam_agent-0_min: 190
    cleaning_beam_agent-1_max: 406
    cleaning_beam_agent-1_mean: 252.76
    cleaning_beam_agent-1_min: 79
    cleaning_beam_agent-2_max: 561
    cleaning_beam_agent-2_mean: 389.18
    cleaning_beam_agent-2_min: 115
    cleaning_beam_agent-3_max: 99
    cleaning_beam_agent-3_mean: 33.83
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 529
    cleaning_beam_agent-4_mean: 397.16
    cleaning_beam_agent-4_min: 247
    cleaning_beam_agent-5_max: 132
    cleaning_beam_agent-5_mean: 42.12
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-25-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1044.9999999999836
  episode_reward_mean: 822.939999999985
  episode_reward_min: 369.000000000003
  episodes_this_iter: 96
  episodes_total: 20448
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19658.209
    learner:
      agent-0:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1486105918884277
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015644491650164127
        model: {}
        policy_loss: -0.0033827396109700203
        total_loss: -0.003559405915439129
        vf_explained_var: 0.014920324087142944
        vf_loss: 18.448566436767578
      agent-1:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 1.112694501876831
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017181100556626916
        model: {}
        policy_loss: -0.004175164736807346
        total_loss: -0.004273703321814537
        vf_explained_var: 0.012687087059020996
        vf_loss: 18.59725570678711
      agent-2:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9942328333854675
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018790768226608634
        model: {}
        policy_loss: -0.003909416496753693
        total_loss: -0.003878656541928649
        vf_explained_var: 0.04727363586425781
        vf_loss: 17.805225372314453
      agent-3:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6860342025756836
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013493181904777884
        model: {}
        policy_loss: -0.0030582784675061703
        total_loss: -0.0026341532357037067
        vf_explained_var: 0.12962491810321808
        vf_loss: 16.315458297729492
      agent-4:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.978178083896637
        entropy_coeff: 0.0017600000137463212
        kl: 0.001828396925702691
        model: {}
        policy_loss: -0.004401187412440777
        total_loss: -0.004321226850152016
        vf_explained_var: 0.03977659344673157
        vf_loss: 18.014713287353516
      agent-5:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7875217795372009
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009282223763875663
        model: {}
        policy_loss: -0.003676973283290863
        total_loss: -0.0033856937661767006
        vf_explained_var: 0.10390418767929077
        vf_loss: 16.773099899291992
    load_time_ms: 13119.702
    num_steps_sampled: 20448000
    num_steps_trained: 20448000
    sample_time_ms: 89743.867
    update_time_ms: 20.725
  iterations_since_restore: 153
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.36114285714286
    ram_util_percent: 10.177142857142854
  pid: 24061
  policy_reward_max:
    agent-0: 174.1666666666667
    agent-1: 174.1666666666667
    agent-2: 174.1666666666667
    agent-3: 174.1666666666667
    agent-4: 174.1666666666667
    agent-5: 174.1666666666667
  policy_reward_mean:
    agent-0: 137.15666666666687
    agent-1: 137.15666666666687
    agent-2: 137.15666666666687
    agent-3: 137.15666666666687
    agent-4: 137.15666666666687
    agent-5: 137.15666666666687
  policy_reward_min:
    agent-0: 61.49999999999991
    agent-1: 61.49999999999991
    agent-2: 61.49999999999991
    agent-3: 61.49999999999991
    agent-4: 61.49999999999991
    agent-5: 61.49999999999991
  sampler_perf:
    mean_env_wait_ms: 24.325039904523297
    mean_inference_ms: 12.296541862075815
    mean_processing_ms: 50.95776505897694
  time_since_restore: 19889.928795814514
  time_this_iter_s: 122.5758969783783
  time_total_s: 29015.940609693527
  timestamp: 1637043919
  timesteps_since_restore: 14688000
  timesteps_this_iter: 96000
  timesteps_total: 20448000
  training_iteration: 213
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    213 |          29015.9 | 20448000 |   822.94 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 72
    apples_agent-0_mean: 6.56
    apples_agent-0_min: 0
    apples_agent-1_max: 85
    apples_agent-1_mean: 31.27
    apples_agent-1_min: 0
    apples_agent-2_max: 163
    apples_agent-2_mean: 4.85
    apples_agent-2_min: 0
    apples_agent-3_max: 162
    apples_agent-3_mean: 106.77
    apples_agent-3_min: 27
    apples_agent-4_max: 33
    apples_agent-4_mean: 1.12
    apples_agent-4_min: 0
    apples_agent-5_max: 133
    apples_agent-5_mean: 89.21
    apples_agent-5_min: 42
    cleaning_beam_agent-0_max: 427
    cleaning_beam_agent-0_mean: 310.11
    cleaning_beam_agent-0_min: 167
    cleaning_beam_agent-1_max: 443
    cleaning_beam_agent-1_mean: 236.48
    cleaning_beam_agent-1_min: 110
    cleaning_beam_agent-2_max: 536
    cleaning_beam_agent-2_mean: 408.07
    cleaning_beam_agent-2_min: 177
    cleaning_beam_agent-3_max: 104
    cleaning_beam_agent-3_mean: 34.32
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 501
    cleaning_beam_agent-4_mean: 413.23
    cleaning_beam_agent-4_min: 207
    cleaning_beam_agent-5_max: 290
    cleaning_beam_agent-5_mean: 46.39
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-27-22
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1026.9999999999704
  episode_reward_mean: 820.2099999999863
  episode_reward_min: 311.9999999999984
  episodes_this_iter: 96
  episodes_total: 20544
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19689.541
    learner:
      agent-0:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1442890167236328
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014384202659130096
        model: {}
        policy_loss: -0.003851706627756357
        total_loss: -0.004020052496343851
        vf_explained_var: 0.027343764901161194
        vf_loss: 18.455896377563477
      agent-1:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1039015054702759
        entropy_coeff: 0.0017600000137463212
        kl: 0.001734548364765942
        model: {}
        policy_loss: -0.0044619059190154076
        total_loss: -0.004475371912121773
        vf_explained_var: -0.013646841049194336
        vf_loss: 19.29364013671875
      agent-2:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9803750514984131
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013983453391119838
        model: {}
        policy_loss: -0.003527289256453514
        total_loss: -0.003420970169827342
        vf_explained_var: 0.03123481571674347
        vf_loss: 18.317508697509766
      agent-3:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6919699907302856
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013574574841186404
        model: {}
        policy_loss: -0.0027420534752309322
        total_loss: -0.0023050294257700443
        vf_explained_var: 0.12503045797348022
        vf_loss: 16.548946380615234
      agent-4:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9709298610687256
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012561456533148885
        model: {}
        policy_loss: -0.004289722070097923
        total_loss: -0.004202753305435181
        vf_explained_var: 0.05649721622467041
        vf_loss: 17.957786560058594
      agent-5:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.791530430316925
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010593559127300978
        model: {}
        policy_loss: -0.0038288976065814495
        total_loss: -0.0035166461020708084
        vf_explained_var: 0.10385331511497498
        vf_loss: 17.053421020507812
    load_time_ms: 13132.778
    num_steps_sampled: 20544000
    num_steps_trained: 20544000
    sample_time_ms: 89745.907
    update_time_ms: 21.039
  iterations_since_restore: 154
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.372000000000002
    ram_util_percent: 10.15485714285714
  pid: 24061
  policy_reward_max:
    agent-0: 171.16666666666626
    agent-1: 171.16666666666626
    agent-2: 171.16666666666626
    agent-3: 171.16666666666626
    agent-4: 171.16666666666626
    agent-5: 171.16666666666626
  policy_reward_mean:
    agent-0: 136.7016666666668
    agent-1: 136.7016666666668
    agent-2: 136.7016666666668
    agent-3: 136.7016666666668
    agent-4: 136.7016666666668
    agent-5: 136.7016666666668
  policy_reward_min:
    agent-0: 51.99999999999988
    agent-1: 51.99999999999988
    agent-2: 51.99999999999988
    agent-3: 51.99999999999988
    agent-4: 51.99999999999988
    agent-5: 51.99999999999988
  sampler_perf:
    mean_env_wait_ms: 24.326028195453496
    mean_inference_ms: 12.295131936289946
    mean_processing_ms: 50.952833409439066
  time_since_restore: 20012.697853803635
  time_this_iter_s: 122.76905798912048
  time_total_s: 29138.709667682648
  timestamp: 1637044042
  timesteps_since_restore: 14784000
  timesteps_this_iter: 96000
  timesteps_total: 20544000
  training_iteration: 214
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    214 |          29138.7 | 20544000 |   820.21 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 107
    apples_agent-0_mean: 5.03
    apples_agent-0_min: 0
    apples_agent-1_max: 87
    apples_agent-1_mean: 30.87
    apples_agent-1_min: 0
    apples_agent-2_max: 79
    apples_agent-2_mean: 2.2
    apples_agent-2_min: 0
    apples_agent-3_max: 172
    apples_agent-3_mean: 110.46
    apples_agent-3_min: 43
    apples_agent-4_max: 38
    apples_agent-4_mean: 1.09
    apples_agent-4_min: 0
    apples_agent-5_max: 165
    apples_agent-5_mean: 90.11
    apples_agent-5_min: 32
    cleaning_beam_agent-0_max: 444
    cleaning_beam_agent-0_mean: 324.26
    cleaning_beam_agent-0_min: 135
    cleaning_beam_agent-1_max: 607
    cleaning_beam_agent-1_mean: 239.48
    cleaning_beam_agent-1_min: 90
    cleaning_beam_agent-2_max: 572
    cleaning_beam_agent-2_mean: 406.4
    cleaning_beam_agent-2_min: 234
    cleaning_beam_agent-3_max: 128
    cleaning_beam_agent-3_mean: 33.56
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 546
    cleaning_beam_agent-4_mean: 412.33
    cleaning_beam_agent-4_min: 240
    cleaning_beam_agent-5_max: 211
    cleaning_beam_agent-5_mean: 44.31
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-29-25
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 993.9999999999825
  episode_reward_mean: 822.0599999999861
  episode_reward_min: 332.00000000000136
  episodes_this_iter: 96
  episodes_total: 20640
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19687.625
    learner:
      agent-0:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1281005144119263
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016285382444038987
        model: {}
        policy_loss: -0.003673542756587267
        total_loss: -0.003849930129945278
        vf_explained_var: 0.0026136189699172974
        vf_loss: 18.090633392333984
      agent-1:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1004139184951782
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013029410038143396
        model: {}
        policy_loss: -0.004138298332691193
        total_loss: -0.004281007684767246
        vf_explained_var: 0.013615682721138
        vf_loss: 17.94010353088379
      agent-2:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9891934394836426
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012894724495708942
        model: {}
        policy_loss: -0.003520134137943387
        total_loss: -0.003509771078824997
        vf_explained_var: 0.03471408784389496
        vf_loss: 17.513320922851562
      agent-3:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6781402826309204
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022232511546462774
        model: {}
        policy_loss: -0.0030372454784810543
        total_loss: -0.0026527114678174257
        vf_explained_var: 0.1285358965396881
        vf_loss: 15.78056526184082
      agent-4:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9585313200950623
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023018294014036655
        model: {}
        policy_loss: -0.004588834010064602
        total_loss: -0.004504951648414135
        vf_explained_var: 0.022472023963928223
        vf_loss: 17.708707809448242
      agent-5:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7756181359291077
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010655070655047894
        model: {}
        policy_loss: -0.003780714701861143
        total_loss: -0.0035389019176363945
        vf_explained_var: 0.11785824596881866
        vf_loss: 16.068979263305664
    load_time_ms: 13144.194
    num_steps_sampled: 20640000
    num_steps_trained: 20640000
    sample_time_ms: 89819.216
    update_time_ms: 21.244
  iterations_since_restore: 155
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.444571428571429
    ram_util_percent: 10.081142857142856
  pid: 24061
  policy_reward_max:
    agent-0: 165.66666666666677
    agent-1: 165.66666666666677
    agent-2: 165.66666666666677
    agent-3: 165.66666666666677
    agent-4: 165.66666666666677
    agent-5: 165.66666666666677
  policy_reward_mean:
    agent-0: 137.0100000000002
    agent-1: 137.0100000000002
    agent-2: 137.0100000000002
    agent-3: 137.0100000000002
    agent-4: 137.0100000000002
    agent-5: 137.0100000000002
  policy_reward_min:
    agent-0: 55.33333333333331
    agent-1: 55.33333333333331
    agent-2: 55.33333333333331
    agent-3: 55.33333333333331
    agent-4: 55.33333333333331
    agent-5: 55.33333333333331
  sampler_perf:
    mean_env_wait_ms: 24.327161361037902
    mean_inference_ms: 12.294105809542684
    mean_processing_ms: 50.94811125102397
  time_since_restore: 20135.32115316391
  time_this_iter_s: 122.62329936027527
  time_total_s: 29261.332967042923
  timestamp: 1637044165
  timesteps_since_restore: 14880000
  timesteps_this_iter: 96000
  timesteps_total: 20640000
  training_iteration: 215
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    215 |          29261.3 | 20640000 |   822.06 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 101
    apples_agent-0_mean: 6.23
    apples_agent-0_min: 0
    apples_agent-1_max: 75
    apples_agent-1_mean: 28.86
    apples_agent-1_min: 0
    apples_agent-2_max: 84
    apples_agent-2_mean: 3.71
    apples_agent-2_min: 0
    apples_agent-3_max: 159
    apples_agent-3_mean: 112.57
    apples_agent-3_min: 0
    apples_agent-4_max: 34
    apples_agent-4_mean: 1.2
    apples_agent-4_min: 0
    apples_agent-5_max: 142
    apples_agent-5_mean: 91.1
    apples_agent-5_min: 37
    cleaning_beam_agent-0_max: 435
    cleaning_beam_agent-0_mean: 315.28
    cleaning_beam_agent-0_min: 166
    cleaning_beam_agent-1_max: 475
    cleaning_beam_agent-1_mean: 240.19
    cleaning_beam_agent-1_min: 72
    cleaning_beam_agent-2_max: 554
    cleaning_beam_agent-2_mean: 399.02
    cleaning_beam_agent-2_min: 149
    cleaning_beam_agent-3_max: 68
    cleaning_beam_agent-3_mean: 29.4
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 547
    cleaning_beam_agent-4_mean: 429.28
    cleaning_beam_agent-4_min: 283
    cleaning_beam_agent-5_max: 183
    cleaning_beam_agent-5_mean: 42.69
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-31-30
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1084.999999999984
  episode_reward_mean: 821.519999999985
  episode_reward_min: 444.0000000000076
  episodes_this_iter: 96
  episodes_total: 20736
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19730.444
    learner:
      agent-0:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1442195177078247
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016905697993934155
        model: {}
        policy_loss: -0.003665956435725093
        total_loss: -0.003799444530159235
        vf_explained_var: 0.01760178804397583
        vf_loss: 18.80333709716797
      agent-1:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 1.103690505027771
        entropy_coeff: 0.0017600000137463212
        kl: 0.002266181632876396
        model: {}
        policy_loss: -0.004461617209017277
        total_loss: -0.004443732090294361
        vf_explained_var: -0.02323797345161438
        vf_loss: 19.603687286376953
      agent-2:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9941480755805969
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015797290252521634
        model: {}
        policy_loss: -0.0038036874029785395
        total_loss: -0.003740833606570959
        vf_explained_var: 0.05134277045726776
        vf_loss: 18.125503540039062
      agent-3:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.669296383857727
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010830976534634829
        model: {}
        policy_loss: -0.002678876742720604
        total_loss: -0.0022298297844827175
        vf_explained_var: 0.14802831411361694
        vf_loss: 16.270078659057617
      agent-4:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9466132521629333
        entropy_coeff: 0.0017600000137463212
        kl: 0.002403797348961234
        model: {}
        policy_loss: -0.004192567430436611
        total_loss: -0.0039990791119635105
        vf_explained_var: 0.023560956120491028
        vf_loss: 18.595186233520508
      agent-5:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7713762521743774
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008755610906518996
        model: {}
        policy_loss: -0.0035666623152792454
        total_loss: -0.003229978960007429
        vf_explained_var: 0.11529257893562317
        vf_loss: 16.94304084777832
    load_time_ms: 13103.57
    num_steps_sampled: 20736000
    num_steps_trained: 20736000
    sample_time_ms: 89995.908
    update_time_ms: 20.858
  iterations_since_restore: 156
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 16.03276836158192
    ram_util_percent: 10.22316384180791
  pid: 24061
  policy_reward_max:
    agent-0: 180.8333333333335
    agent-1: 180.8333333333335
    agent-2: 180.8333333333335
    agent-3: 180.8333333333335
    agent-4: 180.8333333333335
    agent-5: 180.8333333333335
  policy_reward_mean:
    agent-0: 136.92000000000024
    agent-1: 136.92000000000024
    agent-2: 136.92000000000024
    agent-3: 136.92000000000024
    agent-4: 136.92000000000024
    agent-5: 136.92000000000024
  policy_reward_min:
    agent-0: 73.99999999999982
    agent-1: 73.99999999999982
    agent-2: 73.99999999999982
    agent-3: 73.99999999999982
    agent-4: 73.99999999999982
    agent-5: 73.99999999999982
  sampler_perf:
    mean_env_wait_ms: 24.329564904983336
    mean_inference_ms: 12.293725371186706
    mean_processing_ms: 50.94689117935507
  time_since_restore: 20259.97994494438
  time_this_iter_s: 124.6587917804718
  time_total_s: 29385.991758823395
  timestamp: 1637044290
  timesteps_since_restore: 14976000
  timesteps_this_iter: 96000
  timesteps_total: 20736000
  training_iteration: 216
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    216 |            29386 | 20736000 |   821.52 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 89
    apples_agent-0_mean: 6.73
    apples_agent-0_min: 0
    apples_agent-1_max: 82
    apples_agent-1_mean: 29.02
    apples_agent-1_min: 0
    apples_agent-2_max: 219
    apples_agent-2_mean: 5.2
    apples_agent-2_min: 0
    apples_agent-3_max: 158
    apples_agent-3_mean: 109.27
    apples_agent-3_min: 0
    apples_agent-4_max: 17
    apples_agent-4_mean: 0.68
    apples_agent-4_min: 0
    apples_agent-5_max: 162
    apples_agent-5_mean: 92.47
    apples_agent-5_min: 38
    cleaning_beam_agent-0_max: 511
    cleaning_beam_agent-0_mean: 333.31
    cleaning_beam_agent-0_min: 179
    cleaning_beam_agent-1_max: 535
    cleaning_beam_agent-1_mean: 226.65
    cleaning_beam_agent-1_min: 64
    cleaning_beam_agent-2_max: 558
    cleaning_beam_agent-2_mean: 404.38
    cleaning_beam_agent-2_min: 175
    cleaning_beam_agent-3_max: 76
    cleaning_beam_agent-3_mean: 27.88
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 562
    cleaning_beam_agent-4_mean: 452.39
    cleaning_beam_agent-4_min: 344
    cleaning_beam_agent-5_max: 176
    cleaning_beam_agent-5_mean: 41.83
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-33-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1083.9999999999939
  episode_reward_mean: 850.2399999999855
  episode_reward_min: 356.0000000000055
  episodes_this_iter: 96
  episodes_total: 20832
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19714.032
    learner:
      agent-0:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1263551712036133
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013699039118364453
        model: {}
        policy_loss: -0.0037409793585538864
        total_loss: -0.0038680071011185646
        vf_explained_var: -0.001341983675956726
        vf_loss: 18.55359649658203
      agent-1:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0971097946166992
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014423970133066177
        model: {}
        policy_loss: -0.004240971989929676
        total_loss: -0.004235228523612022
        vf_explained_var: -0.02510625123977661
        vf_loss: 19.366519927978516
      agent-2:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9915022850036621
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014517364325001836
        model: {}
        policy_loss: -0.003638113848865032
        total_loss: -0.003531350288540125
        vf_explained_var: 2.8014183044433594e-05
        vf_loss: 18.51806640625
      agent-3:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6476674675941467
        entropy_coeff: 0.0017600000137463212
        kl: 0.001246583997271955
        model: {}
        policy_loss: -0.0027176253497600555
        total_loss: -0.0022506481036543846
        vf_explained_var: 0.12367214262485504
        vf_loss: 16.0687255859375
      agent-4:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9438099265098572
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014774687588214874
        model: {}
        policy_loss: -0.004213307984173298
        total_loss: -0.004033590201288462
        vf_explained_var: -0.002942904829978943
        vf_loss: 18.408239364624023
      agent-5:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7613503336906433
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009536303696222603
        model: {}
        policy_loss: -0.0034955732990056276
        total_loss: -0.0030952850356698036
        vf_explained_var: 0.06568413972854614
        vf_loss: 17.40262222290039
    load_time_ms: 13096.892
    num_steps_sampled: 20832000
    num_steps_trained: 20832000
    sample_time_ms: 89996.332
    update_time_ms: 20.534
  iterations_since_restore: 157
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.466091954022987
    ram_util_percent: 10.144827586206892
  pid: 24061
  policy_reward_max:
    agent-0: 180.66666666666657
    agent-1: 180.66666666666657
    agent-2: 180.66666666666657
    agent-3: 180.66666666666657
    agent-4: 180.66666666666657
    agent-5: 180.66666666666657
  policy_reward_mean:
    agent-0: 141.70666666666685
    agent-1: 141.70666666666685
    agent-2: 141.70666666666685
    agent-3: 141.70666666666685
    agent-4: 141.70666666666685
    agent-5: 141.70666666666685
  policy_reward_min:
    agent-0: 59.33333333333303
    agent-1: 59.33333333333303
    agent-2: 59.33333333333303
    agent-3: 59.33333333333303
    agent-4: 59.33333333333303
    agent-5: 59.33333333333303
  sampler_perf:
    mean_env_wait_ms: 24.33159993258174
    mean_inference_ms: 12.292785806050883
    mean_processing_ms: 50.94306878710882
  time_since_restore: 20382.582790136337
  time_this_iter_s: 122.60284519195557
  time_total_s: 29508.59460401535
  timestamp: 1637044412
  timesteps_since_restore: 15072000
  timesteps_this_iter: 96000
  timesteps_total: 20832000
  training_iteration: 217
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    217 |          29508.6 | 20832000 |   850.24 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 60
    apples_agent-0_mean: 5.1
    apples_agent-0_min: 0
    apples_agent-1_max: 123
    apples_agent-1_mean: 33.89
    apples_agent-1_min: 0
    apples_agent-2_max: 78
    apples_agent-2_mean: 4.54
    apples_agent-2_min: 0
    apples_agent-3_max: 183
    apples_agent-3_mean: 111.27
    apples_agent-3_min: 50
    apples_agent-4_max: 30
    apples_agent-4_mean: 0.74
    apples_agent-4_min: 0
    apples_agent-5_max: 140
    apples_agent-5_mean: 89.46
    apples_agent-5_min: 29
    cleaning_beam_agent-0_max: 492
    cleaning_beam_agent-0_mean: 334.26
    cleaning_beam_agent-0_min: 106
    cleaning_beam_agent-1_max: 611
    cleaning_beam_agent-1_mean: 247.58
    cleaning_beam_agent-1_min: 101
    cleaning_beam_agent-2_max: 594
    cleaning_beam_agent-2_mean: 395.92
    cleaning_beam_agent-2_min: 145
    cleaning_beam_agent-3_max: 139
    cleaning_beam_agent-3_mean: 32.0
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 558
    cleaning_beam_agent-4_mean: 446.58
    cleaning_beam_agent-4_min: 317
    cleaning_beam_agent-5_max: 146
    cleaning_beam_agent-5_mean: 45.75
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-35-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 988.9999999999677
  episode_reward_mean: 843.7399999999849
  episode_reward_min: 410.0000000000032
  episodes_this_iter: 96
  episodes_total: 20928
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19765.074
    learner:
      agent-0:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1162160634994507
        entropy_coeff: 0.0017600000137463212
        kl: 0.002016983926296234
        model: {}
        policy_loss: -0.0039394330233335495
        total_loss: -0.00397653691470623
        vf_explained_var: 0.02364341914653778
        vf_loss: 19.274349212646484
      agent-1:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 1.092586874961853
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014247563667595387
        model: {}
        policy_loss: -0.004200570750981569
        total_loss: -0.004144380800426006
        vf_explained_var: -0.0008588135242462158
        vf_loss: 19.791439056396484
      agent-2:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9882791638374329
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011254440760239959
        model: {}
        policy_loss: -0.003493702504783869
        total_loss: -0.0033479209523648024
        vf_explained_var: 0.05227437615394592
        vf_loss: 18.8515567779541
      agent-3:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6606394052505493
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012585566146299243
        model: {}
        policy_loss: -0.0028469308745115995
        total_loss: -0.0023511264007538557
        vf_explained_var: 0.15290778875350952
        vf_loss: 16.58529281616211
      agent-4:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9528987407684326
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013795106206089258
        model: {}
        policy_loss: -0.003896786365658045
        total_loss: -0.0036756442859768867
        vf_explained_var: 0.028230398893356323
        vf_loss: 18.982418060302734
      agent-5:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7556673288345337
        entropy_coeff: 0.0017600000137463212
        kl: 0.002219631802290678
        model: {}
        policy_loss: -0.003993107005953789
        total_loss: -0.0036247936077415943
        vf_explained_var: 0.13861395418643951
        vf_loss: 16.98285675048828
    load_time_ms: 13080.458
    num_steps_sampled: 20928000
    num_steps_trained: 20928000
    sample_time_ms: 90086.722
    update_time_ms: 21.068
  iterations_since_restore: 158
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.407386363636363
    ram_util_percent: 10.136363636363637
  pid: 24061
  policy_reward_max:
    agent-0: 164.83333333333277
    agent-1: 164.83333333333277
    agent-2: 164.83333333333277
    agent-3: 164.83333333333277
    agent-4: 164.83333333333277
    agent-5: 164.83333333333277
  policy_reward_mean:
    agent-0: 140.62333333333348
    agent-1: 140.62333333333348
    agent-2: 140.62333333333348
    agent-3: 140.62333333333348
    agent-4: 140.62333333333348
    agent-5: 140.62333333333348
  policy_reward_min:
    agent-0: 68.33333333333317
    agent-1: 68.33333333333317
    agent-2: 68.33333333333317
    agent-3: 68.33333333333317
    agent-4: 68.33333333333317
    agent-5: 68.33333333333317
  sampler_perf:
    mean_env_wait_ms: 24.334162603832677
    mean_inference_ms: 12.291451555159945
    mean_processing_ms: 50.93856344150167
  time_since_restore: 20505.561002731323
  time_this_iter_s: 122.97821259498596
  time_total_s: 29631.572816610336
  timestamp: 1637044536
  timesteps_since_restore: 15168000
  timesteps_this_iter: 96000
  timesteps_total: 20928000
  training_iteration: 218
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    218 |          29631.6 | 20928000 |   843.74 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 43
    apples_agent-0_mean: 4.06
    apples_agent-0_min: 0
    apples_agent-1_max: 98
    apples_agent-1_mean: 32.44
    apples_agent-1_min: 0
    apples_agent-2_max: 85
    apples_agent-2_mean: 6.12
    apples_agent-2_min: 0
    apples_agent-3_max: 242
    apples_agent-3_mean: 112.34
    apples_agent-3_min: 31
    apples_agent-4_max: 21
    apples_agent-4_mean: 0.64
    apples_agent-4_min: 0
    apples_agent-5_max: 271
    apples_agent-5_mean: 93.51
    apples_agent-5_min: 24
    cleaning_beam_agent-0_max: 499
    cleaning_beam_agent-0_mean: 341.24
    cleaning_beam_agent-0_min: 193
    cleaning_beam_agent-1_max: 464
    cleaning_beam_agent-1_mean: 248.05
    cleaning_beam_agent-1_min: 102
    cleaning_beam_agent-2_max: 594
    cleaning_beam_agent-2_mean: 391.35
    cleaning_beam_agent-2_min: 203
    cleaning_beam_agent-3_max: 156
    cleaning_beam_agent-3_mean: 31.99
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 564
    cleaning_beam_agent-4_mean: 439.42
    cleaning_beam_agent-4_min: 330
    cleaning_beam_agent-5_max: 209
    cleaning_beam_agent-5_mean: 47.38
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-37-39
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1099.9999999999893
  episode_reward_mean: 848.129999999986
  episode_reward_min: 239.99999999999866
  episodes_this_iter: 96
  episodes_total: 21024
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19703.405
    learner:
      agent-0:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1219708919525146
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013692635111510754
        model: {}
        policy_loss: -0.0035586468875408173
        total_loss: -0.0035976485814899206
        vf_explained_var: 0.04981189966201782
        vf_loss: 19.35668182373047
      agent-1:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0964888334274292
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017263134941458702
        model: {}
        policy_loss: -0.004294830374419689
        total_loss: -0.004170025233179331
        vf_explained_var: -0.0008136630058288574
        vf_loss: 20.54624366760254
      agent-2:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.995800793170929
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016444235807284713
        model: {}
        policy_loss: -0.0038219979032874107
        total_loss: -0.0036608665250241756
        vf_explained_var: 0.07100288569927216
        vf_loss: 19.137420654296875
      agent-3:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6537164449691772
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010960103245452046
        model: {}
        policy_loss: -0.0026968498714268208
        total_loss: -0.002093238290399313
        vf_explained_var: 0.1323964148759842
        vf_loss: 17.541513442993164
      agent-4:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9518558979034424
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015200541820377111
        model: {}
        policy_loss: -0.004085076041519642
        total_loss: -0.0037754199001938105
        vf_explained_var: 0.0249052494764328
        vf_loss: 19.849212646484375
      agent-5:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7569818496704102
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012327264994382858
        model: {}
        policy_loss: -0.0036543142050504684
        total_loss: -0.0031604748219251633
        vf_explained_var: 0.1140815019607544
        vf_loss: 18.26124382019043
    load_time_ms: 13088.297
    num_steps_sampled: 21024000
    num_steps_trained: 21024000
    sample_time_ms: 90158.873
    update_time_ms: 21.111
  iterations_since_restore: 159
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.407471264367816
    ram_util_percent: 10.147126436781607
  pid: 24061
  policy_reward_max:
    agent-0: 183.3333333333327
    agent-1: 183.3333333333327
    agent-2: 183.3333333333327
    agent-3: 183.3333333333327
    agent-4: 183.3333333333327
    agent-5: 183.3333333333327
  policy_reward_mean:
    agent-0: 141.35500000000013
    agent-1: 141.35500000000013
    agent-2: 141.35500000000013
    agent-3: 141.35500000000013
    agent-4: 141.35500000000013
    agent-5: 141.35500000000013
  policy_reward_min:
    agent-0: 39.99999999999999
    agent-1: 39.99999999999999
    agent-2: 39.99999999999999
    agent-3: 39.99999999999999
    agent-4: 39.99999999999999
    agent-5: 39.99999999999999
  sampler_perf:
    mean_env_wait_ms: 24.3356654277756
    mean_inference_ms: 12.290225562168539
    mean_processing_ms: 50.93279535597897
  time_since_restore: 20628.23118162155
  time_this_iter_s: 122.67017889022827
  time_total_s: 29754.242995500565
  timestamp: 1637044659
  timesteps_since_restore: 15264000
  timesteps_this_iter: 96000
  timesteps_total: 21024000
  training_iteration: 219
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    219 |          29754.2 | 21024000 |   848.13 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 38
    apples_agent-0_mean: 2.87
    apples_agent-0_min: 0
    apples_agent-1_max: 133
    apples_agent-1_mean: 33.14
    apples_agent-1_min: 0
    apples_agent-2_max: 172
    apples_agent-2_mean: 6.11
    apples_agent-2_min: 0
    apples_agent-3_max: 165
    apples_agent-3_mean: 111.54
    apples_agent-3_min: 0
    apples_agent-4_max: 78
    apples_agent-4_mean: 2.16
    apples_agent-4_min: 0
    apples_agent-5_max: 227
    apples_agent-5_mean: 89.92
    apples_agent-5_min: 34
    cleaning_beam_agent-0_max: 478
    cleaning_beam_agent-0_mean: 345.6
    cleaning_beam_agent-0_min: 194
    cleaning_beam_agent-1_max: 512
    cleaning_beam_agent-1_mean: 249.24
    cleaning_beam_agent-1_min: 107
    cleaning_beam_agent-2_max: 538
    cleaning_beam_agent-2_mean: 386.53
    cleaning_beam_agent-2_min: 189
    cleaning_beam_agent-3_max: 79
    cleaning_beam_agent-3_mean: 31.55
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 545
    cleaning_beam_agent-4_mean: 438.36
    cleaning_beam_agent-4_min: 257
    cleaning_beam_agent-5_max: 193
    cleaning_beam_agent-5_mean: 49.7
    cleaning_beam_agent-5_min: 14
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-39-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1046.99999999999
  episode_reward_mean: 832.3999999999862
  episode_reward_min: 479.0000000000116
  episodes_this_iter: 96
  episodes_total: 21120
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19685.157
    learner:
      agent-0:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1344189643859863
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014407559065148234
        model: {}
        policy_loss: -0.003401996335014701
        total_loss: -0.003571506356820464
        vf_explained_var: 0.010748565196990967
        vf_loss: 18.270671844482422
      agent-1:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0964479446411133
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013276757672429085
        model: {}
        policy_loss: -0.004137498326599598
        total_loss: -0.004147796891629696
        vf_explained_var: -0.03183808922767639
        vf_loss: 19.194475173950195
      agent-2:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9893020391464233
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019824239425361156
        model: {}
        policy_loss: -0.004114190582185984
        total_loss: -0.0040710377506911755
        vf_explained_var: 0.04170413315296173
        vf_loss: 17.84326171875
      agent-3:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6595657467842102
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014240150339901447
        model: {}
        policy_loss: -0.0028470968827605247
        total_loss: -0.0024359943345189095
        vf_explained_var: 0.14953336119651794
        vf_loss: 15.719382286071777
      agent-4:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9548524618148804
        entropy_coeff: 0.0017600000137463212
        kl: 0.00196260679513216
        model: {}
        policy_loss: -0.0043290043249726295
        total_loss: -0.004225163720548153
        vf_explained_var: 0.033387839794158936
        vf_loss: 17.843780517578125
      agent-5:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7585180997848511
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010612534824758768
        model: {}
        policy_loss: -0.0035440772771835327
        total_loss: -0.003218995872884989
        vf_explained_var: 0.10292086005210876
        vf_loss: 16.600738525390625
    load_time_ms: 13095.392
    num_steps_sampled: 21120000
    num_steps_trained: 21120000
    sample_time_ms: 90185.896
    update_time_ms: 20.623
  iterations_since_restore: 160
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.314204545454546
    ram_util_percent: 10.131818181818181
  pid: 24061
  policy_reward_max:
    agent-0: 174.4999999999997
    agent-1: 174.4999999999997
    agent-2: 174.4999999999997
    agent-3: 174.4999999999997
    agent-4: 174.4999999999997
    agent-5: 174.4999999999997
  policy_reward_mean:
    agent-0: 138.73333333333352
    agent-1: 138.73333333333352
    agent-2: 138.73333333333352
    agent-3: 138.73333333333352
    agent-4: 138.73333333333352
    agent-5: 138.73333333333352
  policy_reward_min:
    agent-0: 79.8333333333336
    agent-1: 79.8333333333336
    agent-2: 79.8333333333336
    agent-3: 79.8333333333336
    agent-4: 79.8333333333336
    agent-5: 79.8333333333336
  sampler_perf:
    mean_env_wait_ms: 24.337418847557522
    mean_inference_ms: 12.289137829373587
    mean_processing_ms: 50.92792902473451
  time_since_restore: 20751.646287679672
  time_this_iter_s: 123.41510605812073
  time_total_s: 29877.658101558685
  timestamp: 1637044782
  timesteps_since_restore: 15360000
  timesteps_this_iter: 96000
  timesteps_total: 21120000
  training_iteration: 220
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    220 |          29877.7 | 21120000 |    832.4 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 36
    apples_agent-0_mean: 4.99
    apples_agent-0_min: 0
    apples_agent-1_max: 135
    apples_agent-1_mean: 32.81
    apples_agent-1_min: 0
    apples_agent-2_max: 58
    apples_agent-2_mean: 1.53
    apples_agent-2_min: 0
    apples_agent-3_max: 169
    apples_agent-3_mean: 112.17
    apples_agent-3_min: 42
    apples_agent-4_max: 57
    apples_agent-4_mean: 1.59
    apples_agent-4_min: 0
    apples_agent-5_max: 162
    apples_agent-5_mean: 93.21
    apples_agent-5_min: 22
    cleaning_beam_agent-0_max: 468
    cleaning_beam_agent-0_mean: 325.51
    cleaning_beam_agent-0_min: 183
    cleaning_beam_agent-1_max: 492
    cleaning_beam_agent-1_mean: 252.62
    cleaning_beam_agent-1_min: 87
    cleaning_beam_agent-2_max: 520
    cleaning_beam_agent-2_mean: 393.8
    cleaning_beam_agent-2_min: 212
    cleaning_beam_agent-3_max: 93
    cleaning_beam_agent-3_mean: 30.88
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 542
    cleaning_beam_agent-4_mean: 441.35
    cleaning_beam_agent-4_min: 318
    cleaning_beam_agent-5_max: 224
    cleaning_beam_agent-5_mean: 46.45
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-41-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1050.9999999999918
  episode_reward_mean: 868.1099999999855
  episode_reward_min: 415.00000000000506
  episodes_this_iter: 96
  episodes_total: 21216
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19694.345
    learner:
      agent-0:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 1.132493495941162
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018503384198993444
        model: {}
        policy_loss: -0.003750130534172058
        total_loss: -0.003908034414052963
        vf_explained_var: 0.02236345410346985
        vf_loss: 18.352828979492188
      agent-1:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0972193479537964
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012014770181849599
        model: {}
        policy_loss: -0.0038915351033210754
        total_loss: -0.003920655231922865
        vf_explained_var: -0.015060365200042725
        vf_loss: 19.019865036010742
      agent-2:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9911279678344727
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018170661060139537
        model: {}
        policy_loss: -0.003743947483599186
        total_loss: -0.0037004793994128704
        vf_explained_var: 0.032196804881095886
        vf_loss: 17.878520965576172
      agent-3:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.645016074180603
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011862731771543622
        model: {}
        policy_loss: -0.0028273388743400574
        total_loss: -0.0022376375272870064
        vf_explained_var: 0.06816014647483826
        vf_loss: 17.249282836914062
      agent-4:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9537472724914551
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011513832723721862
        model: {}
        policy_loss: -0.004109857603907585
        total_loss: -0.00402824766933918
        vf_explained_var: 0.058107972145080566
        vf_loss: 17.60207176208496
      agent-5:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.754870593547821
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009375485824421048
        model: {}
        policy_loss: -0.0035046704579144716
        total_loss: -0.003123136004433036
        vf_explained_var: 0.09577174484729767
        vf_loss: 17.101099014282227
    load_time_ms: 13083.558
    num_steps_sampled: 21216000
    num_steps_trained: 21216000
    sample_time_ms: 90231.961
    update_time_ms: 21.629
  iterations_since_restore: 161
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.379428571428575
    ram_util_percent: 10.145142857142854
  pid: 24061
  policy_reward_max:
    agent-0: 175.16666666666623
    agent-1: 175.16666666666623
    agent-2: 175.16666666666623
    agent-3: 175.16666666666623
    agent-4: 175.16666666666623
    agent-5: 175.16666666666623
  policy_reward_mean:
    agent-0: 144.68500000000006
    agent-1: 144.68500000000006
    agent-2: 144.68500000000006
    agent-3: 144.68500000000006
    agent-4: 144.68500000000006
    agent-5: 144.68500000000006
  policy_reward_min:
    agent-0: 69.16666666666654
    agent-1: 69.16666666666654
    agent-2: 69.16666666666654
    agent-3: 69.16666666666654
    agent-4: 69.16666666666654
    agent-5: 69.16666666666654
  sampler_perf:
    mean_env_wait_ms: 24.33892581587777
    mean_inference_ms: 12.287810133168291
    mean_processing_ms: 50.92301568089842
  time_since_restore: 20874.22519135475
  time_this_iter_s: 122.57890367507935
  time_total_s: 30000.237005233765
  timestamp: 1637044905
  timesteps_since_restore: 15456000
  timesteps_this_iter: 96000
  timesteps_total: 21216000
  training_iteration: 221
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    221 |          30000.2 | 21216000 |   868.11 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 48
    apples_agent-0_mean: 3.96
    apples_agent-0_min: 0
    apples_agent-1_max: 105
    apples_agent-1_mean: 33.15
    apples_agent-1_min: 0
    apples_agent-2_max: 37
    apples_agent-2_mean: 3.03
    apples_agent-2_min: 0
    apples_agent-3_max: 171
    apples_agent-3_mean: 112.22
    apples_agent-3_min: 46
    apples_agent-4_max: 41
    apples_agent-4_mean: 1.96
    apples_agent-4_min: 0
    apples_agent-5_max: 147
    apples_agent-5_mean: 90.13
    apples_agent-5_min: 26
    cleaning_beam_agent-0_max: 478
    cleaning_beam_agent-0_mean: 352.21
    cleaning_beam_agent-0_min: 142
    cleaning_beam_agent-1_max: 400
    cleaning_beam_agent-1_mean: 233.27
    cleaning_beam_agent-1_min: 81
    cleaning_beam_agent-2_max: 563
    cleaning_beam_agent-2_mean: 401.4
    cleaning_beam_agent-2_min: 194
    cleaning_beam_agent-3_max: 76
    cleaning_beam_agent-3_mean: 26.66
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 556
    cleaning_beam_agent-4_mean: 438.04
    cleaning_beam_agent-4_min: 225
    cleaning_beam_agent-5_max: 150
    cleaning_beam_agent-5_mean: 45.17
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-43-50
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1062.9999999999916
  episode_reward_mean: 862.8199999999841
  episode_reward_min: 309.0000000000005
  episodes_this_iter: 96
  episodes_total: 21312
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19706.383
    learner:
      agent-0:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 1.127036452293396
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016393556725233793
        model: {}
        policy_loss: -0.003549609798938036
        total_loss: -0.0035344157367944717
        vf_explained_var: 0.013075411319732666
        vf_loss: 19.987770080566406
      agent-1:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1000832319259644
        entropy_coeff: 0.0017600000137463212
        kl: 0.001894245040602982
        model: {}
        policy_loss: -0.004204942844808102
        total_loss: -0.004027342423796654
        vf_explained_var: -0.030044838786125183
        vf_loss: 21.137441635131836
      agent-2:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9749971628189087
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018706852570176125
        model: {}
        policy_loss: -0.0038465785328298807
        total_loss: -0.0036696139723062515
        vf_explained_var: 0.06150633096694946
        vf_loss: 18.92963409423828
      agent-3:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6367308497428894
        entropy_coeff: 0.0017600000137463212
        kl: 0.001987152500078082
        model: {}
        policy_loss: -0.0028348793275654316
        total_loss: -0.0022922148928046227
        vf_explained_var: 0.17289705574512482
        vf_loss: 16.633121490478516
      agent-4:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9576034545898438
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015707381535321474
        model: {}
        policy_loss: -0.004159057978540659
        total_loss: -0.003921736031770706
        vf_explained_var: 0.05782386660575867
        vf_loss: 19.227022171020508
      agent-5:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7654752731323242
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009098381269723177
        model: {}
        policy_loss: -0.0036834636703133583
        total_loss: -0.003177206963300705
        vf_explained_var: 0.09424249827861786
        vf_loss: 18.53496551513672
    load_time_ms: 12963.404
    num_steps_sampled: 21312000
    num_steps_trained: 21312000
    sample_time_ms: 90340.434
    update_time_ms: 21.443
  iterations_since_restore: 162
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.320224719101123
    ram_util_percent: 10.153370786516852
  pid: 24061
  policy_reward_max:
    agent-0: 177.1666666666664
    agent-1: 177.1666666666664
    agent-2: 177.1666666666664
    agent-3: 177.1666666666664
    agent-4: 177.1666666666664
    agent-5: 177.1666666666664
  policy_reward_mean:
    agent-0: 143.80333333333346
    agent-1: 143.80333333333346
    agent-2: 143.80333333333346
    agent-3: 143.80333333333346
    agent-4: 143.80333333333346
    agent-5: 143.80333333333346
  policy_reward_min:
    agent-0: 51.50000000000003
    agent-1: 51.50000000000003
    agent-2: 51.50000000000003
    agent-3: 51.50000000000003
    agent-4: 51.50000000000003
    agent-5: 51.50000000000003
  sampler_perf:
    mean_env_wait_ms: 24.3415362307597
    mean_inference_ms: 12.287452601928507
    mean_processing_ms: 50.92074440673507
  time_since_restore: 20998.554789066315
  time_this_iter_s: 124.32959771156311
  time_total_s: 30124.566602945328
  timestamp: 1637045030
  timesteps_since_restore: 15552000
  timesteps_this_iter: 96000
  timesteps_total: 21312000
  training_iteration: 222
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    222 |          30124.6 | 21312000 |   862.82 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 70
    apples_agent-0_mean: 3.84
    apples_agent-0_min: 0
    apples_agent-1_max: 106
    apples_agent-1_mean: 33.01
    apples_agent-1_min: 0
    apples_agent-2_max: 56
    apples_agent-2_mean: 2.72
    apples_agent-2_min: 0
    apples_agent-3_max: 351
    apples_agent-3_mean: 116.32
    apples_agent-3_min: 47
    apples_agent-4_max: 59
    apples_agent-4_mean: 1.82
    apples_agent-4_min: 0
    apples_agent-5_max: 208
    apples_agent-5_mean: 90.82
    apples_agent-5_min: 23
    cleaning_beam_agent-0_max: 505
    cleaning_beam_agent-0_mean: 352.91
    cleaning_beam_agent-0_min: 128
    cleaning_beam_agent-1_max: 411
    cleaning_beam_agent-1_mean: 250.44
    cleaning_beam_agent-1_min: 119
    cleaning_beam_agent-2_max: 584
    cleaning_beam_agent-2_mean: 414.95
    cleaning_beam_agent-2_min: 265
    cleaning_beam_agent-3_max: 78
    cleaning_beam_agent-3_mean: 26.85
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 555
    cleaning_beam_agent-4_mean: 439.51
    cleaning_beam_agent-4_min: 294
    cleaning_beam_agent-5_max: 219
    cleaning_beam_agent-5_mean: 43.4
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-45-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1078.9999999999973
  episode_reward_mean: 871.9999999999841
  episode_reward_min: 506.000000000012
  episodes_this_iter: 96
  episodes_total: 21408
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19676.022
    learner:
      agent-0:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1255468130111694
        entropy_coeff: 0.0017600000137463212
        kl: 0.00112240354064852
        model: {}
        policy_loss: -0.003146685194224119
        total_loss: -0.0031554419547319412
        vf_explained_var: 0.0008632242679595947
        vf_loss: 19.722043991088867
      agent-1:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0836454629898071
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018147035734727979
        model: {}
        policy_loss: -0.004605475347489119
        total_loss: -0.004533771425485611
        vf_explained_var: 0.013298600912094116
        vf_loss: 19.78920555114746
      agent-2:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9885439276695251
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016446380177512765
        model: {}
        policy_loss: -0.003793919924646616
        total_loss: -0.0036836806684732437
        vf_explained_var: 0.059409886598587036
        vf_loss: 18.500747680664062
      agent-3:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6365634202957153
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014120650012046099
        model: {}
        policy_loss: -0.002945507178083062
        total_loss: -0.002351773902773857
        vf_explained_var: 0.13020767271518707
        vf_loss: 17.140846252441406
      agent-4:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9480217695236206
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013874201104044914
        model: {}
        policy_loss: -0.004351196810603142
        total_loss: -0.004141917452216148
        vf_explained_var: 0.0490364134311676
        vf_loss: 18.777971267700195
      agent-5:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7387391328811646
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011788848787546158
        model: {}
        policy_loss: -0.003750817384570837
        total_loss: -0.003280663164332509
        vf_explained_var: 0.10805873572826385
        vf_loss: 17.703338623046875
    load_time_ms: 12999.845
    num_steps_sampled: 21408000
    num_steps_trained: 21408000
    sample_time_ms: 90402.409
    update_time_ms: 22.153
  iterations_since_restore: 163
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.340340909090907
    ram_util_percent: 10.132386363636362
  pid: 24061
  policy_reward_max:
    agent-0: 179.83333333333297
    agent-1: 179.83333333333297
    agent-2: 179.83333333333297
    agent-3: 179.83333333333297
    agent-4: 179.83333333333297
    agent-5: 179.83333333333297
  policy_reward_mean:
    agent-0: 145.3333333333334
    agent-1: 145.3333333333334
    agent-2: 145.3333333333334
    agent-3: 145.3333333333334
    agent-4: 145.3333333333334
    agent-5: 145.3333333333334
  policy_reward_min:
    agent-0: 84.33333333333357
    agent-1: 84.33333333333357
    agent-2: 84.33333333333357
    agent-3: 84.33333333333357
    agent-4: 84.33333333333357
    agent-5: 84.33333333333357
  sampler_perf:
    mean_env_wait_ms: 24.343189851860053
    mean_inference_ms: 12.28607388776496
    mean_processing_ms: 50.91633477881619
  time_since_restore: 21121.827628850937
  time_this_iter_s: 123.27283978462219
  time_total_s: 30247.83944272995
  timestamp: 1637045153
  timesteps_since_restore: 15648000
  timesteps_this_iter: 96000
  timesteps_total: 21408000
  training_iteration: 223
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    223 |          30247.8 | 21408000 |      872 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 55
    apples_agent-0_mean: 2.43
    apples_agent-0_min: 0
    apples_agent-1_max: 126
    apples_agent-1_mean: 30.91
    apples_agent-1_min: 0
    apples_agent-2_max: 56
    apples_agent-2_mean: 3.62
    apples_agent-2_min: 0
    apples_agent-3_max: 176
    apples_agent-3_mean: 113.08
    apples_agent-3_min: 9
    apples_agent-4_max: 41
    apples_agent-4_mean: 1.04
    apples_agent-4_min: 0
    apples_agent-5_max: 140
    apples_agent-5_mean: 92.5
    apples_agent-5_min: 32
    cleaning_beam_agent-0_max: 503
    cleaning_beam_agent-0_mean: 363.95
    cleaning_beam_agent-0_min: 184
    cleaning_beam_agent-1_max: 476
    cleaning_beam_agent-1_mean: 264.99
    cleaning_beam_agent-1_min: 116
    cleaning_beam_agent-2_max: 578
    cleaning_beam_agent-2_mean: 406.29
    cleaning_beam_agent-2_min: 168
    cleaning_beam_agent-3_max: 100
    cleaning_beam_agent-3_mean: 26.63
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 580
    cleaning_beam_agent-4_mean: 448.09
    cleaning_beam_agent-4_min: 307
    cleaning_beam_agent-5_max: 139
    cleaning_beam_agent-5_mean: 39.9
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-47-56
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1062.9999999999834
  episode_reward_mean: 883.9499999999855
  episode_reward_min: 407.00000000001006
  episodes_this_iter: 96
  episodes_total: 21504
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19661.72
    learner:
      agent-0:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 1.112318754196167
        entropy_coeff: 0.0017600000137463212
        kl: 0.001837205607444048
        model: {}
        policy_loss: -0.0035192910581827164
        total_loss: -0.0035518519580364227
        vf_explained_var: 0.029854580760002136
        vf_loss: 19.251182556152344
      agent-1:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1053045988082886
        entropy_coeff: 0.0017600000137463212
        kl: 0.001311395550146699
        model: {}
        policy_loss: -0.0038555788341909647
        total_loss: -0.003824138082563877
        vf_explained_var: 0.016368761658668518
        vf_loss: 19.767780303955078
      agent-2:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9802128076553345
        entropy_coeff: 0.0017600000137463212
        kl: 0.002176329493522644
        model: {}
        policy_loss: -0.003541233716532588
        total_loss: -0.0033691420685499907
        vf_explained_var: 0.04590804874897003
        vf_loss: 18.972633361816406
      agent-3:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6288081407546997
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011781456414610147
        model: {}
        policy_loss: -0.002490546554327011
        total_loss: -0.0018664700910449028
        vf_explained_var: 0.12498170137405396
        vf_loss: 17.30778694152832
      agent-4:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.94244784116745
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014934586361050606
        model: {}
        policy_loss: -0.003975383006036282
        total_loss: -0.003685900941491127
        vf_explained_var: 0.018803507089614868
        vf_loss: 19.481904983520508
      agent-5:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.737338662147522
        entropy_coeff: 0.0017600000137463212
        kl: 0.001265327213332057
        model: {}
        policy_loss: -0.003369509242475033
        total_loss: -0.0028604771941900253
        vf_explained_var: 0.09414929151535034
        vf_loss: 18.067489624023438
    load_time_ms: 13011.478
    num_steps_sampled: 21504000
    num_steps_trained: 21504000
    sample_time_ms: 90414.63
    update_time_ms: 21.796
  iterations_since_restore: 164
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.422285714285715
    ram_util_percent: 10.14742857142857
  pid: 24061
  policy_reward_max:
    agent-0: 177.16666666666578
    agent-1: 177.16666666666578
    agent-2: 177.16666666666578
    agent-3: 177.16666666666578
    agent-4: 177.16666666666578
    agent-5: 177.16666666666578
  policy_reward_mean:
    agent-0: 147.32500000000005
    agent-1: 147.32500000000005
    agent-2: 147.32500000000005
    agent-3: 147.32500000000005
    agent-4: 147.32500000000005
    agent-5: 147.32500000000005
  policy_reward_min:
    agent-0: 67.83333333333324
    agent-1: 67.83333333333324
    agent-2: 67.83333333333324
    agent-3: 67.83333333333324
    agent-4: 67.83333333333324
    agent-5: 67.83333333333324
  sampler_perf:
    mean_env_wait_ms: 24.346396909320443
    mean_inference_ms: 12.285046115032038
    mean_processing_ms: 50.91339130303134
  time_since_restore: 21244.78920674324
  time_this_iter_s: 122.96157789230347
  time_total_s: 30370.801020622253
  timestamp: 1637045276
  timesteps_since_restore: 15744000
  timesteps_this_iter: 96000
  timesteps_total: 21504000
  training_iteration: 224
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    224 |          30370.8 | 21504000 |   883.95 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 66
    apples_agent-0_mean: 4.84
    apples_agent-0_min: 0
    apples_agent-1_max: 130
    apples_agent-1_mean: 33.1
    apples_agent-1_min: 0
    apples_agent-2_max: 68
    apples_agent-2_mean: 3.25
    apples_agent-2_min: 0
    apples_agent-3_max: 199
    apples_agent-3_mean: 119.52
    apples_agent-3_min: 46
    apples_agent-4_max: 35
    apples_agent-4_mean: 0.77
    apples_agent-4_min: 0
    apples_agent-5_max: 189
    apples_agent-5_mean: 96.08
    apples_agent-5_min: 22
    cleaning_beam_agent-0_max: 503
    cleaning_beam_agent-0_mean: 354.74
    cleaning_beam_agent-0_min: 177
    cleaning_beam_agent-1_max: 491
    cleaning_beam_agent-1_mean: 257.38
    cleaning_beam_agent-1_min: 96
    cleaning_beam_agent-2_max: 538
    cleaning_beam_agent-2_mean: 396.91
    cleaning_beam_agent-2_min: 113
    cleaning_beam_agent-3_max: 101
    cleaning_beam_agent-3_mean: 28.02
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 565
    cleaning_beam_agent-4_mean: 444.77
    cleaning_beam_agent-4_min: 295
    cleaning_beam_agent-5_max: 161
    cleaning_beam_agent-5_mean: 43.72
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-49-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1054.9999999999795
  episode_reward_mean: 878.1299999999851
  episode_reward_min: 314.0000000000005
  episodes_this_iter: 96
  episodes_total: 21600
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19661.176
    learner:
      agent-0:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1187318563461304
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011943259742110968
        model: {}
        policy_loss: -0.0035636755637824535
        total_loss: -0.0037007476203143597
        vf_explained_var: 0.03391799330711365
        vf_loss: 18.31898307800293
      agent-1:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1045494079589844
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014478234807029366
        model: {}
        policy_loss: -0.004330240190029144
        total_loss: -0.004329540766775608
        vf_explained_var: -0.02737221121788025
        vf_loss: 19.447036743164062
      agent-2:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9911878705024719
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016613599145784974
        model: {}
        policy_loss: -0.0036225870717316866
        total_loss: -0.003599439049139619
        vf_explained_var: 0.06700558960437775
        vf_loss: 17.676406860351562
      agent-3:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6301793456077576
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011039521778002381
        model: {}
        policy_loss: -0.0028126263059675694
        total_loss: -0.0023059872910380363
        vf_explained_var: 0.13504193723201752
        vf_loss: 16.157546997070312
      agent-4:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9538853168487549
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020375121384859085
        model: {}
        policy_loss: -0.004519820213317871
        total_loss: -0.004343059845268726
        vf_explained_var: 0.019480600953102112
        vf_loss: 18.555984497070312
      agent-5:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7417193055152893
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009999459143728018
        model: {}
        policy_loss: -0.0032246815972030163
        total_loss: -0.0027743575628846884
        vf_explained_var: 0.07577717304229736
        vf_loss: 17.557476043701172
    load_time_ms: 12999.769
    num_steps_sampled: 21600000
    num_steps_trained: 21600000
    sample_time_ms: 90427.457
    update_time_ms: 22.535
  iterations_since_restore: 165
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.37542857142857
    ram_util_percent: 10.152
  pid: 24061
  policy_reward_max:
    agent-0: 175.83333333333326
    agent-1: 175.83333333333326
    agent-2: 175.83333333333326
    agent-3: 175.83333333333326
    agent-4: 175.83333333333326
    agent-5: 175.83333333333326
  policy_reward_mean:
    agent-0: 146.35500000000005
    agent-1: 146.35500000000005
    agent-2: 146.35500000000005
    agent-3: 146.35500000000005
    agent-4: 146.35500000000005
    agent-5: 146.35500000000005
  policy_reward_min:
    agent-0: 52.33333333333325
    agent-1: 52.33333333333325
    agent-2: 52.33333333333325
    agent-3: 52.33333333333325
    agent-4: 52.33333333333325
    agent-5: 52.33333333333325
  sampler_perf:
    mean_env_wait_ms: 24.348702308018247
    mean_inference_ms: 12.283919643096022
    mean_processing_ms: 50.90919507895923
  time_since_restore: 21367.398688077927
  time_this_iter_s: 122.60948133468628
  time_total_s: 30493.41050195694
  timestamp: 1637045399
  timesteps_since_restore: 15840000
  timesteps_this_iter: 96000
  timesteps_total: 21600000
  training_iteration: 225
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    225 |          30493.4 | 21600000 |   878.13 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 67
    apples_agent-0_mean: 2.95
    apples_agent-0_min: 0
    apples_agent-1_max: 95
    apples_agent-1_mean: 33.6
    apples_agent-1_min: 0
    apples_agent-2_max: 67
    apples_agent-2_mean: 4.39
    apples_agent-2_min: 0
    apples_agent-3_max: 283
    apples_agent-3_mean: 117.04
    apples_agent-3_min: 63
    apples_agent-4_max: 20
    apples_agent-4_mean: 0.41
    apples_agent-4_min: 0
    apples_agent-5_max: 215
    apples_agent-5_mean: 92.78
    apples_agent-5_min: 31
    cleaning_beam_agent-0_max: 514
    cleaning_beam_agent-0_mean: 362.78
    cleaning_beam_agent-0_min: 132
    cleaning_beam_agent-1_max: 456
    cleaning_beam_agent-1_mean: 236.76
    cleaning_beam_agent-1_min: 76
    cleaning_beam_agent-2_max: 543
    cleaning_beam_agent-2_mean: 391.81
    cleaning_beam_agent-2_min: 147
    cleaning_beam_agent-3_max: 132
    cleaning_beam_agent-3_mean: 25.23
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 593
    cleaning_beam_agent-4_mean: 444.42
    cleaning_beam_agent-4_min: 329
    cleaning_beam_agent-5_max: 221
    cleaning_beam_agent-5_mean: 44.53
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-52-02
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1066.9999999999877
  episode_reward_mean: 879.0299999999846
  episode_reward_min: 577.9999999999995
  episodes_this_iter: 96
  episodes_total: 21696
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19651.877
    learner:
      agent-0:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1050240993499756
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012138027232140303
        model: {}
        policy_loss: -0.0035496545024216175
        total_loss: -0.0036476280074566603
        vf_explained_var: 0.01797124743461609
        vf_loss: 18.468706130981445
      agent-1:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0916081666946411
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015874227974563837
        model: {}
        policy_loss: -0.0044220807030797005
        total_loss: -0.004386265762150288
        vf_explained_var: -0.01626908779144287
        vf_loss: 19.570472717285156
      agent-2:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9950525164604187
        entropy_coeff: 0.0017600000137463212
        kl: 0.00167806725949049
        model: {}
        policy_loss: -0.003465122077614069
        total_loss: -0.0033705108799040318
        vf_explained_var: 0.026861488819122314
        vf_loss: 18.45902442932129
      agent-3:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6297014951705933
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011985638411715627
        model: {}
        policy_loss: -0.0024788514710962772
        total_loss: -0.001887432299554348
        vf_explained_var: 0.08992786705493927
        vf_loss: 16.996950149536133
      agent-4:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9515727758407593
        entropy_coeff: 0.0017600000137463212
        kl: 0.001515748561359942
        model: {}
        policy_loss: -0.004213232081383467
        total_loss: -0.0039243437349796295
        vf_explained_var: -0.03503677248954773
        vf_loss: 19.636600494384766
      agent-5:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7485213279724121
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009265657281503081
        model: {}
        policy_loss: -0.003431849181652069
        total_loss: -0.0029625706374645233
        vf_explained_var: 0.06900304555892944
        vf_loss: 17.866779327392578
    load_time_ms: 13007.41
    num_steps_sampled: 21696000
    num_steps_trained: 21696000
    sample_time_ms: 90204.428
    update_time_ms: 22.596
  iterations_since_restore: 166
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.414367816091955
    ram_util_percent: 10.15862068965517
  pid: 24061
  policy_reward_max:
    agent-0: 177.83333333333334
    agent-1: 177.83333333333334
    agent-2: 177.83333333333334
    agent-3: 177.83333333333334
    agent-4: 177.83333333333334
    agent-5: 177.83333333333334
  policy_reward_mean:
    agent-0: 146.50500000000002
    agent-1: 146.50500000000002
    agent-2: 146.50500000000002
    agent-3: 146.50500000000002
    agent-4: 146.50500000000002
    agent-5: 146.50500000000002
  policy_reward_min:
    agent-0: 96.33333333333375
    agent-1: 96.33333333333375
    agent-2: 96.33333333333375
    agent-3: 96.33333333333375
    agent-4: 96.33333333333375
    agent-5: 96.33333333333375
  sampler_perf:
    mean_env_wait_ms: 24.350358000329997
    mean_inference_ms: 12.282575752816692
    mean_processing_ms: 50.904826027395686
  time_since_restore: 21489.82398581505
  time_this_iter_s: 122.42529773712158
  time_total_s: 30615.83579969406
  timestamp: 1637045522
  timesteps_since_restore: 15936000
  timesteps_this_iter: 96000
  timesteps_total: 21696000
  training_iteration: 226
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    226 |          30615.8 | 21696000 |   879.03 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 1.36
    apples_agent-0_min: 0
    apples_agent-1_max: 124
    apples_agent-1_mean: 29.77
    apples_agent-1_min: 0
    apples_agent-2_max: 75
    apples_agent-2_mean: 4.06
    apples_agent-2_min: 0
    apples_agent-3_max: 168
    apples_agent-3_mean: 109.17
    apples_agent-3_min: 0
    apples_agent-4_max: 46
    apples_agent-4_mean: 2.0
    apples_agent-4_min: 0
    apples_agent-5_max: 199
    apples_agent-5_mean: 92.33
    apples_agent-5_min: 32
    cleaning_beam_agent-0_max: 526
    cleaning_beam_agent-0_mean: 373.32
    cleaning_beam_agent-0_min: 228
    cleaning_beam_agent-1_max: 470
    cleaning_beam_agent-1_mean: 254.64
    cleaning_beam_agent-1_min: 84
    cleaning_beam_agent-2_max: 569
    cleaning_beam_agent-2_mean: 418.72
    cleaning_beam_agent-2_min: 163
    cleaning_beam_agent-3_max: 69
    cleaning_beam_agent-3_mean: 24.37
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 555
    cleaning_beam_agent-4_mean: 444.11
    cleaning_beam_agent-4_min: 176
    cleaning_beam_agent-5_max: 123
    cleaning_beam_agent-5_mean: 43.04
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-54-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1112.0000000000002
  episode_reward_mean: 887.1499999999838
  episode_reward_min: 269.9999999999966
  episodes_this_iter: 96
  episodes_total: 21792
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19649.962
    learner:
      agent-0:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1124322414398193
        entropy_coeff: 0.0017600000137463212
        kl: 0.002021210268139839
        model: {}
        policy_loss: -0.003441480454057455
        total_loss: -0.003379830624908209
        vf_explained_var: 0.012626290321350098
        vf_loss: 20.195281982421875
      agent-1:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1005754470825195
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016776386182755232
        model: {}
        policy_loss: -0.004305799026042223
        total_loss: -0.004113387316465378
        vf_explained_var: -0.024595201015472412
        vf_loss: 21.294269561767578
      agent-2:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9784862399101257
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014286162331700325
        model: {}
        policy_loss: -0.0036775318440049887
        total_loss: -0.003446808084845543
        vf_explained_var: 0.05193409323692322
        vf_loss: 19.52859115600586
      agent-3:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6207008361816406
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008974066004157066
        model: {}
        policy_loss: -0.0026798248291015625
        total_loss: -0.002064261119812727
        vf_explained_var: 0.1717001050710678
        vf_loss: 17.079959869384766
      agent-4:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9494737982749939
        entropy_coeff: 0.0017600000137463212
        kl: 0.001842551864683628
        model: {}
        policy_loss: -0.004306751769036055
        total_loss: -0.004033212549984455
        vf_explained_var: 0.05810484290122986
        vf_loss: 19.446134567260742
      agent-5:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7254105806350708
        entropy_coeff: 0.0017600000137463212
        kl: 0.001039800699800253
        model: {}
        policy_loss: -0.003520847298204899
        total_loss: -0.0029242578893899918
        vf_explained_var: 0.10904598236083984
        vf_loss: 18.733116149902344
    load_time_ms: 13002.095
    num_steps_sampled: 21792000
    num_steps_trained: 21792000
    sample_time_ms: 90158.075
    update_time_ms: 22.732
  iterations_since_restore: 167
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.546242774566474
    ram_util_percent: 10.132369942196533
  pid: 24061
  policy_reward_max:
    agent-0: 185.3333333333332
    agent-1: 185.3333333333332
    agent-2: 185.3333333333332
    agent-3: 185.3333333333332
    agent-4: 185.3333333333332
    agent-5: 185.3333333333332
  policy_reward_mean:
    agent-0: 147.8583333333334
    agent-1: 147.8583333333334
    agent-2: 147.8583333333334
    agent-3: 147.8583333333334
    agent-4: 147.8583333333334
    agent-5: 147.8583333333334
  policy_reward_min:
    agent-0: 44.99999999999988
    agent-1: 44.99999999999988
    agent-2: 44.99999999999988
    agent-3: 44.99999999999988
    agent-4: 44.99999999999988
    agent-5: 44.99999999999988
  sampler_perf:
    mean_env_wait_ms: 24.353218274015557
    mean_inference_ms: 12.28142659727006
    mean_processing_ms: 50.90014121632793
  time_since_restore: 21611.880428791046
  time_this_iter_s: 122.05644297599792
  time_total_s: 30737.89224267006
  timestamp: 1637045644
  timesteps_since_restore: 16032000
  timesteps_this_iter: 96000
  timesteps_total: 21792000
  training_iteration: 227
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    227 |          30737.9 | 21792000 |   887.15 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 85
    apples_agent-0_mean: 4.6
    apples_agent-0_min: 0
    apples_agent-1_max: 103
    apples_agent-1_mean: 34.89
    apples_agent-1_min: 0
    apples_agent-2_max: 90
    apples_agent-2_mean: 3.38
    apples_agent-2_min: 0
    apples_agent-3_max: 170
    apples_agent-3_mean: 113.01
    apples_agent-3_min: 0
    apples_agent-4_max: 34
    apples_agent-4_mean: 1.53
    apples_agent-4_min: 0
    apples_agent-5_max: 165
    apples_agent-5_mean: 91.46
    apples_agent-5_min: 36
    cleaning_beam_agent-0_max: 520
    cleaning_beam_agent-0_mean: 345.51
    cleaning_beam_agent-0_min: 168
    cleaning_beam_agent-1_max: 470
    cleaning_beam_agent-1_mean: 263.01
    cleaning_beam_agent-1_min: 81
    cleaning_beam_agent-2_max: 552
    cleaning_beam_agent-2_mean: 399.88
    cleaning_beam_agent-2_min: 190
    cleaning_beam_agent-3_max: 109
    cleaning_beam_agent-3_mean: 29.56
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 541
    cleaning_beam_agent-4_mean: 438.4
    cleaning_beam_agent-4_min: 235
    cleaning_beam_agent-5_max: 144
    cleaning_beam_agent-5_mean: 37.8
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-56-07
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1105.9999999999793
  episode_reward_mean: 886.8499999999851
  episode_reward_min: 303.0000000000008
  episodes_this_iter: 96
  episodes_total: 21888
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19646.597
    learner:
      agent-0:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1211364269256592
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016719711711630225
        model: {}
        policy_loss: -0.003228316782042384
        total_loss: -0.003167737042531371
        vf_explained_var: 0.025836944580078125
        vf_loss: 20.33784294128418
      agent-1:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0966436862945557
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011727813398465514
        model: {}
        policy_loss: -0.0035150572657585144
        total_loss: -0.0032644846942275763
        vf_explained_var: -0.033292919397354126
        vf_loss: 21.806673049926758
      agent-2:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9864323139190674
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014957658713683486
        model: {}
        policy_loss: -0.0036362153477966785
        total_loss: -0.003385776188224554
        vf_explained_var: 0.040154263377189636
        vf_loss: 19.8656005859375
      agent-3:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6376411318778992
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019151708111166954
        model: {}
        policy_loss: -0.0031111850403249264
        total_loss: -0.002427509520202875
        vf_explained_var: 0.13668617606163025
        vf_loss: 18.05923843383789
      agent-4:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9513521194458008
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017374056624248624
        model: {}
        policy_loss: -0.004106471315026283
        total_loss: -0.0038357158191502094
        vf_explained_var: 0.07356376945972443
        vf_loss: 19.451364517211914
      agent-5:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7052429914474487
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009583914070390165
        model: {}
        policy_loss: -0.003490365343168378
        total_loss: -0.0028679666575044394
        vf_explained_var: 0.11785770952701569
        vf_loss: 18.636255264282227
    load_time_ms: 12999.395
    num_steps_sampled: 21888000
    num_steps_trained: 21888000
    sample_time_ms: 90208.918
    update_time_ms: 22.63
  iterations_since_restore: 168
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.294350282485878
    ram_util_percent: 10.16101694915254
  pid: 24061
  policy_reward_max:
    agent-0: 184.33333333333303
    agent-1: 184.33333333333303
    agent-2: 184.33333333333303
    agent-3: 184.33333333333303
    agent-4: 184.33333333333303
    agent-5: 184.33333333333303
  policy_reward_mean:
    agent-0: 147.80833333333337
    agent-1: 147.80833333333337
    agent-2: 147.80833333333337
    agent-3: 147.80833333333337
    agent-4: 147.80833333333337
    agent-5: 147.80833333333337
  policy_reward_min:
    agent-0: 50.499999999999886
    agent-1: 50.499999999999886
    agent-2: 50.499999999999886
    agent-3: 50.499999999999886
    agent-4: 50.499999999999886
    agent-5: 50.499999999999886
  sampler_perf:
    mean_env_wait_ms: 24.35563323965172
    mean_inference_ms: 12.28050289663999
    mean_processing_ms: 50.89563585910927
  time_since_restore: 21735.318496465683
  time_this_iter_s: 123.43806767463684
  time_total_s: 30861.330310344696
  timestamp: 1637045767
  timesteps_since_restore: 16128000
  timesteps_this_iter: 96000
  timesteps_total: 21888000
  training_iteration: 228
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    228 |          30861.3 | 21888000 |   886.85 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 45
    apples_agent-0_mean: 2.28
    apples_agent-0_min: 0
    apples_agent-1_max: 146
    apples_agent-1_mean: 33.68
    apples_agent-1_min: 0
    apples_agent-2_max: 74
    apples_agent-2_mean: 2.85
    apples_agent-2_min: 0
    apples_agent-3_max: 189
    apples_agent-3_mean: 112.46
    apples_agent-3_min: 28
    apples_agent-4_max: 46
    apples_agent-4_mean: 1.46
    apples_agent-4_min: 0
    apples_agent-5_max: 142
    apples_agent-5_mean: 87.58
    apples_agent-5_min: 25
    cleaning_beam_agent-0_max: 567
    cleaning_beam_agent-0_mean: 371.92
    cleaning_beam_agent-0_min: 220
    cleaning_beam_agent-1_max: 514
    cleaning_beam_agent-1_mean: 271.16
    cleaning_beam_agent-1_min: 106
    cleaning_beam_agent-2_max: 578
    cleaning_beam_agent-2_mean: 424.37
    cleaning_beam_agent-2_min: 198
    cleaning_beam_agent-3_max: 76
    cleaning_beam_agent-3_mean: 23.99
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 533
    cleaning_beam_agent-4_mean: 434.63
    cleaning_beam_agent-4_min: 212
    cleaning_beam_agent-5_max: 172
    cleaning_beam_agent-5_mean: 34.64
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-58-10
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1087.9999999999864
  episode_reward_mean: 911.059999999983
  episode_reward_min: 345.0000000000016
  episodes_this_iter: 96
  episodes_total: 21984
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19663.393
    learner:
      agent-0:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 1.10127592086792
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011455381754785776
        model: {}
        policy_loss: -0.003005137201398611
        total_loss: -0.00299711711704731
        vf_explained_var: 0.013141661882400513
        vf_loss: 19.462665557861328
      agent-1:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0889077186584473
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013921591453254223
        model: {}
        policy_loss: -0.004102800507098436
        total_loss: -0.004008438903838396
        vf_explained_var: 0.009594962000846863
        vf_loss: 20.108373641967773
      agent-2:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9663383960723877
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012633624719455838
        model: {}
        policy_loss: -0.003258874174207449
        total_loss: -0.0029507610015571117
        vf_explained_var: -0.029116034507751465
        vf_loss: 20.088687896728516
      agent-3:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6099354028701782
        entropy_coeff: 0.0017600000137463212
        kl: 0.001416349201463163
        model: {}
        policy_loss: -0.0024994355626404285
        total_loss: -0.0018106899224221706
        vf_explained_var: 0.10787463188171387
        vf_loss: 17.62233543395996
      agent-4:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9551830887794495
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018617082387208939
        model: {}
        policy_loss: -0.004283953458070755
        total_loss: -0.004022898618131876
        vf_explained_var: 0.04612313210964203
        vf_loss: 19.421730041503906
      agent-5:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7062693238258362
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011561003047972918
        model: {}
        policy_loss: -0.0033867424353957176
        total_loss: -0.002779998816549778
        vf_explained_var: 0.09149229526519775
        vf_loss: 18.497787475585938
    load_time_ms: 12981.173
    num_steps_sampled: 21984000
    num_steps_trained: 21984000
    sample_time_ms: 90171.742
    update_time_ms: 23.049
  iterations_since_restore: 169
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.416666666666666
    ram_util_percent: 10.14655172413793
  pid: 24061
  policy_reward_max:
    agent-0: 181.33333333333317
    agent-1: 181.33333333333317
    agent-2: 181.33333333333317
    agent-3: 181.33333333333317
    agent-4: 181.33333333333317
    agent-5: 181.33333333333317
  policy_reward_mean:
    agent-0: 151.84333333333333
    agent-1: 151.84333333333333
    agent-2: 151.84333333333333
    agent-3: 151.84333333333333
    agent-4: 151.84333333333333
    agent-5: 151.84333333333333
  policy_reward_min:
    agent-0: 57.49999999999977
    agent-1: 57.49999999999977
    agent-2: 57.49999999999977
    agent-3: 57.49999999999977
    agent-4: 57.49999999999977
    agent-5: 57.49999999999977
  sampler_perf:
    mean_env_wait_ms: 24.357523813655288
    mean_inference_ms: 12.279302051368914
    mean_processing_ms: 50.889290952412885
  time_since_restore: 21857.608744859695
  time_this_iter_s: 122.29024839401245
  time_total_s: 30983.62055873871
  timestamp: 1637045890
  timesteps_since_restore: 16224000
  timesteps_this_iter: 96000
  timesteps_total: 21984000
  training_iteration: 229
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    229 |          30983.6 | 21984000 |   911.06 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 46
    apples_agent-0_mean: 3.2
    apples_agent-0_min: 0
    apples_agent-1_max: 93
    apples_agent-1_mean: 33.58
    apples_agent-1_min: 0
    apples_agent-2_max: 129
    apples_agent-2_mean: 5.45
    apples_agent-2_min: 0
    apples_agent-3_max: 166
    apples_agent-3_mean: 109.93
    apples_agent-3_min: 55
    apples_agent-4_max: 27
    apples_agent-4_mean: 0.71
    apples_agent-4_min: 0
    apples_agent-5_max: 146
    apples_agent-5_mean: 89.13
    apples_agent-5_min: 23
    cleaning_beam_agent-0_max: 512
    cleaning_beam_agent-0_mean: 374.84
    cleaning_beam_agent-0_min: 213
    cleaning_beam_agent-1_max: 449
    cleaning_beam_agent-1_mean: 279.57
    cleaning_beam_agent-1_min: 103
    cleaning_beam_agent-2_max: 584
    cleaning_beam_agent-2_mean: 399.95
    cleaning_beam_agent-2_min: 143
    cleaning_beam_agent-3_max: 88
    cleaning_beam_agent-3_mean: 28.95
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 562
    cleaning_beam_agent-4_mean: 435.09
    cleaning_beam_agent-4_min: 249
    cleaning_beam_agent-5_max: 167
    cleaning_beam_agent-5_mean: 37.32
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-00-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1097.9999999999895
  episode_reward_mean: 893.9999999999864
  episode_reward_min: 498.0000000000098
  episodes_this_iter: 96
  episodes_total: 22080
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19673.591
    learner:
      agent-0:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1037116050720215
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013797588180750608
        model: {}
        policy_loss: -0.00341405370272696
        total_loss: -0.003433051286265254
        vf_explained_var: 0.01791250705718994
        vf_loss: 19.235380172729492
      agent-1:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0941143035888672
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014118915423750877
        model: {}
        policy_loss: -0.004201679490506649
        total_loss: -0.004147140309214592
        vf_explained_var: 0.0026774853467941284
        vf_loss: 19.801841735839844
      agent-2:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.987517774105072
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013424510834738612
        model: {}
        policy_loss: -0.0031499555334448814
        total_loss: -0.0029670484364032745
        vf_explained_var: 0.03010593354701996
        vf_loss: 19.20938491821289
      agent-3:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6095544695854187
        entropy_coeff: 0.0017600000137463212
        kl: 0.000866726681124419
        model: {}
        policy_loss: -0.002349338261410594
        total_loss: -0.0017461783718317747
        vf_explained_var: 0.13860194385051727
        vf_loss: 16.759740829467773
      agent-4:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9419541954994202
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016757146222516894
        model: {}
        policy_loss: -0.004050368443131447
        total_loss: -0.003803590778261423
        vf_explained_var: 0.03924298286437988
        vf_loss: 19.046157836914062
      agent-5:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7128447890281677
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008139640558511019
        model: {}
        policy_loss: -0.0033209212124347687
        total_loss: -0.002798851579427719
        vf_explained_var: 0.11241476237773895
        vf_loss: 17.766742706298828
    load_time_ms: 12978.863
    num_steps_sampled: 22080000
    num_steps_trained: 22080000
    sample_time_ms: 90019.472
    update_time_ms: 23.722
  iterations_since_restore: 170
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.46551724137931
    ram_util_percent: 10.163218390804596
  pid: 24061
  policy_reward_max:
    agent-0: 182.99999999999957
    agent-1: 182.99999999999957
    agent-2: 182.99999999999957
    agent-3: 182.99999999999957
    agent-4: 182.99999999999957
    agent-5: 182.99999999999957
  policy_reward_mean:
    agent-0: 149.00000000000003
    agent-1: 149.00000000000003
    agent-2: 149.00000000000003
    agent-3: 149.00000000000003
    agent-4: 149.00000000000003
    agent-5: 149.00000000000003
  policy_reward_min:
    agent-0: 83.00000000000018
    agent-1: 83.00000000000018
    agent-2: 83.00000000000018
    agent-3: 83.00000000000018
    agent-4: 83.00000000000018
    agent-5: 83.00000000000018
  sampler_perf:
    mean_env_wait_ms: 24.35983913020112
    mean_inference_ms: 12.278009479124355
    mean_processing_ms: 50.88448993437841
  time_since_restore: 21979.595539569855
  time_this_iter_s: 121.9867947101593
  time_total_s: 31105.607353448868
  timestamp: 1637046012
  timesteps_since_restore: 16320000
  timesteps_this_iter: 96000
  timesteps_total: 22080000
  training_iteration: 230
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    230 |          31105.6 | 22080000 |      894 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 45
    apples_agent-0_mean: 3.14
    apples_agent-0_min: 0
    apples_agent-1_max: 114
    apples_agent-1_mean: 31.68
    apples_agent-1_min: 0
    apples_agent-2_max: 130
    apples_agent-2_mean: 5.48
    apples_agent-2_min: 0
    apples_agent-3_max: 191
    apples_agent-3_mean: 113.05
    apples_agent-3_min: 65
    apples_agent-4_max: 71
    apples_agent-4_mean: 1.42
    apples_agent-4_min: 0
    apples_agent-5_max: 140
    apples_agent-5_mean: 88.63
    apples_agent-5_min: 27
    cleaning_beam_agent-0_max: 523
    cleaning_beam_agent-0_mean: 368.13
    cleaning_beam_agent-0_min: 185
    cleaning_beam_agent-1_max: 483
    cleaning_beam_agent-1_mean: 273.03
    cleaning_beam_agent-1_min: 105
    cleaning_beam_agent-2_max: 555
    cleaning_beam_agent-2_mean: 406.06
    cleaning_beam_agent-2_min: 178
    cleaning_beam_agent-3_max: 74
    cleaning_beam_agent-3_mean: 24.93
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 556
    cleaning_beam_agent-4_mean: 438.91
    cleaning_beam_agent-4_min: 272
    cleaning_beam_agent-5_max: 240
    cleaning_beam_agent-5_mean: 40.5
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-02-15
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1085.9999999999732
  episode_reward_mean: 894.6199999999837
  episode_reward_min: 645.9999999999851
  episodes_this_iter: 96
  episodes_total: 22176
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19653.081
    learner:
      agent-0:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1010198593139648
        entropy_coeff: 0.0017600000137463212
        kl: 0.001338444883003831
        model: {}
        policy_loss: -0.0034642184618860483
        total_loss: -0.0034897017758339643
        vf_explained_var: 0.029445916414260864
        vf_loss: 19.123119354248047
      agent-1:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 1.088061809539795
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017018219223245978
        model: {}
        policy_loss: -0.004024982452392578
        total_loss: -0.003957442473620176
        vf_explained_var: 0.006985917687416077
        vf_loss: 19.825279235839844
      agent-2:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9846688508987427
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018357207300141454
        model: {}
        policy_loss: -0.0036986288614571095
        total_loss: -0.003505270928144455
        vf_explained_var: 0.025828003883361816
        vf_loss: 19.263750076293945
      agent-3:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6029057502746582
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018725170521065593
        model: {}
        policy_loss: -0.002553266007453203
        total_loss: -0.0019045732915401459
        vf_explained_var: 0.12371689081192017
        vf_loss: 17.09809684753418
      agent-4:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9368101358413696
        entropy_coeff: 0.0017600000137463212
        kl: 0.002358632627874613
        model: {}
        policy_loss: -0.004498766735196114
        total_loss: -0.004203836899250746
        vf_explained_var: 0.014416590332984924
        vf_loss: 19.43717384338379
      agent-5:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7216672301292419
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008775647729635239
        model: {}
        policy_loss: -0.0035268194042146206
        total_loss: -0.0029791686683893204
        vf_explained_var: 0.09821245074272156
        vf_loss: 18.177871704101562
    load_time_ms: 12969.051
    num_steps_sampled: 22176000
    num_steps_trained: 22176000
    sample_time_ms: 90050.74
    update_time_ms: 22.744
  iterations_since_restore: 171
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.945142857142857
    ram_util_percent: 10.22285714285714
  pid: 24061
  policy_reward_max:
    agent-0: 180.99999999999991
    agent-1: 180.99999999999991
    agent-2: 180.99999999999991
    agent-3: 180.99999999999991
    agent-4: 180.99999999999991
    agent-5: 180.99999999999991
  policy_reward_mean:
    agent-0: 149.10333333333338
    agent-1: 149.10333333333338
    agent-2: 149.10333333333338
    agent-3: 149.10333333333338
    agent-4: 149.10333333333338
    agent-5: 149.10333333333338
  policy_reward_min:
    agent-0: 107.66666666666696
    agent-1: 107.66666666666696
    agent-2: 107.66666666666696
    agent-3: 107.66666666666696
    agent-4: 107.66666666666696
    agent-5: 107.66666666666696
  sampler_perf:
    mean_env_wait_ms: 24.36272935770412
    mean_inference_ms: 12.277202110650489
    mean_processing_ms: 50.880850277064944
  time_since_restore: 22102.19362449646
  time_this_iter_s: 122.59808492660522
  time_total_s: 31228.205438375473
  timestamp: 1637046135
  timesteps_since_restore: 16416000
  timesteps_this_iter: 96000
  timesteps_total: 22176000
  training_iteration: 231
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    231 |          31228.2 | 22176000 |   894.62 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 344
    apples_agent-0_mean: 5.24
    apples_agent-0_min: 0
    apples_agent-1_max: 134
    apples_agent-1_mean: 31.89
    apples_agent-1_min: 0
    apples_agent-2_max: 127
    apples_agent-2_mean: 6.94
    apples_agent-2_min: 0
    apples_agent-3_max: 174
    apples_agent-3_mean: 104.49
    apples_agent-3_min: 32
    apples_agent-4_max: 49
    apples_agent-4_mean: 1.48
    apples_agent-4_min: 0
    apples_agent-5_max: 185
    apples_agent-5_mean: 88.47
    apples_agent-5_min: 43
    cleaning_beam_agent-0_max: 504
    cleaning_beam_agent-0_mean: 380.22
    cleaning_beam_agent-0_min: 150
    cleaning_beam_agent-1_max: 548
    cleaning_beam_agent-1_mean: 288.68
    cleaning_beam_agent-1_min: 136
    cleaning_beam_agent-2_max: 554
    cleaning_beam_agent-2_mean: 398.47
    cleaning_beam_agent-2_min: 161
    cleaning_beam_agent-3_max: 59
    cleaning_beam_agent-3_mean: 24.61
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 553
    cleaning_beam_agent-4_mean: 449.59
    cleaning_beam_agent-4_min: 324
    cleaning_beam_agent-5_max: 145
    cleaning_beam_agent-5_mean: 42.04
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-04-17
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1073.999999999981
  episode_reward_mean: 877.9699999999851
  episode_reward_min: 525.0000000000075
  episodes_this_iter: 96
  episodes_total: 22272
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19644.942
    learner:
      agent-0:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1078091859817505
        entropy_coeff: 0.0017600000137463212
        kl: 0.001629119971767068
        model: {}
        policy_loss: -0.0031164600513875484
        total_loss: -0.0031840060837566853
        vf_explained_var: 0.004213765263557434
        vf_loss: 18.821964263916016
      agent-1:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0951893329620361
        entropy_coeff: 0.0017600000137463212
        kl: 0.001280690892599523
        model: {}
        policy_loss: -0.003936592023819685
        total_loss: -0.003919596318155527
        vf_explained_var: -0.021678239107131958
        vf_loss: 19.445281982421875
      agent-2:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9959418177604675
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012854525120928884
        model: {}
        policy_loss: -0.003553987480700016
        total_loss: -0.0035272203385829926
        vf_explained_var: 0.06714949011802673
        vf_loss: 17.79619598388672
      agent-3:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6083976030349731
        entropy_coeff: 0.0017600000137463212
        kl: 0.002062652725726366
        model: {}
        policy_loss: -0.0027107298374176025
        total_loss: -0.0021314239129424095
        vf_explained_var: 0.1273365616798401
        vf_loss: 16.500831604003906
      agent-4:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.935602605342865
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014747532550245523
        model: {}
        policy_loss: -0.004000181797891855
        total_loss: -0.003794270334765315
        vf_explained_var: 0.023245960474014282
        vf_loss: 18.525741577148438
      agent-5:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7390364408493042
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009407886536791921
        model: {}
        policy_loss: -0.0035707512870430946
        total_loss: -0.0031435946002602577
        vf_explained_var: 0.10096348822116852
        vf_loss: 17.27859878540039
    load_time_ms: 12979.867
    num_steps_sampled: 22272000
    num_steps_trained: 22272000
    sample_time_ms: 89880.648
    update_time_ms: 23.086
  iterations_since_restore: 172
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.44
    ram_util_percent: 10.157142857142855
  pid: 24061
  policy_reward_max:
    agent-0: 178.9999999999998
    agent-1: 178.9999999999998
    agent-2: 178.9999999999998
    agent-3: 178.9999999999998
    agent-4: 178.9999999999998
    agent-5: 178.9999999999998
  policy_reward_mean:
    agent-0: 146.3283333333334
    agent-1: 146.3283333333334
    agent-2: 146.3283333333334
    agent-3: 146.3283333333334
    agent-4: 146.3283333333334
    agent-5: 146.3283333333334
  policy_reward_min:
    agent-0: 87.50000000000004
    agent-1: 87.50000000000004
    agent-2: 87.50000000000004
    agent-3: 87.50000000000004
    agent-4: 87.50000000000004
    agent-5: 87.50000000000004
  sampler_perf:
    mean_env_wait_ms: 24.366043098023354
    mean_inference_ms: 12.27648571429833
    mean_processing_ms: 50.876119382625205
  time_since_restore: 22224.82346701622
  time_this_iter_s: 122.62984251976013
  time_total_s: 31350.835280895233
  timestamp: 1637046257
  timesteps_since_restore: 16512000
  timesteps_this_iter: 96000
  timesteps_total: 22272000
  training_iteration: 232
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    232 |          31350.8 | 22272000 |   877.97 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 58
    apples_agent-0_mean: 4.29
    apples_agent-0_min: 0
    apples_agent-1_max: 118
    apples_agent-1_mean: 32.24
    apples_agent-1_min: 0
    apples_agent-2_max: 127
    apples_agent-2_mean: 4.62
    apples_agent-2_min: 0
    apples_agent-3_max: 174
    apples_agent-3_mean: 108.49
    apples_agent-3_min: 32
    apples_agent-4_max: 34
    apples_agent-4_mean: 0.8
    apples_agent-4_min: 0
    apples_agent-5_max: 161
    apples_agent-5_mean: 89.1
    apples_agent-5_min: 41
    cleaning_beam_agent-0_max: 521
    cleaning_beam_agent-0_mean: 378.97
    cleaning_beam_agent-0_min: 200
    cleaning_beam_agent-1_max: 481
    cleaning_beam_agent-1_mean: 271.82
    cleaning_beam_agent-1_min: 123
    cleaning_beam_agent-2_max: 544
    cleaning_beam_agent-2_mean: 412.66
    cleaning_beam_agent-2_min: 217
    cleaning_beam_agent-3_max: 120
    cleaning_beam_agent-3_mean: 28.06
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 610
    cleaning_beam_agent-4_mean: 458.93
    cleaning_beam_agent-4_min: 329
    cleaning_beam_agent-5_max: 129
    cleaning_beam_agent-5_mean: 34.47
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-06-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1088.9999999999848
  episode_reward_mean: 897.7299999999851
  episode_reward_min: 615.0000000000039
  episodes_this_iter: 96
  episodes_total: 22368
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19623.853
    learner:
      agent-0:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1021097898483276
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021007638424634933
        model: {}
        policy_loss: -0.0034678070805966854
        total_loss: -0.003462207969278097
        vf_explained_var: -0.0001815110445022583
        vf_loss: 19.453113555908203
      agent-1:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1020311117172241
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016210625180974603
        model: {}
        policy_loss: -0.004404810722917318
        total_loss: -0.0042518130503594875
        vf_explained_var: -0.038727909326553345
        vf_loss: 20.925708770751953
      agent-2:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.982912003993988
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014668648364022374
        model: {}
        policy_loss: -0.003546963445842266
        total_loss: -0.003436291590332985
        vf_explained_var: 0.062272101640701294
        vf_loss: 18.405988693237305
      agent-3:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6071315407752991
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011433875188231468
        model: {}
        policy_loss: -0.0026262535247951746
        total_loss: -0.001971731660887599
        vf_explained_var: 0.10924611985683441
        vf_loss: 17.230737686157227
      agent-4:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9319559931755066
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014982159482315183
        model: {}
        policy_loss: -0.004011109005659819
        total_loss: -0.003723205765709281
        vf_explained_var: 0.018100008368492126
        vf_loss: 19.281448364257812
      agent-5:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7133103013038635
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011045398423448205
        model: {}
        policy_loss: -0.003066089004278183
        total_loss: -0.002482963725924492
        vf_explained_var: 0.07574179768562317
        vf_loss: 18.3855037689209
    load_time_ms: 12925.414
    num_steps_sampled: 22368000
    num_steps_trained: 22368000
    sample_time_ms: 89834.28
    update_time_ms: 22.486
  iterations_since_restore: 173
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.414367816091955
    ram_util_percent: 10.148850574712645
  pid: 24061
  policy_reward_max:
    agent-0: 181.49999999999994
    agent-1: 181.49999999999994
    agent-2: 181.49999999999994
    agent-3: 181.49999999999994
    agent-4: 181.49999999999994
    agent-5: 181.49999999999994
  policy_reward_mean:
    agent-0: 149.62166666666673
    agent-1: 149.62166666666673
    agent-2: 149.62166666666673
    agent-3: 149.62166666666673
    agent-4: 149.62166666666673
    agent-5: 149.62166666666673
  policy_reward_min:
    agent-0: 102.50000000000009
    agent-1: 102.50000000000009
    agent-2: 102.50000000000009
    agent-3: 102.50000000000009
    agent-4: 102.50000000000009
    agent-5: 102.50000000000009
  sampler_perf:
    mean_env_wait_ms: 24.368295246573766
    mean_inference_ms: 12.275327749095874
    mean_processing_ms: 50.870564334905254
  time_since_restore: 22346.89005279541
  time_this_iter_s: 122.06658577919006
  time_total_s: 31472.901866674423
  timestamp: 1637046380
  timesteps_since_restore: 16608000
  timesteps_this_iter: 96000
  timesteps_total: 22368000
  training_iteration: 233
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    233 |          31472.9 | 22368000 |   897.73 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 58
    apples_agent-0_mean: 4.4
    apples_agent-0_min: 0
    apples_agent-1_max: 124
    apples_agent-1_mean: 31.2
    apples_agent-1_min: 0
    apples_agent-2_max: 63
    apples_agent-2_mean: 3.94
    apples_agent-2_min: 0
    apples_agent-3_max: 194
    apples_agent-3_mean: 108.19
    apples_agent-3_min: 30
    apples_agent-4_max: 48
    apples_agent-4_mean: 0.68
    apples_agent-4_min: 0
    apples_agent-5_max: 191
    apples_agent-5_mean: 93.53
    apples_agent-5_min: 16
    cleaning_beam_agent-0_max: 519
    cleaning_beam_agent-0_mean: 377.46
    cleaning_beam_agent-0_min: 209
    cleaning_beam_agent-1_max: 566
    cleaning_beam_agent-1_mean: 256.6
    cleaning_beam_agent-1_min: 103
    cleaning_beam_agent-2_max: 538
    cleaning_beam_agent-2_mean: 430.79
    cleaning_beam_agent-2_min: 237
    cleaning_beam_agent-3_max: 96
    cleaning_beam_agent-3_mean: 23.38
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 558
    cleaning_beam_agent-4_mean: 447.57
    cleaning_beam_agent-4_min: 332
    cleaning_beam_agent-5_max: 85
    cleaning_beam_agent-5_mean: 32.0
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-08-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1095.9999999999798
  episode_reward_mean: 916.2699999999829
  episode_reward_min: 199.0000000000002
  episodes_this_iter: 96
  episodes_total: 22464
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19638.562
    learner:
      agent-0:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1043468713760376
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014237554278224707
        model: {}
        policy_loss: -0.0033541833981871605
        total_loss: -0.003197908867150545
        vf_explained_var: -0.002612754702568054
        vf_loss: 20.9992618560791
      agent-1:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 1.106534481048584
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018414270598441362
        model: {}
        policy_loss: -0.004378683399409056
        total_loss: -0.004099107813090086
        vf_explained_var: -0.016036003828048706
        vf_loss: 22.270763397216797
      agent-2:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9842678904533386
        entropy_coeff: 0.0017600000137463212
        kl: 0.00127469003200531
        model: {}
        policy_loss: -0.0035123517736792564
        total_loss: -0.0032187788747251034
        vf_explained_var: 0.03885483741760254
        vf_loss: 20.25882911682129
      agent-3:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5903816223144531
        entropy_coeff: 0.0017600000137463212
        kl: 0.001137888291850686
        model: {}
        policy_loss: -0.002658688463270664
        total_loss: -0.0018649306148290634
        vf_explained_var: 0.11866836249828339
        vf_loss: 18.328311920166016
      agent-4:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9428484439849854
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018804119899868965
        model: {}
        policy_loss: -0.004118242301046848
        total_loss: -0.003721970599144697
        vf_explained_var: 0.034941449761390686
        vf_loss: 20.556865692138672
      agent-5:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6878504157066345
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018203038489446044
        model: {}
        policy_loss: -0.0032895347103476524
        total_loss: -0.0026366407983005047
        vf_explained_var: 0.12230490148067474
        vf_loss: 18.6351375579834
    load_time_ms: 12917.8
    num_steps_sampled: 22464000
    num_steps_trained: 22464000
    sample_time_ms: 89908.218
    update_time_ms: 22.786
  iterations_since_restore: 174
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.396045197740115
    ram_util_percent: 10.166666666666664
  pid: 24061
  policy_reward_max:
    agent-0: 182.66666666666666
    agent-1: 182.66666666666666
    agent-2: 182.66666666666666
    agent-3: 182.66666666666666
    agent-4: 182.66666666666666
    agent-5: 182.66666666666666
  policy_reward_mean:
    agent-0: 152.71166666666667
    agent-1: 152.71166666666667
    agent-2: 152.71166666666667
    agent-3: 152.71166666666667
    agent-4: 152.71166666666667
    agent-5: 152.71166666666667
  policy_reward_min:
    agent-0: 33.166666666666664
    agent-1: 33.166666666666664
    agent-2: 33.166666666666664
    agent-3: 33.166666666666664
    agent-4: 33.166666666666664
    agent-5: 33.166666666666664
  sampler_perf:
    mean_env_wait_ms: 24.371498144668195
    mean_inference_ms: 12.274513358548443
    mean_processing_ms: 50.86857488453477
  time_since_restore: 22470.564997673035
  time_this_iter_s: 123.67494487762451
  time_total_s: 31596.576811552048
  timestamp: 1637046504
  timesteps_since_restore: 16704000
  timesteps_this_iter: 96000
  timesteps_total: 22464000
  training_iteration: 234
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    234 |          31596.6 | 22464000 |   916.27 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 48
    apples_agent-0_mean: 3.9
    apples_agent-0_min: 0
    apples_agent-1_max: 144
    apples_agent-1_mean: 35.21
    apples_agent-1_min: 0
    apples_agent-2_max: 113
    apples_agent-2_mean: 4.71
    apples_agent-2_min: 0
    apples_agent-3_max: 176
    apples_agent-3_mean: 104.96
    apples_agent-3_min: 0
    apples_agent-4_max: 24
    apples_agent-4_mean: 0.66
    apples_agent-4_min: 0
    apples_agent-5_max: 140
    apples_agent-5_mean: 88.61
    apples_agent-5_min: 25
    cleaning_beam_agent-0_max: 508
    cleaning_beam_agent-0_mean: 378.39
    cleaning_beam_agent-0_min: 146
    cleaning_beam_agent-1_max: 526
    cleaning_beam_agent-1_mean: 241.06
    cleaning_beam_agent-1_min: 103
    cleaning_beam_agent-2_max: 555
    cleaning_beam_agent-2_mean: 419.16
    cleaning_beam_agent-2_min: 199
    cleaning_beam_agent-3_max: 79
    cleaning_beam_agent-3_mean: 22.36
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 563
    cleaning_beam_agent-4_mean: 437.65
    cleaning_beam_agent-4_min: 327
    cleaning_beam_agent-5_max: 134
    cleaning_beam_agent-5_mean: 35.87
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-10-27
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1145.999999999994
  episode_reward_mean: 897.6699999999837
  episode_reward_min: 337.0000000000056
  episodes_this_iter: 96
  episodes_total: 22560
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19645.682
    learner:
      agent-0:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 1.109349250793457
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016162339597940445
        model: {}
        policy_loss: -0.0034350166097283363
        total_loss: -0.0033014563377946615
        vf_explained_var: -0.015598058700561523
        vf_loss: 20.860166549682617
      agent-1:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0969316959381104
        entropy_coeff: 0.0017600000137463212
        kl: 0.001419960055500269
        model: {}
        policy_loss: -0.004099583253264427
        total_loss: -0.003906948491930962
        vf_explained_var: -0.002179965376853943
        vf_loss: 21.23232650756836
      agent-2:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.993168830871582
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012668618001043797
        model: {}
        policy_loss: -0.0036681354977190495
        total_loss: -0.0033564488403499126
        vf_explained_var: 0.010816141963005066
        vf_loss: 20.596622467041016
      agent-3:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5709558129310608
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009956291178241372
        model: {}
        policy_loss: -0.002176913432776928
        total_loss: -0.0013943915255367756
        vf_explained_var: 0.1308142989873886
        vf_loss: 17.874052047729492
      agent-4:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9390595555305481
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015295351622626185
        model: {}
        policy_loss: -0.003999617416411638
        total_loss: -0.0036842962726950645
        vf_explained_var: 0.04871849715709686
        vf_loss: 19.680667877197266
      agent-5:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7020936012268066
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010679865954443812
        model: {}
        policy_loss: -0.003128234762698412
        total_loss: -0.002480599097907543
        vf_explained_var: 0.10160215198993683
        vf_loss: 18.833202362060547
    load_time_ms: 12933.188
    num_steps_sampled: 22560000
    num_steps_trained: 22560000
    sample_time_ms: 89996.847
    update_time_ms: 21.907
  iterations_since_restore: 175
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.377840909090908
    ram_util_percent: 10.1375
  pid: 24061
  policy_reward_max:
    agent-0: 190.99999999999972
    agent-1: 190.99999999999972
    agent-2: 190.99999999999972
    agent-3: 190.99999999999972
    agent-4: 190.99999999999972
    agent-5: 190.99999999999972
  policy_reward_mean:
    agent-0: 149.61166666666665
    agent-1: 149.61166666666665
    agent-2: 149.61166666666665
    agent-3: 149.61166666666665
    agent-4: 149.61166666666665
    agent-5: 149.61166666666665
  policy_reward_min:
    agent-0: 56.16666666666635
    agent-1: 56.16666666666635
    agent-2: 56.16666666666635
    agent-3: 56.16666666666635
    agent-4: 56.16666666666635
    agent-5: 56.16666666666635
  sampler_perf:
    mean_env_wait_ms: 24.374084815586276
    mean_inference_ms: 12.273581660698538
    mean_processing_ms: 50.86493899670656
  time_since_restore: 22594.313232183456
  time_this_iter_s: 123.74823451042175
  time_total_s: 31720.32504606247
  timestamp: 1637046627
  timesteps_since_restore: 16800000
  timesteps_this_iter: 96000
  timesteps_total: 22560000
  training_iteration: 235
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    235 |          31720.3 | 22560000 |   897.67 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 43
    apples_agent-0_mean: 2.43
    apples_agent-0_min: 0
    apples_agent-1_max: 144
    apples_agent-1_mean: 36.29
    apples_agent-1_min: 0
    apples_agent-2_max: 89
    apples_agent-2_mean: 6.1
    apples_agent-2_min: 0
    apples_agent-3_max: 163
    apples_agent-3_mean: 108.52
    apples_agent-3_min: 34
    apples_agent-4_max: 68
    apples_agent-4_mean: 1.8
    apples_agent-4_min: 0
    apples_agent-5_max: 132
    apples_agent-5_mean: 89.92
    apples_agent-5_min: 28
    cleaning_beam_agent-0_max: 514
    cleaning_beam_agent-0_mean: 373.87
    cleaning_beam_agent-0_min: 189
    cleaning_beam_agent-1_max: 448
    cleaning_beam_agent-1_mean: 247.2
    cleaning_beam_agent-1_min: 115
    cleaning_beam_agent-2_max: 589
    cleaning_beam_agent-2_mean: 410.49
    cleaning_beam_agent-2_min: 172
    cleaning_beam_agent-3_max: 87
    cleaning_beam_agent-3_mean: 21.89
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 578
    cleaning_beam_agent-4_mean: 444.78
    cleaning_beam_agent-4_min: 245
    cleaning_beam_agent-5_max: 261
    cleaning_beam_agent-5_mean: 38.94
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-12-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1118.9999999999818
  episode_reward_mean: 899.8599999999843
  episode_reward_min: 418.0000000000035
  episodes_this_iter: 96
  episodes_total: 22656
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19647.766
    learner:
      agent-0:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1069971323013306
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014129565097391605
        model: {}
        policy_loss: -0.003085469361394644
        total_loss: -0.0030805272981524467
        vf_explained_var: 0.023962676525115967
        vf_loss: 19.53255844116211
      agent-1:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 1.112412452697754
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011019400553777814
        model: {}
        policy_loss: -0.004045005887746811
        total_loss: -0.003878200426697731
        vf_explained_var: -0.036350250244140625
        vf_loss: 21.24651527404785
      agent-2:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9862837195396423
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010636972729116678
        model: {}
        policy_loss: -0.003122802823781967
        total_loss: -0.0029334821738302708
        vf_explained_var: 0.05261792242527008
        vf_loss: 19.25183868408203
      agent-3:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6040739417076111
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019503651419654489
        model: {}
        policy_loss: -0.002585124457255006
        total_loss: -0.0019042997155338526
        vf_explained_var: 0.1289212703704834
        vf_loss: 17.439956665039062
      agent-4:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9308245182037354
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015180641785264015
        model: {}
        policy_loss: -0.0038195643573999405
        total_loss: -0.003495784942060709
        vf_explained_var: 0.032026082277297974
        vf_loss: 19.620290756225586
      agent-5:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7052417993545532
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008775260066613555
        model: {}
        policy_loss: -0.003150829579681158
        total_loss: -0.0025481050834059715
        vf_explained_var: 0.09634184837341309
        vf_loss: 18.439491271972656
    load_time_ms: 12899.235
    num_steps_sampled: 22656000
    num_steps_trained: 22656000
    sample_time_ms: 90104.964
    update_time_ms: 23.486
  iterations_since_restore: 176
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.407386363636363
    ram_util_percent: 10.155681818181817
  pid: 24061
  policy_reward_max:
    agent-0: 186.4999999999999
    agent-1: 186.4999999999999
    agent-2: 186.4999999999999
    agent-3: 186.4999999999999
    agent-4: 186.4999999999999
    agent-5: 186.4999999999999
  policy_reward_mean:
    agent-0: 149.97666666666663
    agent-1: 149.97666666666663
    agent-2: 149.97666666666663
    agent-3: 149.97666666666663
    agent-4: 149.97666666666663
    agent-5: 149.97666666666663
  policy_reward_min:
    agent-0: 69.66666666666667
    agent-1: 69.66666666666667
    agent-2: 69.66666666666667
    agent-3: 69.66666666666667
    agent-4: 69.66666666666667
    agent-5: 69.66666666666667
  sampler_perf:
    mean_env_wait_ms: 24.377342998466318
    mean_inference_ms: 12.272985210176458
    mean_processing_ms: 50.862613024018685
  time_since_restore: 22717.49987602234
  time_this_iter_s: 123.18664383888245
  time_total_s: 31843.511689901352
  timestamp: 1637046751
  timesteps_since_restore: 16896000
  timesteps_this_iter: 96000
  timesteps_total: 22656000
  training_iteration: 236
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    236 |          31843.5 | 22656000 |   899.86 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 129
    apples_agent-0_mean: 4.53
    apples_agent-0_min: 0
    apples_agent-1_max: 173
    apples_agent-1_mean: 35.63
    apples_agent-1_min: 0
    apples_agent-2_max: 90
    apples_agent-2_mean: 3.24
    apples_agent-2_min: 0
    apples_agent-3_max: 251
    apples_agent-3_mean: 111.99
    apples_agent-3_min: 34
    apples_agent-4_max: 41
    apples_agent-4_mean: 1.02
    apples_agent-4_min: 0
    apples_agent-5_max: 190
    apples_agent-5_mean: 87.75
    apples_agent-5_min: 29
    cleaning_beam_agent-0_max: 512
    cleaning_beam_agent-0_mean: 371.64
    cleaning_beam_agent-0_min: 171
    cleaning_beam_agent-1_max: 437
    cleaning_beam_agent-1_mean: 233.76
    cleaning_beam_agent-1_min: 91
    cleaning_beam_agent-2_max: 596
    cleaning_beam_agent-2_mean: 412.05
    cleaning_beam_agent-2_min: 242
    cleaning_beam_agent-3_max: 87
    cleaning_beam_agent-3_mean: 22.11
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 573
    cleaning_beam_agent-4_mean: 450.53
    cleaning_beam_agent-4_min: 312
    cleaning_beam_agent-5_max: 145
    cleaning_beam_agent-5_mean: 32.54
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-14-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1085.9999999999898
  episode_reward_mean: 915.4699999999847
  episode_reward_min: 442.00000000000597
  episodes_this_iter: 96
  episodes_total: 22752
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19656.31
    learner:
      agent-0:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0965304374694824
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020213285461068153
        model: {}
        policy_loss: -0.003454921068623662
        total_loss: -0.0033654954750090837
        vf_explained_var: 0.03270861506462097
        vf_loss: 20.193178176879883
      agent-1:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0873165130615234
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011323329526931047
        model: {}
        policy_loss: -0.003995568957179785
        total_loss: -0.003663975978270173
        vf_explained_var: -0.02683258056640625
        vf_loss: 22.452714920043945
      agent-2:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9904298186302185
        entropy_coeff: 0.0017600000137463212
        kl: 0.00141285196878016
        model: {}
        policy_loss: -0.003684336319565773
        total_loss: -0.0034044627100229263
        vf_explained_var: 0.03969414532184601
        vf_loss: 20.230302810668945
      agent-3:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5734375715255737
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016247171442955732
        model: {}
        policy_loss: -0.00255992216989398
        total_loss: -0.0017290404066443443
        vf_explained_var: 0.11149190366268158
        vf_loss: 18.40135955810547
      agent-4:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.931036651134491
        entropy_coeff: 0.0017600000137463212
        kl: 0.002177279908210039
        model: {}
        policy_loss: -0.004469622392207384
        total_loss: -0.00407518120482564
        vf_explained_var: 0.042534828186035156
        vf_loss: 20.330610275268555
      agent-5:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6985281109809875
        entropy_coeff: 0.0017600000137463212
        kl: 0.001208366360515356
        model: {}
        policy_loss: -0.0039215381257236
        total_loss: -0.0031943523790687323
        vf_explained_var: 0.09036169946193695
        vf_loss: 19.565956115722656
    load_time_ms: 12890.416
    num_steps_sampled: 22752000
    num_steps_trained: 22752000
    sample_time_ms: 90195.861
    update_time_ms: 23.355
  iterations_since_restore: 177
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.361142857142859
    ram_util_percent: 10.155428571428567
  pid: 24061
  policy_reward_max:
    agent-0: 180.9999999999997
    agent-1: 180.9999999999997
    agent-2: 180.9999999999997
    agent-3: 180.9999999999997
    agent-4: 180.9999999999997
    agent-5: 180.9999999999997
  policy_reward_mean:
    agent-0: 152.57833333333326
    agent-1: 152.57833333333326
    agent-2: 152.57833333333326
    agent-3: 152.57833333333326
    agent-4: 152.57833333333326
    agent-5: 152.57833333333326
  policy_reward_min:
    agent-0: 73.66666666666681
    agent-1: 73.66666666666681
    agent-2: 73.66666666666681
    agent-3: 73.66666666666681
    agent-4: 73.66666666666681
    agent-5: 73.66666666666681
  sampler_perf:
    mean_env_wait_ms: 24.378904608402344
    mean_inference_ms: 12.27215601409745
    mean_processing_ms: 50.85755285488245
  time_since_restore: 22840.446590423584
  time_this_iter_s: 122.94671440124512
  time_total_s: 31966.458404302597
  timestamp: 1637046874
  timesteps_since_restore: 16992000
  timesteps_this_iter: 96000
  timesteps_total: 22752000
  training_iteration: 237
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    237 |          31966.5 | 22752000 |   915.47 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 75
    apples_agent-0_mean: 3.48
    apples_agent-0_min: 0
    apples_agent-1_max: 121
    apples_agent-1_mean: 30.22
    apples_agent-1_min: 0
    apples_agent-2_max: 78
    apples_agent-2_mean: 4.27
    apples_agent-2_min: 0
    apples_agent-3_max: 175
    apples_agent-3_mean: 110.01
    apples_agent-3_min: 69
    apples_agent-4_max: 34
    apples_agent-4_mean: 1.23
    apples_agent-4_min: 0
    apples_agent-5_max: 154
    apples_agent-5_mean: 86.26
    apples_agent-5_min: 32
    cleaning_beam_agent-0_max: 495
    cleaning_beam_agent-0_mean: 374.23
    cleaning_beam_agent-0_min: 122
    cleaning_beam_agent-1_max: 500
    cleaning_beam_agent-1_mean: 258.86
    cleaning_beam_agent-1_min: 109
    cleaning_beam_agent-2_max: 596
    cleaning_beam_agent-2_mean: 406.56
    cleaning_beam_agent-2_min: 200
    cleaning_beam_agent-3_max: 80
    cleaning_beam_agent-3_mean: 21.37
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 604
    cleaning_beam_agent-4_mean: 458.41
    cleaning_beam_agent-4_min: 337
    cleaning_beam_agent-5_max: 110
    cleaning_beam_agent-5_mean: 34.0
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-16-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1163.0000000000043
  episode_reward_mean: 912.7099999999836
  episode_reward_min: 451.0000000000094
  episodes_this_iter: 96
  episodes_total: 22848
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19663.898
    learner:
      agent-0:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0887675285339355
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014065109426155686
        model: {}
        policy_loss: -0.003311845473945141
        total_loss: -0.0032188501209020615
        vf_explained_var: 0.03935918211936951
        vf_loss: 20.09228515625
      agent-1:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0920040607452393
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014333450235426426
        model: {}
        policy_loss: -0.004176419693976641
        total_loss: -0.0038467436097562313
        vf_explained_var: -0.050840675830841064
        vf_loss: 22.516040802001953
      agent-2:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9869042634963989
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010031480342149734
        model: {}
        policy_loss: -0.0033852537162601948
        total_loss: -0.0031665742862969637
        vf_explained_var: 0.08434928953647614
        vf_loss: 19.55630874633789
      agent-3:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5822601914405823
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011487524025142193
        model: {}
        policy_loss: -0.002415324794128537
        total_loss: -0.0015977919101715088
        vf_explained_var: 0.11340571939945221
        vf_loss: 18.42313003540039
      agent-4:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9266452789306641
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017042981926351786
        model: {}
        policy_loss: -0.004096977412700653
        total_loss: -0.003704958828166127
        vf_explained_var: 0.05030311644077301
        vf_loss: 20.22915267944336
      agent-5:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6987016797065735
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011053953785449266
        model: {}
        policy_loss: -0.003265389706939459
        total_loss: -0.0026024202816188335
        vf_explained_var: 0.12497551739215851
        vf_loss: 18.926834106445312
    load_time_ms: 12892.158
    num_steps_sampled: 22848000
    num_steps_trained: 22848000
    sample_time_ms: 90099.21
    update_time_ms: 23.298
  iterations_since_restore: 178
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.468
    ram_util_percent: 10.168571428571429
  pid: 24061
  policy_reward_max:
    agent-0: 193.8333333333329
    agent-1: 193.8333333333329
    agent-2: 193.8333333333329
    agent-3: 193.8333333333329
    agent-4: 193.8333333333329
    agent-5: 193.8333333333329
  policy_reward_mean:
    agent-0: 152.11833333333328
    agent-1: 152.11833333333328
    agent-2: 152.11833333333328
    agent-3: 152.11833333333328
    agent-4: 152.11833333333328
    agent-5: 152.11833333333328
  policy_reward_min:
    agent-0: 75.16666666666667
    agent-1: 75.16666666666667
    agent-2: 75.16666666666667
    agent-3: 75.16666666666667
    agent-4: 75.16666666666667
    agent-5: 75.16666666666667
  sampler_perf:
    mean_env_wait_ms: 24.38097524841231
    mean_inference_ms: 12.271173669120264
    mean_processing_ms: 50.8522826420901
  time_since_restore: 22963.031700134277
  time_this_iter_s: 122.58510971069336
  time_total_s: 32089.04351401329
  timestamp: 1637046997
  timesteps_since_restore: 17088000
  timesteps_this_iter: 96000
  timesteps_total: 22848000
  training_iteration: 238
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 18.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    238 |            32089 | 22848000 |   912.71 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 49
    apples_agent-0_mean: 3.6
    apples_agent-0_min: 0
    apples_agent-1_max: 116
    apples_agent-1_mean: 37.2
    apples_agent-1_min: 0
    apples_agent-2_max: 36
    apples_agent-2_mean: 2.29
    apples_agent-2_min: 0
    apples_agent-3_max: 173
    apples_agent-3_mean: 109.6
    apples_agent-3_min: 63
    apples_agent-4_max: 41
    apples_agent-4_mean: 0.99
    apples_agent-4_min: 0
    apples_agent-5_max: 136
    apples_agent-5_mean: 89.74
    apples_agent-5_min: 25
    cleaning_beam_agent-0_max: 527
    cleaning_beam_agent-0_mean: 365.08
    cleaning_beam_agent-0_min: 184
    cleaning_beam_agent-1_max: 516
    cleaning_beam_agent-1_mean: 235.4
    cleaning_beam_agent-1_min: 86
    cleaning_beam_agent-2_max: 596
    cleaning_beam_agent-2_mean: 417.41
    cleaning_beam_agent-2_min: 241
    cleaning_beam_agent-3_max: 66
    cleaning_beam_agent-3_mean: 20.12
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 582
    cleaning_beam_agent-4_mean: 454.8
    cleaning_beam_agent-4_min: 282
    cleaning_beam_agent-5_max: 180
    cleaning_beam_agent-5_mean: 32.7
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-18-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1097.9999999999775
  episode_reward_mean: 922.7699999999826
  episode_reward_min: 584.9999999999866
  episodes_this_iter: 96
  episodes_total: 22944
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19679.496
    learner:
      agent-0:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0961244106292725
        entropy_coeff: 0.0017600000137463212
        kl: 0.002409466542303562
        model: {}
        policy_loss: -0.0035508135333657265
        total_loss: -0.003547179512679577
        vf_explained_var: 0.05207382142543793
        vf_loss: 19.328147888183594
      agent-1:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1016790866851807
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017771214479580522
        model: {}
        policy_loss: -0.004249292425811291
        total_loss: -0.003942697308957577
        vf_explained_var: -0.05868816375732422
        vf_loss: 22.455488204956055
      agent-2:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9974556565284729
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015591586707159877
        model: {}
        policy_loss: -0.003543547820299864
        total_loss: -0.003291938453912735
        vf_explained_var: 0.019517451524734497
        vf_loss: 20.07132339477539
      agent-3:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5734179019927979
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016084572998806834
        model: {}
        policy_loss: -0.0026620624121278524
        total_loss: -0.0019043482607230544
        vf_explained_var: 0.12590621411800385
        vf_loss: 17.669260025024414
      agent-4:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9474292397499084
        entropy_coeff: 0.0017600000137463212
        kl: 0.001677889027632773
        model: {}
        policy_loss: -0.0039211297407746315
        total_loss: -0.00357949361205101
        vf_explained_var: 0.035971030592918396
        vf_loss: 20.091102600097656
      agent-5:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7042562961578369
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009790159529075027
        model: {}
        policy_loss: -0.0033015890512615442
        total_loss: -0.0026738252490758896
        vf_explained_var: 0.10423433780670166
        vf_loss: 18.67255401611328
    load_time_ms: 12927.252
    num_steps_sampled: 22944000
    num_steps_trained: 22944000
    sample_time_ms: 90192.164
    update_time_ms: 22.96
  iterations_since_restore: 179
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.338636363636363
    ram_util_percent: 10.089772727272727
  pid: 24061
  policy_reward_max:
    agent-0: 182.9999999999998
    agent-1: 182.9999999999998
    agent-2: 182.9999999999998
    agent-3: 182.9999999999998
    agent-4: 182.9999999999998
    agent-5: 182.9999999999998
  policy_reward_mean:
    agent-0: 153.795
    agent-1: 153.795
    agent-2: 153.795
    agent-3: 153.795
    agent-4: 153.795
    agent-5: 153.795
  policy_reward_min:
    agent-0: 97.50000000000023
    agent-1: 97.50000000000023
    agent-2: 97.50000000000023
    agent-3: 97.50000000000023
    agent-4: 97.50000000000023
    agent-5: 97.50000000000023
  sampler_perf:
    mean_env_wait_ms: 24.383204066684957
    mean_inference_ms: 12.270286746089731
    mean_processing_ms: 50.84870725520014
  time_since_restore: 23086.74140906334
  time_this_iter_s: 123.70970892906189
  time_total_s: 32212.753222942352
  timestamp: 1637047120
  timesteps_since_restore: 17184000
  timesteps_this_iter: 96000
  timesteps_total: 22944000
  training_iteration: 239
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    239 |          32212.8 | 22944000 |   922.77 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 98
    apples_agent-0_mean: 4.32
    apples_agent-0_min: 0
    apples_agent-1_max: 122
    apples_agent-1_mean: 32.14
    apples_agent-1_min: 0
    apples_agent-2_max: 106
    apples_agent-2_mean: 5.95
    apples_agent-2_min: 0
    apples_agent-3_max: 187
    apples_agent-3_mean: 116.03
    apples_agent-3_min: 2
    apples_agent-4_max: 47
    apples_agent-4_mean: 0.62
    apples_agent-4_min: 0
    apples_agent-5_max: 143
    apples_agent-5_mean: 94.25
    apples_agent-5_min: 49
    cleaning_beam_agent-0_max: 550
    cleaning_beam_agent-0_mean: 370.93
    cleaning_beam_agent-0_min: 228
    cleaning_beam_agent-1_max: 515
    cleaning_beam_agent-1_mean: 273.59
    cleaning_beam_agent-1_min: 97
    cleaning_beam_agent-2_max: 555
    cleaning_beam_agent-2_mean: 393.62
    cleaning_beam_agent-2_min: 135
    cleaning_beam_agent-3_max: 58
    cleaning_beam_agent-3_mean: 22.63
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 602
    cleaning_beam_agent-4_mean: 459.92
    cleaning_beam_agent-4_min: 352
    cleaning_beam_agent-5_max: 98
    cleaning_beam_agent-5_mean: 32.0
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-20-44
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1093.9999999999789
  episode_reward_mean: 934.7499999999854
  episode_reward_min: 568.0000000000015
  episodes_this_iter: 96
  episodes_total: 23040
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19675.377
    learner:
      agent-0:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 1.095332384109497
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014916750369593501
        model: {}
        policy_loss: -0.0036626867949962616
        total_loss: -0.0037348666228353977
        vf_explained_var: 0.016496703028678894
        vf_loss: 18.55605697631836
      agent-1:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1151347160339355
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015828526811674237
        model: {}
        policy_loss: -0.003730540629476309
        total_loss: -0.0037366102915257215
        vf_explained_var: -0.008096545934677124
        vf_loss: 19.565698623657227
      agent-2:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9944330453872681
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014833759050816298
        model: {}
        policy_loss: -0.0035931819584220648
        total_loss: -0.003388676093891263
        vf_explained_var: -0.02475076913833618
        vf_loss: 19.547086715698242
      agent-3:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5766505002975464
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019499491900205612
        model: {}
        policy_loss: -0.0022321997676044703
        total_loss: -0.0014996647369116545
        vf_explained_var: 0.06732763350009918
        vf_loss: 17.47440528869629
      agent-4:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9340189695358276
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015418094117194414
        model: {}
        policy_loss: -0.0039471713826060295
        total_loss: -0.0036950446665287018
        vf_explained_var: 0.017214015126228333
        vf_loss: 18.959959030151367
      agent-5:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6963969469070435
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011014747433364391
        model: {}
        policy_loss: -0.0035299151204526424
        total_loss: -0.0028749299235641956
        vf_explained_var: 0.02812829613685608
        vf_loss: 18.80643081665039
    load_time_ms: 12920.981
    num_steps_sampled: 23040000
    num_steps_trained: 23040000
    sample_time_ms: 90355.912
    update_time_ms: 22.696
  iterations_since_restore: 180
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.342613636363637
    ram_util_percent: 10.140909090909089
  pid: 24061
  policy_reward_max:
    agent-0: 182.3333333333332
    agent-1: 182.3333333333332
    agent-2: 182.3333333333332
    agent-3: 182.3333333333332
    agent-4: 182.3333333333332
    agent-5: 182.3333333333332
  policy_reward_mean:
    agent-0: 155.7916666666666
    agent-1: 155.7916666666666
    agent-2: 155.7916666666666
    agent-3: 155.7916666666666
    agent-4: 155.7916666666666
    agent-5: 155.7916666666666
  policy_reward_min:
    agent-0: 94.66666666666704
    agent-1: 94.66666666666704
    agent-2: 94.66666666666704
    agent-3: 94.66666666666704
    agent-4: 94.66666666666704
    agent-5: 94.66666666666704
  sampler_perf:
    mean_env_wait_ms: 24.385441190989173
    mean_inference_ms: 12.26933659350878
    mean_processing_ms: 50.84458104781081
  time_since_restore: 23210.278309822083
  time_this_iter_s: 123.53690075874329
  time_total_s: 32336.290123701096
  timestamp: 1637047244
  timesteps_since_restore: 17280000
  timesteps_this_iter: 96000
  timesteps_total: 23040000
  training_iteration: 240
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    240 |          32336.3 | 23040000 |   934.75 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 66
    apples_agent-0_mean: 4.27
    apples_agent-0_min: 0
    apples_agent-1_max: 114
    apples_agent-1_mean: 28.53
    apples_agent-1_min: 0
    apples_agent-2_max: 122
    apples_agent-2_mean: 5.17
    apples_agent-2_min: 0
    apples_agent-3_max: 236
    apples_agent-3_mean: 109.93
    apples_agent-3_min: 59
    apples_agent-4_max: 41
    apples_agent-4_mean: 1.31
    apples_agent-4_min: 0
    apples_agent-5_max: 252
    apples_agent-5_mean: 92.4
    apples_agent-5_min: 23
    cleaning_beam_agent-0_max: 495
    cleaning_beam_agent-0_mean: 368.34
    cleaning_beam_agent-0_min: 189
    cleaning_beam_agent-1_max: 483
    cleaning_beam_agent-1_mean: 255.07
    cleaning_beam_agent-1_min: 103
    cleaning_beam_agent-2_max: 524
    cleaning_beam_agent-2_mean: 392.45
    cleaning_beam_agent-2_min: 191
    cleaning_beam_agent-3_max: 88
    cleaning_beam_agent-3_mean: 21.92
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 555
    cleaning_beam_agent-4_mean: 454.67
    cleaning_beam_agent-4_min: 357
    cleaning_beam_agent-5_max: 219
    cleaning_beam_agent-5_mean: 35.94
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-22-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1098.0000000000005
  episode_reward_mean: 909.9799999999841
  episode_reward_min: 493.0000000000073
  episodes_this_iter: 96
  episodes_total: 23136
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19702.045
    learner:
      agent-0:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1112632751464844
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016372321406379342
        model: {}
        policy_loss: -0.003079623682424426
        total_loss: -0.003046891652047634
        vf_explained_var: 0.017711207270622253
        vf_loss: 19.885560989379883
      agent-1:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0962464809417725
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012171942507848144
        model: {}
        policy_loss: -0.004061590880155563
        total_loss: -0.003846175968647003
        vf_explained_var: -0.03996986150741577
        vf_loss: 21.44809913635254
      agent-2:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 1.012890100479126
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016952058067545295
        model: {}
        policy_loss: -0.0037655953783541918
        total_loss: -0.0036345894914120436
        vf_explained_var: 0.06367379426956177
        vf_loss: 19.136911392211914
      agent-3:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5775786638259888
        entropy_coeff: 0.0017600000137463212
        kl: 0.000987212173640728
        model: {}
        policy_loss: -0.002385627245530486
        total_loss: -0.0015616397140547633
        vf_explained_var: 0.09148178994655609
        vf_loss: 18.405258178710938
      agent-4:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9505380392074585
        entropy_coeff: 0.0017600000137463212
        kl: 0.00205929740332067
        model: {}
        policy_loss: -0.0041857450269162655
        total_loss: -0.0038516849745064974
        vf_explained_var: 0.02802276611328125
        vf_loss: 20.070087432861328
      agent-5:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7053967714309692
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014559407718479633
        model: {}
        policy_loss: -0.0034894307609647512
        total_loss: -0.00287442933768034
        vf_explained_var: 0.10186930000782013
        vf_loss: 18.565013885498047
    load_time_ms: 12919.313
    num_steps_sampled: 23136000
    num_steps_trained: 23136000
    sample_time_ms: 90335.726
    update_time_ms: 23.015
  iterations_since_restore: 181
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.49542857142857
    ram_util_percent: 10.143428571428569
  pid: 24061
  policy_reward_max:
    agent-0: 182.99999999999994
    agent-1: 182.99999999999994
    agent-2: 182.99999999999994
    agent-3: 182.99999999999994
    agent-4: 182.99999999999994
    agent-5: 182.99999999999994
  policy_reward_mean:
    agent-0: 151.66333333333333
    agent-1: 151.66333333333333
    agent-2: 151.66333333333333
    agent-3: 151.66333333333333
    agent-4: 151.66333333333333
    agent-5: 151.66333333333333
  policy_reward_min:
    agent-0: 82.16666666666654
    agent-1: 82.16666666666654
    agent-2: 82.16666666666654
    agent-3: 82.16666666666654
    agent-4: 82.16666666666654
    agent-5: 82.16666666666654
  sampler_perf:
    mean_env_wait_ms: 24.3870725090866
    mean_inference_ms: 12.268515489582887
    mean_processing_ms: 50.84091547015055
  time_since_restore: 23332.90830731392
  time_this_iter_s: 122.62999749183655
  time_total_s: 32458.920121192932
  timestamp: 1637047367
  timesteps_since_restore: 17376000
  timesteps_this_iter: 96000
  timesteps_total: 23136000
  training_iteration: 241
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    241 |          32458.9 | 23136000 |   909.98 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 78
    apples_agent-0_mean: 4.54
    apples_agent-0_min: 0
    apples_agent-1_max: 105
    apples_agent-1_mean: 32.63
    apples_agent-1_min: 0
    apples_agent-2_max: 119
    apples_agent-2_mean: 4.59
    apples_agent-2_min: 0
    apples_agent-3_max: 167
    apples_agent-3_mean: 105.52
    apples_agent-3_min: 7
    apples_agent-4_max: 30
    apples_agent-4_mean: 1.34
    apples_agent-4_min: 0
    apples_agent-5_max: 127
    apples_agent-5_mean: 87.15
    apples_agent-5_min: 46
    cleaning_beam_agent-0_max: 532
    cleaning_beam_agent-0_mean: 370.83
    cleaning_beam_agent-0_min: 162
    cleaning_beam_agent-1_max: 472
    cleaning_beam_agent-1_mean: 259.36
    cleaning_beam_agent-1_min: 80
    cleaning_beam_agent-2_max: 551
    cleaning_beam_agent-2_mean: 411.42
    cleaning_beam_agent-2_min: 183
    cleaning_beam_agent-3_max: 94
    cleaning_beam_agent-3_mean: 21.46
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 585
    cleaning_beam_agent-4_mean: 452.83
    cleaning_beam_agent-4_min: 347
    cleaning_beam_agent-5_max: 97
    cleaning_beam_agent-5_mean: 30.27
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-24-50
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1097.9999999999905
  episode_reward_mean: 908.379999999985
  episode_reward_min: 309.00000000000017
  episodes_this_iter: 96
  episodes_total: 23232
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19685.311
    learner:
      agent-0:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 1.092362880706787
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015966386999934912
        model: {}
        policy_loss: -0.0033561564050614834
        total_loss: -0.0033191407565027475
        vf_explained_var: 0.01042957603931427
        vf_loss: 19.59575080871582
      agent-1:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0894144773483276
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015278036007657647
        model: {}
        policy_loss: -0.003887893632054329
        total_loss: -0.00367955444380641
        vf_explained_var: -0.04789525270462036
        vf_loss: 21.257061004638672
      agent-2:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9908008575439453
        entropy_coeff: 0.0017600000137463212
        kl: 0.001467766473069787
        model: {}
        policy_loss: -0.0035579947289079428
        total_loss: -0.003431436838582158
        vf_explained_var: 0.062399402260780334
        vf_loss: 18.70365333557129
      agent-3:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5836301445960999
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012405329616740346
        model: {}
        policy_loss: -0.0028286464512348175
        total_loss: -0.002093080896884203
        vf_explained_var: 0.11339141428470612
        vf_loss: 17.627546310424805
      agent-4:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9368524551391602
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014745721127837896
        model: {}
        policy_loss: -0.0038467603735625744
        total_loss: -0.0034913260024040937
        vf_explained_var: -0.0013783127069473267
        vf_loss: 20.042938232421875
      agent-5:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6940521001815796
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005450198659673333
        model: {}
        policy_loss: -0.0029886411502957344
        total_loss: -0.0023961151018738747
        vf_explained_var: 0.08905340731143951
        vf_loss: 18.14056396484375
    load_time_ms: 12916.079
    num_steps_sampled: 23232000
    num_steps_trained: 23232000
    sample_time_ms: 90385.035
    update_time_ms: 22.644
  iterations_since_restore: 182
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.419318181818182
    ram_util_percent: 10.071022727272727
  pid: 24061
  policy_reward_max:
    agent-0: 182.99999999999986
    agent-1: 182.99999999999986
    agent-2: 182.99999999999986
    agent-3: 182.99999999999986
    agent-4: 182.99999999999986
    agent-5: 182.99999999999986
  policy_reward_mean:
    agent-0: 151.3966666666667
    agent-1: 151.3966666666667
    agent-2: 151.3966666666667
    agent-3: 151.3966666666667
    agent-4: 151.3966666666667
    agent-5: 151.3966666666667
  policy_reward_min:
    agent-0: 51.49999999999989
    agent-1: 51.49999999999989
    agent-2: 51.49999999999989
    agent-3: 51.49999999999989
    agent-4: 51.49999999999989
    agent-5: 51.49999999999989
  sampler_perf:
    mean_env_wait_ms: 24.389840549310364
    mean_inference_ms: 12.267691529242713
    mean_processing_ms: 50.83655819215578
  time_since_restore: 23455.854811668396
  time_this_iter_s: 122.94650435447693
  time_total_s: 32581.86662554741
  timestamp: 1637047490
  timesteps_since_restore: 17472000
  timesteps_this_iter: 96000
  timesteps_total: 23232000
  training_iteration: 242
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    242 |          32581.9 | 23232000 |   908.38 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 55
    apples_agent-0_mean: 4.14
    apples_agent-0_min: 0
    apples_agent-1_max: 120
    apples_agent-1_mean: 32.38
    apples_agent-1_min: 0
    apples_agent-2_max: 80
    apples_agent-2_mean: 4.87
    apples_agent-2_min: 0
    apples_agent-3_max: 175
    apples_agent-3_mean: 106.29
    apples_agent-3_min: 56
    apples_agent-4_max: 83
    apples_agent-4_mean: 1.42
    apples_agent-4_min: 0
    apples_agent-5_max: 151
    apples_agent-5_mean: 92.41
    apples_agent-5_min: 26
    cleaning_beam_agent-0_max: 563
    cleaning_beam_agent-0_mean: 394.59
    cleaning_beam_agent-0_min: 220
    cleaning_beam_agent-1_max: 516
    cleaning_beam_agent-1_mean: 252.67
    cleaning_beam_agent-1_min: 81
    cleaning_beam_agent-2_max: 529
    cleaning_beam_agent-2_mean: 406.36
    cleaning_beam_agent-2_min: 216
    cleaning_beam_agent-3_max: 59
    cleaning_beam_agent-3_mean: 22.58
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 538
    cleaning_beam_agent-4_mean: 444.14
    cleaning_beam_agent-4_min: 292
    cleaning_beam_agent-5_max: 183
    cleaning_beam_agent-5_mean: 32.57
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-26-54
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1097.000000000001
  episode_reward_mean: 919.819999999984
  episode_reward_min: 542.0000000000092
  episodes_this_iter: 96
  episodes_total: 23328
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19712.902
    learner:
      agent-0:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0848071575164795
        entropy_coeff: 0.0017600000137463212
        kl: 0.001429854310117662
        model: {}
        policy_loss: -0.0033347569406032562
        total_loss: -0.003259045071899891
        vf_explained_var: 0.06494663655757904
        vf_loss: 19.849727630615234
      agent-1:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0904319286346436
        entropy_coeff: 0.0017600000137463212
        kl: 0.00130272819660604
        model: {}
        policy_loss: -0.004074125550687313
        total_loss: -0.003786488901823759
        vf_explained_var: -0.006776958703994751
        vf_loss: 22.067964553833008
      agent-2:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9871947765350342
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015182585921138525
        model: {}
        policy_loss: -0.003301474265754223
        total_loss: -0.0029689697548747063
        vf_explained_var: 0.03663317859172821
        vf_loss: 20.699623107910156
      agent-3:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5843467116355896
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015288267750293016
        model: {}
        policy_loss: -0.0027860517147928476
        total_loss: -0.0019337765406817198
        vf_explained_var: 0.1113632321357727
        vf_loss: 18.80722427368164
      agent-4:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9319759011268616
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019571837037801743
        model: {}
        policy_loss: -0.0042021749541163445
        total_loss: -0.0037324666045606136
        vf_explained_var: 0.032538771629333496
        vf_loss: 21.099872589111328
      agent-5:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6961755156517029
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013133720494806767
        model: {}
        policy_loss: -0.0034545266535133123
        total_loss: -0.0027701635845005512
        vf_explained_var: 0.10946975648403168
        vf_loss: 19.09634780883789
    load_time_ms: 12913.626
    num_steps_sampled: 23328000
    num_steps_trained: 23328000
    sample_time_ms: 90455.127
    update_time_ms: 22.6
  iterations_since_restore: 183
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.424571428571427
    ram_util_percent: 10.153714285714283
  pid: 24061
  policy_reward_max:
    agent-0: 182.83333333333294
    agent-1: 182.83333333333294
    agent-2: 182.83333333333294
    agent-3: 182.83333333333294
    agent-4: 182.83333333333294
    agent-5: 182.83333333333294
  policy_reward_mean:
    agent-0: 153.30333333333328
    agent-1: 153.30333333333328
    agent-2: 153.30333333333328
    agent-3: 153.30333333333328
    agent-4: 153.30333333333328
    agent-5: 153.30333333333328
  policy_reward_min:
    agent-0: 90.33333333333346
    agent-1: 90.33333333333346
    agent-2: 90.33333333333346
    agent-3: 90.33333333333346
    agent-4: 90.33333333333346
    agent-5: 90.33333333333346
  sampler_perf:
    mean_env_wait_ms: 24.391768424968895
    mean_inference_ms: 12.266679107000733
    mean_processing_ms: 50.832542293009475
  time_since_restore: 23578.851082086563
  time_this_iter_s: 122.99627041816711
  time_total_s: 32704.862895965576
  timestamp: 1637047614
  timesteps_since_restore: 17568000
  timesteps_this_iter: 96000
  timesteps_total: 23328000
  training_iteration: 243
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    243 |          32704.9 | 23328000 |   919.82 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 72
    apples_agent-0_mean: 4.1
    apples_agent-0_min: 0
    apples_agent-1_max: 101
    apples_agent-1_mean: 33.51
    apples_agent-1_min: 0
    apples_agent-2_max: 32
    apples_agent-2_mean: 2.05
    apples_agent-2_min: 0
    apples_agent-3_max: 189
    apples_agent-3_mean: 111.29
    apples_agent-3_min: 57
    apples_agent-4_max: 58
    apples_agent-4_mean: 1.45
    apples_agent-4_min: 0
    apples_agent-5_max: 151
    apples_agent-5_mean: 86.32
    apples_agent-5_min: 10
    cleaning_beam_agent-0_max: 521
    cleaning_beam_agent-0_mean: 383.26
    cleaning_beam_agent-0_min: 233
    cleaning_beam_agent-1_max: 570
    cleaning_beam_agent-1_mean: 272.93
    cleaning_beam_agent-1_min: 85
    cleaning_beam_agent-2_max: 545
    cleaning_beam_agent-2_mean: 419.3
    cleaning_beam_agent-2_min: 153
    cleaning_beam_agent-3_max: 104
    cleaning_beam_agent-3_mean: 23.36
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 541
    cleaning_beam_agent-4_mean: 454.54
    cleaning_beam_agent-4_min: 230
    cleaning_beam_agent-5_max: 202
    cleaning_beam_agent-5_mean: 34.35
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-28-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1129.9999999999836
  episode_reward_mean: 922.3999999999847
  episode_reward_min: 366.00000000000455
  episodes_this_iter: 96
  episodes_total: 23424
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19699.791
    learner:
      agent-0:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0834848880767822
        entropy_coeff: 0.0017600000137463212
        kl: 0.001644448726437986
        model: {}
        policy_loss: -0.0032122405245900154
        total_loss: -0.003025333397090435
        vf_explained_var: 0.012914478778839111
        vf_loss: 20.938413619995117
      agent-1:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 1.094209909439087
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015862323343753815
        model: {}
        policy_loss: -0.004265117458999157
        total_loss: -0.003988269716501236
        vf_explained_var: -0.010361671447753906
        vf_loss: 22.026565551757812
      agent-2:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9833751320838928
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015919588040560484
        model: {}
        policy_loss: -0.00351130124181509
        total_loss: -0.003155538346618414
        vf_explained_var: 0.02807675302028656
        vf_loss: 20.865055084228516
      agent-3:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5862836837768555
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010419265599921346
        model: {}
        policy_loss: -0.002455863170325756
        total_loss: -0.0016574861947447062
        vf_explained_var: 0.14415639638900757
        vf_loss: 18.30239486694336
      agent-4:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.928189218044281
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014102633576840162
        model: {}
        policy_loss: -0.0037491803523153067
        total_loss: -0.003343424294143915
        vf_explained_var: 0.06083446741104126
        vf_loss: 20.393695831298828
      agent-5:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6819814443588257
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011364626698195934
        model: {}
        policy_loss: -0.003229064866900444
        total_loss: -0.002433265093713999
        vf_explained_var: 0.07785068452358246
        vf_loss: 19.96087646484375
    load_time_ms: 12918.465
    num_steps_sampled: 23424000
    num_steps_trained: 23424000
    sample_time_ms: 90426.943
    update_time_ms: 22.358
  iterations_since_restore: 184
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.447727272727274
    ram_util_percent: 10.165340909090908
  pid: 24061
  policy_reward_max:
    agent-0: 188.33333333333337
    agent-1: 188.33333333333337
    agent-2: 188.33333333333337
    agent-3: 188.33333333333337
    agent-4: 188.33333333333337
    agent-5: 188.33333333333337
  policy_reward_mean:
    agent-0: 153.7333333333333
    agent-1: 153.7333333333333
    agent-2: 153.7333333333333
    agent-3: 153.7333333333333
    agent-4: 153.7333333333333
    agent-5: 153.7333333333333
  policy_reward_min:
    agent-0: 60.999999999999766
    agent-1: 60.999999999999766
    agent-2: 60.999999999999766
    agent-3: 60.999999999999766
    agent-4: 60.999999999999766
    agent-5: 60.999999999999766
  sampler_perf:
    mean_env_wait_ms: 24.3946932672372
    mean_inference_ms: 12.265588416603459
    mean_processing_ms: 50.829004429084335
  time_since_restore: 23702.194087028503
  time_this_iter_s: 123.34300494194031
  time_total_s: 32828.20590090752
  timestamp: 1637047737
  timesteps_since_restore: 17664000
  timesteps_this_iter: 96000
  timesteps_total: 23424000
  training_iteration: 244
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    244 |          32828.2 | 23424000 |    922.4 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 57
    apples_agent-0_mean: 3.5
    apples_agent-0_min: 0
    apples_agent-1_max: 107
    apples_agent-1_mean: 32.18
    apples_agent-1_min: 0
    apples_agent-2_max: 45
    apples_agent-2_mean: 2.79
    apples_agent-2_min: 0
    apples_agent-3_max: 543
    apples_agent-3_mean: 121.81
    apples_agent-3_min: 68
    apples_agent-4_max: 25
    apples_agent-4_mean: 0.54
    apples_agent-4_min: 0
    apples_agent-5_max: 467
    apples_agent-5_mean: 94.14
    apples_agent-5_min: 26
    cleaning_beam_agent-0_max: 522
    cleaning_beam_agent-0_mean: 391.46
    cleaning_beam_agent-0_min: 174
    cleaning_beam_agent-1_max: 539
    cleaning_beam_agent-1_mean: 277.14
    cleaning_beam_agent-1_min: 98
    cleaning_beam_agent-2_max: 599
    cleaning_beam_agent-2_mean: 434.21
    cleaning_beam_agent-2_min: 221
    cleaning_beam_agent-3_max: 65
    cleaning_beam_agent-3_mean: 23.09
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 576
    cleaning_beam_agent-4_mean: 469.72
    cleaning_beam_agent-4_min: 335
    cleaning_beam_agent-5_max: 169
    cleaning_beam_agent-5_mean: 28.6
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-31-01
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1129.9999999999836
  episode_reward_mean: 942.7199999999845
  episode_reward_min: 522.00000000001
  episodes_this_iter: 96
  episodes_total: 23520
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19708.673
    learner:
      agent-0:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0883631706237793
        entropy_coeff: 0.0017600000137463212
        kl: 0.001648225006647408
        model: {}
        policy_loss: -0.003101568901911378
        total_loss: -0.003078805049881339
        vf_explained_var: 0.004173353314399719
        vf_loss: 19.38286018371582
      agent-1:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0954664945602417
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013290063943713903
        model: {}
        policy_loss: -0.003996038809418678
        total_loss: -0.003912773914635181
        vf_explained_var: -0.005241274833679199
        vf_loss: 20.112823486328125
      agent-2:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9611349105834961
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018876302056014538
        model: {}
        policy_loss: -0.0035692621022462845
        total_loss: -0.0033230825792998075
        vf_explained_var: 0.007270321249961853
        vf_loss: 19.377769470214844
      agent-3:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5636526942253113
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014751103008165956
        model: {}
        policy_loss: -0.002470297273248434
        total_loss: -0.001724391826428473
        vf_explained_var: 0.08539065718650818
        vf_loss: 17.379369735717773
      agent-4:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9119573831558228
        entropy_coeff: 0.0017600000137463212
        kl: 0.002106280066072941
        model: {}
        policy_loss: -0.003984283655881882
        total_loss: -0.0036284732632339
        vf_explained_var: 0.004024699330329895
        vf_loss: 19.608545303344727
      agent-5:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6669603586196899
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008518415270373225
        model: {}
        policy_loss: -0.0028690537437796593
        total_loss: -0.0021695122122764587
        vf_explained_var: 0.05256359279155731
        vf_loss: 18.73389434814453
    load_time_ms: 12919.85
    num_steps_sampled: 23520000
    num_steps_trained: 23520000
    sample_time_ms: 90466.297
    update_time_ms: 23.074
  iterations_since_restore: 185
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.836158192090396
    ram_util_percent: 10.215819209039546
  pid: 24061
  policy_reward_max:
    agent-0: 188.33333333333337
    agent-1: 188.33333333333337
    agent-2: 188.33333333333337
    agent-3: 188.33333333333337
    agent-4: 188.33333333333337
    agent-5: 188.33333333333337
  policy_reward_mean:
    agent-0: 157.12
    agent-1: 157.12
    agent-2: 157.12
    agent-3: 157.12
    agent-4: 157.12
    agent-5: 157.12
  policy_reward_min:
    agent-0: 87.00000000000003
    agent-1: 87.00000000000003
    agent-2: 87.00000000000003
    agent-3: 87.00000000000003
    agent-4: 87.00000000000003
    agent-5: 87.00000000000003
  sampler_perf:
    mean_env_wait_ms: 24.39849600615402
    mean_inference_ms: 12.264867195874023
    mean_processing_ms: 50.82627201119742
  time_since_restore: 23826.414744615555
  time_this_iter_s: 124.22065758705139
  time_total_s: 32952.42655849457
  timestamp: 1637047861
  timesteps_since_restore: 17760000
  timesteps_this_iter: 96000
  timesteps_total: 23520000
  training_iteration: 245
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    245 |          32952.4 | 23520000 |   942.72 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 111
    apples_agent-0_mean: 3.87
    apples_agent-0_min: 0
    apples_agent-1_max: 102
    apples_agent-1_mean: 31.72
    apples_agent-1_min: 0
    apples_agent-2_max: 79
    apples_agent-2_mean: 4.59
    apples_agent-2_min: 0
    apples_agent-3_max: 184
    apples_agent-3_mean: 113.41
    apples_agent-3_min: 36
    apples_agent-4_max: 92
    apples_agent-4_mean: 2.13
    apples_agent-4_min: 0
    apples_agent-5_max: 176
    apples_agent-5_mean: 93.31
    apples_agent-5_min: 30
    cleaning_beam_agent-0_max: 517
    cleaning_beam_agent-0_mean: 372.24
    cleaning_beam_agent-0_min: 198
    cleaning_beam_agent-1_max: 555
    cleaning_beam_agent-1_mean: 265.1
    cleaning_beam_agent-1_min: 107
    cleaning_beam_agent-2_max: 623
    cleaning_beam_agent-2_mean: 429.15
    cleaning_beam_agent-2_min: 233
    cleaning_beam_agent-3_max: 97
    cleaning_beam_agent-3_mean: 21.4
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 578
    cleaning_beam_agent-4_mean: 467.9
    cleaning_beam_agent-4_min: 276
    cleaning_beam_agent-5_max: 110
    cleaning_beam_agent-5_mean: 28.19
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-33-05
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1170.000000000004
  episode_reward_mean: 930.8999999999828
  episode_reward_min: 360.0000000000057
  episodes_this_iter: 96
  episodes_total: 23616
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19692.932
    learner:
      agent-0:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1025561094284058
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022474750876426697
        model: {}
        policy_loss: -0.0035171667113900185
        total_loss: -0.0034974445588886738
        vf_explained_var: 0.023595720529556274
        vf_loss: 19.60220718383789
      agent-1:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0915592908859253
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013887217501178384
        model: {}
        policy_loss: -0.004084083251655102
        total_loss: -0.0038841625209897757
        vf_explained_var: -0.018529504537582397
        vf_loss: 21.210678100585938
      agent-2:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9605215787887573
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014693050179630518
        model: {}
        policy_loss: -0.003484424203634262
        total_loss: -0.0032065697014331818
        vf_explained_var: 0.02639460563659668
        vf_loss: 19.68372917175293
      agent-3:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.586499035358429
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010159164667129517
        model: {}
        policy_loss: -0.002604150213301182
        total_loss: -0.0018895207904279232
        vf_explained_var: 0.1291939616203308
        vf_loss: 17.468679428100586
      agent-4:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9102068543434143
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013619550736621022
        model: {}
        policy_loss: -0.0037883766926825047
        total_loss: -0.003429180011153221
        vf_explained_var: 0.03833413124084473
        vf_loss: 19.611614227294922
      agent-5:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.680757999420166
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008170738001354039
        model: {}
        policy_loss: -0.00325508089736104
        total_loss: -0.002594702411442995
        vf_explained_var: 0.09968546032905579
        vf_loss: 18.58513641357422
    load_time_ms: 12957.407
    num_steps_sampled: 23616000
    num_steps_trained: 23616000
    sample_time_ms: 90466.253
    update_time_ms: 21.252
  iterations_since_restore: 186
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.417045454545452
    ram_util_percent: 10.073295454545454
  pid: 24061
  policy_reward_max:
    agent-0: 194.99999999999972
    agent-1: 194.99999999999972
    agent-2: 194.99999999999972
    agent-3: 194.99999999999972
    agent-4: 194.99999999999972
    agent-5: 194.99999999999972
  policy_reward_mean:
    agent-0: 155.14999999999998
    agent-1: 155.14999999999998
    agent-2: 155.14999999999998
    agent-3: 155.14999999999998
    agent-4: 155.14999999999998
    agent-5: 155.14999999999998
  policy_reward_min:
    agent-0: 59.999999999999794
    agent-1: 59.999999999999794
    agent-2: 59.999999999999794
    agent-3: 59.999999999999794
    agent-4: 59.999999999999794
    agent-5: 59.999999999999794
  sampler_perf:
    mean_env_wait_ms: 24.401666797121635
    mean_inference_ms: 12.264304604051787
    mean_processing_ms: 50.82297555481236
  time_since_restore: 23949.84611225128
  time_this_iter_s: 123.43136763572693
  time_total_s: 33075.857926130295
  timestamp: 1637047985
  timesteps_since_restore: 17856000
  timesteps_this_iter: 96000
  timesteps_total: 23616000
  training_iteration: 246
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    246 |          33075.9 | 23616000 |    930.9 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 111
    apples_agent-0_mean: 5.87
    apples_agent-0_min: 0
    apples_agent-1_max: 120
    apples_agent-1_mean: 34.38
    apples_agent-1_min: 0
    apples_agent-2_max: 76
    apples_agent-2_mean: 3.58
    apples_agent-2_min: 0
    apples_agent-3_max: 170
    apples_agent-3_mean: 109.61
    apples_agent-3_min: 64
    apples_agent-4_max: 42
    apples_agent-4_mean: 1.7
    apples_agent-4_min: 0
    apples_agent-5_max: 153
    apples_agent-5_mean: 85.48
    apples_agent-5_min: 34
    cleaning_beam_agent-0_max: 541
    cleaning_beam_agent-0_mean: 376.99
    cleaning_beam_agent-0_min: 206
    cleaning_beam_agent-1_max: 576
    cleaning_beam_agent-1_mean: 265.67
    cleaning_beam_agent-1_min: 115
    cleaning_beam_agent-2_max: 660
    cleaning_beam_agent-2_mean: 437.38
    cleaning_beam_agent-2_min: 165
    cleaning_beam_agent-3_max: 56
    cleaning_beam_agent-3_mean: 19.9
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 574
    cleaning_beam_agent-4_mean: 475.78
    cleaning_beam_agent-4_min: 310
    cleaning_beam_agent-5_max: 92
    cleaning_beam_agent-5_mean: 28.14
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-35-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1134.999999999994
  episode_reward_mean: 932.129999999985
  episode_reward_min: 476.0000000000082
  episodes_this_iter: 96
  episodes_total: 23712
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19714.794
    learner:
      agent-0:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0903496742248535
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018129122909158468
        model: {}
        policy_loss: -0.0033833272755146027
        total_loss: -0.0032471641898155212
        vf_explained_var: 0.03867271542549133
        vf_loss: 20.551801681518555
      agent-1:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0839512348175049
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021113662514835596
        model: {}
        policy_loss: -0.004251602105796337
        total_loss: -0.0038675544783473015
        vf_explained_var: -0.04013180732727051
        vf_loss: 22.918006896972656
      agent-2:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9587947726249695
        entropy_coeff: 0.0017600000137463212
        kl: 0.001340002752840519
        model: {}
        policy_loss: -0.0034417016431689262
        total_loss: -0.003096523694694042
        vf_explained_var: 0.051231011748313904
        vf_loss: 20.326570510864258
      agent-3:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5815762281417847
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016915646847337484
        model: {}
        policy_loss: -0.0025117844343185425
        total_loss: -0.0016612857580184937
        vf_explained_var: 0.11559057235717773
        vf_loss: 18.74074363708496
      agent-4:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9050531387329102
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013078558258712292
        model: {}
        policy_loss: -0.0036885293666273355
        total_loss: -0.003203641390427947
        vf_explained_var: 0.02661050856113434
        vf_loss: 20.77779197692871
      agent-5:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6748939752578735
        entropy_coeff: 0.0017600000137463212
        kl: 0.000909386551938951
        model: {}
        policy_loss: -0.003147536888718605
        total_loss: -0.0024535907432436943
        vf_explained_var: 0.13135071098804474
        vf_loss: 18.81757164001465
    load_time_ms: 12950.088
    num_steps_sampled: 23712000
    num_steps_trained: 23712000
    sample_time_ms: 90506.962
    update_time_ms: 21.475
  iterations_since_restore: 187
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.414204545454544
    ram_util_percent: 10.138636363636364
  pid: 24061
  policy_reward_max:
    agent-0: 189.16666666666598
    agent-1: 189.16666666666598
    agent-2: 189.16666666666598
    agent-3: 189.16666666666598
    agent-4: 189.16666666666598
    agent-5: 189.16666666666598
  policy_reward_mean:
    agent-0: 155.35499999999993
    agent-1: 155.35499999999993
    agent-2: 155.35499999999993
    agent-3: 155.35499999999993
    agent-4: 155.35499999999993
    agent-5: 155.35499999999993
  policy_reward_min:
    agent-0: 79.33333333333343
    agent-1: 79.33333333333343
    agent-2: 79.33333333333343
    agent-3: 79.33333333333343
    agent-4: 79.33333333333343
    agent-5: 79.33333333333343
  sampler_perf:
    mean_env_wait_ms: 24.405818696651522
    mean_inference_ms: 12.263307168177244
    mean_processing_ms: 50.82052008662717
  time_since_restore: 24073.334653139114
  time_this_iter_s: 123.48854088783264
  time_total_s: 33199.34646701813
  timestamp: 1637048109
  timesteps_since_restore: 17952000
  timesteps_this_iter: 96000
  timesteps_total: 23712000
  training_iteration: 247
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    247 |          33199.3 | 23712000 |   932.13 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 98
    apples_agent-0_mean: 3.79
    apples_agent-0_min: 0
    apples_agent-1_max: 209
    apples_agent-1_mean: 35.71
    apples_agent-1_min: 0
    apples_agent-2_max: 53
    apples_agent-2_mean: 3.07
    apples_agent-2_min: 0
    apples_agent-3_max: 170
    apples_agent-3_mean: 112.3
    apples_agent-3_min: 62
    apples_agent-4_max: 61
    apples_agent-4_mean: 1.65
    apples_agent-4_min: 0
    apples_agent-5_max: 174
    apples_agent-5_mean: 90.59
    apples_agent-5_min: 44
    cleaning_beam_agent-0_max: 582
    cleaning_beam_agent-0_mean: 375.62
    cleaning_beam_agent-0_min: 200
    cleaning_beam_agent-1_max: 472
    cleaning_beam_agent-1_mean: 257.31
    cleaning_beam_agent-1_min: 109
    cleaning_beam_agent-2_max: 575
    cleaning_beam_agent-2_mean: 430.94
    cleaning_beam_agent-2_min: 239
    cleaning_beam_agent-3_max: 83
    cleaning_beam_agent-3_mean: 23.29
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 558
    cleaning_beam_agent-4_mean: 463.83
    cleaning_beam_agent-4_min: 318
    cleaning_beam_agent-5_max: 76
    cleaning_beam_agent-5_mean: 27.11
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-37-11
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1092.9999999999943
  episode_reward_mean: 924.4999999999859
  episode_reward_min: 419.0000000000105
  episodes_this_iter: 96
  episodes_total: 23808
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19691.999
    learner:
      agent-0:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1034528017044067
        entropy_coeff: 0.0017600000137463212
        kl: 0.002280294429510832
        model: {}
        policy_loss: -0.003526500891894102
        total_loss: -0.003323150333017111
        vf_explained_var: 0.013196781277656555
        vf_loss: 21.454299926757812
      agent-1:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 1.080528736114502
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018025098834186792
        model: {}
        policy_loss: -0.0037501747719943523
        total_loss: -0.003330908715724945
        vf_explained_var: -0.039377033710479736
        vf_loss: 23.209993362426758
      agent-2:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9716987609863281
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017313456628471613
        model: {}
        policy_loss: -0.0036094659008085728
        total_loss: -0.0032626111060380936
        vf_explained_var: 0.05270986258983612
        vf_loss: 20.57045555114746
      agent-3:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5742703080177307
        entropy_coeff: 0.0017600000137463212
        kl: 0.002149320673197508
        model: {}
        policy_loss: -0.00284104747697711
        total_loss: -0.0019947043620049953
        vf_explained_var: 0.14025817811489105
        vf_loss: 18.570575714111328
      agent-4:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9040074944496155
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014951195335015655
        model: {}
        policy_loss: -0.003631272818893194
        total_loss: -0.00309201842173934
        vf_explained_var: 0.021878942847251892
        vf_loss: 21.30306625366211
      agent-5:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6832455396652222
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010571421589702368
        model: {}
        policy_loss: -0.003478830913081765
        total_loss: -0.0026707130018621683
        vf_explained_var: 0.09672847390174866
        vf_loss: 20.106281280517578
    load_time_ms: 12944.164
    num_steps_sampled: 23808000
    num_steps_trained: 23808000
    sample_time_ms: 90460.732
    update_time_ms: 21.385
  iterations_since_restore: 188
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.411494252873565
    ram_util_percent: 10.10229885057471
  pid: 24061
  policy_reward_max:
    agent-0: 182.16666666666683
    agent-1: 182.16666666666683
    agent-2: 182.16666666666683
    agent-3: 182.16666666666683
    agent-4: 182.16666666666683
    agent-5: 182.16666666666683
  policy_reward_mean:
    agent-0: 154.08333333333334
    agent-1: 154.08333333333334
    agent-2: 154.08333333333334
    agent-3: 154.08333333333334
    agent-4: 154.08333333333334
    agent-5: 154.08333333333334
  policy_reward_min:
    agent-0: 69.83333333333334
    agent-1: 69.83333333333334
    agent-2: 69.83333333333334
    agent-3: 69.83333333333334
    agent-4: 69.83333333333334
    agent-5: 69.83333333333334
  sampler_perf:
    mean_env_wait_ms: 24.408095856762426
    mean_inference_ms: 12.262495694630307
    mean_processing_ms: 50.815526542215714
  time_since_restore: 24195.157061100006
  time_this_iter_s: 121.82240796089172
  time_total_s: 33321.16887497902
  timestamp: 1637048231
  timesteps_since_restore: 18048000
  timesteps_this_iter: 96000
  timesteps_total: 23808000
  training_iteration: 248
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    248 |          33321.2 | 23808000 |    924.5 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 55
    apples_agent-0_mean: 3.2
    apples_agent-0_min: 0
    apples_agent-1_max: 109
    apples_agent-1_mean: 33.41
    apples_agent-1_min: 0
    apples_agent-2_max: 46
    apples_agent-2_mean: 2.31
    apples_agent-2_min: 0
    apples_agent-3_max: 159
    apples_agent-3_mean: 114.79
    apples_agent-3_min: 63
    apples_agent-4_max: 20
    apples_agent-4_mean: 0.73
    apples_agent-4_min: 0
    apples_agent-5_max: 151
    apples_agent-5_mean: 94.54
    apples_agent-5_min: 41
    cleaning_beam_agent-0_max: 530
    cleaning_beam_agent-0_mean: 369.63
    cleaning_beam_agent-0_min: 198
    cleaning_beam_agent-1_max: 528
    cleaning_beam_agent-1_mean: 256.94
    cleaning_beam_agent-1_min: 112
    cleaning_beam_agent-2_max: 556
    cleaning_beam_agent-2_mean: 449.02
    cleaning_beam_agent-2_min: 274
    cleaning_beam_agent-3_max: 57
    cleaning_beam_agent-3_mean: 21.02
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 575
    cleaning_beam_agent-4_mean: 476.88
    cleaning_beam_agent-4_min: 364
    cleaning_beam_agent-5_max: 109
    cleaning_beam_agent-5_mean: 25.59
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-39-13
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1096.9999999999889
  episode_reward_mean: 955.1099999999833
  episode_reward_min: 655.000000000006
  episodes_this_iter: 96
  episodes_total: 23904
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19679.998
    learner:
      agent-0:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0995441675186157
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012870965292677283
        model: {}
        policy_loss: -0.003256541909649968
        total_loss: -0.0032935033086687326
        vf_explained_var: 0.029959186911582947
        vf_loss: 18.982358932495117
      agent-1:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0944195985794067
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013088355772197247
        model: {}
        policy_loss: -0.003993298392742872
        total_loss: -0.003723001107573509
        vf_explained_var: -0.07714521884918213
        vf_loss: 21.96477508544922
      agent-2:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9560241103172302
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016162600368261337
        model: {}
        policy_loss: -0.00344068743288517
        total_loss: -0.003137608990073204
        vf_explained_var: -0.023006588220596313
        vf_loss: 19.856826782226562
      agent-3:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5524246096611023
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008271567057818174
        model: {}
        policy_loss: -0.0021318853832781315
        total_loss: -0.0013635782524943352
        vf_explained_var: 0.09336712956428528
        vf_loss: 17.405731201171875
      agent-4:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9024391770362854
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017376242903992534
        model: {}
        policy_loss: -0.0038498553913086653
        total_loss: -0.003548592561855912
        vf_explained_var: 0.03410407900810242
        vf_loss: 18.89554214477539
      agent-5:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.672765851020813
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008720944751985371
        model: {}
        policy_loss: -0.002913041040301323
        total_loss: -0.0022736238315701485
        vf_explained_var: 0.08716706931591034
        vf_loss: 18.234882354736328
    load_time_ms: 12889.776
    num_steps_sampled: 23904000
    num_steps_trained: 23904000
    sample_time_ms: 90416.893
    update_time_ms: 21.491
  iterations_since_restore: 189
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.474137931034482
    ram_util_percent: 10.169540229885056
  pid: 24061
  policy_reward_max:
    agent-0: 182.83333333333286
    agent-1: 182.83333333333286
    agent-2: 182.83333333333286
    agent-3: 182.83333333333286
    agent-4: 182.83333333333286
    agent-5: 182.83333333333286
  policy_reward_mean:
    agent-0: 159.18499999999997
    agent-1: 159.18499999999997
    agent-2: 159.18499999999997
    agent-3: 159.18499999999997
    agent-4: 159.18499999999997
    agent-5: 159.18499999999997
  policy_reward_min:
    agent-0: 109.16666666666669
    agent-1: 109.16666666666669
    agent-2: 109.16666666666669
    agent-3: 109.16666666666669
    agent-4: 109.16666666666669
    agent-5: 109.16666666666669
  sampler_perf:
    mean_env_wait_ms: 24.410774191929644
    mean_inference_ms: 12.261694257150694
    mean_processing_ms: 50.81094907978704
  time_since_restore: 24317.756241321564
  time_this_iter_s: 122.59918022155762
  time_total_s: 33443.76805520058
  timestamp: 1637048353
  timesteps_since_restore: 18144000
  timesteps_this_iter: 96000
  timesteps_total: 23904000
  training_iteration: 249
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    249 |          33443.8 | 23904000 |   955.11 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 51
    apples_agent-0_mean: 4.09
    apples_agent-0_min: 0
    apples_agent-1_max: 159
    apples_agent-1_mean: 36.51
    apples_agent-1_min: 0
    apples_agent-2_max: 146
    apples_agent-2_mean: 6.62
    apples_agent-2_min: 0
    apples_agent-3_max: 157
    apples_agent-3_mean: 106.35
    apples_agent-3_min: 62
    apples_agent-4_max: 73
    apples_agent-4_mean: 2.21
    apples_agent-4_min: 0
    apples_agent-5_max: 167
    apples_agent-5_mean: 89.12
    apples_agent-5_min: 28
    cleaning_beam_agent-0_max: 517
    cleaning_beam_agent-0_mean: 370.65
    cleaning_beam_agent-0_min: 232
    cleaning_beam_agent-1_max: 508
    cleaning_beam_agent-1_mean: 255.49
    cleaning_beam_agent-1_min: 92
    cleaning_beam_agent-2_max: 555
    cleaning_beam_agent-2_mean: 432.19
    cleaning_beam_agent-2_min: 180
    cleaning_beam_agent-3_max: 155
    cleaning_beam_agent-3_mean: 24.48
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 565
    cleaning_beam_agent-4_mean: 472.74
    cleaning_beam_agent-4_min: 260
    cleaning_beam_agent-5_max: 150
    cleaning_beam_agent-5_mean: 30.01
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 5
    fire_beam_agent-2_mean: 0.07
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-41-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1152.0000000000082
  episode_reward_mean: 920.6099999999855
  episode_reward_min: 396.0000000000115
  episodes_this_iter: 96
  episodes_total: 24000
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19685.414
    learner:
      agent-0:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0895475149154663
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012178925098851323
        model: {}
        policy_loss: -0.0031682816334068775
        total_loss: -0.0029066819697618484
        vf_explained_var: 0.029677391052246094
        vf_loss: 21.79201316833496
      agent-1:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0841703414916992
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012418389087542892
        model: {}
        policy_loss: -0.00411940598860383
        total_loss: -0.0036413134075701237
        vf_explained_var: -0.04531210660934448
        vf_loss: 23.862342834472656
      agent-2:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9664283990859985
        entropy_coeff: 0.0017600000137463212
        kl: 0.001346792560070753
        model: {}
        policy_loss: -0.0035147303715348244
        total_loss: -0.0031403370667248964
        vf_explained_var: 0.07451950013637543
        vf_loss: 20.753070831298828
      agent-3:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5791078805923462
        entropy_coeff: 0.0017600000137463212
        kl: 0.00107096740975976
        model: {}
        policy_loss: -0.002740845549851656
        total_loss: -0.0017969852779060602
        vf_explained_var: 0.12140177190303802
        vf_loss: 19.63088607788086
      agent-4:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9054338335990906
        entropy_coeff: 0.0017600000137463212
        kl: 0.001464364118874073
        model: {}
        policy_loss: -0.0037467859219759703
        total_loss: -0.0031786158215254545
        vf_explained_var: 0.03551161289215088
        vf_loss: 21.617351531982422
      agent-5:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6984610557556152
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008537797839380801
        model: {}
        policy_loss: -0.0031567513942718506
        total_loss: -0.002330316696316004
        vf_explained_var: 0.0955122858285904
        vf_loss: 20.55724334716797
    load_time_ms: 12915.119
    num_steps_sampled: 24000000
    num_steps_trained: 24000000
    sample_time_ms: 90271.114
    update_time_ms: 21.106
  iterations_since_restore: 190
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.429142857142857
    ram_util_percent: 10.14457142857143
  pid: 24061
  policy_reward_max:
    agent-0: 191.99999999999972
    agent-1: 191.99999999999972
    agent-2: 191.99999999999972
    agent-3: 191.99999999999972
    agent-4: 191.99999999999972
    agent-5: 191.99999999999972
  policy_reward_mean:
    agent-0: 153.43499999999997
    agent-1: 153.43499999999997
    agent-2: 153.43499999999997
    agent-3: 153.43499999999997
    agent-4: 153.43499999999997
    agent-5: 153.43499999999997
  policy_reward_min:
    agent-0: 65.99999999999997
    agent-1: 65.99999999999997
    agent-2: 65.99999999999997
    agent-3: 65.99999999999997
    agent-4: 65.99999999999997
    agent-5: 65.99999999999997
  sampler_perf:
    mean_env_wait_ms: 24.413487265331923
    mean_inference_ms: 12.2607905157803
    mean_processing_ms: 50.80588504979959
  time_since_restore: 24440.147299528122
  time_this_iter_s: 122.39105820655823
  time_total_s: 33566.159113407135
  timestamp: 1637048476
  timesteps_since_restore: 18240000
  timesteps_this_iter: 96000
  timesteps_total: 24000000
  training_iteration: 250
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    250 |          33566.2 | 24000000 |   920.61 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 114
    apples_agent-0_mean: 5.03
    apples_agent-0_min: 0
    apples_agent-1_max: 120
    apples_agent-1_mean: 33.34
    apples_agent-1_min: 0
    apples_agent-2_max: 107
    apples_agent-2_mean: 5.6
    apples_agent-2_min: 0
    apples_agent-3_max: 150
    apples_agent-3_mean: 106.03
    apples_agent-3_min: 47
    apples_agent-4_max: 40
    apples_agent-4_mean: 1.28
    apples_agent-4_min: 0
    apples_agent-5_max: 179
    apples_agent-5_mean: 91.06
    apples_agent-5_min: 35
    cleaning_beam_agent-0_max: 543
    cleaning_beam_agent-0_mean: 364.2
    cleaning_beam_agent-0_min: 209
    cleaning_beam_agent-1_max: 551
    cleaning_beam_agent-1_mean: 259.62
    cleaning_beam_agent-1_min: 84
    cleaning_beam_agent-2_max: 592
    cleaning_beam_agent-2_mean: 443.39
    cleaning_beam_agent-2_min: 219
    cleaning_beam_agent-3_max: 68
    cleaning_beam_agent-3_mean: 22.4
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 627
    cleaning_beam_agent-4_mean: 479.15
    cleaning_beam_agent-4_min: 329
    cleaning_beam_agent-5_max: 153
    cleaning_beam_agent-5_mean: 27.13
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-43-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1098.9999999999854
  episode_reward_mean: 930.0699999999865
  episode_reward_min: 421.0000000000104
  episodes_this_iter: 96
  episodes_total: 24096
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19683.174
    learner:
      agent-0:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0871602296829224
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012354102218523622
        model: {}
        policy_loss: -0.0034112122375518084
        total_loss: -0.0032768086530268192
        vf_explained_var: 0.017532706260681152
        vf_loss: 20.478063583374023
      agent-1:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0816328525543213
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013354915427044034
        model: {}
        policy_loss: -0.003740626387298107
        total_loss: -0.003336657304316759
        vf_explained_var: -0.08332252502441406
        vf_loss: 23.07640266418457
      agent-2:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9525178670883179
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019421449396759272
        model: {}
        policy_loss: -0.003620784031227231
        total_loss: -0.0033769537694752216
        vf_explained_var: 0.06572103500366211
        vf_loss: 19.20262336730957
      agent-3:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5572952032089233
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011223889887332916
        model: {}
        policy_loss: -0.002417466603219509
        total_loss: -0.00161430099979043
        vf_explained_var: 0.12401002645492554
        vf_loss: 17.840063095092773
      agent-4:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8914252519607544
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015195260057225823
        model: {}
        policy_loss: -0.0038871164433658123
        total_loss: -0.0034393491223454475
        vf_explained_var: 0.016312137246131897
        vf_loss: 20.166763305664062
      agent-5:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6884192824363708
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006357863312587142
        model: {}
        policy_loss: -0.0030809310264885426
        total_loss: -0.002341547980904579
        vf_explained_var: 0.07994738221168518
        vf_loss: 19.510025024414062
    load_time_ms: 12923.137
    num_steps_sampled: 24096000
    num_steps_trained: 24096000
    sample_time_ms: 90300.479
    update_time_ms: 21.138
  iterations_since_restore: 191
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.45085714285714
    ram_util_percent: 10.091428571428569
  pid: 24061
  policy_reward_max:
    agent-0: 183.1666666666669
    agent-1: 183.1666666666669
    agent-2: 183.1666666666669
    agent-3: 183.1666666666669
    agent-4: 183.1666666666669
    agent-5: 183.1666666666669
  policy_reward_mean:
    agent-0: 155.01166666666663
    agent-1: 155.01166666666663
    agent-2: 155.01166666666663
    agent-3: 155.01166666666663
    agent-4: 155.01166666666663
    agent-5: 155.01166666666663
  policy_reward_min:
    agent-0: 70.16666666666656
    agent-1: 70.16666666666656
    agent-2: 70.16666666666656
    agent-3: 70.16666666666656
    agent-4: 70.16666666666656
    agent-5: 70.16666666666656
  sampler_perf:
    mean_env_wait_ms: 24.416718582148473
    mean_inference_ms: 12.260178319112251
    mean_processing_ms: 50.802345072320506
  time_since_restore: 24563.134308576584
  time_this_iter_s: 122.98700904846191
  time_total_s: 33689.1461224556
  timestamp: 1637048599
  timesteps_since_restore: 18336000
  timesteps_this_iter: 96000
  timesteps_total: 24096000
  training_iteration: 251
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    251 |          33689.1 | 24096000 |   930.07 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 34
    apples_agent-0_mean: 2.15
    apples_agent-0_min: 0
    apples_agent-1_max: 126
    apples_agent-1_mean: 32.74
    apples_agent-1_min: 0
    apples_agent-2_max: 73
    apples_agent-2_mean: 4.68
    apples_agent-2_min: 0
    apples_agent-3_max: 172
    apples_agent-3_mean: 109.94
    apples_agent-3_min: 48
    apples_agent-4_max: 71
    apples_agent-4_mean: 1.33
    apples_agent-4_min: 0
    apples_agent-5_max: 156
    apples_agent-5_mean: 87.76
    apples_agent-5_min: 31
    cleaning_beam_agent-0_max: 561
    cleaning_beam_agent-0_mean: 397.62
    cleaning_beam_agent-0_min: 244
    cleaning_beam_agent-1_max: 560
    cleaning_beam_agent-1_mean: 273.49
    cleaning_beam_agent-1_min: 112
    cleaning_beam_agent-2_max: 608
    cleaning_beam_agent-2_mean: 434.74
    cleaning_beam_agent-2_min: 204
    cleaning_beam_agent-3_max: 62
    cleaning_beam_agent-3_mean: 21.55
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 577
    cleaning_beam_agent-4_mean: 475.15
    cleaning_beam_agent-4_min: 294
    cleaning_beam_agent-5_max: 93
    cleaning_beam_agent-5_mean: 25.59
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-45-22
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1142.0000000000002
  episode_reward_mean: 950.389999999985
  episode_reward_min: 440.9999999999992
  episodes_this_iter: 96
  episodes_total: 24192
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19685.848
    learner:
      agent-0:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0844831466674805
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014116597594693303
        model: {}
        policy_loss: -0.003332278458401561
        total_loss: -0.003028829814866185
        vf_explained_var: -0.0063326358795166016
        vf_loss: 22.12136459350586
      agent-1:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0841972827911377
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017890669405460358
        model: {}
        policy_loss: -0.003942991606891155
        total_loss: -0.0035297321155667305
        vf_explained_var: -0.023501217365264893
        vf_loss: 23.214473724365234
      agent-2:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9631370306015015
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016938671469688416
        model: {}
        policy_loss: -0.0034447750076651573
        total_loss: -0.00308303814381361
        vf_explained_var: 0.05942021310329437
        vf_loss: 20.568599700927734
      agent-3:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5450968742370605
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011024255072697997
        model: {}
        policy_loss: -0.0020735347643494606
        total_loss: -0.0011431481689214706
        vf_explained_var: 0.12139686942100525
        vf_loss: 18.897598266601562
      agent-4:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8969507217407227
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015297154895961285
        model: {}
        policy_loss: -0.003801940940320492
        total_loss: -0.0032687108032405376
        vf_explained_var: 0.03773249685764313
        vf_loss: 21.118648529052734
      agent-5:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6694625616073608
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008086534799076617
        model: {}
        policy_loss: -0.0029833223670721054
        total_loss: -0.00215289369225502
        vf_explained_var: 0.10778622329235077
        vf_loss: 20.086820602416992
    load_time_ms: 12915.831
    num_steps_sampled: 24192000
    num_steps_trained: 24192000
    sample_time_ms: 90298.02
    update_time_ms: 21.436
  iterations_since_restore: 192
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.378409090909093
    ram_util_percent: 10.127840909090908
  pid: 24061
  policy_reward_max:
    agent-0: 190.33333333333306
    agent-1: 190.33333333333306
    agent-2: 190.33333333333306
    agent-3: 190.33333333333306
    agent-4: 190.33333333333306
    agent-5: 190.33333333333306
  policy_reward_mean:
    agent-0: 158.39833333333326
    agent-1: 158.39833333333326
    agent-2: 158.39833333333326
    agent-3: 158.39833333333326
    agent-4: 158.39833333333326
    agent-5: 158.39833333333326
  policy_reward_min:
    agent-0: 73.49999999999987
    agent-1: 73.49999999999987
    agent-2: 73.49999999999987
    agent-3: 73.49999999999987
    agent-4: 73.49999999999987
    agent-5: 73.49999999999987
  sampler_perf:
    mean_env_wait_ms: 24.420366216691445
    mean_inference_ms: 12.259180518971958
    mean_processing_ms: 50.79794765859701
  time_since_restore: 24686.001986026764
  time_this_iter_s: 122.86767745018005
  time_total_s: 33812.01379990578
  timestamp: 1637048722
  timesteps_since_restore: 18432000
  timesteps_this_iter: 96000
  timesteps_total: 24192000
  training_iteration: 252
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    252 |            33812 | 24192000 |   950.39 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 57
    apples_agent-0_mean: 4.82
    apples_agent-0_min: 0
    apples_agent-1_max: 137
    apples_agent-1_mean: 36.21
    apples_agent-1_min: 0
    apples_agent-2_max: 57
    apples_agent-2_mean: 2.87
    apples_agent-2_min: 0
    apples_agent-3_max: 158
    apples_agent-3_mean: 106.65
    apples_agent-3_min: 62
    apples_agent-4_max: 42
    apples_agent-4_mean: 0.68
    apples_agent-4_min: 0
    apples_agent-5_max: 147
    apples_agent-5_mean: 89.52
    apples_agent-5_min: 31
    cleaning_beam_agent-0_max: 511
    cleaning_beam_agent-0_mean: 388.34
    cleaning_beam_agent-0_min: 228
    cleaning_beam_agent-1_max: 465
    cleaning_beam_agent-1_mean: 243.41
    cleaning_beam_agent-1_min: 117
    cleaning_beam_agent-2_max: 587
    cleaning_beam_agent-2_mean: 442.76
    cleaning_beam_agent-2_min: 239
    cleaning_beam_agent-3_max: 83
    cleaning_beam_agent-3_mean: 18.55
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 610
    cleaning_beam_agent-4_mean: 470.18
    cleaning_beam_agent-4_min: 309
    cleaning_beam_agent-5_max: 98
    cleaning_beam_agent-5_mean: 25.85
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-47-25
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1123.9999999999823
  episode_reward_mean: 951.069999999984
  episode_reward_min: 608.9999999999957
  episodes_this_iter: 96
  episodes_total: 24288
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19677.903
    learner:
      agent-0:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0838441848754883
        entropy_coeff: 0.0017600000137463212
        kl: 0.001784023828804493
        model: {}
        policy_loss: -0.003286294173449278
        total_loss: -0.003207446075975895
        vf_explained_var: 0.029998943209648132
        vf_loss: 19.864126205444336
      agent-1:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 1.092259168624878
        entropy_coeff: 0.0017600000137463212
        kl: 0.00118710205424577
        model: {}
        policy_loss: -0.00419232901185751
        total_loss: -0.003882914548739791
        vf_explained_var: -0.043989866971969604
        vf_loss: 22.3178653717041
      agent-2:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.960640549659729
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015445828903466463
        model: {}
        policy_loss: -0.0034009870141744614
        total_loss: -0.0030977781862020493
        vf_explained_var: 0.018545672297477722
        vf_loss: 19.939287185668945
      agent-3:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5377683043479919
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011063541751354933
        model: {}
        policy_loss: -0.0022750988136976957
        total_loss: -0.0013753329403698444
        vf_explained_var: 0.07600192725658417
        vf_loss: 18.462371826171875
      agent-4:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8978228569030762
        entropy_coeff: 0.0017600000137463212
        kl: 0.00144419155549258
        model: {}
        policy_loss: -0.003966701216995716
        total_loss: -0.003560067154467106
        vf_explained_var: 0.019192874431610107
        vf_loss: 19.868059158325195
      agent-5:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.661225438117981
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008278556633740664
        model: {}
        policy_loss: -0.0029757816810160875
        total_loss: -0.0021878299303352833
        vf_explained_var: 0.06097260117530823
        vf_loss: 19.517086029052734
    load_time_ms: 12940.233
    num_steps_sampled: 24288000
    num_steps_trained: 24288000
    sample_time_ms: 90249.772
    update_time_ms: 21.394
  iterations_since_restore: 193
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.393142857142859
    ram_util_percent: 10.08342857142857
  pid: 24061
  policy_reward_max:
    agent-0: 187.3333333333338
    agent-1: 187.3333333333338
    agent-2: 187.3333333333338
    agent-3: 187.3333333333338
    agent-4: 187.3333333333338
    agent-5: 187.3333333333338
  policy_reward_mean:
    agent-0: 158.51166666666657
    agent-1: 158.51166666666657
    agent-2: 158.51166666666657
    agent-3: 158.51166666666657
    agent-4: 158.51166666666657
    agent-5: 158.51166666666657
  policy_reward_min:
    agent-0: 101.50000000000057
    agent-1: 101.50000000000057
    agent-2: 101.50000000000057
    agent-3: 101.50000000000057
    agent-4: 101.50000000000057
    agent-5: 101.50000000000057
  sampler_perf:
    mean_env_wait_ms: 24.42317647688718
    mean_inference_ms: 12.258282844653639
    mean_processing_ms: 50.79461544449553
  time_since_restore: 24808.679569721222
  time_this_iter_s: 122.67758369445801
  time_total_s: 33934.691383600235
  timestamp: 1637048845
  timesteps_since_restore: 18528000
  timesteps_this_iter: 96000
  timesteps_total: 24288000
  training_iteration: 253
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    253 |          33934.7 | 24288000 |   951.07 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 33
    apples_agent-0_mean: 3.32
    apples_agent-0_min: 0
    apples_agent-1_max: 121
    apples_agent-1_mean: 33.3
    apples_agent-1_min: 0
    apples_agent-2_max: 58
    apples_agent-2_mean: 2.26
    apples_agent-2_min: 0
    apples_agent-3_max: 294
    apples_agent-3_mean: 112.51
    apples_agent-3_min: 55
    apples_agent-4_max: 35
    apples_agent-4_mean: 1.16
    apples_agent-4_min: 0
    apples_agent-5_max: 227
    apples_agent-5_mean: 90.85
    apples_agent-5_min: 33
    cleaning_beam_agent-0_max: 567
    cleaning_beam_agent-0_mean: 405.83
    cleaning_beam_agent-0_min: 183
    cleaning_beam_agent-1_max: 518
    cleaning_beam_agent-1_mean: 254.32
    cleaning_beam_agent-1_min: 79
    cleaning_beam_agent-2_max: 597
    cleaning_beam_agent-2_mean: 461.6
    cleaning_beam_agent-2_min: 193
    cleaning_beam_agent-3_max: 96
    cleaning_beam_agent-3_mean: 21.96
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 580
    cleaning_beam_agent-4_mean: 465.02
    cleaning_beam_agent-4_min: 309
    cleaning_beam_agent-5_max: 135
    cleaning_beam_agent-5_mean: 26.57
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-49-28
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1147.000000000001
  episode_reward_mean: 949.1499999999868
  episode_reward_min: 492.00000000001063
  episodes_this_iter: 96
  episodes_total: 24384
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19671.689
    learner:
      agent-0:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0591682195663452
        entropy_coeff: 0.0017600000137463212
        kl: 0.00155963737051934
        model: {}
        policy_loss: -0.003159113461151719
        total_loss: -0.002928436268121004
        vf_explained_var: 0.012785807251930237
        vf_loss: 20.948143005371094
      agent-1:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0772541761398315
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009249003487639129
        model: {}
        policy_loss: -0.0038497280329465866
        total_loss: -0.003350652754306793
        vf_explained_var: -0.07514691352844238
        vf_loss: 23.95041847229004
      agent-2:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9331824779510498
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015711798332631588
        model: {}
        policy_loss: -0.0032782633788883686
        total_loss: -0.0028255251236259937
        vf_explained_var: 0.0060064345598220825
        vf_loss: 20.951404571533203
      agent-3:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5434169173240662
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011738562025129795
        model: {}
        policy_loss: -0.0025067636743187904
        total_loss: -0.0015825415030121803
        vf_explained_var: 0.10336895287036896
        vf_loss: 18.80636978149414
      agent-4:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8999203443527222
        entropy_coeff: 0.0017600000137463212
        kl: 0.001803903840482235
        model: {}
        policy_loss: -0.0038980101235210896
        total_loss: -0.0034770111087709665
        vf_explained_var: 0.059518009424209595
        vf_loss: 20.04857063293457
      agent-5:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6655251979827881
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008865422569215298
        model: {}
        policy_loss: -0.003015466034412384
        total_loss: -0.002225774573162198
        vf_explained_var: 0.09973835945129395
        vf_loss: 19.610198974609375
    load_time_ms: 12932.107
    num_steps_sampled: 24384000
    num_steps_trained: 24384000
    sample_time_ms: 90262.8
    update_time_ms: 21.0
  iterations_since_restore: 194
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.452000000000002
    ram_util_percent: 10.139999999999997
  pid: 24061
  policy_reward_max:
    agent-0: 191.16666666666643
    agent-1: 191.16666666666643
    agent-2: 191.16666666666643
    agent-3: 191.16666666666643
    agent-4: 191.16666666666643
    agent-5: 191.16666666666643
  policy_reward_mean:
    agent-0: 158.19166666666663
    agent-1: 158.19166666666663
    agent-2: 158.19166666666663
    agent-3: 158.19166666666663
    agent-4: 158.19166666666663
    agent-5: 158.19166666666663
  policy_reward_min:
    agent-0: 81.99999999999983
    agent-1: 81.99999999999983
    agent-2: 81.99999999999983
    agent-3: 81.99999999999983
    agent-4: 81.99999999999983
    agent-5: 81.99999999999983
  sampler_perf:
    mean_env_wait_ms: 24.427282305067255
    mean_inference_ms: 12.257536143983488
    mean_processing_ms: 50.79217642257225
  time_since_restore: 24932.017072677612
  time_this_iter_s: 123.33750295639038
  time_total_s: 34058.028886556625
  timestamp: 1637048968
  timesteps_since_restore: 18624000
  timesteps_this_iter: 96000
  timesteps_total: 24384000
  training_iteration: 254
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    254 |            34058 | 24384000 |   949.15 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 39
    apples_agent-0_mean: 3.47
    apples_agent-0_min: 0
    apples_agent-1_max: 109
    apples_agent-1_mean: 36.01
    apples_agent-1_min: 0
    apples_agent-2_max: 125
    apples_agent-2_mean: 3.21
    apples_agent-2_min: 0
    apples_agent-3_max: 161
    apples_agent-3_mean: 106.04
    apples_agent-3_min: 21
    apples_agent-4_max: 25
    apples_agent-4_mean: 0.67
    apples_agent-4_min: 0
    apples_agent-5_max: 164
    apples_agent-5_mean: 91.6
    apples_agent-5_min: 48
    cleaning_beam_agent-0_max: 536
    cleaning_beam_agent-0_mean: 402.06
    cleaning_beam_agent-0_min: 170
    cleaning_beam_agent-1_max: 498
    cleaning_beam_agent-1_mean: 256.45
    cleaning_beam_agent-1_min: 99
    cleaning_beam_agent-2_max: 602
    cleaning_beam_agent-2_mean: 468.43
    cleaning_beam_agent-2_min: 209
    cleaning_beam_agent-3_max: 118
    cleaning_beam_agent-3_mean: 18.71
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 603
    cleaning_beam_agent-4_mean: 480.6
    cleaning_beam_agent-4_min: 278
    cleaning_beam_agent-5_max: 79
    cleaning_beam_agent-5_mean: 23.75
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-51-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1168.9999999999964
  episode_reward_mean: 977.4299999999848
  episode_reward_min: 495.00000000000784
  episodes_this_iter: 96
  episodes_total: 24480
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19684.214
    learner:
      agent-0:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0719932317733765
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012021177681162953
        model: {}
        policy_loss: -0.003112053731456399
        total_loss: -0.0029684975743293762
        vf_explained_var: -0.01657089591026306
        vf_loss: 20.302661895751953
      agent-1:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 1.086637258529663
        entropy_coeff: 0.0017600000137463212
        kl: 0.001260500866919756
        model: {}
        policy_loss: -0.004101986065506935
        total_loss: -0.0037543042562901974
        vf_explained_var: -0.07946979999542236
        vf_loss: 22.601621627807617
      agent-2:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.935474157333374
        entropy_coeff: 0.0017600000137463212
        kl: 0.001342680538073182
        model: {}
        policy_loss: -0.003191583789885044
        total_loss: -0.0028712782077491283
        vf_explained_var: 0.009869411587715149
        vf_loss: 19.66744613647461
      agent-3:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5162253379821777
        entropy_coeff: 0.0017600000137463212
        kl: 0.001046608085744083
        model: {}
        policy_loss: -0.002104101236909628
        total_loss: -0.001242103986442089
        vf_explained_var: 0.09007678925991058
        vf_loss: 17.70557975769043
      agent-4:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.882720410823822
        entropy_coeff: 0.0017600000137463212
        kl: 0.001960647525265813
        model: {}
        policy_loss: -0.004104970954358578
        total_loss: -0.003689022269099951
        vf_explained_var: 0.005136817693710327
        vf_loss: 19.695323944091797
      agent-5:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6562840938568115
        entropy_coeff: 0.0017600000137463212
        kl: 0.00113291060552001
        model: {}
        policy_loss: -0.003001130186021328
        total_loss: -0.002123677171766758
        vf_explained_var: 0.014877274632453918
        vf_loss: 20.32512092590332
    load_time_ms: 12915.758
    num_steps_sampled: 24480000
    num_steps_trained: 24480000
    sample_time_ms: 90220.541
    update_time_ms: 21.715
  iterations_since_restore: 195
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.329378531073447
    ram_util_percent: 10.157062146892654
  pid: 24061
  policy_reward_max:
    agent-0: 194.8333333333332
    agent-1: 194.8333333333332
    agent-2: 194.8333333333332
    agent-3: 194.8333333333332
    agent-4: 194.8333333333332
    agent-5: 194.8333333333332
  policy_reward_mean:
    agent-0: 162.90499999999986
    agent-1: 162.90499999999986
    agent-2: 162.90499999999986
    agent-3: 162.90499999999986
    agent-4: 162.90499999999986
    agent-5: 162.90499999999986
  policy_reward_min:
    agent-0: 82.50000000000034
    agent-1: 82.50000000000034
    agent-2: 82.50000000000034
    agent-3: 82.50000000000034
    agent-4: 82.50000000000034
    agent-5: 82.50000000000034
  sampler_perf:
    mean_env_wait_ms: 24.431256729967703
    mean_inference_ms: 12.256809174613865
    mean_processing_ms: 50.78802696190398
  time_since_restore: 25055.775791168213
  time_this_iter_s: 123.75871849060059
  time_total_s: 34181.787605047226
  timestamp: 1637049092
  timesteps_since_restore: 18720000
  timesteps_this_iter: 96000
  timesteps_total: 24480000
  training_iteration: 255
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    255 |          34181.8 | 24480000 |   977.43 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 28
    apples_agent-0_mean: 1.6
    apples_agent-0_min: 0
    apples_agent-1_max: 139
    apples_agent-1_mean: 34.59
    apples_agent-1_min: 0
    apples_agent-2_max: 119
    apples_agent-2_mean: 3.26
    apples_agent-2_min: 0
    apples_agent-3_max: 176
    apples_agent-3_mean: 113.43
    apples_agent-3_min: 68
    apples_agent-4_max: 25
    apples_agent-4_mean: 0.55
    apples_agent-4_min: 0
    apples_agent-5_max: 150
    apples_agent-5_mean: 89.04
    apples_agent-5_min: 49
    cleaning_beam_agent-0_max: 578
    cleaning_beam_agent-0_mean: 400.03
    cleaning_beam_agent-0_min: 163
    cleaning_beam_agent-1_max: 452
    cleaning_beam_agent-1_mean: 264.08
    cleaning_beam_agent-1_min: 97
    cleaning_beam_agent-2_max: 569
    cleaning_beam_agent-2_mean: 461.25
    cleaning_beam_agent-2_min: 192
    cleaning_beam_agent-3_max: 67
    cleaning_beam_agent-3_mean: 20.35
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 605
    cleaning_beam_agent-4_mean: 477.47
    cleaning_beam_agent-4_min: 345
    cleaning_beam_agent-5_max: 188
    cleaning_beam_agent-5_mean: 25.49
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-53-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1117.9999999999845
  episode_reward_mean: 964.6799999999857
  episode_reward_min: 595.9999999999992
  episodes_this_iter: 96
  episodes_total: 24576
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19685.905
    learner:
      agent-0:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0796984434127808
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018542623147368431
        model: {}
        policy_loss: -0.003606896847486496
        total_loss: -0.0035470889415591955
        vf_explained_var: 0.031849443912506104
        vf_loss: 19.600770950317383
      agent-1:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0864068269729614
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011701756156980991
        model: {}
        policy_loss: -0.003963406663388014
        total_loss: -0.003679468994960189
        vf_explained_var: -0.03978770971298218
        vf_loss: 21.96013641357422
      agent-2:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9443005323410034
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015343247214332223
        model: {}
        policy_loss: -0.003374092048034072
        total_loss: -0.0030763254035264254
        vf_explained_var: 0.03515718877315521
        vf_loss: 19.597389221191406
      agent-3:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5556086897850037
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008879122324287891
        model: {}
        policy_loss: -0.0023389053530991077
        total_loss: -0.0014690173557028174
        vf_explained_var: 0.08171038329601288
        vf_loss: 18.47760581970215
      agent-4:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8788158297538757
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022450308315455914
        model: {}
        policy_loss: -0.0037745805457234383
        total_loss: -0.0033546574413776398
        vf_explained_var: 0.0167982280254364
        vf_loss: 19.666362762451172
      agent-5:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6550562977790833
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007479861960746348
        model: {}
        policy_loss: -0.0029522613622248173
        total_loss: -0.002206390956416726
        vf_explained_var: 0.07856312394142151
        vf_loss: 18.987707138061523
    load_time_ms: 13051.348
    num_steps_sampled: 24576000
    num_steps_trained: 24576000
    sample_time_ms: 90245.95
    update_time_ms: 22.038
  iterations_since_restore: 196
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.316853932584273
    ram_util_percent: 10.155056179775281
  pid: 24061
  policy_reward_max:
    agent-0: 186.3333333333331
    agent-1: 186.3333333333331
    agent-2: 186.3333333333331
    agent-3: 186.3333333333331
    agent-4: 186.3333333333331
    agent-5: 186.3333333333331
  policy_reward_mean:
    agent-0: 160.7799999999999
    agent-1: 160.7799999999999
    agent-2: 160.7799999999999
    agent-3: 160.7799999999999
    agent-4: 160.7799999999999
    agent-5: 160.7799999999999
  policy_reward_min:
    agent-0: 99.33333333333331
    agent-1: 99.33333333333331
    agent-2: 99.33333333333331
    agent-3: 99.33333333333331
    agent-4: 99.33333333333331
    agent-5: 99.33333333333331
  sampler_perf:
    mean_env_wait_ms: 24.435311557086887
    mean_inference_ms: 12.255901542436154
    mean_processing_ms: 50.78528490832741
  time_since_restore: 25180.85406255722
  time_this_iter_s: 125.07827138900757
  time_total_s: 34306.86587643623
  timestamp: 1637049218
  timesteps_since_restore: 18816000
  timesteps_this_iter: 96000
  timesteps_total: 24576000
  training_iteration: 256
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    256 |          34306.9 | 24576000 |   964.68 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 68
    apples_agent-0_mean: 2.76
    apples_agent-0_min: 0
    apples_agent-1_max: 110
    apples_agent-1_mean: 33.66
    apples_agent-1_min: 0
    apples_agent-2_max: 48
    apples_agent-2_mean: 2.21
    apples_agent-2_min: 0
    apples_agent-3_max: 192
    apples_agent-3_mean: 117.95
    apples_agent-3_min: 39
    apples_agent-4_max: 38
    apples_agent-4_mean: 0.96
    apples_agent-4_min: 0
    apples_agent-5_max: 141
    apples_agent-5_mean: 90.22
    apples_agent-5_min: 47
    cleaning_beam_agent-0_max: 586
    cleaning_beam_agent-0_mean: 392.78
    cleaning_beam_agent-0_min: 224
    cleaning_beam_agent-1_max: 458
    cleaning_beam_agent-1_mean: 261.58
    cleaning_beam_agent-1_min: 97
    cleaning_beam_agent-2_max: 585
    cleaning_beam_agent-2_mean: 448.58
    cleaning_beam_agent-2_min: 241
    cleaning_beam_agent-3_max: 77
    cleaning_beam_agent-3_mean: 20.0
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 601
    cleaning_beam_agent-4_mean: 497.35
    cleaning_beam_agent-4_min: 347
    cleaning_beam_agent-5_max: 91
    cleaning_beam_agent-5_mean: 21.92
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-55-41
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1125.0000000000064
  episode_reward_mean: 984.399999999986
  episode_reward_min: 462.0000000000044
  episodes_this_iter: 96
  episodes_total: 24672
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19670.848
    learner:
      agent-0:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0760557651519775
        entropy_coeff: 0.0017600000137463212
        kl: 0.001349921803921461
        model: {}
        policy_loss: -0.003402421250939369
        total_loss: -0.003292932640761137
        vf_explained_var: 0.010715380311012268
        vf_loss: 20.033483505249023
      agent-1:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0901710987091064
        entropy_coeff: 0.0017600000137463212
        kl: 0.001130789634771645
        model: {}
        policy_loss: -0.00371649581938982
        total_loss: -0.0034120194613933563
        vf_explained_var: -0.03282366693019867
        vf_loss: 22.231773376464844
      agent-2:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9647947549819946
        entropy_coeff: 0.0017600000137463212
        kl: 0.00135619449429214
        model: {}
        policy_loss: -0.003145380411297083
        total_loss: -0.0028493909630924463
        vf_explained_var: 0.016940802335739136
        vf_loss: 19.940271377563477
      agent-3:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5263055562973022
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007538235513493419
        model: {}
        policy_loss: -0.002232183702290058
        total_loss: -0.0013118023052811623
        vf_explained_var: 0.0725933164358139
        vf_loss: 18.46678924560547
      agent-4:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8684033155441284
        entropy_coeff: 0.0017600000137463212
        kl: 0.001676029758527875
        model: {}
        policy_loss: -0.0035190791822969913
        total_loss: -0.0030817356891930103
        vf_explained_var: 0.018969282507896423
        vf_loss: 19.657329559326172
      agent-5:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6455754637718201
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005996320396661758
        model: {}
        policy_loss: -0.0027751752641052008
        total_loss: -0.0018828993197530508
        vf_explained_var: 0.03159180283546448
        vf_loss: 20.284902572631836
    load_time_ms: 13074.777
    num_steps_sampled: 24672000
    num_steps_trained: 24672000
    sample_time_ms: 90216.542
    update_time_ms: 22.126
  iterations_since_restore: 197
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.369491525423731
    ram_util_percent: 10.0864406779661
  pid: 24061
  policy_reward_max:
    agent-0: 187.49999999999983
    agent-1: 187.49999999999983
    agent-2: 187.49999999999983
    agent-3: 187.49999999999983
    agent-4: 187.49999999999983
    agent-5: 187.49999999999983
  policy_reward_mean:
    agent-0: 164.06666666666652
    agent-1: 164.06666666666652
    agent-2: 164.06666666666652
    agent-3: 164.06666666666652
    agent-4: 164.06666666666652
    agent-5: 164.06666666666652
  policy_reward_min:
    agent-0: 77.00000000000011
    agent-1: 77.00000000000011
    agent-2: 77.00000000000011
    agent-3: 77.00000000000011
    agent-4: 77.00000000000011
    agent-5: 77.00000000000011
  sampler_perf:
    mean_env_wait_ms: 24.438928039276526
    mean_inference_ms: 12.25497211646566
    mean_processing_ms: 50.78302603401704
  time_since_restore: 25304.138835430145
  time_this_iter_s: 123.2847728729248
  time_total_s: 34430.15064930916
  timestamp: 1637049341
  timesteps_since_restore: 18912000
  timesteps_this_iter: 96000
  timesteps_total: 24672000
  training_iteration: 257
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    257 |          34430.2 | 24672000 |    984.4 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 150
    apples_agent-0_mean: 3.72
    apples_agent-0_min: 0
    apples_agent-1_max: 186
    apples_agent-1_mean: 37.46
    apples_agent-1_min: 0
    apples_agent-2_max: 144
    apples_agent-2_mean: 5.38
    apples_agent-2_min: 0
    apples_agent-3_max: 183
    apples_agent-3_mean: 114.13
    apples_agent-3_min: 0
    apples_agent-4_max: 44
    apples_agent-4_mean: 1.55
    apples_agent-4_min: 0
    apples_agent-5_max: 149
    apples_agent-5_mean: 86.42
    apples_agent-5_min: 37
    cleaning_beam_agent-0_max: 538
    cleaning_beam_agent-0_mean: 392.82
    cleaning_beam_agent-0_min: 213
    cleaning_beam_agent-1_max: 450
    cleaning_beam_agent-1_mean: 258.96
    cleaning_beam_agent-1_min: 102
    cleaning_beam_agent-2_max: 634
    cleaning_beam_agent-2_mean: 432.28
    cleaning_beam_agent-2_min: 157
    cleaning_beam_agent-3_max: 60
    cleaning_beam_agent-3_mean: 22.61
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 623
    cleaning_beam_agent-4_mean: 492.92
    cleaning_beam_agent-4_min: 347
    cleaning_beam_agent-5_max: 110
    cleaning_beam_agent-5_mean: 26.4
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-57-44
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1137.9999999999877
  episode_reward_mean: 951.2999999999877
  episode_reward_min: 618.9999999999989
  episodes_this_iter: 96
  episodes_total: 24768
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19641.566
    learner:
      agent-0:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 1.074706792831421
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013094577006995678
        model: {}
        policy_loss: -0.0032978355884552
        total_loss: -0.0032193425577133894
        vf_explained_var: 0.0320989191532135
        vf_loss: 19.699777603149414
      agent-1:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 1.08384370803833
        entropy_coeff: 0.0017600000137463212
        kl: 0.001459031947888434
        model: {}
        policy_loss: -0.004251949023455381
        total_loss: -0.003958016633987427
        vf_explained_var: -0.04192376136779785
        vf_loss: 22.014951705932617
      agent-2:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9602674245834351
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013296124525368214
        model: {}
        policy_loss: -0.003272349014878273
        total_loss: -0.002953170333057642
        vf_explained_var: 0.025904610753059387
        vf_loss: 20.092493057250977
      agent-3:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.547264575958252
        entropy_coeff: 0.0017600000137463212
        kl: 0.000733607797883451
        model: {}
        policy_loss: -0.0022745467722415924
        total_loss: -0.0013665916630998254
        vf_explained_var: 0.07874538004398346
        vf_loss: 18.71142578125
      agent-4:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8733834028244019
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014518988318741322
        model: {}
        policy_loss: -0.003843761282041669
        total_loss: -0.0033366496209055185
        vf_explained_var: -0.007763311266899109
        vf_loss: 20.442642211914062
      agent-5:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6402575969696045
        entropy_coeff: 0.0017600000137463212
        kl: 0.001258643576875329
        model: {}
        policy_loss: -0.0028454018756747246
        total_loss: -0.002071165479719639
        vf_explained_var: 0.08171503245830536
        vf_loss: 19.010934829711914
    load_time_ms: 13204.625
    num_steps_sampled: 24768000
    num_steps_trained: 24768000
    sample_time_ms: 90226.239
    update_time_ms: 22.637
  iterations_since_restore: 198
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.466285714285716
    ram_util_percent: 10.148000000000001
  pid: 24061
  policy_reward_max:
    agent-0: 189.66666666666595
    agent-1: 189.66666666666595
    agent-2: 189.66666666666595
    agent-3: 189.66666666666595
    agent-4: 189.66666666666595
    agent-5: 189.66666666666595
  policy_reward_mean:
    agent-0: 158.54999999999998
    agent-1: 158.54999999999998
    agent-2: 158.54999999999998
    agent-3: 158.54999999999998
    agent-4: 158.54999999999998
    agent-5: 158.54999999999998
  policy_reward_min:
    agent-0: 103.16666666666723
    agent-1: 103.16666666666723
    agent-2: 103.16666666666723
    agent-3: 103.16666666666723
    agent-4: 103.16666666666723
    agent-5: 103.16666666666723
  sampler_perf:
    mean_env_wait_ms: 24.44200984818705
    mean_inference_ms: 12.254059097936766
    mean_processing_ms: 50.77999034161411
  time_since_restore: 25427.094592809677
  time_this_iter_s: 122.95575737953186
  time_total_s: 34553.10640668869
  timestamp: 1637049464
  timesteps_since_restore: 19008000
  timesteps_this_iter: 96000
  timesteps_total: 24768000
  training_iteration: 258
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    258 |          34553.1 | 24768000 |    951.3 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 30
    apples_agent-0_mean: 3.36
    apples_agent-0_min: 0
    apples_agent-1_max: 103
    apples_agent-1_mean: 29.52
    apples_agent-1_min: 0
    apples_agent-2_max: 28
    apples_agent-2_mean: 1.74
    apples_agent-2_min: 0
    apples_agent-3_max: 178
    apples_agent-3_mean: 114.91
    apples_agent-3_min: 64
    apples_agent-4_max: 41
    apples_agent-4_mean: 1.22
    apples_agent-4_min: 0
    apples_agent-5_max: 175
    apples_agent-5_mean: 85.58
    apples_agent-5_min: 48
    cleaning_beam_agent-0_max: 542
    cleaning_beam_agent-0_mean: 382.45
    cleaning_beam_agent-0_min: 217
    cleaning_beam_agent-1_max: 485
    cleaning_beam_agent-1_mean: 261.05
    cleaning_beam_agent-1_min: 104
    cleaning_beam_agent-2_max: 634
    cleaning_beam_agent-2_mean: 441.26
    cleaning_beam_agent-2_min: 278
    cleaning_beam_agent-3_max: 59
    cleaning_beam_agent-3_mean: 19.12
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 585
    cleaning_beam_agent-4_mean: 479.17
    cleaning_beam_agent-4_min: 298
    cleaning_beam_agent-5_max: 93
    cleaning_beam_agent-5_mean: 26.76
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-59-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1139.9999999999898
  episode_reward_mean: 966.7099999999855
  episode_reward_min: 524.0000000000073
  episodes_this_iter: 96
  episodes_total: 24864
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19630.137
    learner:
      agent-0:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 1.088701844215393
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013768284115940332
        model: {}
        policy_loss: -0.0033388955052942038
        total_loss: -0.003295343369245529
        vf_explained_var: 0.025845453143119812
        vf_loss: 19.596698760986328
      agent-1:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 1.098227620124817
        entropy_coeff: 0.0017600000137463212
        kl: 0.001668607466854155
        model: {}
        policy_loss: -0.003953572828322649
        total_loss: -0.0036698519252240658
        vf_explained_var: -0.056333690881729126
        vf_loss: 22.16599464416504
      agent-2:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9598326086997986
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015464433236047626
        model: {}
        policy_loss: -0.00343491043895483
        total_loss: -0.003167492337524891
        vf_explained_var: 0.02159211039543152
        vf_loss: 19.567241668701172
      agent-3:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5280753970146179
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016979756765067577
        model: {}
        policy_loss: -0.002486160956323147
        total_loss: -0.0015692873857915401
        vf_explained_var: 0.06372436881065369
        vf_loss: 18.462881088256836
      agent-4:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8716613054275513
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012917242711409926
        model: {}
        policy_loss: -0.0037617809139192104
        total_loss: -0.003343123011291027
        vf_explained_var: 0.024497583508491516
        vf_loss: 19.527843475341797
      agent-5:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6513237953186035
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007810539100319147
        model: {}
        policy_loss: -0.002993178553879261
        total_loss: -0.002240261761471629
        vf_explained_var: 0.06760059297084808
        vf_loss: 18.992473602294922
    load_time_ms: 13232.38
    num_steps_sampled: 24864000
    num_steps_trained: 24864000
    sample_time_ms: 90236.291
    update_time_ms: 22.594
  iterations_since_restore: 199
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.518285714285716
    ram_util_percent: 10.169714285714281
  pid: 24061
  policy_reward_max:
    agent-0: 189.9999999999996
    agent-1: 189.9999999999996
    agent-2: 189.9999999999996
    agent-3: 189.9999999999996
    agent-4: 189.9999999999996
    agent-5: 189.9999999999996
  policy_reward_mean:
    agent-0: 161.1183333333333
    agent-1: 161.1183333333333
    agent-2: 161.1183333333333
    agent-3: 161.1183333333333
    agent-4: 161.1183333333333
    agent-5: 161.1183333333333
  policy_reward_min:
    agent-0: 87.33333333333351
    agent-1: 87.33333333333351
    agent-2: 87.33333333333351
    agent-3: 87.33333333333351
    agent-4: 87.33333333333351
    agent-5: 87.33333333333351
  sampler_perf:
    mean_env_wait_ms: 24.44547219765519
    mean_inference_ms: 12.25317695982353
    mean_processing_ms: 50.778034251531636
  time_since_restore: 25549.967982530594
  time_this_iter_s: 122.87338972091675
  time_total_s: 34675.97979640961
  timestamp: 1637049587
  timesteps_since_restore: 19104000
  timesteps_this_iter: 96000
  timesteps_total: 24864000
  training_iteration: 259
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    259 |            34676 | 24864000 |   966.71 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 57
    apples_agent-0_mean: 2.64
    apples_agent-0_min: 0
    apples_agent-1_max: 117
    apples_agent-1_mean: 31.25
    apples_agent-1_min: 0
    apples_agent-2_max: 52
    apples_agent-2_mean: 2.95
    apples_agent-2_min: 0
    apples_agent-3_max: 165
    apples_agent-3_mean: 111.55
    apples_agent-3_min: 61
    apples_agent-4_max: 53
    apples_agent-4_mean: 1.16
    apples_agent-4_min: 0
    apples_agent-5_max: 133
    apples_agent-5_mean: 84.06
    apples_agent-5_min: 38
    cleaning_beam_agent-0_max: 574
    cleaning_beam_agent-0_mean: 397.39
    cleaning_beam_agent-0_min: 186
    cleaning_beam_agent-1_max: 510
    cleaning_beam_agent-1_mean: 268.74
    cleaning_beam_agent-1_min: 81
    cleaning_beam_agent-2_max: 571
    cleaning_beam_agent-2_mean: 438.02
    cleaning_beam_agent-2_min: 244
    cleaning_beam_agent-3_max: 68
    cleaning_beam_agent-3_mean: 22.01
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 620
    cleaning_beam_agent-4_mean: 490.59
    cleaning_beam_agent-4_min: 375
    cleaning_beam_agent-5_max: 140
    cleaning_beam_agent-5_mean: 24.67
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.07
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-01-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1128.9999999999889
  episode_reward_mean: 964.3199999999871
  episode_reward_min: 550.0000000000119
  episodes_this_iter: 96
  episodes_total: 24960
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19636.527
    learner:
      agent-0:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0876294374465942
        entropy_coeff: 0.0017600000137463212
        kl: 0.002129112370312214
        model: {}
        policy_loss: -0.0035739662125706673
        total_loss: -0.0033320575021207333
        vf_explained_var: 0.03332236409187317
        vf_loss: 21.561389923095703
      agent-1:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 1.092176079750061
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015364494174718857
        model: {}
        policy_loss: -0.004217957146465778
        total_loss: -0.0037962226197123528
        vf_explained_var: -0.025083795189857483
        vf_loss: 23.43963050842285
      agent-2:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9586756825447083
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012602476635947824
        model: {}
        policy_loss: -0.0033901180140674114
        total_loss: -0.002977072261273861
        vf_explained_var: 0.060495927929878235
        vf_loss: 21.003164291381836
      agent-3:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5438542366027832
        entropy_coeff: 0.0017600000137463212
        kl: 0.000892146781552583
        model: {}
        policy_loss: -0.0027037085965275764
        total_loss: -0.0016977572813630104
        vf_explained_var: 0.11392976343631744
        vf_loss: 19.6313419342041
      agent-4:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8790092468261719
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012069290969520807
        model: {}
        policy_loss: -0.0035377638414502144
        total_loss: -0.002956436015665531
        vf_explained_var: 0.040222421288490295
        vf_loss: 21.283836364746094
      agent-5:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6402404308319092
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007795715937390924
        model: {}
        policy_loss: -0.0028760796412825584
        total_loss: -0.0019735777750611305
        vf_explained_var: 0.09912343323230743
        vf_loss: 20.29327392578125
    load_time_ms: 13222.014
    num_steps_sampled: 24960000
    num_steps_trained: 24960000
    sample_time_ms: 90371.27
    update_time_ms: 22.676
  iterations_since_restore: 200
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.964204545454548
    ram_util_percent: 10.213636363636363
  pid: 24061
  policy_reward_max:
    agent-0: 188.16666666666603
    agent-1: 188.16666666666603
    agent-2: 188.16666666666603
    agent-3: 188.16666666666603
    agent-4: 188.16666666666603
    agent-5: 188.16666666666603
  policy_reward_mean:
    agent-0: 160.7199999999999
    agent-1: 160.7199999999999
    agent-2: 160.7199999999999
    agent-3: 160.7199999999999
    agent-4: 160.7199999999999
    agent-5: 160.7199999999999
  policy_reward_min:
    agent-0: 91.66666666666659
    agent-1: 91.66666666666659
    agent-2: 91.66666666666659
    agent-3: 91.66666666666659
    agent-4: 91.66666666666659
    agent-5: 91.66666666666659
  sampler_perf:
    mean_env_wait_ms: 24.44944826645993
    mean_inference_ms: 12.25293316061397
    mean_processing_ms: 50.77561846617209
  time_since_restore: 25673.659309625626
  time_this_iter_s: 123.69132709503174
  time_total_s: 34799.67112350464
  timestamp: 1637049711
  timesteps_since_restore: 19200000
  timesteps_this_iter: 96000
  timesteps_total: 24960000
  training_iteration: 260
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    260 |          34799.7 | 24960000 |   964.32 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 35
    apples_agent-0_mean: 2.91
    apples_agent-0_min: 0
    apples_agent-1_max: 136
    apples_agent-1_mean: 34.56
    apples_agent-1_min: 0
    apples_agent-2_max: 42
    apples_agent-2_mean: 1.96
    apples_agent-2_min: 0
    apples_agent-3_max: 171
    apples_agent-3_mean: 112.47
    apples_agent-3_min: 56
    apples_agent-4_max: 27
    apples_agent-4_mean: 0.87
    apples_agent-4_min: 0
    apples_agent-5_max: 155
    apples_agent-5_mean: 86.51
    apples_agent-5_min: 36
    cleaning_beam_agent-0_max: 490
    cleaning_beam_agent-0_mean: 371.71
    cleaning_beam_agent-0_min: 231
    cleaning_beam_agent-1_max: 590
    cleaning_beam_agent-1_mean: 258.5
    cleaning_beam_agent-1_min: 92
    cleaning_beam_agent-2_max: 589
    cleaning_beam_agent-2_mean: 449.12
    cleaning_beam_agent-2_min: 272
    cleaning_beam_agent-3_max: 93
    cleaning_beam_agent-3_mean: 20.0
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 635
    cleaning_beam_agent-4_mean: 475.48
    cleaning_beam_agent-4_min: 367
    cleaning_beam_agent-5_max: 106
    cleaning_beam_agent-5_mean: 26.35
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-03-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1135.0000000000095
  episode_reward_mean: 962.7599999999854
  episode_reward_min: 614.9999999999952
  episodes_this_iter: 96
  episodes_total: 25056
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19636.589
    learner:
      agent-0:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1041052341461182
        entropy_coeff: 0.0017600000137463212
        kl: 0.00199833232909441
        model: {}
        policy_loss: -0.0038253972306847572
        total_loss: -0.003690943121910095
        vf_explained_var: 0.01856870949268341
        vf_loss: 20.77679443359375
      agent-1:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 1.091310739517212
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015593136195093393
        model: {}
        policy_loss: -0.004034563899040222
        total_loss: -0.003693884937092662
        vf_explained_var: -0.04820120334625244
        vf_loss: 22.61385726928711
      agent-2:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9602080583572388
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020730867981910706
        model: {}
        policy_loss: -0.0033688643015921116
        total_loss: -0.003052744083106518
        vf_explained_var: 0.033996596932411194
        vf_loss: 20.06085968017578
      agent-3:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5277085900306702
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016865413635969162
        model: {}
        policy_loss: -0.002441203221678734
        total_loss: -0.0014581438153982162
        vf_explained_var: 0.07907497882843018
        vf_loss: 19.118221282958984
      agent-4:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8781313896179199
        entropy_coeff: 0.0017600000137463212
        kl: 0.001764040207490325
        model: {}
        policy_loss: -0.003618343733251095
        total_loss: -0.0030923429876565933
        vf_explained_var: 0.005101978778839111
        vf_loss: 20.715085983276367
      agent-5:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6543495059013367
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014142724685370922
        model: {}
        policy_loss: -0.003010496962815523
        total_loss: -0.0022639594972133636
        vf_explained_var: 0.10242326557636261
        vf_loss: 18.9819278717041
    load_time_ms: 13284.55
    num_steps_sampled: 25056000
    num_steps_trained: 25056000
    sample_time_ms: 90393.971
    update_time_ms: 22.34
  iterations_since_restore: 201
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.430681818181819
    ram_util_percent: 10.152272727272726
  pid: 24061
  policy_reward_max:
    agent-0: 189.16666666666643
    agent-1: 189.16666666666643
    agent-2: 189.16666666666643
    agent-3: 189.16666666666643
    agent-4: 189.16666666666643
    agent-5: 189.16666666666643
  policy_reward_mean:
    agent-0: 160.45999999999992
    agent-1: 160.45999999999992
    agent-2: 160.45999999999992
    agent-3: 160.45999999999992
    agent-4: 160.45999999999992
    agent-5: 160.45999999999992
  policy_reward_min:
    agent-0: 102.50000000000057
    agent-1: 102.50000000000057
    agent-2: 102.50000000000057
    agent-3: 102.50000000000057
    agent-4: 102.50000000000057
    agent-5: 102.50000000000057
  sampler_perf:
    mean_env_wait_ms: 24.452731434930442
    mean_inference_ms: 12.25239493526272
    mean_processing_ms: 50.772734372724635
  time_since_restore: 25797.508233070374
  time_this_iter_s: 123.84892344474792
  time_total_s: 34923.52004694939
  timestamp: 1637049835
  timesteps_since_restore: 19296000
  timesteps_this_iter: 96000
  timesteps_total: 25056000
  training_iteration: 261
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    261 |          34923.5 | 25056000 |   962.76 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 69
    apples_agent-0_mean: 4.64
    apples_agent-0_min: 0
    apples_agent-1_max: 189
    apples_agent-1_mean: 38.44
    apples_agent-1_min: 0
    apples_agent-2_max: 94
    apples_agent-2_mean: 3.06
    apples_agent-2_min: 0
    apples_agent-3_max: 169
    apples_agent-3_mean: 109.61
    apples_agent-3_min: 55
    apples_agent-4_max: 28
    apples_agent-4_mean: 1.06
    apples_agent-4_min: 0
    apples_agent-5_max: 168
    apples_agent-5_mean: 86.58
    apples_agent-5_min: 39
    cleaning_beam_agent-0_max: 549
    cleaning_beam_agent-0_mean: 358.49
    cleaning_beam_agent-0_min: 186
    cleaning_beam_agent-1_max: 520
    cleaning_beam_agent-1_mean: 260.38
    cleaning_beam_agent-1_min: 95
    cleaning_beam_agent-2_max: 621
    cleaning_beam_agent-2_mean: 435.35
    cleaning_beam_agent-2_min: 246
    cleaning_beam_agent-3_max: 79
    cleaning_beam_agent-3_mean: 19.92
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 596
    cleaning_beam_agent-4_mean: 492.67
    cleaning_beam_agent-4_min: 379
    cleaning_beam_agent-5_max: 68
    cleaning_beam_agent-5_mean: 26.03
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-05-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1145.9999999999977
  episode_reward_mean: 964.7599999999845
  episode_reward_min: 669.9999999999834
  episodes_this_iter: 96
  episodes_total: 25152
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19614.029
    learner:
      agent-0:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1019855737686157
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015922016464173794
        model: {}
        policy_loss: -0.0033220937475562096
        total_loss: -0.003110247664153576
        vf_explained_var: 0.003412023186683655
        vf_loss: 21.513397216796875
      agent-1:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1000453233718872
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017790567362681031
        model: {}
        policy_loss: -0.00418129051104188
        total_loss: -0.003882075659930706
        vf_explained_var: -0.019823521375656128
        vf_loss: 22.352951049804688
      agent-2:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9653314352035522
        entropy_coeff: 0.0017600000137463212
        kl: 0.001517122145742178
        model: {}
        policy_loss: -0.003396702464669943
        total_loss: -0.002992052584886551
        vf_explained_var: 0.002820342779159546
        vf_loss: 21.036359786987305
      agent-3:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5185732245445251
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010600071400403976
        model: {}
        policy_loss: -0.002230379730463028
        total_loss: -0.0012627667747437954
        vf_explained_var: 0.10075384378433228
        vf_loss: 18.803001403808594
      agent-4:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8721767663955688
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012957300059497356
        model: {}
        policy_loss: -0.003701592329889536
        total_loss: -0.0032152554485946894
        vf_explained_var: 0.04239727556705475
        vf_loss: 20.213680267333984
      agent-5:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.651419997215271
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012472266098484397
        model: {}
        policy_loss: -0.0032378146424889565
        total_loss: -0.0024626492522656918
        vf_explained_var: 0.10730867087841034
        vf_loss: 19.216636657714844
    load_time_ms: 13306.073
    num_steps_sampled: 25152000
    num_steps_trained: 25152000
    sample_time_ms: 90430.316
    update_time_ms: 21.984
  iterations_since_restore: 202
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.396022727272726
    ram_util_percent: 10.140340909090908
  pid: 24061
  policy_reward_max:
    agent-0: 190.99999999999943
    agent-1: 190.99999999999943
    agent-2: 190.99999999999943
    agent-3: 190.99999999999943
    agent-4: 190.99999999999943
    agent-5: 190.99999999999943
  policy_reward_mean:
    agent-0: 160.79333333333318
    agent-1: 160.79333333333318
    agent-2: 160.79333333333318
    agent-3: 160.79333333333318
    agent-4: 160.79333333333318
    agent-5: 160.79333333333318
  policy_reward_min:
    agent-0: 111.66666666666713
    agent-1: 111.66666666666713
    agent-2: 111.66666666666713
    agent-3: 111.66666666666713
    agent-4: 111.66666666666713
    agent-5: 111.66666666666713
  sampler_perf:
    mean_env_wait_ms: 24.45521604477007
    mean_inference_ms: 12.251744073613938
    mean_processing_ms: 50.76966135700623
  time_since_restore: 25920.792647600174
  time_this_iter_s: 123.28441452980042
  time_total_s: 35046.80446147919
  timestamp: 1637049959
  timesteps_since_restore: 19392000
  timesteps_this_iter: 96000
  timesteps_total: 25152000
  training_iteration: 262
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    262 |          35046.8 | 25152000 |   964.76 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 68
    apples_agent-0_mean: 5.03
    apples_agent-0_min: 0
    apples_agent-1_max: 204
    apples_agent-1_mean: 28.93
    apples_agent-1_min: 0
    apples_agent-2_max: 73
    apples_agent-2_mean: 4.84
    apples_agent-2_min: 0
    apples_agent-3_max: 171
    apples_agent-3_mean: 109.17
    apples_agent-3_min: 42
    apples_agent-4_max: 42
    apples_agent-4_mean: 1.14
    apples_agent-4_min: 0
    apples_agent-5_max: 122
    apples_agent-5_mean: 79.21
    apples_agent-5_min: 38
    cleaning_beam_agent-0_max: 555
    cleaning_beam_agent-0_mean: 365.34
    cleaning_beam_agent-0_min: 234
    cleaning_beam_agent-1_max: 549
    cleaning_beam_agent-1_mean: 289.16
    cleaning_beam_agent-1_min: 109
    cleaning_beam_agent-2_max: 628
    cleaning_beam_agent-2_mean: 424.67
    cleaning_beam_agent-2_min: 159
    cleaning_beam_agent-3_max: 69
    cleaning_beam_agent-3_mean: 23.39
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 607
    cleaning_beam_agent-4_mean: 487.15
    cleaning_beam_agent-4_min: 383
    cleaning_beam_agent-5_max: 85
    cleaning_beam_agent-5_mean: 22.3
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-08-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1172.9999999999982
  episode_reward_mean: 938.9199999999868
  episode_reward_min: 620.9999999999941
  episodes_this_iter: 96
  episodes_total: 25248
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19610.915
    learner:
      agent-0:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 1.110979437828064
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013454575091600418
        model: {}
        policy_loss: -0.0031731100752949715
        total_loss: -0.0031348750926554203
        vf_explained_var: 0.006054520606994629
        vf_loss: 19.935590744018555
      agent-1:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0931744575500488
        entropy_coeff: 0.0017600000137463212
        kl: 0.00137077528052032
        model: {}
        policy_loss: -0.003680076450109482
        total_loss: -0.0034240847453475
        vf_explained_var: -0.0708482563495636
        vf_loss: 21.799785614013672
      agent-2:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9634842872619629
        entropy_coeff: 0.0017600000137463212
        kl: 0.001954265171661973
        model: {}
        policy_loss: -0.0038787727244198322
        total_loss: -0.0036649475805461407
        vf_explained_var: 0.04128545522689819
        vf_loss: 19.09556770324707
      agent-3:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5424421429634094
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014391473960131407
        model: {}
        policy_loss: -0.002657035831362009
        total_loss: -0.0017832578159868717
        vf_explained_var: 0.07857164740562439
        vf_loss: 18.28478240966797
      agent-4:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8845785856246948
        entropy_coeff: 0.0017600000137463212
        kl: 0.001931898295879364
        model: {}
        policy_loss: -0.003944954834878445
        total_loss: -0.0035015949979424477
        vf_explained_var: -0.007356226444244385
        vf_loss: 20.002166748046875
      agent-5:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6429433822631836
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008391664596274495
        model: {}
        policy_loss: -0.002735065296292305
        total_loss: -0.0020335298031568527
        vf_explained_var: 0.08265137672424316
        vf_loss: 18.331161499023438
    load_time_ms: 13392.031
    num_steps_sampled: 25248000
    num_steps_trained: 25248000
    sample_time_ms: 90506.134
    update_time_ms: 21.868
  iterations_since_restore: 203
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.307865168539326
    ram_util_percent: 10.08483146067416
  pid: 24061
  policy_reward_max:
    agent-0: 195.4999999999997
    agent-1: 195.4999999999997
    agent-2: 195.4999999999997
    agent-3: 195.4999999999997
    agent-4: 195.4999999999997
    agent-5: 195.4999999999997
  policy_reward_mean:
    agent-0: 156.48666666666668
    agent-1: 156.48666666666668
    agent-2: 156.48666666666668
    agent-3: 156.48666666666668
    agent-4: 156.48666666666668
    agent-5: 156.48666666666668
  policy_reward_min:
    agent-0: 103.50000000000028
    agent-1: 103.50000000000028
    agent-2: 103.50000000000028
    agent-3: 103.50000000000028
    agent-4: 103.50000000000028
    agent-5: 103.50000000000028
  sampler_perf:
    mean_env_wait_ms: 24.458761836239514
    mean_inference_ms: 12.251130271836722
    mean_processing_ms: 50.76711229033068
  time_since_restore: 26045.07651257515
  time_this_iter_s: 124.28386497497559
  time_total_s: 35171.08832645416
  timestamp: 1637050083
  timesteps_since_restore: 19488000
  timesteps_this_iter: 96000
  timesteps_total: 25248000
  training_iteration: 263
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    263 |          35171.1 | 25248000 |   938.92 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 73
    apples_agent-0_mean: 3.21
    apples_agent-0_min: 0
    apples_agent-1_max: 101
    apples_agent-1_mean: 37.46
    apples_agent-1_min: 0
    apples_agent-2_max: 69
    apples_agent-2_mean: 1.67
    apples_agent-2_min: 0
    apples_agent-3_max: 175
    apples_agent-3_mean: 113.83
    apples_agent-3_min: 0
    apples_agent-4_max: 18
    apples_agent-4_mean: 0.6
    apples_agent-4_min: 0
    apples_agent-5_max: 130
    apples_agent-5_mean: 84.3
    apples_agent-5_min: 41
    cleaning_beam_agent-0_max: 555
    cleaning_beam_agent-0_mean: 385.25
    cleaning_beam_agent-0_min: 279
    cleaning_beam_agent-1_max: 457
    cleaning_beam_agent-1_mean: 260.24
    cleaning_beam_agent-1_min: 113
    cleaning_beam_agent-2_max: 575
    cleaning_beam_agent-2_mean: 436.22
    cleaning_beam_agent-2_min: 285
    cleaning_beam_agent-3_max: 60
    cleaning_beam_agent-3_mean: 19.69
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 614
    cleaning_beam_agent-4_mean: 479.58
    cleaning_beam_agent-4_min: 354
    cleaning_beam_agent-5_max: 166
    cleaning_beam_agent-5_mean: 22.73
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-10-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1157.0000000000002
  episode_reward_mean: 987.8699999999877
  episode_reward_min: 595.0000000000026
  episodes_this_iter: 96
  episodes_total: 25344
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19607.052
    learner:
      agent-0:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0972847938537598
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013868511887267232
        model: {}
        policy_loss: -0.003401857567951083
        total_loss: -0.0032771334517747164
        vf_explained_var: -0.000369146466255188
        vf_loss: 20.559457778930664
      agent-1:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1020814180374146
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013466529780998826
        model: {}
        policy_loss: -0.004020144231617451
        total_loss: -0.0037423972971737385
        vf_explained_var: -0.032294511795043945
        vf_loss: 22.174091339111328
      agent-2:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9613803625106812
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018186623929068446
        model: {}
        policy_loss: -0.003341071540489793
        total_loss: -0.0030554162804037333
        vf_explained_var: 0.008047491312026978
        vf_loss: 19.776853561401367
      agent-3:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5079037547111511
        entropy_coeff: 0.0017600000137463212
        kl: 0.001491114147938788
        model: {}
        policy_loss: -0.002357920166105032
        total_loss: -0.0014289780519902706
        vf_explained_var: 0.09083779156208038
        vf_loss: 18.228515625
      agent-4:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8758838176727295
        entropy_coeff: 0.0017600000137463212
        kl: 0.002460915595293045
        model: {}
        policy_loss: -0.004199092276394367
        total_loss: -0.003758711740374565
        vf_explained_var: 0.031135261058807373
        vf_loss: 19.81934928894043
      agent-5:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.608218789100647
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008532641804777086
        model: {}
        policy_loss: -0.0025188059080392122
        total_loss: -0.0016894226428121328
        vf_explained_var: 0.06879648566246033
        vf_loss: 18.998525619506836
    load_time_ms: 13402.839
    num_steps_sampled: 25344000
    num_steps_trained: 25344000
    sample_time_ms: 90446.94
    update_time_ms: 22.236
  iterations_since_restore: 204
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.489080459770115
    ram_util_percent: 10.147701149425284
  pid: 24061
  policy_reward_max:
    agent-0: 192.83333333333294
    agent-1: 192.83333333333294
    agent-2: 192.83333333333294
    agent-3: 192.83333333333294
    agent-4: 192.83333333333294
    agent-5: 192.83333333333294
  policy_reward_mean:
    agent-0: 164.64499999999987
    agent-1: 164.64499999999987
    agent-2: 164.64499999999987
    agent-3: 164.64499999999987
    agent-4: 164.64499999999987
    agent-5: 164.64499999999987
  policy_reward_min:
    agent-0: 99.16666666666708
    agent-1: 99.16666666666708
    agent-2: 99.16666666666708
    agent-3: 99.16666666666708
    agent-4: 99.16666666666708
    agent-5: 99.16666666666708
  sampler_perf:
    mean_env_wait_ms: 24.461638849199673
    mean_inference_ms: 12.250419986778226
    mean_processing_ms: 50.76432485482302
  time_since_restore: 26167.869313955307
  time_this_iter_s: 122.79280138015747
  time_total_s: 35293.88112783432
  timestamp: 1637050206
  timesteps_since_restore: 19584000
  timesteps_this_iter: 96000
  timesteps_total: 25344000
  training_iteration: 264
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    264 |          35293.9 | 25344000 |   987.87 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 39
    apples_agent-0_mean: 2.96
    apples_agent-0_min: 0
    apples_agent-1_max: 129
    apples_agent-1_mean: 29.01
    apples_agent-1_min: 0
    apples_agent-2_max: 40
    apples_agent-2_mean: 1.66
    apples_agent-2_min: 0
    apples_agent-3_max: 181
    apples_agent-3_mean: 112.43
    apples_agent-3_min: 0
    apples_agent-4_max: 155
    apples_agent-4_mean: 2.79
    apples_agent-4_min: 0
    apples_agent-5_max: 137
    apples_agent-5_mean: 83.22
    apples_agent-5_min: 45
    cleaning_beam_agent-0_max: 590
    cleaning_beam_agent-0_mean: 385.98
    cleaning_beam_agent-0_min: 234
    cleaning_beam_agent-1_max: 495
    cleaning_beam_agent-1_mean: 277.09
    cleaning_beam_agent-1_min: 127
    cleaning_beam_agent-2_max: 572
    cleaning_beam_agent-2_mean: 428.89
    cleaning_beam_agent-2_min: 213
    cleaning_beam_agent-3_max: 81
    cleaning_beam_agent-3_mean: 18.85
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 605
    cleaning_beam_agent-4_mean: 479.85
    cleaning_beam_agent-4_min: 301
    cleaning_beam_agent-5_max: 110
    cleaning_beam_agent-5_mean: 23.76
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-12-10
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1189.0000000000134
  episode_reward_mean: 975.6899999999862
  episode_reward_min: 510.00000000001256
  episodes_this_iter: 96
  episodes_total: 25440
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19596.508
    learner:
      agent-0:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0920910835266113
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015007603215053678
        model: {}
        policy_loss: -0.0034476567525416613
        total_loss: -0.0033218504395335913
        vf_explained_var: 0.00973537564277649
        vf_loss: 20.47884750366211
      agent-1:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1036596298217773
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012901911977678537
        model: {}
        policy_loss: -0.0038904990069568157
        total_loss: -0.0036803954280912876
        vf_explained_var: -0.02720654010772705
        vf_loss: 21.525476455688477
      agent-2:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9652440547943115
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012712019961327314
        model: {}
        policy_loss: -0.002920650877058506
        total_loss: -0.0025947908870875835
        vf_explained_var: 0.004757925868034363
        vf_loss: 20.246862411499023
      agent-3:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4942864179611206
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010641815606504679
        model: {}
        policy_loss: -0.002273243386298418
        total_loss: -0.0013159643858671188
        vf_explained_var: 0.08582800626754761
        vf_loss: 18.272232055664062
      agent-4:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8727929592132568
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018617925234138966
        model: {}
        policy_loss: -0.0038736416026949883
        total_loss: -0.0034090077970176935
        vf_explained_var: 0.01900625228881836
        vf_loss: 20.007518768310547
      agent-5:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6372486352920532
        entropy_coeff: 0.0017600000137463212
        kl: 0.00097837473731488
        model: {}
        policy_loss: -0.002828773111104965
        total_loss: -0.0020802393555641174
        vf_explained_var: 0.08730527758598328
        vf_loss: 18.700891494750977
    load_time_ms: 13438.511
    num_steps_sampled: 25440000
    num_steps_trained: 25440000
    sample_time_ms: 90369.158
    update_time_ms: 20.83
  iterations_since_restore: 205
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.569886363636364
    ram_util_percent: 10.145454545454545
  pid: 24061
  policy_reward_max:
    agent-0: 198.16666666666623
    agent-1: 198.16666666666623
    agent-2: 198.16666666666623
    agent-3: 198.16666666666623
    agent-4: 198.16666666666623
    agent-5: 198.16666666666623
  policy_reward_mean:
    agent-0: 162.61499999999992
    agent-1: 162.61499999999992
    agent-2: 162.61499999999992
    agent-3: 162.61499999999992
    agent-4: 162.61499999999992
    agent-5: 162.61499999999992
  policy_reward_min:
    agent-0: 85.00000000000011
    agent-1: 85.00000000000011
    agent-2: 85.00000000000011
    agent-3: 85.00000000000011
    agent-4: 85.00000000000011
    agent-5: 85.00000000000011
  sampler_perf:
    mean_env_wait_ms: 24.46458947272713
    mean_inference_ms: 12.249722321758405
    mean_processing_ms: 50.76153061477004
  time_since_restore: 26291.10820889473
  time_this_iter_s: 123.23889493942261
  time_total_s: 35417.12002277374
  timestamp: 1637050330
  timesteps_since_restore: 19680000
  timesteps_this_iter: 96000
  timesteps_total: 25440000
  training_iteration: 265
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    265 |          35417.1 | 25440000 |   975.69 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 78
    apples_agent-0_mean: 2.9
    apples_agent-0_min: 0
    apples_agent-1_max: 129
    apples_agent-1_mean: 34.53
    apples_agent-1_min: 0
    apples_agent-2_max: 81
    apples_agent-2_mean: 3.34
    apples_agent-2_min: 0
    apples_agent-3_max: 219
    apples_agent-3_mean: 120.71
    apples_agent-3_min: 73
    apples_agent-4_max: 18
    apples_agent-4_mean: 0.38
    apples_agent-4_min: 0
    apples_agent-5_max: 143
    apples_agent-5_mean: 80.35
    apples_agent-5_min: 29
    cleaning_beam_agent-0_max: 555
    cleaning_beam_agent-0_mean: 394.25
    cleaning_beam_agent-0_min: 225
    cleaning_beam_agent-1_max: 553
    cleaning_beam_agent-1_mean: 285.07
    cleaning_beam_agent-1_min: 91
    cleaning_beam_agent-2_max: 538
    cleaning_beam_agent-2_mean: 415.88
    cleaning_beam_agent-2_min: 187
    cleaning_beam_agent-3_max: 55
    cleaning_beam_agent-3_mean: 19.64
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 587
    cleaning_beam_agent-4_mean: 485.4
    cleaning_beam_agent-4_min: 373
    cleaning_beam_agent-5_max: 74
    cleaning_beam_agent-5_mean: 22.45
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-14-13
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1186.0000000000111
  episode_reward_mean: 998.2299999999882
  episode_reward_min: 687.9999999999922
  episodes_this_iter: 96
  episodes_total: 25536
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19619.346
    learner:
      agent-0:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 1.083875298500061
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016778132412582636
        model: {}
        policy_loss: -0.003350880928337574
        total_loss: -0.003138118190690875
        vf_explained_var: -0.0173642635345459
        vf_loss: 21.20383644104004
      agent-1:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0751643180847168
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016346487682312727
        model: {}
        policy_loss: -0.004202852491289377
        total_loss: -0.0038536442443728447
        vf_explained_var: -0.041926175355911255
        vf_loss: 22.41497230529785
      agent-2:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.976774275302887
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020430705044418573
        model: {}
        policy_loss: -0.0036546182818710804
        total_loss: -0.0033574565313756466
        vf_explained_var: 0.025094807147979736
        vf_loss: 20.162872314453125
      agent-3:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49708741903305054
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010521914809942245
        model: {}
        policy_loss: -0.0021228299010545015
        total_loss: -0.0011473766062408686
        vf_explained_var: 0.07353372871875763
        vf_loss: 18.503276824951172
      agent-4:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8705248832702637
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012527297949418426
        model: {}
        policy_loss: -0.003329894505441189
        total_loss: -0.0028845949564129114
        vf_explained_var: 0.03606076538562775
        vf_loss: 19.77423095703125
      agent-5:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6049554944038391
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010222611017525196
        model: {}
        policy_loss: -0.0025288306642323732
        total_loss: -0.0016108739655464888
        vf_explained_var: 0.04383082687854767
        vf_loss: 19.82675552368164
    load_time_ms: 13320.369
    num_steps_sampled: 25536000
    num_steps_trained: 25536000
    sample_time_ms: 90251.836
    update_time_ms: 20.997
  iterations_since_restore: 206
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.486857142857142
    ram_util_percent: 10.149142857142854
  pid: 24061
  policy_reward_max:
    agent-0: 197.666666666666
    agent-1: 197.666666666666
    agent-2: 197.666666666666
    agent-3: 197.666666666666
    agent-4: 197.666666666666
    agent-5: 197.666666666666
  policy_reward_mean:
    agent-0: 166.37166666666653
    agent-1: 166.37166666666653
    agent-2: 166.37166666666653
    agent-3: 166.37166666666653
    agent-4: 166.37166666666653
    agent-5: 166.37166666666653
  policy_reward_min:
    agent-0: 114.66666666666721
    agent-1: 114.66666666666721
    agent-2: 114.66666666666721
    agent-3: 114.66666666666721
    agent-4: 114.66666666666721
    agent-5: 114.66666666666721
  sampler_perf:
    mean_env_wait_ms: 24.46699963835622
    mean_inference_ms: 12.24880179495856
    mean_processing_ms: 50.75759344183793
  time_since_restore: 26414.05117917061
  time_this_iter_s: 122.9429702758789
  time_total_s: 35540.06299304962
  timestamp: 1637050453
  timesteps_since_restore: 19776000
  timesteps_this_iter: 96000
  timesteps_total: 25536000
  training_iteration: 266
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    266 |          35540.1 | 25536000 |   998.23 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 60
    apples_agent-0_mean: 3.02
    apples_agent-0_min: 0
    apples_agent-1_max: 105
    apples_agent-1_mean: 34.27
    apples_agent-1_min: 0
    apples_agent-2_max: 42
    apples_agent-2_mean: 1.54
    apples_agent-2_min: 0
    apples_agent-3_max: 184
    apples_agent-3_mean: 114.81
    apples_agent-3_min: 55
    apples_agent-4_max: 15
    apples_agent-4_mean: 0.37
    apples_agent-4_min: 0
    apples_agent-5_max: 164
    apples_agent-5_mean: 83.28
    apples_agent-5_min: 17
    cleaning_beam_agent-0_max: 532
    cleaning_beam_agent-0_mean: 406.31
    cleaning_beam_agent-0_min: 175
    cleaning_beam_agent-1_max: 595
    cleaning_beam_agent-1_mean: 278.97
    cleaning_beam_agent-1_min: 101
    cleaning_beam_agent-2_max: 559
    cleaning_beam_agent-2_mean: 433.25
    cleaning_beam_agent-2_min: 234
    cleaning_beam_agent-3_max: 44
    cleaning_beam_agent-3_mean: 15.31
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 641
    cleaning_beam_agent-4_mean: 510.3
    cleaning_beam_agent-4_min: 423
    cleaning_beam_agent-5_max: 123
    cleaning_beam_agent-5_mean: 21.62
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-16-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1173.9999999999902
  episode_reward_mean: 1010.5899999999885
  episode_reward_min: 702.9999999999937
  episodes_this_iter: 96
  episodes_total: 25632
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19632.502
    learner:
      agent-0:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0889356136322021
        entropy_coeff: 0.0017600000137463212
        kl: 0.001999890198931098
        model: {}
        policy_loss: -0.0032789944671094418
        total_loss: -0.003118423977866769
        vf_explained_var: 0.016240209341049194
        vf_loss: 20.770967483520508
      agent-1:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0925389528274536
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018386044539511204
        model: {}
        policy_loss: -0.004311875440180302
        total_loss: -0.003980685491114855
        vf_explained_var: -0.02279864251613617
        vf_loss: 22.54058265686035
      agent-2:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.966748058795929
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014633781975135207
        model: {}
        policy_loss: -0.0032034628093242645
        total_loss: -0.0028779003769159317
        vf_explained_var: 0.020655110478401184
        vf_loss: 20.27037811279297
      agent-3:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47882696986198425
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006528658559545875
        model: {}
        policy_loss: -0.002129199216142297
        total_loss: -0.0009984138887375593
        vf_explained_var: 0.040745168924331665
        vf_loss: 19.735225677490234
      agent-4:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.855129599571228
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011774268932640553
        model: {}
        policy_loss: -0.0035374783910810947
        total_loss: -0.0030383430421352386
        vf_explained_var: 0.028078630566596985
        vf_loss: 20.041635513305664
      agent-5:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5831999182701111
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010440968908369541
        model: {}
        policy_loss: -0.002358631929382682
        total_loss: -0.0013879160396754742
        vf_explained_var: 0.05549055337905884
        vf_loss: 19.97149658203125
    load_time_ms: 13338.027
    num_steps_sampled: 25632000
    num_steps_trained: 25632000
    sample_time_ms: 90201.182
    update_time_ms: 20.849
  iterations_since_restore: 207
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.489204545454548
    ram_util_percent: 10.051704545454545
  pid: 24061
  policy_reward_max:
    agent-0: 195.6666666666665
    agent-1: 195.6666666666665
    agent-2: 195.6666666666665
    agent-3: 195.6666666666665
    agent-4: 195.6666666666665
    agent-5: 195.6666666666665
  policy_reward_mean:
    agent-0: 168.43166666666656
    agent-1: 168.43166666666656
    agent-2: 168.43166666666656
    agent-3: 168.43166666666656
    agent-4: 168.43166666666656
    agent-5: 168.43166666666656
  policy_reward_min:
    agent-0: 117.16666666666707
    agent-1: 117.16666666666707
    agent-2: 117.16666666666707
    agent-3: 117.16666666666707
    agent-4: 117.16666666666707
    agent-5: 117.16666666666707
  sampler_perf:
    mean_env_wait_ms: 24.471000919521295
    mean_inference_ms: 12.247897481043559
    mean_processing_ms: 50.75437636757261
  time_since_restore: 26537.145413160324
  time_this_iter_s: 123.09423398971558
  time_total_s: 35663.15722703934
  timestamp: 1637050576
  timesteps_since_restore: 19872000
  timesteps_this_iter: 96000
  timesteps_total: 25632000
  training_iteration: 267
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    267 |          35663.2 | 25632000 |  1010.59 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 65
    apples_agent-0_mean: 3.26
    apples_agent-0_min: 0
    apples_agent-1_max: 145
    apples_agent-1_mean: 30.68
    apples_agent-1_min: 0
    apples_agent-2_max: 99
    apples_agent-2_mean: 4.53
    apples_agent-2_min: 0
    apples_agent-3_max: 259
    apples_agent-3_mean: 121.13
    apples_agent-3_min: 75
    apples_agent-4_max: 63
    apples_agent-4_mean: 1.75
    apples_agent-4_min: 0
    apples_agent-5_max: 130
    apples_agent-5_mean: 80.1
    apples_agent-5_min: 51
    cleaning_beam_agent-0_max: 522
    cleaning_beam_agent-0_mean: 395.43
    cleaning_beam_agent-0_min: 236
    cleaning_beam_agent-1_max: 527
    cleaning_beam_agent-1_mean: 274.6
    cleaning_beam_agent-1_min: 90
    cleaning_beam_agent-2_max: 585
    cleaning_beam_agent-2_mean: 424.38
    cleaning_beam_agent-2_min: 157
    cleaning_beam_agent-3_max: 83
    cleaning_beam_agent-3_mean: 16.76
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 585
    cleaning_beam_agent-4_mean: 489.39
    cleaning_beam_agent-4_min: 290
    cleaning_beam_agent-5_max: 114
    cleaning_beam_agent-5_mean: 24.68
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-18-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1144.9999999999989
  episode_reward_mean: 990.4199999999864
  episode_reward_min: 645.9999999999931
  episodes_this_iter: 96
  episodes_total: 25728
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19692.428
    learner:
      agent-0:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1069715023040771
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014242404140532017
        model: {}
        policy_loss: -0.003449812065809965
        total_loss: -0.003214724827557802
        vf_explained_var: 0.020194321870803833
        vf_loss: 21.833595275878906
      agent-1:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0923892259597778
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012168592074885964
        model: {}
        policy_loss: -0.003900643903762102
        total_loss: -0.003420168999582529
        vf_explained_var: -0.06264343857765198
        vf_loss: 24.030813217163086
      agent-2:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9716393947601318
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019201518734917045
        model: {}
        policy_loss: -0.0033783833496272564
        total_loss: -0.0030167363584041595
        vf_explained_var: 0.056977152824401855
        vf_loss: 20.71729850769043
      agent-3:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5048754811286926
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016308738850057125
        model: {}
        policy_loss: -0.002730840817093849
        total_loss: -0.001653551124036312
        vf_explained_var: 0.08939968049526215
        vf_loss: 19.658706665039062
      agent-4:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8744839429855347
        entropy_coeff: 0.0017600000137463212
        kl: 0.001860095071606338
        model: {}
        policy_loss: -0.003931917250156403
        total_loss: -0.003321424126625061
        vf_explained_var: 0.02408921718597412
        vf_loss: 21.495849609375
      agent-5:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6217939853668213
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014917375519871712
        model: {}
        policy_loss: -0.0029409569688141346
        total_loss: -0.0020115943625569344
        vf_explained_var: 0.09357331693172455
        vf_loss: 20.23719024658203
    load_time_ms: 13221.309
    num_steps_sampled: 25728000
    num_steps_trained: 25728000
    sample_time_ms: 90290.846
    update_time_ms: 20.88
  iterations_since_restore: 208
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.523428571428573
    ram_util_percent: 10.134285714285717
  pid: 24061
  policy_reward_max:
    agent-0: 190.83333333333366
    agent-1: 190.83333333333366
    agent-2: 190.83333333333366
    agent-3: 190.83333333333366
    agent-4: 190.83333333333366
    agent-5: 190.83333333333366
  policy_reward_mean:
    agent-0: 165.06999999999988
    agent-1: 165.06999999999988
    agent-2: 165.06999999999988
    agent-3: 165.06999999999988
    agent-4: 165.06999999999988
    agent-5: 165.06999999999988
  policy_reward_min:
    agent-0: 107.66666666666703
    agent-1: 107.66666666666703
    agent-2: 107.66666666666703
    agent-3: 107.66666666666703
    agent-4: 107.66666666666703
    agent-5: 107.66666666666703
  sampler_perf:
    mean_env_wait_ms: 24.474093257655735
    mean_inference_ms: 12.247129675547885
    mean_processing_ms: 50.75275662933551
  time_since_restore: 26660.404819250107
  time_this_iter_s: 123.25940608978271
  time_total_s: 35786.41663312912
  timestamp: 1637050699
  timesteps_since_restore: 19968000
  timesteps_this_iter: 96000
  timesteps_total: 25728000
  training_iteration: 268
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 17.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    268 |          35786.4 | 25728000 |   990.42 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 61
    apples_agent-0_mean: 2.02
    apples_agent-0_min: 0
    apples_agent-1_max: 103
    apples_agent-1_mean: 30.41
    apples_agent-1_min: 0
    apples_agent-2_max: 71
    apples_agent-2_mean: 4.83
    apples_agent-2_min: 0
    apples_agent-3_max: 176
    apples_agent-3_mean: 119.83
    apples_agent-3_min: 0
    apples_agent-4_max: 30
    apples_agent-4_mean: 1.1
    apples_agent-4_min: 0
    apples_agent-5_max: 112
    apples_agent-5_mean: 80.16
    apples_agent-5_min: 22
    cleaning_beam_agent-0_max: 545
    cleaning_beam_agent-0_mean: 384.05
    cleaning_beam_agent-0_min: 207
    cleaning_beam_agent-1_max: 564
    cleaning_beam_agent-1_mean: 272.74
    cleaning_beam_agent-1_min: 113
    cleaning_beam_agent-2_max: 612
    cleaning_beam_agent-2_mean: 423.21
    cleaning_beam_agent-2_min: 191
    cleaning_beam_agent-3_max: 93
    cleaning_beam_agent-3_mean: 20.4
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 581
    cleaning_beam_agent-4_mean: 487.95
    cleaning_beam_agent-4_min: 380
    cleaning_beam_agent-5_max: 170
    cleaning_beam_agent-5_mean: 28.74
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.08
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-20-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1204.9999999999893
  episode_reward_mean: 982.259999999988
  episode_reward_min: 609.0000000000008
  episodes_this_iter: 96
  episodes_total: 25824
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19719.37
    learner:
      agent-0:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1093297004699707
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015434938250109553
        model: {}
        policy_loss: -0.0032729865051805973
        total_loss: -0.003072629915550351
        vf_explained_var: 0.03436717391014099
        vf_loss: 21.527803421020508
      agent-1:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0912704467773438
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014909381279721856
        model: {}
        policy_loss: -0.004128481727093458
        total_loss: -0.0036935294046998024
        vf_explained_var: -0.03939521312713623
        vf_loss: 23.555879592895508
      agent-2:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9613592624664307
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015627066604793072
        model: {}
        policy_loss: -0.003189567942172289
        total_loss: -0.002767866011708975
        vf_explained_var: 0.045568451285362244
        vf_loss: 21.13694953918457
      agent-3:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5147260427474976
        entropy_coeff: 0.0017600000137463212
        kl: 0.001065218704752624
        model: {}
        policy_loss: -0.002321362029761076
        total_loss: -0.0011672473046928644
        vf_explained_var: 0.0565514862537384
        vf_loss: 20.600318908691406
      agent-4:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8736371994018555
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019565089605748653
        model: {}
        policy_loss: -0.003745514899492264
        total_loss: -0.003102027578279376
        vf_explained_var: 0.007823795080184937
        vf_loss: 21.810867309570312
      agent-5:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6253530383110046
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009880490833893418
        model: {}
        policy_loss: -0.0028016106225550175
        total_loss: -0.0018454124219715595
        vf_explained_var: 0.08009079098701477
        vf_loss: 20.568174362182617
    load_time_ms: 13209.315
    num_steps_sampled: 25824000
    num_steps_trained: 25824000
    sample_time_ms: 90333.325
    update_time_ms: 20.271
  iterations_since_restore: 209
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.359090909090908
    ram_util_percent: 10.025568181818182
  pid: 24061
  policy_reward_max:
    agent-0: 200.83333333333326
    agent-1: 200.83333333333326
    agent-2: 200.83333333333326
    agent-3: 200.83333333333326
    agent-4: 200.83333333333326
    agent-5: 200.83333333333326
  policy_reward_mean:
    agent-0: 163.70999999999995
    agent-1: 163.70999999999995
    agent-2: 163.70999999999995
    agent-3: 163.70999999999995
    agent-4: 163.70999999999995
    agent-5: 163.70999999999995
  policy_reward_min:
    agent-0: 101.50000000000068
    agent-1: 101.50000000000068
    agent-2: 101.50000000000068
    agent-3: 101.50000000000068
    agent-4: 101.50000000000068
    agent-5: 101.50000000000068
  sampler_perf:
    mean_env_wait_ms: 24.477045532121277
    mean_inference_ms: 12.246546516750454
    mean_processing_ms: 50.75014003295954
  time_since_restore: 26783.838965415955
  time_this_iter_s: 123.43414616584778
  time_total_s: 35909.85077929497
  timestamp: 1637050823
  timesteps_since_restore: 20064000
  timesteps_this_iter: 96000
  timesteps_total: 25824000
  training_iteration: 269
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    269 |          35909.9 | 25824000 |   982.26 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 43
    apples_agent-0_mean: 2.54
    apples_agent-0_min: 0
    apples_agent-1_max: 105
    apples_agent-1_mean: 32.64
    apples_agent-1_min: 0
    apples_agent-2_max: 64
    apples_agent-2_mean: 3.28
    apples_agent-2_min: 0
    apples_agent-3_max: 190
    apples_agent-3_mean: 119.17
    apples_agent-3_min: 72
    apples_agent-4_max: 54
    apples_agent-4_mean: 1.04
    apples_agent-4_min: 0
    apples_agent-5_max: 146
    apples_agent-5_mean: 83.15
    apples_agent-5_min: 45
    cleaning_beam_agent-0_max: 523
    cleaning_beam_agent-0_mean: 395.6
    cleaning_beam_agent-0_min: 227
    cleaning_beam_agent-1_max: 590
    cleaning_beam_agent-1_mean: 278.72
    cleaning_beam_agent-1_min: 108
    cleaning_beam_agent-2_max: 557
    cleaning_beam_agent-2_mean: 426.65
    cleaning_beam_agent-2_min: 151
    cleaning_beam_agent-3_max: 49
    cleaning_beam_agent-3_mean: 18.34
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 610
    cleaning_beam_agent-4_mean: 482.72
    cleaning_beam_agent-4_min: 367
    cleaning_beam_agent-5_max: 76
    cleaning_beam_agent-5_mean: 21.53
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-22-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1208.0000000000018
  episode_reward_mean: 984.8999999999884
  episode_reward_min: 499.00000000000915
  episodes_this_iter: 96
  episodes_total: 25920
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19696.324
    learner:
      agent-0:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0932945013046265
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019198874942958355
        model: {}
        policy_loss: -0.0033018216490745544
        total_loss: -0.003041745163500309
        vf_explained_var: 0.04579855501651764
        vf_loss: 21.84273910522461
      agent-1:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0853921175003052
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017961346311494708
        model: {}
        policy_loss: -0.0039218426682055
        total_loss: -0.0034989917185157537
        vf_explained_var: -0.0064422935247421265
        vf_loss: 23.3314151763916
      agent-2:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9627038836479187
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011863000690937042
        model: {}
        policy_loss: -0.0031922217458486557
        total_loss: -0.0027312319725751877
        vf_explained_var: 0.043402284383773804
        vf_loss: 21.553464889526367
      agent-3:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5045804977416992
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015441982541233301
        model: {}
        policy_loss: -0.0025555030442774296
        total_loss: -0.0014577610418200493
        vf_explained_var: 0.10806412994861603
        vf_loss: 19.85802459716797
      agent-4:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8712621927261353
        entropy_coeff: 0.0017600000137463212
        kl: 0.001693142345175147
        model: {}
        policy_loss: -0.0037222271785140038
        total_loss: -0.0031464966014027596
        vf_explained_var: 0.06554007530212402
        vf_loss: 21.091514587402344
      agent-5:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.622818112373352
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010137456702068448
        model: {}
        policy_loss: -0.0031137755140662193
        total_loss: -0.0022322756703943014
        vf_explained_var: 0.13008059561252594
        vf_loss: 19.776575088500977
    load_time_ms: 13199.002
    num_steps_sampled: 25920000
    num_steps_trained: 25920000
    sample_time_ms: 90316.265
    update_time_ms: 21.417
  iterations_since_restore: 210
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.382954545454544
    ram_util_percent: 10.145454545454545
  pid: 24061
  policy_reward_max:
    agent-0: 201.33333333333275
    agent-1: 201.33333333333275
    agent-2: 201.33333333333275
    agent-3: 201.33333333333275
    agent-4: 201.33333333333275
    agent-5: 201.33333333333275
  policy_reward_mean:
    agent-0: 164.1499999999999
    agent-1: 164.1499999999999
    agent-2: 164.1499999999999
    agent-3: 164.1499999999999
    agent-4: 164.1499999999999
    agent-5: 164.1499999999999
  policy_reward_min:
    agent-0: 83.16666666666663
    agent-1: 83.16666666666663
    agent-2: 83.16666666666663
    agent-3: 83.16666666666663
    agent-4: 83.16666666666663
    agent-5: 83.16666666666663
  sampler_perf:
    mean_env_wait_ms: 24.48011089104688
    mean_inference_ms: 12.245754503978212
    mean_processing_ms: 50.74812943468075
  time_since_restore: 26907.025609254837
  time_this_iter_s: 123.18664383888245
  time_total_s: 36033.03742313385
  timestamp: 1637050946
  timesteps_since_restore: 20160000
  timesteps_this_iter: 96000
  timesteps_total: 25920000
  training_iteration: 270
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    270 |            36033 | 25920000 |    984.9 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 62
    apples_agent-0_mean: 4.63
    apples_agent-0_min: 0
    apples_agent-1_max: 110
    apples_agent-1_mean: 35.85
    apples_agent-1_min: 0
    apples_agent-2_max: 255
    apples_agent-2_mean: 4.47
    apples_agent-2_min: 0
    apples_agent-3_max: 181
    apples_agent-3_mean: 117.66
    apples_agent-3_min: 69
    apples_agent-4_max: 7
    apples_agent-4_mean: 0.08
    apples_agent-4_min: 0
    apples_agent-5_max: 139
    apples_agent-5_mean: 82.92
    apples_agent-5_min: 50
    cleaning_beam_agent-0_max: 542
    cleaning_beam_agent-0_mean: 403.36
    cleaning_beam_agent-0_min: 159
    cleaning_beam_agent-1_max: 511
    cleaning_beam_agent-1_mean: 259.83
    cleaning_beam_agent-1_min: 96
    cleaning_beam_agent-2_max: 595
    cleaning_beam_agent-2_mean: 413.36
    cleaning_beam_agent-2_min: 143
    cleaning_beam_agent-3_max: 57
    cleaning_beam_agent-3_mean: 17.58
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 659
    cleaning_beam_agent-4_mean: 485.87
    cleaning_beam_agent-4_min: 363
    cleaning_beam_agent-5_max: 114
    cleaning_beam_agent-5_mean: 26.35
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-24-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1181.9999999999918
  episode_reward_mean: 985.1099999999881
  episode_reward_min: 585.9999999999976
  episodes_this_iter: 96
  episodes_total: 26016
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19738.761
    learner:
      agent-0:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 1.076317548751831
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014909208985045552
        model: {}
        policy_loss: -0.0034442918840795755
        total_loss: -0.003070457372814417
        vf_explained_var: 0.02628965675830841
        vf_loss: 22.681533813476562
      agent-1:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0875403881072998
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012458317214623094
        model: {}
        policy_loss: -0.0037778629921376705
        total_loss: -0.0032399434130638838
        vf_explained_var: -0.035638391971588135
        vf_loss: 24.5198974609375
      agent-2:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9659616947174072
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017997276736423373
        model: {}
        policy_loss: -0.003466246649622917
        total_loss: -0.0029571428894996643
        vf_explained_var: 0.03715163469314575
        vf_loss: 22.091970443725586
      agent-3:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5000592470169067
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013416250003501773
        model: {}
        policy_loss: -0.0027752015739679337
        total_loss: -0.001577913761138916
        vf_explained_var: 0.08891552686691284
        vf_loss: 20.773923873901367
      agent-4:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8734046816825867
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015445954632014036
        model: {}
        policy_loss: -0.00379311665892601
        total_loss: -0.0031434192787855864
        vf_explained_var: 0.051599279046058655
        vf_loss: 21.868871688842773
      agent-5:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6284071207046509
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011859341757372022
        model: {}
        policy_loss: -0.0028871118556708097
        total_loss: -0.0018153749406337738
        vf_explained_var: 0.067405104637146
        vf_loss: 21.77733612060547
    load_time_ms: 13119.907
    num_steps_sampled: 26016000
    num_steps_trained: 26016000
    sample_time_ms: 90413.076
    update_time_ms: 21.197
  iterations_since_restore: 211
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.36573033707865
    ram_util_percent: 10.1376404494382
  pid: 24061
  policy_reward_max:
    agent-0: 196.99999999999952
    agent-1: 196.99999999999952
    agent-2: 196.99999999999952
    agent-3: 196.99999999999952
    agent-4: 196.99999999999952
    agent-5: 196.99999999999952
  policy_reward_mean:
    agent-0: 164.1849999999999
    agent-1: 164.1849999999999
    agent-2: 164.1849999999999
    agent-3: 164.1849999999999
    agent-4: 164.1849999999999
    agent-5: 164.1849999999999
  policy_reward_min:
    agent-0: 97.66666666666704
    agent-1: 97.66666666666704
    agent-2: 97.66666666666704
    agent-3: 97.66666666666704
    agent-4: 97.66666666666704
    agent-5: 97.66666666666704
  sampler_perf:
    mean_env_wait_ms: 24.48333317281285
    mean_inference_ms: 12.245161550049515
    mean_processing_ms: 50.74612698523555
  time_since_restore: 27031.505656957626
  time_this_iter_s: 124.4800477027893
  time_total_s: 36157.51747083664
  timestamp: 1637051071
  timesteps_since_restore: 20256000
  timesteps_this_iter: 96000
  timesteps_total: 26016000
  training_iteration: 271
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    271 |          36157.5 | 26016000 |   985.11 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 67
    apples_agent-0_mean: 4.09
    apples_agent-0_min: 0
    apples_agent-1_max: 95
    apples_agent-1_mean: 35.99
    apples_agent-1_min: 0
    apples_agent-2_max: 125
    apples_agent-2_mean: 5.93
    apples_agent-2_min: 0
    apples_agent-3_max: 181
    apples_agent-3_mean: 109.71
    apples_agent-3_min: 41
    apples_agent-4_max: 28
    apples_agent-4_mean: 0.69
    apples_agent-4_min: 0
    apples_agent-5_max: 129
    apples_agent-5_mean: 80.69
    apples_agent-5_min: 49
    cleaning_beam_agent-0_max: 564
    cleaning_beam_agent-0_mean: 420.85
    cleaning_beam_agent-0_min: 259
    cleaning_beam_agent-1_max: 524
    cleaning_beam_agent-1_mean: 269.52
    cleaning_beam_agent-1_min: 124
    cleaning_beam_agent-2_max: 665
    cleaning_beam_agent-2_mean: 435.97
    cleaning_beam_agent-2_min: 153
    cleaning_beam_agent-3_max: 104
    cleaning_beam_agent-3_mean: 18.62
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 601
    cleaning_beam_agent-4_mean: 483.08
    cleaning_beam_agent-4_min: 332
    cleaning_beam_agent-5_max: 105
    cleaning_beam_agent-5_mean: 21.38
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-26-35
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1197.999999999999
  episode_reward_mean: 998.3199999999881
  episode_reward_min: 593.9999999999927
  episodes_this_iter: 96
  episodes_total: 26112
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19785.01
    learner:
      agent-0:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0717040300369263
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014624112518504262
        model: {}
        policy_loss: -0.0034350603818893433
        total_loss: -0.0031784744933247566
        vf_explained_var: 0.03839173913002014
        vf_loss: 21.427810668945312
      agent-1:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1049606800079346
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010962854139506817
        model: {}
        policy_loss: -0.0038325549103319645
        total_loss: -0.0034804840106517076
        vf_explained_var: 0.0008898228406906128
        vf_loss: 22.968008041381836
      agent-2:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9448175430297852
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012063075555488467
        model: {}
        policy_loss: -0.0031641877721995115
        total_loss: -0.0026583592407405376
        vf_explained_var: 0.013355791568756104
        vf_loss: 21.687089920043945
      agent-3:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.494437575340271
        entropy_coeff: 0.0017600000137463212
        kl: 0.001013869303278625
        model: {}
        policy_loss: -0.002238481305539608
        total_loss: -0.0011283555068075657
        vf_explained_var: 0.0857478529214859
        vf_loss: 19.80335235595703
      agent-4:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8595529794692993
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017955852672457695
        model: {}
        policy_loss: -0.004001122899353504
        total_loss: -0.0033836092334240675
        vf_explained_var: 0.034482747316360474
        vf_loss: 21.30325698852539
      agent-5:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5936504602432251
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008146340260282159
        model: {}
        policy_loss: -0.0027142995968461037
        total_loss: -0.001815800555050373
        vf_explained_var: 0.11481750011444092
        vf_loss: 19.43324851989746
    load_time_ms: 13102.895
    num_steps_sampled: 26112000
    num_steps_trained: 26112000
    sample_time_ms: 90426.822
    update_time_ms: 21.625
  iterations_since_restore: 212
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.422159090909092
    ram_util_percent: 10.140909090909089
  pid: 24061
  policy_reward_max:
    agent-0: 199.66666666666663
    agent-1: 199.66666666666663
    agent-2: 199.66666666666663
    agent-3: 199.66666666666663
    agent-4: 199.66666666666663
    agent-5: 199.66666666666663
  policy_reward_mean:
    agent-0: 166.38666666666657
    agent-1: 166.38666666666657
    agent-2: 166.38666666666657
    agent-3: 166.38666666666657
    agent-4: 166.38666666666657
    agent-5: 166.38666666666657
  policy_reward_min:
    agent-0: 99.00000000000004
    agent-1: 99.00000000000004
    agent-2: 99.00000000000004
    agent-3: 99.00000000000004
    agent-4: 99.00000000000004
    agent-5: 99.00000000000004
  sampler_perf:
    mean_env_wait_ms: 24.486473921449377
    mean_inference_ms: 12.244767634327893
    mean_processing_ms: 50.74262849814533
  time_since_restore: 27155.14141392708
  time_this_iter_s: 123.6357569694519
  time_total_s: 36281.15322780609
  timestamp: 1637051195
  timesteps_since_restore: 20352000
  timesteps_this_iter: 96000
  timesteps_total: 26112000
  training_iteration: 272
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    272 |          36281.2 | 26112000 |   998.32 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 60
    apples_agent-0_mean: 3.58
    apples_agent-0_min: 0
    apples_agent-1_max: 122
    apples_agent-1_mean: 36.65
    apples_agent-1_min: 0
    apples_agent-2_max: 125
    apples_agent-2_mean: 4.46
    apples_agent-2_min: 0
    apples_agent-3_max: 179
    apples_agent-3_mean: 107.77
    apples_agent-3_min: 5
    apples_agent-4_max: 19
    apples_agent-4_mean: 0.53
    apples_agent-4_min: 0
    apples_agent-5_max: 125
    apples_agent-5_mean: 83.16
    apples_agent-5_min: 53
    cleaning_beam_agent-0_max: 551
    cleaning_beam_agent-0_mean: 402.73
    cleaning_beam_agent-0_min: 250
    cleaning_beam_agent-1_max: 516
    cleaning_beam_agent-1_mean: 265.11
    cleaning_beam_agent-1_min: 115
    cleaning_beam_agent-2_max: 581
    cleaning_beam_agent-2_mean: 442.75
    cleaning_beam_agent-2_min: 153
    cleaning_beam_agent-3_max: 97
    cleaning_beam_agent-3_mean: 15.29
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 651
    cleaning_beam_agent-4_mean: 493.8
    cleaning_beam_agent-4_min: 332
    cleaning_beam_agent-5_max: 79
    cleaning_beam_agent-5_mean: 22.65
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-28-39
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1206.0000000000032
  episode_reward_mean: 1016.1799999999871
  episode_reward_min: 593.9999999999927
  episodes_this_iter: 96
  episodes_total: 26208
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19806.386
    learner:
      agent-0:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0839424133300781
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020698709413409233
        model: {}
        policy_loss: -0.0036713480949401855
        total_loss: -0.0034048017114400864
        vf_explained_var: 0.008968368172645569
        vf_loss: 21.74286651611328
      agent-1:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0914850234985352
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019159391522407532
        model: {}
        policy_loss: -0.00415414571762085
        total_loss: -0.003748517483472824
        vf_explained_var: -0.03647345304489136
        vf_loss: 23.266433715820312
      agent-2:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9545440077781677
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010921956272795796
        model: {}
        policy_loss: -0.002959071658551693
        total_loss: -0.0025330702774226665
        vf_explained_var: -0.01384153962135315
        vf_loss: 21.05995750427246
      agent-3:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4749205708503723
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009992631385102868
        model: {}
        policy_loss: -0.002045501722022891
        total_loss: -0.0009299665689468384
        vf_explained_var: 0.04745955765247345
        vf_loss: 19.51397132873535
      agent-4:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8652105331420898
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013186725554987788
        model: {}
        policy_loss: -0.0034186337143182755
        total_loss: -0.002833900973200798
        vf_explained_var: 0.0013337135314941406
        vf_loss: 21.07503890991211
      agent-5:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5924074649810791
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005621256423182786
        model: {}
        policy_loss: -0.0023961374536156654
        total_loss: -0.0013965140096843243
        vf_explained_var: 0.03470581769943237
        vf_loss: 20.422607421875
    load_time_ms: 13009.168
    num_steps_sampled: 26208000
    num_steps_trained: 26208000
    sample_time_ms: 90430.482
    update_time_ms: 21.595
  iterations_since_restore: 213
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.425
    ram_util_percent: 10.152840909090907
  pid: 24061
  policy_reward_max:
    agent-0: 200.99999999999974
    agent-1: 200.99999999999974
    agent-2: 200.99999999999974
    agent-3: 200.99999999999974
    agent-4: 200.99999999999974
    agent-5: 200.99999999999974
  policy_reward_mean:
    agent-0: 169.3633333333332
    agent-1: 169.3633333333332
    agent-2: 169.3633333333332
    agent-3: 169.3633333333332
    agent-4: 169.3633333333332
    agent-5: 169.3633333333332
  policy_reward_min:
    agent-0: 99.00000000000004
    agent-1: 99.00000000000004
    agent-2: 99.00000000000004
    agent-3: 99.00000000000004
    agent-4: 99.00000000000004
    agent-5: 99.00000000000004
  sampler_perf:
    mean_env_wait_ms: 24.490583682016833
    mean_inference_ms: 12.244162333979562
    mean_processing_ms: 50.739899377558594
  time_since_restore: 27278.75001502037
  time_this_iter_s: 123.60860109329224
  time_total_s: 36404.76182889938
  timestamp: 1637051319
  timesteps_since_restore: 20448000
  timesteps_this_iter: 96000
  timesteps_total: 26208000
  training_iteration: 273
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    273 |          36404.8 | 26208000 |  1016.18 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 76
    apples_agent-0_mean: 4.27
    apples_agent-0_min: 0
    apples_agent-1_max: 133
    apples_agent-1_mean: 35.44
    apples_agent-1_min: 0
    apples_agent-2_max: 371
    apples_agent-2_mean: 9.5
    apples_agent-2_min: 0
    apples_agent-3_max: 184
    apples_agent-3_mean: 110.76
    apples_agent-3_min: 0
    apples_agent-4_max: 78
    apples_agent-4_mean: 2.18
    apples_agent-4_min: 0
    apples_agent-5_max: 148
    apples_agent-5_mean: 81.36
    apples_agent-5_min: 25
    cleaning_beam_agent-0_max: 573
    cleaning_beam_agent-0_mean: 431.24
    cleaning_beam_agent-0_min: 257
    cleaning_beam_agent-1_max: 578
    cleaning_beam_agent-1_mean: 277.08
    cleaning_beam_agent-1_min: 98
    cleaning_beam_agent-2_max: 539
    cleaning_beam_agent-2_mean: 409.59
    cleaning_beam_agent-2_min: 116
    cleaning_beam_agent-3_max: 99
    cleaning_beam_agent-3_mean: 18.22
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 611
    cleaning_beam_agent-4_mean: 487.28
    cleaning_beam_agent-4_min: 278
    cleaning_beam_agent-5_max: 125
    cleaning_beam_agent-5_mean: 25.45
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-30-41
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1169.9999999999902
  episode_reward_mean: 975.1499999999878
  episode_reward_min: 427.0000000000146
  episodes_this_iter: 96
  episodes_total: 26304
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19783.523
    learner:
      agent-0:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0662789344787598
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018800050020217896
        model: {}
        policy_loss: -0.0033788015134632587
        total_loss: -0.00298599386587739
        vf_explained_var: -0.010502934455871582
        vf_loss: 22.694561004638672
      agent-1:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0873117446899414
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016358810244128108
        model: {}
        policy_loss: -0.003557348856702447
        total_loss: -0.0030306309927254915
        vf_explained_var: -0.07750311493873596
        vf_loss: 24.40386199951172
      agent-2:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9618303179740906
        entropy_coeff: 0.0017600000137463212
        kl: 0.001816921285353601
        model: {}
        policy_loss: -0.0035293023101985455
        total_loss: -0.0031148940324783325
        vf_explained_var: 0.05374927818775177
        vf_loss: 21.072315216064453
      agent-3:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5007073879241943
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013929767301306129
        model: {}
        policy_loss: -0.0024646096862852573
        total_loss: -0.0013387647923082113
        vf_explained_var: 0.09692831337451935
        vf_loss: 20.070873260498047
      agent-4:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.873855710029602
        entropy_coeff: 0.0017600000137463212
        kl: 0.001379863591864705
        model: {}
        policy_loss: -0.003659778041765094
        total_loss: -0.0030316775664687157
        vf_explained_var: 0.029923200607299805
        vf_loss: 21.66084098815918
      agent-5:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6106284856796265
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006166576640680432
        model: {}
        policy_loss: -0.002546674571931362
        total_loss: -0.0015870779752731323
        vf_explained_var: 0.09171418845653534
        vf_loss: 20.34304428100586
    load_time_ms: 12993.154
    num_steps_sampled: 26304000
    num_steps_trained: 26304000
    sample_time_ms: 90444.674
    update_time_ms: 22.812
  iterations_since_restore: 214
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.976571428571427
    ram_util_percent: 10.169714285714285
  pid: 24061
  policy_reward_max:
    agent-0: 195.0000000000001
    agent-1: 195.0000000000001
    agent-2: 195.0000000000001
    agent-3: 195.0000000000001
    agent-4: 195.0000000000001
    agent-5: 195.0000000000001
  policy_reward_mean:
    agent-0: 162.52499999999998
    agent-1: 162.52499999999998
    agent-2: 162.52499999999998
    agent-3: 162.52499999999998
    agent-4: 162.52499999999998
    agent-5: 162.52499999999998
  policy_reward_min:
    agent-0: 71.16666666666643
    agent-1: 71.16666666666643
    agent-2: 71.16666666666643
    agent-3: 71.16666666666643
    agent-4: 71.16666666666643
    agent-5: 71.16666666666643
  sampler_perf:
    mean_env_wait_ms: 24.492971039565614
    mean_inference_ms: 12.24341062196542
    mean_processing_ms: 50.735665793902456
  time_since_restore: 27401.283373355865
  time_this_iter_s: 122.533358335495
  time_total_s: 36527.29518723488
  timestamp: 1637051441
  timesteps_since_restore: 20544000
  timesteps_this_iter: 96000
  timesteps_total: 26304000
  training_iteration: 274
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    274 |          36527.3 | 26304000 |   975.15 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 60
    apples_agent-0_mean: 3.05
    apples_agent-0_min: 0
    apples_agent-1_max: 117
    apples_agent-1_mean: 35.53
    apples_agent-1_min: 0
    apples_agent-2_max: 24
    apples_agent-2_mean: 1.57
    apples_agent-2_min: 0
    apples_agent-3_max: 185
    apples_agent-3_mean: 115.52
    apples_agent-3_min: 0
    apples_agent-4_max: 22
    apples_agent-4_mean: 0.44
    apples_agent-4_min: 0
    apples_agent-5_max: 148
    apples_agent-5_mean: 82.98
    apples_agent-5_min: 30
    cleaning_beam_agent-0_max: 555
    cleaning_beam_agent-0_mean: 441.16
    cleaning_beam_agent-0_min: 223
    cleaning_beam_agent-1_max: 529
    cleaning_beam_agent-1_mean: 270.13
    cleaning_beam_agent-1_min: 97
    cleaning_beam_agent-2_max: 612
    cleaning_beam_agent-2_mean: 433.65
    cleaning_beam_agent-2_min: 283
    cleaning_beam_agent-3_max: 42
    cleaning_beam_agent-3_mean: 14.6
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 642
    cleaning_beam_agent-4_mean: 490.08
    cleaning_beam_agent-4_min: 280
    cleaning_beam_agent-5_max: 350
    cleaning_beam_agent-5_mean: 27.15
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-32-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1177.000000000014
  episode_reward_mean: 1002.1599999999878
  episode_reward_min: 216.9999999999969
  episodes_this_iter: 96
  episodes_total: 26400
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19763.039
    learner:
      agent-0:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 1.057497501373291
        entropy_coeff: 0.0017600000137463212
        kl: 0.001062563038431108
        model: {}
        policy_loss: -0.002935430034995079
        total_loss: -0.002551186364144087
        vf_explained_var: 0.02284771203994751
        vf_loss: 22.454376220703125
      agent-1:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 1.093910813331604
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011356467148289084
        model: {}
        policy_loss: -0.004029675852507353
        total_loss: -0.0034872759133577347
        vf_explained_var: -0.028282850980758667
        vf_loss: 24.676807403564453
      agent-2:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9573535323143005
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014953103382140398
        model: {}
        policy_loss: -0.0031120788771659136
        total_loss: -0.0026544826105237007
        vf_explained_var: 0.05141228437423706
        vf_loss: 21.425392150878906
      agent-3:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4684598445892334
        entropy_coeff: 0.0017600000137463212
        kl: 0.001427430659532547
        model: {}
        policy_loss: -0.00264914333820343
        total_loss: -0.00142483483068645
        vf_explained_var: 0.08647501468658447
        vf_loss: 20.48796272277832
      agent-4:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8685669898986816
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014214799739420414
        model: {}
        policy_loss: -0.0038365803193300962
        total_loss: -0.0031551127322018147
        vf_explained_var: 0.035780683159828186
        vf_loss: 22.1014404296875
      agent-5:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5917922258377075
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010244541335850954
        model: {}
        policy_loss: -0.0026721281465142965
        total_loss: -0.0016181879909709096
        vf_explained_var: 0.09496648609638214
        vf_loss: 20.954936981201172
    load_time_ms: 12967.849
    num_steps_sampled: 26400000
    num_steps_trained: 26400000
    sample_time_ms: 90515.742
    update_time_ms: 22.858
  iterations_since_restore: 215
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.501136363636363
    ram_util_percent: 10.120454545454544
  pid: 24061
  policy_reward_max:
    agent-0: 196.16666666666663
    agent-1: 196.16666666666663
    agent-2: 196.16666666666663
    agent-3: 196.16666666666663
    agent-4: 196.16666666666663
    agent-5: 196.16666666666663
  policy_reward_mean:
    agent-0: 167.0266666666665
    agent-1: 167.0266666666665
    agent-2: 167.0266666666665
    agent-3: 167.0266666666665
    agent-4: 167.0266666666665
    agent-5: 167.0266666666665
  policy_reward_min:
    agent-0: 36.16666666666667
    agent-1: 36.16666666666667
    agent-2: 36.16666666666667
    agent-3: 36.16666666666667
    agent-4: 36.16666666666667
    agent-5: 36.16666666666667
  sampler_perf:
    mean_env_wait_ms: 24.496670747386364
    mean_inference_ms: 12.2428198380106
    mean_processing_ms: 50.73308840795302
  time_since_restore: 27524.81123161316
  time_this_iter_s: 123.5278582572937
  time_total_s: 36650.82304549217
  timestamp: 1637051565
  timesteps_since_restore: 20640000
  timesteps_this_iter: 96000
  timesteps_total: 26400000
  training_iteration: 275
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    275 |          36650.8 | 26400000 |  1002.16 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 35
    apples_agent-0_mean: 1.76
    apples_agent-0_min: 0
    apples_agent-1_max: 139
    apples_agent-1_mean: 33.54
    apples_agent-1_min: 0
    apples_agent-2_max: 31
    apples_agent-2_mean: 1.55
    apples_agent-2_min: 0
    apples_agent-3_max: 207
    apples_agent-3_mean: 116.84
    apples_agent-3_min: 59
    apples_agent-4_max: 51
    apples_agent-4_mean: 1.04
    apples_agent-4_min: 0
    apples_agent-5_max: 277
    apples_agent-5_mean: 83.61
    apples_agent-5_min: 49
    cleaning_beam_agent-0_max: 569
    cleaning_beam_agent-0_mean: 434.96
    cleaning_beam_agent-0_min: 299
    cleaning_beam_agent-1_max: 503
    cleaning_beam_agent-1_mean: 265.35
    cleaning_beam_agent-1_min: 84
    cleaning_beam_agent-2_max: 584
    cleaning_beam_agent-2_mean: 435.2
    cleaning_beam_agent-2_min: 268
    cleaning_beam_agent-3_max: 196
    cleaning_beam_agent-3_mean: 17.49
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 612
    cleaning_beam_agent-4_mean: 482.21
    cleaning_beam_agent-4_min: 292
    cleaning_beam_agent-5_max: 88
    cleaning_beam_agent-5_mean: 19.97
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-34-49
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1189.9999999999932
  episode_reward_mean: 1022.8499999999885
  episode_reward_min: 569.0000000000063
  episodes_this_iter: 96
  episodes_total: 26496
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19762.658
    learner:
      agent-0:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0512832403182983
        entropy_coeff: 0.0017600000137463212
        kl: 0.002387598156929016
        model: {}
        policy_loss: -0.003803226165473461
        total_loss: -0.0033818441443145275
        vf_explained_var: -0.004441991448402405
        vf_loss: 22.716373443603516
      agent-1:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0901007652282715
        entropy_coeff: 0.0017600000137463212
        kl: 0.001882856129668653
        model: {}
        policy_loss: -0.004022533074021339
        total_loss: -0.003447013907134533
        vf_explained_var: -0.06353604793548584
        vf_loss: 24.94097900390625
      agent-2:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9516481161117554
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020007872954010963
        model: {}
        policy_loss: -0.0033681169152259827
        total_loss: -0.0028864648193120956
        vf_explained_var: 0.00977933406829834
        vf_loss: 21.56554412841797
      agent-3:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47539475560188293
        entropy_coeff: 0.0017600000137463212
        kl: 0.001445185742340982
        model: {}
        policy_loss: -0.0021696090698242188
        total_loss: -0.0008450029417872429
        vf_explained_var: 0.019690051674842834
        vf_loss: 21.61305046081543
      agent-4:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8674935102462769
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021559568122029305
        model: {}
        policy_loss: -0.0037375688552856445
        total_loss: -0.003133022226393223
        vf_explained_var: 0.037673622369766235
        vf_loss: 21.31329345703125
      agent-5:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5645856857299805
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007880430202931166
        model: {}
        policy_loss: -0.0024368090089410543
        total_loss: -0.0013743240851908922
        vf_explained_var: 0.07822392880916595
        vf_loss: 20.56157112121582
    load_time_ms: 12928.039
    num_steps_sampled: 26496000
    num_steps_trained: 26496000
    sample_time_ms: 90626.861
    update_time_ms: 23.58
  iterations_since_restore: 216
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.453409090909092
    ram_util_percent: 10.085795454545455
  pid: 24061
  policy_reward_max:
    agent-0: 198.3333333333329
    agent-1: 198.3333333333329
    agent-2: 198.3333333333329
    agent-3: 198.3333333333329
    agent-4: 198.3333333333329
    agent-5: 198.3333333333329
  policy_reward_mean:
    agent-0: 170.47499999999985
    agent-1: 170.47499999999985
    agent-2: 170.47499999999985
    agent-3: 170.47499999999985
    agent-4: 170.47499999999985
    agent-5: 170.47499999999985
  policy_reward_min:
    agent-0: 94.83333333333351
    agent-1: 94.83333333333351
    agent-2: 94.83333333333351
    agent-3: 94.83333333333351
    agent-4: 94.83333333333351
    agent-5: 94.83333333333351
  sampler_perf:
    mean_env_wait_ms: 24.500181444701656
    mean_inference_ms: 12.242165424878154
    mean_processing_ms: 50.7312256419146
  time_since_restore: 27648.425040245056
  time_this_iter_s: 123.61380863189697
  time_total_s: 36774.43685412407
  timestamp: 1637051689
  timesteps_since_restore: 20736000
  timesteps_this_iter: 96000
  timesteps_total: 26496000
  training_iteration: 276
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    276 |          36774.4 | 26496000 |  1022.85 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 69
    apples_agent-0_mean: 2.88
    apples_agent-0_min: 0
    apples_agent-1_max: 114
    apples_agent-1_mean: 29.72
    apples_agent-1_min: 0
    apples_agent-2_max: 26
    apples_agent-2_mean: 1.6
    apples_agent-2_min: 0
    apples_agent-3_max: 179
    apples_agent-3_mean: 117.27
    apples_agent-3_min: 59
    apples_agent-4_max: 73
    apples_agent-4_mean: 2.37
    apples_agent-4_min: 0
    apples_agent-5_max: 155
    apples_agent-5_mean: 79.98
    apples_agent-5_min: 39
    cleaning_beam_agent-0_max: 628
    cleaning_beam_agent-0_mean: 442.85
    cleaning_beam_agent-0_min: 223
    cleaning_beam_agent-1_max: 556
    cleaning_beam_agent-1_mean: 268.62
    cleaning_beam_agent-1_min: 108
    cleaning_beam_agent-2_max: 613
    cleaning_beam_agent-2_mean: 450.79
    cleaning_beam_agent-2_min: 272
    cleaning_beam_agent-3_max: 116
    cleaning_beam_agent-3_mean: 17.64
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 619
    cleaning_beam_agent-4_mean: 505.57
    cleaning_beam_agent-4_min: 336
    cleaning_beam_agent-5_max: 84
    cleaning_beam_agent-5_mean: 17.32
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-36-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1183.0000000000264
  episode_reward_mean: 1017.6099999999869
  episode_reward_min: 611.0000000000083
  episodes_this_iter: 96
  episodes_total: 26592
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19750.277
    learner:
      agent-0:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0494319200515747
        entropy_coeff: 0.0017600000137463212
        kl: 0.00107379793189466
        model: {}
        policy_loss: -0.003142910310998559
        total_loss: -0.002852857578545809
        vf_explained_var: 0.011695072054862976
        vf_loss: 21.370521545410156
      agent-1:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0925008058547974
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013989116996526718
        model: {}
        policy_loss: -0.0037844497710466385
        total_loss: -0.0033061178401112556
        vf_explained_var: -0.06415525078773499
        vf_loss: 24.0113525390625
      agent-2:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9500745534896851
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014044065028429031
        model: {}
        policy_loss: -0.0029191230423748493
        total_loss: -0.00246978597715497
        vf_explained_var: -0.008199259638786316
        vf_loss: 21.2147159576416
      agent-3:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4810425937175751
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009494968689978123
        model: {}
        policy_loss: -0.002045856323093176
        total_loss: -0.0009670928120613098
        vf_explained_var: 0.09154301881790161
        vf_loss: 19.253965377807617
      agent-4:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8643838763237
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019311551004648209
        model: {}
        policy_loss: -0.003722495399415493
        total_loss: -0.0031403014436364174
        vf_explained_var: 0.017893508076667786
        vf_loss: 21.035152435302734
      agent-5:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5686729550361633
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007464761729352176
        model: {}
        policy_loss: -0.0027268705889582634
        total_loss: -0.0017587218899279833
        vf_explained_var: 0.08172677457332611
        vf_loss: 19.69013786315918
    load_time_ms: 12902.586
    num_steps_sampled: 26592000
    num_steps_trained: 26592000
    sample_time_ms: 90713.793
    update_time_ms: 23.66
  iterations_since_restore: 217
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.351704545454545
    ram_util_percent: 10.154545454545454
  pid: 24061
  policy_reward_max:
    agent-0: 197.1666666666663
    agent-1: 197.1666666666663
    agent-2: 197.1666666666663
    agent-3: 197.1666666666663
    agent-4: 197.1666666666663
    agent-5: 197.1666666666663
  policy_reward_mean:
    agent-0: 169.6016666666665
    agent-1: 169.6016666666665
    agent-2: 169.6016666666665
    agent-3: 169.6016666666665
    agent-4: 169.6016666666665
    agent-5: 169.6016666666665
  policy_reward_min:
    agent-0: 101.83333333333336
    agent-1: 101.83333333333336
    agent-2: 101.83333333333336
    agent-3: 101.83333333333336
    agent-4: 101.83333333333336
    agent-5: 101.83333333333336
  sampler_perf:
    mean_env_wait_ms: 24.504332730739005
    mean_inference_ms: 12.241307839675152
    mean_processing_ms: 50.72760372744665
  time_since_restore: 27772.036624908447
  time_this_iter_s: 123.61158466339111
  time_total_s: 36898.04843878746
  timestamp: 1637051813
  timesteps_since_restore: 20832000
  timesteps_this_iter: 96000
  timesteps_total: 26592000
  training_iteration: 277
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    277 |            36898 | 26592000 |  1017.61 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 46
    apples_agent-0_mean: 2.61
    apples_agent-0_min: 0
    apples_agent-1_max: 107
    apples_agent-1_mean: 34.33
    apples_agent-1_min: 0
    apples_agent-2_max: 79
    apples_agent-2_mean: 4.74
    apples_agent-2_min: 0
    apples_agent-3_max: 182
    apples_agent-3_mean: 108.36
    apples_agent-3_min: 61
    apples_agent-4_max: 74
    apples_agent-4_mean: 1.68
    apples_agent-4_min: 0
    apples_agent-5_max: 185
    apples_agent-5_mean: 83.96
    apples_agent-5_min: 50
    cleaning_beam_agent-0_max: 562
    cleaning_beam_agent-0_mean: 446.96
    cleaning_beam_agent-0_min: 256
    cleaning_beam_agent-1_max: 584
    cleaning_beam_agent-1_mean: 267.42
    cleaning_beam_agent-1_min: 72
    cleaning_beam_agent-2_max: 622
    cleaning_beam_agent-2_mean: 442.47
    cleaning_beam_agent-2_min: 209
    cleaning_beam_agent-3_max: 93
    cleaning_beam_agent-3_mean: 14.55
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 593
    cleaning_beam_agent-4_mean: 494.29
    cleaning_beam_agent-4_min: 294
    cleaning_beam_agent-5_max: 79
    cleaning_beam_agent-5_mean: 22.95
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-38-56
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1164.9999999999923
  episode_reward_mean: 997.2599999999875
  episode_reward_min: 577.0000000000058
  episodes_this_iter: 96
  episodes_total: 26688
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19729.703
    learner:
      agent-0:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0557305812835693
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012679400388151407
        model: {}
        policy_loss: -0.0034090690314769745
        total_loss: -0.002959602512419224
        vf_explained_var: 0.0007816404104232788
        vf_loss: 23.07549285888672
      agent-1:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0984684228897095
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011890454916283488
        model: {}
        policy_loss: -0.0037412613164633512
        total_loss: -0.0031533928122371435
        vf_explained_var: -0.06509906053543091
        vf_loss: 25.21170425415039
      agent-2:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9534931778907776
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011745053343474865
        model: {}
        policy_loss: -0.0031026825308799744
        total_loss: -0.0025482680648565292
        vf_explained_var: 0.018329381942749023
        vf_loss: 22.32560920715332
      agent-3:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4662618041038513
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012961344327777624
        model: {}
        policy_loss: -0.002161282580345869
        total_loss: -0.0008867252618074417
        vf_explained_var: 0.0796852558851242
        vf_loss: 20.951786041259766
      agent-4:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8654333353042603
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013828445225954056
        model: {}
        policy_loss: -0.0037835901603102684
        total_loss: -0.0031489962711930275
        vf_explained_var: 0.054498568177223206
        vf_loss: 21.577573776245117
      agent-5:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6097167134284973
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008356798207387328
        model: {}
        policy_loss: -0.002763595897704363
        total_loss: -0.001721477136015892
        vf_explained_var: 0.08786167204380035
        vf_loss: 21.152172088623047
    load_time_ms: 12909.188
    num_steps_sampled: 26688000
    num_steps_trained: 26688000
    sample_time_ms: 90775.399
    update_time_ms: 23.783
  iterations_since_restore: 218
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.395454545454548
    ram_util_percent: 10.155113636363636
  pid: 24061
  policy_reward_max:
    agent-0: 194.1666666666667
    agent-1: 194.1666666666667
    agent-2: 194.1666666666667
    agent-3: 194.1666666666667
    agent-4: 194.1666666666667
    agent-5: 194.1666666666667
  policy_reward_mean:
    agent-0: 166.20999999999987
    agent-1: 166.20999999999987
    agent-2: 166.20999999999987
    agent-3: 166.20999999999987
    agent-4: 166.20999999999987
    agent-5: 166.20999999999987
  policy_reward_min:
    agent-0: 96.1666666666667
    agent-1: 96.1666666666667
    agent-2: 96.1666666666667
    agent-3: 96.1666666666667
    agent-4: 96.1666666666667
    agent-5: 96.1666666666667
  sampler_perf:
    mean_env_wait_ms: 24.508384436179018
    mean_inference_ms: 12.240709491288618
    mean_processing_ms: 50.72437713114143
  time_since_restore: 27895.76138496399
  time_this_iter_s: 123.72476005554199
  time_total_s: 37021.773198843
  timestamp: 1637051936
  timesteps_since_restore: 20928000
  timesteps_this_iter: 96000
  timesteps_total: 26688000
  training_iteration: 278
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    278 |          37021.8 | 26688000 |   997.26 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 62
    apples_agent-0_mean: 2.5
    apples_agent-0_min: 0
    apples_agent-1_max: 139
    apples_agent-1_mean: 36.2
    apples_agent-1_min: 0
    apples_agent-2_max: 132
    apples_agent-2_mean: 5.4
    apples_agent-2_min: 0
    apples_agent-3_max: 175
    apples_agent-3_mean: 110.88
    apples_agent-3_min: 66
    apples_agent-4_max: 18
    apples_agent-4_mean: 0.21
    apples_agent-4_min: 0
    apples_agent-5_max: 187
    apples_agent-5_mean: 82.09
    apples_agent-5_min: 41
    cleaning_beam_agent-0_max: 574
    cleaning_beam_agent-0_mean: 454.13
    cleaning_beam_agent-0_min: 249
    cleaning_beam_agent-1_max: 435
    cleaning_beam_agent-1_mean: 254.9
    cleaning_beam_agent-1_min: 105
    cleaning_beam_agent-2_max: 615
    cleaning_beam_agent-2_mean: 433.88
    cleaning_beam_agent-2_min: 223
    cleaning_beam_agent-3_max: 79
    cleaning_beam_agent-3_mean: 16.57
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 582
    cleaning_beam_agent-4_mean: 489.08
    cleaning_beam_agent-4_min: 301
    cleaning_beam_agent-5_max: 123
    cleaning_beam_agent-5_mean: 23.03
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-41-00
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1206.9999999999918
  episode_reward_mean: 1009.2499999999881
  episode_reward_min: 531.0000000000055
  episodes_this_iter: 96
  episodes_total: 26784
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19731.246
    learner:
      agent-0:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0487401485443115
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014637773856520653
        model: {}
        policy_loss: -0.0031112139113247395
        total_loss: -0.002757738810032606
        vf_explained_var: 0.025032147765159607
        vf_loss: 21.99258041381836
      agent-1:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1192612648010254
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016860530013218522
        model: {}
        policy_loss: -0.003997755236923695
        total_loss: -0.0036029722541570663
        vf_explained_var: -0.02097952365875244
        vf_loss: 23.64682960510254
      agent-2:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9620605707168579
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011750380508601665
        model: {}
        policy_loss: -0.0026987725868821144
        total_loss: -0.00220327521674335
        vf_explained_var: 0.014648213982582092
        vf_loss: 21.88722801208496
      agent-3:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48302188515663147
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007463598158210516
        model: {}
        policy_loss: -0.002224650699645281
        total_loss: -0.0010334597900509834
        vf_explained_var: 0.07571664452552795
        vf_loss: 20.413089752197266
      agent-4:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8741111159324646
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012516287388280034
        model: {}
        policy_loss: -0.0032619310077279806
        total_loss: -0.0026342340279370546
        vf_explained_var: 0.03183908760547638
        vf_loss: 21.661354064941406
      agent-5:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5890141129493713
        entropy_coeff: 0.0017600000137463212
        kl: 0.000663489568978548
        model: {}
        policy_loss: -0.0026177437976002693
        total_loss: -0.0016187271103262901
        vf_explained_var: 0.09638316929340363
        vf_loss: 20.356807708740234
    load_time_ms: 12894.637
    num_steps_sampled: 26784000
    num_steps_trained: 26784000
    sample_time_ms: 90750.622
    update_time_ms: 24.081
  iterations_since_restore: 219
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.461931818181819
    ram_util_percent: 10.154545454545454
  pid: 24061
  policy_reward_max:
    agent-0: 201.16666666666674
    agent-1: 201.16666666666674
    agent-2: 201.16666666666674
    agent-3: 201.16666666666674
    agent-4: 201.16666666666674
    agent-5: 201.16666666666674
  policy_reward_mean:
    agent-0: 168.20833333333326
    agent-1: 168.20833333333326
    agent-2: 168.20833333333326
    agent-3: 168.20833333333326
    agent-4: 168.20833333333326
    agent-5: 168.20833333333326
  policy_reward_min:
    agent-0: 88.50000000000023
    agent-1: 88.50000000000023
    agent-2: 88.50000000000023
    agent-3: 88.50000000000023
    agent-4: 88.50000000000023
    agent-5: 88.50000000000023
  sampler_perf:
    mean_env_wait_ms: 24.51252982897928
    mean_inference_ms: 12.240139575563953
    mean_processing_ms: 50.721522142346146
  time_since_restore: 28018.84566473961
  time_this_iter_s: 123.0842797756195
  time_total_s: 37144.85747861862
  timestamp: 1637052060
  timesteps_since_restore: 21024000
  timesteps_this_iter: 96000
  timesteps_total: 26784000
  training_iteration: 279
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    279 |          37144.9 | 26784000 |  1009.25 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 42
    apples_agent-0_mean: 2.35
    apples_agent-0_min: 0
    apples_agent-1_max: 130
    apples_agent-1_mean: 34.08
    apples_agent-1_min: 0
    apples_agent-2_max: 174
    apples_agent-2_mean: 4.34
    apples_agent-2_min: 0
    apples_agent-3_max: 181
    apples_agent-3_mean: 115.92
    apples_agent-3_min: 45
    apples_agent-4_max: 87
    apples_agent-4_mean: 1.7
    apples_agent-4_min: 0
    apples_agent-5_max: 124
    apples_agent-5_mean: 83.5
    apples_agent-5_min: 53
    cleaning_beam_agent-0_max: 606
    cleaning_beam_agent-0_mean: 454.65
    cleaning_beam_agent-0_min: 308
    cleaning_beam_agent-1_max: 510
    cleaning_beam_agent-1_mean: 262.84
    cleaning_beam_agent-1_min: 89
    cleaning_beam_agent-2_max: 588
    cleaning_beam_agent-2_mean: 412.97
    cleaning_beam_agent-2_min: 98
    cleaning_beam_agent-3_max: 98
    cleaning_beam_agent-3_mean: 16.16
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 589
    cleaning_beam_agent-4_mean: 493.91
    cleaning_beam_agent-4_min: 327
    cleaning_beam_agent-5_max: 87
    cleaning_beam_agent-5_mean: 22.38
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-43-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1166.0000000000136
  episode_reward_mean: 1000.7899999999879
  episode_reward_min: 595.0000000000024
  episodes_this_iter: 96
  episodes_total: 26880
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19762.713
    learner:
      agent-0:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0510531663894653
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015664445236325264
        model: {}
        policy_loss: -0.0035081887617707253
        total_loss: -0.0031918524764478207
        vf_explained_var: 0.019933462142944336
        vf_loss: 21.661903381347656
      agent-1:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0900349617004395
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010188189335167408
        model: {}
        policy_loss: -0.003751179203391075
        total_loss: -0.003268472384661436
        vf_explained_var: -0.05630168318748474
        vf_loss: 24.011661529541016
      agent-2:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9775577187538147
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018286309204995632
        model: {}
        policy_loss: -0.0035254936665296555
        total_loss: -0.00312007125467062
        vf_explained_var: 0.025943443179130554
        vf_loss: 21.25922966003418
      agent-3:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4881577789783478
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012783680576831102
        model: {}
        policy_loss: -0.0025292644277215004
        total_loss: -0.001397634856402874
        vf_explained_var: 0.08226317167282104
        vf_loss: 19.90789222717285
      agent-4:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8760132193565369
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013408977538347244
        model: {}
        policy_loss: -0.00367934163659811
        total_loss: -0.0030526500195264816
        vf_explained_var: 0.011407002806663513
        vf_loss: 21.684738159179688
      agent-5:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6030129194259644
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005789590650238097
        model: {}
        policy_loss: -0.0028500466141849756
        total_loss: -0.0018885221797972918
        vf_explained_var: 0.08890925347805023
        vf_loss: 20.228240966796875
    load_time_ms: 12899.189
    num_steps_sampled: 26880000
    num_steps_trained: 26880000
    sample_time_ms: 90792.342
    update_time_ms: 23.027
  iterations_since_restore: 220
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.45625
    ram_util_percent: 10.084090909090909
  pid: 24061
  policy_reward_max:
    agent-0: 194.3333333333331
    agent-1: 194.3333333333331
    agent-2: 194.3333333333331
    agent-3: 194.3333333333331
    agent-4: 194.3333333333331
    agent-5: 194.3333333333331
  policy_reward_mean:
    agent-0: 166.79833333333326
    agent-1: 166.79833333333326
    agent-2: 166.79833333333326
    agent-3: 166.79833333333326
    agent-4: 166.79833333333326
    agent-5: 166.79833333333326
  policy_reward_min:
    agent-0: 99.16666666666686
    agent-1: 99.16666666666686
    agent-2: 99.16666666666686
    agent-3: 99.16666666666686
    agent-4: 99.16666666666686
    agent-5: 99.16666666666686
  sampler_perf:
    mean_env_wait_ms: 24.516090253713216
    mean_inference_ms: 12.239451599467516
    mean_processing_ms: 50.71957729872075
  time_since_restore: 28142.786444187164
  time_this_iter_s: 123.94077944755554
  time_total_s: 37268.79825806618
  timestamp: 1637052184
  timesteps_since_restore: 21120000
  timesteps_this_iter: 96000
  timesteps_total: 26880000
  training_iteration: 280
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    280 |          37268.8 | 26880000 |  1000.79 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 133
    apples_agent-0_mean: 5.19
    apples_agent-0_min: 0
    apples_agent-1_max: 121
    apples_agent-1_mean: 31.74
    apples_agent-1_min: 0
    apples_agent-2_max: 51
    apples_agent-2_mean: 2.12
    apples_agent-2_min: 0
    apples_agent-3_max: 172
    apples_agent-3_mean: 119.33
    apples_agent-3_min: 73
    apples_agent-4_max: 40
    apples_agent-4_mean: 1.13
    apples_agent-4_min: 0
    apples_agent-5_max: 137
    apples_agent-5_mean: 82.46
    apples_agent-5_min: 46
    cleaning_beam_agent-0_max: 545
    cleaning_beam_agent-0_mean: 438.48
    cleaning_beam_agent-0_min: 224
    cleaning_beam_agent-1_max: 594
    cleaning_beam_agent-1_mean: 276.22
    cleaning_beam_agent-1_min: 100
    cleaning_beam_agent-2_max: 582
    cleaning_beam_agent-2_mean: 446.27
    cleaning_beam_agent-2_min: 212
    cleaning_beam_agent-3_max: 82
    cleaning_beam_agent-3_mean: 18.37
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 641
    cleaning_beam_agent-4_mean: 491.09
    cleaning_beam_agent-4_min: 263
    cleaning_beam_agent-5_max: 122
    cleaning_beam_agent-5_mean: 23.58
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-45-07
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1203.0000000000139
  episode_reward_mean: 1005.6499999999895
  episode_reward_min: 646.0000000000133
  episodes_this_iter: 96
  episodes_total: 26976
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19744.43
    learner:
      agent-0:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0534309148788452
        entropy_coeff: 0.0017600000137463212
        kl: 0.002436885377392173
        model: {}
        policy_loss: -0.003667868673801422
        total_loss: -0.003239900805056095
        vf_explained_var: 0.01791965961456299
        vf_loss: 22.8200626373291
      agent-1:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 1.091578483581543
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022396910935640335
        model: {}
        policy_loss: -0.0039712958969175816
        total_loss: -0.0034987558610737324
        vf_explained_var: -0.017735108733177185
        vf_loss: 23.937211990356445
      agent-2:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9515569806098938
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020516738295555115
        model: {}
        policy_loss: -0.0032100570388138294
        total_loss: -0.0027401093393564224
        vf_explained_var: 0.054736360907554626
        vf_loss: 21.446887969970703
      agent-3:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.480476438999176
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012210311833769083
        model: {}
        policy_loss: -0.0022826241329312325
        total_loss: -0.0010554520413279533
        vf_explained_var: 0.08342768251895905
        vf_loss: 20.728118896484375
      agent-4:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8719159364700317
        entropy_coeff: 0.0017600000137463212
        kl: 0.00128295982722193
        model: {}
        policy_loss: -0.0035008196718990803
        total_loss: -0.0028393485117703676
        vf_explained_var: 0.04398141801357269
        vf_loss: 21.960477828979492
      agent-5:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5974530577659607
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006507443031296134
        model: {}
        policy_loss: -0.002660689875483513
        total_loss: -0.0015779645182192326
        vf_explained_var: 0.078497976064682
        vf_loss: 21.34243392944336
    load_time_ms: 12916.632
    num_steps_sampled: 26976000
    num_steps_trained: 26976000
    sample_time_ms: 90640.164
    update_time_ms: 23.194
  iterations_since_restore: 221
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.470454545454546
    ram_util_percent: 10.13806818181818
  pid: 24061
  policy_reward_max:
    agent-0: 200.4999999999996
    agent-1: 200.4999999999996
    agent-2: 200.4999999999996
    agent-3: 200.4999999999996
    agent-4: 200.4999999999996
    agent-5: 200.4999999999996
  policy_reward_mean:
    agent-0: 167.6083333333333
    agent-1: 167.6083333333333
    agent-2: 167.6083333333333
    agent-3: 167.6083333333333
    agent-4: 167.6083333333333
    agent-5: 167.6083333333333
  policy_reward_min:
    agent-0: 107.66666666666698
    agent-1: 107.66666666666698
    agent-2: 107.66666666666698
    agent-3: 107.66666666666698
    agent-4: 107.66666666666698
    agent-5: 107.66666666666698
  sampler_perf:
    mean_env_wait_ms: 24.520145151319248
    mean_inference_ms: 12.238960235544228
    mean_processing_ms: 50.71730150752651
  time_since_restore: 28265.699640989304
  time_this_iter_s: 122.91319680213928
  time_total_s: 37391.71145486832
  timestamp: 1637052307
  timesteps_since_restore: 21216000
  timesteps_this_iter: 96000
  timesteps_total: 26976000
  training_iteration: 281
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    281 |          37391.7 | 26976000 |  1005.65 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 50
    apples_agent-0_mean: 2.18
    apples_agent-0_min: 0
    apples_agent-1_max: 121
    apples_agent-1_mean: 38.11
    apples_agent-1_min: 0
    apples_agent-2_max: 88
    apples_agent-2_mean: 3.26
    apples_agent-2_min: 0
    apples_agent-3_max: 180
    apples_agent-3_mean: 115.34
    apples_agent-3_min: 65
    apples_agent-4_max: 6
    apples_agent-4_mean: 0.11
    apples_agent-4_min: 0
    apples_agent-5_max: 134
    apples_agent-5_mean: 82.76
    apples_agent-5_min: 38
    cleaning_beam_agent-0_max: 636
    cleaning_beam_agent-0_mean: 448.59
    cleaning_beam_agent-0_min: 310
    cleaning_beam_agent-1_max: 460
    cleaning_beam_agent-1_mean: 257.02
    cleaning_beam_agent-1_min: 129
    cleaning_beam_agent-2_max: 541
    cleaning_beam_agent-2_mean: 411.54
    cleaning_beam_agent-2_min: 194
    cleaning_beam_agent-3_max: 134
    cleaning_beam_agent-3_mean: 18.95
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 647
    cleaning_beam_agent-4_mean: 495.5
    cleaning_beam_agent-4_min: 367
    cleaning_beam_agent-5_max: 98
    cleaning_beam_agent-5_mean: 22.24
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-47-11
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1224.0000000000125
  episode_reward_mean: 1019.8199999999904
  episode_reward_min: 528.0000000000017
  episodes_this_iter: 96
  episodes_total: 27072
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19764.854
    learner:
      agent-0:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0517785549163818
        entropy_coeff: 0.0017600000137463212
        kl: 0.00122585310600698
        model: {}
        policy_loss: -0.003012394765391946
        total_loss: -0.0026495056226849556
        vf_explained_var: -0.006022945046424866
        vf_loss: 22.14019012451172
      agent-1:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 1.109436273574829
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012825377052649856
        model: {}
        policy_loss: -0.003970874473452568
        total_loss: -0.0034778397530317307
        vf_explained_var: -0.06527021527290344
        vf_loss: 24.456398010253906
      agent-2:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9776167273521423
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016444344073534012
        model: {}
        policy_loss: -0.003352633211761713
        total_loss: -0.002898138016462326
        vf_explained_var: -0.002227991819381714
        vf_loss: 21.751012802124023
      agent-3:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4831051826477051
        entropy_coeff: 0.0017600000137463212
        kl: 0.001123601570725441
        model: {}
        policy_loss: -0.0023257331922650337
        total_loss: -0.0011243131011724472
        vf_explained_var: 0.046068236231803894
        vf_loss: 20.516855239868164
      agent-4:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8566139936447144
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016050091944634914
        model: {}
        policy_loss: -0.0036924611777067184
        total_loss: -0.0031134141609072685
        vf_explained_var: 0.03984127938747406
        vf_loss: 20.866886138916016
      agent-5:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5762069821357727
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008456070208922029
        model: {}
        policy_loss: -0.002744956873357296
        total_loss: -0.0016909507103264332
        vf_explained_var: 0.05958060920238495
        vf_loss: 20.68130111694336
    load_time_ms: 12908.911
    num_steps_sampled: 27072000
    num_steps_trained: 27072000
    sample_time_ms: 90664.656
    update_time_ms: 22.742
  iterations_since_restore: 222
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.320338983050846
    ram_util_percent: 10.149152542372882
  pid: 24061
  policy_reward_max:
    agent-0: 203.99999999999966
    agent-1: 203.99999999999966
    agent-2: 203.99999999999966
    agent-3: 203.99999999999966
    agent-4: 203.99999999999966
    agent-5: 203.99999999999966
  policy_reward_mean:
    agent-0: 169.96999999999986
    agent-1: 169.96999999999986
    agent-2: 169.96999999999986
    agent-3: 169.96999999999986
    agent-4: 169.96999999999986
    agent-5: 169.96999999999986
  policy_reward_min:
    agent-0: 87.99999999999997
    agent-1: 87.99999999999997
    agent-2: 87.99999999999997
    agent-3: 87.99999999999997
    agent-4: 87.99999999999997
    agent-5: 87.99999999999997
  sampler_perf:
    mean_env_wait_ms: 24.52361575113988
    mean_inference_ms: 12.238588702853242
    mean_processing_ms: 50.71526381058231
  time_since_restore: 28389.742092370987
  time_this_iter_s: 124.04245138168335
  time_total_s: 37515.75390625
  timestamp: 1637052431
  timesteps_since_restore: 21312000
  timesteps_this_iter: 96000
  timesteps_total: 27072000
  training_iteration: 282
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    282 |          37515.8 | 27072000 |  1019.82 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 47
    apples_agent-0_mean: 3.95
    apples_agent-0_min: 0
    apples_agent-1_max: 128
    apples_agent-1_mean: 37.45
    apples_agent-1_min: 0
    apples_agent-2_max: 44
    apples_agent-2_mean: 1.38
    apples_agent-2_min: 0
    apples_agent-3_max: 258
    apples_agent-3_mean: 123.16
    apples_agent-3_min: 59
    apples_agent-4_max: 68
    apples_agent-4_mean: 1.57
    apples_agent-4_min: 0
    apples_agent-5_max: 294
    apples_agent-5_mean: 85.49
    apples_agent-5_min: 24
    cleaning_beam_agent-0_max: 571
    cleaning_beam_agent-0_mean: 440.22
    cleaning_beam_agent-0_min: 157
    cleaning_beam_agent-1_max: 504
    cleaning_beam_agent-1_mean: 268.34
    cleaning_beam_agent-1_min: 86
    cleaning_beam_agent-2_max: 549
    cleaning_beam_agent-2_mean: 426.39
    cleaning_beam_agent-2_min: 190
    cleaning_beam_agent-3_max: 100
    cleaning_beam_agent-3_mean: 18.79
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 613
    cleaning_beam_agent-4_mean: 499.3
    cleaning_beam_agent-4_min: 356
    cleaning_beam_agent-5_max: 208
    cleaning_beam_agent-5_mean: 21.95
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-49-15
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1207.0000000000011
  episode_reward_mean: 995.8299999999887
  episode_reward_min: 375.0000000000019
  episodes_this_iter: 96
  episodes_total: 27168
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19733.423
    learner:
      agent-0:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0600926876068115
        entropy_coeff: 0.0017600000137463212
        kl: 0.001403760863468051
        model: {}
        policy_loss: -0.0030894249211996794
        total_loss: -0.0028757676482200623
        vf_explained_var: 0.035389289259910583
        vf_loss: 20.794166564941406
      agent-1:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0874931812286377
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014007332501932979
        model: {}
        policy_loss: -0.003928320482373238
        total_loss: -0.003478133585304022
        vf_explained_var: -0.07095155119895935
        vf_loss: 23.64173698425293
      agent-2:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9742327928543091
        entropy_coeff: 0.0017600000137463212
        kl: 0.001572188688442111
        model: {}
        policy_loss: -0.003420080989599228
        total_loss: -0.0030459912959486246
        vf_explained_var: 0.021112069487571716
        vf_loss: 20.887388229370117
      agent-3:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47942912578582764
        entropy_coeff: 0.0017600000137463212
        kl: 0.001372822211124003
        model: {}
        policy_loss: -0.0024604354985058308
        total_loss: -0.0012898831628262997
        vf_explained_var: 0.0562271922826767
        vf_loss: 20.143489837646484
      agent-4:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8520984053611755
        entropy_coeff: 0.0017600000137463212
        kl: 0.002049095230177045
        model: {}
        policy_loss: -0.0037994571030139923
        total_loss: -0.0032111271284520626
        vf_explained_var: 0.023221150040626526
        vf_loss: 20.88021469116211
      agent-5:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5893665552139282
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013488309923559427
        model: {}
        policy_loss: -0.0027551897801458836
        total_loss: -0.0018209959380328655
        vf_explained_var: 0.0829375684261322
        vf_loss: 19.714767456054688
    load_time_ms: 12894.123
    num_steps_sampled: 27168000
    num_steps_trained: 27168000
    sample_time_ms: 90675.005
    update_time_ms: 23.237
  iterations_since_restore: 223
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.478285714285713
    ram_util_percent: 10.13885714285714
  pid: 24061
  policy_reward_max:
    agent-0: 201.16666666666637
    agent-1: 201.16666666666637
    agent-2: 201.16666666666637
    agent-3: 201.16666666666637
    agent-4: 201.16666666666637
    agent-5: 201.16666666666637
  policy_reward_mean:
    agent-0: 165.97166666666658
    agent-1: 165.97166666666658
    agent-2: 165.97166666666658
    agent-3: 165.97166666666658
    agent-4: 165.97166666666658
    agent-5: 165.97166666666658
  policy_reward_min:
    agent-0: 62.49999999999983
    agent-1: 62.49999999999983
    agent-2: 62.49999999999983
    agent-3: 62.49999999999983
    agent-4: 62.49999999999983
    agent-5: 62.49999999999983
  sampler_perf:
    mean_env_wait_ms: 24.527879443448914
    mean_inference_ms: 12.238135034890693
    mean_processing_ms: 50.713604964801725
  time_since_restore: 28512.968503713608
  time_this_iter_s: 123.22641134262085
  time_total_s: 37638.98031759262
  timestamp: 1637052555
  timesteps_since_restore: 21408000
  timesteps_this_iter: 96000
  timesteps_total: 27168000
  training_iteration: 283
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    283 |            37639 | 27168000 |   995.83 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 72
    apples_agent-0_mean: 2.79
    apples_agent-0_min: 0
    apples_agent-1_max: 135
    apples_agent-1_mean: 32.74
    apples_agent-1_min: 0
    apples_agent-2_max: 104
    apples_agent-2_mean: 4.06
    apples_agent-2_min: 0
    apples_agent-3_max: 220
    apples_agent-3_mean: 116.19
    apples_agent-3_min: 24
    apples_agent-4_max: 67
    apples_agent-4_mean: 1.74
    apples_agent-4_min: 0
    apples_agent-5_max: 167
    apples_agent-5_mean: 82.51
    apples_agent-5_min: 25
    cleaning_beam_agent-0_max: 569
    cleaning_beam_agent-0_mean: 448.38
    cleaning_beam_agent-0_min: 166
    cleaning_beam_agent-1_max: 530
    cleaning_beam_agent-1_mean: 267.92
    cleaning_beam_agent-1_min: 108
    cleaning_beam_agent-2_max: 553
    cleaning_beam_agent-2_mean: 434.32
    cleaning_beam_agent-2_min: 238
    cleaning_beam_agent-3_max: 137
    cleaning_beam_agent-3_mean: 18.65
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 676
    cleaning_beam_agent-4_mean: 507.96
    cleaning_beam_agent-4_min: 347
    cleaning_beam_agent-5_max: 125
    cleaning_beam_agent-5_mean: 22.08
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-51-18
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1248.000000000001
  episode_reward_mean: 1007.679999999991
  episode_reward_min: 222.9999999999975
  episodes_this_iter: 96
  episodes_total: 27264
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19773.902
    learner:
      agent-0:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 1.054824948310852
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015674957539886236
        model: {}
        policy_loss: -0.0032710907980799675
        total_loss: -0.002719614189118147
        vf_explained_var: 0.0014661997556686401
        vf_loss: 24.079681396484375
      agent-1:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0905308723449707
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012940220767632127
        model: {}
        policy_loss: -0.003932294435799122
        total_loss: -0.003342051524668932
        vf_explained_var: -0.01672367751598358
        vf_loss: 25.09575080871582
      agent-2:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9544783234596252
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012862691655755043
        model: {}
        policy_loss: -0.0033333729952573776
        total_loss: -0.0028343815356492996
        vf_explained_var: 0.0855422168970108
        vf_loss: 21.78875160217285
      agent-3:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47423481941223145
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008687488734722137
        model: {}
        policy_loss: -0.0020942287519574165
        total_loss: -0.0008541382849216461
        vf_explained_var: 0.1200205534696579
        vf_loss: 20.74742889404297
      agent-4:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8515876531600952
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015248069539666176
        model: {}
        policy_loss: -0.0035741969477385283
        total_loss: -0.002737106755375862
        vf_explained_var: 0.017731428146362305
        vf_loss: 23.358871459960938
      agent-5:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5714493989944458
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008065002621151507
        model: {}
        policy_loss: -0.0026833610609173775
        total_loss: -0.0015547750517725945
        vf_explained_var: 0.10577350854873657
        vf_loss: 21.34334945678711
    load_time_ms: 12899.969
    num_steps_sampled: 27264000
    num_steps_trained: 27264000
    sample_time_ms: 90670.509
    update_time_ms: 22.368
  iterations_since_restore: 224
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.440340909090908
    ram_util_percent: 10.125
  pid: 24061
  policy_reward_max:
    agent-0: 208.00000000000028
    agent-1: 208.00000000000028
    agent-2: 208.00000000000028
    agent-3: 208.00000000000028
    agent-4: 208.00000000000028
    agent-5: 208.00000000000028
  policy_reward_mean:
    agent-0: 167.94666666666657
    agent-1: 167.94666666666657
    agent-2: 167.94666666666657
    agent-3: 167.94666666666657
    agent-4: 167.94666666666657
    agent-5: 167.94666666666657
  policy_reward_min:
    agent-0: 37.16666666666668
    agent-1: 37.16666666666668
    agent-2: 37.16666666666668
    agent-3: 37.16666666666668
    agent-4: 37.16666666666668
    agent-5: 37.16666666666668
  sampler_perf:
    mean_env_wait_ms: 24.531377892286216
    mean_inference_ms: 12.237565879449596
    mean_processing_ms: 50.710877868179395
  time_since_restore: 28635.928440332413
  time_this_iter_s: 122.95993661880493
  time_total_s: 37761.940254211426
  timestamp: 1637052678
  timesteps_since_restore: 21504000
  timesteps_this_iter: 96000
  timesteps_total: 27264000
  training_iteration: 284
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    284 |          37761.9 | 27264000 |  1007.68 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 29
    apples_agent-0_mean: 1.43
    apples_agent-0_min: 0
    apples_agent-1_max: 119
    apples_agent-1_mean: 32.19
    apples_agent-1_min: 0
    apples_agent-2_max: 122
    apples_agent-2_mean: 4.53
    apples_agent-2_min: 0
    apples_agent-3_max: 207
    apples_agent-3_mean: 118.73
    apples_agent-3_min: 51
    apples_agent-4_max: 33
    apples_agent-4_mean: 0.42
    apples_agent-4_min: 0
    apples_agent-5_max: 146
    apples_agent-5_mean: 84.91
    apples_agent-5_min: 54
    cleaning_beam_agent-0_max: 593
    cleaning_beam_agent-0_mean: 461.26
    cleaning_beam_agent-0_min: 264
    cleaning_beam_agent-1_max: 573
    cleaning_beam_agent-1_mean: 282.61
    cleaning_beam_agent-1_min: 129
    cleaning_beam_agent-2_max: 563
    cleaning_beam_agent-2_mean: 427.26
    cleaning_beam_agent-2_min: 239
    cleaning_beam_agent-3_max: 157
    cleaning_beam_agent-3_mean: 16.17
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 594
    cleaning_beam_agent-4_mean: 513.38
    cleaning_beam_agent-4_min: 325
    cleaning_beam_agent-5_max: 51
    cleaning_beam_agent-5_mean: 17.04
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-53-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1248.000000000001
  episode_reward_mean: 1027.6499999999912
  episode_reward_min: 384.00000000000915
  episodes_this_iter: 96
  episodes_total: 27360
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19807.038
    learner:
      agent-0:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 1.053570032119751
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009093525586649776
        model: {}
        policy_loss: -0.0027296626940369606
        total_loss: -0.002285268157720566
        vf_explained_var: 0.014314264059066772
        vf_loss: 22.98676300048828
      agent-1:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1121329069137573
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012250213185325265
        model: {}
        policy_loss: -0.0037925823125988245
        total_loss: -0.003325029043480754
        vf_explained_var: -0.00732402503490448
        vf_loss: 24.249086380004883
      agent-2:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.958716869354248
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015615011798217893
        model: {}
        policy_loss: -0.0031121065840125084
        total_loss: -0.0026456452906131744
        vf_explained_var: 0.07233595848083496
        vf_loss: 21.538026809692383
      agent-3:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4598945379257202
        entropy_coeff: 0.0017600000137463212
        kl: 0.001304487930610776
        model: {}
        policy_loss: -0.002425895072519779
        total_loss: -0.001122497022151947
        vf_explained_var: 0.07329162955284119
        vf_loss: 21.128124237060547
      agent-4:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.858329176902771
        entropy_coeff: 0.0017600000137463212
        kl: 0.001068380894139409
        model: {}
        policy_loss: -0.003508917521685362
        total_loss: -0.002719382755458355
        vf_explained_var: 0.01646403968334198
        vf_loss: 23.00189781188965
      agent-5:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.562886118888855
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006846589385531843
        model: {}
        policy_loss: -0.0028018481098115444
        total_loss: -0.0016507827676832676
        vf_explained_var: 0.08773146569728851
        vf_loss: 21.417484283447266
    load_time_ms: 12903.159
    num_steps_sampled: 27360000
    num_steps_trained: 27360000
    sample_time_ms: 90744.561
    update_time_ms: 22.662
  iterations_since_restore: 225
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.43954802259887
    ram_util_percent: 10.137288135593218
  pid: 24061
  policy_reward_max:
    agent-0: 208.00000000000028
    agent-1: 208.00000000000028
    agent-2: 208.00000000000028
    agent-3: 208.00000000000028
    agent-4: 208.00000000000028
    agent-5: 208.00000000000028
  policy_reward_mean:
    agent-0: 171.2749999999999
    agent-1: 171.2749999999999
    agent-2: 171.2749999999999
    agent-3: 171.2749999999999
    agent-4: 171.2749999999999
    agent-5: 171.2749999999999
  policy_reward_min:
    agent-0: 63.99999999999982
    agent-1: 63.99999999999982
    agent-2: 63.99999999999982
    agent-3: 63.99999999999982
    agent-4: 63.99999999999982
    agent-5: 63.99999999999982
  sampler_perf:
    mean_env_wait_ms: 24.536152251158814
    mean_inference_ms: 12.23733447672835
    mean_processing_ms: 50.70949690587096
  time_since_restore: 28760.58606481552
  time_this_iter_s: 124.65762448310852
  time_total_s: 37886.597878694534
  timestamp: 1637052803
  timesteps_since_restore: 21600000
  timesteps_this_iter: 96000
  timesteps_total: 27360000
  training_iteration: 285
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    285 |          37886.6 | 27360000 |  1027.65 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 33
    apples_agent-0_mean: 1.85
    apples_agent-0_min: 0
    apples_agent-1_max: 124
    apples_agent-1_mean: 34.08
    apples_agent-1_min: 0
    apples_agent-2_max: 28
    apples_agent-2_mean: 1.11
    apples_agent-2_min: 0
    apples_agent-3_max: 179
    apples_agent-3_mean: 118.04
    apples_agent-3_min: 51
    apples_agent-4_max: 38
    apples_agent-4_mean: 1.21
    apples_agent-4_min: 0
    apples_agent-5_max: 134
    apples_agent-5_mean: 82.04
    apples_agent-5_min: 40
    cleaning_beam_agent-0_max: 581
    cleaning_beam_agent-0_mean: 458.07
    cleaning_beam_agent-0_min: 339
    cleaning_beam_agent-1_max: 588
    cleaning_beam_agent-1_mean: 260.18
    cleaning_beam_agent-1_min: 85
    cleaning_beam_agent-2_max: 605
    cleaning_beam_agent-2_mean: 440.21
    cleaning_beam_agent-2_min: 315
    cleaning_beam_agent-3_max: 157
    cleaning_beam_agent-3_mean: 17.18
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 585
    cleaning_beam_agent-4_mean: 499.03
    cleaning_beam_agent-4_min: 325
    cleaning_beam_agent-5_max: 60
    cleaning_beam_agent-5_mean: 18.18
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-55-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1205.0000000000068
  episode_reward_mean: 1025.0399999999895
  episode_reward_min: 384.00000000000915
  episodes_this_iter: 96
  episodes_total: 27456
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19793.635
    learner:
      agent-0:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0635520219802856
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015655800234526396
        model: {}
        policy_loss: -0.0031023668125271797
        total_loss: -0.0028067915700376034
        vf_explained_var: -0.0021590888500213623
        vf_loss: 21.674264907836914
      agent-1:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1030299663543701
        entropy_coeff: 0.0017600000137463212
        kl: 0.001869440427981317
        model: {}
        policy_loss: -0.0039533209055662155
        total_loss: -0.0034509780816733837
        vf_explained_var: -0.059984296560287476
        vf_loss: 24.43672752380371
      agent-2:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9649558067321777
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014446862041950226
        model: {}
        policy_loss: -0.0031769394408911467
        total_loss: -0.002742559416219592
        vf_explained_var: -0.004974782466888428
        vf_loss: 21.327030181884766
      agent-3:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45121854543685913
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011148779885843396
        model: {}
        policy_loss: -0.001996382139623165
        total_loss: -0.0007856162264943123
        vf_explained_var: 0.0448525995016098
        vf_loss: 20.049104690551758
      agent-4:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.866325855255127
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019315921235829592
        model: {}
        policy_loss: -0.0037271403707563877
        total_loss: -0.003127272240817547
        vf_explained_var: 0.014907494187355042
        vf_loss: 21.246034622192383
      agent-5:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.562523603439331
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009252309682779014
        model: {}
        policy_loss: -0.0022949781268835068
        total_loss: -0.0012154323048889637
        vf_explained_var: 0.05170685052871704
        vf_loss: 20.695842742919922
    load_time_ms: 12911.535
    num_steps_sampled: 27456000
    num_steps_trained: 27456000
    sample_time_ms: 90684.36
    update_time_ms: 21.921
  iterations_since_restore: 226
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.382386363636364
    ram_util_percent: 10.059659090909092
  pid: 24061
  policy_reward_max:
    agent-0: 200.83333333333283
    agent-1: 200.83333333333283
    agent-2: 200.83333333333283
    agent-3: 200.83333333333283
    agent-4: 200.83333333333283
    agent-5: 200.83333333333283
  policy_reward_mean:
    agent-0: 170.83999999999986
    agent-1: 170.83999999999986
    agent-2: 170.83999999999986
    agent-3: 170.83999999999986
    agent-4: 170.83999999999986
    agent-5: 170.83999999999986
  policy_reward_min:
    agent-0: 63.99999999999982
    agent-1: 63.99999999999982
    agent-2: 63.99999999999982
    agent-3: 63.99999999999982
    agent-4: 63.99999999999982
    agent-5: 63.99999999999982
  sampler_perf:
    mean_env_wait_ms: 24.53934362945446
    mean_inference_ms: 12.236797225729168
    mean_processing_ms: 50.706599548406395
  time_since_restore: 28883.539180994034
  time_this_iter_s: 122.95311617851257
  time_total_s: 38009.55099487305
  timestamp: 1637052926
  timesteps_since_restore: 21696000
  timesteps_this_iter: 96000
  timesteps_total: 27456000
  training_iteration: 286
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    286 |          38009.6 | 27456000 |  1025.04 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 52
    apples_agent-0_mean: 2.53
    apples_agent-0_min: 0
    apples_agent-1_max: 140
    apples_agent-1_mean: 33.56
    apples_agent-1_min: 0
    apples_agent-2_max: 48
    apples_agent-2_mean: 1.87
    apples_agent-2_min: 0
    apples_agent-3_max: 192
    apples_agent-3_mean: 112.15
    apples_agent-3_min: 76
    apples_agent-4_max: 62
    apples_agent-4_mean: 0.86
    apples_agent-4_min: 0
    apples_agent-5_max: 116
    apples_agent-5_mean: 82.11
    apples_agent-5_min: 44
    cleaning_beam_agent-0_max: 598
    cleaning_beam_agent-0_mean: 457.85
    cleaning_beam_agent-0_min: 275
    cleaning_beam_agent-1_max: 563
    cleaning_beam_agent-1_mean: 267.95
    cleaning_beam_agent-1_min: 121
    cleaning_beam_agent-2_max: 623
    cleaning_beam_agent-2_mean: 440.11
    cleaning_beam_agent-2_min: 287
    cleaning_beam_agent-3_max: 50
    cleaning_beam_agent-3_mean: 14.21
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 650
    cleaning_beam_agent-4_mean: 503.26
    cleaning_beam_agent-4_min: 398
    cleaning_beam_agent-5_max: 146
    cleaning_beam_agent-5_mean: 18.9
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-57-30
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1200.9999999999884
  episode_reward_mean: 1052.849999999992
  episode_reward_min: 646.0000000000032
  episodes_this_iter: 96
  episodes_total: 27552
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19780.05
    learner:
      agent-0:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0548510551452637
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015176241286098957
        model: {}
        policy_loss: -0.0031788465566933155
        total_loss: -0.002744421362876892
        vf_explained_var: 0.0007758289575576782
        vf_loss: 22.909631729125977
      agent-1:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1038978099822998
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015994921559467912
        model: {}
        policy_loss: -0.00406042393296957
        total_loss: -0.0034304996952414513
        vf_explained_var: -0.06946858763694763
        vf_loss: 25.727827072143555
      agent-2:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9729609489440918
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015764825511723757
        model: {}
        policy_loss: -0.0031638566870242357
        total_loss: -0.002708248095586896
        vf_explained_var: 0.03198157250881195
        vf_loss: 21.680208206176758
      agent-3:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4480510950088501
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011737988097593188
        model: {}
        policy_loss: -0.001977191772311926
        total_loss: -0.0006728288717567921
        vf_explained_var: 0.05257841944694519
        vf_loss: 20.929325103759766
      agent-4:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8591497540473938
        entropy_coeff: 0.0017600000137463212
        kl: 0.002018206287175417
        model: {}
        policy_loss: -0.003601395059376955
        total_loss: -0.0028863560874015093
        vf_explained_var: 0.009546637535095215
        vf_loss: 22.271421432495117
      agent-5:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5438883900642395
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006476190173998475
        model: {}
        policy_loss: -0.0025149215944111347
        total_loss: -0.0014109164476394653
        vf_explained_var: 0.09521558880805969
        vf_loss: 20.612510681152344
    load_time_ms: 12890.969
    num_steps_sampled: 27552000
    num_steps_trained: 27552000
    sample_time_ms: 90767.965
    update_time_ms: 22.021
  iterations_since_restore: 227
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.37897727272727
    ram_util_percent: 10.123295454545454
  pid: 24061
  policy_reward_max:
    agent-0: 200.16666666666683
    agent-1: 200.16666666666683
    agent-2: 200.16666666666683
    agent-3: 200.16666666666683
    agent-4: 200.16666666666683
    agent-5: 200.16666666666683
  policy_reward_mean:
    agent-0: 175.47499999999985
    agent-1: 175.47499999999985
    agent-2: 175.47499999999985
    agent-3: 175.47499999999985
    agent-4: 175.47499999999985
    agent-5: 175.47499999999985
  policy_reward_min:
    agent-0: 107.6666666666668
    agent-1: 107.6666666666668
    agent-2: 107.6666666666668
    agent-3: 107.6666666666668
    agent-4: 107.6666666666668
    agent-5: 107.6666666666668
  sampler_perf:
    mean_env_wait_ms: 24.54353071914177
    mean_inference_ms: 12.236197731287152
    mean_processing_ms: 50.70411754418643
  time_since_restore: 29007.673551797867
  time_this_iter_s: 124.13437080383301
  time_total_s: 38133.68536567688
  timestamp: 1637053050
  timesteps_since_restore: 21792000
  timesteps_this_iter: 96000
  timesteps_total: 27552000
  training_iteration: 287
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    287 |          38133.7 | 27552000 |  1052.85 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 73
    apples_agent-0_mean: 2.05
    apples_agent-0_min: 0
    apples_agent-1_max: 116
    apples_agent-1_mean: 29.94
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 1.77
    apples_agent-2_min: 0
    apples_agent-3_max: 174
    apples_agent-3_mean: 110.89
    apples_agent-3_min: 70
    apples_agent-4_max: 21
    apples_agent-4_mean: 0.34
    apples_agent-4_min: 0
    apples_agent-5_max: 188
    apples_agent-5_mean: 83.39
    apples_agent-5_min: 18
    cleaning_beam_agent-0_max: 658
    cleaning_beam_agent-0_mean: 454.63
    cleaning_beam_agent-0_min: 244
    cleaning_beam_agent-1_max: 575
    cleaning_beam_agent-1_mean: 263.26
    cleaning_beam_agent-1_min: 80
    cleaning_beam_agent-2_max: 545
    cleaning_beam_agent-2_mean: 433.0
    cleaning_beam_agent-2_min: 269
    cleaning_beam_agent-3_max: 43
    cleaning_beam_agent-3_mean: 13.42
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 646
    cleaning_beam_agent-4_mean: 511.49
    cleaning_beam_agent-4_min: 406
    cleaning_beam_agent-5_max: 271
    cleaning_beam_agent-5_mean: 20.57
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-59-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1188.0000000000064
  episode_reward_mean: 1037.3199999999902
  episode_reward_min: 772.9999999999793
  episodes_this_iter: 96
  episodes_total: 27648
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19763.898
    learner:
      agent-0:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0629429817199707
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010577149223536253
        model: {}
        policy_loss: -0.0031002690084278584
        total_loss: -0.002730055246502161
        vf_explained_var: 0.010889336466789246
        vf_loss: 22.40991973876953
      agent-1:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1004773378372192
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017403623787686229
        model: {}
        policy_loss: -0.004097416065633297
        total_loss: -0.0035460833460092545
        vf_explained_var: -0.05670318007469177
        vf_loss: 24.881744384765625
      agent-2:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9720667600631714
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018255691975355148
        model: {}
        policy_loss: -0.003370249178260565
        total_loss: -0.002869467483833432
        vf_explained_var: 0.00023667514324188232
        vf_loss: 22.116195678710938
      agent-3:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4450839161872864
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008948210161179304
        model: {}
        policy_loss: -0.0022897240705788136
        total_loss: -0.0010014772415161133
        vf_explained_var: 0.05202309787273407
        vf_loss: 20.71590805053711
      agent-4:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.867520272731781
        entropy_coeff: 0.0017600000137463212
        kl: 0.002169126644730568
        model: {}
        policy_loss: -0.00366464676335454
        total_loss: -0.0030175549909472466
        vf_explained_var: 0.021063432097434998
        vf_loss: 21.739276885986328
      agent-5:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5317244529724121
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009388711187057197
        model: {}
        policy_loss: -0.002615035045892
        total_loss: -0.0014669750817120075
        vf_explained_var: 0.07113334536552429
        vf_loss: 20.838956832885742
    load_time_ms: 12865.09
    num_steps_sampled: 27648000
    num_steps_trained: 27648000
    sample_time_ms: 90813.431
    update_time_ms: 21.919
  iterations_since_restore: 228
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.366101694915255
    ram_util_percent: 10.13728813559322
  pid: 24061
  policy_reward_max:
    agent-0: 197.99999999999972
    agent-1: 197.99999999999972
    agent-2: 197.99999999999972
    agent-3: 197.99999999999972
    agent-4: 197.99999999999972
    agent-5: 197.99999999999972
  policy_reward_mean:
    agent-0: 172.88666666666654
    agent-1: 172.88666666666654
    agent-2: 172.88666666666654
    agent-3: 172.88666666666654
    agent-4: 172.88666666666654
    agent-5: 172.88666666666654
  policy_reward_min:
    agent-0: 128.83333333333385
    agent-1: 128.83333333333385
    agent-2: 128.83333333333385
    agent-3: 128.83333333333385
    agent-4: 128.83333333333385
    agent-5: 128.83333333333385
  sampler_perf:
    mean_env_wait_ms: 24.547498533299457
    mean_inference_ms: 12.23563871185535
    mean_processing_ms: 50.70208038714167
  time_since_restore: 29131.427461385727
  time_this_iter_s: 123.75390958786011
  time_total_s: 38257.43927526474
  timestamp: 1637053174
  timesteps_since_restore: 21888000
  timesteps_this_iter: 96000
  timesteps_total: 27648000
  training_iteration: 288
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    288 |          38257.4 | 27648000 |  1037.32 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 51
    apples_agent-0_mean: 2.37
    apples_agent-0_min: 0
    apples_agent-1_max: 132
    apples_agent-1_mean: 32.82
    apples_agent-1_min: 0
    apples_agent-2_max: 95
    apples_agent-2_mean: 3.34
    apples_agent-2_min: 0
    apples_agent-3_max: 177
    apples_agent-3_mean: 118.72
    apples_agent-3_min: 73
    apples_agent-4_max: 15
    apples_agent-4_mean: 0.15
    apples_agent-4_min: 0
    apples_agent-5_max: 124
    apples_agent-5_mean: 79.97
    apples_agent-5_min: 27
    cleaning_beam_agent-0_max: 548
    cleaning_beam_agent-0_mean: 436.46
    cleaning_beam_agent-0_min: 249
    cleaning_beam_agent-1_max: 509
    cleaning_beam_agent-1_mean: 266.4
    cleaning_beam_agent-1_min: 99
    cleaning_beam_agent-2_max: 593
    cleaning_beam_agent-2_mean: 401.74
    cleaning_beam_agent-2_min: 229
    cleaning_beam_agent-3_max: 115
    cleaning_beam_agent-3_mean: 18.71
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 623
    cleaning_beam_agent-4_mean: 499.77
    cleaning_beam_agent-4_min: 398
    cleaning_beam_agent-5_max: 165
    cleaning_beam_agent-5_mean: 20.88
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-01-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1172.999999999999
  episode_reward_mean: 1021.4699999999903
  episode_reward_min: 594.000000000003
  episodes_this_iter: 96
  episodes_total: 27744
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19747.289
    learner:
      agent-0:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0962103605270386
        entropy_coeff: 0.0017600000137463212
        kl: 0.002726322505623102
        model: {}
        policy_loss: -0.0035390015691518784
        total_loss: -0.003355119377374649
        vf_explained_var: 0.007249504327774048
        vf_loss: 21.13211441040039
      agent-1:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1070225238800049
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018578097224235535
        model: {}
        policy_loss: -0.0039356667548418045
        total_loss: -0.0034831580705940723
        vf_explained_var: -0.08147218823432922
        vf_loss: 24.008697509765625
      agent-2:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9895951747894287
        entropy_coeff: 0.0017600000137463212
        kl: 0.001835416303947568
        model: {}
        policy_loss: -0.0033661830238997936
        total_loss: -0.003021780401468277
        vf_explained_var: 0.010362297296524048
        vf_loss: 20.860929489135742
      agent-3:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.489254891872406
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007086293771862984
        model: {}
        policy_loss: -0.0020620780996978283
        total_loss: -0.0009344764985144138
        vf_explained_var: 0.05067409574985504
        vf_loss: 19.886873245239258
      agent-4:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8668227195739746
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013428751844912767
        model: {}
        policy_loss: -0.003419524058699608
        total_loss: -0.002821369096636772
        vf_explained_var: -0.002573639154434204
        vf_loss: 21.237634658813477
      agent-5:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5465986728668213
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010608185548335314
        model: {}
        policy_loss: -0.0027939784340560436
        total_loss: -0.001801816513761878
        vf_explained_var: 0.07643508911132812
        vf_loss: 19.5417423248291
    load_time_ms: 12869.211
    num_steps_sampled: 27744000
    num_steps_trained: 27744000
    sample_time_ms: 90896.119
    update_time_ms: 22.005
  iterations_since_restore: 229
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 16.011931818181818
    ram_util_percent: 10.139772727272726
  pid: 24061
  policy_reward_max:
    agent-0: 195.49999999999977
    agent-1: 195.49999999999977
    agent-2: 195.49999999999977
    agent-3: 195.49999999999977
    agent-4: 195.49999999999977
    agent-5: 195.49999999999977
  policy_reward_mean:
    agent-0: 170.2449999999999
    agent-1: 170.2449999999999
    agent-2: 170.2449999999999
    agent-3: 170.2449999999999
    agent-4: 170.2449999999999
    agent-5: 170.2449999999999
  policy_reward_min:
    agent-0: 99.00000000000013
    agent-1: 99.00000000000013
    agent-2: 99.00000000000013
    agent-3: 99.00000000000013
    agent-4: 99.00000000000013
    agent-5: 99.00000000000013
  sampler_perf:
    mean_env_wait_ms: 24.551259092709778
    mean_inference_ms: 12.235395236094439
    mean_processing_ms: 50.700985448438104
  time_since_restore: 29255.190149784088
  time_this_iter_s: 123.7626883983612
  time_total_s: 38381.2019636631
  timestamp: 1637053298
  timesteps_since_restore: 21984000
  timesteps_this_iter: 96000
  timesteps_total: 27744000
  training_iteration: 289
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    289 |          38381.2 | 27744000 |  1021.47 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 29
    apples_agent-0_mean: 1.96
    apples_agent-0_min: 0
    apples_agent-1_max: 113
    apples_agent-1_mean: 33.03
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 3.72
    apples_agent-2_min: 0
    apples_agent-3_max: 186
    apples_agent-3_mean: 119.0
    apples_agent-3_min: 51
    apples_agent-4_max: 57
    apples_agent-4_mean: 1.42
    apples_agent-4_min: 0
    apples_agent-5_max: 143
    apples_agent-5_mean: 79.93
    apples_agent-5_min: 34
    cleaning_beam_agent-0_max: 576
    cleaning_beam_agent-0_mean: 426.98
    cleaning_beam_agent-0_min: 248
    cleaning_beam_agent-1_max: 596
    cleaning_beam_agent-1_mean: 272.54
    cleaning_beam_agent-1_min: 98
    cleaning_beam_agent-2_max: 565
    cleaning_beam_agent-2_mean: 393.63
    cleaning_beam_agent-2_min: 217
    cleaning_beam_agent-3_max: 71
    cleaning_beam_agent-3_mean: 20.39
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 591
    cleaning_beam_agent-4_mean: 476.69
    cleaning_beam_agent-4_min: 314
    cleaning_beam_agent-5_max: 276
    cleaning_beam_agent-5_mean: 26.57
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-03-41
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1196.0000000000011
  episode_reward_mean: 1001.4599999999888
  episode_reward_min: 345.00000000000114
  episodes_this_iter: 96
  episodes_total: 27840
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19699.523
    learner:
      agent-0:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0824053287506104
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012639110209420323
        model: {}
        policy_loss: -0.002981171477586031
        total_loss: -0.0027192411944270134
        vf_explained_var: 0.013611167669296265
        vf_loss: 21.66967010498047
      agent-1:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 1.112200379371643
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012241591466590762
        model: {}
        policy_loss: -0.0042673135176301
        total_loss: -0.003922171425074339
        vf_explained_var: -0.01837301254272461
        vf_loss: 23.02613639831543
      agent-2:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9928877353668213
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018464097520336509
        model: {}
        policy_loss: -0.003474927507340908
        total_loss: -0.002990096341818571
        vf_explained_var: -0.015442267060279846
        vf_loss: 22.323116302490234
      agent-3:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48904627561569214
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011016735807061195
        model: {}
        policy_loss: -0.0024529341608285904
        total_loss: -0.0013124849647283554
        vf_explained_var: 0.08551008999347687
        vf_loss: 20.011709213256836
      agent-4:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.881084680557251
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017599391285330057
        model: {}
        policy_loss: -0.0037546774838119745
        total_loss: -0.0031952429562807083
        vf_explained_var: 0.044554367661476135
        vf_loss: 21.101451873779297
      agent-5:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5558357834815979
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009658091585151851
        model: {}
        policy_loss: -0.0027204370126128197
        total_loss: -0.0016319381538778543
        vf_explained_var: 0.06172028183937073
        vf_loss: 20.667722702026367
    load_time_ms: 12865.899
    num_steps_sampled: 27840000
    num_steps_trained: 27840000
    sample_time_ms: 90814.623
    update_time_ms: 21.867
  iterations_since_restore: 230
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.433714285714284
    ram_util_percent: 10.129714285714288
  pid: 24061
  policy_reward_max:
    agent-0: 199.33333333333317
    agent-1: 199.33333333333317
    agent-2: 199.33333333333317
    agent-3: 199.33333333333317
    agent-4: 199.33333333333317
    agent-5: 199.33333333333317
  policy_reward_mean:
    agent-0: 166.90999999999988
    agent-1: 166.90999999999988
    agent-2: 166.90999999999988
    agent-3: 166.90999999999988
    agent-4: 166.90999999999988
    agent-5: 166.90999999999988
  policy_reward_min:
    agent-0: 57.49999999999982
    agent-1: 57.49999999999982
    agent-2: 57.49999999999982
    agent-3: 57.49999999999982
    agent-4: 57.49999999999982
    agent-5: 57.49999999999982
  sampler_perf:
    mean_env_wait_ms: 24.5541068765095
    mean_inference_ms: 12.234820175463927
    mean_processing_ms: 50.69913717242938
  time_since_restore: 29377.838376522064
  time_this_iter_s: 122.64822673797607
  time_total_s: 38503.85019040108
  timestamp: 1637053421
  timesteps_since_restore: 22080000
  timesteps_this_iter: 96000
  timesteps_total: 27840000
  training_iteration: 290
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    290 |          38503.9 | 27840000 |  1001.46 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 124
    apples_agent-0_mean: 2.75
    apples_agent-0_min: 0
    apples_agent-1_max: 135
    apples_agent-1_mean: 32.3
    apples_agent-1_min: 0
    apples_agent-2_max: 186
    apples_agent-2_mean: 5.12
    apples_agent-2_min: 0
    apples_agent-3_max: 247
    apples_agent-3_mean: 124.74
    apples_agent-3_min: 67
    apples_agent-4_max: 16
    apples_agent-4_mean: 0.33
    apples_agent-4_min: 0
    apples_agent-5_max: 164
    apples_agent-5_mean: 84.55
    apples_agent-5_min: 38
    cleaning_beam_agent-0_max: 567
    cleaning_beam_agent-0_mean: 432.76
    cleaning_beam_agent-0_min: 229
    cleaning_beam_agent-1_max: 596
    cleaning_beam_agent-1_mean: 274.53
    cleaning_beam_agent-1_min: 110
    cleaning_beam_agent-2_max: 595
    cleaning_beam_agent-2_mean: 407.32
    cleaning_beam_agent-2_min: 114
    cleaning_beam_agent-3_max: 72
    cleaning_beam_agent-3_mean: 19.58
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 596
    cleaning_beam_agent-4_mean: 489.7
    cleaning_beam_agent-4_min: 388
    cleaning_beam_agent-5_max: 97
    cleaning_beam_agent-5_mean: 21.84
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-05-44
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1198.9999999999884
  episode_reward_mean: 1029.9099999999903
  episode_reward_min: 599.0000000000064
  episodes_this_iter: 96
  episodes_total: 27936
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19677.84
    learner:
      agent-0:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 1.07704758644104
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015415712259709835
        model: {}
        policy_loss: -0.003441771026700735
        total_loss: -0.003160384250804782
        vf_explained_var: 0.012664169073104858
        vf_loss: 21.769916534423828
      agent-1:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1132011413574219
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014175151009112597
        model: {}
        policy_loss: -0.004067332483828068
        total_loss: -0.00361078605055809
        vf_explained_var: -0.049363791942596436
        vf_loss: 24.157833099365234
      agent-2:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9776981472969055
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011723884381353855
        model: {}
        policy_loss: -0.0032190177589654922
        total_loss: -0.002796607557684183
        vf_explained_var: 0.022750124335289
        vf_loss: 21.431570053100586
      agent-3:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4692772328853607
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009551027324050665
        model: {}
        policy_loss: -0.0022870423272252083
        total_loss: -0.0010590008459985256
        vf_explained_var: 0.05264073610305786
        vf_loss: 20.53968620300293
      agent-4:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8639171123504639
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018317061476409435
        model: {}
        policy_loss: -0.0038560889661312103
        total_loss: -0.003134652506560087
        vf_explained_var: -0.01037384569644928
        vf_loss: 22.419288635253906
      agent-5:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5205986499786377
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017189988866448402
        model: {}
        policy_loss: -0.002804255113005638
        total_loss: -0.0016912054270505905
        vf_explained_var: 0.0846872627735138
        vf_loss: 20.29302215576172
    load_time_ms: 12927.905
    num_steps_sampled: 27936000
    num_steps_trained: 27936000
    sample_time_ms: 90789.652
    update_time_ms: 22.737
  iterations_since_restore: 231
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.482285714285712
    ram_util_percent: 10.139428571428573
  pid: 24061
  policy_reward_max:
    agent-0: 199.8333333333335
    agent-1: 199.8333333333335
    agent-2: 199.8333333333335
    agent-3: 199.8333333333335
    agent-4: 199.8333333333335
    agent-5: 199.8333333333335
  policy_reward_mean:
    agent-0: 171.6516666666665
    agent-1: 171.6516666666665
    agent-2: 171.6516666666665
    agent-3: 171.6516666666665
    agent-4: 171.6516666666665
    agent-5: 171.6516666666665
  policy_reward_min:
    agent-0: 99.83333333333319
    agent-1: 99.83333333333319
    agent-2: 99.83333333333319
    agent-3: 99.83333333333319
    agent-4: 99.83333333333319
    agent-5: 99.83333333333319
  sampler_perf:
    mean_env_wait_ms: 24.556615549925354
    mean_inference_ms: 12.23409626629033
    mean_processing_ms: 50.69685124397896
  time_since_restore: 29500.90703868866
  time_this_iter_s: 123.06866216659546
  time_total_s: 38626.91885256767
  timestamp: 1637053544
  timesteps_since_restore: 22176000
  timesteps_this_iter: 96000
  timesteps_total: 27936000
  training_iteration: 291
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    291 |          38626.9 | 27936000 |  1029.91 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 85
    apples_agent-0_mean: 5.0
    apples_agent-0_min: 0
    apples_agent-1_max: 97
    apples_agent-1_mean: 34.03
    apples_agent-1_min: 0
    apples_agent-2_max: 294
    apples_agent-2_mean: 6.98
    apples_agent-2_min: 0
    apples_agent-3_max: 226
    apples_agent-3_mean: 124.85
    apples_agent-3_min: 68
    apples_agent-4_max: 59
    apples_agent-4_mean: 0.87
    apples_agent-4_min: 0
    apples_agent-5_max: 161
    apples_agent-5_mean: 80.32
    apples_agent-5_min: 40
    cleaning_beam_agent-0_max: 542
    cleaning_beam_agent-0_mean: 410.22
    cleaning_beam_agent-0_min: 178
    cleaning_beam_agent-1_max: 439
    cleaning_beam_agent-1_mean: 270.41
    cleaning_beam_agent-1_min: 110
    cleaning_beam_agent-2_max: 549
    cleaning_beam_agent-2_mean: 405.32
    cleaning_beam_agent-2_min: 90
    cleaning_beam_agent-3_max: 88
    cleaning_beam_agent-3_mean: 20.36
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 609
    cleaning_beam_agent-4_mean: 486.04
    cleaning_beam_agent-4_min: 341
    cleaning_beam_agent-5_max: 91
    cleaning_beam_agent-5_mean: 19.5
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-07-48
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1225.0000000000066
  episode_reward_mean: 1012.6199999999902
  episode_reward_min: 535.0000000000078
  episodes_this_iter: 96
  episodes_total: 28032
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19636.108
    learner:
      agent-0:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0973544120788574
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010203582933172584
        model: {}
        policy_loss: -0.003103881608694792
        total_loss: -0.00271069398149848
        vf_explained_var: 0.014604732394218445
        vf_loss: 23.245302200317383
      agent-1:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1129112243652344
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018138966988772154
        model: {}
        policy_loss: -0.004216302186250687
        total_loss: -0.003553084097802639
        vf_explained_var: -0.0829751193523407
        vf_loss: 26.21942901611328
      agent-2:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9753350615501404
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024757885839790106
        model: {}
        policy_loss: -0.003339173272252083
        total_loss: -0.002817368134856224
        vf_explained_var: 0.043468177318573
        vf_loss: 22.3839111328125
      agent-3:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4822359085083008
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010915842140093446
        model: {}
        policy_loss: -0.002473813481628895
        total_loss: -0.0012177340686321259
        vf_explained_var: 0.0956280529499054
        vf_loss: 21.04815673828125
      agent-4:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.864176869392395
        entropy_coeff: 0.0017600000137463212
        kl: 0.001605873927474022
        model: {}
        policy_loss: -0.003866767045110464
        total_loss: -0.0030547238420695066
        vf_explained_var: 0.008722752332687378
        vf_loss: 23.32989501953125
      agent-5:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5281335115432739
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011889347806572914
        model: {}
        policy_loss: -0.002665462903678417
        total_loss: -0.0014352556318044662
        vf_explained_var: 0.07507742941379547
        vf_loss: 21.597213745117188
    load_time_ms: 12920.987
    num_steps_sampled: 28032000
    num_steps_trained: 28032000
    sample_time_ms: 90792.883
    update_time_ms: 22.831
  iterations_since_restore: 232
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.493220338983052
    ram_util_percent: 10.120338983050846
  pid: 24061
  policy_reward_max:
    agent-0: 204.16666666666646
    agent-1: 204.16666666666646
    agent-2: 204.16666666666646
    agent-3: 204.16666666666646
    agent-4: 204.16666666666646
    agent-5: 204.16666666666646
  policy_reward_mean:
    agent-0: 168.7699999999999
    agent-1: 168.7699999999999
    agent-2: 168.7699999999999
    agent-3: 168.7699999999999
    agent-4: 168.7699999999999
    agent-5: 168.7699999999999
  policy_reward_min:
    agent-0: 89.16666666666686
    agent-1: 89.16666666666686
    agent-2: 89.16666666666686
    agent-3: 89.16666666666686
    agent-4: 89.16666666666686
    agent-5: 89.16666666666686
  sampler_perf:
    mean_env_wait_ms: 24.55982433141652
    mean_inference_ms: 12.233760243709266
    mean_processing_ms: 50.69557520592035
  time_since_restore: 29624.483420848846
  time_this_iter_s: 123.57638216018677
  time_total_s: 38750.49523472786
  timestamp: 1637053668
  timesteps_since_restore: 22272000
  timesteps_this_iter: 96000
  timesteps_total: 28032000
  training_iteration: 292
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    292 |          38750.5 | 28032000 |  1012.62 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 84
    apples_agent-0_mean: 3.14
    apples_agent-0_min: 0
    apples_agent-1_max: 118
    apples_agent-1_mean: 30.82
    apples_agent-1_min: 0
    apples_agent-2_max: 93
    apples_agent-2_mean: 4.95
    apples_agent-2_min: 0
    apples_agent-3_max: 181
    apples_agent-3_mean: 119.58
    apples_agent-3_min: 0
    apples_agent-4_max: 64
    apples_agent-4_mean: 1.93
    apples_agent-4_min: 0
    apples_agent-5_max: 135
    apples_agent-5_mean: 78.73
    apples_agent-5_min: 18
    cleaning_beam_agent-0_max: 522
    cleaning_beam_agent-0_mean: 408.58
    cleaning_beam_agent-0_min: 249
    cleaning_beam_agent-1_max: 527
    cleaning_beam_agent-1_mean: 277.33
    cleaning_beam_agent-1_min: 98
    cleaning_beam_agent-2_max: 575
    cleaning_beam_agent-2_mean: 410.42
    cleaning_beam_agent-2_min: 141
    cleaning_beam_agent-3_max: 81
    cleaning_beam_agent-3_mean: 17.82
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 586
    cleaning_beam_agent-4_mean: 473.18
    cleaning_beam_agent-4_min: 346
    cleaning_beam_agent-5_max: 139
    cleaning_beam_agent-5_mean: 19.65
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-09-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1229.0000000000007
  episode_reward_mean: 1016.0499999999913
  episode_reward_min: 566.0000000000095
  episodes_this_iter: 96
  episodes_total: 28128
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19666.901
    learner:
      agent-0:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 1.088503360748291
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017624427564442158
        model: {}
        policy_loss: -0.0033454103395342827
        total_loss: -0.0028462084010243416
        vf_explained_var: 0.01244884729385376
        vf_loss: 24.149656295776367
      agent-1:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0859168767929077
        entropy_coeff: 0.0017600000137463212
        kl: 0.001454910496249795
        model: {}
        policy_loss: -0.00390934944152832
        total_loss: -0.0032476841006428003
        vf_explained_var: -0.01844087243080139
        vf_loss: 25.728755950927734
      agent-2:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9826956987380981
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013298988342285156
        model: {}
        policy_loss: -0.002971665933728218
        total_loss: -0.0023089777678251266
        vf_explained_var: 0.02110251784324646
        vf_loss: 23.92230224609375
      agent-3:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45436185598373413
        entropy_coeff: 0.0017600000137463212
        kl: 0.001319917500950396
        model: {}
        policy_loss: -0.0025530229322612286
        total_loss: -0.001215895637869835
        vf_explained_var: 0.12076859176158905
        vf_loss: 21.368017196655273
      agent-4:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8752424120903015
        entropy_coeff: 0.0017600000137463212
        kl: 0.001205165171995759
        model: {}
        policy_loss: -0.003487874288111925
        total_loss: -0.002728546503931284
        vf_explained_var: 0.06833797693252563
        vf_loss: 22.99761199951172
      agent-5:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5097747445106506
        entropy_coeff: 0.0017600000137463212
        kl: 0.0003799822588916868
        model: {}
        policy_loss: -0.002132969908416271
        total_loss: -0.0008523836731910706
        vf_explained_var: 0.10652875900268555
        vf_loss: 21.777889251708984
    load_time_ms: 12938.295
    num_steps_sampled: 28128000
    num_steps_trained: 28128000
    sample_time_ms: 90763.271
    update_time_ms: 22.657
  iterations_since_restore: 233
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.456818181818182
    ram_util_percent: 10.171590909090908
  pid: 24061
  policy_reward_max:
    agent-0: 204.8333333333335
    agent-1: 204.8333333333335
    agent-2: 204.8333333333335
    agent-3: 204.8333333333335
    agent-4: 204.8333333333335
    agent-5: 204.8333333333335
  policy_reward_mean:
    agent-0: 169.34166666666653
    agent-1: 169.34166666666653
    agent-2: 169.34166666666653
    agent-3: 169.34166666666653
    agent-4: 169.34166666666653
    agent-5: 169.34166666666653
  policy_reward_min:
    agent-0: 94.33333333333361
    agent-1: 94.33333333333361
    agent-2: 94.33333333333361
    agent-3: 94.33333333333361
    agent-4: 94.33333333333361
    agent-5: 94.33333333333361
  sampler_perf:
    mean_env_wait_ms: 24.562530200733864
    mean_inference_ms: 12.233377586561458
    mean_processing_ms: 50.69368082591322
  time_since_restore: 29747.889534235
  time_this_iter_s: 123.40611338615417
  time_total_s: 38873.901348114014
  timestamp: 1637053791
  timesteps_since_restore: 22368000
  timesteps_this_iter: 96000
  timesteps_total: 28128000
  training_iteration: 293
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    293 |          38873.9 | 28128000 |  1016.05 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 84
    apples_agent-0_mean: 3.01
    apples_agent-0_min: 0
    apples_agent-1_max: 118
    apples_agent-1_mean: 29.56
    apples_agent-1_min: 0
    apples_agent-2_max: 74
    apples_agent-2_mean: 2.81
    apples_agent-2_min: 0
    apples_agent-3_max: 246
    apples_agent-3_mean: 126.64
    apples_agent-3_min: 0
    apples_agent-4_max: 52
    apples_agent-4_mean: 1.5
    apples_agent-4_min: 0
    apples_agent-5_max: 165
    apples_agent-5_mean: 79.06
    apples_agent-5_min: 38
    cleaning_beam_agent-0_max: 529
    cleaning_beam_agent-0_mean: 419.94
    cleaning_beam_agent-0_min: 249
    cleaning_beam_agent-1_max: 538
    cleaning_beam_agent-1_mean: 293.88
    cleaning_beam_agent-1_min: 114
    cleaning_beam_agent-2_max: 535
    cleaning_beam_agent-2_mean: 392.74
    cleaning_beam_agent-2_min: 194
    cleaning_beam_agent-3_max: 71
    cleaning_beam_agent-3_mean: 18.02
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 587
    cleaning_beam_agent-4_mean: 481.69
    cleaning_beam_agent-4_min: 404
    cleaning_beam_agent-5_max: 71
    cleaning_beam_agent-5_mean: 17.17
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-11-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1231.9999999999982
  episode_reward_mean: 1033.8099999999895
  episode_reward_min: 614.0000000000133
  episodes_this_iter: 96
  episodes_total: 28224
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19667.189
    learner:
      agent-0:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0902057886123657
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016215103678405285
        model: {}
        policy_loss: -0.003070928854867816
        total_loss: -0.0026972023770213127
        vf_explained_var: 0.006143897771835327
        vf_loss: 22.924882888793945
      agent-1:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1128804683685303
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014700070023536682
        model: {}
        policy_loss: -0.00411906186491251
        total_loss: -0.003531081136316061
        vf_explained_var: -0.0661838948726654
        vf_loss: 25.46649932861328
      agent-2:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9828740954399109
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019324191380292177
        model: {}
        policy_loss: -0.0035090739838778973
        total_loss: -0.002989790868014097
        vf_explained_var: 0.017252609133720398
        vf_loss: 22.49142837524414
      agent-3:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45869335532188416
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014349817065522075
        model: {}
        policy_loss: -0.002637505531311035
        total_loss: -0.0013574399054050446
        vf_explained_var: 0.07755720615386963
        vf_loss: 20.8736515045166
      agent-4:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8639949560165405
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015676466282457113
        model: {}
        policy_loss: -0.003582943230867386
        total_loss: -0.00289186742156744
        vf_explained_var: 0.0484442412853241
        vf_loss: 22.117080688476562
      agent-5:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4830464720726013
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006436342373490334
        model: {}
        policy_loss: -0.002210945123806596
        total_loss: -0.0009452811209484935
        vf_explained_var: 0.07288073003292084
        vf_loss: 21.158218383789062
    load_time_ms: 12916.364
    num_steps_sampled: 28224000
    num_steps_trained: 28224000
    sample_time_ms: 90798.44
    update_time_ms: 22.567
  iterations_since_restore: 234
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.421714285714287
    ram_util_percent: 10.138285714285717
  pid: 24061
  policy_reward_max:
    agent-0: 205.3333333333334
    agent-1: 205.3333333333334
    agent-2: 205.3333333333334
    agent-3: 205.3333333333334
    agent-4: 205.3333333333334
    agent-5: 205.3333333333334
  policy_reward_mean:
    agent-0: 172.30166666666653
    agent-1: 172.30166666666653
    agent-2: 172.30166666666653
    agent-3: 172.30166666666653
    agent-4: 172.30166666666653
    agent-5: 172.30166666666653
  policy_reward_min:
    agent-0: 102.33333333333387
    agent-1: 102.33333333333387
    agent-2: 102.33333333333387
    agent-3: 102.33333333333387
    agent-4: 102.33333333333387
    agent-5: 102.33333333333387
  sampler_perf:
    mean_env_wait_ms: 24.56499402891759
    mean_inference_ms: 12.232689522201058
    mean_processing_ms: 50.69109481563524
  time_since_restore: 29871.003162384033
  time_this_iter_s: 123.11362814903259
  time_total_s: 38997.014976263046
  timestamp: 1637053915
  timesteps_since_restore: 22464000
  timesteps_this_iter: 96000
  timesteps_total: 28224000
  training_iteration: 294
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    294 |            38997 | 28224000 |  1033.81 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 60
    apples_agent-0_mean: 4.18
    apples_agent-0_min: 0
    apples_agent-1_max: 116
    apples_agent-1_mean: 27.8
    apples_agent-1_min: 0
    apples_agent-2_max: 51
    apples_agent-2_mean: 1.53
    apples_agent-2_min: 0
    apples_agent-3_max: 254
    apples_agent-3_mean: 117.25
    apples_agent-3_min: 0
    apples_agent-4_max: 69
    apples_agent-4_mean: 1.45
    apples_agent-4_min: 0
    apples_agent-5_max: 196
    apples_agent-5_mean: 81.15
    apples_agent-5_min: 52
    cleaning_beam_agent-0_max: 563
    cleaning_beam_agent-0_mean: 404.91
    cleaning_beam_agent-0_min: 229
    cleaning_beam_agent-1_max: 541
    cleaning_beam_agent-1_mean: 284.46
    cleaning_beam_agent-1_min: 129
    cleaning_beam_agent-2_max: 538
    cleaning_beam_agent-2_mean: 416.22
    cleaning_beam_agent-2_min: 216
    cleaning_beam_agent-3_max: 130
    cleaning_beam_agent-3_mean: 17.9
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 635
    cleaning_beam_agent-4_mean: 481.95
    cleaning_beam_agent-4_min: 299
    cleaning_beam_agent-5_max: 98
    cleaning_beam_agent-5_mean: 17.75
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-13-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1227.000000000001
  episode_reward_mean: 1026.719999999991
  episode_reward_min: 595.000000000006
  episodes_this_iter: 96
  episodes_total: 28320
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19655.128
    learner:
      agent-0:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 1.09602689743042
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016436087898910046
        model: {}
        policy_loss: -0.0032910662703216076
        total_loss: -0.002921693492680788
        vf_explained_var: 0.0004377514123916626
        vf_loss: 22.98379898071289
      agent-1:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0998272895812988
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016312985680997372
        model: {}
        policy_loss: -0.003705422393977642
        total_loss: -0.0030663548968732357
        vf_explained_var: -0.07264253497123718
        vf_loss: 25.74763298034668
      agent-2:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9713090658187866
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016455778386443853
        model: {}
        policy_loss: -0.0032358812168240547
        total_loss: -0.002687806263566017
        vf_explained_var: 0.009708642959594727
        vf_loss: 22.57579803466797
      agent-3:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45369821786880493
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008164601167663932
        model: {}
        policy_loss: -0.0023437459021806717
        total_loss: -0.0010000737383961678
        vf_explained_var: 0.061615169048309326
        vf_loss: 21.421810150146484
      agent-4:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.857750654220581
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018626363016664982
        model: {}
        policy_loss: -0.003628136357292533
        total_loss: -0.002871623495593667
        vf_explained_var: 0.02140939235687256
        vf_loss: 22.66151237487793
      agent-5:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4938340485095978
        entropy_coeff: 0.0017600000137463212
        kl: 0.00047196378000080585
        model: {}
        policy_loss: -0.001951583195477724
        total_loss: -0.0006876502884551883
        vf_explained_var: 0.06997650861740112
        vf_loss: 21.33079719543457
    load_time_ms: 12898.703
    num_steps_sampled: 28320000
    num_steps_trained: 28320000
    sample_time_ms: 90657.228
    update_time_ms: 22.05
  iterations_since_restore: 235
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.486931818181818
    ram_util_percent: 10.092045454545454
  pid: 24061
  policy_reward_max:
    agent-0: 204.50000000000034
    agent-1: 204.50000000000034
    agent-2: 204.50000000000034
    agent-3: 204.50000000000034
    agent-4: 204.50000000000034
    agent-5: 204.50000000000034
  policy_reward_mean:
    agent-0: 171.1199999999998
    agent-1: 171.1199999999998
    agent-2: 171.1199999999998
    agent-3: 171.1199999999998
    agent-4: 171.1199999999998
    agent-5: 171.1199999999998
  policy_reward_min:
    agent-0: 99.1666666666669
    agent-1: 99.1666666666669
    agent-2: 99.1666666666669
    agent-3: 99.1666666666669
    agent-4: 99.1666666666669
    agent-5: 99.1666666666669
  sampler_perf:
    mean_env_wait_ms: 24.567403793792984
    mean_inference_ms: 12.232382644126218
    mean_processing_ms: 50.68927013219948
  time_since_restore: 29993.872044086456
  time_this_iter_s: 122.8688817024231
  time_total_s: 39119.88385796547
  timestamp: 1637054038
  timesteps_since_restore: 22560000
  timesteps_this_iter: 96000
  timesteps_total: 28320000
  training_iteration: 295
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    295 |          39119.9 | 28320000 |  1026.72 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 58
    apples_agent-0_mean: 2.9
    apples_agent-0_min: 0
    apples_agent-1_max: 147
    apples_agent-1_mean: 31.4
    apples_agent-1_min: 0
    apples_agent-2_max: 51
    apples_agent-2_mean: 2.37
    apples_agent-2_min: 0
    apples_agent-3_max: 389
    apples_agent-3_mean: 115.58
    apples_agent-3_min: 0
    apples_agent-4_max: 28
    apples_agent-4_mean: 0.66
    apples_agent-4_min: 0
    apples_agent-5_max: 328
    apples_agent-5_mean: 83.67
    apples_agent-5_min: 38
    cleaning_beam_agent-0_max: 546
    cleaning_beam_agent-0_mean: 410.94
    cleaning_beam_agent-0_min: 259
    cleaning_beam_agent-1_max: 610
    cleaning_beam_agent-1_mean: 290.48
    cleaning_beam_agent-1_min: 124
    cleaning_beam_agent-2_max: 587
    cleaning_beam_agent-2_mean: 435.94
    cleaning_beam_agent-2_min: 208
    cleaning_beam_agent-3_max: 112
    cleaning_beam_agent-3_mean: 15.36
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 570
    cleaning_beam_agent-4_mean: 483.98
    cleaning_beam_agent-4_min: 182
    cleaning_beam_agent-5_max: 86
    cleaning_beam_agent-5_mean: 18.68
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-16-02
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1203.9999999999975
  episode_reward_mean: 1025.479999999989
  episode_reward_min: 103.00000000000087
  episodes_this_iter: 96
  episodes_total: 28416
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19668.137
    learner:
      agent-0:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0799299478530884
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013054866576567292
        model: {}
        policy_loss: -0.002928976435214281
        total_loss: -0.0023846353869885206
        vf_explained_var: 0.02337592840194702
        vf_loss: 24.450153350830078
      agent-1:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0965486764907837
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013733810046687722
        model: {}
        policy_loss: -0.00393656175583601
        total_loss: -0.003128236625343561
        vf_explained_var: -0.04476514458656311
        vf_loss: 27.382482528686523
      agent-2:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.974087119102478
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017090300098061562
        model: {}
        policy_loss: -0.003241189755499363
        total_loss: -0.002597515471279621
        vf_explained_var: 0.055760011076927185
        vf_loss: 23.580684661865234
      agent-3:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4414933919906616
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007038546609692276
        model: {}
        policy_loss: -0.0020746150985360146
        total_loss: -0.0005201529711484909
        vf_explained_var: 0.06421329081058502
        vf_loss: 23.314926147460938
      agent-4:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8581421375274658
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014417401980608702
        model: {}
        policy_loss: -0.003709412645548582
        total_loss: -0.0028888420201838017
        vf_explained_var: 0.07678356766700745
        vf_loss: 23.309032440185547
      agent-5:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5161073207855225
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008081456180661917
        model: {}
        policy_loss: -0.0021031247451901436
        total_loss: -0.0007451064884662628
        vf_explained_var: 0.09994058310985565
        vf_loss: 22.66366958618164
    load_time_ms: 12899.568
    num_steps_sampled: 28416000
    num_steps_trained: 28416000
    sample_time_ms: 90727.435
    update_time_ms: 21.949
  iterations_since_restore: 236
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.434659090909092
    ram_util_percent: 10.164772727272725
  pid: 24061
  policy_reward_max:
    agent-0: 200.6666666666665
    agent-1: 200.6666666666665
    agent-2: 200.6666666666665
    agent-3: 200.6666666666665
    agent-4: 200.6666666666665
    agent-5: 200.6666666666665
  policy_reward_mean:
    agent-0: 170.91333333333318
    agent-1: 170.91333333333318
    agent-2: 170.91333333333318
    agent-3: 170.91333333333318
    agent-4: 170.91333333333318
    agent-5: 170.91333333333318
  policy_reward_min:
    agent-0: 17.166666666666657
    agent-1: 17.166666666666657
    agent-2: 17.166666666666657
    agent-3: 17.166666666666657
    agent-4: 17.166666666666657
    agent-5: 17.166666666666657
  sampler_perf:
    mean_env_wait_ms: 24.570268307545057
    mean_inference_ms: 12.231914574239486
    mean_processing_ms: 50.68683120479324
  time_since_restore: 30117.690984487534
  time_this_iter_s: 123.81894040107727
  time_total_s: 39243.70279836655
  timestamp: 1637054162
  timesteps_since_restore: 22656000
  timesteps_this_iter: 96000
  timesteps_total: 28416000
  training_iteration: 296
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    296 |          39243.7 | 28416000 |  1025.48 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 96
    apples_agent-0_mean: 3.5
    apples_agent-0_min: 0
    apples_agent-1_max: 98
    apples_agent-1_mean: 31.41
    apples_agent-1_min: 0
    apples_agent-2_max: 80
    apples_agent-2_mean: 2.82
    apples_agent-2_min: 0
    apples_agent-3_max: 223
    apples_agent-3_mean: 119.37
    apples_agent-3_min: 80
    apples_agent-4_max: 10
    apples_agent-4_mean: 0.21
    apples_agent-4_min: 0
    apples_agent-5_max: 165
    apples_agent-5_mean: 84.08
    apples_agent-5_min: 42
    cleaning_beam_agent-0_max: 546
    cleaning_beam_agent-0_mean: 403.64
    cleaning_beam_agent-0_min: 255
    cleaning_beam_agent-1_max: 518
    cleaning_beam_agent-1_mean: 296.04
    cleaning_beam_agent-1_min: 134
    cleaning_beam_agent-2_max: 636
    cleaning_beam_agent-2_mean: 433.22
    cleaning_beam_agent-2_min: 264
    cleaning_beam_agent-3_max: 52
    cleaning_beam_agent-3_mean: 15.28
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 611
    cleaning_beam_agent-4_mean: 494.38
    cleaning_beam_agent-4_min: 349
    cleaning_beam_agent-5_max: 50
    cleaning_beam_agent-5_mean: 16.24
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-18-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1237.0000000000018
  episode_reward_mean: 1055.4999999999893
  episode_reward_min: 673.9999999999835
  episodes_this_iter: 96
  episodes_total: 28512
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19677.517
    learner:
      agent-0:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0774033069610596
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019242143025621772
        model: {}
        policy_loss: -0.0034963658545166254
        total_loss: -0.0031390096992254257
        vf_explained_var: 0.009692057967185974
        vf_loss: 22.535831451416016
      agent-1:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 1.095274567604065
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011158271227031946
        model: {}
        policy_loss: -0.0040764110162854195
        total_loss: -0.0033531449735164642
        vf_explained_var: -0.09218364953994751
        vf_loss: 26.50948715209961
      agent-2:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9536666870117188
        entropy_coeff: 0.0017600000137463212
        kl: 0.001208574860356748
        model: {}
        policy_loss: -0.0029089346062391996
        total_loss: -0.002386990934610367
        vf_explained_var: 0.005823105573654175
        vf_loss: 22.00398063659668
      agent-3:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4330030679702759
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011368287960067391
        model: {}
        policy_loss: -0.0019987942650914192
        total_loss: -0.0006450386135838926
        vf_explained_var: 0.036350518465042114
        vf_loss: 21.158401489257812
      agent-4:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.854379415512085
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020154942758381367
        model: {}
        policy_loss: -0.0036087986081838608
        total_loss: -0.0028728852048516273
        vf_explained_var: 0.00766947865486145
        vf_loss: 22.396221160888672
      agent-5:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.506527304649353
        entropy_coeff: 0.0017600000137463212
        kl: 0.000633914431091398
        model: {}
        policy_loss: -0.0025742659345269203
        total_loss: -0.0013247395399957895
        vf_explained_var: 0.052951425313949585
        vf_loss: 21.410152435302734
    load_time_ms: 12911.682
    num_steps_sampled: 28512000
    num_steps_trained: 28512000
    sample_time_ms: 90575.173
    update_time_ms: 21.71
  iterations_since_restore: 237
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.527999999999999
    ram_util_percent: 10.077142857142855
  pid: 24061
  policy_reward_max:
    agent-0: 206.16666666666674
    agent-1: 206.16666666666674
    agent-2: 206.16666666666674
    agent-3: 206.16666666666674
    agent-4: 206.16666666666674
    agent-5: 206.16666666666674
  policy_reward_mean:
    agent-0: 175.91666666666646
    agent-1: 175.91666666666646
    agent-2: 175.91666666666646
    agent-3: 175.91666666666646
    agent-4: 175.91666666666646
    agent-5: 175.91666666666646
  policy_reward_min:
    agent-0: 112.333333333334
    agent-1: 112.333333333334
    agent-2: 112.333333333334
    agent-3: 112.333333333334
    agent-4: 112.333333333334
    agent-5: 112.333333333334
  sampler_perf:
    mean_env_wait_ms: 24.573388515334937
    mean_inference_ms: 12.231324464886299
    mean_processing_ms: 50.68467124100157
  time_since_restore: 30240.444849729538
  time_this_iter_s: 122.7538652420044
  time_total_s: 39366.45666360855
  timestamp: 1637054284
  timesteps_since_restore: 22752000
  timesteps_this_iter: 96000
  timesteps_total: 28512000
  training_iteration: 297
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    297 |          39366.5 | 28512000 |   1055.5 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 96
    apples_agent-0_mean: 4.08
    apples_agent-0_min: 0
    apples_agent-1_max: 194
    apples_agent-1_mean: 30.38
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 3.2
    apples_agent-2_min: 0
    apples_agent-3_max: 226
    apples_agent-3_mean: 117.19
    apples_agent-3_min: 45
    apples_agent-4_max: 38
    apples_agent-4_mean: 1.11
    apples_agent-4_min: 0
    apples_agent-5_max: 136
    apples_agent-5_mean: 82.88
    apples_agent-5_min: 42
    cleaning_beam_agent-0_max: 554
    cleaning_beam_agent-0_mean: 406.45
    cleaning_beam_agent-0_min: 255
    cleaning_beam_agent-1_max: 575
    cleaning_beam_agent-1_mean: 298.91
    cleaning_beam_agent-1_min: 136
    cleaning_beam_agent-2_max: 636
    cleaning_beam_agent-2_mean: 429.27
    cleaning_beam_agent-2_min: 258
    cleaning_beam_agent-3_max: 104
    cleaning_beam_agent-3_mean: 16.89
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 609
    cleaning_beam_agent-4_mean: 492.98
    cleaning_beam_agent-4_min: 401
    cleaning_beam_agent-5_max: 106
    cleaning_beam_agent-5_mean: 15.98
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-20-07
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1197.0000000000089
  episode_reward_mean: 1035.9699999999905
  episode_reward_min: 378.0000000000069
  episodes_this_iter: 96
  episodes_total: 28608
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19711.598
    learner:
      agent-0:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0809807777404785
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010699023259803653
        model: {}
        policy_loss: -0.0031824554316699505
        total_loss: -0.002851303666830063
        vf_explained_var: 0.029420852661132812
        vf_loss: 22.336767196655273
      agent-1:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1002823114395142
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017389431595802307
        model: {}
        policy_loss: -0.004120700992643833
        total_loss: -0.0035804100334644318
        vf_explained_var: -0.0372776985168457
        vf_loss: 24.767881393432617
      agent-2:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9616519808769226
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019508450059220195
        model: {}
        policy_loss: -0.003447180148214102
        total_loss: -0.002910565584897995
        vf_explained_var: 0.018653929233551025
        vf_loss: 22.291215896606445
      agent-3:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43845507502555847
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015601984923705459
        model: {}
        policy_loss: -0.002418141346424818
        total_loss: -0.0010901466012001038
        vf_explained_var: 0.07488438487052917
        vf_loss: 20.996747970581055
      agent-4:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8505237698554993
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015150465769693255
        model: {}
        policy_loss: -0.0036770631559193134
        total_loss: -0.003025307785719633
        vf_explained_var: 0.06297501921653748
        vf_loss: 21.486770629882812
      agent-5:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5117614269256592
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005655670538544655
        model: {}
        policy_loss: -0.002521337242797017
        total_loss: -0.001352175371721387
        vf_explained_var: 0.0955716073513031
        vf_loss: 20.698625564575195
    load_time_ms: 12919.665
    num_steps_sampled: 28608000
    num_steps_trained: 28608000
    sample_time_ms: 90402.884
    update_time_ms: 20.923
  iterations_since_restore: 238
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.523999999999996
    ram_util_percent: 10.070285714285719
  pid: 24061
  policy_reward_max:
    agent-0: 199.49999999999912
    agent-1: 199.49999999999912
    agent-2: 199.49999999999912
    agent-3: 199.49999999999912
    agent-4: 199.49999999999912
    agent-5: 199.49999999999912
  policy_reward_mean:
    agent-0: 172.6616666666665
    agent-1: 172.6616666666665
    agent-2: 172.6616666666665
    agent-3: 172.6616666666665
    agent-4: 172.6616666666665
    agent-5: 172.6616666666665
  policy_reward_min:
    agent-0: 62.999999999999694
    agent-1: 62.999999999999694
    agent-2: 62.999999999999694
    agent-3: 62.999999999999694
    agent-4: 62.999999999999694
    agent-5: 62.999999999999694
  sampler_perf:
    mean_env_wait_ms: 24.576378050653926
    mean_inference_ms: 12.230750675857461
    mean_processing_ms: 50.68162761699844
  time_since_restore: 30362.886255025864
  time_this_iter_s: 122.44140529632568
  time_total_s: 39488.89806890488
  timestamp: 1637054407
  timesteps_since_restore: 22848000
  timesteps_this_iter: 96000
  timesteps_total: 28608000
  training_iteration: 298
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 18.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    298 |          39488.9 | 28608000 |  1035.97 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 66
    apples_agent-0_mean: 3.14
    apples_agent-0_min: 0
    apples_agent-1_max: 182
    apples_agent-1_mean: 35.28
    apples_agent-1_min: 0
    apples_agent-2_max: 89
    apples_agent-2_mean: 2.63
    apples_agent-2_min: 0
    apples_agent-3_max: 214
    apples_agent-3_mean: 119.6
    apples_agent-3_min: 66
    apples_agent-4_max: 48
    apples_agent-4_mean: 0.48
    apples_agent-4_min: 0
    apples_agent-5_max: 209
    apples_agent-5_mean: 85.9
    apples_agent-5_min: 57
    cleaning_beam_agent-0_max: 539
    cleaning_beam_agent-0_mean: 409.68
    cleaning_beam_agent-0_min: 239
    cleaning_beam_agent-1_max: 601
    cleaning_beam_agent-1_mean: 298.52
    cleaning_beam_agent-1_min: 112
    cleaning_beam_agent-2_max: 552
    cleaning_beam_agent-2_mean: 432.22
    cleaning_beam_agent-2_min: 224
    cleaning_beam_agent-3_max: 79
    cleaning_beam_agent-3_mean: 13.09
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 617
    cleaning_beam_agent-4_mean: 506.28
    cleaning_beam_agent-4_min: 388
    cleaning_beam_agent-5_max: 52
    cleaning_beam_agent-5_mean: 16.18
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-22-11
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1213.0000000000082
  episode_reward_mean: 1068.4799999999927
  episode_reward_min: 758.9999999999977
  episodes_this_iter: 96
  episodes_total: 28704
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19744.735
    learner:
      agent-0:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0938624143600464
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015090766828507185
        model: {}
        policy_loss: -0.003135537728667259
        total_loss: -0.0029195663519203663
        vf_explained_var: 0.042704612016677856
        vf_loss: 21.411705017089844
      agent-1:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0935344696044922
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011953804641962051
        model: {}
        policy_loss: -0.003800121136009693
        total_loss: -0.003219792153686285
        vf_explained_var: -0.0437585711479187
        vf_loss: 25.049495697021484
      agent-2:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.960277259349823
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012760787503793836
        model: {}
        policy_loss: -0.002765157725661993
        total_loss: -0.0022965893149375916
        vf_explained_var: 0.0062445104122161865
        vf_loss: 21.5865478515625
      agent-3:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42181938886642456
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010077925398945808
        model: {}
        policy_loss: -0.001927895937114954
        total_loss: -0.0005725747905671597
        vf_explained_var: 0.03224833309650421
        vf_loss: 20.97722816467285
      agent-4:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8468407392501831
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015720331575721502
        model: {}
        policy_loss: -0.0032732216641306877
        total_loss: -0.0025364672765135765
        vf_explained_var: -0.002328440546989441
        vf_loss: 22.271942138671875
      agent-5:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5034681558609009
        entropy_coeff: 0.0017600000137463212
        kl: 0.00045802793465554714
        model: {}
        policy_loss: -0.002055269666016102
        total_loss: -0.000794856867287308
        vf_explained_var: 0.04011757671833038
        vf_loss: 21.46516990661621
    load_time_ms: 12922.911
    num_steps_sampled: 28704000
    num_steps_trained: 28704000
    sample_time_ms: 90410.85
    update_time_ms: 21.041
  iterations_since_restore: 239
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.448022598870057
    ram_util_percent: 10.07909604519774
  pid: 24061
  policy_reward_max:
    agent-0: 202.16666666666626
    agent-1: 202.16666666666626
    agent-2: 202.16666666666626
    agent-3: 202.16666666666626
    agent-4: 202.16666666666626
    agent-5: 202.16666666666626
  policy_reward_mean:
    agent-0: 178.07999999999984
    agent-1: 178.07999999999984
    agent-2: 178.07999999999984
    agent-3: 178.07999999999984
    agent-4: 178.07999999999984
    agent-5: 178.07999999999984
  policy_reward_min:
    agent-0: 126.50000000000028
    agent-1: 126.50000000000028
    agent-2: 126.50000000000028
    agent-3: 126.50000000000028
    agent-4: 126.50000000000028
    agent-5: 126.50000000000028
  sampler_perf:
    mean_env_wait_ms: 24.579778900937967
    mean_inference_ms: 12.230281837932825
    mean_processing_ms: 50.679841008672554
  time_since_restore: 30487.115347623825
  time_this_iter_s: 124.22909259796143
  time_total_s: 39613.12716150284
  timestamp: 1637054531
  timesteps_since_restore: 22944000
  timesteps_this_iter: 96000
  timesteps_total: 28704000
  training_iteration: 299
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 18.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    299 |          39613.1 | 28704000 |  1068.48 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 59
    apples_agent-0_mean: 3.77
    apples_agent-0_min: 0
    apples_agent-1_max: 154
    apples_agent-1_mean: 33.3
    apples_agent-1_min: 0
    apples_agent-2_max: 90
    apples_agent-2_mean: 3.69
    apples_agent-2_min: 0
    apples_agent-3_max: 214
    apples_agent-3_mean: 119.8
    apples_agent-3_min: 75
    apples_agent-4_max: 8
    apples_agent-4_mean: 0.08
    apples_agent-4_min: 0
    apples_agent-5_max: 209
    apples_agent-5_mean: 85.39
    apples_agent-5_min: 55
    cleaning_beam_agent-0_max: 570
    cleaning_beam_agent-0_mean: 405.28
    cleaning_beam_agent-0_min: 244
    cleaning_beam_agent-1_max: 613
    cleaning_beam_agent-1_mean: 307.94
    cleaning_beam_agent-1_min: 147
    cleaning_beam_agent-2_max: 565
    cleaning_beam_agent-2_mean: 424.98
    cleaning_beam_agent-2_min: 227
    cleaning_beam_agent-3_max: 56
    cleaning_beam_agent-3_mean: 14.25
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 555
    cleaning_beam_agent-4_mean: 494.4
    cleaning_beam_agent-4_min: 386
    cleaning_beam_agent-5_max: 79
    cleaning_beam_agent-5_mean: 14.85
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-24-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1217.0000000000036
  episode_reward_mean: 1064.6899999999926
  episode_reward_min: 770.9999999999898
  episodes_this_iter: 96
  episodes_total: 28800
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19769.028
    learner:
      agent-0:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1037100553512573
        entropy_coeff: 0.0017600000137463212
        kl: 0.001948515186086297
        model: {}
        policy_loss: -0.003241142723709345
        total_loss: -0.0029525691643357277
        vf_explained_var: 0.017464086413383484
        vf_loss: 22.311012268066406
      agent-1:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 1.094077706336975
        entropy_coeff: 0.0017600000137463212
        kl: 0.001224552746862173
        model: {}
        policy_loss: -0.004384593106806278
        total_loss: -0.0038255671970546246
        vf_explained_var: -0.003595709800720215
        vf_loss: 24.846057891845703
      agent-2:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9565069079399109
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012864168966189027
        model: {}
        policy_loss: -0.0029778387397527695
        total_loss: -0.002418794669210911
        vf_explained_var: 0.008326277136802673
        vf_loss: 22.424962997436523
      agent-3:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41972649097442627
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007372603286057711
        model: {}
        policy_loss: -0.0019637933000922203
        total_loss: -0.000577577855437994
        vf_explained_var: 0.05378197133541107
        vf_loss: 21.24933624267578
      agent-4:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.851937472820282
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010627100709825754
        model: {}
        policy_loss: -0.003114272141829133
        total_loss: -0.002398831769824028
        vf_explained_var: 0.019933372735977173
        vf_loss: 22.148496627807617
      agent-5:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5176580548286438
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010264478623867035
        model: {}
        policy_loss: -0.0024250433780252934
        total_loss: -0.0011273715645074844
        vf_explained_var: 0.03353342413902283
        vf_loss: 22.087493896484375
    load_time_ms: 12904.291
    num_steps_sampled: 28800000
    num_steps_trained: 28800000
    sample_time_ms: 90404.976
    update_time_ms: 21.237
  iterations_since_restore: 240
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.469540229885059
    ram_util_percent: 10.144252873563214
  pid: 24061
  policy_reward_max:
    agent-0: 202.83333333333357
    agent-1: 202.83333333333357
    agent-2: 202.83333333333357
    agent-3: 202.83333333333357
    agent-4: 202.83333333333357
    agent-5: 202.83333333333357
  policy_reward_mean:
    agent-0: 177.44833333333318
    agent-1: 177.44833333333318
    agent-2: 177.44833333333318
    agent-3: 177.44833333333318
    agent-4: 177.44833333333318
    agent-5: 177.44833333333318
  policy_reward_min:
    agent-0: 128.50000000000034
    agent-1: 128.50000000000034
    agent-2: 128.50000000000034
    agent-3: 128.50000000000034
    agent-4: 128.50000000000034
    agent-5: 128.50000000000034
  sampler_perf:
    mean_env_wait_ms: 24.582867709138117
    mean_inference_ms: 12.229596357069054
    mean_processing_ms: 50.6771614809936
  time_since_restore: 30609.73616528511
  time_this_iter_s: 122.6208176612854
  time_total_s: 39735.74797916412
  timestamp: 1637054654
  timesteps_since_restore: 23040000
  timesteps_this_iter: 96000
  timesteps_total: 28800000
  training_iteration: 300
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    300 |          39735.7 | 28800000 |  1064.69 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 88
    apples_agent-0_mean: 5.37
    apples_agent-0_min: 0
    apples_agent-1_max: 155
    apples_agent-1_mean: 30.08
    apples_agent-1_min: 0
    apples_agent-2_max: 50
    apples_agent-2_mean: 3.27
    apples_agent-2_min: 0
    apples_agent-3_max: 183
    apples_agent-3_mean: 116.09
    apples_agent-3_min: 0
    apples_agent-4_max: 52
    apples_agent-4_mean: 0.73
    apples_agent-4_min: 0
    apples_agent-5_max: 126
    apples_agent-5_mean: 83.25
    apples_agent-5_min: 45
    cleaning_beam_agent-0_max: 509
    cleaning_beam_agent-0_mean: 387.72
    cleaning_beam_agent-0_min: 189
    cleaning_beam_agent-1_max: 647
    cleaning_beam_agent-1_mean: 307.99
    cleaning_beam_agent-1_min: 122
    cleaning_beam_agent-2_max: 533
    cleaning_beam_agent-2_mean: 417.35
    cleaning_beam_agent-2_min: 212
    cleaning_beam_agent-3_max: 74
    cleaning_beam_agent-3_mean: 16.81
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 593
    cleaning_beam_agent-4_mean: 486.51
    cleaning_beam_agent-4_min: 343
    cleaning_beam_agent-5_max: 87
    cleaning_beam_agent-5_mean: 18.89
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-26-18
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1181.0000000000134
  episode_reward_mean: 1017.7999999999934
  episode_reward_min: 504.00000000000693
  episodes_this_iter: 96
  episodes_total: 28896
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19797.713
    learner:
      agent-0:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0928471088409424
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012740649981424212
        model: {}
        policy_loss: -0.0030586631037294865
        total_loss: -0.0027141892351210117
        vf_explained_var: 0.03858998417854309
        vf_loss: 22.678831100463867
      agent-1:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0854376554489136
        entropy_coeff: 0.0017600000137463212
        kl: 0.001465454464778304
        model: {}
        policy_loss: -0.004139904864132404
        total_loss: -0.003458211896941066
        vf_explained_var: -0.07280334830284119
        vf_loss: 25.92064666748047
      agent-2:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9675486087799072
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018330883467569947
        model: {}
        policy_loss: -0.0033623715862631798
        total_loss: -0.0028237272053956985
        vf_explained_var: 0.04759794473648071
        vf_loss: 22.415271759033203
      agent-3:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4331221282482147
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013341603334993124
        model: {}
        policy_loss: -0.002243656665086746
        total_loss: -0.0009716660715639591
        vf_explained_var: 0.1344926655292511
        vf_loss: 20.34282875061035
      agent-4:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8597820997238159
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018218804616481066
        model: {}
        policy_loss: -0.003718477673828602
        total_loss: -0.0029113488271832466
        vf_explained_var: 0.01591600477695465
        vf_loss: 23.203481674194336
      agent-5:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5357073545455933
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009343846468254924
        model: {}
        policy_loss: -0.00236678309738636
        total_loss: -0.0010882476344704628
        vf_explained_var: 0.06247207522392273
        vf_loss: 22.213762283325195
    load_time_ms: 12826.519
    num_steps_sampled: 28896000
    num_steps_trained: 28896000
    sample_time_ms: 90490.638
    update_time_ms: 20.378
  iterations_since_restore: 241
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.405084745762714
    ram_util_percent: 10.131638418079095
  pid: 24061
  policy_reward_max:
    agent-0: 196.8333333333333
    agent-1: 196.8333333333333
    agent-2: 196.8333333333333
    agent-3: 196.8333333333333
    agent-4: 196.8333333333333
    agent-5: 196.8333333333333
  policy_reward_mean:
    agent-0: 169.63333333333324
    agent-1: 169.63333333333324
    agent-2: 169.63333333333324
    agent-3: 169.63333333333324
    agent-4: 169.63333333333324
    agent-5: 169.63333333333324
  policy_reward_min:
    agent-0: 84.0000000000001
    agent-1: 84.0000000000001
    agent-2: 84.0000000000001
    agent-3: 84.0000000000001
    agent-4: 84.0000000000001
    agent-5: 84.0000000000001
  sampler_perf:
    mean_env_wait_ms: 24.585427610445205
    mean_inference_ms: 12.229330128862337
    mean_processing_ms: 50.67490155583749
  time_since_restore: 30733.175941467285
  time_this_iter_s: 123.43977618217468
  time_total_s: 39859.1877553463
  timestamp: 1637054778
  timesteps_since_restore: 23136000
  timesteps_this_iter: 96000
  timesteps_total: 28896000
  training_iteration: 301
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 18.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    301 |          39859.2 | 28896000 |   1017.8 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 30
    apples_agent-0_mean: 2.05
    apples_agent-0_min: 0
    apples_agent-1_max: 139
    apples_agent-1_mean: 36.43
    apples_agent-1_min: 0
    apples_agent-2_max: 230
    apples_agent-2_mean: 2.93
    apples_agent-2_min: 0
    apples_agent-3_max: 205
    apples_agent-3_mean: 119.7
    apples_agent-3_min: 56
    apples_agent-4_max: 18
    apples_agent-4_mean: 0.71
    apples_agent-4_min: 0
    apples_agent-5_max: 135
    apples_agent-5_mean: 82.0
    apples_agent-5_min: 40
    cleaning_beam_agent-0_max: 507
    cleaning_beam_agent-0_mean: 394.31
    cleaning_beam_agent-0_min: 248
    cleaning_beam_agent-1_max: 455
    cleaning_beam_agent-1_mean: 279.43
    cleaning_beam_agent-1_min: 107
    cleaning_beam_agent-2_max: 526
    cleaning_beam_agent-2_mean: 408.64
    cleaning_beam_agent-2_min: 87
    cleaning_beam_agent-3_max: 42
    cleaning_beam_agent-3_mean: 12.9
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 614
    cleaning_beam_agent-4_mean: 488.23
    cleaning_beam_agent-4_min: 324
    cleaning_beam_agent-5_max: 137
    cleaning_beam_agent-5_mean: 16.77
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-28-22
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1259.999999999992
  episode_reward_mean: 1060.0399999999922
  episode_reward_min: 537.0000000000007
  episodes_this_iter: 96
  episodes_total: 28992
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19813.546
    learner:
      agent-0:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 1.100818157196045
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015303094405680895
        model: {}
        policy_loss: -0.0032957298681139946
        total_loss: -0.0028234869241714478
        vf_explained_var: -0.012299507856369019
        vf_loss: 24.096843719482422
      agent-1:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0844061374664307
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013531084405258298
        model: {}
        policy_loss: -0.0041181244887411594
        total_loss: -0.00333941588178277
        vf_explained_var: -0.046122074127197266
        vf_loss: 26.87262725830078
      agent-2:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.95650714635849
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014722226187586784
        model: {}
        policy_loss: -0.0030945881735533476
        total_loss: -0.002457549562677741
        vf_explained_var: 0.01836813986301422
        vf_loss: 23.20489501953125
      agent-3:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40695664286613464
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009908201172947884
        model: {}
        policy_loss: -0.002053382806479931
        total_loss: -0.0005178153514862061
        vf_explained_var: 0.04918970167636871
        vf_loss: 22.518112182617188
      agent-4:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8581684231758118
        entropy_coeff: 0.0017600000137463212
        kl: 0.001730309915728867
        model: {}
        policy_loss: -0.0033557689748704433
        total_loss: -0.0025293880607932806
        vf_explained_var: 0.030442938208580017
        vf_loss: 23.367589950561523
      agent-5:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5130733847618103
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008041950641199946
        model: {}
        policy_loss: -0.0023275152780115604
        total_loss: -0.000998609815724194
        vf_explained_var: 0.07408683001995087
        vf_loss: 22.31911849975586
    load_time_ms: 12911.138
    num_steps_sampled: 28992000
    num_steps_trained: 28992000
    sample_time_ms: 90390.76
    update_time_ms: 20.548
  iterations_since_restore: 242
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.34715909090909
    ram_util_percent: 10.105113636363637
  pid: 24061
  policy_reward_max:
    agent-0: 209.99999999999983
    agent-1: 209.99999999999983
    agent-2: 209.99999999999983
    agent-3: 209.99999999999983
    agent-4: 209.99999999999983
    agent-5: 209.99999999999983
  policy_reward_mean:
    agent-0: 176.6733333333332
    agent-1: 176.6733333333332
    agent-2: 176.6733333333332
    agent-3: 176.6733333333332
    agent-4: 176.6733333333332
    agent-5: 176.6733333333332
  policy_reward_min:
    agent-0: 89.50000000000001
    agent-1: 89.50000000000001
    agent-2: 89.50000000000001
    agent-3: 89.50000000000001
    agent-4: 89.50000000000001
    agent-5: 89.50000000000001
  sampler_perf:
    mean_env_wait_ms: 24.587042105014444
    mean_inference_ms: 12.228806812275714
    mean_processing_ms: 50.672671475393884
  time_since_restore: 30856.738634347916
  time_this_iter_s: 123.5626928806305
  time_total_s: 39982.75044822693
  timestamp: 1637054902
  timesteps_since_restore: 23232000
  timesteps_this_iter: 96000
  timesteps_total: 28992000
  training_iteration: 302
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    302 |          39982.8 | 28992000 |  1060.04 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 37
    apples_agent-0_mean: 1.38
    apples_agent-0_min: 0
    apples_agent-1_max: 109
    apples_agent-1_mean: 31.55
    apples_agent-1_min: 0
    apples_agent-2_max: 27
    apples_agent-2_mean: 1.31
    apples_agent-2_min: 0
    apples_agent-3_max: 224
    apples_agent-3_mean: 118.3
    apples_agent-3_min: 48
    apples_agent-4_max: 50
    apples_agent-4_mean: 1.07
    apples_agent-4_min: 0
    apples_agent-5_max: 181
    apples_agent-5_mean: 83.25
    apples_agent-5_min: 52
    cleaning_beam_agent-0_max: 526
    cleaning_beam_agent-0_mean: 402.04
    cleaning_beam_agent-0_min: 277
    cleaning_beam_agent-1_max: 563
    cleaning_beam_agent-1_mean: 297.22
    cleaning_beam_agent-1_min: 123
    cleaning_beam_agent-2_max: 572
    cleaning_beam_agent-2_mean: 428.47
    cleaning_beam_agent-2_min: 211
    cleaning_beam_agent-3_max: 110
    cleaning_beam_agent-3_mean: 13.07
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 576
    cleaning_beam_agent-4_mean: 487.04
    cleaning_beam_agent-4_min: 379
    cleaning_beam_agent-5_max: 89
    cleaning_beam_agent-5_mean: 15.38
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-30-25
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1269.000000000011
  episode_reward_mean: 1073.0599999999938
  episode_reward_min: 602.9999999999937
  episodes_this_iter: 96
  episodes_total: 29088
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19783.736
    learner:
      agent-0:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1011908054351807
        entropy_coeff: 0.0017600000137463212
        kl: 0.001678400905802846
        model: {}
        policy_loss: -0.0032814566511660814
        total_loss: -0.0029080486856400967
        vf_explained_var: 0.022975191473960876
        vf_loss: 23.115036010742188
      agent-1:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0929393768310547
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014108279719948769
        model: {}
        policy_loss: -0.0038451123982667923
        total_loss: -0.003135979175567627
        vf_explained_var: -0.0463065505027771
        vf_loss: 26.327056884765625
      agent-2:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9554962515830994
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017911340110003948
        model: {}
        policy_loss: -0.0031060082837939262
        total_loss: -0.0025277063250541687
        vf_explained_var: 0.03488340973854065
        vf_loss: 22.599721908569336
      agent-3:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4053071141242981
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008647999493405223
        model: {}
        policy_loss: -0.0020629162900149822
        total_loss: -0.000519266352057457
        vf_explained_var: 0.03817906975746155
        vf_loss: 22.569915771484375
      agent-4:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8601603507995605
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020786463283002377
        model: {}
        policy_loss: -0.003555081784725189
        total_loss: -0.002723296172916889
        vf_explained_var: 0.020077571272850037
        vf_loss: 23.456680297851562
      agent-5:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5032098889350891
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008342075743712485
        model: {}
        policy_loss: -0.002217154484242201
        total_loss: -0.0008711880072951317
        vf_explained_var: 0.0640174150466919
        vf_loss: 22.316146850585938
    load_time_ms: 12907.071
    num_steps_sampled: 29088000
    num_steps_trained: 29088000
    sample_time_ms: 90373.397
    update_time_ms: 20.515
  iterations_since_restore: 243
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.50971428571429
    ram_util_percent: 10.138857142857145
  pid: 24061
  policy_reward_max:
    agent-0: 211.49999999999986
    agent-1: 211.49999999999986
    agent-2: 211.49999999999986
    agent-3: 211.49999999999986
    agent-4: 211.49999999999986
    agent-5: 211.49999999999986
  policy_reward_mean:
    agent-0: 178.84333333333316
    agent-1: 178.84333333333316
    agent-2: 178.84333333333316
    agent-3: 178.84333333333316
    agent-4: 178.84333333333316
    agent-5: 178.84333333333316
  policy_reward_min:
    agent-0: 100.50000000000054
    agent-1: 100.50000000000054
    agent-2: 100.50000000000054
    agent-3: 100.50000000000054
    agent-4: 100.50000000000054
    agent-5: 100.50000000000054
  sampler_perf:
    mean_env_wait_ms: 24.589844867016158
    mean_inference_ms: 12.228501390997156
    mean_processing_ms: 50.67076939885221
  time_since_restore: 30979.630279541016
  time_this_iter_s: 122.89164519309998
  time_total_s: 40105.64209342003
  timestamp: 1637055025
  timesteps_since_restore: 23328000
  timesteps_this_iter: 96000
  timesteps_total: 29088000
  training_iteration: 303
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 18.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    303 |          40105.6 | 29088000 |  1073.06 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 78
    apples_agent-0_mean: 2.63
    apples_agent-0_min: 0
    apples_agent-1_max: 101
    apples_agent-1_mean: 31.9
    apples_agent-1_min: 0
    apples_agent-2_max: 112
    apples_agent-2_mean: 3.14
    apples_agent-2_min: 0
    apples_agent-3_max: 277
    apples_agent-3_mean: 115.94
    apples_agent-3_min: 61
    apples_agent-4_max: 61
    apples_agent-4_mean: 1.26
    apples_agent-4_min: 0
    apples_agent-5_max: 322
    apples_agent-5_mean: 84.29
    apples_agent-5_min: 40
    cleaning_beam_agent-0_max: 521
    cleaning_beam_agent-0_mean: 387.04
    cleaning_beam_agent-0_min: 231
    cleaning_beam_agent-1_max: 549
    cleaning_beam_agent-1_mean: 282.77
    cleaning_beam_agent-1_min: 136
    cleaning_beam_agent-2_max: 571
    cleaning_beam_agent-2_mean: 431.81
    cleaning_beam_agent-2_min: 200
    cleaning_beam_agent-3_max: 85
    cleaning_beam_agent-3_mean: 17.12
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 589
    cleaning_beam_agent-4_mean: 466.89
    cleaning_beam_agent-4_min: 331
    cleaning_beam_agent-5_max: 86
    cleaning_beam_agent-5_mean: 16.44
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-32-28
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1258.9999999999945
  episode_reward_mean: 1027.9399999999932
  episode_reward_min: 370.99999999999727
  episodes_this_iter: 96
  episodes_total: 29184
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19783.606
    learner:
      agent-0:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1086417436599731
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019468613900244236
        model: {}
        policy_loss: -0.003127044066786766
        total_loss: -0.0025091564748436213
        vf_explained_var: -0.008798718452453613
        vf_loss: 25.690980911254883
      agent-1:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0808472633361816
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010581647511571646
        model: {}
        policy_loss: -0.003951956517994404
        total_loss: -0.0030983630567789078
        vf_explained_var: -0.043645769357681274
        vf_loss: 27.558826446533203
      agent-2:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9607135057449341
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016833983827382326
        model: {}
        policy_loss: -0.0030974256806075573
        total_loss: -0.002430351683869958
        vf_explained_var: 0.07142066955566406
        vf_loss: 23.579301834106445
      agent-3:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4372248649597168
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007606691215187311
        model: {}
        policy_loss: -0.0021116065327078104
        total_loss: -0.0006440082797780633
        vf_explained_var: 0.11917337775230408
        vf_loss: 22.371139526367188
      agent-4:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8597015142440796
        entropy_coeff: 0.0017600000137463212
        kl: 0.00219766516238451
        model: {}
        policy_loss: -0.003615791443735361
        total_loss: -0.0026604575105011463
        vf_explained_var: 0.03275175392627716
        vf_loss: 24.684112548828125
      agent-5:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5281699895858765
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011684988858178258
        model: {}
        policy_loss: -0.0023887609131634235
        total_loss: -0.0010564262047410011
        vf_explained_var: 0.11411918699741364
        vf_loss: 22.6191463470459
    load_time_ms: 12920.781
    num_steps_sampled: 29184000
    num_steps_trained: 29184000
    sample_time_ms: 90382.351
    update_time_ms: 20.323
  iterations_since_restore: 244
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.94715909090909
    ram_util_percent: 10.223295454545452
  pid: 24061
  policy_reward_max:
    agent-0: 209.83333333333275
    agent-1: 209.83333333333275
    agent-2: 209.83333333333275
    agent-3: 209.83333333333275
    agent-4: 209.83333333333275
    agent-5: 209.83333333333275
  policy_reward_mean:
    agent-0: 171.32333333333318
    agent-1: 171.32333333333318
    agent-2: 171.32333333333318
    agent-3: 171.32333333333318
    agent-4: 171.32333333333318
    agent-5: 171.32333333333318
  policy_reward_min:
    agent-0: 61.83333333333332
    agent-1: 61.83333333333332
    agent-2: 61.83333333333332
    agent-3: 61.83333333333332
    agent-4: 61.83333333333332
    agent-5: 61.83333333333332
  sampler_perf:
    mean_env_wait_ms: 24.591939703358545
    mean_inference_ms: 12.228125794892161
    mean_processing_ms: 50.66876741913967
  time_since_restore: 31102.959394693375
  time_this_iter_s: 123.32911515235901
  time_total_s: 40228.97120857239
  timestamp: 1637055148
  timesteps_since_restore: 23424000
  timesteps_this_iter: 96000
  timesteps_total: 29184000
  training_iteration: 304
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 18.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    304 |            40229 | 29184000 |  1027.94 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 38
    apples_agent-0_mean: 2.92
    apples_agent-0_min: 0
    apples_agent-1_max: 132
    apples_agent-1_mean: 30.59
    apples_agent-1_min: 0
    apples_agent-2_max: 65
    apples_agent-2_mean: 3.58
    apples_agent-2_min: 0
    apples_agent-3_max: 208
    apples_agent-3_mean: 119.9
    apples_agent-3_min: 71
    apples_agent-4_max: 40
    apples_agent-4_mean: 0.72
    apples_agent-4_min: 0
    apples_agent-5_max: 141
    apples_agent-5_mean: 84.57
    apples_agent-5_min: 43
    cleaning_beam_agent-0_max: 508
    cleaning_beam_agent-0_mean: 370.1
    cleaning_beam_agent-0_min: 218
    cleaning_beam_agent-1_max: 635
    cleaning_beam_agent-1_mean: 313.37
    cleaning_beam_agent-1_min: 111
    cleaning_beam_agent-2_max: 566
    cleaning_beam_agent-2_mean: 429.56
    cleaning_beam_agent-2_min: 227
    cleaning_beam_agent-3_max: 78
    cleaning_beam_agent-3_mean: 14.43
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 599
    cleaning_beam_agent-4_mean: 491.98
    cleaning_beam_agent-4_min: 323
    cleaning_beam_agent-5_max: 56
    cleaning_beam_agent-5_mean: 15.56
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-34-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1259.0000000000036
  episode_reward_mean: 1058.7599999999925
  episode_reward_min: 602.999999999998
  episodes_this_iter: 96
  episodes_total: 29280
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19747.955
    learner:
      agent-0:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1195685863494873
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012146418448537588
        model: {}
        policy_loss: -0.0031068320386111736
        total_loss: -0.0027447049506008625
        vf_explained_var: -0.00370180606842041
        vf_loss: 23.325668334960938
      agent-1:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0686219930648804
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015235103201121092
        model: {}
        policy_loss: -0.004146837629377842
        total_loss: -0.003501342609524727
        vf_explained_var: -0.04202762246131897
        vf_loss: 25.262706756591797
      agent-2:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9515004754066467
        entropy_coeff: 0.0017600000137463212
        kl: 0.001066041411831975
        model: {}
        policy_loss: -0.0029255130793899298
        total_loss: -0.002318735234439373
        vf_explained_var: 0.0012011528015136719
        vf_loss: 22.81419563293457
      agent-3:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4037846326828003
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010284065501764417
        model: {}
        policy_loss: -0.002162102609872818
        total_loss: -0.0007120305672287941
        vf_explained_var: 0.0457221120595932
        vf_loss: 21.607349395751953
      agent-4:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8578362464904785
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014669930096715689
        model: {}
        policy_loss: -0.0034845208283513784
        total_loss: -0.0026721537578850985
        vf_explained_var: -0.009416833519935608
        vf_loss: 23.221614837646484
      agent-5:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5042968988418579
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007660512928850949
        model: {}
        policy_loss: -0.0022230036556720734
        total_loss: -0.0010109250433743
        vf_explained_var: 0.09167969226837158
        vf_loss: 20.99639129638672
    load_time_ms: 12926.097
    num_steps_sampled: 29280000
    num_steps_trained: 29280000
    sample_time_ms: 90494.072
    update_time_ms: 22.662
  iterations_since_restore: 245
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.439772727272725
    ram_util_percent: 10.139772727272726
  pid: 24061
  policy_reward_max:
    agent-0: 209.8333333333331
    agent-1: 209.8333333333331
    agent-2: 209.8333333333331
    agent-3: 209.8333333333331
    agent-4: 209.8333333333331
    agent-5: 209.8333333333331
  policy_reward_mean:
    agent-0: 176.4599999999998
    agent-1: 176.4599999999998
    agent-2: 176.4599999999998
    agent-3: 176.4599999999998
    agent-4: 176.4599999999998
    agent-5: 176.4599999999998
  policy_reward_min:
    agent-0: 100.5000000000001
    agent-1: 100.5000000000001
    agent-2: 100.5000000000001
    agent-3: 100.5000000000001
    agent-4: 100.5000000000001
    agent-5: 100.5000000000001
  sampler_perf:
    mean_env_wait_ms: 24.59417106928316
    mean_inference_ms: 12.227794170648242
    mean_processing_ms: 50.66684040073022
  time_since_restore: 31226.67084288597
  time_this_iter_s: 123.71144819259644
  time_total_s: 40352.682656764984
  timestamp: 1637055272
  timesteps_since_restore: 23520000
  timesteps_this_iter: 96000
  timesteps_total: 29280000
  training_iteration: 305
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    305 |          40352.7 | 29280000 |  1058.76 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 64
    apples_agent-0_mean: 5.26
    apples_agent-0_min: 0
    apples_agent-1_max: 150
    apples_agent-1_mean: 33.11
    apples_agent-1_min: 0
    apples_agent-2_max: 87
    apples_agent-2_mean: 5.92
    apples_agent-2_min: 0
    apples_agent-3_max: 194
    apples_agent-3_mean: 112.88
    apples_agent-3_min: 5
    apples_agent-4_max: 52
    apples_agent-4_mean: 1.4
    apples_agent-4_min: 0
    apples_agent-5_max: 130
    apples_agent-5_mean: 81.65
    apples_agent-5_min: 51
    cleaning_beam_agent-0_max: 514
    cleaning_beam_agent-0_mean: 367.55
    cleaning_beam_agent-0_min: 159
    cleaning_beam_agent-1_max: 496
    cleaning_beam_agent-1_mean: 301.1
    cleaning_beam_agent-1_min: 142
    cleaning_beam_agent-2_max: 585
    cleaning_beam_agent-2_mean: 414.87
    cleaning_beam_agent-2_min: 158
    cleaning_beam_agent-3_max: 84
    cleaning_beam_agent-3_mean: 16.69
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 592
    cleaning_beam_agent-4_mean: 485.27
    cleaning_beam_agent-4_min: 301
    cleaning_beam_agent-5_max: 71
    cleaning_beam_agent-5_mean: 18.3
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-36-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1205.0000000000114
  episode_reward_mean: 1021.0099999999907
  episode_reward_min: 450.000000000009
  episodes_this_iter: 96
  episodes_total: 29376
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19737.275
    learner:
      agent-0:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1222939491271973
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012385252630338073
        model: {}
        policy_loss: -0.0030488406773656607
        total_loss: -0.0027685377281159163
        vf_explained_var: 0.0017118006944656372
        vf_loss: 22.5554141998291
      agent-1:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 1.082750678062439
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014330617850646377
        model: {}
        policy_loss: -0.0043832678347826
        total_loss: -0.0037632323801517487
        vf_explained_var: -0.09155842661857605
        vf_loss: 25.25678062438965
      agent-2:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9666197299957275
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011955243535339832
        model: {}
        policy_loss: -0.0030642738565802574
        total_loss: -0.002437048126012087
        vf_explained_var: -0.03606405854225159
        vf_loss: 23.284759521484375
      agent-3:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4341447055339813
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015182776842266321
        model: {}
        policy_loss: -0.0023863434325903654
        total_loss: -0.0010802573524415493
        vf_explained_var: 0.07737080752849579
        vf_loss: 20.70180320739746
      agent-4:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8601082563400269
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017996456008404493
        model: {}
        policy_loss: -0.0034904289059340954
        total_loss: -0.0029139420948922634
        vf_explained_var: 0.07211573421955109
        vf_loss: 20.902772903442383
      agent-5:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.532323956489563
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007104561082087457
        model: {}
        policy_loss: -0.002416379516944289
        total_loss: -0.0012341177789494395
        vf_explained_var: 0.0643053650856018
        vf_loss: 21.191539764404297
    load_time_ms: 12928.029
    num_steps_sampled: 29376000
    num_steps_trained: 29376000
    sample_time_ms: 90483.219
    update_time_ms: 22.479
  iterations_since_restore: 246
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.281920903954802
    ram_util_percent: 10.127118644067796
  pid: 24061
  policy_reward_max:
    agent-0: 200.83333333333246
    agent-1: 200.83333333333246
    agent-2: 200.83333333333246
    agent-3: 200.83333333333246
    agent-4: 200.83333333333246
    agent-5: 200.83333333333246
  policy_reward_mean:
    agent-0: 170.1683333333332
    agent-1: 170.1683333333332
    agent-2: 170.1683333333332
    agent-3: 170.1683333333332
    agent-4: 170.1683333333332
    agent-5: 170.1683333333332
  policy_reward_min:
    agent-0: 75.00000000000001
    agent-1: 75.00000000000001
    agent-2: 75.00000000000001
    agent-3: 75.00000000000001
    agent-4: 75.00000000000001
    agent-5: 75.00000000000001
  sampler_perf:
    mean_env_wait_ms: 24.595607463082064
    mean_inference_ms: 12.227240737269376
    mean_processing_ms: 50.664152901491896
  time_since_restore: 31350.306885242462
  time_this_iter_s: 123.63604235649109
  time_total_s: 40476.318699121475
  timestamp: 1637055396
  timesteps_since_restore: 23616000
  timesteps_this_iter: 96000
  timesteps_total: 29376000
  training_iteration: 306
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    306 |          40476.3 | 29376000 |  1021.01 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 113
    apples_agent-0_mean: 5.55
    apples_agent-0_min: 0
    apples_agent-1_max: 93
    apples_agent-1_mean: 27.85
    apples_agent-1_min: 0
    apples_agent-2_max: 112
    apples_agent-2_mean: 4.89
    apples_agent-2_min: 0
    apples_agent-3_max: 354
    apples_agent-3_mean: 119.48
    apples_agent-3_min: 55
    apples_agent-4_max: 52
    apples_agent-4_mean: 1.19
    apples_agent-4_min: 0
    apples_agent-5_max: 307
    apples_agent-5_mean: 83.7
    apples_agent-5_min: 51
    cleaning_beam_agent-0_max: 499
    cleaning_beam_agent-0_mean: 369.56
    cleaning_beam_agent-0_min: 213
    cleaning_beam_agent-1_max: 513
    cleaning_beam_agent-1_mean: 314.72
    cleaning_beam_agent-1_min: 115
    cleaning_beam_agent-2_max: 566
    cleaning_beam_agent-2_mean: 432.53
    cleaning_beam_agent-2_min: 178
    cleaning_beam_agent-3_max: 84
    cleaning_beam_agent-3_mean: 11.99
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 568
    cleaning_beam_agent-4_mean: 490.53
    cleaning_beam_agent-4_min: 301
    cleaning_beam_agent-5_max: 70
    cleaning_beam_agent-5_mean: 15.36
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-38-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1249.9999999999911
  episode_reward_mean: 1064.6999999999944
  episode_reward_min: 450.000000000009
  episodes_this_iter: 96
  episodes_total: 29472
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19740.04
    learner:
      agent-0:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1278767585754395
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015813888749107718
        model: {}
        policy_loss: -0.0031222011893987656
        total_loss: -0.0027332098688930273
        vf_explained_var: 0.016642272472381592
        vf_loss: 23.74056625366211
      agent-1:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 1.080851674079895
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013960401993244886
        model: {}
        policy_loss: -0.0037488117814064026
        total_loss: -0.002984246937558055
        vf_explained_var: -0.05661541223526001
        vf_loss: 26.66864585876465
      agent-2:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9513364434242249
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018891707295551896
        model: {}
        policy_loss: -0.0032154694199562073
        total_loss: -0.002603676402941346
        vf_explained_var: 0.03917902708053589
        vf_loss: 22.861452102661133
      agent-3:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39054739475250244
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015728475991636515
        model: {}
        policy_loss: -0.0022561275400221348
        total_loss: -0.0007802913896739483
        vf_explained_var: 0.06792448461055756
        vf_loss: 21.631986618041992
      agent-4:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8592895269393921
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017515842337161303
        model: {}
        policy_loss: -0.003561060642823577
        total_loss: -0.0027165450155735016
        vf_explained_var: 0.009315818548202515
        vf_loss: 23.568683624267578
      agent-5:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49389946460723877
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008044069400057197
        model: {}
        policy_loss: -0.0023496784269809723
        total_loss: -0.001004253514111042
        vf_explained_var: 0.06884646415710449
        vf_loss: 22.146886825561523
    load_time_ms: 12929.893
    num_steps_sampled: 29472000
    num_steps_trained: 29472000
    sample_time_ms: 90575.264
    update_time_ms: 23.424
  iterations_since_restore: 247
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.460795454545456
    ram_util_percent: 10.16590909090909
  pid: 24061
  policy_reward_max:
    agent-0: 208.33333333333348
    agent-1: 208.33333333333348
    agent-2: 208.33333333333348
    agent-3: 208.33333333333348
    agent-4: 208.33333333333348
    agent-5: 208.33333333333348
  policy_reward_mean:
    agent-0: 177.44999999999985
    agent-1: 177.44999999999985
    agent-2: 177.44999999999985
    agent-3: 177.44999999999985
    agent-4: 177.44999999999985
    agent-5: 177.44999999999985
  policy_reward_min:
    agent-0: 75.00000000000001
    agent-1: 75.00000000000001
    agent-2: 75.00000000000001
    agent-3: 75.00000000000001
    agent-4: 75.00000000000001
    agent-5: 75.00000000000001
  sampler_perf:
    mean_env_wait_ms: 24.59859091618737
    mean_inference_ms: 12.226633936777311
    mean_processing_ms: 50.6619368742418
  time_since_restore: 31474.037558078766
  time_this_iter_s: 123.73067283630371
  time_total_s: 40600.04937195778
  timestamp: 1637055520
  timesteps_since_restore: 23712000
  timesteps_this_iter: 96000
  timesteps_total: 29472000
  training_iteration: 307
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    307 |            40600 | 29472000 |   1064.7 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 48
    apples_agent-0_mean: 4.39
    apples_agent-0_min: 0
    apples_agent-1_max: 172
    apples_agent-1_mean: 38.82
    apples_agent-1_min: 0
    apples_agent-2_max: 27
    apples_agent-2_mean: 1.19
    apples_agent-2_min: 0
    apples_agent-3_max: 194
    apples_agent-3_mean: 123.96
    apples_agent-3_min: 49
    apples_agent-4_max: 45
    apples_agent-4_mean: 0.54
    apples_agent-4_min: 0
    apples_agent-5_max: 132
    apples_agent-5_mean: 81.68
    apples_agent-5_min: 58
    cleaning_beam_agent-0_max: 479
    cleaning_beam_agent-0_mean: 367.1
    cleaning_beam_agent-0_min: 210
    cleaning_beam_agent-1_max: 563
    cleaning_beam_agent-1_mean: 292.28
    cleaning_beam_agent-1_min: 120
    cleaning_beam_agent-2_max: 573
    cleaning_beam_agent-2_mean: 425.07
    cleaning_beam_agent-2_min: 281
    cleaning_beam_agent-3_max: 50
    cleaning_beam_agent-3_mean: 13.27
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 582
    cleaning_beam_agent-4_mean: 478.8
    cleaning_beam_agent-4_min: 336
    cleaning_beam_agent-5_max: 73
    cleaning_beam_agent-5_mean: 17.17
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-40-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1240.0
  episode_reward_mean: 1060.4899999999911
  episode_reward_min: 649.9999999999911
  episodes_this_iter: 96
  episodes_total: 29568
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19734.846
    learner:
      agent-0:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 1.118751049041748
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015793959610164165
        model: {}
        policy_loss: -0.003407775890082121
        total_loss: -0.0030056447722017765
        vf_explained_var: 0.02660459280014038
        vf_loss: 23.711332321166992
      agent-1:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0742669105529785
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012674655299633741
        model: {}
        policy_loss: -0.0038730185478925705
        total_loss: -0.0030043618753552437
        vf_explained_var: -0.06999638676643372
        vf_loss: 27.59368324279785
      agent-2:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9566574692726135
        entropy_coeff: 0.0017600000137463212
        kl: 0.001791169517673552
        model: {}
        policy_loss: -0.0033946926705539227
        total_loss: -0.0028177648782730103
        vf_explained_var: 0.05168911814689636
        vf_loss: 22.6064510345459
      agent-3:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41554972529411316
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008654791163280606
        model: {}
        policy_loss: -0.00217616674490273
        total_loss: -0.000703723169863224
        vf_explained_var: 0.06997357308864594
        vf_loss: 22.038118362426758
      agent-4:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8601038455963135
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015377774834632874
        model: {}
        policy_loss: -0.0035961680114269257
        total_loss: -0.0027139736339449883
        vf_explained_var: 0.008539482951164246
        vf_loss: 23.959758758544922
      agent-5:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.501379668712616
        entropy_coeff: 0.0017600000137463212
        kl: 0.0004892130382359028
        model: {}
        policy_loss: -0.002279637847095728
        total_loss: -0.000914327334612608
        vf_explained_var: 0.07287262380123138
        vf_loss: 22.477386474609375
    load_time_ms: 12938.404
    num_steps_sampled: 29568000
    num_steps_trained: 29568000
    sample_time_ms: 90595.874
    update_time_ms: 23.722
  iterations_since_restore: 248
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.440571428571426
    ram_util_percent: 10.146857142857142
  pid: 24061
  policy_reward_max:
    agent-0: 206.6666666666672
    agent-1: 206.6666666666672
    agent-2: 206.6666666666672
    agent-3: 206.6666666666672
    agent-4: 206.6666666666672
    agent-5: 206.6666666666672
  policy_reward_mean:
    agent-0: 176.74833333333322
    agent-1: 176.74833333333322
    agent-2: 176.74833333333322
    agent-3: 176.74833333333322
    agent-4: 176.74833333333322
    agent-5: 176.74833333333322
  policy_reward_min:
    agent-0: 108.33333333333346
    agent-1: 108.33333333333346
    agent-2: 108.33333333333346
    agent-3: 108.33333333333346
    agent-4: 108.33333333333346
    agent-5: 108.33333333333346
  sampler_perf:
    mean_env_wait_ms: 24.60024771246124
    mean_inference_ms: 12.225996820810927
    mean_processing_ms: 50.65859723010662
  time_since_restore: 31596.744042634964
  time_this_iter_s: 122.70648455619812
  time_total_s: 40722.75585651398
  timestamp: 1637055643
  timesteps_since_restore: 23808000
  timesteps_this_iter: 96000
  timesteps_total: 29568000
  training_iteration: 308
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 18.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    308 |          40722.8 | 29568000 |  1060.49 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 93
    apples_agent-0_mean: 5.89
    apples_agent-0_min: 0
    apples_agent-1_max: 166
    apples_agent-1_mean: 32.94
    apples_agent-1_min: 0
    apples_agent-2_max: 50
    apples_agent-2_mean: 2.02
    apples_agent-2_min: 0
    apples_agent-3_max: 194
    apples_agent-3_mean: 121.42
    apples_agent-3_min: 57
    apples_agent-4_max: 57
    apples_agent-4_mean: 1.21
    apples_agent-4_min: 0
    apples_agent-5_max: 147
    apples_agent-5_mean: 82.15
    apples_agent-5_min: 39
    cleaning_beam_agent-0_max: 572
    cleaning_beam_agent-0_mean: 380.38
    cleaning_beam_agent-0_min: 193
    cleaning_beam_agent-1_max: 494
    cleaning_beam_agent-1_mean: 293.18
    cleaning_beam_agent-1_min: 122
    cleaning_beam_agent-2_max: 578
    cleaning_beam_agent-2_mean: 421.8
    cleaning_beam_agent-2_min: 226
    cleaning_beam_agent-3_max: 41
    cleaning_beam_agent-3_mean: 12.34
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 565
    cleaning_beam_agent-4_mean: 476.77
    cleaning_beam_agent-4_min: 271
    cleaning_beam_agent-5_max: 103
    cleaning_beam_agent-5_mean: 15.36
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-42-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1222.0000000000007
  episode_reward_mean: 1060.3999999999917
  episode_reward_min: 455.00000000001444
  episodes_this_iter: 96
  episodes_total: 29664
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19719.149
    learner:
      agent-0:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1177101135253906
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018723913235589862
        model: {}
        policy_loss: -0.0033522769808769226
        total_loss: -0.002970408881083131
        vf_explained_var: 0.046263739466667175
        vf_loss: 23.49036979675293
      agent-1:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0772323608398438
        entropy_coeff: 0.0017600000137463212
        kl: 0.001223539700731635
        model: {}
        policy_loss: -0.004056693986058235
        total_loss: -0.003185164649039507
        vf_explained_var: -0.0781259834766388
        vf_loss: 27.674623489379883
      agent-2:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9555375576019287
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014386114198714495
        model: {}
        policy_loss: -0.0029950221069157124
        total_loss: -0.0022300975397229195
        vf_explained_var: -0.010567530989646912
        vf_loss: 24.466684341430664
      agent-3:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40078580379486084
        entropy_coeff: 0.0017600000137463212
        kl: 0.001142105320468545
        model: {}
        policy_loss: -0.002355039119720459
        total_loss: -0.0008983556181192398
        vf_explained_var: 0.10181479156017303
        vf_loss: 21.620685577392578
      agent-4:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8507487773895264
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020914894994348288
        model: {}
        policy_loss: -0.0039715892635285854
        total_loss: -0.0031679884996265173
        vf_explained_var: 0.05953814089298248
        vf_loss: 23.009193420410156
      agent-5:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5029653906822205
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005445496644824743
        model: {}
        policy_loss: -0.002219385700300336
        total_loss: -0.0008518558461219072
        vf_explained_var: 0.07800176739692688
        vf_loss: 22.527509689331055
    load_time_ms: 12939.947
    num_steps_sampled: 29664000
    num_steps_trained: 29664000
    sample_time_ms: 90593.393
    update_time_ms: 23.79
  iterations_since_restore: 249
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.399435028248586
    ram_util_percent: 10.133898305084745
  pid: 24061
  policy_reward_max:
    agent-0: 203.6666666666666
    agent-1: 203.6666666666666
    agent-2: 203.6666666666666
    agent-3: 203.6666666666666
    agent-4: 203.6666666666666
    agent-5: 203.6666666666666
  policy_reward_mean:
    agent-0: 176.73333333333318
    agent-1: 176.73333333333318
    agent-2: 176.73333333333318
    agent-3: 176.73333333333318
    agent-4: 176.73333333333318
    agent-5: 176.73333333333318
  policy_reward_min:
    agent-0: 75.83333333333344
    agent-1: 75.83333333333344
    agent-2: 75.83333333333344
    agent-3: 75.83333333333344
    agent-4: 75.83333333333344
    agent-5: 75.83333333333344
  sampler_perf:
    mean_env_wait_ms: 24.60306760783237
    mean_inference_ms: 12.225576235542395
    mean_processing_ms: 50.65680799813912
  time_since_restore: 31720.77841901779
  time_this_iter_s: 124.03437638282776
  time_total_s: 40846.790232896805
  timestamp: 1637055767
  timesteps_since_restore: 23904000
  timesteps_this_iter: 96000
  timesteps_total: 29664000
  training_iteration: 309
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    309 |          40846.8 | 29664000 |   1060.4 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 88
    apples_agent-0_mean: 5.53
    apples_agent-0_min: 0
    apples_agent-1_max: 124
    apples_agent-1_mean: 30.52
    apples_agent-1_min: 0
    apples_agent-2_max: 120
    apples_agent-2_mean: 3.11
    apples_agent-2_min: 0
    apples_agent-3_max: 389
    apples_agent-3_mean: 121.33
    apples_agent-3_min: 0
    apples_agent-4_max: 15
    apples_agent-4_mean: 0.21
    apples_agent-4_min: 0
    apples_agent-5_max: 770
    apples_agent-5_mean: 90.57
    apples_agent-5_min: 39
    cleaning_beam_agent-0_max: 518
    cleaning_beam_agent-0_mean: 373.31
    cleaning_beam_agent-0_min: 112
    cleaning_beam_agent-1_max: 637
    cleaning_beam_agent-1_mean: 311.66
    cleaning_beam_agent-1_min: 124
    cleaning_beam_agent-2_max: 589
    cleaning_beam_agent-2_mean: 412.96
    cleaning_beam_agent-2_min: 271
    cleaning_beam_agent-3_max: 85
    cleaning_beam_agent-3_mean: 14.06
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 602
    cleaning_beam_agent-4_mean: 481.18
    cleaning_beam_agent-4_min: 387
    cleaning_beam_agent-5_max: 95
    cleaning_beam_agent-5_mean: 15.47
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-44-50
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1229.9999999999961
  episode_reward_mean: 1052.2399999999918
  episode_reward_min: 354.0000000000035
  episodes_this_iter: 96
  episodes_total: 29760
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19718.056
    learner:
      agent-0:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1133522987365723
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016712479991838336
        model: {}
        policy_loss: -0.003268318483605981
        total_loss: -0.002908166963607073
        vf_explained_var: -0.017623603343963623
        vf_loss: 23.196533203125
      agent-1:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 1.080868124961853
        entropy_coeff: 0.0017600000137463212
        kl: 0.001377269858494401
        model: {}
        policy_loss: -0.0037496546283364296
        total_loss: -0.0031203697435557842
        vf_explained_var: -0.0774071216583252
        vf_loss: 25.316131591796875
      agent-2:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9516583681106567
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017188303172588348
        model: {}
        policy_loss: -0.0031767936889082193
        total_loss: -0.002646793145686388
        vf_explained_var: 0.02692246437072754
        vf_loss: 22.049205780029297
      agent-3:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40413397550582886
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013499147025868297
        model: {}
        policy_loss: -0.0020612478256225586
        total_loss: -0.0005748001858592033
        vf_explained_var: 0.029036536812782288
        vf_loss: 21.97723960876465
      agent-4:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.846247136592865
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016140484949573874
        model: {}
        policy_loss: -0.003640018869191408
        total_loss: -0.0028382125310599804
        vf_explained_var: -2.466142177581787e-05
        vf_loss: 22.912017822265625
      agent-5:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5029823780059814
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008870286983437836
        model: {}
        policy_loss: -0.002166468184441328
        total_loss: -0.0008428404107689857
        vf_explained_var: 0.032166555523872375
        vf_loss: 22.088783264160156
    load_time_ms: 12937.84
    num_steps_sampled: 29760000
    num_steps_trained: 29760000
    sample_time_ms: 90654.771
    update_time_ms: 23.969
  iterations_since_restore: 250
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.450857142857144
    ram_util_percent: 10.07142857142857
  pid: 24061
  policy_reward_max:
    agent-0: 205.00000000000028
    agent-1: 205.00000000000028
    agent-2: 205.00000000000028
    agent-3: 205.00000000000028
    agent-4: 205.00000000000028
    agent-5: 205.00000000000028
  policy_reward_mean:
    agent-0: 175.37333333333325
    agent-1: 175.37333333333325
    agent-2: 175.37333333333325
    agent-3: 175.37333333333325
    agent-4: 175.37333333333325
    agent-5: 175.37333333333325
  policy_reward_min:
    agent-0: 58.999999999999964
    agent-1: 58.999999999999964
    agent-2: 58.999999999999964
    agent-3: 58.999999999999964
    agent-4: 58.999999999999964
    agent-5: 58.999999999999964
  sampler_perf:
    mean_env_wait_ms: 24.605284313108385
    mean_inference_ms: 12.225282733629937
    mean_processing_ms: 50.655527809671845
  time_since_restore: 31844.00648880005
  time_this_iter_s: 123.22806978225708
  time_total_s: 40970.01830267906
  timestamp: 1637055890
  timesteps_since_restore: 24000000
  timesteps_this_iter: 96000
  timesteps_total: 29760000
  training_iteration: 310
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    310 |            40970 | 29760000 |  1052.24 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 51
    apples_agent-0_mean: 3.67
    apples_agent-0_min: 0
    apples_agent-1_max: 97
    apples_agent-1_mean: 32.26
    apples_agent-1_min: 0
    apples_agent-2_max: 57
    apples_agent-2_mean: 2.83
    apples_agent-2_min: 0
    apples_agent-3_max: 207
    apples_agent-3_mean: 122.79
    apples_agent-3_min: 51
    apples_agent-4_max: 23
    apples_agent-4_mean: 0.56
    apples_agent-4_min: 0
    apples_agent-5_max: 116
    apples_agent-5_mean: 79.55
    apples_agent-5_min: 30
    cleaning_beam_agent-0_max: 518
    cleaning_beam_agent-0_mean: 374.7
    cleaning_beam_agent-0_min: 198
    cleaning_beam_agent-1_max: 580
    cleaning_beam_agent-1_mean: 304.77
    cleaning_beam_agent-1_min: 104
    cleaning_beam_agent-2_max: 632
    cleaning_beam_agent-2_mean: 416.34
    cleaning_beam_agent-2_min: 211
    cleaning_beam_agent-3_max: 117
    cleaning_beam_agent-3_mean: 17.46
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 560
    cleaning_beam_agent-4_mean: 471.02
    cleaning_beam_agent-4_min: 374
    cleaning_beam_agent-5_max: 95
    cleaning_beam_agent-5_mean: 17.18
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.07
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-47-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1259.0000000000111
  episode_reward_mean: 1039.8399999999936
  episode_reward_min: 317.00000000000097
  episodes_this_iter: 96
  episodes_total: 29856
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19785.317
    learner:
      agent-0:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 1.123874545097351
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017929987516254187
        model: {}
        policy_loss: -0.003575854003429413
        total_loss: -0.003143006470054388
        vf_explained_var: 0.021905645728111267
        vf_loss: 24.10868263244629
      agent-1:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0787274837493896
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015617487952113152
        model: {}
        policy_loss: -0.00371280824765563
        total_loss: -0.002890397794544697
        vf_explained_var: -0.0828077495098114
        vf_loss: 27.209686279296875
      agent-2:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9563018679618835
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011976984096691012
        model: {}
        policy_loss: -0.003134307451546192
        total_loss: -0.0025073466822504997
        vf_explained_var: 0.05970415472984314
        vf_loss: 23.10051727294922
      agent-3:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4486093521118164
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012264796532690525
        model: {}
        policy_loss: -0.0022913003340363503
        total_loss: -0.00090398034080863
        vf_explained_var: 0.11192454397678375
        vf_loss: 21.76870346069336
      agent-4:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8524236679077148
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020311097614467144
        model: {}
        policy_loss: -0.004261975642293692
        total_loss: -0.0033532697707414627
        vf_explained_var: 0.021471574902534485
        vf_loss: 24.089675903320312
      agent-5:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5237692594528198
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006747203296981752
        model: {}
        policy_loss: -0.0025756193790584803
        total_loss: -0.001317743444815278
        vf_explained_var: 0.11818459630012512
        vf_loss: 21.797094345092773
    load_time_ms: 16758.137
    num_steps_sampled: 29856000
    num_steps_trained: 29856000
    sample_time_ms: 90951.147
    update_time_ms: 24.77
  iterations_since_restore: 251
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 18.40296610169492
    ram_util_percent: 13.671186440677964
  pid: 24061
  policy_reward_max:
    agent-0: 209.83333333333277
    agent-1: 209.83333333333277
    agent-2: 209.83333333333277
    agent-3: 209.83333333333277
    agent-4: 209.83333333333277
    agent-5: 209.83333333333277
  policy_reward_mean:
    agent-0: 173.3066666666665
    agent-1: 173.3066666666665
    agent-2: 173.3066666666665
    agent-3: 173.3066666666665
    agent-4: 173.3066666666665
    agent-5: 173.3066666666665
  policy_reward_min:
    agent-0: 52.83333333333331
    agent-1: 52.83333333333331
    agent-2: 52.83333333333331
    agent-3: 52.83333333333331
    agent-4: 52.83333333333331
    agent-5: 52.83333333333331
  sampler_perf:
    mean_env_wait_ms: 24.608294415876298
    mean_inference_ms: 12.22526756761841
    mean_processing_ms: 50.659282515944284
  time_since_restore: 32009.323529720306
  time_this_iter_s: 165.31704092025757
  time_total_s: 41135.33534359932
  timestamp: 1637056056
  timesteps_since_restore: 24096000
  timesteps_this_iter: 96000
  timesteps_total: 29856000
  training_iteration: 311
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 32.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    311 |          41135.3 | 29856000 |  1039.84 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 58
    apples_agent-0_mean: 4.69
    apples_agent-0_min: 0
    apples_agent-1_max: 95
    apples_agent-1_mean: 24.48
    apples_agent-1_min: 0
    apples_agent-2_max: 53
    apples_agent-2_mean: 2.02
    apples_agent-2_min: 0
    apples_agent-3_max: 219
    apples_agent-3_mean: 124.4
    apples_agent-3_min: 43
    apples_agent-4_max: 59
    apples_agent-4_mean: 1.38
    apples_agent-4_min: 0
    apples_agent-5_max: 178
    apples_agent-5_mean: 81.86
    apples_agent-5_min: 42
    cleaning_beam_agent-0_max: 491
    cleaning_beam_agent-0_mean: 370.94
    cleaning_beam_agent-0_min: 225
    cleaning_beam_agent-1_max: 558
    cleaning_beam_agent-1_mean: 335.43
    cleaning_beam_agent-1_min: 153
    cleaning_beam_agent-2_max: 598
    cleaning_beam_agent-2_mean: 429.87
    cleaning_beam_agent-2_min: 197
    cleaning_beam_agent-3_max: 108
    cleaning_beam_agent-3_mean: 14.81
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 585
    cleaning_beam_agent-4_mean: 465.83
    cleaning_beam_agent-4_min: 316
    cleaning_beam_agent-5_max: 175
    cleaning_beam_agent-5_mean: 19.22
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-50-22
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1236.999999999996
  episode_reward_mean: 1053.3199999999918
  episode_reward_min: 537.0000000000038
  episodes_this_iter: 96
  episodes_total: 29952
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19858.316
    learner:
      agent-0:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1262457370758057
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014322834322229028
        model: {}
        policy_loss: -0.003159543499350548
        total_loss: -0.002752720844000578
        vf_explained_var: 0.004472747445106506
        vf_loss: 23.89015007019043
      agent-1:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0772868394851685
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013435435248538852
        model: {}
        policy_loss: -0.003760088700801134
        total_loss: -0.0030783885158598423
        vf_explained_var: -0.047223448753356934
        vf_loss: 25.7772216796875
      agent-2:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9453772306442261
        entropy_coeff: 0.0017600000137463212
        kl: 0.001532884780317545
        model: {}
        policy_loss: -0.003074594773352146
        total_loss: -0.002396221971139312
        vf_explained_var: 0.017991751432418823
        vf_loss: 23.422372817993164
      agent-3:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40300339460372925
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011314090806990862
        model: {}
        policy_loss: -0.0022425560746341944
        total_loss: -0.0008042731788009405
        vf_explained_var: 0.09577776491641998
        vf_loss: 21.475688934326172
      agent-4:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8507211208343506
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015675366157665849
        model: {}
        policy_loss: -0.0038975621573626995
        total_loss: -0.003072204766795039
        vf_explained_var: 0.03971710801124573
        vf_loss: 23.226255416870117
      agent-5:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.51747065782547
        entropy_coeff: 0.0017600000137463212
        kl: 0.001024942728690803
        model: {}
        policy_loss: -0.0027014045044779778
        total_loss: -0.0013581939274445176
        vf_explained_var: 0.07131773233413696
        vf_loss: 22.539581298828125
    load_time_ms: 19378.064
    num_steps_sampled: 29952000
    num_steps_trained: 29952000
    sample_time_ms: 92450.526
    update_time_ms: 25.631
  iterations_since_restore: 252
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.63177966101695
    ram_util_percent: 17.709322033898303
  pid: 24061
  policy_reward_max:
    agent-0: 206.16666666666657
    agent-1: 206.16666666666657
    agent-2: 206.16666666666657
    agent-3: 206.16666666666657
    agent-4: 206.16666666666657
    agent-5: 206.16666666666657
  policy_reward_mean:
    agent-0: 175.5533333333332
    agent-1: 175.5533333333332
    agent-2: 175.5533333333332
    agent-3: 175.5533333333332
    agent-4: 175.5533333333332
    agent-5: 175.5533333333332
  policy_reward_min:
    agent-0: 89.5000000000002
    agent-1: 89.5000000000002
    agent-2: 89.5000000000002
    agent-3: 89.5000000000002
    agent-4: 89.5000000000002
    agent-5: 89.5000000000002
  sampler_perf:
    mean_env_wait_ms: 24.617900989763072
    mean_inference_ms: 12.22799589027921
    mean_processing_ms: 50.673352595474256
  time_since_restore: 32174.860857009888
  time_this_iter_s: 165.5373272895813
  time_total_s: 41300.8726708889
  timestamp: 1637056222
  timesteps_since_restore: 24192000
  timesteps_this_iter: 96000
  timesteps_total: 29952000
  training_iteration: 312
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 32.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    312 |          41300.9 | 29952000 |  1053.32 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 65
    apples_agent-0_mean: 2.96
    apples_agent-0_min: 0
    apples_agent-1_max: 87
    apples_agent-1_mean: 31.12
    apples_agent-1_min: 0
    apples_agent-2_max: 151
    apples_agent-2_mean: 3.23
    apples_agent-2_min: 0
    apples_agent-3_max: 189
    apples_agent-3_mean: 117.27
    apples_agent-3_min: 0
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 157
    apples_agent-5_mean: 83.59
    apples_agent-5_min: 42
    cleaning_beam_agent-0_max: 499
    cleaning_beam_agent-0_mean: 362.89
    cleaning_beam_agent-0_min: 150
    cleaning_beam_agent-1_max: 606
    cleaning_beam_agent-1_mean: 300.49
    cleaning_beam_agent-1_min: 135
    cleaning_beam_agent-2_max: 584
    cleaning_beam_agent-2_mean: 427.59
    cleaning_beam_agent-2_min: 168
    cleaning_beam_agent-3_max: 47
    cleaning_beam_agent-3_mean: 12.17
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 559
    cleaning_beam_agent-4_mean: 487.72
    cleaning_beam_agent-4_min: 382
    cleaning_beam_agent-5_max: 175
    cleaning_beam_agent-5_mean: 18.95
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-52-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1236.999999999996
  episode_reward_mean: 1067.6699999999926
  episode_reward_min: 599.9999999999959
  episodes_this_iter: 96
  episodes_total: 30048
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 19967.199
    learner:
      agent-0:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1281659603118896
        entropy_coeff: 0.0017600000137463212
        kl: 0.002054253127425909
        model: {}
        policy_loss: -0.003555048955604434
        total_loss: -0.0033602211624383926
        vf_explained_var: 0.01898615062236786
        vf_loss: 21.80398941040039
      agent-1:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0758733749389648
        entropy_coeff: 0.0017600000137463212
        kl: 0.002104653976857662
        model: {}
        policy_loss: -0.004228807985782623
        total_loss: -0.0037127425894141197
        vf_explained_var: -0.02925780415534973
        vf_loss: 24.096017837524414
      agent-2:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9445493221282959
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017239695880562067
        model: {}
        policy_loss: -0.003044901881366968
        total_loss: -0.0025218899827450514
        vf_explained_var: 0.014723971486091614
        vf_loss: 21.854196548461914
      agent-3:
        cur_kl_coeff: 6.018531165893214e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3893550634384155
        entropy_coeff: 0.0017600000137463212
        kl: 0.00068361044395715
        model: {}
        policy_loss: -0.0018635588930919766
        total_loss: -0.000532638281583786
        vf_explained_var: 0.07951566576957703
        vf_loss: 20.161865234375
      agent-4:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8498461246490479
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018474464304745197
        model: {}
        policy_loss: -0.0034458674490451813
        total_loss: -0.0026645660400390625
        vf_explained_var: -0.014789402484893799
        vf_loss: 22.770299911499023
      agent-5:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.51224285364151
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007266886532306671
        model: {}
        policy_loss: -0.0023839951027184725
        total_loss: -0.0012169043766334653
        vf_explained_var: 0.08182249963283539
        vf_loss: 20.68636703491211
    load_time_ms: 21547.668
    num_steps_sampled: 30048000
    num_steps_trained: 30048000
    sample_time_ms: 93513.775
    update_time_ms: 25.817
  iterations_since_restore: 253
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.027354260089684
    ram_util_percent: 17.7762331838565
  pid: 24061
  policy_reward_max:
    agent-0: 206.16666666666657
    agent-1: 206.16666666666657
    agent-2: 206.16666666666657
    agent-3: 206.16666666666657
    agent-4: 206.16666666666657
    agent-5: 206.16666666666657
  policy_reward_mean:
    agent-0: 177.94499999999988
    agent-1: 177.94499999999988
    agent-2: 177.94499999999988
    agent-3: 177.94499999999988
    agent-4: 177.94499999999988
    agent-5: 177.94499999999988
  policy_reward_min:
    agent-0: 100.00000000000038
    agent-1: 100.00000000000038
    agent-2: 100.00000000000038
    agent-3: 100.00000000000038
    agent-4: 100.00000000000038
    agent-5: 100.00000000000038
  sampler_perf:
    mean_env_wait_ms: 24.628035212177032
    mean_inference_ms: 12.231267277877382
    mean_processing_ms: 50.68877600564233
  time_since_restore: 32331.24710202217
  time_this_iter_s: 156.38624501228333
  time_total_s: 41457.258915901184
  timestamp: 1637056378
  timesteps_since_restore: 24288000
  timesteps_this_iter: 96000
  timesteps_total: 30048000
  training_iteration: 313
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 32.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    313 |          41457.3 | 30048000 |  1067.67 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 55
    apples_agent-0_mean: 5.84
    apples_agent-0_min: 0
    apples_agent-1_max: 114
    apples_agent-1_mean: 29.14
    apples_agent-1_min: 0
    apples_agent-2_max: 173
    apples_agent-2_mean: 5.18
    apples_agent-2_min: 0
    apples_agent-3_max: 255
    apples_agent-3_mean: 131.29
    apples_agent-3_min: 70
    apples_agent-4_max: 11
    apples_agent-4_mean: 0.11
    apples_agent-4_min: 0
    apples_agent-5_max: 191
    apples_agent-5_mean: 85.55
    apples_agent-5_min: 48
    cleaning_beam_agent-0_max: 461
    cleaning_beam_agent-0_mean: 360.27
    cleaning_beam_agent-0_min: 160
    cleaning_beam_agent-1_max: 616
    cleaning_beam_agent-1_mean: 296.09
    cleaning_beam_agent-1_min: 123
    cleaning_beam_agent-2_max: 601
    cleaning_beam_agent-2_mean: 416.98
    cleaning_beam_agent-2_min: 182
    cleaning_beam_agent-3_max: 67
    cleaning_beam_agent-3_mean: 14.79
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 605
    cleaning_beam_agent-4_mean: 497.08
    cleaning_beam_agent-4_min: 413
    cleaning_beam_agent-5_max: 115
    cleaning_beam_agent-5_mean: 18.64
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-55-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1238.0000000000048
  episode_reward_mean: 1063.7199999999925
  episode_reward_min: 715.0000000000009
  episodes_this_iter: 96
  episodes_total: 30144
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20019.267
    learner:
      agent-0:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1334716081619263
        entropy_coeff: 0.0017600000137463212
        kl: 0.00229622982442379
        model: {}
        policy_loss: -0.0037479856982827187
        total_loss: -0.003520682454109192
        vf_explained_var: 0.02788539230823517
        vf_loss: 22.222126007080078
      agent-1:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 1.086213231086731
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014157256809994578
        model: {}
        policy_loss: -0.00407345499843359
        total_loss: -0.0035236903931945562
        vf_explained_var: -0.03902333974838257
        vf_loss: 24.615028381347656
      agent-2:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9434921145439148
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016976249171420932
        model: {}
        policy_loss: -0.003272474743425846
        total_loss: -0.002644144231453538
        vf_explained_var: -0.0034358054399490356
        vf_loss: 22.88875961303711
      agent-3:
        cur_kl_coeff: 3.009265582946607e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4078000783920288
        entropy_coeff: 0.0017600000137463212
        kl: 0.000430717715062201
        model: {}
        policy_loss: -0.0018615835579112172
        total_loss: -0.0004518160130828619
        vf_explained_var: 0.059357449412345886
        vf_loss: 21.27496337890625
      agent-4:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8456838726997375
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013848154339939356
        model: {}
        policy_loss: -0.0035408276598900557
        total_loss: -0.0027101640589535236
        vf_explained_var: -0.010935261845588684
        vf_loss: 23.190662384033203
      agent-5:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5344088077545166
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007446105591952801
        model: {}
        policy_loss: -0.002618097234517336
        total_loss: -0.0013967048143967986
        vf_explained_var: 0.06319940090179443
        vf_loss: 21.619522094726562
    load_time_ms: 24475.918
    num_steps_sampled: 30144000
    num_steps_trained: 30144000
    sample_time_ms: 94665.144
    update_time_ms: 26.784
  iterations_since_restore: 254
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.614893617021277
    ram_util_percent: 17.75106382978723
  pid: 24061
  policy_reward_max:
    agent-0: 206.33333333333275
    agent-1: 206.33333333333275
    agent-2: 206.33333333333275
    agent-3: 206.33333333333275
    agent-4: 206.33333333333275
    agent-5: 206.33333333333275
  policy_reward_mean:
    agent-0: 177.28666666666658
    agent-1: 177.28666666666658
    agent-2: 177.28666666666658
    agent-3: 177.28666666666658
    agent-4: 177.28666666666658
    agent-5: 177.28666666666658
  policy_reward_min:
    agent-0: 119.16666666666735
    agent-1: 119.16666666666735
    agent-2: 119.16666666666735
    agent-3: 119.16666666666735
    agent-4: 119.16666666666735
    agent-5: 119.16666666666735
  sampler_perf:
    mean_env_wait_ms: 24.637839073892756
    mean_inference_ms: 12.234555206326247
    mean_processing_ms: 50.70503271307793
  time_since_restore: 32495.958417654037
  time_this_iter_s: 164.71131563186646
  time_total_s: 41621.97023153305
  timestamp: 1637056543
  timesteps_since_restore: 24384000
  timesteps_this_iter: 96000
  timesteps_total: 30144000
  training_iteration: 314
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    314 |            41622 | 30144000 |  1063.72 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 94
    apples_agent-0_mean: 4.06
    apples_agent-0_min: 0
    apples_agent-1_max: 176
    apples_agent-1_mean: 36.19
    apples_agent-1_min: 0
    apples_agent-2_max: 81
    apples_agent-2_mean: 2.84
    apples_agent-2_min: 0
    apples_agent-3_max: 197
    apples_agent-3_mean: 124.29
    apples_agent-3_min: 77
    apples_agent-4_max: 27
    apples_agent-4_mean: 0.46
    apples_agent-4_min: 0
    apples_agent-5_max: 128
    apples_agent-5_mean: 84.02
    apples_agent-5_min: 48
    cleaning_beam_agent-0_max: 479
    cleaning_beam_agent-0_mean: 354.74
    cleaning_beam_agent-0_min: 157
    cleaning_beam_agent-1_max: 566
    cleaning_beam_agent-1_mean: 292.88
    cleaning_beam_agent-1_min: 117
    cleaning_beam_agent-2_max: 603
    cleaning_beam_agent-2_mean: 440.12
    cleaning_beam_agent-2_min: 280
    cleaning_beam_agent-3_max: 49
    cleaning_beam_agent-3_mean: 13.64
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 632
    cleaning_beam_agent-4_mean: 486.54
    cleaning_beam_agent-4_min: 362
    cleaning_beam_agent-5_max: 115
    cleaning_beam_agent-5_mean: 19.35
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-58-11
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1244.9999999999907
  episode_reward_mean: 1060.699999999992
  episode_reward_min: 698.9999999999967
  episodes_this_iter: 96
  episodes_total: 30240
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20141.968
    learner:
      agent-0:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1364471912384033
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013958961935713887
        model: {}
        policy_loss: -0.0035577290691435337
        total_loss: -0.003361697308719158
        vf_explained_var: 0.004861056804656982
        vf_loss: 21.961811065673828
      agent-1:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0824147462844849
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015033693052828312
        model: {}
        policy_loss: -0.004025335423648357
        total_loss: -0.003411127720028162
        vf_explained_var: -0.08671411871910095
        vf_loss: 25.192543029785156
      agent-2:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.948559045791626
        entropy_coeff: 0.0017600000137463212
        kl: 0.002071734983474016
        model: {}
        policy_loss: -0.003240007907152176
        total_loss: -0.002735380083322525
        vf_explained_var: 0.004643380641937256
        vf_loss: 21.740903854370117
      agent-3:
        cur_kl_coeff: 1.5046327914733034e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40851402282714844
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013610380701720715
        model: {}
        policy_loss: -0.0021990439854562283
        total_loss: -0.0007891603745520115
        vf_explained_var: 0.022185251116752625
        vf_loss: 21.28867530822754
      agent-4:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8558878898620605
        entropy_coeff: 0.0017600000137463212
        kl: 0.001310847932472825
        model: {}
        policy_loss: -0.0036526916082948446
        total_loss: -0.00289020873606205
        vf_explained_var: -0.015021979808807373
        vf_loss: 22.688474655151367
      agent-5:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5365177392959595
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007007258245721459
        model: {}
        policy_loss: -0.002531715203076601
        total_loss: -0.0013757040724158287
        vf_explained_var: 0.06436043977737427
        vf_loss: 21.0028018951416
    load_time_ms: 26003.76
    num_steps_sampled: 30240000
    num_steps_trained: 30240000
    sample_time_ms: 95447.855
    update_time_ms: 24.579
  iterations_since_restore: 255
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.61857142857143
    ram_util_percent: 15.610952380952382
  pid: 24061
  policy_reward_max:
    agent-0: 207.50000000000009
    agent-1: 207.50000000000009
    agent-2: 207.50000000000009
    agent-3: 207.50000000000009
    agent-4: 207.50000000000009
    agent-5: 207.50000000000009
  policy_reward_mean:
    agent-0: 176.7833333333332
    agent-1: 176.7833333333332
    agent-2: 176.7833333333332
    agent-3: 176.7833333333332
    agent-4: 176.7833333333332
    agent-5: 176.7833333333332
  policy_reward_min:
    agent-0: 116.50000000000004
    agent-1: 116.50000000000004
    agent-2: 116.50000000000004
    agent-3: 116.50000000000004
    agent-4: 116.50000000000004
    agent-5: 116.50000000000004
  sampler_perf:
    mean_env_wait_ms: 24.64531748082839
    mean_inference_ms: 12.2366726222533
    mean_processing_ms: 50.71634901377949
  time_since_restore: 32644.03294301033
  time_this_iter_s: 148.07452535629272
  time_total_s: 41770.04475688934
  timestamp: 1637056691
  timesteps_since_restore: 24480000
  timesteps_this_iter: 96000
  timesteps_total: 30240000
  training_iteration: 315
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    315 |            41770 | 30240000 |   1060.7 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 48
    apples_agent-0_mean: 3.73
    apples_agent-0_min: 0
    apples_agent-1_max: 98
    apples_agent-1_mean: 33.68
    apples_agent-1_min: 0
    apples_agent-2_max: 51
    apples_agent-2_mean: 3.16
    apples_agent-2_min: 0
    apples_agent-3_max: 204
    apples_agent-3_mean: 122.95
    apples_agent-3_min: 2
    apples_agent-4_max: 30
    apples_agent-4_mean: 0.61
    apples_agent-4_min: 0
    apples_agent-5_max: 131
    apples_agent-5_mean: 82.35
    apples_agent-5_min: 23
    cleaning_beam_agent-0_max: 475
    cleaning_beam_agent-0_mean: 364.53
    cleaning_beam_agent-0_min: 197
    cleaning_beam_agent-1_max: 547
    cleaning_beam_agent-1_mean: 293.31
    cleaning_beam_agent-1_min: 71
    cleaning_beam_agent-2_max: 582
    cleaning_beam_agent-2_mean: 432.94
    cleaning_beam_agent-2_min: 240
    cleaning_beam_agent-3_max: 55
    cleaning_beam_agent-3_mean: 13.55
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 612
    cleaning_beam_agent-4_mean: 483.41
    cleaning_beam_agent-4_min: 320
    cleaning_beam_agent-5_max: 149
    cleaning_beam_agent-5_mean: 18.99
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-00-39
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1244.9999999999907
  episode_reward_mean: 1047.199999999992
  episode_reward_min: 256.99999999999545
  episodes_this_iter: 96
  episodes_total: 30336
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20199.184
    learner:
      agent-0:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1276395320892334
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013800153974443674
        model: {}
        policy_loss: -0.0032400963827967644
        total_loss: -0.0027917656116187572
        vf_explained_var: -0.007649675011634827
        vf_loss: 24.329763412475586
      agent-1:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0722497701644897
        entropy_coeff: 0.0017600000137463212
        kl: 0.001105656148865819
        model: {}
        policy_loss: -0.003922873176634312
        total_loss: -0.0032064225524663925
        vf_explained_var: -0.04384422302246094
        vf_loss: 26.036090850830078
      agent-2:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9354833960533142
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018262236844748259
        model: {}
        policy_loss: -0.0035044020041823387
        total_loss: -0.0029169386252760887
        vf_explained_var: 0.07366201281547546
        vf_loss: 22.339155197143555
      agent-3:
        cur_kl_coeff: 7.523163957366517e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4069058895111084
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010349308140575886
        model: {}
        policy_loss: -0.0023832623846828938
        total_loss: -0.0008853215840645134
        vf_explained_var: 0.07801704108715057
        vf_loss: 22.140968322753906
      agent-4:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.857199490070343
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019305141177028418
        model: {}
        policy_loss: -0.0036826333962380886
        total_loss: -0.002851532306522131
        vf_explained_var: 0.03513437509536743
        vf_loss: 23.397748947143555
      agent-5:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5465363264083862
        entropy_coeff: 0.0017600000137463212
        kl: 0.000876627687830478
        model: {}
        policy_loss: -0.002943859901279211
        total_loss: -0.0016970521537587047
        vf_explained_var: 0.10146386921405792
        vf_loss: 22.087125778198242
    load_time_ms: 27612.19
    num_steps_sampled: 30336000
    num_steps_trained: 30336000
    sample_time_ms: 96144.55
    update_time_ms: 25.464
  iterations_since_restore: 256
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.79428571428571
    ram_util_percent: 15.671904761904761
  pid: 24061
  policy_reward_max:
    agent-0: 207.50000000000009
    agent-1: 207.50000000000009
    agent-2: 207.50000000000009
    agent-3: 207.50000000000009
    agent-4: 207.50000000000009
    agent-5: 207.50000000000009
  policy_reward_mean:
    agent-0: 174.53333333333322
    agent-1: 174.53333333333322
    agent-2: 174.53333333333322
    agent-3: 174.53333333333322
    agent-4: 174.53333333333322
    agent-5: 174.53333333333322
  policy_reward_min:
    agent-0: 42.833333333333265
    agent-1: 42.833333333333265
    agent-2: 42.833333333333265
    agent-3: 42.833333333333265
    agent-4: 42.833333333333265
    agent-5: 42.833333333333265
  sampler_perf:
    mean_env_wait_ms: 24.652920614887222
    mean_inference_ms: 12.239059685778823
    mean_processing_ms: 50.72794170775695
  time_since_restore: 32791.2740585804
  time_this_iter_s: 147.24111557006836
  time_total_s: 41917.28587245941
  timestamp: 1637056839
  timesteps_since_restore: 24576000
  timesteps_this_iter: 96000
  timesteps_total: 30336000
  training_iteration: 316
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    316 |          41917.3 | 30336000 |   1047.2 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 92
    apples_agent-0_mean: 5.95
    apples_agent-0_min: 0
    apples_agent-1_max: 125
    apples_agent-1_mean: 33.73
    apples_agent-1_min: 0
    apples_agent-2_max: 58
    apples_agent-2_mean: 3.27
    apples_agent-2_min: 0
    apples_agent-3_max: 184
    apples_agent-3_mean: 116.4
    apples_agent-3_min: 2
    apples_agent-4_max: 48
    apples_agent-4_mean: 1.8
    apples_agent-4_min: 0
    apples_agent-5_max: 180
    apples_agent-5_mean: 85.37
    apples_agent-5_min: 23
    cleaning_beam_agent-0_max: 495
    cleaning_beam_agent-0_mean: 367.98
    cleaning_beam_agent-0_min: 138
    cleaning_beam_agent-1_max: 517
    cleaning_beam_agent-1_mean: 297.03
    cleaning_beam_agent-1_min: 103
    cleaning_beam_agent-2_max: 628
    cleaning_beam_agent-2_mean: 435.36
    cleaning_beam_agent-2_min: 250
    cleaning_beam_agent-3_max: 77
    cleaning_beam_agent-3_mean: 14.93
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 583
    cleaning_beam_agent-4_mean: 488.65
    cleaning_beam_agent-4_min: 320
    cleaning_beam_agent-5_max: 208
    cleaning_beam_agent-5_mean: 22.18
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-02-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1219.000000000004
  episode_reward_mean: 1032.0699999999904
  episode_reward_min: 256.99999999999545
  episodes_this_iter: 96
  episodes_total: 30432
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20250.789
    learner:
      agent-0:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 1.124221920967102
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015510744415223598
        model: {}
        policy_loss: -0.0032137534581124783
        total_loss: -0.00281247915700078
        vf_explained_var: 0.003720581531524658
        vf_loss: 23.79903793334961
      agent-1:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0773814916610718
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014976175734773278
        model: {}
        policy_loss: -0.0041496362537145615
        total_loss: -0.0034277881495654583
        vf_explained_var: -0.06891870498657227
        vf_loss: 26.18040657043457
      agent-2:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9519526362419128
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018425049493089318
        model: {}
        policy_loss: -0.0033279526978731155
        total_loss: -0.0026995218358933926
        vf_explained_var: 0.03621819615364075
        vf_loss: 23.038658142089844
      agent-3:
        cur_kl_coeff: 3.7615819786832586e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41721808910369873
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005424283444881439
        model: {}
        policy_loss: -0.0017776592867448926
        total_loss: -0.00029781111516058445
        vf_explained_var: 0.07314041256904602
        vf_loss: 22.14156150817871
      agent-4:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8487004637718201
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014699011808261275
        model: {}
        policy_loss: -0.003919550683349371
        total_loss: -0.0029975567013025284
        vf_explained_var: -0.00674864649772644
        vf_loss: 24.15706443786621
      agent-5:
        cur_kl_coeff: 6.018531165893214e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.540419340133667
        entropy_coeff: 0.0017600000137463212
        kl: 0.001309516141191125
        model: {}
        policy_loss: -0.002915964461863041
        total_loss: -0.0016347095370292664
        vf_explained_var: 0.07746484875679016
        vf_loss: 22.323917388916016
    load_time_ms: 28386.423
    num_steps_sampled: 30432000
    num_steps_trained: 30432000
    sample_time_ms: 96716.141
    update_time_ms: 24.457
  iterations_since_restore: 257
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.23908629441624
    ram_util_percent: 15.57157360406091
  pid: 24061
  policy_reward_max:
    agent-0: 203.16666666666595
    agent-1: 203.16666666666595
    agent-2: 203.16666666666595
    agent-3: 203.16666666666595
    agent-4: 203.16666666666595
    agent-5: 203.16666666666595
  policy_reward_mean:
    agent-0: 172.01166666666654
    agent-1: 172.01166666666654
    agent-2: 172.01166666666654
    agent-3: 172.01166666666654
    agent-4: 172.01166666666654
    agent-5: 172.01166666666654
  policy_reward_min:
    agent-0: 42.833333333333265
    agent-1: 42.833333333333265
    agent-2: 42.833333333333265
    agent-3: 42.833333333333265
    agent-4: 42.833333333333265
    agent-5: 42.833333333333265
  sampler_perf:
    mean_env_wait_ms: 24.659998784724497
    mean_inference_ms: 12.241038204733163
    mean_processing_ms: 50.737465172512785
  time_since_restore: 32929.01022815704
  time_this_iter_s: 137.7361695766449
  time_total_s: 42055.02204203606
  timestamp: 1637056977
  timesteps_since_restore: 24672000
  timesteps_this_iter: 96000
  timesteps_total: 30432000
  training_iteration: 317
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 28.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    317 |            42055 | 30432000 |  1032.07 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 86
    apples_agent-0_mean: 1.84
    apples_agent-0_min: 0
    apples_agent-1_max: 101
    apples_agent-1_mean: 30.55
    apples_agent-1_min: 0
    apples_agent-2_max: 354
    apples_agent-2_mean: 5.62
    apples_agent-2_min: 0
    apples_agent-3_max: 194
    apples_agent-3_mean: 126.62
    apples_agent-3_min: 82
    apples_agent-4_max: 60
    apples_agent-4_mean: 1.97
    apples_agent-4_min: 0
    apples_agent-5_max: 133
    apples_agent-5_mean: 85.5
    apples_agent-5_min: 59
    cleaning_beam_agent-0_max: 503
    cleaning_beam_agent-0_mean: 368.45
    cleaning_beam_agent-0_min: 181
    cleaning_beam_agent-1_max: 620
    cleaning_beam_agent-1_mean: 321.58
    cleaning_beam_agent-1_min: 121
    cleaning_beam_agent-2_max: 606
    cleaning_beam_agent-2_mean: 447.65
    cleaning_beam_agent-2_min: 99
    cleaning_beam_agent-3_max: 115
    cleaning_beam_agent-3_mean: 16.89
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 594
    cleaning_beam_agent-4_mean: 489.47
    cleaning_beam_agent-4_min: 273
    cleaning_beam_agent-5_max: 88
    cleaning_beam_agent-5_mean: 16.54
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-05-52
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1233.999999999988
  episode_reward_mean: 1065.3499999999938
  episode_reward_min: 466.0000000000119
  episodes_this_iter: 96
  episodes_total: 30528
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20332.357
    learner:
      agent-0:
        cur_kl_coeff: 6.018531165893214e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1316038370132446
        entropy_coeff: 0.0017600000137463212
        kl: 0.002358224941417575
        model: {}
        policy_loss: -0.0033307280391454697
        total_loss: -0.0029986172448843718
        vf_explained_var: 0.012864753603935242
        vf_loss: 23.237327575683594
      agent-1:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0535212755203247
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014548328472301364
        model: {}
        policy_loss: -0.0038221683353185654
        total_loss: -0.0030674170702695847
        vf_explained_var: -0.0528925359249115
        vf_loss: 26.08951759338379
      agent-2:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.926994264125824
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017995141679421067
        model: {}
        policy_loss: -0.0030830814503133297
        total_loss: -0.002429298358038068
        vf_explained_var: 0.030341044068336487
        vf_loss: 22.852916717529297
      agent-3:
        cur_kl_coeff: 1.8807909893416293e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4083232879638672
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008408755529671907
        model: {}
        policy_loss: -0.0020115599036216736
        total_loss: -0.000537388026714325
        vf_explained_var: 0.06742116808891296
        vf_loss: 21.928234100341797
      agent-4:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8575631976127625
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012282547540962696
        model: {}
        policy_loss: -0.0034292801283299923
        total_loss: -0.0025570844300091267
        vf_explained_var: 0.011972412467002869
        vf_loss: 23.81509780883789
      agent-5:
        cur_kl_coeff: 3.009265582946607e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.515910804271698
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009600629564374685
        model: {}
        policy_loss: -0.002300277119502425
        total_loss: -0.0010662751737982035
        vf_explained_var: 0.10900132358074188
        vf_loss: 21.420059204101562
    load_time_ms: 29682.765
    num_steps_sampled: 30528000
    num_steps_trained: 30528000
    sample_time_ms: 100636.442
    update_time_ms: 24.452
  iterations_since_restore: 258
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.732400000000002
    ram_util_percent: 16.9052
  pid: 24061
  policy_reward_max:
    agent-0: 205.66666666666637
    agent-1: 205.66666666666637
    agent-2: 205.66666666666637
    agent-3: 205.66666666666637
    agent-4: 205.66666666666637
    agent-5: 205.66666666666637
  policy_reward_mean:
    agent-0: 177.55833333333325
    agent-1: 177.55833333333325
    agent-2: 177.55833333333325
    agent-3: 177.55833333333325
    agent-4: 177.55833333333325
    agent-5: 177.55833333333325
  policy_reward_min:
    agent-0: 77.66666666666659
    agent-1: 77.66666666666659
    agent-2: 77.66666666666659
    agent-3: 77.66666666666659
    agent-4: 77.66666666666659
    agent-5: 77.66666666666659
  sampler_perf:
    mean_env_wait_ms: 24.668555009392175
    mean_inference_ms: 12.24317216194877
    mean_processing_ms: 50.74789464646691
  time_since_restore: 33104.73590564728
  time_this_iter_s: 175.72567749023438
  time_total_s: 42230.74771952629
  timestamp: 1637057152
  timesteps_since_restore: 24768000
  timesteps_this_iter: 96000
  timesteps_total: 30528000
  training_iteration: 318
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 32.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    318 |          42230.7 | 30528000 |  1065.35 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 67
    apples_agent-0_mean: 1.69
    apples_agent-0_min: 0
    apples_agent-1_max: 122
    apples_agent-1_mean: 34.89
    apples_agent-1_min: 0
    apples_agent-2_max: 104
    apples_agent-2_mean: 3.52
    apples_agent-2_min: 0
    apples_agent-3_max: 211
    apples_agent-3_mean: 118.59
    apples_agent-3_min: 0
    apples_agent-4_max: 45
    apples_agent-4_mean: 1.02
    apples_agent-4_min: 0
    apples_agent-5_max: 156
    apples_agent-5_mean: 82.13
    apples_agent-5_min: 44
    cleaning_beam_agent-0_max: 508
    cleaning_beam_agent-0_mean: 376.43
    cleaning_beam_agent-0_min: 253
    cleaning_beam_agent-1_max: 616
    cleaning_beam_agent-1_mean: 313.65
    cleaning_beam_agent-1_min: 128
    cleaning_beam_agent-2_max: 595
    cleaning_beam_agent-2_mean: 439.66
    cleaning_beam_agent-2_min: 201
    cleaning_beam_agent-3_max: 202
    cleaning_beam_agent-3_mean: 15.88
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 633
    cleaning_beam_agent-4_mean: 496.49
    cleaning_beam_agent-4_min: 262
    cleaning_beam_agent-5_max: 133
    cleaning_beam_agent-5_mean: 18.31
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-08-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1246.999999999993
  episode_reward_mean: 1074.7499999999934
  episode_reward_min: 590.999999999999
  episodes_this_iter: 96
  episodes_total: 30624
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20425.028
    learner:
      agent-0:
        cur_kl_coeff: 3.009265582946607e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1358373165130615
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024559726007282734
        model: {}
        policy_loss: -0.003522984217852354
        total_loss: -0.0031574515160173178
        vf_explained_var: -0.018962383270263672
        vf_loss: 23.64607810974121
      agent-1:
        cur_kl_coeff: 6.018531165893214e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0670783519744873
        entropy_coeff: 0.0017600000137463212
        kl: 0.001079728826880455
        model: {}
        policy_loss: -0.003748842980712652
        total_loss: -0.0030956482514739037
        vf_explained_var: -0.02674850821495056
        vf_loss: 25.312522888183594
      agent-2:
        cur_kl_coeff: 6.018531165893214e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9208281636238098
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018179211765527725
        model: {}
        policy_loss: -0.0027961276937276125
        total_loss: -0.0021779867820441723
        vf_explained_var: 0.034724161028862
        vf_loss: 22.38800621032715
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40904703736305237
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011329920962452888
        model: {}
        policy_loss: -0.0020464067347347736
        total_loss: -0.0006036004051566124
        vf_explained_var: 0.06601573526859283
        vf_loss: 21.627286911010742
      agent-4:
        cur_kl_coeff: 6.018531165893214e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8414041996002197
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013126629637554288
        model: {}
        policy_loss: -0.0037735297810286283
        total_loss: -0.002935828175395727
        vf_explained_var: 0.028665557503700256
        vf_loss: 23.18572998046875
      agent-5:
        cur_kl_coeff: 1.5046327914733034e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5310333371162415
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008845165139064193
        model: {}
        policy_loss: -0.0023194197565317154
        total_loss: -0.000996786868199706
        vf_explained_var: 0.06087839603424072
        vf_loss: 22.57251739501953
    load_time_ms: 32290.089
    num_steps_sampled: 30624000
    num_steps_trained: 30624000
    sample_time_ms: 101663.01
    update_time_ms: 25.122
  iterations_since_restore: 259
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.034347826086954
    ram_util_percent: 17.094347826086956
  pid: 24061
  policy_reward_max:
    agent-0: 207.83333333333357
    agent-1: 207.83333333333357
    agent-2: 207.83333333333357
    agent-3: 207.83333333333357
    agent-4: 207.83333333333357
    agent-5: 207.83333333333357
  policy_reward_mean:
    agent-0: 179.12499999999986
    agent-1: 179.12499999999986
    agent-2: 179.12499999999986
    agent-3: 179.12499999999986
    agent-4: 179.12499999999986
    agent-5: 179.12499999999986
  policy_reward_min:
    agent-0: 98.50000000000028
    agent-1: 98.50000000000028
    agent-2: 98.50000000000028
    agent-3: 98.50000000000028
    agent-4: 98.50000000000028
    agent-5: 98.50000000000028
  sampler_perf:
    mean_env_wait_ms: 24.677717656403004
    mean_inference_ms: 12.24564015318438
    mean_processing_ms: 50.76066376149069
  time_since_restore: 33266.110869407654
  time_this_iter_s: 161.37496376037598
  time_total_s: 42392.12268328667
  timestamp: 1637057314
  timesteps_since_restore: 24864000
  timesteps_this_iter: 96000
  timesteps_total: 30624000
  training_iteration: 319
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 33.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    319 |          42392.1 | 30624000 |  1074.75 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 43
    apples_agent-0_mean: 1.79
    apples_agent-0_min: 0
    apples_agent-1_max: 124
    apples_agent-1_mean: 30.91
    apples_agent-1_min: 0
    apples_agent-2_max: 102
    apples_agent-2_mean: 4.86
    apples_agent-2_min: 0
    apples_agent-3_max: 187
    apples_agent-3_mean: 124.58
    apples_agent-3_min: 74
    apples_agent-4_max: 48
    apples_agent-4_mean: 0.9
    apples_agent-4_min: 0
    apples_agent-5_max: 128
    apples_agent-5_mean: 82.61
    apples_agent-5_min: 54
    cleaning_beam_agent-0_max: 508
    cleaning_beam_agent-0_mean: 363.51
    cleaning_beam_agent-0_min: 223
    cleaning_beam_agent-1_max: 827
    cleaning_beam_agent-1_mean: 320.72
    cleaning_beam_agent-1_min: 94
    cleaning_beam_agent-2_max: 627
    cleaning_beam_agent-2_mean: 457.95
    cleaning_beam_agent-2_min: 134
    cleaning_beam_agent-3_max: 113
    cleaning_beam_agent-3_mean: 15.1
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 602
    cleaning_beam_agent-4_mean: 493.64
    cleaning_beam_agent-4_min: 392
    cleaning_beam_agent-5_max: 92
    cleaning_beam_agent-5_mean: 17.44
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-11-27
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1252.9999999999982
  episode_reward_mean: 1088.509999999994
  episode_reward_min: 593.000000000001
  episodes_this_iter: 96
  episodes_total: 30720
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20472.145
    learner:
      agent-0:
        cur_kl_coeff: 1.5046327914733034e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1177634000778198
        entropy_coeff: 0.0017600000137463212
        kl: 0.002007904928177595
        model: {}
        policy_loss: -0.0031785117462277412
        total_loss: -0.0028642278630286455
        vf_explained_var: -0.013000547885894775
        vf_loss: 22.815473556518555
      agent-1:
        cur_kl_coeff: 3.009265582946607e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0633885860443115
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009827432222664356
        model: {}
        policy_loss: -0.003967849537730217
        total_loss: -0.0032158950343728065
        vf_explained_var: -0.07938870787620544
        vf_loss: 26.235218048095703
      agent-2:
        cur_kl_coeff: 3.009265582946607e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9119901657104492
        entropy_coeff: 0.0017600000137463212
        kl: 0.001271274988539517
        model: {}
        policy_loss: -0.002496616216376424
        total_loss: -0.002025966765359044
        vf_explained_var: 0.07554228603839874
        vf_loss: 20.757530212402344
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.404224693775177
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006913046818226576
        model: {}
        policy_loss: -0.00224351161159575
        total_loss: -0.0008170709479600191
        vf_explained_var: 0.045343294739723206
        vf_loss: 21.37874984741211
      agent-4:
        cur_kl_coeff: 3.009265582946607e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8537188768386841
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011733409482985735
        model: {}
        policy_loss: -0.0035926273558288813
        total_loss: -0.0028572315350174904
        vf_explained_var: 0.03648492693901062
        vf_loss: 22.379379272460938
      agent-5:
        cur_kl_coeff: 7.523163957366517e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5028368234634399
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009548991220071912
        model: {}
        policy_loss: -0.002535443287342787
        total_loss: -0.0012821962591260672
        vf_explained_var: 0.07969245314598083
        vf_loss: 21.382400512695312
    load_time_ms: 36217.239
    num_steps_sampled: 30720000
    num_steps_trained: 30720000
    sample_time_ms: 102591.808
    update_time_ms: 25.953
  iterations_since_restore: 260
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.431300813008132
    ram_util_percent: 18.098373983739833
  pid: 24061
  policy_reward_max:
    agent-0: 208.83333333333312
    agent-1: 208.83333333333312
    agent-2: 208.83333333333312
    agent-3: 208.83333333333312
    agent-4: 208.83333333333312
    agent-5: 208.83333333333312
  policy_reward_mean:
    agent-0: 181.41833333333324
    agent-1: 181.41833333333324
    agent-2: 181.41833333333324
    agent-3: 181.41833333333324
    agent-4: 181.41833333333324
    agent-5: 181.41833333333324
  policy_reward_min:
    agent-0: 98.83333333333331
    agent-1: 98.83333333333331
    agent-2: 98.83333333333331
    agent-3: 98.83333333333331
    agent-4: 98.83333333333331
    agent-5: 98.83333333333331
  sampler_perf:
    mean_env_wait_ms: 24.687967499797413
    mean_inference_ms: 12.248500274279609
    mean_processing_ms: 50.77427367658386
  time_since_restore: 33438.460406541824
  time_this_iter_s: 172.34953713417053
  time_total_s: 42564.47222042084
  timestamp: 1637057487
  timesteps_since_restore: 24960000
  timesteps_this_iter: 96000
  timesteps_total: 30720000
  training_iteration: 320
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 33.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    320 |          42564.5 | 30720000 |  1088.51 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 61
    apples_agent-0_mean: 2.14
    apples_agent-0_min: 0
    apples_agent-1_max: 110
    apples_agent-1_mean: 33.08
    apples_agent-1_min: 0
    apples_agent-2_max: 60
    apples_agent-2_mean: 3.14
    apples_agent-2_min: 0
    apples_agent-3_max: 175
    apples_agent-3_mean: 119.08
    apples_agent-3_min: 0
    apples_agent-4_max: 54
    apples_agent-4_mean: 1.11
    apples_agent-4_min: 0
    apples_agent-5_max: 137
    apples_agent-5_mean: 79.99
    apples_agent-5_min: 43
    cleaning_beam_agent-0_max: 484
    cleaning_beam_agent-0_mean: 361.18
    cleaning_beam_agent-0_min: 255
    cleaning_beam_agent-1_max: 592
    cleaning_beam_agent-1_mean: 295.58
    cleaning_beam_agent-1_min: 105
    cleaning_beam_agent-2_max: 640
    cleaning_beam_agent-2_mean: 455.67
    cleaning_beam_agent-2_min: 208
    cleaning_beam_agent-3_max: 158
    cleaning_beam_agent-3_mean: 17.5
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 618
    cleaning_beam_agent-4_mean: 488.65
    cleaning_beam_agent-4_min: 334
    cleaning_beam_agent-5_max: 57
    cleaning_beam_agent-5_mean: 16.77
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-14-01
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1239.000000000002
  episode_reward_mean: 1078.9699999999937
  episode_reward_min: 314.00000000000057
  episodes_this_iter: 96
  episodes_total: 30816
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20458.794
    learner:
      agent-0:
        cur_kl_coeff: 7.523163957366517e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1300129890441895
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016377418069168925
        model: {}
        policy_loss: -0.0033299378119409084
        total_loss: -0.003016410395503044
        vf_explained_var: 0.02372300624847412
        vf_loss: 23.023500442504883
      agent-1:
        cur_kl_coeff: 1.5046327914733034e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 1.077317237854004
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010941150831058621
        model: {}
        policy_loss: -0.004073936492204666
        total_loss: -0.003238774836063385
        vf_explained_var: -0.09670141339302063
        vf_loss: 27.312362670898438
      agent-2:
        cur_kl_coeff: 1.5046327914733034e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9247577786445618
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015102067263796926
        model: {}
        policy_loss: -0.002752224914729595
        total_loss: -0.0021917251870036125
        vf_explained_var: 0.06931205093860626
        vf_loss: 21.880762100219727
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41162657737731934
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007430106634274125
        model: {}
        policy_loss: -0.0022519677877426147
        total_loss: -0.0007422496564686298
        vf_explained_var: 0.05730883777141571
        vf_loss: 22.341821670532227
      agent-4:
        cur_kl_coeff: 1.5046327914733034e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8549943566322327
        entropy_coeff: 0.0017600000137463212
        kl: 0.001988430740311742
        model: {}
        policy_loss: -0.004070490598678589
        total_loss: -0.00330164423212409
        vf_explained_var: 0.06448628008365631
        vf_loss: 22.736373901367188
      agent-5:
        cur_kl_coeff: 3.7615819786832586e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5118999481201172
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006674291798844934
        model: {}
        policy_loss: -0.0022908910177648067
        total_loss: -0.0009015236864797771
        vf_explained_var: 0.05580270290374756
        vf_loss: 22.903133392333984
    load_time_ms: 34534.787
    num_steps_sampled: 30816000
    num_steps_trained: 30816000
    sample_time_ms: 103169.566
    update_time_ms: 24.977
  iterations_since_restore: 261
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.841176470588234
    ram_util_percent: 18.80588235294117
  pid: 24061
  policy_reward_max:
    agent-0: 206.5000000000001
    agent-1: 206.5000000000001
    agent-2: 206.5000000000001
    agent-3: 206.5000000000001
    agent-4: 206.5000000000001
    agent-5: 206.5000000000001
  policy_reward_mean:
    agent-0: 179.8283333333332
    agent-1: 179.8283333333332
    agent-2: 179.8283333333332
    agent-3: 179.8283333333332
    agent-4: 179.8283333333332
    agent-5: 179.8283333333332
  policy_reward_min:
    agent-0: 52.333333333333215
    agent-1: 52.333333333333215
    agent-2: 52.333333333333215
    agent-3: 52.333333333333215
    agent-4: 52.333333333333215
    agent-5: 52.333333333333215
  sampler_perf:
    mean_env_wait_ms: 24.69688449456377
    mean_inference_ms: 12.251033295555429
    mean_processing_ms: 50.78583300580862
  time_since_restore: 33592.632763147354
  time_this_iter_s: 154.17235660552979
  time_total_s: 42718.64457702637
  timestamp: 1637057641
  timesteps_since_restore: 25056000
  timesteps_this_iter: 96000
  timesteps_total: 30816000
  training_iteration: 321
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    321 |          42718.6 | 30816000 |  1078.97 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 90
    apples_agent-0_mean: 3.14
    apples_agent-0_min: 0
    apples_agent-1_max: 109
    apples_agent-1_mean: 33.92
    apples_agent-1_min: 0
    apples_agent-2_max: 89
    apples_agent-2_mean: 5.12
    apples_agent-2_min: 0
    apples_agent-3_max: 179
    apples_agent-3_mean: 126.31
    apples_agent-3_min: 65
    apples_agent-4_max: 35
    apples_agent-4_mean: 0.39
    apples_agent-4_min: 0
    apples_agent-5_max: 165
    apples_agent-5_mean: 85.03
    apples_agent-5_min: 48
    cleaning_beam_agent-0_max: 493
    cleaning_beam_agent-0_mean: 375.11
    cleaning_beam_agent-0_min: 253
    cleaning_beam_agent-1_max: 529
    cleaning_beam_agent-1_mean: 305.78
    cleaning_beam_agent-1_min: 105
    cleaning_beam_agent-2_max: 625
    cleaning_beam_agent-2_mean: 464.6
    cleaning_beam_agent-2_min: 181
    cleaning_beam_agent-3_max: 55
    cleaning_beam_agent-3_mean: 15.77
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 589
    cleaning_beam_agent-4_mean: 490.1
    cleaning_beam_agent-4_min: 377
    cleaning_beam_agent-5_max: 78
    cleaning_beam_agent-5_mean: 18.86
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-16-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1246.999999999994
  episode_reward_mean: 1086.7899999999927
  episode_reward_min: 663.000000000002
  episodes_this_iter: 96
  episodes_total: 30912
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20461.93
    learner:
      agent-0:
        cur_kl_coeff: 3.7615819786832586e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 1.110876202583313
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019190595485270023
        model: {}
        policy_loss: -0.002749894978478551
        total_loss: -0.002477307105436921
        vf_explained_var: -0.0006210058927536011
        vf_loss: 22.277360916137695
      agent-1:
        cur_kl_coeff: 7.523163957366517e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0761394500732422
        entropy_coeff: 0.0017600000137463212
        kl: 0.001153710181824863
        model: {}
        policy_loss: -0.004116520751267672
        total_loss: -0.0035602166317403316
        vf_explained_var: -0.024523749947547913
        vf_loss: 24.503101348876953
      agent-2:
        cur_kl_coeff: 7.523163957366517e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9138630032539368
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013007787056267262
        model: {}
        policy_loss: -0.0028179993387311697
        total_loss: -0.0022662708070129156
        vf_explained_var: 0.029994994401931763
        vf_loss: 21.601255416870117
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4035901427268982
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012122507905587554
        model: {}
        policy_loss: -0.002263703616335988
        total_loss: -0.0008449219167232513
        vf_explained_var: 0.038695648312568665
        vf_loss: 21.29097557067871
      agent-4:
        cur_kl_coeff: 7.523163957366517e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8633933663368225
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016131021548062563
        model: {}
        policy_loss: -0.0037995963357388973
        total_loss: -0.003060355316847563
        vf_explained_var: 0.013935700058937073
        vf_loss: 22.58815574645996
      agent-5:
        cur_kl_coeff: 1.8807909893416293e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5172572731971741
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008845208212733269
        model: {}
        policy_loss: -0.0023450730368494987
        total_loss: -0.0011086026206612587
        vf_explained_var: 0.07345196604728699
        vf_loss: 21.46841812133789
    load_time_ms: 33262.624
    num_steps_sampled: 30912000
    num_steps_trained: 30912000
    sample_time_ms: 102618.071
    update_time_ms: 24.552
  iterations_since_restore: 262
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.43062200956938
    ram_util_percent: 18.736842105263154
  pid: 24061
  policy_reward_max:
    agent-0: 207.83333333333331
    agent-1: 207.83333333333331
    agent-2: 207.83333333333331
    agent-3: 207.83333333333331
    agent-4: 207.83333333333331
    agent-5: 207.83333333333331
  policy_reward_mean:
    agent-0: 181.1316666666665
    agent-1: 181.1316666666665
    agent-2: 181.1316666666665
    agent-3: 181.1316666666665
    agent-4: 181.1316666666665
    agent-5: 181.1316666666665
  policy_reward_min:
    agent-0: 110.50000000000043
    agent-1: 110.50000000000043
    agent-2: 110.50000000000043
    agent-3: 110.50000000000043
    agent-4: 110.50000000000043
    agent-5: 110.50000000000043
  sampler_perf:
    mean_env_wait_ms: 24.70704740426509
    mean_inference_ms: 12.253982086702015
    mean_processing_ms: 50.79844292139503
  time_since_restore: 33739.95030260086
  time_this_iter_s: 147.31753945350647
  time_total_s: 42865.962116479874
  timestamp: 1637057789
  timesteps_since_restore: 25152000
  timesteps_this_iter: 96000
  timesteps_total: 30912000
  training_iteration: 322
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    322 |            42866 | 30912000 |  1086.79 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 58
    apples_agent-0_mean: 3.51
    apples_agent-0_min: 0
    apples_agent-1_max: 156
    apples_agent-1_mean: 30.85
    apples_agent-1_min: 0
    apples_agent-2_max: 111
    apples_agent-2_mean: 3.1
    apples_agent-2_min: 0
    apples_agent-3_max: 225
    apples_agent-3_mean: 133.08
    apples_agent-3_min: 78
    apples_agent-4_max: 51
    apples_agent-4_mean: 1.09
    apples_agent-4_min: 0
    apples_agent-5_max: 172
    apples_agent-5_mean: 84.6
    apples_agent-5_min: 53
    cleaning_beam_agent-0_max: 501
    cleaning_beam_agent-0_mean: 386.44
    cleaning_beam_agent-0_min: 250
    cleaning_beam_agent-1_max: 622
    cleaning_beam_agent-1_mean: 301.02
    cleaning_beam_agent-1_min: 133
    cleaning_beam_agent-2_max: 608
    cleaning_beam_agent-2_mean: 470.62
    cleaning_beam_agent-2_min: 205
    cleaning_beam_agent-3_max: 68
    cleaning_beam_agent-3_mean: 13.78
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 589
    cleaning_beam_agent-4_mean: 488.57
    cleaning_beam_agent-4_min: 386
    cleaning_beam_agent-5_max: 95
    cleaning_beam_agent-5_mean: 16.61
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-18-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1234.9999999999764
  episode_reward_mean: 1084.6799999999937
  episode_reward_min: 806.9999999999773
  episodes_this_iter: 96
  episodes_total: 31008
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20448.401
    learner:
      agent-0:
        cur_kl_coeff: 1.8807909893416293e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0958057641983032
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014073712518438697
        model: {}
        policy_loss: -0.0031475024297833443
        total_loss: -0.0028415964916348457
        vf_explained_var: 0.0014114528894424438
        vf_loss: 22.345203399658203
      agent-1:
        cur_kl_coeff: 3.7615819786832586e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0674757957458496
        entropy_coeff: 0.0017600000137463212
        kl: 0.001389174023643136
        model: {}
        policy_loss: -0.004061446990817785
        total_loss: -0.0033622724004089832
        vf_explained_var: -0.08335033059120178
        vf_loss: 25.779281616210938
      agent-2:
        cur_kl_coeff: 3.7615819786832586e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9114816784858704
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013438432943075895
        model: {}
        policy_loss: -0.0026008598506450653
        total_loss: -0.002066634828224778
        vf_explained_var: 0.0395631343126297
        vf_loss: 21.384319305419922
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40581822395324707
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009756138897500932
        model: {}
        policy_loss: -0.0020904531702399254
        total_loss: -0.0006716882344335318
        vf_explained_var: 0.04612824320793152
        vf_loss: 21.330076217651367
      agent-4:
        cur_kl_coeff: 3.7615819786832586e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8457139134407043
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021958444267511368
        model: {}
        policy_loss: -0.0038817543536424637
        total_loss: -0.003011148888617754
        vf_explained_var: -0.02888225018978119
        vf_loss: 23.590612411499023
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5098548531532288
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006691321614198387
        model: {}
        policy_loss: -0.0024181660264730453
        total_loss: -0.0011598877608776093
        vf_explained_var: 0.059888869524002075
        vf_loss: 21.556201934814453
    load_time_ms: 31423.582
    num_steps_sampled: 31008000
    num_steps_trained: 31008000
    sample_time_ms: 102511.247
    update_time_ms: 24.327
  iterations_since_restore: 263
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.744102564102562
    ram_util_percent: 17.824615384615385
  pid: 24061
  policy_reward_max:
    agent-0: 205.83333333333363
    agent-1: 205.83333333333363
    agent-2: 205.83333333333363
    agent-3: 205.83333333333363
    agent-4: 205.83333333333363
    agent-5: 205.83333333333363
  policy_reward_mean:
    agent-0: 180.77999999999983
    agent-1: 180.77999999999983
    agent-2: 180.77999999999983
    agent-3: 180.77999999999983
    agent-4: 180.77999999999983
    agent-5: 180.77999999999983
  policy_reward_min:
    agent-0: 134.50000000000034
    agent-1: 134.50000000000034
    agent-2: 134.50000000000034
    agent-3: 134.50000000000034
    agent-4: 134.50000000000034
    agent-5: 134.50000000000034
  sampler_perf:
    mean_env_wait_ms: 24.716884692678654
    mean_inference_ms: 12.25682567822281
    mean_processing_ms: 50.811586453701764
  time_since_restore: 33876.677109241486
  time_this_iter_s: 136.726806640625
  time_total_s: 43002.6889231205
  timestamp: 1637057926
  timesteps_since_restore: 25248000
  timesteps_this_iter: 96000
  timesteps_total: 31008000
  training_iteration: 323
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    323 |          43002.7 | 31008000 |  1084.68 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 34
    apples_agent-0_mean: 1.96
    apples_agent-0_min: 0
    apples_agent-1_max: 116
    apples_agent-1_mean: 29.64
    apples_agent-1_min: 0
    apples_agent-2_max: 118
    apples_agent-2_mean: 2.78
    apples_agent-2_min: 0
    apples_agent-3_max: 455
    apples_agent-3_mean: 132.67
    apples_agent-3_min: 84
    apples_agent-4_max: 19
    apples_agent-4_mean: 0.34
    apples_agent-4_min: 0
    apples_agent-5_max: 466
    apples_agent-5_mean: 88.14
    apples_agent-5_min: 50
    cleaning_beam_agent-0_max: 534
    cleaning_beam_agent-0_mean: 378.74
    cleaning_beam_agent-0_min: 261
    cleaning_beam_agent-1_max: 522
    cleaning_beam_agent-1_mean: 314.0
    cleaning_beam_agent-1_min: 132
    cleaning_beam_agent-2_max: 621
    cleaning_beam_agent-2_mean: 458.28
    cleaning_beam_agent-2_min: 247
    cleaning_beam_agent-3_max: 52
    cleaning_beam_agent-3_mean: 13.91
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 616
    cleaning_beam_agent-4_mean: 497.58
    cleaning_beam_agent-4_min: 408
    cleaning_beam_agent-5_max: 124
    cleaning_beam_agent-5_mean: 16.55
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.08
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-21-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1256.0000000000136
  episode_reward_mean: 1090.6499999999974
  episode_reward_min: 645.9999999999993
  episodes_this_iter: 96
  episodes_total: 31104
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20482.495
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1230401992797852
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017171427607536316
        model: {}
        policy_loss: -0.003240905934944749
        total_loss: -0.003018462099134922
        vf_explained_var: 0.02861487865447998
        vf_loss: 21.98990249633789
      agent-1:
        cur_kl_coeff: 1.8807909893416293e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0779154300689697
        entropy_coeff: 0.0017600000137463212
        kl: 0.001759109552949667
        model: {}
        policy_loss: -0.004131513647735119
        total_loss: -0.00348869152367115
        vf_explained_var: -0.05878448486328125
        vf_loss: 25.399545669555664
      agent-2:
        cur_kl_coeff: 1.8807909893416293e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9305534362792969
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024118840228766203
        model: {}
        policy_loss: -0.002986401319503784
        total_loss: -0.0023942277766764164
        vf_explained_var: 0.01449216902256012
        vf_loss: 22.299482345581055
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.392765611410141
        entropy_coeff: 0.0017600000137463212
        kl: 0.00123760593123734
        model: {}
        policy_loss: -0.0018531817477196455
        total_loss: -0.0003464559558779001
        vf_explained_var: 0.023546084761619568
        vf_loss: 21.979948043823242
      agent-4:
        cur_kl_coeff: 1.8807909893416293e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8559529781341553
        entropy_coeff: 0.0017600000137463212
        kl: 0.001930375350639224
        model: {}
        policy_loss: -0.003909015096724033
        total_loss: -0.0029973220080137253
        vf_explained_var: -0.033514440059661865
        vf_loss: 24.181720733642578
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5010982155799866
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006833865190856159
        model: {}
        policy_loss: -0.002046339213848114
        total_loss: -0.0006043575704097748
        vf_explained_var: -0.006787553429603577
        vf_loss: 23.239179611206055
    load_time_ms: 30826.885
    num_steps_sampled: 31104000
    num_steps_trained: 31104000
    sample_time_ms: 102372.953
    update_time_ms: 23.154
  iterations_since_restore: 264
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.814666666666664
    ram_util_percent: 18.475555555555555
  pid: 24061
  policy_reward_max:
    agent-0: 209.3333333333332
    agent-1: 209.3333333333332
    agent-2: 209.3333333333332
    agent-3: 209.3333333333332
    agent-4: 209.3333333333332
    agent-5: 209.3333333333332
  policy_reward_mean:
    agent-0: 181.77499999999986
    agent-1: 181.77499999999986
    agent-2: 181.77499999999986
    agent-3: 181.77499999999986
    agent-4: 181.77499999999986
    agent-5: 181.77499999999986
  policy_reward_min:
    agent-0: 107.66666666666657
    agent-1: 107.66666666666657
    agent-2: 107.66666666666657
    agent-3: 107.66666666666657
    agent-4: 107.66666666666657
    agent-5: 107.66666666666657
  sampler_perf:
    mean_env_wait_ms: 24.726810551945686
    mean_inference_ms: 12.259673019896741
    mean_processing_ms: 50.824025460219154
  time_since_restore: 34034.379583120346
  time_this_iter_s: 157.70247387886047
  time_total_s: 43160.39139699936
  timestamp: 1637058084
  timesteps_since_restore: 25344000
  timesteps_this_iter: 96000
  timesteps_total: 31104000
  training_iteration: 324
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 33.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    324 |          43160.4 | 31104000 |  1090.65 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 38
    apples_agent-0_mean: 2.39
    apples_agent-0_min: 0
    apples_agent-1_max: 141
    apples_agent-1_mean: 32.99
    apples_agent-1_min: 0
    apples_agent-2_max: 38
    apples_agent-2_mean: 1.98
    apples_agent-2_min: 0
    apples_agent-3_max: 212
    apples_agent-3_mean: 125.47
    apples_agent-3_min: 74
    apples_agent-4_max: 59
    apples_agent-4_mean: 1.11
    apples_agent-4_min: 0
    apples_agent-5_max: 165
    apples_agent-5_mean: 81.09
    apples_agent-5_min: 41
    cleaning_beam_agent-0_max: 485
    cleaning_beam_agent-0_mean: 373.29
    cleaning_beam_agent-0_min: 256
    cleaning_beam_agent-1_max: 529
    cleaning_beam_agent-1_mean: 296.16
    cleaning_beam_agent-1_min: 115
    cleaning_beam_agent-2_max: 607
    cleaning_beam_agent-2_mean: 432.97
    cleaning_beam_agent-2_min: 202
    cleaning_beam_agent-3_max: 81
    cleaning_beam_agent-3_mean: 16.46
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 587
    cleaning_beam_agent-4_mean: 485.75
    cleaning_beam_agent-4_min: 389
    cleaning_beam_agent-5_max: 124
    cleaning_beam_agent-5_mean: 18.98
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-24-22
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1242.9999999999932
  episode_reward_mean: 1059.2399999999916
  episode_reward_min: 711.9999999999907
  episodes_this_iter: 96
  episodes_total: 31200
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20474.863
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1222093105316162
        entropy_coeff: 0.0017600000137463212
        kl: 0.001495511271059513
        model: {}
        policy_loss: -0.0031729612965136766
        total_loss: -0.0029114203061908484
        vf_explained_var: 0.0092630535364151
        vf_loss: 22.36630630493164
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0811183452606201
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012840430717915297
        model: {}
        policy_loss: -0.003833790309727192
        total_loss: -0.003165089525282383
        vf_explained_var: -0.0859735906124115
        vf_loss: 25.714698791503906
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9291820526123047
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015048946952447295
        model: {}
        policy_loss: -0.0028322506695985794
        total_loss: -0.0022688554599881172
        vf_explained_var: 0.02674967050552368
        vf_loss: 21.98756217956543
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42569899559020996
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006854278035461903
        model: {}
        policy_loss: -0.002201605588197708
        total_loss: -0.0008259224705398083
        vf_explained_var: 0.061172306537628174
        vf_loss: 21.249113082885742
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8643782138824463
        entropy_coeff: 0.0017600000137463212
        kl: 0.001228862558491528
        model: {}
        policy_loss: -0.0037250500172376633
        total_loss: -0.002955125644803047
        vf_explained_var: 0.0036308467388153076
        vf_loss: 22.912290573120117
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5119426846504211
        entropy_coeff: 0.0017600000137463212
        kl: 0.001257212134078145
        model: {}
        policy_loss: -0.002656251657754183
        total_loss: -0.001432672142982483
        vf_explained_var: 0.06936590373516083
        vf_loss: 21.245962142944336
    load_time_ms: 33549.008
    num_steps_sampled: 31200000
    num_steps_trained: 31200000
    sample_time_ms: 102630.411
    update_time_ms: 23.681
  iterations_since_restore: 265
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.33779527559055
    ram_util_percent: 19.58385826771654
  pid: 24061
  policy_reward_max:
    agent-0: 207.16666666666632
    agent-1: 207.16666666666632
    agent-2: 207.16666666666632
    agent-3: 207.16666666666632
    agent-4: 207.16666666666632
    agent-5: 207.16666666666632
  policy_reward_mean:
    agent-0: 176.53999999999985
    agent-1: 176.53999999999985
    agent-2: 176.53999999999985
    agent-3: 176.53999999999985
    agent-4: 176.53999999999985
    agent-5: 176.53999999999985
  policy_reward_min:
    agent-0: 118.66666666666703
    agent-1: 118.66666666666703
    agent-2: 118.66666666666703
    agent-3: 118.66666666666703
    agent-4: 118.66666666666703
    agent-5: 118.66666666666703
  sampler_perf:
    mean_env_wait_ms: 24.73614365479167
    mean_inference_ms: 12.262793001077625
    mean_processing_ms: 50.838649547380854
  time_since_restore: 34212.24836111069
  time_this_iter_s: 177.8687779903412
  time_total_s: 43338.2601749897
  timestamp: 1637058262
  timesteps_since_restore: 25440000
  timesteps_this_iter: 96000
  timesteps_total: 31200000
  training_iteration: 325
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    325 |          43338.3 | 31200000 |  1059.24 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 28
    apples_agent-0_mean: 0.94
    apples_agent-0_min: 0
    apples_agent-1_max: 130
    apples_agent-1_mean: 27.22
    apples_agent-1_min: 0
    apples_agent-2_max: 44
    apples_agent-2_mean: 1.82
    apples_agent-2_min: 0
    apples_agent-3_max: 263
    apples_agent-3_mean: 130.33
    apples_agent-3_min: 75
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 126
    apples_agent-5_mean: 82.57
    apples_agent-5_min: 35
    cleaning_beam_agent-0_max: 477
    cleaning_beam_agent-0_mean: 376.87
    cleaning_beam_agent-0_min: 256
    cleaning_beam_agent-1_max: 541
    cleaning_beam_agent-1_mean: 299.27
    cleaning_beam_agent-1_min: 129
    cleaning_beam_agent-2_max: 642
    cleaning_beam_agent-2_mean: 460.01
    cleaning_beam_agent-2_min: 260
    cleaning_beam_agent-3_max: 47
    cleaning_beam_agent-3_mean: 12.85
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 613
    cleaning_beam_agent-4_mean: 494.5
    cleaning_beam_agent-4_min: 399
    cleaning_beam_agent-5_max: 119
    cleaning_beam_agent-5_mean: 16.3
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-26-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1263.0000000000089
  episode_reward_mean: 1105.149999999994
  episode_reward_min: 889.9999999999853
  episodes_this_iter: 96
  episodes_total: 31296
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20500.346
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1114033460617065
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015454899985343218
        model: {}
        policy_loss: -0.003019306343048811
        total_loss: -0.0027882088907063007
        vf_explained_var: -0.03120243549346924
        vf_loss: 21.871681213378906
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0773568153381348
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020431859884411097
        model: {}
        policy_loss: -0.00383351044729352
        total_loss: -0.0031297137029469013
        vf_explained_var: -0.08879658579826355
        vf_loss: 25.999481201171875
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9159301519393921
        entropy_coeff: 0.0017600000137463212
        kl: 0.002036034595221281
        model: {}
        policy_loss: -0.0028057722374796867
        total_loss: -0.002231478225439787
        vf_explained_var: -0.0340900719165802
        vf_loss: 21.86330795288086
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3820948004722595
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012489805230870843
        model: {}
        policy_loss: -0.0020102106500416994
        total_loss: -0.000597085920162499
        vf_explained_var: 0.016589581966400146
        vf_loss: 20.856124877929688
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8477036356925964
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021027647890150547
        model: {}
        policy_loss: -0.003972471226006746
        total_loss: -0.0032081063836812973
        vf_explained_var: -0.013469740748405457
        vf_loss: 22.56324577331543
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4745418429374695
        entropy_coeff: 0.0017600000137463212
        kl: 0.0003879085707012564
        model: {}
        policy_loss: -0.0017962181009352207
        total_loss: -0.0004923357628285885
        vf_explained_var: 0.027407974004745483
        vf_loss: 21.39075469970703
    load_time_ms: 33947.438
    num_steps_sampled: 31296000
    num_steps_trained: 31296000
    sample_time_ms: 103054.157
    update_time_ms: 22.86
  iterations_since_restore: 266
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.88603603603603
    ram_util_percent: 19.55
  pid: 24061
  policy_reward_max:
    agent-0: 210.49999999999994
    agent-1: 210.49999999999994
    agent-2: 210.49999999999994
    agent-3: 210.49999999999994
    agent-4: 210.49999999999994
    agent-5: 210.49999999999994
  policy_reward_mean:
    agent-0: 184.19166666666658
    agent-1: 184.19166666666658
    agent-2: 184.19166666666658
    agent-3: 184.19166666666658
    agent-4: 184.19166666666658
    agent-5: 184.19166666666658
  policy_reward_min:
    agent-0: 148.3333333333334
    agent-1: 148.3333333333334
    agent-2: 148.3333333333334
    agent-3: 148.3333333333334
    agent-4: 148.3333333333334
    agent-5: 148.3333333333334
  sampler_perf:
    mean_env_wait_ms: 24.745962049075
    mean_inference_ms: 12.266020389660929
    mean_processing_ms: 50.85395995074061
  time_since_restore: 34368.012785196304
  time_this_iter_s: 155.76442408561707
  time_total_s: 43494.02459907532
  timestamp: 1637058418
  timesteps_since_restore: 25536000
  timesteps_this_iter: 96000
  timesteps_total: 31296000
  training_iteration: 326
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    326 |            43494 | 31296000 |  1105.15 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 38
    apples_agent-0_mean: 1.59
    apples_agent-0_min: 0
    apples_agent-1_max: 139
    apples_agent-1_mean: 35.31
    apples_agent-1_min: 0
    apples_agent-2_max: 148
    apples_agent-2_mean: 6.42
    apples_agent-2_min: 0
    apples_agent-3_max: 203
    apples_agent-3_mean: 126.0
    apples_agent-3_min: 63
    apples_agent-4_max: 8
    apples_agent-4_mean: 0.17
    apples_agent-4_min: 0
    apples_agent-5_max: 154
    apples_agent-5_mean: 80.75
    apples_agent-5_min: 48
    cleaning_beam_agent-0_max: 519
    cleaning_beam_agent-0_mean: 371.88
    cleaning_beam_agent-0_min: 196
    cleaning_beam_agent-1_max: 516
    cleaning_beam_agent-1_mean: 298.36
    cleaning_beam_agent-1_min: 116
    cleaning_beam_agent-2_max: 591
    cleaning_beam_agent-2_mean: 431.04
    cleaning_beam_agent-2_min: 172
    cleaning_beam_agent-3_max: 77
    cleaning_beam_agent-3_mean: 15.73
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 582
    cleaning_beam_agent-4_mean: 499.25
    cleaning_beam_agent-4_min: 342
    cleaning_beam_agent-5_max: 75
    cleaning_beam_agent-5_mean: 15.74
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-29-17
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1269.0000000000082
  episode_reward_mean: 1079.5799999999942
  episode_reward_min: 528.0000000000098
  episodes_this_iter: 96
  episodes_total: 31392
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20513.95
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1101610660552979
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015065225306898355
        model: {}
        policy_loss: -0.0030665197409689426
        total_loss: -0.0026675285771489143
        vf_explained_var: 0.014114603400230408
        vf_loss: 23.528749465942383
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0962936878204346
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015388301108032465
        model: {}
        policy_loss: -0.004323693923652172
        total_loss: -0.0035935500636696815
        vf_explained_var: -0.05602562427520752
        vf_loss: 26.596235275268555
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9419349431991577
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013340828008949757
        model: {}
        policy_loss: -0.002867761766538024
        total_loss: -0.002334343735128641
        vf_explained_var: 0.08343857526779175
        vf_loss: 21.91223907470703
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41548436880111694
        entropy_coeff: 0.0017600000137463212
        kl: 0.001002717181108892
        model: {}
        policy_loss: -0.0024276934564113617
        total_loss: -0.00100602675229311
        vf_explained_var: 0.0990050733089447
        vf_loss: 21.529186248779297
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8392321467399597
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014824201352894306
        model: {}
        policy_loss: -0.0035251029767096043
        total_loss: -0.0026280065067112446
        vf_explained_var: 0.016062229871749878
        vf_loss: 23.741432189941406
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4995557963848114
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007646869053132832
        model: {}
        policy_loss: -0.002465632278472185
        total_loss: -0.0011125197634100914
        vf_explained_var: 0.08039003610610962
        vf_loss: 22.323314666748047
    load_time_ms: 33620.731
    num_steps_sampled: 31392000
    num_steps_trained: 31392000
    sample_time_ms: 103521.792
    update_time_ms: 23.028
  iterations_since_restore: 267
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.15707070707071
    ram_util_percent: 19.397979797979804
  pid: 24061
  policy_reward_max:
    agent-0: 211.49999999999986
    agent-1: 211.49999999999986
    agent-2: 211.49999999999986
    agent-3: 211.49999999999986
    agent-4: 211.49999999999986
    agent-5: 211.49999999999986
  policy_reward_mean:
    agent-0: 179.9299999999998
    agent-1: 179.9299999999998
    agent-2: 179.9299999999998
    agent-3: 179.9299999999998
    agent-4: 179.9299999999998
    agent-5: 179.9299999999998
  policy_reward_min:
    agent-0: 88.00000000000016
    agent-1: 88.00000000000016
    agent-2: 88.00000000000016
    agent-3: 88.00000000000016
    agent-4: 88.00000000000016
    agent-5: 88.00000000000016
  sampler_perf:
    mean_env_wait_ms: 24.756002951723172
    mean_inference_ms: 12.269520547063612
    mean_processing_ms: 50.87099812524027
  time_since_restore: 34507.27349257469
  time_this_iter_s: 139.26070737838745
  time_total_s: 43633.285306453705
  timestamp: 1637058557
  timesteps_since_restore: 25632000
  timesteps_this_iter: 96000
  timesteps_total: 31392000
  training_iteration: 327
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 35.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    327 |          43633.3 | 31392000 |  1079.58 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 66
    apples_agent-0_mean: 4.85
    apples_agent-0_min: 0
    apples_agent-1_max: 116
    apples_agent-1_mean: 32.87
    apples_agent-1_min: 0
    apples_agent-2_max: 51
    apples_agent-2_mean: 2.17
    apples_agent-2_min: 0
    apples_agent-3_max: 196
    apples_agent-3_mean: 121.79
    apples_agent-3_min: 63
    apples_agent-4_max: 27
    apples_agent-4_mean: 0.71
    apples_agent-4_min: 0
    apples_agent-5_max: 124
    apples_agent-5_mean: 83.61
    apples_agent-5_min: 29
    cleaning_beam_agent-0_max: 466
    cleaning_beam_agent-0_mean: 356.57
    cleaning_beam_agent-0_min: 169
    cleaning_beam_agent-1_max: 521
    cleaning_beam_agent-1_mean: 285.61
    cleaning_beam_agent-1_min: 118
    cleaning_beam_agent-2_max: 624
    cleaning_beam_agent-2_mean: 442.17
    cleaning_beam_agent-2_min: 172
    cleaning_beam_agent-3_max: 47
    cleaning_beam_agent-3_mean: 13.72
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 614
    cleaning_beam_agent-4_mean: 506.02
    cleaning_beam_agent-4_min: 409
    cleaning_beam_agent-5_max: 179
    cleaning_beam_agent-5_mean: 17.21
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.08
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-31-52
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1233.999999999985
  episode_reward_mean: 1089.2299999999946
  episode_reward_min: 528.0000000000098
  episodes_this_iter: 96
  episodes_total: 31488
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20490.985
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.128912329673767
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024316557683050632
        model: {}
        policy_loss: -0.003607003716751933
        total_loss: -0.003310366766527295
        vf_explained_var: 0.004522159695625305
        vf_loss: 22.835222244262695
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0880658626556396
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019010251853615046
        model: {}
        policy_loss: -0.00421983003616333
        total_loss: -0.0035807923413813114
        vf_explained_var: -0.05545976758003235
        vf_loss: 25.54033660888672
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9270907640457153
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013599132653325796
        model: {}
        policy_loss: -0.0026798793114721775
        total_loss: -0.0020293539855629206
        vf_explained_var: 0.0014846622943878174
        vf_loss: 22.82206153869629
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39327022433280945
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014563007280230522
        model: {}
        policy_loss: -0.0022560814395546913
        total_loss: -0.0008464902639389038
        vf_explained_var: 0.08766834437847137
        vf_loss: 21.0174560546875
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8433873057365417
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012110541574656963
        model: {}
        policy_loss: -0.003463251981884241
        total_loss: -0.0026062054093927145
        vf_explained_var: -0.008169040083885193
        vf_loss: 23.41407012939453
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49144190549850464
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005393467145040631
        model: {}
        policy_loss: -0.002114546252414584
        total_loss: -0.0006618564948439598
        vf_explained_var: 0.01943761110305786
        vf_loss: 23.176279067993164
    load_time_ms: 34528.152
    num_steps_sampled: 31488000
    num_steps_trained: 31488000
    sample_time_ms: 100590.898
    update_time_ms: 23.278
  iterations_since_restore: 268
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.912217194570133
    ram_util_percent: 19.6158371040724
  pid: 24061
  policy_reward_max:
    agent-0: 205.66666666666646
    agent-1: 205.66666666666646
    agent-2: 205.66666666666646
    agent-3: 205.66666666666646
    agent-4: 205.66666666666646
    agent-5: 205.66666666666646
  policy_reward_mean:
    agent-0: 181.53833333333313
    agent-1: 181.53833333333313
    agent-2: 181.53833333333313
    agent-3: 181.53833333333313
    agent-4: 181.53833333333313
    agent-5: 181.53833333333313
  policy_reward_min:
    agent-0: 88.00000000000016
    agent-1: 88.00000000000016
    agent-2: 88.00000000000016
    agent-3: 88.00000000000016
    agent-4: 88.00000000000016
    agent-5: 88.00000000000016
  sampler_perf:
    mean_env_wait_ms: 24.766532533406792
    mean_inference_ms: 12.272304035710283
    mean_processing_ms: 50.88767366942172
  time_since_restore: 34662.52872252464
  time_this_iter_s: 155.25522994995117
  time_total_s: 43788.540536403656
  timestamp: 1637058712
  timesteps_since_restore: 25728000
  timesteps_this_iter: 96000
  timesteps_total: 31488000
  training_iteration: 328
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    328 |          43788.5 | 31488000 |  1089.23 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 52
    apples_agent-0_mean: 2.72
    apples_agent-0_min: 0
    apples_agent-1_max: 169
    apples_agent-1_mean: 34.34
    apples_agent-1_min: 0
    apples_agent-2_max: 30
    apples_agent-2_mean: 2.08
    apples_agent-2_min: 0
    apples_agent-3_max: 196
    apples_agent-3_mean: 115.27
    apples_agent-3_min: 63
    apples_agent-4_max: 3
    apples_agent-4_mean: 0.03
    apples_agent-4_min: 0
    apples_agent-5_max: 140
    apples_agent-5_mean: 79.54
    apples_agent-5_min: 58
    cleaning_beam_agent-0_max: 455
    cleaning_beam_agent-0_mean: 363.77
    cleaning_beam_agent-0_min: 221
    cleaning_beam_agent-1_max: 599
    cleaning_beam_agent-1_mean: 287.34
    cleaning_beam_agent-1_min: 115
    cleaning_beam_agent-2_max: 593
    cleaning_beam_agent-2_mean: 436.22
    cleaning_beam_agent-2_min: 241
    cleaning_beam_agent-3_max: 68
    cleaning_beam_agent-3_mean: 13.0
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 604
    cleaning_beam_agent-4_mean: 502.27
    cleaning_beam_agent-4_min: 378
    cleaning_beam_agent-5_max: 104
    cleaning_beam_agent-5_mean: 16.1
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-34-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1225.9999999999914
  episode_reward_mean: 1095.2899999999936
  episode_reward_min: 695.9999999999947
  episodes_this_iter: 96
  episodes_total: 31584
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20455.55
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1206929683685303
        entropy_coeff: 0.0017600000137463212
        kl: 0.001781410537660122
        model: {}
        policy_loss: -0.00344167766161263
        total_loss: -0.0032093788031488657
        vf_explained_var: -0.02334761619567871
        vf_loss: 22.0472354888916
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0839850902557373
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016992829041555524
        model: {}
        policy_loss: -0.004330120049417019
        total_loss: -0.0036590148229151964
        vf_explained_var: -0.10903167724609375
        vf_loss: 25.789175033569336
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9340116381645203
        entropy_coeff: 0.0017600000137463212
        kl: 0.001970175188034773
        model: {}
        policy_loss: -0.0033135637640953064
        total_loss: -0.0028800424188375473
        vf_explained_var: 0.03279416263103485
        vf_loss: 20.773813247680664
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3867938816547394
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008324361406266689
        model: {}
        policy_loss: -0.002012507990002632
        total_loss: -0.0006482759490609169
        vf_explained_var: 0.057107239961624146
        vf_loss: 20.449865341186523
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8407502770423889
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012780062388628721
        model: {}
        policy_loss: -0.0035907519049942493
        total_loss: -0.002894829958677292
        vf_explained_var: 0.016692176461219788
        vf_loss: 21.756412506103516
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49141690135002136
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005983359296806157
        model: {}
        policy_loss: -0.0020204992033541203
        total_loss: -0.0007216446101665497
        vf_explained_var: 0.03611084818840027
        vf_loss: 21.637474060058594
    load_time_ms: 35186.16
    num_steps_sampled: 31584000
    num_steps_trained: 31584000
    sample_time_ms: 100246.478
    update_time_ms: 22.64
  iterations_since_restore: 269
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.18119658119658
    ram_util_percent: 18.326923076923077
  pid: 24061
  policy_reward_max:
    agent-0: 204.3333333333331
    agent-1: 204.3333333333331
    agent-2: 204.3333333333331
    agent-3: 204.3333333333331
    agent-4: 204.3333333333331
    agent-5: 204.3333333333331
  policy_reward_mean:
    agent-0: 182.54833333333318
    agent-1: 182.54833333333318
    agent-2: 182.54833333333318
    agent-3: 182.54833333333318
    agent-4: 182.54833333333318
    agent-5: 182.54833333333318
  policy_reward_min:
    agent-0: 116.00000000000053
    agent-1: 116.00000000000053
    agent-2: 116.00000000000053
    agent-3: 116.00000000000053
    agent-4: 116.00000000000053
    agent-5: 116.00000000000053
  sampler_perf:
    mean_env_wait_ms: 24.776111336656083
    mean_inference_ms: 12.274704248653755
    mean_processing_ms: 50.901962725856805
  time_since_restore: 34826.68726396561
  time_this_iter_s: 164.15854144096375
  time_total_s: 43952.69907784462
  timestamp: 1637058877
  timesteps_since_restore: 25824000
  timesteps_this_iter: 96000
  timesteps_total: 31584000
  training_iteration: 329
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 31.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    329 |          43952.7 | 31584000 |  1095.29 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 97
    apples_agent-0_mean: 2.55
    apples_agent-0_min: 0
    apples_agent-1_max: 155
    apples_agent-1_mean: 33.26
    apples_agent-1_min: 0
    apples_agent-2_max: 71
    apples_agent-2_mean: 3.89
    apples_agent-2_min: 0
    apples_agent-3_max: 210
    apples_agent-3_mean: 126.71
    apples_agent-3_min: 71
    apples_agent-4_max: 51
    apples_agent-4_mean: 1.0
    apples_agent-4_min: 0
    apples_agent-5_max: 140
    apples_agent-5_mean: 79.52
    apples_agent-5_min: 41
    cleaning_beam_agent-0_max: 465
    cleaning_beam_agent-0_mean: 362.32
    cleaning_beam_agent-0_min: 245
    cleaning_beam_agent-1_max: 610
    cleaning_beam_agent-1_mean: 284.85
    cleaning_beam_agent-1_min: 117
    cleaning_beam_agent-2_max: 579
    cleaning_beam_agent-2_mean: 415.54
    cleaning_beam_agent-2_min: 163
    cleaning_beam_agent-3_max: 59
    cleaning_beam_agent-3_mean: 14.56
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 607
    cleaning_beam_agent-4_mean: 500.46
    cleaning_beam_agent-4_min: 309
    cleaning_beam_agent-5_max: 96
    cleaning_beam_agent-5_mean: 17.81
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-37-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1275.0000000000105
  episode_reward_mean: 1069.739999999995
  episode_reward_min: 449.00000000000597
  episodes_this_iter: 96
  episodes_total: 31680
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20492.304
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1067306995391846
        entropy_coeff: 0.0017600000137463212
        kl: 0.002095721662044525
        model: {}
        policy_loss: -0.0031747426837682724
        total_loss: -0.0029379846528172493
        vf_explained_var: 0.03448699414730072
        vf_loss: 21.846040725708008
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0903772115707397
        entropy_coeff: 0.0017600000137463212
        kl: 0.001286723418161273
        model: {}
        policy_loss: -0.004146579187363386
        total_loss: -0.003347681602463126
        vf_explained_var: -0.1315901279449463
        vf_loss: 27.179624557495117
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9441957473754883
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021790843456983566
        model: {}
        policy_loss: -0.0027660191990435123
        total_loss: -0.0022234907373785973
        vf_explained_var: 0.02870909869670868
        vf_loss: 22.04315185546875
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4039589464664459
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012967069633305073
        model: {}
        policy_loss: -0.0022798452991992235
        total_loss: -0.0009214119054377079
        vf_explained_var: 0.08678409457206726
        vf_loss: 20.69399642944336
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8480821251869202
        entropy_coeff: 0.0017600000137463212
        kl: 0.002141188131645322
        model: {}
        policy_loss: -0.0038938531652092934
        total_loss: -0.0031175781041383743
        vf_explained_var: 0.011227428913116455
        vf_loss: 22.688983917236328
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5044934749603271
        entropy_coeff: 0.0017600000137463212
        kl: 0.00037325124139897525
        model: {}
        policy_loss: -0.0019331658259034157
        total_loss: -0.000558849424123764
        vf_explained_var: 0.027459144592285156
        vf_loss: 22.622230529785156
    load_time_ms: 34732.64
    num_steps_sampled: 31680000
    num_steps_trained: 31680000
    sample_time_ms: 100157.427
    update_time_ms: 22.034
  iterations_since_restore: 270
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.745188284518825
    ram_util_percent: 18.925523012552304
  pid: 24061
  policy_reward_max:
    agent-0: 212.4999999999999
    agent-1: 212.4999999999999
    agent-2: 212.4999999999999
    agent-3: 212.4999999999999
    agent-4: 212.4999999999999
    agent-5: 212.4999999999999
  policy_reward_mean:
    agent-0: 178.28999999999985
    agent-1: 178.28999999999985
    agent-2: 178.28999999999985
    agent-3: 178.28999999999985
    agent-4: 178.28999999999985
    agent-5: 178.28999999999985
  policy_reward_min:
    agent-0: 74.8333333333333
    agent-1: 74.8333333333333
    agent-2: 74.8333333333333
    agent-3: 74.8333333333333
    agent-4: 74.8333333333333
    agent-5: 74.8333333333333
  sampler_perf:
    mean_env_wait_ms: 24.785232966753792
    mean_inference_ms: 12.277096163397783
    mean_processing_ms: 50.91492835563657
  time_since_restore: 34993.98998403549
  time_this_iter_s: 167.30272006988525
  time_total_s: 44120.001797914505
  timestamp: 1637059044
  timesteps_since_restore: 25920000
  timesteps_this_iter: 96000
  timesteps_total: 31680000
  training_iteration: 330
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    330 |            44120 | 31680000 |  1069.74 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 53
    apples_agent-0_mean: 1.39
    apples_agent-0_min: 0
    apples_agent-1_max: 166
    apples_agent-1_mean: 38.74
    apples_agent-1_min: 0
    apples_agent-2_max: 58
    apples_agent-2_mean: 1.6
    apples_agent-2_min: 0
    apples_agent-3_max: 216
    apples_agent-3_mean: 125.18
    apples_agent-3_min: 36
    apples_agent-4_max: 20
    apples_agent-4_mean: 0.59
    apples_agent-4_min: 0
    apples_agent-5_max: 128
    apples_agent-5_mean: 80.08
    apples_agent-5_min: 50
    cleaning_beam_agent-0_max: 489
    cleaning_beam_agent-0_mean: 371.07
    cleaning_beam_agent-0_min: 251
    cleaning_beam_agent-1_max: 497
    cleaning_beam_agent-1_mean: 290.1
    cleaning_beam_agent-1_min: 106
    cleaning_beam_agent-2_max: 596
    cleaning_beam_agent-2_mean: 439.66
    cleaning_beam_agent-2_min: 287
    cleaning_beam_agent-3_max: 58
    cleaning_beam_agent-3_mean: 12.39
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 603
    cleaning_beam_agent-4_mean: 492.45
    cleaning_beam_agent-4_min: 324
    cleaning_beam_agent-5_max: 75
    cleaning_beam_agent-5_mean: 15.69
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-39-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1274.0000000000095
  episode_reward_mean: 1074.609999999993
  episode_reward_min: 719.0000000000043
  episodes_this_iter: 96
  episodes_total: 31776
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20442.545
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.110231637954712
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022341390140354633
        model: {}
        policy_loss: -0.003413853235542774
        total_loss: -0.003144881222397089
        vf_explained_var: 0.007448673248291016
        vf_loss: 22.229766845703125
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.088173508644104
        entropy_coeff: 0.0017600000137463212
        kl: 0.001237621996551752
        model: {}
        policy_loss: -0.003928284626454115
        total_loss: -0.0032585011795163155
        vf_explained_var: -0.10137549042701721
        vf_loss: 25.849721908569336
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9307703971862793
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015766059514135122
        model: {}
        policy_loss: -0.0025251712650060654
        total_loss: -0.0019670138135552406
        vf_explained_var: 0.01840272545814514
        vf_loss: 21.963096618652344
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3972444534301758
        entropy_coeff: 0.0017600000137463212
        kl: 0.001161115476861596
        model: {}
        policy_loss: -0.001959696412086487
        total_loss: -0.0005424944683909416
        vf_explained_var: 0.058830440044403076
        vf_loss: 21.163482666015625
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8413270115852356
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014297539601102471
        model: {}
        policy_loss: -0.0035537434741854668
        total_loss: -0.002771420869976282
        vf_explained_var: 0.006972149014472961
        vf_loss: 22.630596160888672
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5017249584197998
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011430415324866772
        model: {}
        policy_loss: -0.002355337142944336
        total_loss: -0.0010447271633893251
        vf_explained_var: 0.04727862775325775
        vf_loss: 21.93648338317871
    load_time_ms: 34558.79
    num_steps_sampled: 31776000
    num_steps_trained: 31776000
    sample_time_ms: 100262.527
    update_time_ms: 22.302
  iterations_since_restore: 271
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.993577981651377
    ram_util_percent: 19.95596330275229
  pid: 24061
  policy_reward_max:
    agent-0: 212.3333333333335
    agent-1: 212.3333333333335
    agent-2: 212.3333333333335
    agent-3: 212.3333333333335
    agent-4: 212.3333333333335
    agent-5: 212.3333333333335
  policy_reward_mean:
    agent-0: 179.1016666666665
    agent-1: 179.1016666666665
    agent-2: 179.1016666666665
    agent-3: 179.1016666666665
    agent-4: 179.1016666666665
    agent-5: 179.1016666666665
  policy_reward_min:
    agent-0: 119.8333333333335
    agent-1: 119.8333333333335
    agent-2: 119.8333333333335
    agent-3: 119.8333333333335
    agent-4: 119.8333333333335
    agent-5: 119.8333333333335
  sampler_perf:
    mean_env_wait_ms: 24.795880058668175
    mean_inference_ms: 12.279598157351172
    mean_processing_ms: 50.93113283024286
  time_since_restore: 35146.89077782631
  time_this_iter_s: 152.90079379081726
  time_total_s: 44272.90259170532
  timestamp: 1637059197
  timesteps_since_restore: 26016000
  timesteps_this_iter: 96000
  timesteps_total: 31776000
  training_iteration: 331
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 36.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    331 |          44272.9 | 31776000 |  1074.61 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 55
    apples_agent-0_mean: 2.51
    apples_agent-0_min: 0
    apples_agent-1_max: 93
    apples_agent-1_mean: 31.0
    apples_agent-1_min: 0
    apples_agent-2_max: 62
    apples_agent-2_mean: 3.21
    apples_agent-2_min: 0
    apples_agent-3_max: 212
    apples_agent-3_mean: 125.74
    apples_agent-3_min: 36
    apples_agent-4_max: 54
    apples_agent-4_mean: 1.02
    apples_agent-4_min: 0
    apples_agent-5_max: 130
    apples_agent-5_mean: 78.06
    apples_agent-5_min: 24
    cleaning_beam_agent-0_max: 459
    cleaning_beam_agent-0_mean: 355.28
    cleaning_beam_agent-0_min: 208
    cleaning_beam_agent-1_max: 500
    cleaning_beam_agent-1_mean: 286.43
    cleaning_beam_agent-1_min: 127
    cleaning_beam_agent-2_max: 619
    cleaning_beam_agent-2_mean: 423.53
    cleaning_beam_agent-2_min: 250
    cleaning_beam_agent-3_max: 86
    cleaning_beam_agent-3_mean: 14.04
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 629
    cleaning_beam_agent-4_mean: 493.43
    cleaning_beam_agent-4_min: 368
    cleaning_beam_agent-5_max: 137
    cleaning_beam_agent-5_mean: 15.92
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-42-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1253.0000000000089
  episode_reward_mean: 1050.8999999999924
  episode_reward_min: 346.99999999999835
  episodes_this_iter: 96
  episodes_total: 31872
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20415.654
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.124509334564209
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013844096101820469
        model: {}
        policy_loss: -0.0029072007164359093
        total_loss: -0.002490165876224637
        vf_explained_var: 0.015325665473937988
        vf_loss: 23.961688995361328
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1083000898361206
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013818306615576148
        model: {}
        policy_loss: -0.004179744049906731
        total_loss: -0.003547107335180044
        vf_explained_var: -0.04374885559082031
        vf_loss: 25.832456588745117
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9265914559364319
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020913518965244293
        model: {}
        policy_loss: -0.003284094389528036
        total_loss: -0.0026655979454517365
        vf_explained_var: 0.07598166167736053
        vf_loss: 22.492996215820312
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4183008372783661
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013383319601416588
        model: {}
        policy_loss: -0.0023953942582011223
        total_loss: -0.000961806159466505
        vf_explained_var: 0.10859265923500061
        vf_loss: 21.698015213012695
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8564615249633789
        entropy_coeff: 0.0017600000137463212
        kl: 0.001419513369910419
        model: {}
        policy_loss: -0.003603538265451789
        total_loss: -0.0027838086243718863
        vf_explained_var: 0.04904651641845703
        vf_loss: 23.271018981933594
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5209306478500366
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008144197054207325
        model: {}
        policy_loss: -0.002360896673053503
        total_loss: -0.0009429743513464928
        vf_explained_var: 0.04804182052612305
        vf_loss: 23.347597122192383
    load_time_ms: 33282.299
    num_steps_sampled: 31872000
    num_steps_trained: 31872000
    sample_time_ms: 100269.03
    update_time_ms: 21.436
  iterations_since_restore: 272
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.37708333333333
    ram_util_percent: 19.783333333333335
  pid: 24061
  policy_reward_max:
    agent-0: 208.8333333333332
    agent-1: 208.8333333333332
    agent-2: 208.8333333333332
    agent-3: 208.8333333333332
    agent-4: 208.8333333333332
    agent-5: 208.8333333333332
  policy_reward_mean:
    agent-0: 175.1499999999999
    agent-1: 175.1499999999999
    agent-2: 175.1499999999999
    agent-3: 175.1499999999999
    agent-4: 175.1499999999999
    agent-5: 175.1499999999999
  policy_reward_min:
    agent-0: 57.833333333333336
    agent-1: 57.833333333333336
    agent-2: 57.833333333333336
    agent-3: 57.833333333333336
    agent-4: 57.833333333333336
    agent-5: 57.833333333333336
  sampler_perf:
    mean_env_wait_ms: 24.805281731828714
    mean_inference_ms: 12.28201345137068
    mean_processing_ms: 50.94574024665755
  time_since_restore: 35281.22277092934
  time_this_iter_s: 134.33199310302734
  time_total_s: 44407.23458480835
  timestamp: 1637059332
  timesteps_since_restore: 26112000
  timesteps_this_iter: 96000
  timesteps_total: 31872000
  training_iteration: 332
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 36.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    332 |          44407.2 | 31872000 |   1050.9 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 30
    apples_agent-0_mean: 1.56
    apples_agent-0_min: 0
    apples_agent-1_max: 106
    apples_agent-1_mean: 29.84
    apples_agent-1_min: 0
    apples_agent-2_max: 77
    apples_agent-2_mean: 2.25
    apples_agent-2_min: 0
    apples_agent-3_max: 324
    apples_agent-3_mean: 135.81
    apples_agent-3_min: 78
    apples_agent-4_max: 52
    apples_agent-4_mean: 1.15
    apples_agent-4_min: 0
    apples_agent-5_max: 205
    apples_agent-5_mean: 78.77
    apples_agent-5_min: 13
    cleaning_beam_agent-0_max: 422
    cleaning_beam_agent-0_mean: 351.16
    cleaning_beam_agent-0_min: 230
    cleaning_beam_agent-1_max: 500
    cleaning_beam_agent-1_mean: 285.57
    cleaning_beam_agent-1_min: 137
    cleaning_beam_agent-2_max: 626
    cleaning_beam_agent-2_mean: 452.69
    cleaning_beam_agent-2_min: 165
    cleaning_beam_agent-3_max: 89
    cleaning_beam_agent-3_mean: 12.49
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 602
    cleaning_beam_agent-4_mean: 493.46
    cleaning_beam_agent-4_min: 399
    cleaning_beam_agent-5_max: 117
    cleaning_beam_agent-5_mean: 16.73
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 3
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-44-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1273.9999999999914
  episode_reward_mean: 1085.3199999999947
  episode_reward_min: 690.0000000000039
  episodes_this_iter: 96
  episodes_total: 31968
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20384.267
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1377239227294922
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013678697869181633
        model: {}
        policy_loss: -0.0028416134882718325
        total_loss: -0.002627577167004347
        vf_explained_var: -0.010264456272125244
        vf_loss: 22.164295196533203
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1022837162017822
        entropy_coeff: 0.0017600000137463212
        kl: 0.002006789669394493
        model: {}
        policy_loss: -0.004177016206085682
        total_loss: -0.0036172522231936455
        vf_explained_var: -0.07594159245491028
        vf_loss: 24.99784278869629
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9053601026535034
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015386065933853388
        model: {}
        policy_loss: -0.0027898538392037153
        total_loss: -0.002247240860015154
        vf_explained_var: 0.02370394766330719
        vf_loss: 21.360477447509766
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3928501307964325
        entropy_coeff: 0.0017600000137463212
        kl: 0.000818261003587395
        model: {}
        policy_loss: -0.001990587217733264
        total_loss: -0.0005557064432650805
        vf_explained_var: 0.03324681520462036
        vf_loss: 21.262950897216797
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.850420355796814
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014219926670193672
        model: {}
        policy_loss: -0.0036066831089556217
        total_loss: -0.002831143792718649
        vf_explained_var: -0.01292872428894043
        vf_loss: 22.722801208496094
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49383658170700073
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006748602027073503
        model: {}
        policy_loss: -0.0018998760497197509
        total_loss: -0.0005968182813376188
        vf_explained_var: 0.039051175117492676
        vf_loss: 21.722087860107422
    load_time_ms: 33102.079
    num_steps_sampled: 31968000
    num_steps_trained: 31968000
    sample_time_ms: 100225.261
    update_time_ms: 21.908
  iterations_since_restore: 273
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.818848167539265
    ram_util_percent: 19.890575916230368
  pid: 24061
  policy_reward_max:
    agent-0: 212.33333333333346
    agent-1: 212.33333333333346
    agent-2: 212.33333333333346
    agent-3: 212.33333333333346
    agent-4: 212.33333333333346
    agent-5: 212.33333333333346
  policy_reward_mean:
    agent-0: 180.88666666666649
    agent-1: 180.88666666666649
    agent-2: 180.88666666666649
    agent-3: 180.88666666666649
    agent-4: 180.88666666666649
    agent-5: 180.88666666666649
  policy_reward_min:
    agent-0: 115.0000000000006
    agent-1: 115.0000000000006
    agent-2: 115.0000000000006
    agent-3: 115.0000000000006
    agent-4: 115.0000000000006
    agent-5: 115.0000000000006
  sampler_perf:
    mean_env_wait_ms: 24.815521376202415
    mean_inference_ms: 12.284507967543584
    mean_processing_ms: 50.96063780397961
  time_since_restore: 35415.41178250313
  time_this_iter_s: 134.1890115737915
  time_total_s: 44541.42359638214
  timestamp: 1637059466
  timesteps_since_restore: 26208000
  timesteps_this_iter: 96000
  timesteps_total: 31968000
  training_iteration: 333
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 36.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    333 |          44541.4 | 31968000 |  1085.32 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 53
    apples_agent-0_mean: 2.59
    apples_agent-0_min: 0
    apples_agent-1_max: 123
    apples_agent-1_mean: 34.87
    apples_agent-1_min: 0
    apples_agent-2_max: 90
    apples_agent-2_mean: 1.97
    apples_agent-2_min: 0
    apples_agent-3_max: 260
    apples_agent-3_mean: 132.86
    apples_agent-3_min: 71
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 237
    apples_agent-5_mean: 85.69
    apples_agent-5_min: 54
    cleaning_beam_agent-0_max: 457
    cleaning_beam_agent-0_mean: 348.99
    cleaning_beam_agent-0_min: 239
    cleaning_beam_agent-1_max: 600
    cleaning_beam_agent-1_mean: 289.74
    cleaning_beam_agent-1_min: 139
    cleaning_beam_agent-2_max: 668
    cleaning_beam_agent-2_mean: 460.37
    cleaning_beam_agent-2_min: 313
    cleaning_beam_agent-3_max: 41
    cleaning_beam_agent-3_mean: 12.38
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 588
    cleaning_beam_agent-4_mean: 487.76
    cleaning_beam_agent-4_min: 372
    cleaning_beam_agent-5_max: 61
    cleaning_beam_agent-5_mean: 12.93
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-46-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1248.0000000000034
  episode_reward_mean: 1092.8099999999943
  episode_reward_min: 625.0000000000016
  episodes_this_iter: 96
  episodes_total: 32064
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20330.634
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1329673528671265
        entropy_coeff: 0.0017600000137463212
        kl: 0.002088698325678706
        model: {}
        policy_loss: -0.0031261874828487635
        total_loss: -0.0029631180223077536
        vf_explained_var: -0.00833660364151001
        vf_loss: 21.57094955444336
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0832512378692627
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011743613285943866
        model: {}
        policy_loss: -0.003802768886089325
        total_loss: -0.003315872512757778
        vf_explained_var: -0.06382066011428833
        vf_loss: 23.93427085876465
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9185813665390015
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015874202363193035
        model: {}
        policy_loss: -0.002710666973143816
        total_loss: -0.00222234008833766
        vf_explained_var: 0.013298392295837402
        vf_loss: 21.050283432006836
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3855886161327362
        entropy_coeff: 0.0017600000137463212
        kl: 0.001052309526130557
        model: {}
        policy_loss: -0.0024363063275814056
        total_loss: -0.000997674884274602
        vf_explained_var: 0.011398032307624817
        vf_loss: 21.172658920288086
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8616610765457153
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016957195475697517
        model: {}
        policy_loss: -0.003702924121171236
        total_loss: -0.003015912137925625
        vf_explained_var: -0.008023962378501892
        vf_loss: 22.035327911376953
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49811214208602905
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007726483745500445
        model: {}
        policy_loss: -0.002234006766229868
        total_loss: -0.0010174134513363242
        vf_explained_var: 0.05235143005847931
        vf_loss: 20.932723999023438
    load_time_ms: 30901.082
    num_steps_sampled: 32064000
    num_steps_trained: 32064000
    sample_time_ms: 100042.57
    update_time_ms: 22.021
  iterations_since_restore: 274
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.13421052631579
    ram_util_percent: 18.876315789473686
  pid: 24061
  policy_reward_max:
    agent-0: 207.9999999999999
    agent-1: 207.9999999999999
    agent-2: 207.9999999999999
    agent-3: 207.9999999999999
    agent-4: 207.9999999999999
    agent-5: 207.9999999999999
  policy_reward_mean:
    agent-0: 182.13499999999985
    agent-1: 182.13499999999985
    agent-2: 182.13499999999985
    agent-3: 182.13499999999985
    agent-4: 182.13499999999985
    agent-5: 182.13499999999985
  policy_reward_min:
    agent-0: 104.16666666666724
    agent-1: 104.16666666666724
    agent-2: 104.16666666666724
    agent-3: 104.16666666666724
    agent-4: 104.16666666666724
    agent-5: 104.16666666666724
  sampler_perf:
    mean_env_wait_ms: 24.82509458978027
    mean_inference_ms: 12.286983901132965
    mean_processing_ms: 50.973924028092206
  time_since_restore: 35548.65644669533
  time_this_iter_s: 133.2446641921997
  time_total_s: 44674.66826057434
  timestamp: 1637059600
  timesteps_since_restore: 26304000
  timesteps_this_iter: 96000
  timesteps_total: 32064000
  training_iteration: 334
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 36.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    334 |          44674.7 | 32064000 |  1092.81 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 68
    apples_agent-0_mean: 4.71
    apples_agent-0_min: 0
    apples_agent-1_max: 139
    apples_agent-1_mean: 35.44
    apples_agent-1_min: 0
    apples_agent-2_max: 68
    apples_agent-2_mean: 2.18
    apples_agent-2_min: 0
    apples_agent-3_max: 214
    apples_agent-3_mean: 129.83
    apples_agent-3_min: 62
    apples_agent-4_max: 1
    apples_agent-4_mean: 0.01
    apples_agent-4_min: 0
    apples_agent-5_max: 187
    apples_agent-5_mean: 81.09
    apples_agent-5_min: 14
    cleaning_beam_agent-0_max: 461
    cleaning_beam_agent-0_mean: 337.63
    cleaning_beam_agent-0_min: 203
    cleaning_beam_agent-1_max: 628
    cleaning_beam_agent-1_mean: 308.75
    cleaning_beam_agent-1_min: 121
    cleaning_beam_agent-2_max: 618
    cleaning_beam_agent-2_mean: 460.44
    cleaning_beam_agent-2_min: 250
    cleaning_beam_agent-3_max: 51
    cleaning_beam_agent-3_mean: 13.66
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 571
    cleaning_beam_agent-4_mean: 484.42
    cleaning_beam_agent-4_min: 386
    cleaning_beam_agent-5_max: 112
    cleaning_beam_agent-5_mean: 18.96
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.1
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-49-10
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1239.0000000000018
  episode_reward_mean: 1065.7999999999925
  episode_reward_min: 561.0000000000026
  episodes_this_iter: 96
  episodes_total: 32160
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20272.101
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1163105964660645
        entropy_coeff: 0.0017600000137463212
        kl: 0.001286687096580863
        model: {}
        policy_loss: -0.003192529082298279
        total_loss: -0.0029214229434728622
        vf_explained_var: 0.019806936383247375
        vf_loss: 22.358135223388672
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0810527801513672
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014841353986412287
        model: {}
        policy_loss: -0.004047670401632786
        total_loss: -0.0034899685997515917
        vf_explained_var: -0.058861225843429565
        vf_loss: 24.603553771972656
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9072795510292053
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020291730761528015
        model: {}
        policy_loss: -0.002862820401787758
        total_loss: -0.0023405265528708696
        vf_explained_var: 0.07255709171295166
        vf_loss: 21.191082000732422
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40370726585388184
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009296011994592845
        model: {}
        policy_loss: -0.002095947042107582
        total_loss: -0.0007238956168293953
        vf_explained_var: 0.08813498914241791
        vf_loss: 20.825767517089844
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8623184561729431
        entropy_coeff: 0.0017600000137463212
        kl: 0.001612310647033155
        model: {}
        policy_loss: -0.003703036345541477
        total_loss: -0.002874051220715046
        vf_explained_var: -0.021167457103729248
        vf_loss: 23.466678619384766
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5278817415237427
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010414617136120796
        model: {}
        policy_loss: -0.0025144582614302635
        total_loss: -0.001302120741456747
        vf_explained_var: 0.06701864302158356
        vf_loss: 21.4140625
    load_time_ms: 28411.51
    num_steps_sampled: 32160000
    num_steps_trained: 32160000
    sample_time_ms: 99799.873
    update_time_ms: 21.671
  iterations_since_restore: 275
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.147196261682243
    ram_util_percent: 19.526635514018686
  pid: 24061
  policy_reward_max:
    agent-0: 206.49999999999983
    agent-1: 206.49999999999983
    agent-2: 206.49999999999983
    agent-3: 206.49999999999983
    agent-4: 206.49999999999983
    agent-5: 206.49999999999983
  policy_reward_mean:
    agent-0: 177.6333333333332
    agent-1: 177.6333333333332
    agent-2: 177.6333333333332
    agent-3: 177.6333333333332
    agent-4: 177.6333333333332
    agent-5: 177.6333333333332
  policy_reward_min:
    agent-0: 93.4999999999999
    agent-1: 93.4999999999999
    agent-2: 93.4999999999999
    agent-3: 93.4999999999999
    agent-4: 93.4999999999999
    agent-5: 93.4999999999999
  sampler_perf:
    mean_env_wait_ms: 24.835414632510023
    mean_inference_ms: 12.289459600114698
    mean_processing_ms: 50.98864619999344
  time_since_restore: 35698.503786325455
  time_this_iter_s: 149.84733963012695
  time_total_s: 44824.51560020447
  timestamp: 1637059750
  timesteps_since_restore: 26400000
  timesteps_this_iter: 96000
  timesteps_total: 32160000
  training_iteration: 335
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 33.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    335 |          44824.5 | 32160000 |   1065.8 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 55
    apples_agent-0_mean: 2.26
    apples_agent-0_min: 0
    apples_agent-1_max: 122
    apples_agent-1_mean: 34.49
    apples_agent-1_min: 0
    apples_agent-2_max: 41
    apples_agent-2_mean: 1.13
    apples_agent-2_min: 0
    apples_agent-3_max: 361
    apples_agent-3_mean: 138.08
    apples_agent-3_min: 75
    apples_agent-4_max: 22
    apples_agent-4_mean: 0.51
    apples_agent-4_min: 0
    apples_agent-5_max: 310
    apples_agent-5_mean: 81.81
    apples_agent-5_min: 42
    cleaning_beam_agent-0_max: 444
    cleaning_beam_agent-0_mean: 342.06
    cleaning_beam_agent-0_min: 190
    cleaning_beam_agent-1_max: 541
    cleaning_beam_agent-1_mean: 283.23
    cleaning_beam_agent-1_min: 124
    cleaning_beam_agent-2_max: 676
    cleaning_beam_agent-2_mean: 475.77
    cleaning_beam_agent-2_min: 260
    cleaning_beam_agent-3_max: 64
    cleaning_beam_agent-3_mean: 14.49
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 561
    cleaning_beam_agent-4_mean: 474.8
    cleaning_beam_agent-4_min: 376
    cleaning_beam_agent-5_max: 57
    cleaning_beam_agent-5_mean: 15.12
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-52-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1234.0000000000089
  episode_reward_mean: 1073.309999999993
  episode_reward_min: 793.9999999999858
  episodes_this_iter: 96
  episodes_total: 32256
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20286.219
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1121432781219482
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017700486350804567
        model: {}
        policy_loss: -0.0031201737001538277
        total_loss: -0.0028072569984942675
        vf_explained_var: 0.008147194981575012
        vf_loss: 22.702880859375
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0867055654525757
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013795086415484548
        model: {}
        policy_loss: -0.003965522162616253
        total_loss: -0.0032795690931379795
        vf_explained_var: -0.07913440465927124
        vf_loss: 25.985523223876953
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9224353432655334
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024213604629039764
        model: {}
        policy_loss: -0.0030836742371320724
        total_loss: -0.002424546517431736
        vf_explained_var: 0.0038093626499176025
        vf_loss: 22.826160430908203
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38501352071762085
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012498863507062197
        model: {}
        policy_loss: -0.002050482900813222
        total_loss: -0.0005563196027651429
        vf_explained_var: 0.05172248184680939
        vf_loss: 21.717880249023438
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8754354119300842
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018166964873671532
        model: {}
        policy_loss: -0.0037215612828731537
        total_loss: -0.0029149395413696766
        vf_explained_var: -0.007129579782485962
        vf_loss: 23.473896026611328
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.505412757396698
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007262179860845208
        model: {}
        policy_loss: -0.0020450949668884277
        total_loss: -0.0007155872881412506
        vf_explained_var: 0.04695144295692444
        vf_loss: 22.19033432006836
    load_time_ms: 30685.888
    num_steps_sampled: 32256000
    num_steps_trained: 32256000
    sample_time_ms: 99602.503
    update_time_ms: 22.963
  iterations_since_restore: 276
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.520634920634922
    ram_util_percent: 20.626984126984127
  pid: 24061
  policy_reward_max:
    agent-0: 205.66666666666632
    agent-1: 205.66666666666632
    agent-2: 205.66666666666632
    agent-3: 205.66666666666632
    agent-4: 205.66666666666632
    agent-5: 205.66666666666632
  policy_reward_mean:
    agent-0: 178.88499999999988
    agent-1: 178.88499999999988
    agent-2: 178.88499999999988
    agent-3: 178.88499999999988
    agent-4: 178.88499999999988
    agent-5: 178.88499999999988
  policy_reward_min:
    agent-0: 132.33333333333337
    agent-1: 132.33333333333337
    agent-2: 132.33333333333337
    agent-3: 132.33333333333337
    agent-4: 132.33333333333337
    agent-5: 132.33333333333337
  sampler_perf:
    mean_env_wait_ms: 24.84485659081775
    mean_inference_ms: 12.292206129819547
    mean_processing_ms: 51.004166249057114
  time_since_restore: 35875.23597764969
  time_this_iter_s: 176.732191324234
  time_total_s: 45001.2477915287
  timestamp: 1637059926
  timesteps_since_restore: 26496000
  timesteps_this_iter: 96000
  timesteps_total: 32256000
  training_iteration: 336
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 37.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    336 |          45001.2 | 32256000 |  1073.31 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 67
    apples_agent-0_mean: 4.71
    apples_agent-0_min: 0
    apples_agent-1_max: 172
    apples_agent-1_mean: 37.32
    apples_agent-1_min: 0
    apples_agent-2_max: 156
    apples_agent-2_mean: 2.88
    apples_agent-2_min: 0
    apples_agent-3_max: 328
    apples_agent-3_mean: 126.13
    apples_agent-3_min: 60
    apples_agent-4_max: 31
    apples_agent-4_mean: 0.89
    apples_agent-4_min: 0
    apples_agent-5_max: 292
    apples_agent-5_mean: 80.04
    apples_agent-5_min: 44
    cleaning_beam_agent-0_max: 436
    cleaning_beam_agent-0_mean: 339.3
    cleaning_beam_agent-0_min: 185
    cleaning_beam_agent-1_max: 585
    cleaning_beam_agent-1_mean: 287.97
    cleaning_beam_agent-1_min: 127
    cleaning_beam_agent-2_max: 676
    cleaning_beam_agent-2_mean: 457.21
    cleaning_beam_agent-2_min: 147
    cleaning_beam_agent-3_max: 97
    cleaning_beam_agent-3_mean: 14.68
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 579
    cleaning_beam_agent-4_mean: 465.9
    cleaning_beam_agent-4_min: 287
    cleaning_beam_agent-5_max: 56
    cleaning_beam_agent-5_mean: 16.56
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-54-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1273.0000000000014
  episode_reward_mean: 1070.609999999992
  episode_reward_min: 494.99999999999886
  episodes_this_iter: 96
  episodes_total: 32352
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20278.496
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1328657865524292
        entropy_coeff: 0.0017600000137463212
        kl: 0.001601901836693287
        model: {}
        policy_loss: -0.0031257488299161196
        total_loss: -0.002672513946890831
        vf_explained_var: 0.013383254408836365
        vf_loss: 24.470813751220703
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0692713260650635
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011777740437537432
        model: {}
        policy_loss: -0.0038140006363391876
        total_loss: -0.0030643646605312824
        vf_explained_var: -0.03414759039878845
        vf_loss: 26.315563201904297
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9219732880592346
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019125312101095915
        model: {}
        policy_loss: -0.002756903413683176
        total_loss: -0.002064941916614771
        vf_explained_var: 0.0679314136505127
        vf_loss: 23.14632797241211
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.392655611038208
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010752305388450623
        model: {}
        policy_loss: -0.0020539816468954086
        total_loss: -0.0004784155171364546
        vf_explained_var: 0.0860118716955185
        vf_loss: 22.66641616821289
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8825066685676575
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014773316215723753
        model: {}
        policy_loss: -0.0038276081904768944
        total_loss: -0.0030853189527988434
        vf_explained_var: 0.08622580766677856
        vf_loss: 22.954980850219727
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5078116059303284
        entropy_coeff: 0.0017600000137463212
        kl: 0.000720566778909415
        model: {}
        policy_loss: -0.002318931743502617
        total_loss: -0.0009068823419511318
        vf_explained_var: 0.07715417444705963
        vf_loss: 23.057992935180664
    load_time_ms: 30954.127
    num_steps_sampled: 32352000
    num_steps_trained: 32352000
    sample_time_ms: 99325.472
    update_time_ms: 23.699
  iterations_since_restore: 277
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.973232323232327
    ram_util_percent: 19.564646464646465
  pid: 24061
  policy_reward_max:
    agent-0: 212.16666666666646
    agent-1: 212.16666666666646
    agent-2: 212.16666666666646
    agent-3: 212.16666666666646
    agent-4: 212.16666666666646
    agent-5: 212.16666666666646
  policy_reward_mean:
    agent-0: 178.4349999999999
    agent-1: 178.4349999999999
    agent-2: 178.4349999999999
    agent-3: 178.4349999999999
    agent-4: 178.4349999999999
    agent-5: 178.4349999999999
  policy_reward_min:
    agent-0: 82.49999999999991
    agent-1: 82.49999999999991
    agent-2: 82.49999999999991
    agent-3: 82.49999999999991
    agent-4: 82.49999999999991
    agent-5: 82.49999999999991
  sampler_perf:
    mean_env_wait_ms: 24.853766464896623
    mean_inference_ms: 12.294735725715304
    mean_processing_ms: 51.017530185435234
  time_since_restore: 36014.328639507294
  time_this_iter_s: 139.09266185760498
  time_total_s: 45140.34045338631
  timestamp: 1637060066
  timesteps_since_restore: 26592000
  timesteps_this_iter: 96000
  timesteps_total: 32352000
  training_iteration: 337
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 35.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    337 |          45140.3 | 32352000 |  1070.61 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 105
    apples_agent-0_mean: 5.16
    apples_agent-0_min: 0
    apples_agent-1_max: 89
    apples_agent-1_mean: 31.58
    apples_agent-1_min: 0
    apples_agent-2_max: 145
    apples_agent-2_mean: 5.6
    apples_agent-2_min: 0
    apples_agent-3_max: 197
    apples_agent-3_mean: 133.75
    apples_agent-3_min: 88
    apples_agent-4_max: 24
    apples_agent-4_mean: 0.54
    apples_agent-4_min: 0
    apples_agent-5_max: 130
    apples_agent-5_mean: 78.4
    apples_agent-5_min: 32
    cleaning_beam_agent-0_max: 444
    cleaning_beam_agent-0_mean: 343.31
    cleaning_beam_agent-0_min: 221
    cleaning_beam_agent-1_max: 625
    cleaning_beam_agent-1_mean: 295.84
    cleaning_beam_agent-1_min: 118
    cleaning_beam_agent-2_max: 611
    cleaning_beam_agent-2_mean: 444.95
    cleaning_beam_agent-2_min: 150
    cleaning_beam_agent-3_max: 36
    cleaning_beam_agent-3_mean: 13.96
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 595
    cleaning_beam_agent-4_mean: 466.14
    cleaning_beam_agent-4_min: 358
    cleaning_beam_agent-5_max: 49
    cleaning_beam_agent-5_mean: 14.58
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-56-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1214.9999999999936
  episode_reward_mean: 1078.449999999992
  episode_reward_min: 716.9999999999822
  episodes_this_iter: 96
  episodes_total: 32448
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20222.533
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1161905527114868
        entropy_coeff: 0.0017600000137463212
        kl: 0.001291208784095943
        model: {}
        policy_loss: -0.0031053246930241585
        total_loss: -0.00289069302380085
        vf_explained_var: 0.0015957355499267578
        vf_loss: 21.79126739501953
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0704903602600098
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011084615252912045
        model: {}
        policy_loss: -0.00393383763730526
        total_loss: -0.003295842558145523
        vf_explained_var: -0.12133598327636719
        vf_loss: 25.220592498779297
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9258471131324768
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017196476692333817
        model: {}
        policy_loss: -0.002746337093412876
        total_loss: -0.0022674580104649067
        vf_explained_var: 0.0341658741235733
        vf_loss: 21.083690643310547
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3946422338485718
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009530957322567701
        model: {}
        policy_loss: -0.002066589891910553
        total_loss: -0.0006866937037557364
        vf_explained_var: 0.046918317675590515
        vf_loss: 20.74468421936035
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8721798062324524
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014136256650090218
        model: {}
        policy_loss: -0.0036367587745189667
        total_loss: -0.0029019047506153584
        vf_explained_var: -0.01903402805328369
        vf_loss: 22.698890686035156
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5054415464401245
        entropy_coeff: 0.0017600000137463212
        kl: 0.00090135628124699
        model: {}
        policy_loss: -0.00233154883608222
        total_loss: -0.0011897450312972069
        vf_explained_var: 0.07658973336219788
        vf_loss: 20.313819885253906
    load_time_ms: 28954.678
    num_steps_sampled: 32448000
    num_steps_trained: 32448000
    sample_time_ms: 99274.591
    update_time_ms: 23.527
  iterations_since_restore: 278
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.591099476439798
    ram_util_percent: 20.69581151832461
  pid: 24061
  policy_reward_max:
    agent-0: 202.5
    agent-1: 202.5
    agent-2: 202.5
    agent-3: 202.5
    agent-4: 202.5
    agent-5: 202.5
  policy_reward_mean:
    agent-0: 179.7416666666665
    agent-1: 179.7416666666665
    agent-2: 179.7416666666665
    agent-3: 179.7416666666665
    agent-4: 179.7416666666665
    agent-5: 179.7416666666665
  policy_reward_min:
    agent-0: 119.50000000000033
    agent-1: 119.50000000000033
    agent-2: 119.50000000000033
    agent-3: 119.50000000000033
    agent-4: 119.50000000000033
    agent-5: 119.50000000000033
  sampler_perf:
    mean_env_wait_ms: 24.862866230687473
    mean_inference_ms: 12.29748687235708
    mean_processing_ms: 51.032385285657845
  time_since_restore: 36148.485661029816
  time_this_iter_s: 134.15702152252197
  time_total_s: 45274.49747490883
  timestamp: 1637060200
  timesteps_since_restore: 26688000
  timesteps_this_iter: 96000
  timesteps_total: 32448000
  training_iteration: 338
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 38.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    338 |          45274.5 | 32448000 |  1078.45 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 59
    apples_agent-0_mean: 3.02
    apples_agent-0_min: 0
    apples_agent-1_max: 121
    apples_agent-1_mean: 35.98
    apples_agent-1_min: 0
    apples_agent-2_max: 142
    apples_agent-2_mean: 3.88
    apples_agent-2_min: 0
    apples_agent-3_max: 210
    apples_agent-3_mean: 129.91
    apples_agent-3_min: 59
    apples_agent-4_max: 76
    apples_agent-4_mean: 1.21
    apples_agent-4_min: 0
    apples_agent-5_max: 200
    apples_agent-5_mean: 79.92
    apples_agent-5_min: 42
    cleaning_beam_agent-0_max: 452
    cleaning_beam_agent-0_mean: 351.97
    cleaning_beam_agent-0_min: 177
    cleaning_beam_agent-1_max: 492
    cleaning_beam_agent-1_mean: 288.07
    cleaning_beam_agent-1_min: 88
    cleaning_beam_agent-2_max: 629
    cleaning_beam_agent-2_mean: 450.4
    cleaning_beam_agent-2_min: 162
    cleaning_beam_agent-3_max: 73
    cleaning_beam_agent-3_mean: 13.74
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 570
    cleaning_beam_agent-4_mean: 465.66
    cleaning_beam_agent-4_min: 311
    cleaning_beam_agent-5_max: 131
    cleaning_beam_agent-5_mean: 16.78
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-58-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1256.0000000000061
  episode_reward_mean: 1081.489999999993
  episode_reward_min: 719.0000000000024
  episodes_this_iter: 96
  episodes_total: 32544
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20255.912
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.126580834388733
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022608996368944645
        model: {}
        policy_loss: -0.0032763092312961817
        total_loss: -0.0029454329051077366
        vf_explained_var: 0.015101924538612366
        vf_loss: 23.136587142944336
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0925664901733398
        entropy_coeff: 0.0017600000137463212
        kl: 0.001262123929336667
        model: {}
        policy_loss: -0.004129086155444384
        total_loss: -0.003448456758633256
        vf_explained_var: -0.06566447019577026
        vf_loss: 26.035459518432617
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9250084161758423
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017810235731303692
        model: {}
        policy_loss: -0.002836182713508606
        total_loss: -0.002147439168766141
        vf_explained_var: 0.011951282620429993
        vf_loss: 23.167577743530273
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3840677738189697
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008534200023859739
        model: {}
        policy_loss: -0.002016777638345957
        total_loss: -0.0005062730051577091
        vf_explained_var: 0.06720122694969177
        vf_loss: 21.864646911621094
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8762788772583008
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014788154512643814
        model: {}
        policy_loss: -0.0038970615714788437
        total_loss: -0.0030588763765990734
        vf_explained_var: 0.0020265430212020874
        vf_loss: 23.804363250732422
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.511397123336792
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012215592432767153
        model: {}
        policy_loss: -0.0022587848361581564
        total_loss: -0.000873154029250145
        vf_explained_var: 0.041678041219711304
        vf_loss: 22.85692024230957
    load_time_ms: 25939.143
    num_steps_sampled: 32544000
    num_steps_trained: 32544000
    sample_time_ms: 99342.962
    update_time_ms: 23.723
  iterations_since_restore: 279
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.434895833333332
    ram_util_percent: 20.886979166666666
  pid: 24061
  policy_reward_max:
    agent-0: 209.3333333333333
    agent-1: 209.3333333333333
    agent-2: 209.3333333333333
    agent-3: 209.3333333333333
    agent-4: 209.3333333333333
    agent-5: 209.3333333333333
  policy_reward_mean:
    agent-0: 180.24833333333322
    agent-1: 180.24833333333322
    agent-2: 180.24833333333322
    agent-3: 180.24833333333322
    agent-4: 180.24833333333322
    agent-5: 180.24833333333322
  policy_reward_min:
    agent-0: 119.83333333333331
    agent-1: 119.83333333333331
    agent-2: 119.83333333333331
    agent-3: 119.83333333333331
    agent-4: 119.83333333333331
    agent-5: 119.83333333333331
  sampler_perf:
    mean_env_wait_ms: 24.871956467818936
    mean_inference_ms: 12.300133005358093
    mean_processing_ms: 51.04638488962153
  time_since_restore: 36283.50031352043
  time_this_iter_s: 135.01465249061584
  time_total_s: 45409.512127399445
  timestamp: 1637060335
  timesteps_since_restore: 26784000
  timesteps_this_iter: 96000
  timesteps_total: 32544000
  training_iteration: 339
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 38.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    339 |          45409.5 | 32544000 |  1081.49 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 44
    apples_agent-0_mean: 3.43
    apples_agent-0_min: 0
    apples_agent-1_max: 292
    apples_agent-1_mean: 42.6
    apples_agent-1_min: 0
    apples_agent-2_max: 60
    apples_agent-2_mean: 2.95
    apples_agent-2_min: 0
    apples_agent-3_max: 260
    apples_agent-3_mean: 126.9
    apples_agent-3_min: 59
    apples_agent-4_max: 76
    apples_agent-4_mean: 1.27
    apples_agent-4_min: 0
    apples_agent-5_max: 120
    apples_agent-5_mean: 79.74
    apples_agent-5_min: 49
    cleaning_beam_agent-0_max: 457
    cleaning_beam_agent-0_mean: 355.6
    cleaning_beam_agent-0_min: 252
    cleaning_beam_agent-1_max: 516
    cleaning_beam_agent-1_mean: 268.24
    cleaning_beam_agent-1_min: 83
    cleaning_beam_agent-2_max: 610
    cleaning_beam_agent-2_mean: 447.48
    cleaning_beam_agent-2_min: 155
    cleaning_beam_agent-3_max: 90
    cleaning_beam_agent-3_mean: 15.02
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 558
    cleaning_beam_agent-4_mean: 458.22
    cleaning_beam_agent-4_min: 311
    cleaning_beam_agent-5_max: 61
    cleaning_beam_agent-5_mean: 15.27
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-01-30
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1262.9999999999984
  episode_reward_mean: 1064.8299999999917
  episode_reward_min: 498.00000000001023
  episodes_this_iter: 96
  episodes_total: 32640
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20258.608
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1216593980789185
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017676647985354066
        model: {}
        policy_loss: -0.003283151425421238
        total_loss: -0.0029446296393871307
        vf_explained_var: -0.003088906407356262
        vf_loss: 23.126428604125977
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0928218364715576
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010321683948859572
        model: {}
        policy_loss: -0.003926784731447697
        total_loss: -0.0032645438332110643
        vf_explained_var: -0.09620031714439392
        vf_loss: 25.8560791015625
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9340649843215942
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018129842355847359
        model: {}
        policy_loss: -0.003433023113757372
        total_loss: -0.0029358440078794956
        vf_explained_var: 0.07045577466487885
        vf_loss: 21.411312103271484
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39469102025032043
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007266935426741838
        model: {}
        policy_loss: -0.002080148085951805
        total_loss: -0.0006213309243321419
        vf_explained_var: 0.06616964936256409
        vf_loss: 21.53473663330078
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8821473121643066
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019882468041032553
        model: {}
        policy_loss: -0.004046515561640263
        total_loss: -0.003272358328104019
        vf_explained_var: -0.001347169280052185
        vf_loss: 23.267372131347656
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5178382992744446
        entropy_coeff: 0.0017600000137463212
        kl: 0.000717317161615938
        model: {}
        policy_loss: -0.00250595947727561
        total_loss: -0.0013071298599243164
        vf_explained_var: 0.08679081499576569
        vf_loss: 21.102251052856445
    load_time_ms: 24611.774
    num_steps_sampled: 32640000
    num_steps_trained: 32640000
    sample_time_ms: 99378.943
    update_time_ms: 25.201
  iterations_since_restore: 280
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.33727272727273
    ram_util_percent: 20.394090909090913
  pid: 24061
  policy_reward_max:
    agent-0: 210.5000000000001
    agent-1: 210.5000000000001
    agent-2: 210.5000000000001
    agent-3: 210.5000000000001
    agent-4: 210.5000000000001
    agent-5: 210.5000000000001
  policy_reward_mean:
    agent-0: 177.47166666666652
    agent-1: 177.47166666666652
    agent-2: 177.47166666666652
    agent-3: 177.47166666666652
    agent-4: 177.47166666666652
    agent-5: 177.47166666666652
  policy_reward_min:
    agent-0: 83.00000000000007
    agent-1: 83.00000000000007
    agent-2: 83.00000000000007
    agent-3: 83.00000000000007
    agent-4: 83.00000000000007
    agent-5: 83.00000000000007
  sampler_perf:
    mean_env_wait_ms: 24.88085274971264
    mean_inference_ms: 12.302563968479928
    mean_processing_ms: 51.061410883148085
  time_since_restore: 36437.82755422592
  time_this_iter_s: 154.3272407054901
  time_total_s: 45563.839368104935
  timestamp: 1637060490
  timesteps_since_restore: 26880000
  timesteps_this_iter: 96000
  timesteps_total: 32640000
  training_iteration: 340
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 38.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    340 |          45563.8 | 32640000 |  1064.83 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 46
    apples_agent-0_mean: 2.6
    apples_agent-0_min: 0
    apples_agent-1_max: 194
    apples_agent-1_mean: 36.89
    apples_agent-1_min: 0
    apples_agent-2_max: 45
    apples_agent-2_mean: 1.81
    apples_agent-2_min: 0
    apples_agent-3_max: 217
    apples_agent-3_mean: 120.61
    apples_agent-3_min: 57
    apples_agent-4_max: 32
    apples_agent-4_mean: 0.47
    apples_agent-4_min: 0
    apples_agent-5_max: 116
    apples_agent-5_mean: 79.78
    apples_agent-5_min: 36
    cleaning_beam_agent-0_max: 485
    cleaning_beam_agent-0_mean: 356.56
    cleaning_beam_agent-0_min: 181
    cleaning_beam_agent-1_max: 566
    cleaning_beam_agent-1_mean: 275.33
    cleaning_beam_agent-1_min: 84
    cleaning_beam_agent-2_max: 613
    cleaning_beam_agent-2_mean: 451.13
    cleaning_beam_agent-2_min: 155
    cleaning_beam_agent-3_max: 110
    cleaning_beam_agent-3_mean: 15.02
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 572
    cleaning_beam_agent-4_mean: 442.82
    cleaning_beam_agent-4_min: 345
    cleaning_beam_agent-5_max: 65
    cleaning_beam_agent-5_mean: 15.61
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-04-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1219.9999999999955
  episode_reward_mean: 1064.4399999999912
  episode_reward_min: 498.00000000001023
  episodes_this_iter: 96
  episodes_total: 32736
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20264.374
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1170859336853027
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023644627071917057
        model: {}
        policy_loss: -0.0033416207879781723
        total_loss: -0.003051276318728924
        vf_explained_var: -0.012306809425354004
        vf_loss: 22.564149856567383
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0808839797973633
        entropy_coeff: 0.0017600000137463212
        kl: 0.001400717068463564
        model: {}
        policy_loss: -0.0039896173402667046
        total_loss: -0.0034400252625346184
        vf_explained_var: -0.07870545983314514
        vf_loss: 24.519489288330078
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.92765873670578
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014547102618962526
        model: {}
        policy_loss: -0.0029539684765040874
        total_loss: -0.002455434761941433
        vf_explained_var: 0.04380024969577789
        vf_loss: 21.312137603759766
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3875736892223358
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011436957865953445
        model: {}
        policy_loss: -0.0023095495998859406
        total_loss: -0.0009037340059876442
        vf_explained_var: 0.06321002542972565
        vf_loss: 20.879438400268555
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8799920082092285
        entropy_coeff: 0.0017600000137463212
        kl: 0.001603010925464332
        model: {}
        policy_loss: -0.003721395507454872
        total_loss: -0.0029989266768097878
        vf_explained_var: -0.010636746883392334
        vf_loss: 22.712568283081055
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5285988450050354
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009052228997461498
        model: {}
        policy_loss: -0.0025977957993745804
        total_loss: -0.0013713592197746038
        vf_explained_var: 0.041316330432891846
        vf_loss: 21.567699432373047
    load_time_ms: 25331.306
    num_steps_sampled: 32736000
    num_steps_trained: 32736000
    sample_time_ms: 99239.58
    update_time_ms: 24.806
  iterations_since_restore: 281
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.622907488986783
    ram_util_percent: 21.403964757709254
  pid: 24061
  policy_reward_max:
    agent-0: 203.3333333333335
    agent-1: 203.3333333333335
    agent-2: 203.3333333333335
    agent-3: 203.3333333333335
    agent-4: 203.3333333333335
    agent-5: 203.3333333333335
  policy_reward_mean:
    agent-0: 177.40666666666647
    agent-1: 177.40666666666647
    agent-2: 177.40666666666647
    agent-3: 177.40666666666647
    agent-4: 177.40666666666647
    agent-5: 177.40666666666647
  policy_reward_min:
    agent-0: 83.00000000000007
    agent-1: 83.00000000000007
    agent-2: 83.00000000000007
    agent-3: 83.00000000000007
    agent-4: 83.00000000000007
    agent-5: 83.00000000000007
  sampler_perf:
    mean_env_wait_ms: 24.8896462193054
    mean_inference_ms: 12.30493193963037
    mean_processing_ms: 51.07519004983488
  time_since_restore: 36596.618001937866
  time_this_iter_s: 158.79044771194458
  time_total_s: 45722.62981581688
  timestamp: 1637060649
  timesteps_since_restore: 26976000
  timesteps_this_iter: 96000
  timesteps_total: 32736000
  training_iteration: 341
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 38.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    341 |          45722.6 | 32736000 |  1064.44 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 81
    apples_agent-0_mean: 2.69
    apples_agent-0_min: 0
    apples_agent-1_max: 165
    apples_agent-1_mean: 35.77
    apples_agent-1_min: 0
    apples_agent-2_max: 90
    apples_agent-2_mean: 2.6
    apples_agent-2_min: 0
    apples_agent-3_max: 181
    apples_agent-3_mean: 118.21
    apples_agent-3_min: 74
    apples_agent-4_max: 33
    apples_agent-4_mean: 0.74
    apples_agent-4_min: 0
    apples_agent-5_max: 129
    apples_agent-5_mean: 79.32
    apples_agent-5_min: 26
    cleaning_beam_agent-0_max: 440
    cleaning_beam_agent-0_mean: 350.33
    cleaning_beam_agent-0_min: 226
    cleaning_beam_agent-1_max: 517
    cleaning_beam_agent-1_mean: 294.11
    cleaning_beam_agent-1_min: 125
    cleaning_beam_agent-2_max: 613
    cleaning_beam_agent-2_mean: 457.65
    cleaning_beam_agent-2_min: 232
    cleaning_beam_agent-3_max: 81
    cleaning_beam_agent-3_mean: 15.87
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 530
    cleaning_beam_agent-4_mean: 445.12
    cleaning_beam_agent-4_min: 345
    cleaning_beam_agent-5_max: 80
    cleaning_beam_agent-5_mean: 14.97
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-06-25
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1216.0
  episode_reward_mean: 1064.2799999999938
  episode_reward_min: 640.9999999999895
  episodes_this_iter: 96
  episodes_total: 32832
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20235.705
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1066277027130127
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020702306646853685
        model: {}
        policy_loss: -0.0031363372690975666
        total_loss: -0.002726546488702297
        vf_explained_var: 0.016020700335502625
        vf_loss: 23.574546813964844
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0851703882217407
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020302659831941128
        model: {}
        policy_loss: -0.00418761745095253
        total_loss: -0.0034453892149031162
        vf_explained_var: -0.08408603072166443
        vf_loss: 26.521272659301758
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9186866879463196
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011892132461071014
        model: {}
        policy_loss: -0.002897847443819046
        total_loss: -0.002189182909205556
        vf_explained_var: 0.02931278944015503
        vf_loss: 23.255542755126953
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3936019241809845
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010368155781179667
        model: {}
        policy_loss: -0.0022087451070547104
        total_loss: -0.0006493274122476578
        vf_explained_var: 0.060699790716171265
        vf_loss: 22.52157211303711
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8723005652427673
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014125300804153085
        model: {}
        policy_loss: -0.004031199961900711
        total_loss: -0.0031105969101190567
        vf_explained_var: -0.011351868510246277
        vf_loss: 24.558504104614258
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5177343487739563
        entropy_coeff: 0.0017600000137463212
        kl: 0.00048412056639790535
        model: {}
        policy_loss: -0.002436887938529253
        total_loss: -0.0011217541759833694
        vf_explained_var: 0.08447393774986267
        vf_loss: 22.263458251953125
    load_time_ms: 25388.804
    num_steps_sampled: 32832000
    num_steps_trained: 32832000
    sample_time_ms: 99317.243
    update_time_ms: 25.249
  iterations_since_restore: 282
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.470466321243524
    ram_util_percent: 21.198963730569947
  pid: 24061
  policy_reward_max:
    agent-0: 202.66666666666654
    agent-1: 202.66666666666654
    agent-2: 202.66666666666654
    agent-3: 202.66666666666654
    agent-4: 202.66666666666654
    agent-5: 202.66666666666654
  policy_reward_mean:
    agent-0: 177.37999999999985
    agent-1: 177.37999999999985
    agent-2: 177.37999999999985
    agent-3: 177.37999999999985
    agent-4: 177.37999999999985
    agent-5: 177.37999999999985
  policy_reward_min:
    agent-0: 106.83333333333339
    agent-1: 106.83333333333339
    agent-2: 106.83333333333339
    agent-3: 106.83333333333339
    agent-4: 106.83333333333339
    agent-5: 106.83333333333339
  sampler_perf:
    mean_env_wait_ms: 24.89923597402164
    mean_inference_ms: 12.30756468259147
    mean_processing_ms: 51.09200429194708
  time_since_restore: 36732.02081036568
  time_this_iter_s: 135.40280842781067
  time_total_s: 45858.03262424469
  timestamp: 1637060785
  timesteps_since_restore: 27072000
  timesteps_this_iter: 96000
  timesteps_total: 32832000
  training_iteration: 342
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 38.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    342 |            45858 | 32832000 |  1064.28 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 32
    apples_agent-0_mean: 1.28
    apples_agent-0_min: 0
    apples_agent-1_max: 167
    apples_agent-1_mean: 34.05
    apples_agent-1_min: 0
    apples_agent-2_max: 91
    apples_agent-2_mean: 3.06
    apples_agent-2_min: 0
    apples_agent-3_max: 247
    apples_agent-3_mean: 120.68
    apples_agent-3_min: 74
    apples_agent-4_max: 7
    apples_agent-4_mean: 0.07
    apples_agent-4_min: 0
    apples_agent-5_max: 139
    apples_agent-5_mean: 79.93
    apples_agent-5_min: 47
    cleaning_beam_agent-0_max: 460
    cleaning_beam_agent-0_mean: 361.82
    cleaning_beam_agent-0_min: 254
    cleaning_beam_agent-1_max: 506
    cleaning_beam_agent-1_mean: 286.75
    cleaning_beam_agent-1_min: 99
    cleaning_beam_agent-2_max: 640
    cleaning_beam_agent-2_mean: 465.35
    cleaning_beam_agent-2_min: 205
    cleaning_beam_agent-3_max: 63
    cleaning_beam_agent-3_mean: 13.05
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 584
    cleaning_beam_agent-4_mean: 451.6
    cleaning_beam_agent-4_min: 353
    cleaning_beam_agent-5_max: 59
    cleaning_beam_agent-5_mean: 13.91
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-08-39
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1236.0000000000018
  episode_reward_mean: 1087.4799999999952
  episode_reward_min: 644.0000000000082
  episodes_this_iter: 96
  episodes_total: 32928
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20241.974
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1240496635437012
        entropy_coeff: 0.0017600000137463212
        kl: 0.0025068807881325483
        model: {}
        policy_loss: -0.0034895804710686207
        total_loss: -0.0032896925695240498
        vf_explained_var: 0.0077836960554122925
        vf_loss: 21.782129287719727
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0807973146438599
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018726172856986523
        model: {}
        policy_loss: -0.004204586148262024
        total_loss: -0.003615410067141056
        vf_explained_var: -0.08539021015167236
        vf_loss: 24.91381072998047
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9195097088813782
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014963276917114854
        model: {}
        policy_loss: -0.0030074231326580048
        total_loss: -0.002527328208088875
        vf_explained_var: 0.04220329225063324
        vf_loss: 20.984317779541016
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36427128314971924
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009515222627669573
        model: {}
        policy_loss: -0.00207165302708745
        total_loss: -0.0006022476591169834
        vf_explained_var: 0.03772096335887909
        vf_loss: 21.105220794677734
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.875978946685791
        entropy_coeff: 0.0017600000137463212
        kl: 0.001923603587783873
        model: {}
        policy_loss: -0.0036810142919421196
        total_loss: -0.0030244849622249603
        vf_explained_var: 0.016098693013191223
        vf_loss: 21.98255157470703
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48224112391471863
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013607447035610676
        model: {}
        policy_loss: -0.0023595821112394333
        total_loss: -0.0010862266644835472
        vf_explained_var: 0.0464024543762207
        vf_loss: 21.22098159790039
    load_time_ms: 25429.362
    num_steps_sampled: 32928000
    num_steps_trained: 32928000
    sample_time_ms: 99208.212
    update_time_ms: 25.439
  iterations_since_restore: 283
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.23926701570681
    ram_util_percent: 20.219895287958114
  pid: 24061
  policy_reward_max:
    agent-0: 205.9999999999999
    agent-1: 205.9999999999999
    agent-2: 205.9999999999999
    agent-3: 205.9999999999999
    agent-4: 205.9999999999999
    agent-5: 205.9999999999999
  policy_reward_mean:
    agent-0: 181.24666666666647
    agent-1: 181.24666666666647
    agent-2: 181.24666666666647
    agent-3: 181.24666666666647
    agent-4: 181.24666666666647
    agent-5: 181.24666666666647
  policy_reward_min:
    agent-0: 107.33333333333356
    agent-1: 107.33333333333356
    agent-2: 107.33333333333356
    agent-3: 107.33333333333356
    agent-4: 107.33333333333356
    agent-5: 107.33333333333356
  sampler_perf:
    mean_env_wait_ms: 24.907425667574714
    mean_inference_ms: 12.309549708275354
    mean_processing_ms: 51.10366679941052
  time_since_restore: 36865.595746040344
  time_this_iter_s: 133.57493567466736
  time_total_s: 45991.60755991936
  timestamp: 1637060919
  timesteps_since_restore: 27168000
  timesteps_this_iter: 96000
  timesteps_total: 32928000
  training_iteration: 343
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 39.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    343 |          45991.6 | 32928000 |  1087.48 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 57
    apples_agent-0_mean: 2.17
    apples_agent-0_min: 0
    apples_agent-1_max: 144
    apples_agent-1_mean: 34.63
    apples_agent-1_min: 0
    apples_agent-2_max: 106
    apples_agent-2_mean: 2.23
    apples_agent-2_min: 0
    apples_agent-3_max: 188
    apples_agent-3_mean: 120.85
    apples_agent-3_min: 79
    apples_agent-4_max: 21
    apples_agent-4_mean: 0.37
    apples_agent-4_min: 0
    apples_agent-5_max: 141
    apples_agent-5_mean: 78.76
    apples_agent-5_min: 53
    cleaning_beam_agent-0_max: 477
    cleaning_beam_agent-0_mean: 364.06
    cleaning_beam_agent-0_min: 242
    cleaning_beam_agent-1_max: 637
    cleaning_beam_agent-1_mean: 281.59
    cleaning_beam_agent-1_min: 82
    cleaning_beam_agent-2_max: 613
    cleaning_beam_agent-2_mean: 450.0
    cleaning_beam_agent-2_min: 231
    cleaning_beam_agent-3_max: 41
    cleaning_beam_agent-3_mean: 13.75
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 546
    cleaning_beam_agent-4_mean: 448.77
    cleaning_beam_agent-4_min: 348
    cleaning_beam_agent-5_max: 36
    cleaning_beam_agent-5_mean: 11.91
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-11-30
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1267.999999999998
  episode_reward_mean: 1085.4299999999926
  episode_reward_min: 774.9999999999893
  episodes_this_iter: 96
  episodes_total: 33024
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20253.328
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1177451610565186
        entropy_coeff: 0.0017600000137463212
        kl: 0.001592474989593029
        model: {}
        policy_loss: -0.003009835723787546
        total_loss: -0.002802932634949684
        vf_explained_var: 0.02232949435710907
        vf_loss: 21.741334915161133
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0776915550231934
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015951250679790974
        model: {}
        policy_loss: -0.003832378890365362
        total_loss: -0.0031810076907277107
        vf_explained_var: -0.10728064179420471
        vf_loss: 25.481063842773438
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9242598414421082
        entropy_coeff: 0.0017600000137463212
        kl: 0.001782984472811222
        model: {}
        policy_loss: -0.0031360159628093243
        total_loss: -0.002549289260059595
        vf_explained_var: 0.005588993430137634
        vf_loss: 22.134248733520508
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37337374687194824
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010975440964102745
        model: {}
        policy_loss: -0.002107323380187154
        total_loss: -0.0006331715267151594
        vf_explained_var: 0.04207119345664978
        vf_loss: 21.31287956237793
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8639030456542969
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015820509288460016
        model: {}
        policy_loss: -0.0036871330812573433
        total_loss: -0.0029339399188756943
        vf_explained_var: -0.006462380290031433
        vf_loss: 22.73660659790039
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4634050726890564
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006577848107554018
        model: {}
        policy_loss: -0.00235864520072937
        total_loss: -0.001064341515302658
        vf_explained_var: 0.05793885886669159
        vf_loss: 21.098949432373047
    load_time_ms: 29141.948
    num_steps_sampled: 33024000
    num_steps_trained: 33024000
    sample_time_ms: 99297.909
    update_time_ms: 25.431
  iterations_since_restore: 284
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.00857142857143
    ram_util_percent: 21.69877551020408
  pid: 24061
  policy_reward_max:
    agent-0: 211.3333333333333
    agent-1: 211.3333333333333
    agent-2: 211.3333333333333
    agent-3: 211.3333333333333
    agent-4: 211.3333333333333
    agent-5: 211.3333333333333
  policy_reward_mean:
    agent-0: 180.90499999999986
    agent-1: 180.90499999999986
    agent-2: 180.90499999999986
    agent-3: 180.90499999999986
    agent-4: 180.90499999999986
    agent-5: 180.90499999999986
  policy_reward_min:
    agent-0: 129.16666666666714
    agent-1: 129.16666666666714
    agent-2: 129.16666666666714
    agent-3: 129.16666666666714
    agent-4: 129.16666666666714
    agent-5: 129.16666666666714
  sampler_perf:
    mean_env_wait_ms: 24.916063038982458
    mean_inference_ms: 12.31184187200927
    mean_processing_ms: 51.11774674485483
  time_since_restore: 37037.05610871315
  time_this_iter_s: 171.4603626728058
  time_total_s: 46163.06792259216
  timestamp: 1637061090
  timesteps_since_restore: 27264000
  timesteps_this_iter: 96000
  timesteps_total: 33024000
  training_iteration: 344
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    344 |          46163.1 | 33024000 |  1085.43 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 64
    apples_agent-0_mean: 1.28
    apples_agent-0_min: 0
    apples_agent-1_max: 132
    apples_agent-1_mean: 37.01
    apples_agent-1_min: 0
    apples_agent-2_max: 61
    apples_agent-2_mean: 4.03
    apples_agent-2_min: 0
    apples_agent-3_max: 153
    apples_agent-3_mean: 105.2
    apples_agent-3_min: 61
    apples_agent-4_max: 33
    apples_agent-4_mean: 0.89
    apples_agent-4_min: 0
    apples_agent-5_max: 148
    apples_agent-5_mean: 76.42
    apples_agent-5_min: 45
    cleaning_beam_agent-0_max: 457
    cleaning_beam_agent-0_mean: 355.66
    cleaning_beam_agent-0_min: 238
    cleaning_beam_agent-1_max: 536
    cleaning_beam_agent-1_mean: 286.27
    cleaning_beam_agent-1_min: 93
    cleaning_beam_agent-2_max: 611
    cleaning_beam_agent-2_mean: 459.76
    cleaning_beam_agent-2_min: 242
    cleaning_beam_agent-3_max: 93
    cleaning_beam_agent-3_mean: 15.75
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 540
    cleaning_beam_agent-4_mean: 452.82
    cleaning_beam_agent-4_min: 297
    cleaning_beam_agent-5_max: 51
    cleaning_beam_agent-5_mean: 14.76
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 3
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.08
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-13-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1225.9999999999864
  episode_reward_mean: 1060.1399999999956
  episode_reward_min: 425.0000000000022
  episodes_this_iter: 96
  episodes_total: 33120
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20247.676
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1101465225219727
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016478326870128512
        model: {}
        policy_loss: -0.0031418660655617714
        total_loss: -0.002590525895357132
        vf_explained_var: 0.010015144944190979
        vf_loss: 25.05196762084961
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0730204582214355
        entropy_coeff: 0.0017600000137463212
        kl: 0.001003562705591321
        model: {}
        policy_loss: -0.0037825077306479216
        total_loss: -0.0029956018552184105
        vf_explained_var: -0.04515334963798523
        vf_loss: 26.754209518432617
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9331170320510864
        entropy_coeff: 0.0017600000137463212
        kl: 0.001710166223347187
        model: {}
        policy_loss: -0.003173968754708767
        total_loss: -0.0024245737586170435
        vf_explained_var: 0.053917720913887024
        vf_loss: 23.916790008544922
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40234696865081787
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009884350001811981
        model: {}
        policy_loss: -0.002252677921205759
        total_loss: -0.0007403921335935593
        vf_explained_var: 0.12293580174446106
        vf_loss: 22.204164505004883
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.877348780632019
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013893608702346683
        model: {}
        policy_loss: -0.003589991945773363
        total_loss: -0.0026292959228157997
        vf_explained_var: 0.01703934371471405
        vf_loss: 25.048311233520508
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4950728416442871
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006792354397475719
        model: {}
        policy_loss: -0.0025686989538371563
        total_loss: -0.001167129259556532
        vf_explained_var: 0.10451400279998779
        vf_loss: 22.728927612304688
    load_time_ms: 27600.673
    num_steps_sampled: 33120000
    num_steps_trained: 33120000
    sample_time_ms: 99102.733
    update_time_ms: 25.613
  iterations_since_restore: 285
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.127659574468087
    ram_util_percent: 17.068617021276594
  pid: 24061
  policy_reward_max:
    agent-0: 204.3333333333333
    agent-1: 204.3333333333333
    agent-2: 204.3333333333333
    agent-3: 204.3333333333333
    agent-4: 204.3333333333333
    agent-5: 204.3333333333333
  policy_reward_mean:
    agent-0: 176.68999999999988
    agent-1: 176.68999999999988
    agent-2: 176.68999999999988
    agent-3: 176.68999999999988
    agent-4: 176.68999999999988
    agent-5: 176.68999999999988
  policy_reward_min:
    agent-0: 70.83333333333333
    agent-1: 70.83333333333333
    agent-2: 70.83333333333333
    agent-3: 70.83333333333333
    agent-4: 70.83333333333333
    agent-5: 70.83333333333333
  sampler_perf:
    mean_env_wait_ms: 24.921815217038993
    mean_inference_ms: 12.31334495842861
    mean_processing_ms: 51.12529090966866
  time_since_restore: 37169.49767994881
  time_this_iter_s: 132.44157123565674
  time_total_s: 46295.50949382782
  timestamp: 1637061223
  timesteps_since_restore: 27360000
  timesteps_this_iter: 96000
  timesteps_total: 33120000
  training_iteration: 345
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    345 |          46295.5 | 33120000 |  1060.14 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 64
    apples_agent-0_mean: 3.3
    apples_agent-0_min: 0
    apples_agent-1_max: 132
    apples_agent-1_mean: 33.94
    apples_agent-1_min: 0
    apples_agent-2_max: 37
    apples_agent-2_mean: 2.17
    apples_agent-2_min: 0
    apples_agent-3_max: 457
    apples_agent-3_mean: 108.55
    apples_agent-3_min: 49
    apples_agent-4_max: 32
    apples_agent-4_mean: 1.76
    apples_agent-4_min: 0
    apples_agent-5_max: 436
    apples_agent-5_mean: 80.99
    apples_agent-5_min: 45
    cleaning_beam_agent-0_max: 453
    cleaning_beam_agent-0_mean: 354.62
    cleaning_beam_agent-0_min: 206
    cleaning_beam_agent-1_max: 514
    cleaning_beam_agent-1_mean: 279.9
    cleaning_beam_agent-1_min: 143
    cleaning_beam_agent-2_max: 621
    cleaning_beam_agent-2_mean: 465.21
    cleaning_beam_agent-2_min: 251
    cleaning_beam_agent-3_max: 112
    cleaning_beam_agent-3_mean: 16.62
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 587
    cleaning_beam_agent-4_mean: 451.12
    cleaning_beam_agent-4_min: 323
    cleaning_beam_agent-5_max: 88
    cleaning_beam_agent-5_mean: 16.41
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-15-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1229.0000000000275
  episode_reward_mean: 1064.1499999999944
  episode_reward_min: 397.0000000000079
  episodes_this_iter: 96
  episodes_total: 33216
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20164.985
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1133365631103516
        entropy_coeff: 0.0017600000137463212
        kl: 0.002095585223287344
        model: {}
        policy_loss: -0.003169110044836998
        total_loss: -0.0025330400094389915
        vf_explained_var: -0.0005436241626739502
        vf_loss: 25.955440521240234
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0835331678390503
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016443129861727357
        model: {}
        policy_loss: -0.0040029436349868774
        total_loss: -0.0031104255467653275
        vf_explained_var: -0.06387379765510559
        vf_loss: 27.99534797668457
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9239248037338257
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013980742078274488
        model: {}
        policy_loss: -0.002928516361862421
        total_loss: -0.0020788288675248623
        vf_explained_var: 0.04628753662109375
        vf_loss: 24.757936477661133
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3851611018180847
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010543556418269873
        model: {}
        policy_loss: -0.0022354270331561565
        total_loss: -0.000567177776247263
        vf_explained_var: 0.09572681784629822
        vf_loss: 23.461326599121094
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8753756284713745
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016754688695073128
        model: {}
        policy_loss: -0.0038922373205423355
        total_loss: -0.002924408996477723
        vf_explained_var: 0.0413377583026886
        vf_loss: 25.084884643554688
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48932331800460815
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009440705762244761
        model: {}
        policy_loss: -0.0021935729309916496
        total_loss: -0.0007151083555072546
        vf_explained_var: 0.1034238338470459
        vf_loss: 23.3967342376709
    load_time_ms: 23639.584
    num_steps_sampled: 33216000
    num_steps_trained: 33216000
    sample_time_ms: 98495.537
    update_time_ms: 23.871
  iterations_since_restore: 286
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 18.56756756756757
    ram_util_percent: 15.415135135135134
  pid: 24061
  policy_reward_max:
    agent-0: 204.83333333333312
    agent-1: 204.83333333333312
    agent-2: 204.83333333333312
    agent-3: 204.83333333333312
    agent-4: 204.83333333333312
    agent-5: 204.83333333333312
  policy_reward_mean:
    agent-0: 177.35833333333318
    agent-1: 177.35833333333318
    agent-2: 177.35833333333318
    agent-3: 177.35833333333318
    agent-4: 177.35833333333318
    agent-5: 177.35833333333318
  policy_reward_min:
    agent-0: 66.16666666666649
    agent-1: 66.16666666666649
    agent-2: 66.16666666666649
    agent-3: 66.16666666666649
    agent-4: 66.16666666666649
    agent-5: 66.16666666666649
  sampler_perf:
    mean_env_wait_ms: 24.925338692209497
    mean_inference_ms: 12.31398712490633
    mean_processing_ms: 51.128436139009466
  time_since_restore: 37299.61356306076
  time_this_iter_s: 130.11588311195374
  time_total_s: 46425.62537693977
  timestamp: 1637061353
  timesteps_since_restore: 27456000
  timesteps_this_iter: 96000
  timesteps_total: 33216000
  training_iteration: 346
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    346 |          46425.6 | 33216000 |  1064.15 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 52
    apples_agent-0_mean: 2.36
    apples_agent-0_min: 0
    apples_agent-1_max: 189
    apples_agent-1_mean: 28.95
    apples_agent-1_min: 0
    apples_agent-2_max: 88
    apples_agent-2_mean: 3.58
    apples_agent-2_min: 0
    apples_agent-3_max: 185
    apples_agent-3_mean: 110.5
    apples_agent-3_min: 0
    apples_agent-4_max: 75
    apples_agent-4_mean: 1.6
    apples_agent-4_min: 0
    apples_agent-5_max: 122
    apples_agent-5_mean: 79.64
    apples_agent-5_min: 39
    cleaning_beam_agent-0_max: 456
    cleaning_beam_agent-0_mean: 358.61
    cleaning_beam_agent-0_min: 241
    cleaning_beam_agent-1_max: 563
    cleaning_beam_agent-1_mean: 292.16
    cleaning_beam_agent-1_min: 97
    cleaning_beam_agent-2_max: 625
    cleaning_beam_agent-2_mean: 468.21
    cleaning_beam_agent-2_min: 236
    cleaning_beam_agent-3_max: 48
    cleaning_beam_agent-3_mean: 13.44
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 544
    cleaning_beam_agent-4_mean: 441.46
    cleaning_beam_agent-4_min: 307
    cleaning_beam_agent-5_max: 48
    cleaning_beam_agent-5_mean: 12.78
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-18-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1244.9999999999868
  episode_reward_mean: 1090.1499999999944
  episode_reward_min: 491.00000000000705
  episodes_this_iter: 96
  episodes_total: 33312
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20152.349
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1037147045135498
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017090820474550128
        model: {}
        policy_loss: -0.0030383982229977846
        total_loss: -0.002613129559904337
        vf_explained_var: 0.021989747881889343
        vf_loss: 23.678077697753906
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0798271894454956
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017108547035604715
        model: {}
        policy_loss: -0.003994916565716267
        total_loss: -0.003146757371723652
        vf_explained_var: -0.08701002597808838
        vf_loss: 27.486530303955078
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9179166555404663
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020315463189035654
        model: {}
        policy_loss: -0.002842182293534279
        total_loss: -0.0021649813279509544
        vf_explained_var: 0.051739081740379333
        vf_loss: 22.927349090576172
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36321187019348145
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009347116574645042
        model: {}
        policy_loss: -0.0022272546775639057
        total_loss: -0.0005918658571317792
        vf_explained_var: 0.061990559101104736
        vf_loss: 22.746400833129883
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8680994510650635
        entropy_coeff: 0.0017600000137463212
        kl: 0.001749904709868133
        model: {}
        policy_loss: -0.0039604282937943935
        total_loss: -0.0030839485116302967
        vf_explained_var: 0.03088340163230896
        vf_loss: 24.04332733154297
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46554791927337646
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007072975859045982
        model: {}
        policy_loss: -0.002250507939606905
        total_loss: -0.0007265862077474594
        vf_explained_var: 0.05291202664375305
        vf_loss: 23.432863235473633
    load_time_ms: 23069.377
    num_steps_sampled: 33312000
    num_steps_trained: 33312000
    sample_time_ms: 98122.503
    update_time_ms: 23.461
  iterations_since_restore: 287
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 18.52
    ram_util_percent: 15.39027027027027
  pid: 24061
  policy_reward_max:
    agent-0: 207.50000000000014
    agent-1: 207.50000000000014
    agent-2: 207.50000000000014
    agent-3: 207.50000000000014
    agent-4: 207.50000000000014
    agent-5: 207.50000000000014
  policy_reward_mean:
    agent-0: 181.6916666666665
    agent-1: 181.6916666666665
    agent-2: 181.6916666666665
    agent-3: 181.6916666666665
    agent-4: 181.6916666666665
    agent-5: 181.6916666666665
  policy_reward_min:
    agent-0: 81.83333333333347
    agent-1: 81.83333333333347
    agent-2: 81.83333333333347
    agent-3: 81.83333333333347
    agent-4: 81.83333333333347
    agent-5: 81.83333333333347
  sampler_perf:
    mean_env_wait_ms: 24.928919006044374
    mean_inference_ms: 12.314773466684692
    mean_processing_ms: 51.13237567404909
  time_since_restore: 37429.15261077881
  time_this_iter_s: 129.5390477180481
  time_total_s: 46555.16442465782
  timestamp: 1637061483
  timesteps_since_restore: 27552000
  timesteps_this_iter: 96000
  timesteps_total: 33312000
  training_iteration: 347
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    347 |          46555.2 | 33312000 |  1090.15 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 59
    apples_agent-0_mean: 2.09
    apples_agent-0_min: 0
    apples_agent-1_max: 195
    apples_agent-1_mean: 38.43
    apples_agent-1_min: 0
    apples_agent-2_max: 70
    apples_agent-2_mean: 2.31
    apples_agent-2_min: 0
    apples_agent-3_max: 281
    apples_agent-3_mean: 111.59
    apples_agent-3_min: 75
    apples_agent-4_max: 55
    apples_agent-4_mean: 0.82
    apples_agent-4_min: 0
    apples_agent-5_max: 110
    apples_agent-5_mean: 77.11
    apples_agent-5_min: 52
    cleaning_beam_agent-0_max: 452
    cleaning_beam_agent-0_mean: 352.39
    cleaning_beam_agent-0_min: 244
    cleaning_beam_agent-1_max: 470
    cleaning_beam_agent-1_mean: 267.56
    cleaning_beam_agent-1_min: 97
    cleaning_beam_agent-2_max: 687
    cleaning_beam_agent-2_mean: 477.5
    cleaning_beam_agent-2_min: 290
    cleaning_beam_agent-3_max: 67
    cleaning_beam_agent-3_mean: 14.5
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 572
    cleaning_beam_agent-4_mean: 457.93
    cleaning_beam_agent-4_min: 295
    cleaning_beam_agent-5_max: 86
    cleaning_beam_agent-5_mean: 14.41
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-20-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1225.999999999998
  episode_reward_mean: 1086.399999999992
  episode_reward_min: 536.0000000000032
  episodes_this_iter: 96
  episodes_total: 33408
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20169.652
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1108708381652832
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018463853048160672
        model: {}
        policy_loss: -0.00315268337726593
        total_loss: -0.0028243951965123415
        vf_explained_var: 0.018507719039916992
        vf_loss: 22.83420181274414
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0895545482635498
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019955395255237818
        model: {}
        policy_loss: -0.0040594907477498055
        total_loss: -0.003405625931918621
        vf_explained_var: -0.06091797351837158
        vf_loss: 25.714815139770508
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9157800674438477
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017687147483229637
        model: {}
        policy_loss: -0.0030622915364801884
        total_loss: -0.0024299994111061096
        vf_explained_var: 0.034458160400390625
        vf_loss: 22.440662384033203
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3829285204410553
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011977874673902988
        model: {}
        policy_loss: -0.0021163756027817726
        total_loss: -0.00054195336997509
        vf_explained_var: 0.03615361452102661
        vf_loss: 22.483753204345703
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8683393001556396
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015870024217292666
        model: {}
        policy_loss: -0.003754671663045883
        total_loss: -0.002905838191509247
        vf_explained_var: -0.001678362488746643
        vf_loss: 23.77107810974121
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4672207832336426
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006204773089848459
        model: {}
        policy_loss: -0.0021923682652413845
        total_loss: -0.000874002929776907
        vf_explained_var: 0.09331174194812775
        vf_loss: 21.406707763671875
    load_time_ms: 23017.792
    num_steps_sampled: 33408000
    num_steps_trained: 33408000
    sample_time_ms: 97785.924
    update_time_ms: 22.987
  iterations_since_restore: 288
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 17.79731182795699
    ram_util_percent: 15.362903225806452
  pid: 24061
  policy_reward_max:
    agent-0: 204.33333333333331
    agent-1: 204.33333333333331
    agent-2: 204.33333333333331
    agent-3: 204.33333333333331
    agent-4: 204.33333333333331
    agent-5: 204.33333333333331
  policy_reward_mean:
    agent-0: 181.06666666666644
    agent-1: 181.06666666666644
    agent-2: 181.06666666666644
    agent-3: 181.06666666666644
    agent-4: 181.06666666666644
    agent-5: 181.06666666666644
  policy_reward_min:
    agent-0: 89.3333333333334
    agent-1: 89.3333333333334
    agent-2: 89.3333333333334
    agent-3: 89.3333333333334
    agent-4: 89.3333333333334
    agent-5: 89.3333333333334
  sampler_perf:
    mean_env_wait_ms: 24.932860962866926
    mean_inference_ms: 12.315604041301832
    mean_processing_ms: 51.135843391421105
  time_since_restore: 37559.634341955185
  time_this_iter_s: 130.48173117637634
  time_total_s: 46685.6461558342
  timestamp: 1637061614
  timesteps_since_restore: 27648000
  timesteps_this_iter: 96000
  timesteps_total: 33408000
  training_iteration: 348
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 27.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    348 |          46685.6 | 33408000 |   1086.4 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 0.91
    apples_agent-0_min: 0
    apples_agent-1_max: 172
    apples_agent-1_mean: 31.78
    apples_agent-1_min: 0
    apples_agent-2_max: 125
    apples_agent-2_mean: 3.57
    apples_agent-2_min: 0
    apples_agent-3_max: 165
    apples_agent-3_mean: 109.61
    apples_agent-3_min: 65
    apples_agent-4_max: 51
    apples_agent-4_mean: 1.95
    apples_agent-4_min: 0
    apples_agent-5_max: 142
    apples_agent-5_mean: 77.39
    apples_agent-5_min: 30
    cleaning_beam_agent-0_max: 461
    cleaning_beam_agent-0_mean: 357.49
    cleaning_beam_agent-0_min: 235
    cleaning_beam_agent-1_max: 540
    cleaning_beam_agent-1_mean: 291.64
    cleaning_beam_agent-1_min: 93
    cleaning_beam_agent-2_max: 642
    cleaning_beam_agent-2_mean: 478.23
    cleaning_beam_agent-2_min: 152
    cleaning_beam_agent-3_max: 100
    cleaning_beam_agent-3_mean: 13.11
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 571
    cleaning_beam_agent-4_mean: 447.08
    cleaning_beam_agent-4_min: 210
    cleaning_beam_agent-5_max: 88
    cleaning_beam_agent-5_mean: 11.4
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.07
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-22-21
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1245.0000000000152
  episode_reward_mean: 1096.4799999999946
  episode_reward_min: 419.00000000000415
  episodes_this_iter: 96
  episodes_total: 33504
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20091.313
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.104891061782837
        entropy_coeff: 0.0017600000137463212
        kl: 0.002340575447306037
        model: {}
        policy_loss: -0.0031011560931801796
        total_loss: -0.0027073449455201626
        vf_explained_var: 0.014676541090011597
        vf_loss: 23.384204864501953
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0685224533081055
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015739165246486664
        model: {}
        policy_loss: -0.004089707508683205
        total_loss: -0.0033160047605633736
        vf_explained_var: -0.07284027338027954
        vf_loss: 26.543033599853516
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9099765419960022
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014551011845469475
        model: {}
        policy_loss: -0.0032713045366108418
        total_loss: -0.002468318212777376
        vf_explained_var: -0.009655565023422241
        vf_loss: 24.04544448852539
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3643878698348999
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006941664614714682
        model: {}
        policy_loss: -0.001880194991827011
        total_loss: -0.00028029177337884903
        vf_explained_var: 0.061111852526664734
        vf_loss: 22.412277221679688
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8808507919311523
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019158448558300734
        model: {}
        policy_loss: -0.0038266966585069895
        total_loss: -0.00300565455108881
        vf_explained_var: 0.026724979281425476
        vf_loss: 23.713397979736328
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47192102670669556
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009138872846961021
        model: {}
        policy_loss: -0.002056693658232689
        total_loss: -0.0006893700920045376
        vf_explained_var: 0.09143899381160736
        vf_loss: 21.97904396057129
    load_time_ms: 22805.182
    num_steps_sampled: 33504000
    num_steps_trained: 33504000
    sample_time_ms: 97324.443
    update_time_ms: 23.091
  iterations_since_restore: 289
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.278571428571427
    ram_util_percent: 14.978021978021978
  pid: 24061
  policy_reward_max:
    agent-0: 207.4999999999997
    agent-1: 207.4999999999997
    agent-2: 207.4999999999997
    agent-3: 207.4999999999997
    agent-4: 207.4999999999997
    agent-5: 207.4999999999997
  policy_reward_mean:
    agent-0: 182.74666666666656
    agent-1: 182.74666666666656
    agent-2: 182.74666666666656
    agent-3: 182.74666666666656
    agent-4: 182.74666666666656
    agent-5: 182.74666666666656
  policy_reward_min:
    agent-0: 69.83333333333316
    agent-1: 69.83333333333316
    agent-2: 69.83333333333316
    agent-3: 69.83333333333316
    agent-4: 69.83333333333316
    agent-5: 69.83333333333316
  sampler_perf:
    mean_env_wait_ms: 24.93508990039968
    mean_inference_ms: 12.315893586203297
    mean_processing_ms: 51.136052123174906
  time_since_restore: 37687.070302248
  time_this_iter_s: 127.43596029281616
  time_total_s: 46813.082116127014
  timestamp: 1637061741
  timesteps_since_restore: 27744000
  timesteps_this_iter: 96000
  timesteps_total: 33504000
  training_iteration: 349
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    349 |          46813.1 | 33504000 |  1096.48 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 60
    apples_agent-0_mean: 2.33
    apples_agent-0_min: 0
    apples_agent-1_max: 134
    apples_agent-1_mean: 32.66
    apples_agent-1_min: 0
    apples_agent-2_max: 158
    apples_agent-2_mean: 6.59
    apples_agent-2_min: 0
    apples_agent-3_max: 169
    apples_agent-3_mean: 104.94
    apples_agent-3_min: 60
    apples_agent-4_max: 32
    apples_agent-4_mean: 0.82
    apples_agent-4_min: 0
    apples_agent-5_max: 123
    apples_agent-5_mean: 75.12
    apples_agent-5_min: 36
    cleaning_beam_agent-0_max: 456
    cleaning_beam_agent-0_mean: 352.84
    cleaning_beam_agent-0_min: 192
    cleaning_beam_agent-1_max: 501
    cleaning_beam_agent-1_mean: 262.91
    cleaning_beam_agent-1_min: 94
    cleaning_beam_agent-2_max: 666
    cleaning_beam_agent-2_mean: 465.54
    cleaning_beam_agent-2_min: 152
    cleaning_beam_agent-3_max: 90
    cleaning_beam_agent-3_mean: 13.06
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 557
    cleaning_beam_agent-4_mean: 442.47
    cleaning_beam_agent-4_min: 345
    cleaning_beam_agent-5_max: 51
    cleaning_beam_agent-5_mean: 13.18
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-24-30
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1290.0000000000077
  episode_reward_mean: 1071.7999999999922
  episode_reward_min: 653.9999999999915
  episodes_this_iter: 96
  episodes_total: 33600
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20030.322
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1158398389816284
        entropy_coeff: 0.0017600000137463212
        kl: 0.002384243533015251
        model: {}
        policy_loss: -0.00336285587400198
        total_loss: -0.003022939432412386
        vf_explained_var: 0.014442965388298035
        vf_loss: 23.037940979003906
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0977680683135986
        entropy_coeff: 0.0017600000137463212
        kl: 0.001051740488037467
        model: {}
        policy_loss: -0.0038717316929250956
        total_loss: -0.0033095816615968943
        vf_explained_var: -0.04534107446670532
        vf_loss: 24.94223403930664
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9132120609283447
        entropy_coeff: 0.0017600000137463212
        kl: 0.00150466023478657
        model: {}
        policy_loss: -0.0030113584361970425
        total_loss: -0.002414341550320387
        vf_explained_var: 0.05862908065319061
        vf_loss: 22.042701721191406
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37227702140808105
        entropy_coeff: 0.0017600000137463212
        kl: 0.001046832767315209
        model: {}
        policy_loss: -0.0021438170224428177
        total_loss: -0.000670852605253458
        vf_explained_var: 0.09013086557388306
        vf_loss: 21.281681060791016
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.878544270992279
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013980664080008864
        model: {}
        policy_loss: -0.004073416814208031
        total_loss: -0.0033004749566316605
        vf_explained_var: 0.015110626816749573
        vf_loss: 23.1917724609375
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4922965466976166
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006993294227868319
        model: {}
        policy_loss: -0.002083756495267153
        total_loss: -0.0007245868910104036
        vf_explained_var: 0.058453693985939026
        vf_loss: 22.256132125854492
    load_time_ms: 20849.994
    num_steps_sampled: 33600000
    num_steps_trained: 33600000
    sample_time_ms: 96755.697
    update_time_ms: 20.743
  iterations_since_restore: 290
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.232786885245904
    ram_util_percent: 15.045901639344258
  pid: 24061
  policy_reward_max:
    agent-0: 215.00000000000017
    agent-1: 215.00000000000017
    agent-2: 215.00000000000017
    agent-3: 215.00000000000017
    agent-4: 215.00000000000017
    agent-5: 215.00000000000017
  policy_reward_mean:
    agent-0: 178.63333333333324
    agent-1: 178.63333333333324
    agent-2: 178.63333333333324
    agent-3: 178.63333333333324
    agent-4: 178.63333333333324
    agent-5: 178.63333333333324
  policy_reward_min:
    agent-0: 109.0000000000002
    agent-1: 109.0000000000002
    agent-2: 109.0000000000002
    agent-3: 109.0000000000002
    agent-4: 109.0000000000002
    agent-5: 109.0000000000002
  sampler_perf:
    mean_env_wait_ms: 24.93690913472762
    mean_inference_ms: 12.316183965733385
    mean_processing_ms: 51.13628574688402
  time_since_restore: 37815.52418088913
  time_this_iter_s: 128.45387864112854
  time_total_s: 46941.53599476814
  timestamp: 1637061870
  timesteps_since_restore: 27840000
  timesteps_this_iter: 96000
  timesteps_total: 33600000
  training_iteration: 350
  trial_id: '00000'
  
[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
== Status ==
Memory usage on this node: 27.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    350 |          46941.5 | 33600000 |   1071.8 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 76
    apples_agent-0_mean: 2.8
    apples_agent-0_min: 0
    apples_agent-1_max: 155
    apples_agent-1_mean: 32.29
    apples_agent-1_min: 0
    apples_agent-2_max: 60
    apples_agent-2_mean: 4.49
    apples_agent-2_min: 0
    apples_agent-3_max: 169
    apples_agent-3_mean: 100.04
    apples_agent-3_min: 41
    apples_agent-4_max: 27
    apples_agent-4_mean: 0.57
    apples_agent-4_min: 0
    apples_agent-5_max: 127
    apples_agent-5_mean: 77.14
    apples_agent-5_min: 39
    cleaning_beam_agent-0_max: 451
    cleaning_beam_agent-0_mean: 346.56
    cleaning_beam_agent-0_min: 173
    cleaning_beam_agent-1_max: 534
    cleaning_beam_agent-1_mean: 277.63
    cleaning_beam_agent-1_min: 48
    cleaning_beam_agent-2_max: 649
    cleaning_beam_agent-2_mean: 489.66
    cleaning_beam_agent-2_min: 241
    cleaning_beam_agent-3_max: 77
    cleaning_beam_agent-3_mean: 14.36
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 551
    cleaning_beam_agent-4_mean: 438.4
    cleaning_beam_agent-4_min: 303
    cleaning_beam_agent-5_max: 84
    cleaning_beam_agent-5_mean: 13.68
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-26-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1241.999999999997
  episode_reward_mean: 1063.0899999999938
  episode_reward_min: 582.0000000000028
  episodes_this_iter: 96
  episodes_total: 33696
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20034.793
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.115526556968689
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015670383581891656
        model: {}
        policy_loss: -0.002951482543721795
        total_loss: -0.002405377570539713
        vf_explained_var: -0.007286474108695984
        vf_loss: 25.09432601928711
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0853043794631958
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012516165152192116
        model: {}
        policy_loss: -0.003776589408516884
        total_loss: -0.003188554896041751
        vf_explained_var: 0.004772782325744629
        vf_loss: 24.98170280456543
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8884870409965515
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018018223345279694
        model: {}
        policy_loss: -0.0031343598384410143
        total_loss: -0.0024740174412727356
        vf_explained_var: 0.10533101856708527
        vf_loss: 22.24081802368164
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3931444585323334
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009418724221177399
        model: {}
        policy_loss: -0.0022744950838387012
        total_loss: -0.0007501707877963781
        vf_explained_var: 0.1081421822309494
        vf_loss: 22.1626033782959
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8777098655700684
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017176178516820073
        model: {}
        policy_loss: -0.003958675544708967
        total_loss: -0.003041802905499935
        vf_explained_var: 0.012065082788467407
        vf_loss: 24.616443634033203
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4816766381263733
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006735608330927789
        model: {}
        policy_loss: -0.002422736957669258
        total_loss: -0.00098930555395782
        vf_explained_var: 0.0842171460390091
        vf_loss: 22.81182289123535
    load_time_ms: 18330.516
    num_steps_sampled: 33696000
    num_steps_trained: 33696000
    sample_time_ms: 96243.152
    update_time_ms: 21.544
  iterations_since_restore: 291
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.349180327868856
    ram_util_percent: 15.046994535519122
  pid: 24061
  policy_reward_max:
    agent-0: 207.0000000000002
    agent-1: 207.0000000000002
    agent-2: 207.0000000000002
    agent-3: 207.0000000000002
    agent-4: 207.0000000000002
    agent-5: 207.0000000000002
  policy_reward_mean:
    agent-0: 177.1816666666665
    agent-1: 177.1816666666665
    agent-2: 177.1816666666665
    agent-3: 177.1816666666665
    agent-4: 177.1816666666665
    agent-5: 177.1816666666665
  policy_reward_min:
    agent-0: 97.00000000000016
    agent-1: 97.00000000000016
    agent-2: 97.00000000000016
    agent-3: 97.00000000000016
    agent-4: 97.00000000000016
    agent-5: 97.00000000000016
  sampler_perf:
    mean_env_wait_ms: 24.939328846193295
    mean_inference_ms: 12.316506986660816
    mean_processing_ms: 51.137465938000844
  time_since_restore: 37944.01003861427
  time_this_iter_s: 128.48585772514343
  time_total_s: 47070.021852493286
  timestamp: 1637061998
  timesteps_since_restore: 27936000
  timesteps_this_iter: 96000
  timesteps_total: 33696000
  training_iteration: 351
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    351 |            47070 | 33696000 |  1063.09 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 43
    apples_agent-0_mean: 2.01
    apples_agent-0_min: 0
    apples_agent-1_max: 111
    apples_agent-1_mean: 26.93
    apples_agent-1_min: 0
    apples_agent-2_max: 64
    apples_agent-2_mean: 2.52
    apples_agent-2_min: 0
    apples_agent-3_max: 287
    apples_agent-3_mean: 109.1
    apples_agent-3_min: 64
    apples_agent-4_max: 53
    apples_agent-4_mean: 1.01
    apples_agent-4_min: 0
    apples_agent-5_max: 215
    apples_agent-5_mean: 77.21
    apples_agent-5_min: 44
    cleaning_beam_agent-0_max: 472
    cleaning_beam_agent-0_mean: 347.89
    cleaning_beam_agent-0_min: 172
    cleaning_beam_agent-1_max: 503
    cleaning_beam_agent-1_mean: 269.67
    cleaning_beam_agent-1_min: 48
    cleaning_beam_agent-2_max: 681
    cleaning_beam_agent-2_mean: 507.09
    cleaning_beam_agent-2_min: 262
    cleaning_beam_agent-3_max: 44
    cleaning_beam_agent-3_mean: 12.72
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 555
    cleaning_beam_agent-4_mean: 431.74
    cleaning_beam_agent-4_min: 303
    cleaning_beam_agent-5_max: 115
    cleaning_beam_agent-5_mean: 15.61
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.08
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-28-44
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1224.9999999999993
  episode_reward_mean: 1065.759999999993
  episode_reward_min: 586.0000000000042
  episodes_this_iter: 96
  episodes_total: 33792
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20032.842
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1110457181930542
        entropy_coeff: 0.0017600000137463212
        kl: 0.0028336532413959503
        model: {}
        policy_loss: -0.003557629883289337
        total_loss: -0.0032147131860256195
        vf_explained_var: -0.007824242115020752
        vf_loss: 22.983551025390625
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0995303392410278
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014772352296859026
        model: {}
        policy_loss: -0.0038377149030566216
        total_loss: -0.0031980229541659355
        vf_explained_var: -0.11786884069442749
        vf_loss: 25.748661041259766
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8957785367965698
        entropy_coeff: 0.0017600000137463212
        kl: 0.001620227936655283
        model: {}
        policy_loss: -0.0032685818150639534
        total_loss: -0.0025507104583084583
        vf_explained_var: -0.007466495037078857
        vf_loss: 22.944435119628906
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3702085614204407
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010297895641997457
        model: {}
        policy_loss: -0.002141188131645322
        total_loss: -0.0006943732150830328
        vf_explained_var: 0.07894536852836609
        vf_loss: 20.983816146850586
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8793306350708008
        entropy_coeff: 0.0017600000137463212
        kl: 0.001520367688499391
        model: {}
        policy_loss: -0.003567672800272703
        total_loss: -0.002818959765136242
        vf_explained_var: -0.0045127272605896
        vf_loss: 22.963361740112305
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4692980647087097
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008827476412989199
        model: {}
        policy_loss: -0.002152770757675171
        total_loss: -0.0008229352533817291
        vf_explained_var: 0.05971917510032654
        vf_loss: 21.558006286621094
    load_time_ms: 18201.431
    num_steps_sampled: 33792000
    num_steps_trained: 33792000
    sample_time_ms: 95362.096
    update_time_ms: 21.815
  iterations_since_restore: 292
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.346368715083798
    ram_util_percent: 15.053072625698322
  pid: 24061
  policy_reward_max:
    agent-0: 204.16666666666654
    agent-1: 204.16666666666654
    agent-2: 204.16666666666654
    agent-3: 204.16666666666654
    agent-4: 204.16666666666654
    agent-5: 204.16666666666654
  policy_reward_mean:
    agent-0: 177.6266666666665
    agent-1: 177.6266666666665
    agent-2: 177.6266666666665
    agent-3: 177.6266666666665
    agent-4: 177.6266666666665
    agent-5: 177.6266666666665
  policy_reward_min:
    agent-0: 97.66666666666694
    agent-1: 97.66666666666694
    agent-2: 97.66666666666694
    agent-3: 97.66666666666694
    agent-4: 97.66666666666694
    agent-5: 97.66666666666694
  sampler_perf:
    mean_env_wait_ms: 24.940577154796006
    mean_inference_ms: 12.316582008295663
    mean_processing_ms: 51.135303218437166
  time_since_restore: 38069.28848862648
  time_this_iter_s: 125.27845001220703
  time_total_s: 47195.30030250549
  timestamp: 1637062124
  timesteps_since_restore: 28032000
  timesteps_this_iter: 96000
  timesteps_total: 33792000
  training_iteration: 352
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/35.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.138:24061 |    352 |          47195.3 | 33792000 |  1065.76 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=24061)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe33d834588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 50
    apples_agent-0_mean: 3.24
    apples_agent-0_min: 0
    apples_agent-1_max: 132
    apples_agent-1_mean: 35.69
    apples_agent-1_min: 0
    apples_agent-2_max: 56
    apples_agent-2_mean: 1.49
    apples_agent-2_min: 0
    apples_agent-3_max: 186
    apples_agent-3_mean: 100.83
    apples_agent-3_min: 34
    apples_agent-4_max: 48
    apples_agent-4_mean: 1.31
    apples_agent-4_min: 0
    apples_agent-5_max: 127
    apples_agent-5_mean: 75.76
    apples_agent-5_min: 37
    cleaning_beam_agent-0_max: 459
    cleaning_beam_agent-0_mean: 358.84
    cleaning_beam_agent-0_min: 178
    cleaning_beam_agent-1_max: 467
    cleaning_beam_agent-1_mean: 268.5
    cleaning_beam_agent-1_min: 111
    cleaning_beam_agent-2_max: 676
    cleaning_beam_agent-2_mean: 487.84
    cleaning_beam_agent-2_min: 231
    cleaning_beam_agent-3_max: 118
    cleaning_beam_agent-3_mean: 15.56
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 551
    cleaning_beam_agent-4_mean: 431.21
    cleaning_beam_agent-4_min: 270
    cleaning_beam_agent-5_max: 67
    cleaning_beam_agent-5_mean: 13.2
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-30-48
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1236.0000000000098
  episode_reward_mean: 1066.609999999994
  episode_reward_min: 422.99999999999767
  episodes_this_iter: 96
  episodes_total: 33888
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu138.cluster.local
  info:
    grad_time_ms: 20031.018
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.115319013595581
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012726807035505772
        model: {}
        policy_loss: -0.003154153935611248
        total_loss: -0.0026188925839960575
        vf_explained_var: 0.03905750811100006
        vf_loss: 24.982210159301758
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0957196950912476
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017581665888428688
        model: {}
        policy_loss: -0.004522881470620632
        total_loss: -0.003554407972842455
        vf_explained_var: -0.09952235221862793
        vf_loss: 28.969406127929688
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9106258153915405
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013009067624807358
        model: {}
        policy_loss: -0.00297939102165401
        total_loss: -0.0021389182657003403
        vf_explained_var: 0.05996344983577728
        vf_loss: 24.431743621826172
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39254143834114075
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008023593109101057
        model: {}
        policy_loss: -0.0024878419935703278
        total_loss: -0.0008507599122822285
        vf_explained_var: 0.1045503318309784
        vf_loss: 23.27953338623047
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8783182501792908
        entropy_coeff: 0.0017600000137463212
        kl: 0.001978403888642788
        model: {}
        policy_loss: -0.003723400877788663
        total_loss: -0.0027055288664996624
        vf_explained_var: 0.015992194414138794
        vf_loss: 25.637134552001953
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48480021953582764
        entropy_coeff: 0.0017600000137463212
        kl: 0.000707317260093987
        model: {}
        policy_loss: -0.0025207665748894215
        total_loss: -0.000988996122032404
        vf_explained_var: 0.08559569716453552
        vf_loss: 23.85018539428711
    load_time_ms: 18060.778
    num_steps_sampled: 33888000
    num_steps_trained: 33888000
    sample_time_ms: 94534.27
    update_time_ms: 21.282
  iterations_since_restore: 293
  node_ip: 172.17.8.138
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 15.926553672316384
    ram_util_percent: 15.152542372881356
  pid: 24061
  policy_reward_max:
    agent-0: 206.00000000000017
    agent-1: 206.00000000000017
    agent-2: 206.00000000000017
    agent-3: 206.00000000000017
    agent-4: 206.00000000000017
    agent-5: 206.00000000000017
  policy_reward_mean:
    agent-0: 177.76833333333326
    agent-1: 177.76833333333326
    agent-2: 177.76833333333326
    agent-3: 177.76833333333326
    agent-4: 177.76833333333326
    agent-5: 177.76833333333326
  policy_reward_min:
    agent-0: 70.50000000000004
    agent-1: 70.50000000000004
    agent-2: 70.50000000000004
    agent-3: 70.50000000000004
    agent-4: 70.50000000000004
    agent-5: 70.50000000000004
  sampler_perf:
    mean_env_wait_ms: 24.940755416105098
    mean_inference_ms: 12.316257784954276
    mean_processing_ms: 51.13222775973508
  time_since_restore: 38193.13051080704
  time_this_iter_s: 123.84202218055725
  time_total_s: 47319.14232468605
  timestamp: 1637062248
  timesteps_since_restore: 28128000
  timesteps_this_iter: 96000
  timesteps_total: 33888000
  training_iteration: 353
  trial_id: '00000'
  == Status ==
Memory usage on this node: 11.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_2teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------+
| Trial name                           | status   | loc   |
|--------------------------------------+----------+-------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  |       |
+--------------------------------------+----------+-------+


[2m[36m(pid=30197)[0m 2021-11-17 16:31:35,049	INFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution
[2m[36m(pid=30197)[0m 2021-11-17 16:31:35,055	INFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=30197)[0m 2021-11-17 16:33:32,422	INFO trainable.py:180 -- _setup took 117.373 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(pid=30197)[0m 2021-11-17 16:33:32,422	INFO trainable.py:217 -- Getting current IP.
[2m[36m(pid=30197)[0m 2021-11-17 16:33:32,423	WARNING util.py:37 -- Install gputil for GPU system monitoring.
[2m[36m(pid=30197)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f41e0e81588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 3.0
    apples_agent-0_min: 0
    apples_agent-1_max: 22
    apples_agent-1_mean: 3.46875
    apples_agent-1_min: 0
    apples_agent-2_max: 33
    apples_agent-2_mean: 4.020833333333333
    apples_agent-2_min: 0
    apples_agent-3_max: 25
    apples_agent-3_mean: 4.104166666666667
    apples_agent-3_min: 0
    apples_agent-4_max: 22
    apples_agent-4_mean: 3.4895833333333335
    apples_agent-4_min: 0
    apples_agent-5_max: 16
    apples_agent-5_mean: 3.0
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 126
    cleaning_beam_agent-0_mean: 107.1875
    cleaning_beam_agent-0_min: 82
    cleaning_beam_agent-1_max: 127
    cleaning_beam_agent-1_mean: 102.63541666666667
    cleaning_beam_agent-1_min: 81
    cleaning_beam_agent-2_max: 134
    cleaning_beam_agent-2_mean: 111.78125
    cleaning_beam_agent-2_min: 89
    cleaning_beam_agent-3_max: 133
    cleaning_beam_agent-3_mean: 104.77083333333333
    cleaning_beam_agent-3_min: 82
    cleaning_beam_agent-4_max: 140
    cleaning_beam_agent-4_mean: 110.82291666666667
    cleaning_beam_agent-4_min: 81
    cleaning_beam_agent-5_max: 129
    cleaning_beam_agent-5_mean: 104.86458333333333
    cleaning_beam_agent-5_min: 82
    fire_beam_agent-0_max: 134
    fire_beam_agent-0_mean: 112.65625
    fire_beam_agent-0_min: 90
    fire_beam_agent-1_max: 141
    fire_beam_agent-1_mean: 106.91666666666667
    fire_beam_agent-1_min: 71
    fire_beam_agent-2_max: 133
    fire_beam_agent-2_mean: 109.02083333333333
    fire_beam_agent-2_min: 82
    fire_beam_agent-3_max: 134
    fire_beam_agent-3_mean: 107.91666666666667
    fire_beam_agent-3_min: 88
    fire_beam_agent-4_max: 135
    fire_beam_agent-4_mean: 111.61458333333333
    fire_beam_agent-4_min: 86
    fire_beam_agent-5_max: 130
    fire_beam_agent-5_mean: 108.61458333333333
    fire_beam_agent-5_min: 84
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-17_16-36-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: -3305.000000000095
  episode_reward_mean: -6979.72916666661
  episode_reward_min: -11062.000000000102
  episodes_this_iter: 96
  episodes_total: 96
  experiment_id: 79b578a554754ec4a502419d8b89326f
  experiment_tag: '0'
  hostname: gpu060
  info:
    grad_time_ms: 23605.627
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0012600000482052565
        entropy: 2.1859500408172607
        entropy_coeff: 0.0017600000137463212
        kl: 0.010663731954991817
        model: {}
        policy_loss: -0.002393722999840975
        total_loss: 1.2783238887786865
        vf_explained_var: -0.0005269646644592285
        vf_loss: 12824.322265625
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0012600000482052565
        entropy: 2.182645797729492
        entropy_coeff: 0.0017600000137463212
        kl: 0.008637686260044575
        model: {}
        policy_loss: -0.0009001188445836306
        total_loss: 1.279212474822998
        vf_explained_var: -0.0006985962390899658
        vf_loss: 12822.265625
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0012600000482052565
        entropy: 2.184854507446289
        entropy_coeff: 0.0017600000137463212
        kl: 0.010742174461483955
        model: {}
        policy_loss: -0.003703818656504154
        total_loss: 1.272478699684143
        vf_explained_var: -0.0007236301898956299
        vf_loss: 12778.7939453125
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0012600000482052565
        entropy: 2.185420036315918
        entropy_coeff: 0.0017600000137463212
        kl: 0.009494200348854065
        model: {}
        policy_loss: -0.0020586992613971233
        total_loss: 1.357284426689148
        vf_explained_var: -0.0005345046520233154
        vf_loss: 13612.908203125
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0012600000482052565
        entropy: 2.1911911964416504
        entropy_coeff: 0.0017600000137463212
        kl: 0.005420736037194729
        model: {}
        policy_loss: -0.0014578034169971943
        total_loss: 1.3467367887496948
        vf_explained_var: -0.000729292631149292
        vf_loss: 13509.669921875
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0012600000482052565
        entropy: 2.1861181259155273
        entropy_coeff: 0.0017600000137463212
        kl: 0.009546598419547081
        model: {}
        policy_loss: -0.0020748022943735123
        total_loss: 1.3657699823379517
        vf_explained_var: -0.0005672574043273926
        vf_loss: 13697.830078125
    load_time_ms: 45811.914
    num_steps_sampled: 96000
    num_steps_trained: 96000
    sample_time_ms: 123244.557
    update_time_ms: 3980.812
  iterations_since_restore: 1
  node_ip: 172.17.8.60
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.624829931972787
    ram_util_percent: 11.4
  pid: 30197
  policy_reward_max:
    agent-0: -613.9999999999991
    agent-1: -613.9999999999991
    agent-2: -613.9999999999991
    agent-3: -485.99999999999807
    agent-4: -485.99999999999807
    agent-5: -485.99999999999807
  policy_reward_mean:
    agent-0: -1151.3750000000002
    agent-1: -1151.3750000000002
    agent-2: -1151.3750000000002
    agent-3: -1175.2013888888894
    agent-4: -1175.2013888888894
    agent-5: -1175.2013888888894
  policy_reward_min:
    agent-0: -2003.3333333333285
    agent-1: -2003.3333333333285
    agent-2: -2003.3333333333285
    agent-3: -2096.999999999998
    agent-4: -2096.999999999998
    agent-5: -2096.999999999998
  sampler_perf:
    mean_env_wait_ms: 28.994176533076907
    mean_inference_ms: 15.109303312781174
    mean_processing_ms: 64.8644059410184
  time_since_restore: 200.27699208259583
  time_this_iter_s: 200.27699208259583
  time_total_s: 200.27699208259583
  timestamp: 1637185018
  timesteps_since_restore: 96000
  timesteps_this_iter: 96000
  timesteps_total: 96000
  training_iteration: 1
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_2teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+-------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |    ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+-------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.60:30197 |      1 |          200.277 | 96000 | -6979.73 |
+--------------------------------------+----------+-------------------+--------+------------------+-------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30197)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f41e0e81588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 2.89
    apples_agent-0_min: 0
    apples_agent-1_max: 20
    apples_agent-1_mean: 4.12
    apples_agent-1_min: 0
    apples_agent-2_max: 24
    apples_agent-2_mean: 3.91
    apples_agent-2_min: 0
    apples_agent-3_max: 30
    apples_agent-3_mean: 4.9
    apples_agent-3_min: 0
    apples_agent-4_max: 27
    apples_agent-4_mean: 4.12
    apples_agent-4_min: 0
    apples_agent-5_max: 32
    apples_agent-5_mean: 4.0
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 159
    cleaning_beam_agent-0_mean: 129.77
    cleaning_beam_agent-0_min: 102
    cleaning_beam_agent-1_max: 134
    cleaning_beam_agent-1_mean: 114.9
    cleaning_beam_agent-1_min: 86
    cleaning_beam_agent-2_max: 150
    cleaning_beam_agent-2_mean: 119.05
    cleaning_beam_agent-2_min: 91
    cleaning_beam_agent-3_max: 138
    cleaning_beam_agent-3_mean: 108.81
    cleaning_beam_agent-3_min: 79
    cleaning_beam_agent-4_max: 135
    cleaning_beam_agent-4_mean: 110.22
    cleaning_beam_agent-4_min: 88
    cleaning_beam_agent-5_max: 128
    cleaning_beam_agent-5_mean: 108.07
    cleaning_beam_agent-5_min: 86
    fire_beam_agent-0_max: 134
    fire_beam_agent-0_mean: 89.23
    fire_beam_agent-0_min: 70
    fire_beam_agent-1_max: 126
    fire_beam_agent-1_mean: 101.3
    fire_beam_agent-1_min: 79
    fire_beam_agent-2_max: 119
    fire_beam_agent-2_mean: 101.24
    fire_beam_agent-2_min: 82
    fire_beam_agent-3_max: 120
    fire_beam_agent-3_mean: 84.1
    fire_beam_agent-3_min: 64
    fire_beam_agent-4_max: 128
    fire_beam_agent-4_mean: 84.52
    fire_beam_agent-4_min: 63
    fire_beam_agent-5_max: 130
    fire_beam_agent-5_mean: 106.16
    fire_beam_agent-5_min: 78
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-17_16-40-18
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: -2978.0000000000227
  episode_reward_mean: -5978.139999999967
  episode_reward_min: -10339.00000000006
  episodes_this_iter: 96
  episodes_total: 192
  experiment_id: 79b578a554754ec4a502419d8b89326f
  experiment_tag: '0'
  hostname: gpu060
  info:
    grad_time_ms: 18388.803
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.001254009548574686
        entropy: 2.183828353881836
        entropy_coeff: 0.0017600000137463212
        kl: 0.004541321657598019
        model: {}
        policy_loss: -0.0011388682760298252
        total_loss: 0.7772670388221741
        vf_explained_var: -0.0007911324501037598
        vf_loss: 7813.41259765625
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.001254009548574686
        entropy: 2.1857309341430664
        entropy_coeff: 0.0017600000137463212
        kl: 0.00455935113132
        model: {}
        policy_loss: -0.0015005506575107574
        total_loss: 0.7810218334197998
        vf_explained_var: -0.0007290542125701904
        vf_loss: 7854.57373046875
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.001254009548574686
        entropy: 2.1809425354003906
        entropy_coeff: 0.0017600000137463212
        kl: 0.0029785276856273413
        model: {}
        policy_loss: -0.0007788683287799358
        total_loss: 0.7749238610267639
        vf_explained_var: -0.0007916092872619629
        vf_loss: 7789.45458984375
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.001254009548574686
        entropy: 2.1798598766326904
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012462242739275098
        model: {}
        policy_loss: -0.00013923179358243942
        total_loss: 0.7281532287597656
        vf_explained_var: -0.0007843375205993652
        vf_loss: 7318.79736328125
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.001254009548574686
        entropy: 2.1769142150878906
        entropy_coeff: 0.0017600000137463212
        kl: 0.006008964031934738
        model: {}
        policy_loss: -0.0018242187798023224
        total_loss: 0.7299173474311829
        vf_explained_var: -0.0006496310234069824
        vf_loss: 7343.7119140625
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.001254009548574686
        entropy: 2.1869966983795166
        entropy_coeff: 0.0017600000137463212
        kl: 0.0028266608715057373
        model: {}
        policy_loss: -0.00030355434864759445
        total_loss: 0.7354140281677246
        vf_explained_var: -0.0007635056972503662
        vf_loss: 7390.013671875
    load_time_ms: 41829.101
    num_steps_sampled: 192000
    num_steps_trained: 192000
    sample_time_ms: 134231.291
    update_time_ms: 2035.782
  iterations_since_restore: 2
  node_ip: 172.17.8.60
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.275087719298245
    ram_util_percent: 12.439649122807019
  pid: 30197
  policy_reward_max:
    agent-0: -470.3333333333314
    agent-1: -470.3333333333314
    agent-2: -470.3333333333314
    agent-3: -369.00000000000085
    agent-4: -369.00000000000085
    agent-5: -369.00000000000085
  policy_reward_mean:
    agent-0: -1008.6833333333343
    agent-1: -1008.6833333333343
    agent-2: -1008.6833333333343
    agent-3: -984.0300000000005
    agent-4: -984.0300000000005
    agent-5: -984.0300000000005
  policy_reward_min:
    agent-0: -1792.9999999999966
    agent-1: -1792.9999999999966
    agent-2: -1792.9999999999966
    agent-3: -1780.6666666666606
    agent-4: -1780.6666666666606
    agent-5: -1780.6666666666606
  sampler_perf:
    mean_env_wait_ms: 29.44452997852908
    mean_inference_ms: 15.152562189586366
    mean_processing_ms: 65.67253566216075
  time_since_restore: 396.8992884159088
  time_this_iter_s: 196.622296333313
  time_total_s: 396.8992884159088
  timestamp: 1637185218
  timesteps_since_restore: 192000
  timesteps_this_iter: 96000
  timesteps_total: 192000
  training_iteration: 2
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_2teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+--------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |     ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+--------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.60:30197 |      2 |          396.899 | 192000 | -5978.14 |
+--------------------------------------+----------+-------------------+--------+------------------+--------+----------+


[2m[36m(pid=30197)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f41e0e81588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 3.79
    apples_agent-0_min: 0
    apples_agent-1_max: 26
    apples_agent-1_mean: 4.29
    apples_agent-1_min: 0
    apples_agent-2_max: 29
    apples_agent-2_mean: 4.39
    apples_agent-2_min: 0
    apples_agent-3_max: 30
    apples_agent-3_mean: 4.38
    apples_agent-3_min: 0
    apples_agent-4_max: 24
    apples_agent-4_mean: 3.84
    apples_agent-4_min: 0
    apples_agent-5_max: 27
    apples_agent-5_mean: 4.0
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 149
    cleaning_beam_agent-0_mean: 125.96
    cleaning_beam_agent-0_min: 108
    cleaning_beam_agent-1_max: 137
    cleaning_beam_agent-1_mean: 113.41
    cleaning_beam_agent-1_min: 88
    cleaning_beam_agent-2_max: 154
    cleaning_beam_agent-2_mean: 120.73
    cleaning_beam_agent-2_min: 96
    cleaning_beam_agent-3_max: 138
    cleaning_beam_agent-3_mean: 109.7
    cleaning_beam_agent-3_min: 89
    cleaning_beam_agent-4_max: 147
    cleaning_beam_agent-4_mean: 110.04
    cleaning_beam_agent-4_min: 82
    cleaning_beam_agent-5_max: 151
    cleaning_beam_agent-5_mean: 122.75
    cleaning_beam_agent-5_min: 89
    fire_beam_agent-0_max: 106
    fire_beam_agent-0_mean: 71.92
    fire_beam_agent-0_min: 56
    fire_beam_agent-1_max: 109
    fire_beam_agent-1_mean: 85.71
    fire_beam_agent-1_min: 61
    fire_beam_agent-2_max: 106
    fire_beam_agent-2_mean: 82.71
    fire_beam_agent-2_min: 57
    fire_beam_agent-3_max: 98
    fire_beam_agent-3_mean: 76.2
    fire_beam_agent-3_min: 61
    fire_beam_agent-4_max: 102
    fire_beam_agent-4_mean: 65.04
    fire_beam_agent-4_min: 44
    fire_beam_agent-5_max: 119
    fire_beam_agent-5_mean: 97.39
    fire_beam_agent-5_min: 75
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-17_16-43-35
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: -2849.0000000000027
  episode_reward_mean: -4806.909999999998
  episode_reward_min: -8242.999999999902
  episodes_this_iter: 96
  episodes_total: 288
  experiment_id: 79b578a554754ec4a502419d8b89326f
  experiment_tag: '0'
  hostname: gpu060
  info:
    grad_time_ms: 16623.287
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0012480191653594375
        entropy: 2.1676838397979736
        entropy_coeff: 0.0017600000137463212
        kl: 0.006585358642041683
        model: {}
        policy_loss: -0.0007997526554390788
        total_loss: 0.3968524932861328
        vf_explained_var: -0.0016414821147918701
        vf_loss: 4008.088623046875
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0012480191653594375
        entropy: 2.182889938354492
        entropy_coeff: 0.0017600000137463212
        kl: 0.004661469254642725
        model: {}
        policy_loss: -0.0014194874092936516
        total_loss: 0.3956003785133362
        vf_explained_var: -0.001557469367980957
        vf_loss: 4003.956298828125
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0012480191653594375
        entropy: 2.172910213470459
        entropy_coeff: 0.0017600000137463212
        kl: 0.007053624838590622
        model: {}
        policy_loss: -0.0021436638198792934
        total_loss: 0.3931354284286499
        vf_explained_var: -0.001660376787185669
        vf_loss: 3983.98095703125
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0012480191653594375
        entropy: 2.1619482040405273
        entropy_coeff: 0.0017600000137463212
        kl: 0.004335787147283554
        model: {}
        policy_loss: -0.0009490458760410547
        total_loss: 0.363141804933548
        vf_explained_var: -0.0018624961376190186
        vf_loss: 3674.623046875
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0012480191653594375
        entropy: 2.1729893684387207
        entropy_coeff: 0.0017600000137463212
        kl: 0.002082226797938347
        model: {}
        policy_loss: -0.0002492950879968703
        total_loss: 0.3666645288467407
        vf_explained_var: -0.001492619514465332
        vf_loss: 3703.218505859375
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0012480191653594375
        entropy: 2.1801798343658447
        entropy_coeff: 0.0017600000137463212
        kl: 0.007073272485285997
        model: {}
        policy_loss: -0.0013527215924113989
        total_loss: 0.36654454469680786
        vf_explained_var: -0.0017532408237457275
        vf_loss: 3710.2705078125
    load_time_ms: 38879.88
    num_steps_sampled: 288000
    num_steps_trained: 288000
    sample_time_ms: 139678.133
    update_time_ms: 1379.553
  iterations_since_restore: 3
  node_ip: 172.17.8.60
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.24341637010676
    ram_util_percent: 12.21387900355872
  pid: 30197
  policy_reward_max:
    agent-0: -391.6666666666651
    agent-1: -391.6666666666651
    agent-2: -391.6666666666651
    agent-3: -415.33333333333286
    agent-4: -415.33333333333286
    agent-5: -415.33333333333286
  policy_reward_mean:
    agent-0: -816.3800000000006
    agent-1: -816.3800000000006
    agent-2: -816.3800000000006
    agent-3: -785.9233333333339
    agent-4: -785.9233333333339
    agent-5: -785.9233333333339
  policy_reward_min:
    agent-0: -1457.9999999999968
    agent-1: -1457.9999999999968
    agent-2: -1457.9999999999968
    agent-3: -1463.3333333333337
    agent-4: -1463.3333333333337
    agent-5: -1463.3333333333337
  sampler_perf:
    mean_env_wait_ms: 29.403970516622863
    mean_inference_ms: 15.046478747164342
    mean_processing_ms: 65.8112271612926
  time_since_restore: 593.71351313591
  time_this_iter_s: 196.81422472000122
  time_total_s: 593.71351313591
  timestamp: 1637185415
  timesteps_since_restore: 288000
  timesteps_this_iter: 96000
  timesteps_total: 288000
  training_iteration: 3
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_2teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+--------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |     ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+--------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.60:30197 |      3 |          593.714 | 288000 | -4806.91 |
+--------------------------------------+----------+-------------------+--------+------------------+--------+----------+


[2m[36m(pid=30197)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f41e0e81588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 31
    apples_agent-0_mean: 3.55
    apples_agent-0_min: 0
    apples_agent-1_max: 23
    apples_agent-1_mean: 3.83
    apples_agent-1_min: 0
    apples_agent-2_max: 17
    apples_agent-2_mean: 3.04
    apples_agent-2_min: 0
    apples_agent-3_max: 23
    apples_agent-3_mean: 4.68
    apples_agent-3_min: 0
    apples_agent-4_max: 21
    apples_agent-4_mean: 3.88
    apples_agent-4_min: 0
    apples_agent-5_max: 21
    apples_agent-5_mean: 4.05
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 147
    cleaning_beam_agent-0_mean: 119.74
    cleaning_beam_agent-0_min: 90
    cleaning_beam_agent-1_max: 143
    cleaning_beam_agent-1_mean: 120.89
    cleaning_beam_agent-1_min: 96
    cleaning_beam_agent-2_max: 145
    cleaning_beam_agent-2_mean: 116.45
    cleaning_beam_agent-2_min: 92
    cleaning_beam_agent-3_max: 122
    cleaning_beam_agent-3_mean: 96.34
    cleaning_beam_agent-3_min: 74
    cleaning_beam_agent-4_max: 122
    cleaning_beam_agent-4_mean: 102.12
    cleaning_beam_agent-4_min: 67
    cleaning_beam_agent-5_max: 161
    cleaning_beam_agent-5_mean: 131.83
    cleaning_beam_agent-5_min: 108
    fire_beam_agent-0_max: 85
    fire_beam_agent-0_mean: 62.8
    fire_beam_agent-0_min: 44
    fire_beam_agent-1_max: 90
    fire_beam_agent-1_mean: 67.56
    fire_beam_agent-1_min: 48
    fire_beam_agent-2_max: 100
    fire_beam_agent-2_mean: 74.84
    fire_beam_agent-2_min: 51
    fire_beam_agent-3_max: 98
    fire_beam_agent-3_mean: 57.59
    fire_beam_agent-3_min: 42
    fire_beam_agent-4_max: 91
    fire_beam_agent-4_mean: 67.29
    fire_beam_agent-4_min: 51
    fire_beam_agent-5_max: 109
    fire_beam_agent-5_mean: 86.09
    fire_beam_agent-5_min: 64
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-17_16-46-48
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: -2372.9999999999945
  episode_reward_mean: -4255.760000000018
  episode_reward_min: -7684.999999999898
  episodes_this_iter: 96
  episodes_total: 384
  experiment_id: 79b578a554754ec4a502419d8b89326f
  experiment_tag: '0'
  hostname: gpu060
  info:
    grad_time_ms: 15767.28
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0012420287821441889
        entropy: 2.16341495513916
        entropy_coeff: 0.0017600000137463212
        kl: 0.00580952363088727
        model: {}
        policy_loss: -0.001566362101584673
        total_loss: 0.26418617367744446
        vf_explained_var: -0.002389758825302124
        vf_loss: 2689.7919921875
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012420287821441889
        entropy: 2.1627209186553955
        entropy_coeff: 0.0017600000137463212
        kl: 0.008259042166173458
        model: {}
        policy_loss: -0.0009230095893144608
        total_loss: 0.26427584886550903
        vf_explained_var: -0.0021983087062835693
        vf_loss: 2685.9228515625
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0012420287821441889
        entropy: 2.159046173095703
        entropy_coeff: 0.0017600000137463212
        kl: 0.007674522697925568
        model: {}
        policy_loss: -0.0015286770649254322
        total_loss: 0.26346611976623535
        vf_explained_var: -0.0023332834243774414
        vf_loss: 2680.27294921875
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012420287821441889
        entropy: 2.1588144302368164
        entropy_coeff: 0.0017600000137463212
        kl: 0.004551255144178867
        model: {}
        policy_loss: -0.00044581759721040726
        total_loss: 0.2245490550994873
        vf_explained_var: -0.002243548631668091
        vf_loss: 2285.66845703125
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0012420287821441889
        entropy: 2.1680798530578613
        entropy_coeff: 0.0017600000137463212
        kl: 0.006221157964318991
        model: {}
        policy_loss: -0.0021725352853536606
        total_loss: 0.22545307874679565
        vf_explained_var: -0.0019104182720184326
        vf_loss: 2308.193115234375
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0012420287821441889
        entropy: 2.171177387237549
        entropy_coeff: 0.0017600000137463212
        kl: 0.00672657135874033
        model: {}
        policy_loss: -0.0018231417052447796
        total_loss: 0.22462250292301178
        vf_explained_var: -0.0021986663341522217
        vf_loss: 2295.9423828125
    load_time_ms: 39006.299
    num_steps_sampled: 384000
    num_steps_trained: 384000
    sample_time_ms: 139744.701
    update_time_ms: 1049.163
  iterations_since_restore: 4
  node_ip: 172.17.8.60
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.495620437956205
    ram_util_percent: 12.180291970802918
  pid: 30197
  policy_reward_max:
    agent-0: -340.666666666666
    agent-1: -340.666666666666
    agent-2: -340.666666666666
    agent-3: -297.33333333333303
    agent-4: -297.33333333333303
    agent-5: -297.33333333333303
  policy_reward_mean:
    agent-0: -726.5600000000006
    agent-1: -726.5600000000006
    agent-2: -726.5600000000006
    agent-3: -692.026666666667
    agent-4: -692.026666666667
    agent-5: -692.026666666667
  policy_reward_min:
    agent-0: -1352.6666666666652
    agent-1: -1352.6666666666652
    agent-2: -1352.6666666666652
    agent-3: -1209.0
    agent-4: -1209.0
    agent-5: -1209.0
  sampler_perf:
    mean_env_wait_ms: 29.30110417430803
    mean_inference_ms: 14.983272936090154
    mean_processing_ms: 65.88519903356305
  time_since_restore: 786.415673494339
  time_this_iter_s: 192.70216035842896
  time_total_s: 786.415673494339
  timestamp: 1637185608
  timesteps_since_restore: 384000
  timesteps_this_iter: 96000
  timesteps_total: 384000
  training_iteration: 4
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_2teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+--------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |     ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+--------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.60:30197 |      4 |          786.416 | 384000 | -4255.76 |
+--------------------------------------+----------+-------------------+--------+------------------+--------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 21
    apples_agent-0_mean: 3.97
    apples_agent-0_min: 0
    apples_agent-1_max: 29
    apples_agent-1_mean: 4.2
    apples_agent-1_min: 0
    apples_agent-2_max: 16
    apples_agent-2_mean: 4.0
    apples_agent-2_min: 0
    apples_agent-3_max: 23
    apples_agent-3_mean: 4.2
    apples_agent-3_min: 0
    apples_agent-4_max: 38
    apples_agent-4_mean: 4.1
    apples_agent-4_min: 0
    apples_agent-5_max: 16
    apples_agent-5_mean: 3.91
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 148
    cleaning_beam_agent-0_mean: 123.39
    cleaning_beam_agent-0_min: 101
    cleaning_beam_agent-1_max: 156
    cleaning_beam_agent-1_mean: 124.05
    cleaning_beam_agent-1_min: 95
    cleaning_beam_agent-2_max: 153
    cleaning_beam_agent-2_mean: 126.69
    cleaning_beam_agent-2_min: 102
    cleaning_beam_agent-3_max: 105
    cleaning_beam_agent-3_mean: 83.67
    cleaning_beam_agent-3_min: 65
    cleaning_beam_agent-4_max: 116
    cleaning_beam_agent-4_mean: 87.48
    cleaning_beam_agent-4_min: 69
    cleaning_beam_agent-5_max: 152
    cleaning_beam_agent-5_mean: 128.7
    cleaning_beam_agent-5_min: 95
    fire_beam_agent-0_max: 64
    fire_beam_agent-0_mean: 46.96
    fire_beam_agent-0_min: 28
    fire_beam_agent-1_max: 76
    fire_beam_agent-1_mean: 50.38
    fire_beam_agent-1_min: 34
    fire_beam_agent-2_max: 85
    fire_beam_agent-2_mean: 55.68
    fire_beam_agent-2_min: 33
    fire_beam_agent-3_max: 65
    fire_beam_agent-3_mean: 53.44
    fire_beam_agent-3_min: 34
    fire_beam_agent-4_max: 77
    fire_beam_agent-4_mean: 48.54
    fire_beam_agent-4_min: 33
    fire_beam_agent-5_max: 102
    fire_beam_agent-5_mean: 67.81
    fire_beam_agent-5_min: 50
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-17_16-49-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: -1642.9999999999864
  episode_reward_mean: -3381.0700000000147
  episode_reward_min: -5838.9999999999345
  episodes_this_iter: 96
  episodes_total: 480
  experiment_id: 79b578a554754ec4a502419d8b89326f
  experiment_tag: '0'
  hostname: gpu060
  info:
    grad_time_ms: 15271.731
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0012360383989289403
        entropy: 2.1498732566833496
        entropy_coeff: 0.0017600000137463212
        kl: 0.002235406544059515
        model: {}
        policy_loss: -0.0008878645021468401
        total_loss: 0.13849353790283203
        vf_explained_var: -0.0026769042015075684
        vf_loss: 1429.4163818359375
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012360383989289403
        entropy: 2.144317150115967
        entropy_coeff: 0.0017600000137463212
        kl: 0.010061427019536495
        model: {}
        policy_loss: -0.0016432981938123703
        total_loss: 0.1378108114004135
        vf_explained_var: -0.0026310384273529053
        vf_loss: 1427.25048828125
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0012360383989289403
        entropy: 2.141054391860962
        entropy_coeff: 0.0017600000137463212
        kl: 0.003481340128928423
        model: {}
        policy_loss: -0.0011240728199481964
        total_loss: 0.1380680948495865
        vf_explained_var: -0.002488553524017334
        vf_loss: 1426.123046875
      agent-3:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0012360383989289403
        entropy: 2.1466689109802246
        entropy_coeff: 0.0017600000137463212
        kl: 0.003847871907055378
        model: {}
        policy_loss: -0.0008431448368355632
        total_loss: 0.13376036286354065
        vf_explained_var: -0.0026989877223968506
        vf_loss: 1382.8546142578125
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0012360383989289403
        entropy: 2.145434856414795
        entropy_coeff: 0.0017600000137463212
        kl: 0.004266155883669853
        model: {}
        policy_loss: -0.001180419116280973
        total_loss: 0.1355217844247818
        vf_explained_var: -0.002231389284133911
        vf_loss: 1400.515625
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0012360383989289403
        entropy: 2.163546085357666
        entropy_coeff: 0.0017600000137463212
        kl: 0.0072485641576349735
        model: {}
        policy_loss: -0.0018064994364976883
        total_loss: 0.1332988142967224
        vf_explained_var: -0.0026859939098358154
        vf_loss: 1381.883056640625
    load_time_ms: 35921.328
    num_steps_sampled: 480000
    num_steps_trained: 480000
    sample_time_ms: 142338.687
    update_time_ms: 853.362
  iterations_since_restore: 5
  node_ip: 172.17.8.60
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.755350553505533
    ram_util_percent: 12.072693726937269
  pid: 30197
  policy_reward_max:
    agent-0: -173.00000000000009
    agent-1: -173.00000000000009
    agent-2: -173.00000000000009
    agent-3: -237.33333333333377
    agent-4: -237.33333333333377
    agent-5: -237.33333333333377
  policy_reward_mean:
    agent-0: -567.45
    agent-1: -567.45
    agent-2: -567.45
    agent-3: -559.5733333333333
    agent-4: -559.5733333333333
    agent-5: -559.5733333333333
  policy_reward_min:
    agent-0: -1274.3333333333323
    agent-1: -1274.3333333333323
    agent-2: -1274.3333333333323
    agent-3: -1008.3333333333344
    agent-4: -1008.3333333333344
    agent-5: -1008.3333333333344
  sampler_perf:
    mean_env_wait_ms: 29.168193326606833
    mean_inference_ms: 14.959256754321036
    mean_processing_ms: 65.97838330806542
  time_since_restore: 976.2561120986938
  time_this_iter_s: 189.84043860435486
  time_total_s: 976.2561120986938
  timestamp: 1637185798
  timesteps_since_restore: 480000
  timesteps_this_iter: 96000
  timesteps_total: 480000
  training_iteration: 5
  trial_id: '00000'
  
[2m[36m(pid=30197)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f41e0e81588> -> 96 episodes
== Status ==
Memory usage on this node: 21.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_2teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+--------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |     ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+--------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.60:30197 |      5 |          976.256 | 480000 | -3381.07 |
+--------------------------------------+----------+-------------------+--------+------------------+--------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30197)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f41e0e81588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 4.35
    apples_agent-0_min: 0
    apples_agent-1_max: 21
    apples_agent-1_mean: 4.37
    apples_agent-1_min: 0
    apples_agent-2_max: 31
    apples_agent-2_mean: 4.53
    apples_agent-2_min: 0
    apples_agent-3_max: 27
    apples_agent-3_mean: 4.6
    apples_agent-3_min: 0
    apples_agent-4_max: 26
    apples_agent-4_mean: 4.16
    apples_agent-4_min: 0
    apples_agent-5_max: 25
    apples_agent-5_mean: 3.31
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 163
    cleaning_beam_agent-0_mean: 131.4
    cleaning_beam_agent-0_min: 102
    cleaning_beam_agent-1_max: 135
    cleaning_beam_agent-1_mean: 113.18
    cleaning_beam_agent-1_min: 84
    cleaning_beam_agent-2_max: 162
    cleaning_beam_agent-2_mean: 134.98
    cleaning_beam_agent-2_min: 108
    cleaning_beam_agent-3_max: 95
    cleaning_beam_agent-3_mean: 74.16
    cleaning_beam_agent-3_min: 55
    cleaning_beam_agent-4_max: 116
    cleaning_beam_agent-4_mean: 95.11
    cleaning_beam_agent-4_min: 66
    cleaning_beam_agent-5_max: 156
    cleaning_beam_agent-5_mean: 129.28
    cleaning_beam_agent-5_min: 108
    fire_beam_agent-0_max: 53
    fire_beam_agent-0_mean: 34.55
    fire_beam_agent-0_min: 21
    fire_beam_agent-1_max: 63
    fire_beam_agent-1_mean: 40.61
    fire_beam_agent-1_min: 25
    fire_beam_agent-2_max: 69
    fire_beam_agent-2_mean: 43.76
    fire_beam_agent-2_min: 24
    fire_beam_agent-3_max: 62
    fire_beam_agent-3_mean: 46.15
    fire_beam_agent-3_min: 23
    fire_beam_agent-4_max: 55
    fire_beam_agent-4_mean: 39.99
    fire_beam_agent-4_min: 27
    fire_beam_agent-5_max: 75
    fire_beam_agent-5_mean: 49.95
    fire_beam_agent-5_min: 37
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-17_16-53-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: -1079.0000000000061
  episode_reward_mean: -2485.5799999999995
  episode_reward_min: -5838.9999999999345
  episodes_this_iter: 96
  episodes_total: 576
  experiment_id: 79b578a554754ec4a502419d8b89326f
  experiment_tag: '0'
  hostname: gpu060
  info:
    grad_time_ms: 14944.333
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012300480157136917
        entropy: 2.1348748207092285
        entropy_coeff: 0.0017600000137463212
        kl: 0.006633767858147621
        model: {}
        policy_loss: -0.0008001655805855989
        total_loss: 0.054329391568899155
        vf_explained_var: -0.0034086406230926514
        vf_loss: 585.552490234375
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012300480157136917
        entropy: 2.0841259956359863
        entropy_coeff: 0.0017600000137463212
        kl: 0.022372454404830933
        model: {}
        policy_loss: 0.000990736298263073
        total_loss: 0.05706179887056351
        vf_explained_var: -0.0030838847160339355
        vf_loss: 586.2049560546875
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012300480157136917
        entropy: 2.136475086212158
        entropy_coeff: 0.0017600000137463212
        kl: 0.012947448529303074
        model: {}
        policy_loss: 0.0005285923834890127
        total_loss: 0.05590976029634476
        vf_explained_var: -0.0031273066997528076
        vf_loss: 584.9400634765625
      agent-3:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 0.0012300480157136917
        entropy: 2.1281092166900635
        entropy_coeff: 0.0017600000137463212
        kl: 0.02186853252351284
        model: {}
        policy_loss: -0.000753528147470206
        total_loss: 0.05974718555808067
        vf_explained_var: -0.003952234983444214
        vf_loss: 639.728271484375
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012300480157136917
        entropy: 2.1195478439331055
        entropy_coeff: 0.0017600000137463212
        kl: 0.011648427695035934
        model: {}
        policy_loss: -0.0005263593047857285
        total_loss: 0.06067174673080444
        vf_explained_var: -0.0031034350395202637
        vf_loss: 643.4608764648438
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0012300480157136917
        entropy: 2.1541905403137207
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022596041671931744
        model: {}
        policy_loss: -0.0007858206517994404
        total_loss: 0.05953006446361542
        vf_explained_var: -0.0035959482192993164
        vf_loss: 638.8130493164062
    load_time_ms: 34439.189
    num_steps_sampled: 576000
    num_steps_trained: 576000
    sample_time_ms: 143555.546
    update_time_ms: 721.944
  iterations_since_restore: 6
  node_ip: 172.17.8.60
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.56531365313653
    ram_util_percent: 12.057564575645758
  pid: 30197
  policy_reward_max:
    agent-0: -121.33333333333303
    agent-1: -121.33333333333303
    agent-2: -121.33333333333303
    agent-3: -181.666666666667
    agent-4: -181.666666666667
    agent-5: -181.666666666667
  policy_reward_mean:
    agent-0: -409.51999999999987
    agent-1: -409.51999999999987
    agent-2: -409.51999999999987
    agent-3: -419.0066666666665
    agent-4: -419.0066666666665
    agent-5: -419.0066666666665
  policy_reward_min:
    agent-0: -1274.3333333333323
    agent-1: -1274.3333333333323
    agent-2: -1274.3333333333323
    agent-3: -904.0000000000023
    agent-4: -904.0000000000023
    agent-5: -904.0000000000023
  sampler_perf:
    mean_env_wait_ms: 29.03859267196507
    mean_inference_ms: 14.923736624897662
    mean_processing_ms: 66.03141972606024
  time_since_restore: 1166.3864121437073
  time_this_iter_s: 190.13030004501343
  time_total_s: 1166.3864121437073
  timestamp: 1637185989
  timesteps_since_restore: 576000
  timesteps_this_iter: 96000
  timesteps_total: 576000
  training_iteration: 6
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_2teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+--------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |     ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+--------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.60:30197 |      6 |          1166.39 | 576000 | -2485.58 |
+--------------------------------------+----------+-------------------+--------+------------------+--------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 3.56
    apples_agent-0_min: 0
    apples_agent-1_max: 20
    apples_agent-1_mean: 3.93
    apples_agent-1_min: 0
    apples_agent-2_max: 37
    apples_agent-2_mean: 5.33
    apples_agent-2_min: 0
    apples_agent-3_max: 31
    apples_agent-3_mean: 4.41
    apples_agent-3_min: 0
    apples_agent-4_max: 32
    apples_agent-4_mean: 3.75
    apples_agent-4_min: 0
    apples_agent-5_max: 25
    apples_agent-5_mean: 3.6
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 169
    cleaning_beam_agent-0_mean: 139.82
    cleaning_beam_agent-0_min: 116
    cleaning_beam_agent-1_max: 162
    cleaning_beam_agent-1_mean: 122.27
    cleaning_beam_agent-1_min: 100
    cleaning_beam_agent-2_max: 162
    cleaning_beam_agent-2_mean: 104.34
    cleaning_beam_agent-2_min: 84
    cleaning_beam_agent-3_max: 107
    cleaning_beam_agent-3_mean: 86.3
    cleaning_beam_agent-3_min: 55
    cleaning_beam_agent-4_max: 107
    cleaning_beam_agent-4_mean: 79.37
    cleaning_beam_agent-4_min: 55
    cleaning_beam_agent-5_max: 155
    cleaning_beam_agent-5_mean: 125.86
    cleaning_beam_agent-5_min: 106
    fire_beam_agent-0_max: 41
    fire_beam_agent-0_mean: 24.05
    fire_beam_agent-0_min: 12
    fire_beam_agent-1_max: 49
    fire_beam_agent-1_mean: 29.01
    fire_beam_agent-1_min: 17
    fire_beam_agent-2_max: 49
    fire_beam_agent-2_mean: 34.8
    fire_beam_agent-2_min: 20
    fire_beam_agent-3_max: 62
    fire_beam_agent-3_mean: 31.62
    fire_beam_agent-3_min: 18
    fire_beam_agent-4_max: 48
    fire_beam_agent-4_mean: 33.28
    fire_beam_agent-4_min: 19
    fire_beam_agent-5_max: 63
    fire_beam_agent-5_mean: 39.06
    fire_beam_agent-5_min: 25
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-17_16-56-10
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: -967.0000000000081
  episode_reward_mean: -2001.799999999997
  episode_reward_min: -3681.0000000000036
  episodes_this_iter: 96
  episodes_total: 672
  experiment_id: 79b578a554754ec4a502419d8b89326f
  experiment_tag: '0'
  hostname: gpu060
  info:
    grad_time_ms: 14687.5
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012240576324984431
        entropy: 2.093815326690674
        entropy_coeff: 0.0017600000137463212
        kl: 0.01723065786063671
        model: {}
        policy_loss: -0.0004641816485673189
        total_loss: 0.0458206832408905
        vf_explained_var: -0.004122436046600342
        vf_loss: 491.08453369140625
      agent-1:
        cur_kl_coeff: 0.07500000298023224
        cur_lr: 0.0012240576324984431
        entropy: 2.093463897705078
        entropy_coeff: 0.0017600000137463212
        kl: 0.00853144284337759
        model: {}
        policy_loss: -0.0016240549739450216
        total_loss: 0.04438722878694534
        vf_explained_var: -0.0037194490432739258
        vf_loss: 490.5592346191406
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012240576324984431
        entropy: 2.1252522468566895
        entropy_coeff: 0.0017600000137463212
        kl: 0.01217464730143547
        model: {}
        policy_loss: 9.825918823480606e-05
        total_loss: 0.04608621075749397
        vf_explained_var: -0.003917574882507324
        vf_loss: 491.1966552734375
      agent-3:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0012240576324984431
        entropy: 2.122431755065918
        entropy_coeff: 0.0017600000137463212
        kl: 0.01898161694407463
        model: {}
        policy_loss: -0.0016423917841166258
        total_loss: 0.045385390520095825
        vf_explained_var: -0.0036153197288513184
        vf_loss: 504.0735778808594
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012240576324984431
        entropy: 2.0960440635681152
        entropy_coeff: 0.0017600000137463212
        kl: 0.005328906234353781
        model: {}
        policy_loss: -0.0014229386579245329
        total_loss: 0.045389559119939804
        vf_explained_var: -0.002909332513809204
        vf_loss: 502.3509521484375
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012240576324984431
        entropy: 2.1310958862304688
        entropy_coeff: 0.0017600000137463212
        kl: 0.006174403242766857
        model: {}
        policy_loss: -0.0010353182442486286
        total_loss: 0.04595203325152397
        vf_explained_var: -0.0035834014415740967
        vf_loss: 504.2935791015625
    load_time_ms: 33353.823
    num_steps_sampled: 672000
    num_steps_trained: 672000
    sample_time_ms: 143184.344
    update_time_ms: 631.85
  iterations_since_restore: 7
  node_ip: 172.17.8.60
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.219767441860466
    ram_util_percent: 12.075193798449611
  pid: 30197
  policy_reward_max:
    agent-0: -91.33333333333329
    agent-1: -91.33333333333329
    agent-2: -91.33333333333329
    agent-3: -96.99999999999991
    agent-4: -96.99999999999991
    agent-5: -96.99999999999991
  policy_reward_mean:
    agent-0: -334.43999999999994
    agent-1: -334.43999999999994
    agent-2: -334.43999999999994
    agent-3: -332.8266666666665
    agent-4: -332.8266666666665
    agent-5: -332.8266666666665
  policy_reward_min:
    agent-0: -677.6666666666667
    agent-1: -677.6666666666667
    agent-2: -677.6666666666667
    agent-3: -886.9999999999999
    agent-4: -886.9999999999999
    agent-5: -886.9999999999999
  sampler_perf:
    mean_env_wait_ms: 28.888971803388085
    mean_inference_ms: 14.911335008068214
    mean_processing_ms: 66.07014558204156
  time_since_restore: 1347.5484523773193
  time_this_iter_s: 181.16204023361206
  time_total_s: 1347.5484523773193
  timestamp: 1637186170
  timesteps_since_restore: 672000
  timesteps_this_iter: 96000
  timesteps_total: 672000
  training_iteration: 7
  trial_id: '00000'
  
[2m[36m(pid=30197)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f41e0e81588> -> 96 episodes
== Status ==
Memory usage on this node: 21.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_2teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+--------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |     ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+--------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.60:30197 |      7 |          1347.55 | 672000 |  -2001.8 |
+--------------------------------------+----------+-------------------+--------+------------------+--------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 3.24
    apples_agent-0_min: 0
    apples_agent-1_max: 32
    apples_agent-1_mean: 6.1
    apples_agent-1_min: 0
    apples_agent-2_max: 27
    apples_agent-2_mean: 4.44
    apples_agent-2_min: 0
    apples_agent-3_max: 28
    apples_agent-3_mean: 4.88
    apples_agent-3_min: 0
    apples_agent-4_max: 23
    apples_agent-4_mean: 4.45
    apples_agent-4_min: 0
    apples_agent-5_max: 24
    apples_agent-5_mean: 4.94
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 183
    cleaning_beam_agent-0_mean: 156.49
    cleaning_beam_agent-0_min: 129
    cleaning_beam_agent-1_max: 139
    cleaning_beam_agent-1_mean: 120.21
    cleaning_beam_agent-1_min: 96
    cleaning_beam_agent-2_max: 136
    cleaning_beam_agent-2_mean: 108.53
    cleaning_beam_agent-2_min: 86
    cleaning_beam_agent-3_max: 135
    cleaning_beam_agent-3_mean: 111.31
    cleaning_beam_agent-3_min: 76
    cleaning_beam_agent-4_max: 96
    cleaning_beam_agent-4_mean: 75.29
    cleaning_beam_agent-4_min: 56
    cleaning_beam_agent-5_max: 153
    cleaning_beam_agent-5_mean: 113.11
    cleaning_beam_agent-5_min: 88
    fire_beam_agent-0_max: 31
    fire_beam_agent-0_mean: 17.3
    fire_beam_agent-0_min: 9
    fire_beam_agent-1_max: 35
    fire_beam_agent-1_mean: 21.77
    fire_beam_agent-1_min: 11
    fire_beam_agent-2_max: 41
    fire_beam_agent-2_mean: 28.3
    fire_beam_agent-2_min: 14
    fire_beam_agent-3_max: 34
    fire_beam_agent-3_mean: 22.61
    fire_beam_agent-3_min: 11
    fire_beam_agent-4_max: 40
    fire_beam_agent-4_mean: 24.66
    fire_beam_agent-4_min: 12
    fire_beam_agent-5_max: 42
    fire_beam_agent-5_mean: 28.56
    fire_beam_agent-5_min: 13
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-17_16-59-05
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: -379.99999999999744
  episode_reward_mean: -1387.189999999999
  episode_reward_min: -2683.0000000000014
  episodes_this_iter: 96
  episodes_total: 768
  experiment_id: 79b578a554754ec4a502419d8b89326f
  experiment_tag: '0'
  hostname: gpu060
  info:
    grad_time_ms: 14485.536
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012180672492831945
        entropy: 2.090684413909912
        entropy_coeff: 0.0017600000137463212
        kl: 0.004663273226469755
        model: {}
        policy_loss: -0.0009698059875518084
        total_loss: 0.033955201506614685
        vf_explained_var: -0.0036624372005462646
        vf_loss: 383.7144775390625
      agent-1:
        cur_kl_coeff: 0.07500000298023224
        cur_lr: 0.0012180672492831945
        entropy: 2.1011369228363037
        entropy_coeff: 0.0017600000137463212
        kl: 0.003771076677367091
        model: {}
        policy_loss: -0.0009149843826889992
        total_loss: 0.03395245224237442
        vf_explained_var: -0.003292471170425415
        vf_loss: 382.82611083984375
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012180672492831945
        entropy: 2.1234333515167236
        entropy_coeff: 0.0017600000137463212
        kl: 0.006808724254369736
        model: {}
        policy_loss: -0.0005898728268221021
        total_loss: 0.03448134660720825
        vf_explained_var: -0.003536820411682129
        vf_loss: 384.6802978515625
      agent-3:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0012180672492831945
        entropy: 2.1104133129119873
        entropy_coeff: 0.0017600000137463212
        kl: 0.009051361121237278
        model: {}
        policy_loss: -0.0009773834608495235
        total_loss: 0.0302199088037014
        vf_explained_var: -0.003962188959121704
        vf_loss: 347.4190673828125
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012180672492831945
        entropy: 2.0713725090026855
        entropy_coeff: 0.0017600000137463212
        kl: 0.006535917520523071
        model: {}
        policy_loss: -0.0010431682458147407
        total_loss: 0.03030211478471756
        vf_explained_var: -0.00319826602935791
        vf_loss: 346.64105224609375
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012180672492831945
        entropy: 2.1142332553863525
        entropy_coeff: 0.0017600000137463212
        kl: 0.005340799689292908
        model: {}
        policy_loss: -0.0008035106584429741
        total_loss: 0.030915388837456703
        vf_explained_var: -0.0037807822227478027
        vf_loss: 351.72918701171875
    load_time_ms: 32415.459
    num_steps_sampled: 768000
    num_steps_trained: 768000
    sample_time_ms: 142220.042
    update_time_ms: 562.846
  iterations_since_restore: 8
  node_ip: 172.17.8.60
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.637751004016064
    ram_util_percent: 11.908835341365462
  pid: 30197
  policy_reward_max:
    agent-0: -54.000000000000085
    agent-1: -54.000000000000085
    agent-2: -54.000000000000085
    agent-3: -72.66666666666667
    agent-4: -72.66666666666667
    agent-5: -72.66666666666667
  policy_reward_mean:
    agent-0: -232.93
    agent-1: -232.93
    agent-2: -232.93
    agent-3: -229.46666666666667
    agent-4: -229.46666666666667
    agent-5: -229.46666666666667
  policy_reward_min:
    agent-0: -520.0
    agent-1: -520.0
    agent-2: -520.0
    agent-3: -537.6666666666665
    agent-4: -537.6666666666665
    agent-5: -537.6666666666665
  sampler_perf:
    mean_env_wait_ms: 28.74480801220419
    mean_inference_ms: 14.899724443710062
    mean_processing_ms: 66.0602323243177
  time_since_restore: 1522.1028118133545
  time_this_iter_s: 174.55435943603516
  time_total_s: 1522.1028118133545
  timestamp: 1637186345
  timesteps_since_restore: 768000
  timesteps_this_iter: 96000
  timesteps_total: 768000
  training_iteration: 8
  trial_id: '00000'
  
[2m[36m(pid=30197)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f41e0e81588> -> 96 episodes
== Status ==
Memory usage on this node: 21.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_2teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+--------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |     ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+--------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.60:30197 |      8 |           1522.1 | 768000 | -1387.19 |
+--------------------------------------+----------+-------------------+--------+------------------+--------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 25
    apples_agent-0_mean: 4.05
    apples_agent-0_min: 0
    apples_agent-1_max: 29
    apples_agent-1_mean: 5.77
    apples_agent-1_min: 0
    apples_agent-2_max: 27
    apples_agent-2_mean: 4.9
    apples_agent-2_min: 0
    apples_agent-3_max: 21
    apples_agent-3_mean: 4.17
    apples_agent-3_min: 0
    apples_agent-4_max: 25
    apples_agent-4_mean: 4.65
    apples_agent-4_min: 0
    apples_agent-5_max: 27
    apples_agent-5_mean: 5.4
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 217
    cleaning_beam_agent-0_mean: 182.65
    cleaning_beam_agent-0_min: 157
    cleaning_beam_agent-1_max: 157
    cleaning_beam_agent-1_mean: 133.16
    cleaning_beam_agent-1_min: 109
    cleaning_beam_agent-2_max: 182
    cleaning_beam_agent-2_mean: 148.51
    cleaning_beam_agent-2_min: 100
    cleaning_beam_agent-3_max: 141
    cleaning_beam_agent-3_mean: 113.84
    cleaning_beam_agent-3_min: 90
    cleaning_beam_agent-4_max: 87
    cleaning_beam_agent-4_mean: 69.78
    cleaning_beam_agent-4_min: 52
    cleaning_beam_agent-5_max: 128
    cleaning_beam_agent-5_mean: 104.41
    cleaning_beam_agent-5_min: 80
    fire_beam_agent-0_max: 24
    fire_beam_agent-0_mean: 12.53
    fire_beam_agent-0_min: 5
    fire_beam_agent-1_max: 29
    fire_beam_agent-1_mean: 15.04
    fire_beam_agent-1_min: 4
    fire_beam_agent-2_max: 48
    fire_beam_agent-2_mean: 31.1
    fire_beam_agent-2_min: 20
    fire_beam_agent-3_max: 31
    fire_beam_agent-3_mean: 17.18
    fire_beam_agent-3_min: 8
    fire_beam_agent-4_max: 33
    fire_beam_agent-4_mean: 15.74
    fire_beam_agent-4_min: 7
    fire_beam_agent-5_max: 35
    fire_beam_agent-5_mean: 19.89
    fire_beam_agent-5_min: 8
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-17_17-01-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: -411.99999999999886
  episode_reward_mean: -1088.9800000000002
  episode_reward_min: -2054.9999999999936
  episodes_this_iter: 96
  episodes_total: 864
  experiment_id: 79b578a554754ec4a502419d8b89326f
  experiment_tag: '0'
  hostname: gpu060
  info:
    grad_time_ms: 14339.208
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0012120767496526241
        entropy: 2.0760657787323
        entropy_coeff: 0.0017600000137463212
        kl: 0.01431322656571865
        model: {}
        policy_loss: -0.001319430535659194
        total_loss: 0.028404731303453445
        vf_explained_var: -0.0055540502071380615
        vf_loss: 330.2020568847656
      agent-1:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0012120767496526241
        entropy: 2.087456703186035
        entropy_coeff: 0.0017600000137463212
        kl: 0.014377017505466938
        model: {}
        policy_loss: -0.0019800972659140825
        total_loss: 0.027669787406921387
        vf_explained_var: -0.005093395709991455
        vf_loss: 327.84674072265625
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012120767496526241
        entropy: 2.1138594150543213
        entropy_coeff: 0.0017600000137463212
        kl: 0.013869713991880417
        model: {}
        policy_loss: -0.0013047857210040092
        total_loss: 0.02823506109416485
        vf_explained_var: -0.005990147590637207
        vf_loss: 325.66748046875
      agent-3:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0012120767496526241
        entropy: 2.0955088138580322
        entropy_coeff: 0.0017600000137463212
        kl: 0.0062303245067596436
        model: {}
        policy_loss: -0.0005463785491883755
        total_loss: 0.028771566227078438
        vf_explained_var: -0.007171005010604858
        vf_loss: 328.8922424316406
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012120767496526241
        entropy: 2.0203375816345215
        entropy_coeff: 0.0017600000137463212
        kl: 0.008935803547501564
        model: {}
        policy_loss: -0.0018579727038741112
        total_loss: 0.027936775237321854
        vf_explained_var: -0.005271971225738525
        vf_loss: 329.03753662109375
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012120767496526241
        entropy: 2.112795829772949
        entropy_coeff: 0.0017600000137463212
        kl: 0.01108588743954897
        model: {}
        policy_loss: -0.00042654655408114195
        total_loss: 0.02968396432697773
        vf_explained_var: -0.007037103176116943
        vf_loss: 332.7474060058594
    load_time_ms: 31995.186
    num_steps_sampled: 864000
    num_steps_trained: 864000
    sample_time_ms: 141135.817
    update_time_ms: 508.052
  iterations_since_restore: 9
  node_ip: 172.17.8.60
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.753012048192772
    ram_util_percent: 11.92128514056225
  pid: 30197
  policy_reward_max:
    agent-0: -59.66666666666669
    agent-1: -59.66666666666669
    agent-2: -59.66666666666669
    agent-3: -48.66666666666664
    agent-4: -48.66666666666664
    agent-5: -48.66666666666664
  policy_reward_mean:
    agent-0: -179.57666666666668
    agent-1: -179.57666666666668
    agent-2: -179.57666666666668
    agent-3: -183.41666666666669
    agent-4: -183.41666666666669
    agent-5: -183.41666666666669
  policy_reward_min:
    agent-0: -416.66666666666663
    agent-1: -416.66666666666663
    agent-2: -416.66666666666663
    agent-3: -400.66666666666663
    agent-4: -400.66666666666663
    agent-5: -400.66666666666663
  sampler_perf:
    mean_env_wait_ms: 28.657558323213458
    mean_inference_ms: 14.886094536676746
    mean_processing_ms: 66.0581740513471
  time_since_restore: 1696.5817987918854
  time_this_iter_s: 174.47898697853088
  time_total_s: 1696.5817987918854
  timestamp: 1637186519
  timesteps_since_restore: 864000
  timesteps_this_iter: 96000
  timesteps_total: 864000
  training_iteration: 9
  trial_id: '00000'
  
[2m[36m(pid=30197)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f41e0e81588> -> 96 episodes
== Status ==
Memory usage on this node: 21.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_2teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+--------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |     ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+--------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.60:30197 |      9 |          1696.58 | 864000 | -1088.98 |
+--------------------------------------+----------+-------------------+--------+------------------+--------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 4.37
    apples_agent-0_min: 0
    apples_agent-1_max: 23
    apples_agent-1_mean: 4.05
    apples_agent-1_min: 0
    apples_agent-2_max: 20
    apples_agent-2_mean: 4.86
    apples_agent-2_min: 0
    apples_agent-3_max: 38
    apples_agent-3_mean: 6.67
    apples_agent-3_min: 0
    apples_agent-4_max: 25
    apples_agent-4_mean: 4.69
    apples_agent-4_min: 0
    apples_agent-5_max: 23
    apples_agent-5_mean: 4.23
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 217
    cleaning_beam_agent-0_mean: 155.12
    cleaning_beam_agent-0_min: 133
    cleaning_beam_agent-1_max: 195
    cleaning_beam_agent-1_mean: 164.52
    cleaning_beam_agent-1_min: 121
    cleaning_beam_agent-2_max: 170
    cleaning_beam_agent-2_mean: 141.77
    cleaning_beam_agent-2_min: 120
    cleaning_beam_agent-3_max: 132
    cleaning_beam_agent-3_mean: 102.48
    cleaning_beam_agent-3_min: 74
    cleaning_beam_agent-4_max: 84
    cleaning_beam_agent-4_mean: 67.11
    cleaning_beam_agent-4_min: 53
    cleaning_beam_agent-5_max: 192
    cleaning_beam_agent-5_mean: 150.32
    cleaning_beam_agent-5_min: 104
    fire_beam_agent-0_max: 17
    fire_beam_agent-0_mean: 9.33
    fire_beam_agent-0_min: 2
    fire_beam_agent-1_max: 21
    fire_beam_agent-1_mean: 10.54
    fire_beam_agent-1_min: 3
    fire_beam_agent-2_max: 37
    fire_beam_agent-2_mean: 19.28
    fire_beam_agent-2_min: 10
    fire_beam_agent-3_max: 21
    fire_beam_agent-3_mean: 11.87
    fire_beam_agent-3_min: 1
    fire_beam_agent-4_max: 25
    fire_beam_agent-4_mean: 10.4
    fire_beam_agent-4_min: 4
    fire_beam_agent-5_max: 31
    fire_beam_agent-5_mean: 17.42
    fire_beam_agent-5_min: 9
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-17_17-04-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: -168.00000000000009
  episode_reward_mean: -757.64
  episode_reward_min: -1426.000000000003
  episodes_this_iter: 96
  episodes_total: 960
  experiment_id: 79b578a554754ec4a502419d8b89326f
  experiment_tag: '0'
  hostname: gpu060
  info:
    grad_time_ms: 14183.565
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0012060863664373755
        entropy: 2.0698466300964355
        entropy_coeff: 0.0017600000137463212
        kl: 0.010205686092376709
        model: {}
        policy_loss: -0.0008545904420316219
        total_loss: 0.027921199798583984
        vf_explained_var: -0.00522845983505249
        vf_loss: 321.6358642578125
      agent-1:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0012060863664373755
        entropy: 2.090583324432373
        entropy_coeff: 0.0017600000137463212
        kl: 0.01553710363805294
        model: {}
        policy_loss: 0.00033312710002064705
        total_loss: 0.02918498031795025
        vf_explained_var: -0.005012392997741699
        vf_loss: 319.4864501953125
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012060863664373755
        entropy: 2.0792906284332275
        entropy_coeff: 0.0017600000137463212
        kl: 0.006304941605776548
        model: {}
        policy_loss: -0.0014810310676693916
        total_loss: 0.025794275104999542
        vf_explained_var: -0.0047550201416015625
        vf_loss: 306.19610595703125
      agent-3:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0012060863664373755
        entropy: 1.9707567691802979
        entropy_coeff: 0.0017600000137463212
        kl: 0.014962272718548775
        model: {}
        policy_loss: -0.0016062143258750439
        total_loss: 0.02506233938038349
        vf_explained_var: -0.006154239177703857
        vf_loss: 298.5654296875
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012060863664373755
        entropy: 2.0020315647125244
        entropy_coeff: 0.0017600000137463212
        kl: 0.008322448469698429
        model: {}
        policy_loss: -0.0005441800458356738
        total_loss: 0.027008263394236565
        vf_explained_var: -0.005108296871185303
        vf_loss: 306.5989990234375
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012060863664373755
        entropy: 2.090538501739502
        entropy_coeff: 0.0017600000137463212
        kl: 0.0033283166121691465
        model: {}
        policy_loss: -0.0012746996944770217
        total_loss: 0.02579227089881897
        vf_explained_var: -0.006303578615188599
        vf_loss: 305.799072265625
    load_time_ms: 31872.98
    num_steps_sampled: 960000
    num_steps_trained: 960000
    sample_time_ms: 140593.514
    update_time_ms: 463.16
  iterations_since_restore: 10
  node_ip: 172.17.8.60
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.274117647058823
    ram_util_percent: 11.855686274509804
  pid: 30197
  policy_reward_max:
    agent-0: -7.333333333333331
    agent-1: -7.333333333333331
    agent-2: -7.333333333333331
    agent-3: -21.000000000000004
    agent-4: -21.000000000000004
    agent-5: -21.000000000000004
  policy_reward_mean:
    agent-0: -122.09666666666666
    agent-1: -122.09666666666666
    agent-2: -122.09666666666666
    agent-3: -130.45
    agent-4: -130.45
    agent-5: -130.45
  policy_reward_min:
    agent-0: -306.3333333333335
    agent-1: -306.3333333333335
    agent-2: -306.3333333333335
    agent-3: -309.6666666666667
    agent-4: -309.6666666666667
    agent-5: -309.6666666666667
  sampler_perf:
    mean_env_wait_ms: 28.58205030930541
    mean_inference_ms: 14.860363642716893
    mean_processing_ms: 66.08220113562032
  time_since_restore: 1875.9824657440186
  time_this_iter_s: 179.40066695213318
  time_total_s: 1875.9824657440186
  timestamp: 1637186699
  timesteps_since_restore: 960000
  timesteps_this_iter: 96000
  timesteps_total: 960000
  training_iteration: 10
  trial_id: '00000'
  
[2m[36m(pid=30197)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f41e0e81588> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_2teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+--------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |     ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+--------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.60:30197 |     10 |          1875.98 | 960000 |  -757.64 |
+--------------------------------------+----------+-------------------+--------+------------------+--------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30197)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f41e0e81588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 4.24
    apples_agent-0_min: 0
    apples_agent-1_max: 19
    apples_agent-1_mean: 4.55
    apples_agent-1_min: 0
    apples_agent-2_max: 27
    apples_agent-2_mean: 4.43
    apples_agent-2_min: 0
    apples_agent-3_max: 34
    apples_agent-3_mean: 9.27
    apples_agent-3_min: 0
    apples_agent-4_max: 29
    apples_agent-4_mean: 4.52
    apples_agent-4_min: 0
    apples_agent-5_max: 25
    apples_agent-5_mean: 5.17
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 183
    cleaning_beam_agent-0_mean: 158.05
    cleaning_beam_agent-0_min: 129
    cleaning_beam_agent-1_max: 190
    cleaning_beam_agent-1_mean: 162.71
    cleaning_beam_agent-1_min: 134
    cleaning_beam_agent-2_max: 213
    cleaning_beam_agent-2_mean: 167.63
    cleaning_beam_agent-2_min: 126
    cleaning_beam_agent-3_max: 116
    cleaning_beam_agent-3_mean: 78.86
    cleaning_beam_agent-3_min: 58
    cleaning_beam_agent-4_max: 112
    cleaning_beam_agent-4_mean: 83.51
    cleaning_beam_agent-4_min: 58
    cleaning_beam_agent-5_max: 188
    cleaning_beam_agent-5_mean: 159.22
    cleaning_beam_agent-5_min: 127
    fire_beam_agent-0_max: 13
    fire_beam_agent-0_mean: 6.7
    fire_beam_agent-0_min: 2
    fire_beam_agent-1_max: 13
    fire_beam_agent-1_mean: 6.21
    fire_beam_agent-1_min: 1
    fire_beam_agent-2_max: 28
    fire_beam_agent-2_mean: 15.59
    fire_beam_agent-2_min: 8
    fire_beam_agent-3_max: 17
    fire_beam_agent-3_mean: 8.51
    fire_beam_agent-3_min: 2
    fire_beam_agent-4_max: 17
    fire_beam_agent-4_mean: 7.09
    fire_beam_agent-4_min: 1
    fire_beam_agent-5_max: 23
    fire_beam_agent-5_mean: 11.84
    fire_beam_agent-5_min: 4
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-17_17-07-56
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: -65.00000000000006
  episode_reward_mean: -527.59
  episode_reward_min: -1273.9999999999986
  episodes_this_iter: 96
  episodes_total: 1056
  experiment_id: 79b578a554754ec4a502419d8b89326f
  experiment_tag: '0'
  hostname: gpu060
  info:
    grad_time_ms: 13095.535
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.001200095983222127
        entropy: 2.0305492877960205
        entropy_coeff: 0.0017600000137463212
        kl: 0.01033553946763277
        model: {}
        policy_loss: -0.0014296384761109948
        total_loss: 0.0227462537586689
        vf_explained_var: -0.004968136548995972
        vf_loss: 274.9126892089844
      agent-1:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.001200095983222127
        entropy: 2.0251405239105225
        entropy_coeff: 0.0017600000137463212
        kl: 0.01592807099223137
        model: {}
        policy_loss: -0.0006494275294244289
        total_loss: 0.02396530844271183
        vf_explained_var: -0.004889577627182007
        vf_loss: 275.81683349609375
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.001200095983222127
        entropy: 2.0603573322296143
        entropy_coeff: 0.0017600000137463212
        kl: 0.005049913190305233
        model: {}
        policy_loss: -0.0015152674168348312
        total_loss: 0.02097729593515396
        vf_explained_var: -0.00464552640914917
        vf_loss: 258.6629638671875
      agent-3:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.001200095983222127
        entropy: 1.9410123825073242
        entropy_coeff: 0.0017600000137463212
        kl: 0.007666607387363911
        model: {}
        policy_loss: 1.0026851668953896e-05
        total_loss: 0.02319493144750595
        vf_explained_var: -0.0068191587924957275
        vf_loss: 264.57330322265625
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.001200095983222127
        entropy: 1.9804327487945557
        entropy_coeff: 0.0017600000137463212
        kl: 0.006220126058906317
        model: {}
        policy_loss: -0.000987740932032466
        total_loss: 0.02298319712281227
        vf_explained_var: -0.005258411169052124
        vf_loss: 271.4549560546875
      agent-5:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.001200095983222127
        entropy: 2.0551609992980957
        entropy_coeff: 0.0017600000137463212
        kl: 0.014568058773875237
        model: {}
        policy_loss: -0.001310931984335184
        total_loss: 0.022379040718078613
        vf_explained_var: -0.006648153066635132
        vf_loss: 269.4285583496094
    load_time_ms: 30231.4
    num_steps_sampled: 1056000
    num_steps_trained: 1056000
    sample_time_ms: 141714.42
    update_time_ms: 71.89
  iterations_since_restore: 11
  node_ip: 172.17.8.60
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.528458498023717
    ram_util_percent: 11.760079051383398
  pid: 30197
  policy_reward_max:
    agent-0: -1.1102230246251565e-16
    agent-1: -1.1102230246251565e-16
    agent-2: -1.1102230246251565e-16
    agent-3: -8.99999999999999
    agent-4: -8.99999999999999
    agent-5: -8.99999999999999
  policy_reward_mean:
    agent-0: -91.81333333333333
    agent-1: -91.81333333333333
    agent-2: -91.81333333333333
    agent-3: -84.05
    agent-4: -84.05
    agent-5: -84.05
  policy_reward_min:
    agent-0: -223.3333333333333
    agent-1: -223.3333333333333
    agent-2: -223.3333333333333
    agent-3: -231.33333333333331
    agent-4: -231.33333333333331
    agent-5: -231.33333333333331
  sampler_perf:
    mean_env_wait_ms: 28.530662130094132
    mean_inference_ms: 14.845088447366084
    mean_processing_ms: 66.07220816030697
  time_since_restore: 2052.7709789276123
  time_this_iter_s: 176.78851318359375
  time_total_s: 2052.7709789276123
  timestamp: 1637186876
  timesteps_since_restore: 1056000
  timesteps_this_iter: 96000
  timesteps_total: 1056000
  training_iteration: 11
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_2teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.60:30197 |     11 |          2052.77 | 1056000 |  -527.59 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 32
    apples_agent-0_mean: 4.73
    apples_agent-0_min: 0
    apples_agent-1_max: 34
    apples_agent-1_mean: 4.46
    apples_agent-1_min: 0
    apples_agent-2_max: 26
    apples_agent-2_mean: 5.01
    apples_agent-2_min: 0
    apples_agent-3_max: 34
    apples_agent-3_mean: 10.02
    apples_agent-3_min: 0
    apples_agent-4_max: 16
    apples_agent-4_mean: 3.59
    apples_agent-4_min: 0
    apples_agent-5_max: 36
    apples_agent-5_mean: 4.58
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 182
    cleaning_beam_agent-0_mean: 130.54
    cleaning_beam_agent-0_min: 105
    cleaning_beam_agent-1_max: 247
    cleaning_beam_agent-1_mean: 196.43
    cleaning_beam_agent-1_min: 158
    cleaning_beam_agent-2_max: 195
    cleaning_beam_agent-2_mean: 161.33
    cleaning_beam_agent-2_min: 132
    cleaning_beam_agent-3_max: 87
    cleaning_beam_agent-3_mean: 69.77
    cleaning_beam_agent-3_min: 52
    cleaning_beam_agent-4_max: 94
    cleaning_beam_agent-4_mean: 72.26
    cleaning_beam_agent-4_min: 58
    cleaning_beam_agent-5_max: 275
    cleaning_beam_agent-5_mean: 238.46
    cleaning_beam_agent-5_min: 137
    fire_beam_agent-0_max: 13
    fire_beam_agent-0_mean: 4.85
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 13
    fire_beam_agent-1_mean: 4.62
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 19
    fire_beam_agent-2_mean: 11.29
    fire_beam_agent-2_min: 6
    fire_beam_agent-3_max: 12
    fire_beam_agent-3_mean: 6.13
    fire_beam_agent-3_min: 1
    fire_beam_agent-4_max: 10
    fire_beam_agent-4_mean: 5.18
    fire_beam_agent-4_min: 1
    fire_beam_agent-5_max: 21
    fire_beam_agent-5_mean: 10.84
    fire_beam_agent-5_min: 3
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-17_17-11-00
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: -48.00000000000053
  episode_reward_mean: -389.95000000000016
  episode_reward_min: -916.0
  episodes_this_iter: 96
  episodes_total: 1152
  experiment_id: 79b578a554754ec4a502419d8b89326f
  experiment_tag: '0'
  hostname: gpu060
  info:
    grad_time_ms: 13059.371
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0011941056000068784
        entropy: 2.014430046081543
        entropy_coeff: 0.0017600000137463212
        kl: 0.01061437837779522
        model: {}
        policy_loss: -0.0008308505639433861
        total_loss: 0.01803538203239441
        vf_explained_var: -0.00677904486656189
        vf_loss: 221.46261596679688
      agent-1:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0011941056000068784
        entropy: 2.0115981101989746
        entropy_coeff: 0.0017600000137463212
        kl: 0.011449932120740414
        model: {}
        policy_loss: -0.001032267464324832
        total_loss: 0.018102366477251053
        vf_explained_var: -0.005892932415008545
        vf_loss: 222.4567413330078
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0011941056000068784
        entropy: 2.053852081298828
        entropy_coeff: 0.0017600000137463212
        kl: 0.005348674487322569
        model: {}
        policy_loss: -0.001355794258415699
        total_loss: 0.015734588727355003
        vf_explained_var: -0.005588024854660034
        vf_loss: 204.37728881835938
      agent-3:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0011941056000068784
        entropy: 1.8641703128814697
        entropy_coeff: 0.0017600000137463212
        kl: 0.012294353917241096
        model: {}
        policy_loss: -0.0012235487811267376
        total_loss: 0.01658238098025322
        vf_explained_var: -0.005686163902282715
        vf_loss: 208.56350708007812
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0011941056000068784
        entropy: 1.9682997465133667
        entropy_coeff: 0.0017600000137463212
        kl: 0.005118754226714373
        model: {}
        policy_loss: -0.0012690533185377717
        total_loss: 0.01736847683787346
        vf_explained_var: -0.004955768585205078
        vf_loss: 218.45799255371094
      agent-5:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0011941056000068784
        entropy: 2.0152454376220703
        entropy_coeff: 0.0017600000137463212
        kl: 0.01076918002218008
        model: {}
        policy_loss: -0.0007050798740237951
        total_loss: 0.017407625913619995
        vf_explained_var: -0.005478709936141968
        vf_loss: 213.90310668945312
    load_time_ms: 29365.75
    num_steps_sampled: 1152000
    num_steps_trained: 1152000
    sample_time_ms: 141283.335
    update_time_ms: 65.305
  iterations_since_restore: 12
  node_ip: 172.17.8.60
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.009578544061306
    ram_util_percent: 11.85823754789272
  pid: 30197
  policy_reward_max:
    agent-0: -0.9999999999999998
    agent-1: -0.9999999999999998
    agent-2: -0.9999999999999998
    agent-3: 4.333333333333333
    agent-4: 4.333333333333333
    agent-5: 4.333333333333333
  policy_reward_mean:
    agent-0: -65.94666666666667
    agent-1: -65.94666666666667
    agent-2: -65.94666666666667
    agent-3: -64.03666666666668
    agent-4: -64.03666666666668
    agent-5: -64.03666666666668
  policy_reward_min:
    agent-0: -189.66666666666663
    agent-1: -189.66666666666663
    agent-2: -189.66666666666663
    agent-3: -156.33333333333337
    agent-4: -156.33333333333337
    agent-5: -156.33333333333337
  sampler_perf:
    mean_env_wait_ms: 28.49460634935783
    mean_inference_ms: 14.824604577315021
    mean_processing_ms: 66.38588009820138
  time_since_restore: 2235.8378751277924
  time_this_iter_s: 183.06689620018005
  time_total_s: 2235.8378751277924
  timestamp: 1637187060
  timesteps_since_restore: 1152000
  timesteps_this_iter: 96000
  timesteps_total: 1152000
  training_iteration: 12
  trial_id: '00000'
  
[2m[36m(pid=30197)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f41e0e81588> -> 96 episodes
== Status ==
Memory usage on this node: 21.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_2teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.60:30197 |     12 |          2235.84 | 1152000 |  -389.95 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 21
    apples_agent-0_mean: 4.01
    apples_agent-0_min: 0
    apples_agent-1_max: 19
    apples_agent-1_mean: 5.19
    apples_agent-1_min: 0
    apples_agent-2_max: 20
    apples_agent-2_mean: 5.22
    apples_agent-2_min: 0
    apples_agent-3_max: 39
    apples_agent-3_mean: 10.4
    apples_agent-3_min: 0
    apples_agent-4_max: 22
    apples_agent-4_mean: 4.35
    apples_agent-4_min: 0
    apples_agent-5_max: 31
    apples_agent-5_mean: 4.82
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 175
    cleaning_beam_agent-0_mean: 141.23
    cleaning_beam_agent-0_min: 119
    cleaning_beam_agent-1_max: 243
    cleaning_beam_agent-1_mean: 201.54
    cleaning_beam_agent-1_min: 168
    cleaning_beam_agent-2_max: 185
    cleaning_beam_agent-2_mean: 156.42
    cleaning_beam_agent-2_min: 125
    cleaning_beam_agent-3_max: 107
    cleaning_beam_agent-3_mean: 76.31
    cleaning_beam_agent-3_min: 56
    cleaning_beam_agent-4_max: 83
    cleaning_beam_agent-4_mean: 64.64
    cleaning_beam_agent-4_min: 47
    cleaning_beam_agent-5_max: 274
    cleaning_beam_agent-5_mean: 239.89
    cleaning_beam_agent-5_min: 209
    fire_beam_agent-0_max: 7
    fire_beam_agent-0_mean: 3.4
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 9
    fire_beam_agent-1_mean: 3.32
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 14
    fire_beam_agent-2_mean: 8.22
    fire_beam_agent-2_min: 4
    fire_beam_agent-3_max: 12
    fire_beam_agent-3_mean: 4.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 9
    fire_beam_agent-4_mean: 3.51
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 13
    fire_beam_agent-5_mean: 7.06
    fire_beam_agent-5_min: 2
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-17_17-14-05
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 28.999999999999975
  episode_reward_mean: -238.09999999999985
  episode_reward_min: -576.0000000000013
  episodes_this_iter: 96
  episodes_total: 1248
  experiment_id: 79b578a554754ec4a502419d8b89326f
  experiment_tag: '0'
  hostname: gpu060
  info:
    grad_time_ms: 13034.791
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0011881152167916298
        entropy: 2.0043740272521973
        entropy_coeff: 0.0017600000137463212
        kl: 0.014315112493932247
        model: {}
        policy_loss: -0.0011948260944336653
        total_loss: 0.013452854938805103
        vf_explained_var: -0.006366610527038574
        vf_loss: 178.1750030517578
      agent-1:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0011881152167916298
        entropy: 1.9814019203186035
        entropy_coeff: 0.0017600000137463212
        kl: 0.011298434808850288
        model: {}
        policy_loss: -0.0010732348309829831
        total_loss: 0.013626625761389732
        vf_explained_var: -0.005691349506378174
        vf_loss: 177.63433837890625
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0011881152167916298
        entropy: 2.0391292572021484
        entropy_coeff: 0.0017600000137463212
        kl: 0.00689177168533206
        model: {}
        policy_loss: -0.001367633929476142
        total_loss: 0.011669136583805084
        vf_explained_var: -0.004921168088912964
        vf_loss: 162.8105010986328
      agent-3:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0011881152167916298
        entropy: 1.8114936351776123
        entropy_coeff: 0.0017600000137463212
        kl: 0.01569424569606781
        model: {}
        policy_loss: -0.0011073050554841757
        total_loss: 0.012951021082699299
        vf_explained_var: -0.00672420859336853
        vf_loss: 169.52285766601562
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0011881152167916298
        entropy: 1.9713797569274902
        entropy_coeff: 0.0017600000137463212
        kl: 0.010212993249297142
        model: {}
        policy_loss: -0.0004588956944644451
        total_loss: 0.013814290054142475
        vf_explained_var: -0.0057350099086761475
        vf_loss: 172.32168579101562
      agent-5:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0011881152167916298
        entropy: 2.0128934383392334
        entropy_coeff: 0.0017600000137463212
        kl: 0.012299537658691406
        model: {}
        policy_loss: -0.001518724486231804
        total_loss: 0.012325671501457691
        vf_explained_var: -0.006209135055541992
        vf_loss: 170.7960205078125
    load_time_ms: 28905.492
    num_steps_sampled: 1248000
    num_steps_trained: 1248000
    sample_time_ms: 140627.172
    update_time_ms: 65.239
  iterations_since_restore: 13
  node_ip: 172.17.8.60
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.851136363636368
    ram_util_percent: 11.876515151515154
  pid: 30197
  policy_reward_max:
    agent-0: 8.33333333333333
    agent-1: 8.33333333333333
    agent-2: 8.33333333333333
    agent-3: 22.666666666666654
    agent-4: 22.666666666666654
    agent-5: 22.666666666666654
  policy_reward_mean:
    agent-0: -41.44333333333333
    agent-1: -41.44333333333333
    agent-2: -41.44333333333333
    agent-3: -37.92333333333333
    agent-4: -37.92333333333333
    agent-5: -37.92333333333333
  policy_reward_min:
    agent-0: -131.00000000000003
    agent-1: -131.00000000000003
    agent-2: -131.00000000000003
    agent-3: -119.66666666666669
    agent-4: -119.66666666666669
    agent-5: -119.66666666666669
  sampler_perf:
    mean_env_wait_ms: 28.47075781402732
    mean_inference_ms: 14.81699620098739
    mean_processing_ms: 66.35627717979106
  time_since_restore: 2421.2286038398743
  time_this_iter_s: 185.3907287120819
  time_total_s: 2421.2286038398743
  timestamp: 1637187245
  timesteps_since_restore: 1248000
  timesteps_this_iter: 96000
  timesteps_total: 1248000
  training_iteration: 13
  trial_id: '00000'
  
[2m[36m(pid=30197)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f41e0e81588> -> 96 episodes
