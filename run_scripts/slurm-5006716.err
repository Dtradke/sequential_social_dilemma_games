 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-17 17:38:55,678	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 54.61 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-17 17:38:55,991	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 0 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-17 17:38:56,771	WARNING logger.py:328 -- Could not instantiate TBXLogger: [Errno 28] No space left on device.
slurmstepd: error: *** JOB 5006716 ON gpu148 CANCELLED AT 2021-11-17T18:44:55 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-17 18:47:17,676	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 53.9 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-17 18:47:17,798	WARNING services.py:928 -- Redis failed to start, retrying now.
2021-11-17 18:47:18,071	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21415858176 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-17 18:47:18,753	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-17 18:47:18,892	INFO trial_runner.py:169 -- Resuming trial.
2021-11-17 18:47:18,892	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-17 18:47:19,024	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
slurmstepd: error: *** JOB 5006716 ON gpu046 CANCELLED AT 2021-11-17T19:48:00 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-17 19:51:07,179	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 49.19 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-17 19:51:07,465	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21154668544 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-17 19:51:08,214	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-17 19:51:08,405	INFO trial_runner.py:169 -- Resuming trial.
2021-11-17 19:51:08,405	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-17 19:51:08,587	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-17 20:07:59,947	ERROR trial_runner.py:349 -- Trial Runner checkpointing failed.
Traceback (most recent call last):
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 347, in step
    self.checkpoint()
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 278, in checkpoint
    os.rename(tmp_file_name, self.checkpoint_file)
FileNotFoundError: [Errno 2] No such file or directory: '/h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics/.tmp_checkpoint' -> '/h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics/experiment_state-2021-11-17_19-51-08.json'
slurmstepd: error: *** JOB 5006716 ON gpu039 CANCELLED AT 2021-11-17T20:51:01 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-17 20:54:07,800	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 53.45 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-17 20:54:08,085	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21454901248 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-17 20:54:08,957	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-17 20:54:09,204	INFO trial_runner.py:169 -- Resuming trial.
2021-11-17 20:54:09,204	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-17 20:54:09,427	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
slurmstepd: error: *** JOB 5006716 ON gpu011 CANCELLED AT 2021-11-17T21:54:00 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-17 21:57:07,176	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 51.29 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-17 21:57:07,449	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21472788480 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-17 21:57:08,443	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-17 21:57:08,980	INFO trial_runner.py:169 -- Resuming trial.
2021-11-17 21:57:08,980	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-17 21:57:09,392	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-17 22:38:36,494	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0567042827606201 seconds to complete, which may be a performance bottleneck.
slurmstepd: error: *** JOB 5006716 ON gpu006 CANCELLED AT 2021-11-17T22:57:00 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-17 23:00:06,804	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 48.6 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-17 23:00:07,130	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21318647808 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-17 23:00:08,096	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-17 23:00:08,540	INFO trial_runner.py:169 -- Resuming trial.
2021-11-17 23:00:08,540	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-17 23:00:09,125	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
slurmstepd: error: *** JOB 5006716 ON gpu006 CANCELLED AT 2021-11-18T00:03:00 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-18 00:06:04,998	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 51.2 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-18 00:06:05,284	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21415829504 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-18 00:06:06,180	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-18 00:06:06,540	INFO trial_runner.py:169 -- Resuming trial.
2021-11-18 00:06:06,540	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-18 00:06:06,939	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
slurmstepd: error: *** JOB 5006716 ON gpu046 CANCELLED AT 2021-11-18T01:11:01 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-18 01:14:05,624	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 53.88 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-18 01:14:05,870	WARNING services.py:928 -- Redis failed to start, retrying now.
2021-11-18 01:14:06,036	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21471617024 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-18 01:14:07,328	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-18 01:14:07,914	INFO trial_runner.py:169 -- Resuming trial.
2021-11-18 01:14:07,915	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-18 01:14:08,195	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-18 02:05:57,507	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6435873508453369 seconds to complete, which may be a performance bottleneck.
slurmstepd: error: *** JOB 5006716 ON gpu004 CANCELLED AT 2021-11-18T02:14:01 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-18 02:17:05,112	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 52.82 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-18 02:17:05,379	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21470031872 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-18 02:17:06,369	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-18 02:17:06,828	INFO trial_runner.py:169 -- Resuming trial.
2021-11-18 02:17:06,829	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-18 02:17:07,273	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-18 04:58:49,284	ERROR trial_runner.py:349 -- Trial Runner checkpointing failed.
Traceback (most recent call last):
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 347, in step
    self.checkpoint()
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 278, in checkpoint
    os.rename(tmp_file_name, self.checkpoint_file)
FileNotFoundError: [Errno 2] No such file or directory: '/h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics/.tmp_checkpoint' -> '/h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics/experiment_state-2021-11-18_02-17-06.json'
2021-11-18 06:43:09,977	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.511620044708252 seconds to complete, which may be a performance bottleneck.
2021-11-18 06:55:22,370	ERROR trial_runner.py:349 -- Trial Runner checkpointing failed.
Traceback (most recent call last):
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 347, in step
    self.checkpoint()
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 278, in checkpoint
    os.rename(tmp_file_name, self.checkpoint_file)
FileNotFoundError: [Errno 2] No such file or directory: '/h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics/.tmp_checkpoint' -> '/h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics/experiment_state-2021-11-18_02-17-06.json'
2021-11-18 07:17:55,952	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.50946044921875 seconds to complete, which may be a performance bottleneck.
2021-11-18 07:52:41,573	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5178675651550293 seconds to complete, which may be a performance bottleneck.
2021-11-18 08:09:02,269	WARNING util.py:137 -- The `process_trial_save` operation took 0.5077872276306152 seconds to complete, which may be a performance bottleneck.
2021-11-18 08:09:02,270	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-18 08:29:30,291	WARNING util.py:137 -- The `process_trial_save` operation took 0.6576931476593018 seconds to complete, which may be a performance bottleneck.
2021-11-18 08:29:30,291	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-18 08:49:56,046	WARNING util.py:137 -- The `process_trial_save` operation took 0.5183050632476807 seconds to complete, which may be a performance bottleneck.
2021-11-18 08:49:56,046	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-18 08:51:59,679	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5559248924255371 seconds to complete, which may be a performance bottleneck.
2021-11-18 09:08:20,430	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.572711706161499 seconds to complete, which may be a performance bottleneck.
2021-11-18 09:10:22,883	WARNING util.py:137 -- The `process_trial_save` operation took 0.5287821292877197 seconds to complete, which may be a performance bottleneck.
2021-11-18 09:10:22,884	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-18 09:30:50,586	WARNING util.py:137 -- The `process_trial_save` operation took 0.5657615661621094 seconds to complete, which may be a performance bottleneck.
2021-11-18 09:30:50,586	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
slurmstepd: error: *** JOB 5006716 ON gpu005 CANCELLED AT 2021-11-18T09:37:01 ***
