 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-15 16:59:57,719	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 54.83 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-15 16:59:58,020	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 4078448640 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-15 17:05:29,075	WARNING util.py:137 -- The `fetch_result` operation took 1.02827787399292 seconds to complete, which may be a performance bottleneck.
2021-11-15 17:05:30,694	WARNING util.py:137 -- The `process_trial` operation took 2.708235740661621 seconds to complete, which may be a performance bottleneck.
2021-11-15 17:59:47,404	WARNING util.py:137 -- The `experiment_checkpoint` operation took 3.0416314601898193 seconds to complete, which may be a performance bottleneck.
slurmstepd: error: *** JOB 4985082 ON gpu048 CANCELLED AT 2021-11-15T17:59:51 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-15 18:02:53,293	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 53.73 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-15 18:02:53,590	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 13082578944 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-15 18:02:54,315	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-15 18:02:54,506	INFO trial_runner.py:169 -- Resuming trial.
2021-11-15 18:02:54,506	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-15 18:02:54,637	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-15 18:08:41,925	WARNING util.py:137 -- The `fetch_result` operation took 1.5083072185516357 seconds to complete, which may be a performance bottleneck.
2021-11-15 18:08:43,030	WARNING util.py:137 -- The `process_trial` operation took 2.651885747909546 seconds to complete, which may be a performance bottleneck.
2021-11-15 18:08:46,590	WARNING util.py:137 -- The `experiment_checkpoint` operation took 3.5598127841949463 seconds to complete, which may be a performance bottleneck.
2021-11-15 18:08:46,591	ERROR trial_runner.py:349 -- Trial Runner checkpointing failed.
Traceback (most recent call last):
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 347, in step
    self.checkpoint()
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 278, in checkpoint
    os.rename(tmp_file_name, self.checkpoint_file)
FileNotFoundError: [Errno 2] No such file or directory: '/h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics/.tmp_checkpoint' -> '/h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics/experiment_state-2021-11-15_18-02-54.json'
2021-11-15 18:12:00,820	WARNING util.py:137 -- The `process_trial` operation took 0.5173838138580322 seconds to complete, which may be a performance bottleneck.
2021-11-15 19:01:01,136	WARNING util.py:137 -- The `process_trial` operation took 1.4224672317504883 seconds to complete, which may be a performance bottleneck.
2021-11-15 19:01:11,506	WARNING util.py:137 -- The `process_trial_save` operation took 6.286566734313965 seconds to complete, which may be a performance bottleneck.
2021-11-15 19:01:11,506	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
slurmstepd: error: *** JOB 4985082 ON gpu159 CANCELLED AT 2021-11-15T20:28:22 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-15 20:41:55,281	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 54.7 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-15 20:41:55,573	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 7387164672 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-15 20:41:56,338	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-15 20:41:56,533	INFO trial_runner.py:169 -- Resuming trial.
2021-11-15 20:41:56,533	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-15 20:41:56,720	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-15 20:47:44,877	WARNING util.py:137 -- The `fetch_result` operation took 0.8158345222473145 seconds to complete, which may be a performance bottleneck.
2021-11-15 20:47:45,728	WARNING util.py:137 -- The `process_trial` operation took 1.7548389434814453 seconds to complete, which may be a performance bottleneck.
2021-11-15 20:47:47,760	WARNING util.py:137 -- The `experiment_checkpoint` operation took 2.0263636112213135 seconds to complete, which may be a performance bottleneck.
2021-11-15 20:57:07,166	WARNING util.py:137 -- The `process_trial` operation took 0.7783541679382324 seconds to complete, which may be a performance bottleneck.
2021-11-15 21:38:49,878	WARNING util.py:137 -- The `process_trial_save` operation took 0.5654833316802979 seconds to complete, which may be a performance bottleneck.
2021-11-15 21:38:49,879	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-15 22:26:39,737	WARNING util.py:137 -- The `process_trial` operation took 0.7058470249176025 seconds to complete, which may be a performance bottleneck.
2021-11-15 22:37:46,697	WARNING util.py:137 -- The `process_trial` operation took 0.5743186473846436 seconds to complete, which may be a performance bottleneck.
2021-11-15 22:50:59,612	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.2374019622802734 seconds to complete, which may be a performance bottleneck.
2021-11-15 23:22:11,646	WARNING util.py:137 -- The `process_trial` operation took 2.075847864151001 seconds to complete, which may be a performance bottleneck.
2021-11-15 23:40:07,479	WARNING util.py:137 -- The `process_trial` operation took 0.5022115707397461 seconds to complete, which may be a performance bottleneck.
2021-11-15 23:42:23,141	WARNING util.py:137 -- The `process_trial` operation took 1.0116987228393555 seconds to complete, which may be a performance bottleneck.
2021-11-16 00:55:56,337	WARNING util.py:137 -- The `experiment_checkpoint` operation took 20.0403470993042 seconds to complete, which may be a performance bottleneck.
2021-11-16 08:17:19,906	WARNING util.py:137 -- The `process_trial_save` operation took 1.0087966918945312 seconds to complete, which may be a performance bottleneck.
2021-11-16 08:17:19,906	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
slurmstepd: error: *** JOB 4985082 ON gpu043 CANCELLED AT 2021-11-16T12:28:15 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-16 12:30:30,872	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 49.07 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-16 12:30:31,151	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 16842272768 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-16 12:30:32,150	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-16 12:30:32,688	INFO trial_runner.py:169 -- Resuming trial.
2021-11-16 12:30:32,688	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-16 12:30:33,213	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-16 12:35:54,673	WARNING util.py:137 -- The `fetch_result` operation took 2.119770050048828 seconds to complete, which may be a performance bottleneck.
2021-11-16 12:35:56,355	WARNING util.py:137 -- The `process_trial` operation took 3.9288573265075684 seconds to complete, which may be a performance bottleneck.
2021-11-16 12:36:00,238	WARNING util.py:137 -- The `experiment_checkpoint` operation took 3.8751208782196045 seconds to complete, which may be a performance bottleneck.
2021-11-16 13:17:58,591	WARNING util.py:137 -- The `experiment_checkpoint` operation took 13.65369701385498 seconds to complete, which may be a performance bottleneck.
2021-11-16 13:28:14,592	WARNING util.py:137 -- The `process_trial_save` operation took 0.6224770545959473 seconds to complete, which may be a performance bottleneck.
2021-11-16 13:28:14,592	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-16 14:00:53,050	WARNING util.py:137 -- The `process_trial` operation took 0.6725232601165771 seconds to complete, which may be a performance bottleneck.
2021-11-16 14:12:18,614	WARNING util.py:137 -- The `process_trial` operation took 0.5172698497772217 seconds to complete, which may be a performance bottleneck.
2021-11-16 14:16:56,022	WARNING util.py:137 -- The `process_trial` operation took 0.5851593017578125 seconds to complete, which may be a performance bottleneck.
2021-11-16 14:25:55,391	WARNING util.py:137 -- The `experiment_checkpoint` operation took 6.7868266105651855 seconds to complete, which may be a performance bottleneck.
2021-11-16 14:39:17,776	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.8114900588989258 seconds to complete, which may be a performance bottleneck.
2021-11-16 14:45:52,928	WARNING util.py:137 -- The `process_trial` operation took 0.5281155109405518 seconds to complete, which may be a performance bottleneck.
2021-11-16 15:01:18,090	WARNING util.py:137 -- The `process_trial_save` operation took 0.829622745513916 seconds to complete, which may be a performance bottleneck.
2021-11-16 15:01:18,090	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-16 15:07:52,969	WARNING util.py:137 -- The `process_trial` operation took 0.5106410980224609 seconds to complete, which may be a performance bottleneck.
2021-11-16 15:29:45,773	WARNING util.py:137 -- The `process_trial` operation took 0.5324416160583496 seconds to complete, which may be a performance bottleneck.
2021-11-16 15:42:54,595	WARNING util.py:137 -- The `process_trial` operation took 0.5178532600402832 seconds to complete, which may be a performance bottleneck.
2021-11-16 15:45:08,156	WARNING util.py:137 -- The `process_trial_save` operation took 1.4406616687774658 seconds to complete, which may be a performance bottleneck.
2021-11-16 15:45:08,156	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-16 16:28:49,452	WARNING util.py:137 -- The `process_trial_save` operation took 0.9110667705535889 seconds to complete, which may be a performance bottleneck.
2021-11-16 16:28:49,453	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
slurmstepd: error: *** JOB 4985082 ON gpu039 CANCELLED AT 2021-11-16T16:35:06 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-16 16:37:27,550	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 50.89 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-16 16:37:27,847	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21474795520 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-16 16:37:29,002	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-16 16:37:29,883	INFO trial_runner.py:169 -- Resuming trial.
2021-11-16 16:37:29,883	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-16 16:37:30,255	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-16 16:37:30,464	ERROR trial_runner.py:349 -- Trial Runner checkpointing failed.
Traceback (most recent call last):
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 347, in step
    self.checkpoint()
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 278, in checkpoint
    os.rename(tmp_file_name, self.checkpoint_file)
FileNotFoundError: [Errno 2] No such file or directory: '/h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics/.tmp_checkpoint' -> '/h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics/experiment_state-2021-11-16_16-37-29.json'
2021-11-16 16:43:02,820	WARNING util.py:137 -- The `fetch_result` operation took 2.0714876651763916 seconds to complete, which may be a performance bottleneck.
2021-11-16 16:43:05,104	WARNING util.py:137 -- The `process_trial` operation took 4.46804666519165 seconds to complete, which may be a performance bottleneck.
2021-11-16 16:43:10,931	WARNING util.py:137 -- The `experiment_checkpoint` operation took 5.821340799331665 seconds to complete, which may be a performance bottleneck.
2021-11-16 17:07:55,398	WARNING util.py:137 -- The `process_trial` operation took 13.979485511779785 seconds to complete, which may be a performance bottleneck.
2021-11-16 17:39:12,053	WARNING util.py:137 -- The `process_trial_save` operation took 0.9439578056335449 seconds to complete, which may be a performance bottleneck.
2021-11-16 17:39:12,071	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-16 18:18:25,647	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.2087230682373047 seconds to complete, which may be a performance bottleneck.
slurmstepd: error: *** JOB 4985082 ON gpu128 CANCELLED AT 2021-11-16T19:43:08 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-16 19:45:29,304	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 53.71 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-16 19:45:29,602	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 90112 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-16 19:45:30,931	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-16 19:45:31,825	INFO trial_runner.py:169 -- Resuming trial.
2021-11-16 19:45:31,826	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-16 19:45:32,226	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-16 19:51:27,555	WARNING util.py:137 -- The `fetch_result` operation took 3.2992165088653564 seconds to complete, which may be a performance bottleneck.
2021-11-16 19:51:31,134	WARNING util.py:137 -- The `process_trial` operation took 7.001827955245972 seconds to complete, which may be a performance bottleneck.
2021-11-16 19:51:38,678	WARNING util.py:137 -- The `experiment_checkpoint` operation took 7.526445388793945 seconds to complete, which may be a performance bottleneck.
2021-11-16 20:09:10,141	WARNING util.py:137 -- The `experiment_checkpoint` operation took 15.849462985992432 seconds to complete, which may be a performance bottleneck.
2021-11-16 20:39:57,107	WARNING util.py:137 -- The `process_trial_save` operation took 0.8904600143432617 seconds to complete, which may be a performance bottleneck.
2021-11-16 20:39:57,108	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-16 21:24:17,637	WARNING util.py:137 -- The `process_trial_save` operation took 1.1300714015960693 seconds to complete, which may be a performance bottleneck.
2021-11-16 21:24:17,638	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-16 23:12:09,525	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5957765579223633 seconds to complete, which may be a performance bottleneck.
2021-11-16 23:14:16,375	ERROR trial_runner.py:349 -- Trial Runner checkpointing failed.
Traceback (most recent call last):
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 347, in step
    self.checkpoint()
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 278, in checkpoint
    os.rename(tmp_file_name, self.checkpoint_file)
FileNotFoundError: [Errno 2] No such file or directory: '/h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics/.tmp_checkpoint' -> '/h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics/experiment_state-2021-11-16_19-45-31.json'
slurmstepd: error: *** JOB 4985082 ON gpu148 CANCELLED AT 2021-11-16T23:17:02 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-16 23:19:24,792	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 54.2 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-16 23:19:25,058	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21471932416 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-16 23:19:26,588	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-16 23:19:27,238	INFO trial_runner.py:169 -- Resuming trial.
2021-11-16 23:19:27,238	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-16 23:19:27,716	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-16 23:25:16,394	WARNING util.py:137 -- The `fetch_result` operation took 0.8376381397247314 seconds to complete, which may be a performance bottleneck.
2021-11-16 23:25:17,098	WARNING util.py:137 -- The `process_trial` operation took 1.649125337600708 seconds to complete, which may be a performance bottleneck.
2021-11-16 23:25:19,568	WARNING util.py:137 -- The `experiment_checkpoint` operation took 2.462036609649658 seconds to complete, which may be a performance bottleneck.
2021-11-16 23:34:53,132	WARNING util.py:137 -- The `experiment_checkpoint` operation took 14.03873085975647 seconds to complete, which may be a performance bottleneck.
2021-11-16 23:46:47,709	WARNING util.py:137 -- The `process_trial` operation took 0.506049394607544 seconds to complete, which may be a performance bottleneck.
2021-11-16 23:49:56,489	WARNING util.py:137 -- The `process_trial` operation took 0.5842046737670898 seconds to complete, which may be a performance bottleneck.
slurmstepd: error: *** JOB 4985082 ON gpu003 CANCELLED AT 2021-11-17T00:21:00 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-17 00:23:17,087	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 51.73 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-17 00:23:17,379	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 20405104640 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-17 00:23:18,649	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-17 00:23:19,496	INFO trial_runner.py:169 -- Resuming trial.
2021-11-17 00:23:19,496	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-17 00:23:20,003	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-17 00:29:10,013	WARNING util.py:137 -- The `fetch_result` operation took 5.014802694320679 seconds to complete, which may be a performance bottleneck.
2021-11-17 00:29:14,386	WARNING util.py:137 -- The `process_trial` operation took 9.54680848121643 seconds to complete, which may be a performance bottleneck.
2021-11-17 00:29:24,697	WARNING util.py:137 -- The `experiment_checkpoint` operation took 10.304613828659058 seconds to complete, which may be a performance bottleneck.
2021-11-17 00:38:26,482	WARNING util.py:137 -- The `experiment_checkpoint` operation took 9.050830841064453 seconds to complete, which may be a performance bottleneck.
2021-11-17 01:22:25,337	WARNING util.py:137 -- The `process_trial_save` operation took 0.9482369422912598 seconds to complete, which may be a performance bottleneck.
2021-11-17 01:22:25,337	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
slurmstepd: error: *** JOB 4985082 ON gpu156 CANCELLED AT 2021-11-17T01:26:20 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-17 01:28:40,054	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 50.79 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-17 01:28:40,352	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 45056 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-17 01:28:41,931	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-17 01:28:42,633	INFO trial_runner.py:169 -- Resuming trial.
2021-11-17 01:28:42,634	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-17 01:28:43,106	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-17 01:35:04,039	WARNING util.py:137 -- The `fetch_result` operation took 1.1605520248413086 seconds to complete, which may be a performance bottleneck.
2021-11-17 01:35:04,637	WARNING util.py:137 -- The `process_trial` operation took 1.7935264110565186 seconds to complete, which may be a performance bottleneck.
2021-11-17 01:35:13,133	WARNING util.py:137 -- The `experiment_checkpoint` operation took 8.496254444122314 seconds to complete, which may be a performance bottleneck.
2021-11-17 01:43:57,466	WARNING util.py:137 -- The `experiment_checkpoint` operation took 17.119359970092773 seconds to complete, which may be a performance bottleneck.
2021-11-17 02:39:13,116	WARNING util.py:137 -- The `process_trial_save` operation took 1.0534563064575195 seconds to complete, which may be a performance bottleneck.
2021-11-17 02:39:13,116	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
slurmstepd: error: *** JOB 4985082 ON gpu148 CANCELLED AT 2021-11-17T03:22:12 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-17 03:24:58,363	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 51.34 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-17 03:24:58,661	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 45056 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-17 03:25:00,039	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-17 03:25:00,908	INFO trial_runner.py:169 -- Resuming trial.
2021-11-17 03:25:00,908	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-17 03:25:01,362	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-17 03:41:04,811	WARNING util.py:137 -- The `experiment_checkpoint` operation took 12.674366235733032 seconds to complete, which may be a performance bottleneck.
2021-11-17 04:33:25,820	WARNING util.py:137 -- The `process_trial_save` operation took 0.8137500286102295 seconds to complete, which may be a performance bottleneck.
2021-11-17 04:33:25,821	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
slurmstepd: error: *** JOB 4985082 ON gpu148 CANCELLED AT 2021-11-17T04:37:24 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-17 04:39:40,909	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 49.59 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-17 04:39:41,193	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21411262464 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-17 04:39:42,436	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-17 04:39:43,252	INFO trial_runner.py:169 -- Resuming trial.
2021-11-17 04:39:43,252	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-17 04:39:43,918	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-17 04:45:41,058	WARNING util.py:137 -- The `fetch_result` operation took 1.2273025512695312 seconds to complete, which may be a performance bottleneck.
2021-11-17 04:45:42,148	WARNING util.py:137 -- The `process_trial` operation took 2.3587794303894043 seconds to complete, which may be a performance bottleneck.
2021-11-17 04:45:45,282	WARNING util.py:137 -- The `experiment_checkpoint` operation took 3.123901128768921 seconds to complete, which may be a performance bottleneck.
2021-11-17 04:56:14,094	WARNING util.py:137 -- The `experiment_checkpoint` operation took 13.383841514587402 seconds to complete, which may be a performance bottleneck.
2021-11-17 04:56:14,100	ERROR trial_runner.py:349 -- Trial Runner checkpointing failed.
Traceback (most recent call last):
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 347, in step
    self.checkpoint()
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 278, in checkpoint
    os.rename(tmp_file_name, self.checkpoint_file)
FileNotFoundError: [Errno 2] No such file or directory: '/h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics/.tmp_checkpoint' -> '/h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics/experiment_state-2021-11-17_04-39-43.json'
slurmstepd: error: *** JOB 4985082 ON gpu036 CANCELLED AT 2021-11-17T05:45:28 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-17 05:47:46,943	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 52.9 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-17 05:47:47,239	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 45056 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-17 05:47:48,581	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-17 05:47:49,452	INFO trial_runner.py:169 -- Resuming trial.
2021-11-17 05:47:49,453	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-17 05:47:49,910	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-17 06:04:14,773	WARNING util.py:137 -- The `experiment_checkpoint` operation took 13.469141006469727 seconds to complete, which may be a performance bottleneck.
2021-11-17 06:55:58,277	WARNING util.py:137 -- The `process_trial_save` operation took 0.8686172962188721 seconds to complete, which may be a performance bottleneck.
2021-11-17 06:55:58,277	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
slurmstepd: error: *** JOB 4985082 ON gpu148 CANCELLED AT 2021-11-17T07:38:02 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-17 07:40:13,843	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 53.53 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-17 07:40:14,131	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21412007936 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-17 07:40:15,969	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-17 07:40:16,505	INFO trial_runner.py:169 -- Resuming trial.
2021-11-17 07:40:16,505	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-17 07:40:17,026	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-17 07:46:14,526	WARNING util.py:137 -- The `fetch_result` operation took 1.281113862991333 seconds to complete, which may be a performance bottleneck.
2021-11-17 07:46:15,619	WARNING util.py:137 -- The `process_trial` operation took 2.5358622074127197 seconds to complete, which may be a performance bottleneck.
2021-11-17 07:46:19,024	WARNING util.py:137 -- The `experiment_checkpoint` operation took 3.382798194885254 seconds to complete, which may be a performance bottleneck.
2021-11-17 07:53:50,054	WARNING util.py:137 -- The `experiment_checkpoint` operation took 14.239873886108398 seconds to complete, which may be a performance bottleneck.
2021-11-17 07:53:50,054	ERROR trial_runner.py:349 -- Trial Runner checkpointing failed.
Traceback (most recent call last):
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 347, in step
    self.checkpoint()
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 278, in checkpoint
    os.rename(tmp_file_name, self.checkpoint_file)
FileNotFoundError: [Errno 2] No such file or directory: '/h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics/.tmp_checkpoint' -> '/h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics/experiment_state-2021-11-17_07-40-16.json'
slurmstepd: error: *** JOB 4985082 ON gpu044 CANCELLED AT 2021-11-17T08:41:00 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-17 08:43:39,557	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 53.9 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-17 08:43:39,830	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21470441472 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-17 08:43:41,129	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-17 08:43:42,007	INFO trial_runner.py:169 -- Resuming trial.
2021-11-17 08:43:42,007	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-17 08:43:42,578	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-17 08:49:21,417	WARNING util.py:137 -- The `fetch_result` operation took 0.6069567203521729 seconds to complete, which may be a performance bottleneck.
2021-11-17 08:49:21,838	WARNING util.py:137 -- The `process_trial` operation took 1.138730525970459 seconds to complete, which may be a performance bottleneck.
2021-11-17 08:49:22,720	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8740890026092529 seconds to complete, which may be a performance bottleneck.
2021-11-17 08:56:13,557	WARNING util.py:137 -- The `experiment_checkpoint` operation took 3.296570301055908 seconds to complete, which may be a performance bottleneck.
2021-11-17 09:46:57,888	WARNING util.py:137 -- The `process_trial_save` operation took 0.8063907623291016 seconds to complete, which may be a performance bottleneck.
2021-11-17 09:46:57,889	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
slurmstepd: error: *** JOB 4985082 ON gpu009 CANCELLED AT 2021-11-17T09:49:00 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-17 09:51:12,870	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 53.62 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-17 09:51:13,159	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21471940608 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-17 09:51:14,456	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-17 09:51:15,268	INFO trial_runner.py:169 -- Resuming trial.
2021-11-17 09:51:15,268	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-17 09:51:15,865	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-17 09:56:36,213	WARNING util.py:137 -- The `fetch_result` operation took 1.0337779521942139 seconds to complete, which may be a performance bottleneck.
2021-11-17 09:56:37,379	WARNING util.py:137 -- The `process_trial` operation took 2.26251482963562 seconds to complete, which may be a performance bottleneck.
2021-11-17 09:56:40,786	WARNING util.py:137 -- The `experiment_checkpoint` operation took 3.3900563716888428 seconds to complete, which may be a performance bottleneck.
2021-11-17 10:02:38,769	WARNING util.py:137 -- The `experiment_checkpoint` operation took 10.592690467834473 seconds to complete, which may be a performance bottleneck.
2021-11-17 10:47:18,571	WARNING util.py:137 -- The `process_trial_save` operation took 0.8649344444274902 seconds to complete, which may be a performance bottleneck.
2021-11-17 10:47:18,571	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
slurmstepd: error: *** JOB 4985082 ON gpu005 CANCELLED AT 2021-11-17T10:51:19 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-17 10:53:32,736	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 52.8 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-17 10:53:33,036	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 20816166912 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-17 10:53:34,447	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-17 10:53:35,332	INFO trial_runner.py:169 -- Resuming trial.
2021-11-17 10:53:35,332	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-17 10:53:35,829	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-17 10:59:34,598	WARNING util.py:137 -- The `fetch_result` operation took 3.9860033988952637 seconds to complete, which may be a performance bottleneck.
2021-11-17 10:59:38,778	WARNING util.py:137 -- The `process_trial` operation took 8.311726331710815 seconds to complete, which may be a performance bottleneck.
2021-11-17 10:59:51,699	WARNING util.py:137 -- The `experiment_checkpoint` operation took 12.897885084152222 seconds to complete, which may be a performance bottleneck.
2021-11-17 11:06:51,837	WARNING util.py:137 -- The `experiment_checkpoint` operation took 15.029656171798706 seconds to complete, which may be a performance bottleneck.
2021-11-17 11:56:19,058	WARNING util.py:137 -- The `process_trial_save` operation took 0.9694702625274658 seconds to complete, which may be a performance bottleneck.
2021-11-17 11:56:19,058	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-17 12:04:38,893	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5420231819152832 seconds to complete, which may be a performance bottleneck.
slurmstepd: error: *** JOB 4985082 ON gpu127 CANCELLED AT 2021-11-17T12:20:34 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-17 12:22:47,754	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 48.23 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-17 12:22:48,050	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 18496581632 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-17 12:22:49,409	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-17 12:22:50,338	INFO trial_runner.py:169 -- Resuming trial.
2021-11-17 12:22:50,339	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-17 12:22:50,931	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-17 12:28:54,046	WARNING util.py:137 -- The `fetch_result` operation took 1.0343706607818604 seconds to complete, which may be a performance bottleneck.
2021-11-17 12:28:55,141	WARNING util.py:137 -- The `process_trial` operation took 2.2181906700134277 seconds to complete, which may be a performance bottleneck.
2021-11-17 12:29:04,178	WARNING util.py:137 -- The `experiment_checkpoint` operation took 9.016618967056274 seconds to complete, which may be a performance bottleneck.
2021-11-17 12:29:06,268	WARNING util.py:137 -- The `on_step_begin` operation took 0.5664753913879395 seconds to complete, which may be a performance bottleneck.
2021-11-17 12:33:40,061	WARNING util.py:137 -- The `experiment_checkpoint` operation took 18.659119129180908 seconds to complete, which may be a performance bottleneck.
2021-11-17 12:37:00,930	WARNING util.py:137 -- The `process_trial` operation took 0.5264139175415039 seconds to complete, which may be a performance bottleneck.
2021-11-17 13:27:55,747	WARNING util.py:137 -- The `process_trial_save` operation took 1.0155701637268066 seconds to complete, which may be a performance bottleneck.
2021-11-17 13:27:55,766	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-17 13:30:54,910	WARNING util.py:137 -- The `experiment_checkpoint` operation took 7.30298376083374 seconds to complete, which may be a performance bottleneck.
slurmstepd: error: *** JOB 4985082 ON gpu026 CANCELLED AT 2021-11-17T13:42:52 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-17 13:45:02,130	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 53.57 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-17 13:45:02,405	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 18650726400 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-17 13:45:03,848	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-17 13:45:04,703	INFO trial_runner.py:169 -- Resuming trial.
2021-11-17 13:45:04,703	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-17 13:45:05,330	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-17 13:50:38,695	WARNING util.py:137 -- The `fetch_result` operation took 2.114382743835449 seconds to complete, which may be a performance bottleneck.
2021-11-17 13:50:40,960	WARNING util.py:137 -- The `process_trial` operation took 4.479566812515259 seconds to complete, which may be a performance bottleneck.
2021-11-17 13:50:46,705	WARNING util.py:137 -- The `experiment_checkpoint` operation took 5.738717794418335 seconds to complete, which may be a performance bottleneck.
2021-11-17 13:54:23,430	WARNING util.py:137 -- The `experiment_checkpoint` operation took 14.94702672958374 seconds to complete, which may be a performance bottleneck.
2021-11-17 14:44:07,555	WARNING util.py:137 -- The `process_trial_save` operation took 1.1477205753326416 seconds to complete, which may be a performance bottleneck.
2021-11-17 14:44:07,555	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-17 14:46:43,981	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.7673320770263672 seconds to complete, which may be a performance bottleneck.
slurmstepd: error: *** JOB 4985082 ON gpu026 CANCELLED AT 2021-11-17T14:46:53 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-17 14:49:07,815	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 53.12 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-17 14:49:08,105	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 17659342848 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-17 14:49:09,680	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-17 14:49:10,564	INFO trial_runner.py:169 -- Resuming trial.
2021-11-17 14:49:10,564	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-17 14:49:11,068	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-17 14:55:23,815	WARNING util.py:137 -- The `fetch_result` operation took 4.364854574203491 seconds to complete, which may be a performance bottleneck.
2021-11-17 14:55:27,975	WARNING util.py:137 -- The `process_trial` operation took 8.640407800674438 seconds to complete, which may be a performance bottleneck.
2021-11-17 14:55:40,440	WARNING util.py:137 -- The `experiment_checkpoint` operation took 12.447500944137573 seconds to complete, which may be a performance bottleneck.
2021-11-17 14:59:10,023	WARNING util.py:137 -- The `experiment_checkpoint` operation took 17.722853183746338 seconds to complete, which may be a performance bottleneck.
2021-11-17 15:46:38,081	WARNING util.py:137 -- The `process_trial_save` operation took 10.045267581939697 seconds to complete, which may be a performance bottleneck.
2021-11-17 15:46:38,081	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
slurmstepd: error: *** JOB 4985082 ON gpu130 CANCELLED AT 2021-11-17T15:49:28 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-17 15:51:37,920	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 49.15 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-17 15:51:38,215	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21473935360 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-17 15:57:02,761	WARNING util.py:137 -- The `fetch_result` operation took 0.952683687210083 seconds to complete, which may be a performance bottleneck.
2021-11-17 15:57:03,915	WARNING util.py:137 -- The `process_trial` operation took 2.17765474319458 seconds to complete, which may be a performance bottleneck.
2021-11-17 16:25:22,475	WARNING util.py:137 -- The `experiment_checkpoint` operation took 3.624659776687622 seconds to complete, which may be a performance bottleneck.
slurmstepd: error: *** JOB 4985082 ON gpu016 CANCELLED AT 2021-11-17T16:51:31 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-17 16:54:57,958	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 48.59 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-17 16:54:58,245	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21473939456 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-17 17:00:13,367	WARNING util.py:137 -- The `fetch_result` operation took 0.7367141246795654 seconds to complete, which may be a performance bottleneck.
2021-11-17 17:00:14,401	WARNING util.py:137 -- The `process_trial` operation took 1.8377504348754883 seconds to complete, which may be a performance bottleneck.
2021-11-17 17:13:38,075	ERROR trial_runner.py:519 -- Trial BaselinePPOTrainer_cleanup_env_00000: Error processing event.
Traceback (most recent call last):
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 502, in _process_trial
    result, terminate=(decision == TrialScheduler.STOP))
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial.py", line 472, in update_last_result
    self.result_logger.on_result(self.last_result)
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/logger.py", line 336, in on_result
    _logger.on_result(result)
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/logger.py", line 116, in on_result
    self.local_out.flush()
OSError: [Errno 116] Stale file handle
2021-11-17 17:13:39,135	ERROR trial_executor.py:64 -- Trial BaselinePPOTrainer_cleanup_env_00000: Error checkpointing trial metadata.
Traceback (most recent call last):
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 502, in _process_trial
    result, terminate=(decision == TrialScheduler.STOP))
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial.py", line 472, in update_last_result
    self.result_logger.on_result(self.last_result)
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/logger.py", line 336, in on_result
    _logger.on_result(result)
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/logger.py", line 116, in on_result
    self.local_out.flush()
OSError: [Errno 116] Stale file handle

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_executor.py", line 61, in try_checkpoint_metadata
    self._cached_trial_state[trial.trial_id] = trial.__getstate__()
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial.py", line 550, in __getstate__
    self.result_logger.flush(sync_down=False)
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/logger.py", line 350, in flush
    _logger.flush()
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/logger.py", line 122, in flush
    self.local_out.flush()
OSError: [Errno 116] Stale file handle
2021-11-17 17:13:39,145	ERROR ray_trial_executor.py:277 -- Trial BaselinePPOTrainer_cleanup_env_00000: Error stopping runner.
Traceback (most recent call last):
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 502, in _process_trial
    result, terminate=(decision == TrialScheduler.STOP))
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial.py", line 472, in update_last_result
    self.result_logger.on_result(self.last_result)
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/logger.py", line 336, in on_result
    _logger.on_result(result)
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/logger.py", line 116, in on_result
    self.local_out.flush()
OSError: [Errno 116] Stale file handle

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py", line 266, in _stop_trial
    trial.write_error_log(error_msg)
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial.py", line 360, in write_error_log
    with open(self.error_file, "a+") as f:
FileNotFoundError: [Errno 2] No such file or directory: '/h/dtradke/ray_results/cleanup_baseline_PPO_2teams_6agents_custom_metrics/BaselinePPOTrainer_cleanup_env_0_2021-11-17_16-54-587hq2rk5p/error.txt'
2021-11-17 17:13:39,147	ERROR trial_executor.py:64 -- Trial BaselinePPOTrainer_cleanup_env_00000: Error checkpointing trial metadata.
Traceback (most recent call last):
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 502, in _process_trial
    result, terminate=(decision == TrialScheduler.STOP))
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial.py", line 472, in update_last_result
    self.result_logger.on_result(self.last_result)
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/logger.py", line 336, in on_result
    _logger.on_result(result)
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/logger.py", line 116, in on_result
    self.local_out.flush()
OSError: [Errno 116] Stale file handle

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py", line 266, in _stop_trial
    trial.write_error_log(error_msg)
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial.py", line 360, in write_error_log
    with open(self.error_file, "a+") as f:
FileNotFoundError: [Errno 2] No such file or directory: '/h/dtradke/ray_results/cleanup_baseline_PPO_2teams_6agents_custom_metrics/BaselinePPOTrainer_cleanup_env_0_2021-11-17_16-54-587hq2rk5p/error.txt'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_executor.py", line 61, in try_checkpoint_metadata
    self._cached_trial_state[trial.trial_id] = trial.__getstate__()
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial.py", line 550, in __getstate__
    self.result_logger.flush(sync_down=False)
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/logger.py", line 350, in flush
    _logger.flush()
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/logger.py", line 122, in flush
    self.local_out.flush()
OSError: [Errno 116] Stale file handle
2021-11-17 17:13:39,200	WARNING util.py:137 -- The `process_trial` operation took 1.1700129508972168 seconds to complete, which may be a performance bottleneck.
Traceback (most recent call last):
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 502, in _process_trial
    result, terminate=(decision == TrialScheduler.STOP))
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial.py", line 472, in update_last_result
    self.result_logger.on_result(self.last_result)
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/logger.py", line 336, in on_result
    _logger.on_result(result)
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/logger.py", line 116, in on_result
    self.local_out.flush()
OSError: [Errno 116] Stale file handle

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "train.py", line 432, in <module>
    run(parsed_args, experiment)
  File "train.py", line 425, in run
    reuse_actors=args.tune_hparams,
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/tune.py", line 325, in run
    runner.step()
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 339, in step
    self._process_events()  # blocking
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 452, in _process_events
    self._process_trial(trial)
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 520, in _process_trial
    self._process_trial_failure(trial, traceback.format_exc())
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 582, in _process_trial_failure
    self._try_recover(trial, error_msg)
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 630, in _try_recover
    trial.result_logger.flush()
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/logger.py", line 350, in flush
    _logger.flush()
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/logger.py", line 122, in flush
    self.local_out.flush()
OSError: [Errno 116] Stale file handle
2021-11-17 17:13:39,858	WARNING worker.py:1090 -- A worker died or was killed while executing task ffffffffffffffff54761bfd0100.
2021-11-17 17:13:39,858	WARNING worker.py:1090 -- A worker died or was killed while executing task ffffffffffffffff9b1908ea0100.
2021-11-17 17:13:39,858	WARNING worker.py:1090 -- A worker died or was killed while executing task ffffffffffffffffd03b8d120100.
2021-11-17 17:13:39,859	WARNING worker.py:1090 -- A worker died or was killed while executing task ffffffffffffffff4d81fd5d0100.
2021-11-17 17:13:39,859	WARNING worker.py:1090 -- A worker died or was killed while executing task ffffffffffffffff7a78cec90100.
2021-11-17 17:13:39,859	WARNING worker.py:1090 -- A worker died or was killed while executing task ffffffffffffffff2512146c0100.
